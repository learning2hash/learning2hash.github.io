[{"key": "2019scratch", "year": "2019", "citations": "110", "title": "SCRATCH: A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval", "abstract": "<p>In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method\u2014Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [12.861410140991211, 2.686936855316162], "cluster": 6}, {"key": "2025scratch", "year": "2019", "citations": "110", "title": "SCRATCH: A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval", "abstract": "<p>In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method\u2014Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [12.861191749572754, 2.6869874000549316], "cluster": 6}, {"key": "abbad2020graph", "year": "2020", "citations": "1", "title": "A Graph-based Approach To Derive The Geodesic Distance On Statistical Manifolds: Application To Multimedia Information Retrieval", "abstract": "<p>In this paper, we leverage the properties of non-Euclidean Geometry to define\nthe Geodesic distance (GD) on the space of statistical manifolds. The Geodesic\ndistance is a real and intuitive similarity measure that is a good alternative\nto the purely statistical and extensively used Kullback-Leibler divergence\n(KLD). Despite the effectiveness of the GD, a closed-form does not exist for\nmany manifolds, since the geodesic equations are hard to solve. This explains\nthat the major studies have been content to use numerical approximations.\nNevertheless, most of those do not take account of the manifold properties,\nwhich leads to a loss of information and thus to low performances. We propose\nan approximation of the Geodesic distance through a graph-based method. This\nlatter permits to well represent the structure of the statistical manifold, and\nrespects its geometrical properties. Our main aim is to compare the graph-based\napproximation to the state of the art approximations. Thus, the proposed\napproach is evaluated for two statistical manifolds, namely the Weibull\nmanifold and the Gamma manifold, considering the Content-Based Texture\nRetrieval application on different databases.</p>\n", "tags": ["Graph-Based-Ann"], "tsne_embedding": [62.33473205566406, 11.484620094299316], "cluster": 9}, {"key": "abbad2021graph", "year": "2020", "citations": "1", "title": "A Graph-based Approach To Derive The Geodesic Distance On Statistical Manifolds: Application To Multimedia Information Retrieval", "abstract": "<p>In this paper, we leverage the properties of non-Euclidean Geometry to define\nthe Geodesic distance (GD) on the space of statistical manifolds. The Geodesic\ndistance is a real and intuitive similarity measure that is a good alternative\nto the purely statistical and extensively used Kullback-Leibler divergence\n(KLD). Despite the effectiveness of the GD, a closed-form does not exist for\nmany manifolds, since the geodesic equations are hard to solve. This explains\nthat the major studies have been content to use numerical approximations.\nNevertheless, most of those do not take account of the manifold properties,\nwhich leads to a loss of information and thus to low performances. We propose\nan approximation of the Geodesic distance through a graph-based method. This\nlatter permits to well represent the structure of the statistical manifold, and\nrespects its geometrical properties. Our main aim is to compare the graph-based\napproximation to the state of the art approximations. Thus, the proposed\napproach is evaluated for two statistical manifolds, namely the Weibull\nmanifold and the Gamma manifold, considering the Content-Based Texture\nRetrieval application on different databases.</p>\n", "tags": ["Graph-Based-Ann"], "tsne_embedding": [62.334869384765625, 11.4846830368042], "cluster": 9}, {"key": "abdallah2024arabicaqa", "year": "2024", "citations": "7", "title": "Arabicaqa: A Comprehensive Dataset For Arabic Question Answering", "abstract": "<p>In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.</p>\n", "tags": ["Text-Retrieval", "Scalability", "SIGIR", "Datasets", "Evaluation"], "tsne_embedding": [6.3845648765563965, -31.984235763549805], "cluster": 7}, {"key": "abdulahhad2018concept", "year": "2018", "citations": "2", "title": "Concept Embedding For Information Retrieval", "abstract": "<p>Concepts are used to solve the term-mismatch problem. However, we need an\neffective similarity measure between concepts. Word embedding presents a\npromising solution. We present in this study three approaches to build concepts\nvectors based on words vectors. We use a vector-based measure to estimate\ninter-concepts similarity. Our experiments show promising results. Furthermore,\nwords and concepts become comparable. This could be used to improve conceptual\nindexing process.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-38.65964126586914, -23.147476196289062], "cluster": 5}, {"key": "abdulahhad2020concept", "year": "2018", "citations": "2", "title": "Concept Embedding For Information Retrieval", "abstract": "<p>Concepts are used to solve the term-mismatch problem. However, we need an\neffective similarity measure between concepts. Word embedding presents a\npromising solution. We present in this study three approaches to build concepts\nvectors based on words vectors. We use a vector-based measure to estimate\ninter-concepts similarity. Our experiments show promising results. Furthermore,\nwords and concepts become comparable. This could be used to improve conceptual\nindexing process.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-38.659645080566406, -23.147476196289062], "cluster": 5}, {"key": "abouelnaga2021distillpose", "year": "2021", "citations": "7", "title": "Distillpose: Lightweight Camera Localization Using Auxiliary Learning", "abstract": "<p>We propose a lightweight retrieval-based pipeline to predict 6DOF camera\nposes from RGB images. Our pipeline uses a convolutional neural network (CNN)\nto encode a query image as a feature vector. A nearest neighbor lookup finds\nthe pose-wise nearest database image. A siamese convolutional neural network\nregresses the relative pose from the nearest neighboring database image to the\nquery image. The relative pose is then applied to the nearest neighboring\nabsolute pose to obtain the query image\u2019s final absolute pose prediction. Our\nmodel is a distilled version of NN-Net that reduces its parameters by 98.87%,\ninformation retrieval feature vector size by 87.5%, and inference time by\n89.18% without a significant decrease in localization accuracy.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-52.788230895996094, -4.10336971282959], "cluster": 0}, {"key": "adir2022privacy", "year": "2022", "citations": "5", "title": "Privacy-preserving Record Linkage Using Local Sensitive Hash And Private Set Intersection", "abstract": "<p>The amount of data stored in data repositories increases every year. This\nmakes it challenging to link records between different datasets across\ncompanies and even internally, while adhering to privacy regulations. Address\nor name changes, and even different spelling used for entity data, can prevent\ncompanies from using private deduplication or record-linking solutions such as\nprivate set intersection (PSI). To this end, we propose a new and efficient\nprivacy-preserving record linkage (PPRL) protocol that combines PSI and local\nsensitive hash (LSH) functions, and runs in linear time. We explain the privacy\nguarantees that our protocol provides and demonstrate its practicality by\nexecuting the protocol over two datasets with \\(2^{20}\\) records each, in \\(11-45\\)\nminutes, depending on network settings.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [34.85667037963867, 3.2776882648468018], "cluster": 9}, {"key": "afshari2021similarity", "year": "2021", "citations": "3", "title": "A Similarity Measure Of Histopathology Images By Deep Embeddings", "abstract": "<p>Histopathology digital scans are large-size images that contain valuable\ninformation at the pixel level. Content-based comparison of these images is a\nchallenging task. This study proposes a content-based similarity measure for\nhigh-resolution gigapixel histopathology images. The proposed similarity\nmeasure is an expansion of cosine vector similarity to a matrix. Each image is\ndivided into same-size patches with a meaningful amount of information (i.e.,\ncontained enough tissue). The similarity is measured by the extraction of\npatch-level deep embeddings of the last pooling layer of a pre-trained deep\nmodel at four different magnification levels, namely, 1x, 2.5x, 5x, and 10x\nmagnifications. In addition, for faster measurement, embedding reduction is\ninvestigated. Finally, to assess the proposed method, an image search method is\nimplemented. Results show that the similarity measure represents the slide\nlabels with a maximum accuracy of 93.18% for top-5 search at 5x magnification.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-30.605243682861328, 14.767309188842773], "cluster": 0}, {"key": "agarwaal2023robust", "year": "2023", "citations": "1", "title": "Robust And Lightweight Audio Fingerprint For Automatic Content Recognition", "abstract": "<p>This research paper presents a novel audio fingerprinting system for\nAutomatic Content Recognition (ACR). By using signal processing techniques and\nstatistical transformations, our proposed method generates compact fingerprints\nof audio segments that are robust to noise degradations present in real-world\naudio. The system is designed to be highly scalable, with the ability to\nidentify thousands of hours of content using fingerprints generated from\nmillions of TVs. The fingerprint\u2019s high temporal correlation and utilization of\nexisting GPU-compatible Approximate Nearest Neighbour (ANN) search algorithms\nmake this possible. Furthermore, the fingerprint generation can run on\nlow-power devices with limited compute, making it accessible to a wide range of\napplications. Experimental results show improvements in our proposed system\ncompared to a min-hash based audio fingerprint on all evaluated metrics,\nincluding accuracy on proprietary ACR datasets, retrieval speed, memory usage,\nand robustness to various noises. For similar retrieval accuracy, our system is\n30x faster and uses 6x fewer fingerprints than the min-hash method.</p>\n", "tags": ["Robustness", "Memory-Efficiency", "Similarity-Search", "Datasets"], "tsne_embedding": [3.1003499031066895, 33.73664093017578], "cluster": 4}, {"key": "agarwal2021dynamic", "year": "2021", "citations": "2", "title": "Dynamic Enumeration Of Similarity Joins", "abstract": "<p>This paper considers enumerating answers to similarity-join queries under\ndynamic updates: Given two sets of \\(n\\) points \\(A,B\\) in \\(\\mathbb{R}^d\\), a metric\n\\(\\phi(\\cdot)\\), and a distance threshold \\(r &gt; 0\\), report all pairs of points\n\\((a, b) \\in A \\times B\\) with \\(\\phi(a,b) \\le r\\). Our goal is to store \\(A,B\\) into\na dynamic data structure that, whenever asked, can enumerate all result pairs\nwith worst-case delay guarantee, i.e., the time between enumerating two\nconsecutive pairs is bounded. Furthermore, the data structure can be\nefficiently updated when a point is inserted into or deleted from \\(A\\) or \\(B\\).\n  We propose several efficient data structures for answering similarity-join\nqueries in low dimension. For exact enumeration of similarity join, we present\nnear-linear-size data structures for \\(\\ell_1, \\ell_\\infty\\) metrics with\n\\(log^{O(1)} n\\) update time and delay. We show that such a data structure is\nnot feasible for the \\(\u2113\u2082\\) metric for \\(d \\ge 4\\). For approximate enumeration\nof similarity join, where the distance threshold is a soft constraint, we\nobtain a unified linear-size data structure for \\(\\ell_p\\) metric, with\n\\(log^{O(1)} n\\) delay and update time. In high dimensions, we present an\nefficient data structure with worst-case delay-guarantee using locality\nsensitive hashing (LSH).</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [19.94786262512207, 50.43955993652344], "cluster": 4}, {"key": "agarwal2023mabnet", "year": "2023", "citations": "0", "title": "Mabnet: Master Assistant Buddy Network With Hybrid Learning For Image Retrieval", "abstract": "<p>Image retrieval has garnered growing interest in recent times. The current\napproaches are either supervised or self-supervised. These methods do not\nexploit the benefits of hybrid learning using both supervision and\nself-supervision. We present a novel Master Assistant Buddy Network (MABNet)\nfor image retrieval which incorporates both learning mechanisms. MABNet\nconsists of master and assistant blocks, both learning independently through\nsupervision and collectively via self-supervision. The master guides the\nassistant by providing its knowledge base as a reference for self-supervision\nand the assistant reports its knowledge back to the master by weight transfer.\nWe perform extensive experiments on public datasets with and without\npost-processing.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Datasets", "Supervised", "Re-Ranking"], "tsne_embedding": [-39.429229736328125, -0.7721848487854004], "cluster": 0}, {"key": "agarwal2024omnisearchsage", "year": "2024", "citations": "1", "title": "Omnisearchsage: Multi-task Multi-entity Embeddings For Pinterest Search", "abstract": "<p>In this paper, we present OmniSearchSage, a versatile and scalable system for\nunderstanding search queries, pins, and products for Pinterest search. We\njointly learn a unified query embedding coupled with pin and product\nembeddings, leading to an improvement of \\(&gt;8%\\) relevance, \\(&gt;7%\\) engagement,\nand \\(&gt;5%\\) ads CTR in Pinterest\u2019s production search system. The main\ncontributors to these gains are improved content understanding, better\nmulti-task learning, and real-time serving. We enrich our entity\nrepresentations using diverse text derived from image captions from a\ngenerative LLM, historical engagement, and user-curated boards. Our multitask\nlearning setup produces a single search query embedding in the same space as\npin and product embeddings and compatible with pre-existing pin and product\nembeddings. We show the value of each feature through ablation studies, and\nshow the effectiveness of a unified model compared to standalone counterparts.\nFinally, we share how these embeddings have been deployed across the Pinterest\nsearch stack, from retrieval to ranking, scaling to serve \\(300k\\) requests per\nsecond at low latency. Our implementation of this work is available at\nhttps://github.com/pinterest/atg-research/tree/main/omnisearchsage.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [-20.195743560791016, -32.89040756225586], "cluster": 3}, {"key": "aggarwal2019multitask", "year": "2019", "citations": "2", "title": "Multitask Text-to-visual Embedding With Titles And Clickthrough Data", "abstract": "<p>Text-visual (or called semantic-visual) embedding is a central problem in\nvision-language research. It typically involves mapping of an image and a text\ndescription to a common feature space through a CNN image encoder and a RNN\nlanguage encoder. In this paper, we propose a new method for learning\ntext-visual embedding using both image titles and click-through data from an\nimage search engine. We also propose a new triplet loss function by modeling\npositive awareness of the embedding, and introduce a novel mini-batch-based\nhard negative sampling approach for better data efficiency in the learning\nprocess. Experimental results show that our proposed method outperforms\nexisting methods, and is also effective for real-world text-to-visual\nretrieval.</p>\n", "tags": ["Efficiency", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-25.115983963012695, -25.034080505371094], "cluster": 5}, {"key": "aghazadeh2016near", "year": "2016", "citations": "0", "title": "Near-isometric Binary Hashing For Large-scale Datasets", "abstract": "<p>We develop a scalable algorithm to learn binary hash codes for indexing\nlarge-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependent\nhashing scheme that quantizes the output of a learned low-dimensional embedding\nto obtain a binary hash code. In contrast to conventional hashing schemes,\nwhich typically rely on an \\(\u2113\u2082\\)-norm (i.e., average distortion)\nminimization, NIBH is based on a \\(\\ell_{\\infty}\\)-norm (i.e., worst-case\ndistortion) minimization that provides several benefits, including superior\ndistance, ranking, and near-neighbor preservation performance. We develop a\npractical and efficient algorithm for NIBH based on column generation that\nscales well to large datasets. A range of experimental evaluations demonstrate\nthe superiority of NIBH over ten state-of-the-art binary hashing schemes.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [7.977275848388672, 33.5783576965332], "cluster": 4}, {"key": "aghazadeh2020distributed", "year": "2020", "citations": "0", "title": "A Distributed Approximate Nearest Neighbor Method For Real-time Face Recognition", "abstract": "<p>Nowadays, face recognition and more generally image recognition have many\napplications in the modern world and are widely used in our daily tasks. This\npaper aims to propose a distributed approximate nearest neighbor (ANN) method\nfor real-time face recognition using a big dataset that involves a lot of\nclasses. The proposed approach is based on using a clustering method to\nseparate the dataset into different clusters and on specifying the importance\nof each cluster by defining cluster weights. To this end, reference instances\nare selected from each cluster based on the cluster weights using a maximum\nlikelihood approach. This process leads to a more informed selection of\ninstances, so it enhances the performance of the algorithm. Experimental\nresults confirm the efficiency of the proposed method and its out-performance\nin terms of accuracy and the processing time.</p>\n", "tags": ["Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [-6.527834415435791, -11.086248397827148], "cluster": 1}, {"key": "agrawal2020tag", "year": "2021", "citations": "21", "title": "Tag Embedding Based Personalized Point Of Interest Recommendation System", "abstract": "<p>Personalized Point of Interest recommendation is very helpful for satisfying\nusers\u2019 needs at new places. In this article, we propose a tag embedding based\nmethod for Personalized Recommendation of Point Of Interest. We model the\nrelationship between tags corresponding to Point Of Interest. The model\nprovides representative embedding corresponds to a tag in a way that related\ntags will be closer. We model Point of Interest-based on tag embedding and also\nmodel the users (user profile) based on the Point Of Interest rated by them.\nfinally, we rank the user\u2019s candidate Point Of Interest based on cosine\nsimilarity between user\u2019s embedding and Point of Interest\u2019s embedding. Further,\nwe find the parameters required to model user by discrete optimizing over\ndifferent measures (like ndcg@5, MRR, \u2026). We also analyze the result while\nconsidering the same parameters for all users and individual parameters for\neach user. Along with it we also analyze the effect on the result while\nchanging the dataset to model the relationship between tags. Our method also\nminimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase\n2 dataset and have significant improvement over all the measures on the state\nof the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,\nwhich shows the effectiveness of the method.</p>\n", "tags": ["Recommender-Systems", "Datasets"], "tsne_embedding": [3.8617300987243652, -8.826427459716797], "cluster": 6}, {"key": "aguerrebere2023similarity", "year": "2023", "citations": "15", "title": "Similarity Search In The Blink Of An Eye With Compressed Indices", "abstract": "<p>Nowadays, data is represented by vectors. Retrieving those vectors, among\nmillions and billions, that are similar to a given query is a ubiquitous\nproblem, known as similarity search, of relevance for a wide range of\napplications. Graph-based indices are currently the best performing techniques\nfor billion-scale similarity search. However, their random-access memory\npattern presents challenges to realize their full potential. In this work, we\npresent new techniques and systems for creating faster and smaller graph-based\nindices. To this end, we introduce a novel vector compression method,\nLocally-adaptive Vector Quantization (LVQ), that uses per-vector scaling and\nscalar quantization to improve search performance with fast similarity\ncomputations and a reduced effective bandwidth, while decreasing memory\nfootprint and barely impacting accuracy. LVQ, when combined with a new\nhigh-performance computing system for graph-based similarity search,\nestablishes the new state of the art in terms of performance and memory\nfootprint. For billions of vectors, LVQ outcompetes the second-best\nalternatives: (1) in the low-memory regime, by up to 20.7x in throughput with\nup to a 3x memory footprint reduction, and (2) in the high-throughput regime by\n5.8x with 1.4x less memory.</p>\n", "tags": ["Graph-Based-Ann", "Quantization", "Scalability", "Similarity-Search", "Memory-Efficiency", "Large-Scale-Search", "Evaluation"], "tsne_embedding": [41.21028137207031, 22.806167602539062], "cluster": 2}, {"key": "aguerrebere2024locally", "year": "2024", "citations": "0", "title": "Locally-adaptive Quantization For Streaming Vector Search", "abstract": "<p>Retrieving the most similar vector embeddings to a given query among a\nmassive collection of vectors has long been a key component of countless\nreal-world applications. The recently introduced Retrieval-Augmented Generation\nis one of the most prominent examples. For many of these applications, the\ndatabase evolves over time by inserting new data and removing outdated data. In\nthese cases, the retrieval problem is known as streaming similarity search.\nWhile Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector\ncompression method, yields state-of-the-art search performance for non-evolving\ndatabases, its usefulness in the streaming setting has not been yet\nestablished. In this work, we study LVQ in streaming similarity search. In\nsupport of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and\nmulti-means LVQ that boost its search performance by up to 28% and 27%,\nrespectively. Our studies show that LVQ and its new variants enable blazing\nfast vector search, outperforming its closest competitor by up to 9.4x for\nidentically distributed data and by up to 8.8x under the challenging scenario\nof data distribution shifts (i.e., where the statistical distribution of the\ndata changes over time). We release our contributions as part of Scalable\nVector Search, an open-source library for high-performance similarity search.</p>\n", "tags": ["Quantization", "Tools-&-Libraries", "Evaluation", "Similarity-Search"], "tsne_embedding": [0.9806201457977295, -23.264997482299805], "cluster": 7}, {"key": "aharon2024classification", "year": "2024", "citations": "0", "title": "A Classification-by-retrieval Framework For Few-shot Anomaly Detection To Detect API Injection Attacks", "abstract": "<p>Application Programming Interface (API) Injection attacks refer to the unauthorized or malicious use of APIs, which are often exploited to gain access to sensitive data or manipulate online systems for illicit purposes. Identifying actors that deceitfully utilize an API poses a demanding problem. Although there have been notable advancements and contributions in the field of API security, there remains a significant challenge when dealing with attackers who use novel approaches that don\u2019t match the well-known payloads commonly seen in attacks. Also, attackers may exploit standard functionalities unconventionally and with objectives surpassing their intended boundaries. Thus, API security needs to be more sophisticated and dynamic than ever, with advanced computational intelligence methods, such as machine learning models that can quickly identify and respond to abnormal behavior. In response to these challenges, we propose a novel unsupervised few-shot anomaly detection framework composed of two main parts: First, we train a dedicated generic language model for API based on FastText embedding. Next, we use Approximate Nearest Neighbor search in a classification-by-retrieval approach. Our framework allows for training a fast, lightweight classification model using only a few examples of normal API requests. We evaluated the performance of our framework using the CSIC 2010 and ATRDF 2023 datasets. The results demonstrate that our framework improves API attack detection accuracy compared to the state-of-the-art (SOTA) unsupervised anomaly detection baselines.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Tools-&-Libraries", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [29.3551025390625, -16.952478408813477], "cluster": 7}, {"key": "ahle2016parameter", "year": "2017", "citations": "19", "title": "Parameter-free Locality Sensitive Hashing For Spherical Range Reporting", "abstract": "<p>We present a data structure for <em>spherical range reporting</em> on a point set\n\\(S\\), i.e., reporting all points in \\(S\\) that lie within radius \\(r\\) of a given\nquery point \\(q\\). Our solution builds upon the Locality-Sensitive Hashing (LSH)\nframework of Indyk and Motwani, which represents the asymptotically best\nsolutions to near neighbor problems in high dimensions. While traditional LSH\ndata structures have several parameters whose optimal values depend on the\ndistance distribution from \\(q\\) to the points of \\(S\\), our data structure is\nparameter-free, except for the space usage, which is configurable by the user.\nNevertheless, its expected query time basically matches that of an LSH data\nstructure whose parameters have been <em>optimally chosen for the data and query</em>\nin question under the given space constraints. In particular, our data\nstructure provides a smooth trade-off between hard queries (typically addressed\nby standard LSH) and easy queries such as those where the number of points to\nreport is a constant fraction of \\(S\\), or where almost all points in \\(S\\) are far\naway from the query point. In contrast, known data structures fix LSH\nparameters based on certain parameters of the input alone.\n  The algorithm has expected query time bounded by \\(O(t (n/t)^\\rho)\\), where \\(t\\)\nis the number of points to report and \\(\\rho\\in (0,1)\\) depends on the data\ndistribution and the strength of the LSH family used. We further present a\nparameter-free way of using multi-probing, for LSH families that support it,\nand show that for many such families this approach allows us to get expected\nquery time close to \\(O(n^\\rho+t)\\), which is the best we can hope to achieve\nusing LSH. The previously best running time in high dimensions was \\(\u03a9(t\nn^\\rho)\\). For many data distributions where the intrinsic dimensionality of the\npoint set close to \\(q\\) is low, we can give improved upper bounds on the\nexpected query time.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [21.397459030151367, 48.58234405517578], "cluster": 4}, {"key": "ahle2017optimal", "year": "2017", "citations": "13", "title": "Optimal Las Vegas Locality Sensitive Data Structures", "abstract": "<p>We show that approximate similarity (near neighbour) search can be solved in\nhigh dimensions with performance matching state of the art (data independent)\nLocality Sensitive Hashing, but with a guarantee of no false negatives.\n  Specifically, we give two data structures for common problems.\n  For \\(c\\)-approximate near neighbour in Hamming space we get query time\n\\(dn^{1/c+o(1)}\\) and space \\(dn^{1+1/c+o(1)}\\) matching that of\n\\cite{indyk1998approximate} and answering a long standing open question\nfrom~\\cite{indyk2000dimensionality} and~\\cite{pagh2016locality} in the\naffirmative.\n  By means of a new deterministic reduction from \\(\\ell_1\\) to Hamming we also\nsolve \\(\\ell_1\\) and \\(\u2113\u2082\\) with query time \\(d^2n^{1/c+o(1)}\\) and space \\(d^2\nn^{1+1/c+o(1)}\\).\n  For \\((s_1,s_2)\\)-approximate Jaccard similarity we get query time\n\\(dn^{\\rho+o(1)}\\) and space \\(dn^{1+\\rho+o(1)}\\),\n\\(\\rho=log\\frac{1+s_1}{2s_1}\\big/log\\frac{1+s_2}{2s_2}\\), when sets have equal\nsize, matching the performance of~\\cite{tobias2016}.\n  The algorithms are based on space partitions, as with classic LSH, but we\nconstruct these using a combination of brute force, tensoring, perfect hashing\nand splitter functions `a la~\\cite{naor1995splitters}. We also show a new\ndimensionality reduction lemma with 1-sided error.</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [22.1679630279541, 49.71842575073242], "cluster": 4}, {"key": "ahle2019subsets", "year": "2020", "citations": "5", "title": "Subsets And Supermajorities: Optimal Hashing-based Set Similarity Search", "abstract": "<p>We formulate and optimally solve a new generalized Set Similarity Search\nproblem, which assumes the size of the database and query sets are known in\nadvance. By creating polylog copies of our data-structure, we optimally solve\nany symmetric Approximate Set Similarity Search problem, including approximate\nversions of Subset Search, Maximum Inner Product Search (MIPS), Jaccard\nSimilarity Search and Partial Match.\n  Our algorithm can be seen as a natural generalization of previous work on Set\nas well as Euclidean Similarity Search, but conceptually it differs by\noptimally exploiting the information present in the sets as well as their\ncomplements, and doing so asymmetrically between queries and stored sets. Doing\nso we improve upon the best previous work: MinHash [J. Discrete Algorithms\n1998], SimHash [STOC 2002], Spherical LSF [SODA 2016, 2017] and Chosen Path\n[STOC 2017] by as much as a factor \\(n^{0.14}\\) in both time and space; or in the\nnear-constant time regime, in space, by an arbitrarily large polynomial factor.\n  Turning the geometric concept, based on Boolean supermajority functions, into\na practical algorithm requires ideas from branching random walks on \\(\\mathbb\nZ^2\\), for which we give the first non-asymptotic near tight analysis.\n  Our lower bounds follow from new hypercontractive arguments, which can be\nseen as characterizing the exact family of similarity search problems for which\nsupermajorities are optimal. The optimality holds for among all hashing based\ndata structures in the random setting, and by reductions, for 1 cell and 2 cell\nprobe data structures. As a side effect, we obtain new hypercontractive bounds\non the directed noise operator \\(T^{p_1 \\to p_2}_\\rho\\).</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search"], "tsne_embedding": [20.608957290649414, 45.8910026550293], "cluster": 4}, {"key": "ahle2020problem", "year": "2020", "citations": "2", "title": "On The Problem Of \\(p_1^{-1}\\) In Locality-sensitive Hashing", "abstract": "<p>A Locality-Sensitive Hash (LSH) function is called\n\\((r,cr,p_1,p_2)\\)-sensitive, if two data-points with a distance less than \\(r\\)\ncollide with probability at least \\(p_1\\) while data points with a distance\ngreater than \\(cr\\) collide with probability at most \\(p_2\\). These functions form\nthe basis of the successful Indyk-Motwani algorithm (STOC 1998) for nearest\nneighbour problems. In particular one may build a \\(c\\)-approximate nearest\nneighbour data structure with query time \\(\\tilde O(n^\\rho/p_1)\\) where\n\\(\\rho=\\frac{log1/p_1}{log1/p_2}\\in(0,1)\\). That is, sub-linear time, as long\nas \\(p_1\\) is not too small. This is significant since most high dimensional\nnearest neighbour problems suffer from the curse of dimensionality, and can\u2019t\nbe solved exact, faster than a brute force linear-time scan of the database.\n  Unfortunately, the best LSH functions tend to have very low collision\nprobabilities, \\(p_1\\) and \\(p_2\\). Including the best functions for Cosine and\nJaccard Similarity. This means that the \\(n^\\rho/p_1\\) query time of LSH is often\nnot sub-linear after all, even for approximate nearest neighbours!\n  In this paper, we improve the general Indyk-Motwani algorithm to reduce the\nquery time of LSH to \\(\\tilde O(n^\\rho/p_1^{1-\\rho})\\) (and the space usage\ncorrespondingly.) Since \\(n^\\rho p_1^{\\rho-1} &lt; n \\Leftrightarrow p_1 &gt; n^{-1}\\),\nour algorithm always obtains sublinear query time, for any collision\nprobabilities at least \\(1/n\\). For \\(p_1\\) and \\(p_2\\) small enough, our improvement\nover all previous methods can be <em>up to a factor \\(n\\)</em> in both query time\nand space.\n  The improvement comes from a simple change to the Indyk-Motwani algorithm,\nwhich can easily be implemented in existing software packages.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search"], "tsne_embedding": [21.949007034301758, 49.413631439208984], "cluster": 4}, {"key": "ajanthan2023adaptive", "year": "2023", "citations": "0", "title": "Adaptive Cross Batch Normalization For Metric Learning", "abstract": "<p>Metric learning is a fundamental problem in computer vision whereby a model\nis trained to learn a semantically useful embedding space via ranking losses.\nTraditionally, the effectiveness of a ranking loss depends on the minibatch\nsize, and is, therefore, inherently limited by the memory constraints of the\nunderlying hardware. While simply accumulating the embeddings across\nminibatches has proved useful (Wang et al. [2020]), we show that it is equally\nimportant to ensure that the accumulated embeddings are up to date. In\nparticular, it is necessary to circumvent the representational drift between\nthe accumulated embeddings and the feature embeddings at the current training\niteration as the learnable parameters are being updated. In this paper, we\nmodel representational drift as distribution misalignment and tackle it using\nmoment matching. The result is a simple method for updating the stored\nembeddings to match the first and second moments of the current embeddings at\neach training iteration. Experiments on three popular image retrieval datasets,\nnamely, SOP, In-Shop, and DeepFashion2, demonstrate that our approach\nsignificantly improves the performance in all scenarios.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-19.699630737304688, -16.397459030151367], "cluster": 1}, {"key": "ak2021fashionsearchnet", "year": "2021", "citations": "4", "title": "Fashionsearchnet-v2: Learning Attribute Representations With Localization For Image Retrieval With Attribute Manipulation", "abstract": "<p>The focus of this paper is on the problem of image retrieval with attribute\nmanipulation. Our proposed work is able to manipulate the desired attributes of\nthe query image while maintaining its other attributes. For example, the collar\nattribute of the query image can be changed from round to v-neck to retrieve\nsimilar images from a large dataset. A key challenge in e-commerce is that\nimages have multiple attributes where users would like to manipulate and it is\nimportant to estimate discriminative feature representations for each of these\nattributes. The proposed FashionSearchNet-v2 architecture is able to learn\nattribute specific representations by leveraging on its weakly-supervised\nlocalization module, which ignores the unrelated features of attributes in the\nfeature space, thus improving the similarity learning. The network is jointly\ntrained with the combination of attribute classification and triplet ranking\nloss to estimate local representations. These local representations are then\nmerged into a single global representation based on the instructed attribute\nmanipulation where desired images can be retrieved with a distance metric. The\nproposed method also provides explainability for its retrieval process to help\nprovide additional information on the attention of the network. Experiments\nperformed on several datasets that are rich in terms of the number of\nattributes show that FashionSearchNet-v2 outperforms the other state-of-the-art\nattribute manipulation techniques. Different than our earlier work\n(FashionSearchNet), we propose several improvements in the learning procedure\nand show that the proposed FashionSearchNet-v2 can be generalized to different\ndomains other than fashion.</p>\n", "tags": ["Supervised", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-33.02716827392578, -4.751641750335693], "cluster": 0}, {"key": "aksoy2022satellite", "year": "2022", "citations": "8", "title": "Satellite Image Search In Agoraeo", "abstract": "<p>The growing operational capability of global Earth Observation (EO) creates\nnew opportunities for data-driven approaches to understand and protect our\nplanet. However, the current use of EO archives is very restricted due to the\nhuge archive sizes and the limited exploration capabilities provided by EO\nplatforms. To address this limitation, we have recently proposed MiLaN, a\ncontent-based image retrieval approach for fast similarity search in satellite\nimage archives. MiLaN is a deep hashing network based on metric learning that\nencodes high-dimensional image features into compact binary hash codes. We use\nthese codes as keys in a hash table to enable real-time nearest neighbor search\nand highly accurate retrieval. In this demonstration, we showcase the\nefficiency of MiLaN by integrating it with EarthQube, a browser and search\nengine within AgoraEO. EarthQube supports interactive visual exploration and\nQuery-by-Example over satellite image repositories. Demo visitors will interact\nwith EarthQube playing the role of different users that search images in a\nlarge-scale remote sensing archive by their semantic content and apply other\nfilters.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Similarity-Search", "Image-Retrieval", "Scalability", "Neural-Hashing"], "tsne_embedding": [-40.15346145629883, -7.799892902374268], "cluster": 0}, {"key": "aladago2024semantic", "year": "2024", "citations": "0", "title": "Semantic Compositions Enhance Vision-language Contrastive Learning", "abstract": "<p>In the field of vision-language contrastive learning, models such as CLIP\ncapitalize on matched image-caption pairs as positive examples and leverage\nwithin-batch non-matching pairs as negatives. This approach has led to\nremarkable outcomes in zero-shot image classification, cross-modal retrieval,\nand linear evaluation tasks. We show that the zero-shot classification and\nretrieval capabilities of CLIP-like models can be improved significantly\nthrough the introduction of semantically composite examples during pretraining.\nInspired by CutMix in vision categorization, we create semantically composite\nimage-caption pairs by merging elements from two distinct instances in the\ndataset via a novel procedure. Our method fuses the captions and blends 50% of\neach image to form a new composite sample. This simple technique (termed CLIP-C\nfor CLIP Compositions), devoid of any additional computational overhead or\nincrease in model parameters, significantly improves zero-shot image\nclassification and cross-modal retrieval. The benefits of CLIP-C are\nparticularly pronounced in settings with relatively limited pretraining data.</p>\n", "tags": ["Self-Supervised", "Few-Shot-&-Zero-Shot", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-32.640350341796875, -28.875402450561523], "cluster": 5}, {"key": "alemu2018multi", "year": "2019", "citations": "23", "title": "Multi-feature Fusion For Image Retrieval Using Constrained Dominant Sets", "abstract": "<p>Aggregating different image features for image retrieval has recently shown\nits effectiveness. While highly effective, though, the question of how to\nuplift the impact of the best features for a specific query image persists as\nan open computer vision problem. In this paper, we propose a computationally\nefficient approach to fuse several hand-crafted and deep features, based on the\nprobabilistic distribution of a given membership score of a constrained cluster\nin an unsupervised manner. First, we introduce an incremental nearest neighbor\n(NN) selection method, whereby we dynamically select k-NN to the query. We then\nbuild several graphs from the obtained NN sets and employ constrained dominant\nsets (CDS) on each graph G to assign edge weights which consider the intrinsic\nmanifold structure of the graph, and detect false matches to the query.\nFinally, we elaborate the computation of feature positive-impact weight (PIW)\nbased on the dispersive degree of the characteristics vector. To this end, we\nexploit the entropy of a cluster membership-score distribution. In addition,\nthe final NN set bypasses a heuristic voting scheme. Experiments on several\nretrieval benchmark datasets show that our method can improve the\nstate-of-the-art result.</p>\n", "tags": ["Unsupervised", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [51.96869659423828, -1.8044211864471436], "cluster": 9}, {"key": "alfasly2024zero", "year": "2024", "citations": "0", "title": "Zero-shot Whole Slide Image Retrieval In Histopathology Using Embeddings Of Foundation Models", "abstract": "<p>We have tested recently published foundation models for histopathology for\nimage retrieval. We report macro average of F1 score for top-1 retrieval,\nmajority of top-3 retrievals, and majority of top-5 retrievals. We perform\nzero-shot retrievals, i.e., we do not alter embeddings and we do not train any\nclassifier. As test data, we used diagnostic slides of TCGA, The Cancer Genome\nAtlas, consisting of 23 organs and 117 cancer subtypes. As a search platform we\nused Yottixel that enabled us to perform WSI search using patches. Achieved F1\nscores show low performance, e.g., for top-5 retrievals, 27% +/- 13%\n(Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40%+/-13% (Yottixel-Virchow),\n41%+/-13% (Yottixel-GigaPath), and 41%+/-14% (GigaPath WSI).</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Image-Retrieval"], "tsne_embedding": [-45.2857780456543, 12.068448066711426], "cluster": 0}, {"key": "alhalah2018traversing", "year": "2018", "citations": "0", "title": "Traversing The Continuous Spectrum Of Image Retrieval With Deep Dynamic Models", "abstract": "<p>We introduce the first work to tackle the image retrieval problem as a\ncontinuous operation. While the proposed approaches in the literature can be\nroughly categorized into two main groups: category- and instance-based\nretrieval, in this work we show that the retrieval task is much richer and more\ncomplex. Image similarity goes beyond this discrete vantage point and spans a\ncontinuous spectrum among the classical operating points of category and\ninstance similarity. However, current retrieval models are static and incapable\nof exploring this rich structure of the retrieval space since they are trained\nand evaluated with a single operating point as a target objective. Hence, we\nintroduce a novel retrieval model that for a given query is capable of\nproducing a dynamic embedding that can target an arbitrary point along the\ncontinuous retrieval spectrum. Our model disentangles the visual signal of a\nquery image into its basic components of categorical and attribute information.\nFurthermore, using a continuous control parameter our model learns to\nreconstruct a dynamic embedding of the query by mixing these components with\ndifferent proportions to target a specific point along the retrieval simplex.\nWe demonstrate our idea in a comprehensive evaluation of the proposed model and\nhighlight the advantages of our approach against a set of well-established\ndiscrete retrieval models.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-24.652381896972656, -28.5532283782959], "cluster": 5}, {"key": "alibey2022gsv", "year": "2022", "citations": "57", "title": "Gsv-cities: Toward Appropriate Supervised Visual Place Recognition", "abstract": "<p>This paper aims to investigate representation learning for large scale visual\nplace recognition, which consists of determining the location depicted in a\nquery image by referring to a database of reference images. This is a\nchallenging task due to the large-scale environmental changes that can occur\nover time (i.e., weather, illumination, season, traffic, occlusion). Progress\nis currently challenged by the lack of large databases with accurate ground\ntruth. To address this challenge, we introduce GSV-Cities, a new image dataset\nproviding the widest geographic coverage to date with highly accurate ground\ntruth, covering more than 40 cities across all continents over a 14-year\nperiod. We subsequently explore the full potential of recent advances in deep\nmetric learning to train networks specifically for place recognition, and\nevaluate how different loss functions influence performance. In addition, we\nshow that performance of existing methods substantially improves when trained\non GSV-Cities. Finally, we introduce a new fully convolutional aggregation\nlayer that outperforms existing techniques, including GeM, NetVLAD and\nCosPlace, and establish a new state-of-the-art on large-scale benchmarks, such\nas Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are\navailable for research purposes at https://github.com/amaralibey/gsv-cities.</p>\n", "tags": ["Distance-Metric-Learning", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [49.9181022644043, -10.61689281463623], "cluster": 9}, {"key": "alomari2019scalable", "year": "2019", "citations": "2", "title": "Scalable Source Code Similarity Detection In Large Code Repositories", "abstract": "<p>Source code similarity are increasingly used in application development to\nidentify clones, isolate bugs, and find copy-rights violations. Similar code\nfragments can be very problematic due to the fact that errors in the original\ncode must be fixed in every copy. Other maintenance changes, such as extensions\nor patches, must be applied multiple times. Furthermore, the diversity of\ncoding styles and flexibility of modern languages makes it difficult and cost\nineffective to manually inspect large code repositories. Therefore, detection\nis only feasible by automatic techniques. We present an efficient and scalable\napproach for similar code fragment identification based on source code control\nflow graphs fingerprinting. The source code is processed to generate control\nflow graphs that are then hashed to create a unique fingerprint of the code\ncapturing semantics as well as syntax similarity. The fingerprints can then be\nefficiently stored and retrieved to perform similarity search between code\nfragments. Experimental results from our prototype implementation supports the\nvalidity of our approach and show its effectiveness and efficiency in\ncomparison with other solutions.</p>\n", "tags": ["Efficiency", "Evaluation", "Similarity-Search"], "tsne_embedding": [26.0396728515625, -16.981769561767578], "cluster": 7}, {"key": "amara2021nearest", "year": "2022", "citations": "1", "title": "Nearest Neighbor Search With Compact Codes: A Decoder Perspective", "abstract": "<p>Modern approaches for fast retrieval of similar vectors on billion-scaled\ndatasets rely on compressed-domain approaches such as binary sketches or\nproduct quantization. These methods minimize a certain loss, typically the mean\nsquared error or other objective functions tailored to the retrieval problem.\nIn this paper, we re-interpret popular methods such as binary hashing or\nproduct quantizers as auto-encoders, and point out that they implicitly make\nsuboptimal assumptions on the form of the decoder. We design\nbackward-compatible decoders that improve the reconstruction of the vectors\nfrom the same codes, which translates to a better performance in nearest\nneighbor search. Our method significantly improves over binary hashing methods\nor product quantization on popular benchmarks.</p>\n", "tags": ["Datasets", "Evaluation", "Compact-Codes", "Quantization", "Efficiency", "Hashing-Methods", "Multimodal-Retrieval", "Large-Scale-Search", "Scalability"], "tsne_embedding": [1.4346513748168945, 38.10881042480469], "cluster": 4}, {"key": "amato2016aggregating", "year": "2017", "citations": "11", "title": "Aggregating Binary Local Descriptors For Image Retrieval", "abstract": "<p>Content-Based Image Retrieval based on local features is computationally\nexpensive because of the complexity of both extraction and matching of local\nfeature. On one hand, the cost for extracting, representing, and comparing\nlocal visual descriptors has been dramatically reduced by recently proposed\nbinary local features. On the other hand, aggregation techniques provide a\nmeaningful summarization of all the extracted feature of an image into a single\ndescriptor, allowing us to speed up and scale up the image search. Only a few\nworks have recently mixed together these two research directions, defining\naggregation methods for binary local features, in order to leverage on the\nadvantage of both approaches. In this paper, we report an extensive comparison\namong state-of-the-art aggregation methods applied to binary features. Then, we\nmathematically formalize the application of Fisher Kernels to Bernoulli Mixture\nModels. Finally, we investigate the combination of the aggregated binary\nfeatures with the emerging Convolutional Neural Network (CNN) features. Our\nresults show that aggregation methods on binary features are effective and\nrepresent a worthwhile alternative to the direct matching. Moreover, the\ncombination of the CNN with the Fisher Vector (FV) built upon binary features\nallowed us to obtain a relative improvement over the CNN results that is in\nline with that recently obtained using the combination of the CNN with the FV\nbuilt upon SIFTs. The advantage of using the FV built upon binary features is\nthat the extraction process of binary features is about two order of magnitude\nfaster than SIFTs.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-45.282318115234375, 5.294288635253906], "cluster": 0}, {"key": "amato2016using", "year": "2016", "citations": "2", "title": "Using Apache Lucene To Search Vector Of Locally Aggregated Descriptors", "abstract": "<p>Surrogate Text Representation (STR) is a profitable solution to efficient\nsimilarity search on metric space using conventional text search engines, such\nas Apache Lucene. This technique is based on comparing the permutations of some\nreference objects in place of the original metric distance. However, the\nAchilles heel of STR approach is the need to reorder the result set of the\nsearch according to the metric distance. This forces to use a support database\nto store the original objects, which requires efficient random I/O on a fast\nsecondary memory (such as flash-based storages). In this paper, we propose to\nextend the Surrogate Text Representation to specifically address a class of\nvisual metric objects known as Vector of Locally Aggregated Descriptors (VLAD).\nThis approach is based on representing the individual sub-vectors forming the\nVLAD vector with the STR, providing a finer representation of the vector and\nenabling us to get rid of the reordering phase. The experiments on a publicly\navailable dataset show that the extended STR outperforms the baseline STR\nachieving satisfactory performance near to the one obtained with the original\nVLAD vectors.</p>\n", "tags": ["Evaluation", "Similarity-Search", "Text-Retrieval", "Datasets"], "tsne_embedding": [-37.69911193847656, 11.855026245117188], "cluster": 0}, {"key": "amrouche2021hashing", "year": "2021", "citations": "5", "title": "Hashing And Metric Learning For Charged Particle Tracking", "abstract": "<p>We propose a novel approach to charged particle tracking at high intensity\nparticle colliders based on Approximate Nearest Neighbors search. With hundreds\nof thousands of measurements per collision to be reconstructed e.g. at the High\nLuminosity Large Hadron Collider, the currently employed combinatorial track\nfinding approaches become inadequate. Here, we use hashing techniques to\nseparate measurements into buckets of 20-50 hits and increase their purity\nusing metric learning. Two different approaches are studied to further resolve\ntracks inside buckets: Local Fisher Discriminant Analysis and Neural Networks\nfor triplet similarity learning. We demonstrate the proposed approach on\nsimulated collisions and show significant speed improvement with bucket\ntracking efficiency of 96% and a fake rate of 8% on unseen particle events.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Distance-Metric-Learning"], "tsne_embedding": [28.384052276611328, 29.15773582458496], "cluster": 2}, {"key": "an2020fast", "year": "2022", "citations": "44", "title": "Fast And Incremental Loop Closure Detection With Deep Features And Proximity Graphs", "abstract": "<p>In recent years, the robotics community has extensively examined methods\nconcerning the place recognition task within the scope of simultaneous\nlocalization and mapping applications.This article proposes an appearance-based\nloop closure detection pipeline named ``FILD++\u201d (Fast and Incremental Loop\nclosure Detection).First, the system is fed by consecutive images and, via\npassing them twice through a single convolutional neural network, global and\nlocal deep features are extracted.Subsequently, a hierarchical navigable\nsmall-world graph incrementally constructs a visual database representing the\nrobot\u2019s traversed path based on the computed global features.Finally, a query\nimage, grabbed each time step, is set to retrieve similar locations on the\ntraversed route.An image-to-image pairing follows, which exploits local\nfeatures to evaluate the spatial information. Thus, in the proposed article, we\npropose a single network for global and local feature extraction in contrast to\nour previous work (FILD), while an exhaustive search for the verification\nprocess is adopted over the generated deep local features avoiding the\nutilization of hash codes. Exhaustive experiments on eleven publicly available\ndatasets exhibit the system\u2019s high performance (achieving the highest recall\nscore on eight of them) and low execution times (22.05 ms on average in New\nCollege, which is the largest one containing 52480 images) compared to other\nstate-of-the-art approaches.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [-35.997013092041016, -8.59046459197998], "cluster": 5}, {"key": "an2023unicom", "year": "2023", "citations": "10", "title": "Unicom: Universal And Compact Representation Learning For Image Retrieval", "abstract": "<p>Modern image retrieval methods typically rely on fine-tuning pre-trained\nencoders to extract image-level descriptors. However, the most widely used\nmodels are pre-trained on ImageNet-1K with limited classes. The pre-trained\nfeature representation is therefore not universal enough to generalize well to\nthe diverse open-world classes. In this paper, we first cluster the large-scale\nLAION400M into one million pseudo classes based on the joint textual and visual\nfeatures extracted by the CLIP model. Due to the confusion of label\ngranularity, the automatically clustered dataset inevitably contains heavy\ninter-class conflict. To alleviate such conflict, we randomly select partial\ninter-class prototypes to construct the margin-based softmax loss. To further\nenhance the low-dimensional feature representation, we randomly select partial\nfeature dimensions when calculating the similarities between embeddings and\nclass-wise prototypes. The dual random partial selections are with respect to\nthe class dimension and the feature dimension of the prototype matrix, making\nthe classification conflict-robust and the feature embedding compact. Our\nmethod significantly outperforms state-of-the-art unsupervised and supervised\nimage retrieval approaches on multiple benchmarks. The code and pre-trained\nmodels are released to facilitate future research\nhttps://github.com/deepglint/unicom.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [-26.96397590637207, -5.681517601013184], "cluster": 1}, {"key": "an2024accurate", "year": "2024", "citations": "0", "title": "Accurate And Fast Pixel Retrieval With Spatial And Uncertainty Aware Hypergraph Diffusion", "abstract": "<p>This paper presents a novel method designed to enhance the efficiency and\naccuracy of both image retrieval and pixel retrieval. Traditional diffusion\nmethods struggle to propagate spatial information effectively in conventional\ngraphs due to their reliance on scalar edge weights. To overcome this\nlimitation, we introduce a hypergraph-based framework, uniquely capable of\nefficiently propagating spatial information using local features during query\ntime, thereby accurately retrieving and localizing objects within a database.\n  Additionally, we innovatively utilize the structural information of the image\ngraph through a technique we term \u201ccommunity selection\u201d. This approach allows\nfor the assessment of the initial search result\u2019s uncertainty and facilitates\nan optimal balance between accuracy and speed. This is particularly crucial in\nreal-world applications where such trade-offs are often necessary.\n  Our experimental results, conducted on the (P)ROxford and (P)RParis datasets,\ndemonstrate the significant superiority of our method over existing diffusion\ntechniques. We achieve state-of-the-art (SOTA) accuracy in both image-level and\npixel-level retrieval, while also maintaining impressive processing speed. This\ndual achievement underscores the effectiveness of our hypergraph-based\nframework and community selection technique, marking a notable advancement in\nthe field of content-based image retrieval.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Image-Retrieval", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [52.184627532958984, -8.955293655395508], "cluster": 9}, {"key": "ananthakrishnan2025can", "year": "2025", "citations": "0", "title": "Can Cross Encoders Produce Useful Sentence Embeddings?", "abstract": "<p>Cross encoders (CEs) are trained with sentence pairs to detect relatedness.\nAs CEs require sentence pairs at inference, the prevailing view is that they\ncan only be used as re-rankers in information retrieval pipelines. Dual\nencoders (DEs) are instead used to embed sentences, where sentence pairs are\nencoded by two separate encoders with shared weights at training, and a loss\nfunction that ensures the pair\u2019s embeddings lie close in vector space if the\nsentences are related. DEs however, require much larger datasets to train, and\nare less accurate than CEs. We report a curious finding that embeddings from\nearlier layers of CEs can in fact be used within an information retrieval\npipeline. We show how to exploit CEs to distill a lighter-weight DE, with a\n5.15x speedup in inference time.</p>\n", "tags": ["Efficiency", "Datasets"], "tsne_embedding": [-2.7937731742858887, -37.15370559692383], "cluster": 3}, {"key": "andoni2008near", "year": "2008", "citations": "1420", "title": "Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions", "abstract": "<p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>\n", "tags": ["Efficiency", "Hashing-Methods"], "tsne_embedding": [27.369638442993164, 43.09668731689453], "cluster": 4}, {"key": "andoni2015practical", "year": "2015", "citations": "239", "title": "Practical And Optimal LSH For Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular\ndistance that yields an approximate Near Neighbor Search algorithm with the\nasymptotically optimal running time exponent. Unlike earlier algorithms with this\nproperty (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe\nversion of this algorithm and conduct an experimental evaluation on real\nand synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the\nquality of any LSH family for angular distance. Our lower bound implies that the\nabove LSH family exhibits a trade-off between evaluation time and quality that is\nclose to optimal for a natural class of LSH functions.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [15.913895606994629, 41.79035186767578], "cluster": 4}, {"key": "andoni2016lower", "year": "2016", "citations": "4", "title": "Lower Bounds On Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>We show tight lower bounds for the entire trade-off between space and query\ntime for the Approximate Near Neighbor search problem. Our lower bounds hold in\na restricted model of computation, which captures all hashing-based approaches.\nIn articular, our lower bound matches the upper bound recently shown in\n[Laarhoven 2015] for the random instance on a Euclidean sphere (which we show\nin fact extends to the entire space \\(\\mathbb{R}^d\\) using the techniques from\n[Andoni, Razenshteyn 2015]).\n  We also show tight, unconditional cell-probe lower bounds for one and two\nprobes, improving upon the best known bounds from [Panigrahy, Talwar, Wieder\n2010]. In particular, this is the first space lower bound (for any static data\nstructure) for two probes which is not polynomially smaller than for one probe.\nTo show the result for two probes, we establish and exploit a connection to\nlocally-decodable codes.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [26.403505325317383, 43.148616790771484], "cluster": 4}, {"key": "andoni2016optimal", "year": "2017", "citations": "66", "title": "Optimal Hashing-based Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>[See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n\\(c\\)-Approximate Near Neighbor Search problem. For the \\(d\\)-dimensional Euclidean\nspace and \\(n\\)-point datasets, we develop a data structure with space \\(n^{1 +\n\\rho_u + o(1)} + O(dn)\\) and query time \\(n^{\\rho_q + o(1)} + d n^{o(1)}\\) for\nevery \\(\\rho_u, \\rho_q \\geq 0\\) such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor \\(c &gt; 1\\), improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for \\(\\rho_q = 0\\), improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Datasets"], "tsne_embedding": [22.751232147216797, 48.93223190307617], "cluster": 4}, {"key": "andoni2018approximate", "year": "2019", "citations": "49", "title": "Approximate Nearest Neighbor Search In High Dimensions", "abstract": "<p>The nearest neighbor problem is defined as follows: Given a set \\(P\\) of \\(n\\)\npoints in some metric space \\((X,D)\\), build a data structure that, given any\npoint \\(q\\), returns a point in \\(P\\) that is closest to \\(q\\) (its \u201cnearest\nneighbor\u201d in \\(P\\)). The data structure stores additional information about the\nset \\(P\\), which is then used to find the nearest neighbor without computing all\ndistances between \\(q\\) and \\(P\\). The problem has a wide range of applications in\nmachine learning, computer vision, databases and other fields.\n  To reduce the time needed to find nearest neighbors and the amount of memory\nused by the data structure, one can formulate the {\\em approximate} nearest\nneighbor problem, where the the goal is to return any point \\(p\u2019 \\in P\\) such\nthat the distance from \\(q\\) to \\(p\u2019\\) is at most \\(c \\cdot \\min_{p \\in P} D(q,p)\\),\nfor some \\(c \\geq 1\\). Over the last two decades, many efficient solutions to\nthis problem were developed. In this article we survey these developments, as\nwell as their connections to questions in geometric functional analysis and\ncombinatorial geometry.</p>\n", "tags": ["Survey-Paper"], "tsne_embedding": [22.65188217163086, 45.203590393066406], "cluster": 4}, {"key": "andoni2021average", "year": "2021", "citations": "0", "title": "From Average Embeddings To Nearest Neighbor Search", "abstract": "<p>In this note, we show that one can use average embeddings, introduced\nrecently in [Naor\u201920, arXiv:1905.01280], to obtain efficient algorithms for\napproximate nearest neighbor search. In particular, a metric \\(X\\) embeds into\n\\(\u2113\u2082\\) on average, with distortion \\(D\\), if, for any distribution \\(\\mu\\) on\n\\(X\\), the embedding is \\(D\\) Lipschitz and the (square of) distance does not\ndecrease on average (wrt \\(\\mu\\)). In particular existence of such an embedding\n(assuming it is efficient) implies a \\(O(D^3)\\) approximate nearest neighbor\nsearch under \\(X\\). This can be seen as a strengthening of the classic\n(bi-Lipschitz) embedding approach to nearest neighbor search, and is another\napplication of data-dependent hashing paradigm.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [13.4790620803833, 42.95515823364258], "cluster": 4}, {"key": "andoni2025near", "year": "2008", "citations": "1420", "title": "Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions", "abstract": "<p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>\n", "tags": ["Efficiency", "Hashing-Methods"], "tsne_embedding": [27.36963653564453, 43.0968017578125], "cluster": 4}, {"key": "andoni2025practical", "year": "2015", "citations": "239", "title": "Practical And Optimal LSH For Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular\ndistance that yields an approximate Near Neighbor Search algorithm with the\nasymptotically optimal running time exponent. Unlike earlier algorithms with this\nproperty (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe\nversion of this algorithm and conduct an experimental evaluation on real\nand synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the\nquality of any LSH family for angular distance. Our lower bound implies that the\nabove LSH family exhibits a trade-off between evaluation time and quality that is\nclose to optimal for a natural class of LSH functions.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [15.91384506225586, 41.790428161621094], "cluster": 4}, {"key": "andr\u00e92017accelerated", "year": "2017", "citations": "14", "title": "Accelerated Nearest Neighbor Search With Quick ADC", "abstract": "<p>Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).</p>\n", "tags": ["Evaluation", "Compact-Codes", "Quantization", "Efficiency", "Multimodal-Retrieval"], "tsne_embedding": [34.903377532958984, 19.942808151245117], "cluster": 2}, {"key": "andr\u00e92017exploiting", "year": "2017", "citations": "0", "title": "Exploiting Modern Hardware For High-dimensional Nearest Neighbor Search", "abstract": "<p>Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.</p>\n", "tags": ["Compact-Codes", "Quantization"], "tsne_embedding": [35.01605224609375, 18.754934310913086], "cluster": 2}, {"key": "andr\u00e92019derived", "year": "2019", "citations": "1", "title": "Derived Codebooks For High-accuracy Nearest Neighbor Search", "abstract": "<p>High-dimensional Nearest Neighbor (NN) search is central in multimedia search\nsystems. Product Quantization (PQ) is a widespread NN search technique which\nhas a high performance and good scalability. PQ compresses high-dimensional\nvectors into compact codes thanks to a combination of quantizers. Large\ndatabases can, therefore, be stored entirely in RAM, enabling fast responses to\nNN queries. In almost all cases, PQ uses 8-bit quantizers as they offer low\nresponse times. In this paper, we advocate the use of 16-bit quantizers.\nCompared to 8-bit quantizers, 16-bit quantizers boost accuracy but they\nincrease response time by a factor of 3 to 10. We propose a novel approach that\nallows 16-bit quantizers to offer the same response time as 8-bit quantizers,\nwhile still providing a boost of accuracy. Our approach builds on two key\nideas: (i) the construction of derived codebooks that allow a fast and\napproximate distance evaluation, and (ii) a two-pass NN search procedure which\nbuilds a candidate set using the derived codebooks, and then refines it using\n16-bit quantizers. On 1 billion SIFT vectors, with an inverted index, our\napproach offers a Recall@100 of 0.85 in 5.2 ms. By contrast, 16-bit quantizers\nalone offer a Recall@100 of 0.85 in 39 ms, and 8-bit quantizers a Recall@100 of\n0.82 in 3.8 ms.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Scalability", "Compact-Codes", "Evaluation"], "tsne_embedding": [34.538902282714844, 20.515470504760742], "cluster": 2}, {"key": "appalaraju2017image", "year": "2017", "citations": "58", "title": "Image Similarity Using Deep CNN And Curriculum Learning", "abstract": "<p>Image similarity involves fetching similar looking images given a reference\nimage. Our solution called SimNet, is a deep siamese network which is trained\non pairs of positive and negative images using a novel online pair mining\nstrategy inspired by Curriculum learning. We also created a multi-scale CNN,\nwhere the final image embedding is a joint representation of top as well as\nlower layer embedding\u2019s. We go on to show that this multi-scale siamese network\nis better at capturing fine grained image similarities than traditional CNN\u2019s.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-12.850279808044434, -26.880102157592773], "cluster": 3}, {"key": "argerich2017generic", "year": "2017", "citations": "1", "title": "Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH", "abstract": "<p>In this paper we propose the creation of generic LSH families for the angular\ndistance based on Johnson-Lindenstrauss projections. We show that feature\nhashing is a valid J-L projection and propose two new LSH families based on\nfeature hashing. These new LSH families are tested on both synthetic and real\ndatasets with very good results and a considerable performance improvement over\nother LSH families. While the theoretical analysis is done for the angular\ndistance, these families can also be used in practice for the euclidean\ndistance with excellent results [2]. Our tests using real datasets show that\nthe proposed LSH functions work well for the euclidean distance.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Datasets", "Evaluation"], "tsne_embedding": [14.797571182250977, 41.47431564331055], "cluster": 4}, {"key": "arora2023linking", "year": "2023", "citations": "1", "title": "Linking Representations With Multimodal Contrastive Learning", "abstract": "<p>Many applications require linking individuals, firms, or locations across\ndatasets. Most widely used methods, especially in social science, do not employ\ndeep learning, with record linkage commonly approached using string matching\ntechniques. Moreover, existing methods do not exploit the inherently multimodal\nnature of documents. In historical record linkage applications, documents are\ntypically noisily transcribed by optical character recognition (OCR). Linkage\nwith just OCR\u2019ed texts may fail due to noise, whereas linkage with just image\ncrops may also fail because vision models lack language understanding (e.g., of\nabbreviations or other different ways of writing firm names). To leverage\nmultimodal learning, this study develops CLIPPINGS (Contrastively LInking\nPooled Pre-trained Embeddings). CLIPPINGS aligns symmetric vision and language\nbi-encoders, through contrastive language-image pre-training on document images\nand their corresponding OCR\u2019ed texts. It then contrastively learns a metric\nspace where the pooled image-text embedding for a given instance is close to\nembeddings in the same class (e.g., the same firm or location) and distant from\nembeddings of a different class. Data are linked by treating linkage as a\nnearest neighbor retrieval problem with the multimodal embeddings. CLIPPINGS\noutperforms widely used string matching methods by a wide margin in linking\nmid-20th century Japanese firms across financial documents. A purely\nself-supervised model - trained only by aligning the embeddings for the image\ncrop of a firm name and its corresponding OCR\u2019ed text - also outperforms\npopular string matching methods. Fascinatingly, a multimodally pre-trained\nvision-only encoder outperforms a unimodally pre-trained vision-only encoder,\nillustrating the power of multimodal pre-training even if only one modality is\navailable for linking at inference time.</p>\n", "tags": ["Supervised", "Self-Supervised", "Datasets"], "tsne_embedding": [-19.87571907043457, -1.069757103919983], "cluster": 1}, {"key": "aroraa2024smart", "year": "2024", "citations": "0", "title": "Smart Multi-modal Search: Contextual Sparse And Dense Embedding Integration In Adobe Express", "abstract": "<p>As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-33.706581115722656, -32.87070083618164], "cluster": 5}, {"key": "arponen2019shrewd", "year": "2019", "citations": "2", "title": "SHREWD: Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing", "abstract": "<p>Using class labels to represent class similarity is a typical approach to\ntraining deep hashing systems for retrieval; samples from the same or different\nclasses take binary 1 or 0 similarity values. This similarity does not model\nthe full rich knowledge of semantic relations that may be present between data\npoints. In this work we build upon the idea of using semantic hierarchies to\nform distance metrics between all available sample labels; for example cat to\ndog has a smaller distance than cat to guitar. We combine this type of semantic\ndistance into a loss function to promote similar distances between the deep\nneural network embeddings. We also introduce an empirical Kullback-Leibler\ndivergence loss term to promote binarization and uniformity of the embeddings.\nWe test the resulting SHREWD method and demonstrate improvements in\nhierarchical retrieval scores using compact, binary hash codes instead of real\nvalued ones, and show that in a weakly supervised hashing setting we are able\nto learn competitively without explicitly relying on class labels, but instead\non similarities between labels.</p>\n", "tags": ["Supervised", "Hashing-Methods", "Neural-Hashing", "Distance-Metric-Learning"], "tsne_embedding": [3.012488603591919, -2.9663844108581543], "cluster": 6}, {"key": "arponen2020learning", "year": "2020", "citations": "0", "title": "Learning To Hash With Semantic Similarity Metrics And Empirical KL Divergence", "abstract": "<p>Learning to hash is an efficient paradigm for exact and approximate nearest\nneighbor search from massive databases. Binary hash codes are typically\nextracted from an image by rounding output features from a CNN, which is\ntrained on a supervised binary similar/ dissimilar task. Drawbacks of this\napproach are: (i) resulting codes do not necessarily capture semantic\nsimilarity of the input data (ii) rounding results in information loss,\nmanifesting as decreased retrieval performance and (iii) Using only class-wise\nsimilarity as a target can lead to trivial solutions, simply encoding\nclassifier outputs rather than learning more intricate relations, which is not\ndetected by most performance metrics. We overcome (i) via a novel loss function\nencouraging the relative hash code distances of learned features to match those\nderived from their targets. We address (ii) via a differentiable estimate of\nthe KL divergence between network outputs and a binary target distribution,\nresulting in minimal information loss when the features are rounded to binary.\nFinally, we resolve (iii) by focusing on a hierarchical precision metric.\nEfficiency of the methods is demonstrated with semantic image retrieval on the\nCIFAR-100, ImageNet and Conceptual Captions datasets, using similarities\ninferred from the WordNet label hierarchy or sentence embeddings.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-5.0450592041015625, 18.090816497802734], "cluster": 8}, {"key": "artetxe2018margin", "year": "2019", "citations": "163", "title": "Margin-based Parallel Corpus Mining With Multilingual Sentence Embeddings", "abstract": "<p>Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [9.863475799560547, -15.9730863571167], "cluster": 7}, {"key": "artetxe2018massively", "year": "2019", "citations": "747", "title": "Massively Multilingual Sentence Embeddings For Zero-shot Cross-lingual Transfer And Beyond", "abstract": "<p>We introduce an architecture to learn joint multilingual sentence\nrepresentations for 93 languages, belonging to more than 30 different families\nand written in 28 different scripts. Our system uses a single BiLSTM encoder\nwith a shared BPE vocabulary for all languages, which is coupled with an\nauxiliary decoder and trained on publicly available parallel corpora. This\nenables us to learn a classifier on top of the resulting embeddings using\nEnglish annotated data only, and transfer it to any of the 93 languages without\nany modification. Our experiments in cross-lingual natural language inference\n(XNLI dataset), cross-lingual document classification (MLDoc dataset) and\nparallel corpus mining (BUCC dataset) show the effectiveness of our approach.\nWe also introduce a new test set of aligned sentences in 112 languages, and\nshow that our sentence embeddings obtain strong results in multilingual\nsimilarity search even for low-resource languages. Our implementation, the\npre-trained encoder and the multilingual test set are available at\nhttps://github.com/facebookresearch/LASER</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Similarity-Search", "Datasets"], "tsne_embedding": [-5.857154846191406, -38.219078063964844], "cluster": 3}, {"key": "artetxe2019bilingual", "year": "2019", "citations": "53", "title": "Bilingual Lexicon Induction Through Unsupervised Machine Translation", "abstract": "<p>A recent research line has obtained strong results on bilingual lexicon\ninduction by aligning independently trained word embeddings in two languages\nand using the resulting cross-lingual embeddings to induce word translation\npairs through nearest neighbor or related retrieval methods. In this paper, we\npropose an alternative approach to this problem that builds on the recent work\non unsupervised machine translation. This way, instead of directly inducing a\nbilingual lexicon from cross-lingual embeddings, we use them to build a\nphrase-table, combine it with a language model, and use the resulting machine\ntranslation system to generate a synthetic parallel corpus, from which we\nextract the bilingual lexicon using statistical word alignment techniques. As\nsuch, our method can work with any word embedding and cross-lingual mapping\ntechnique, and it does not require any additional resource besides the\nmonolingual corpus used to train the embeddings. When evaluated on the exact\nsame cross-lingual embeddings, our proposed method obtains an average\nimprovement of 6 accuracy points over nearest neighbor and 4 points over CSLS\nretrieval, establishing a new state-of-the-art in the standard MUSE dataset.</p>\n", "tags": ["Unsupervised", "Datasets"], "tsne_embedding": [-5.2367658615112305, -38.58451843261719], "cluster": 3}, {"key": "artetxe2019margin", "year": "2019", "citations": "163", "title": "Margin-based Parallel Corpus Mining With Multilingual Sentence Embeddings", "abstract": "<p>Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [9.863515853881836, -15.973024368286133], "cluster": 7}, {"key": "aum\u00fcller2017distance", "year": "2018", "citations": "18", "title": "Distance-sensitive Hashing", "abstract": "<p>Locality-sensitive hashing (LSH) is an important tool for managing\nhigh-dimensional noisy or uncertain data, for example in connection with data\ncleaning (similarity join) and noise-robust search (similarity search).\nHowever, for a number of problems the LSH framework is not known to yield good\nsolutions, and instead ad hoc solutions have been designed for particular\nsimilarity and distance measures. For example, this is true for\noutput-sensitive similarity search/join, and for indexes supporting annulus\nqueries that aim to report a point close to a certain given distance from the\nquery point.\n  In this paper we initiate the study of distance-sensitive hashing (DSH), a\ngeneralization of LSH that seeks a family of hash functions such that the\nprobability of two points having the same hash value is a given function of the\ndistance between them. More precisely, given a distance space \\((X,\n\\text{dist})\\) and a \u201ccollision probability function\u201d (CPF) \\(f\\colon\n\\mathbb{R}\\rightarrow [0,1]\\) we seek a distribution over pairs of functions\n\\((h,g)\\) such that for every pair of points \\(x, y \\in X\\) the collision\nprobability is \\(\\Pr[h(x)=g(y)] = f(\\text{dist}(x,y))\\). Locality-sensitive\nhashing is the study of how fast a CPF can decrease as the distance grows. For\nmany spaces, \\(f\\) can be made exponentially decreasing even if we restrict\nattention to the symmetric case where \\(g=h\\). We show that the asymmetry\nachieved by having a pair of functions makes it possible to achieve CPFs that\nare, for example, increasing or unimodal, and show how this leads to principled\nsolutions to problems not addressed by the LSH framework. This includes a novel\napplication to privacy-preserving distance estimation. We believe that the DSH\nframework will find further applications in high-dimensional data management.</p>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search"], "tsne_embedding": [19.866010665893555, 50.32911682128906], "cluster": 4}, {"key": "aum\u00fcller2018ann", "year": "2019", "citations": "194", "title": "Ann-benchmarks: A Benchmarking Tool For Approximate Nearest Neighbor Algorithms", "abstract": "<p>This paper describes ANN-Benchmarks, a tool for evaluating the performance of\nin-memory approximate nearest neighbor algorithms. It provides a standard\ninterface for measuring the performance and quality achieved by nearest\nneighbor algorithms on different standard data sets. It supports several\ndifferent ways of integrating \\(k\\)-NN algorithms, and its configuration system\nautomatically tests a range of parameter settings for each algorithm.\nAlgorithms are compared with respect to many different (approximate) quality\nmeasures, and adding more is easy and fast; the included plotting front-ends\ncan visualise these as images, \\(\\LaTeX\\) plots, and websites with interactive\nplots. ANN-Benchmarks aims to provide a constantly updated overview of the\ncurrent state of the art of \\(k\\)-NN algorithms. In the short term, this overview\nallows users to choose the correct \\(k\\)-NN algorithm and parameters for their\nsimilarity search task; in the longer term, algorithm designers will be able to\nuse this overview to test and refine automatic parameter tuning. The paper\ngives an overview of the system, evaluates the results of the benchmark, and\npoints out directions for future work. Interestingly, very different approaches\nto \\(k\\)-NN search yield comparable quality-performance trade-offs. The system is\navailable at http://ann-benchmarks.com .</p>\n", "tags": ["Survey-Paper", "Evaluation", "Similarity-Search"], "tsne_embedding": [51.05579376220703, 24.038490295410156], "cluster": 2}, {"key": "aum\u00fcller2019fair", "year": "2020", "citations": "16", "title": "Fair Near Neighbor Search: Independent Range Sampling In High Dimensions", "abstract": "<p>Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. There are several variants of the similarity\nsearch problem, and one of the most relevant is the \\(r\\)-near neighbor (\\(r\\)-NN)\nproblem: given a radius \\(r&gt;0\\) and a set of points \\(S\\), construct a data\nstructure that, for any given query point \\(q\\), returns a point \\(p\\) within\ndistance at most \\(r\\) from \\(q\\). In this paper, we study the \\(r\\)-NN problem in\nthe light of fairness. We consider fairness in the sense of equal opportunity:\nall points that are within distance \\(r\\) from the query should have the same\nprobability to be returned. In the low-dimensional case, this problem was first\nstudied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the\ntheoretically strongest approach to similarity search in high dimensions, does\nnot provide such a fairness guarantee. To address this, we propose efficient\ndata structures for \\(r\\)-NN where all points in \\(S\\) that are near \\(q\\) have the\nsame probability to be selected and returned by the query. Specifically, we\nfirst propose a black-box approach that, given any LSH scheme, constructs a\ndata structure for uniformly sampling points in the neighborhood of a query.\nThen, we develop a data structure for fair similarity search under inner\nproduct that requires nearly-linear space and exploits locality sensitive\nfilters. The paper concludes with an experimental evaluation that highlights\n(un)fairness in a recommendation setting on real-world datasets and discusses\nthe inherent unfairness introduced by solving other variants of the problem.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [27.925872802734375, 49.0728874206543], "cluster": 4}, {"key": "aum\u00fcller2019puffinn", "year": "2019", "citations": "6", "title": "PUFFINN: Parameterless And Universally Fast Finding Of Nearest Neighbors", "abstract": "<p>We present PUFFINN, a parameterless LSH-based index for solving the\n\\(k\\)-nearest neighbor problem with probabilistic guarantees. By parameterless we\nmean that the user is only required to specify the amount of memory the index\nis supposed to use and the result quality that should be achieved. The index\ncombines several heuristic ideas known in the literature. By small adaptions to\nthe query algorithm, we make heuristics rigorous. We perform experiments on\nreal-world and synthetic inputs to evaluate implementation choices and show\nthat the implementation satisfies the quality guarantees while being\ncompetitive with other state-of-the-art approaches to nearest neighbor search.\n  We describe a novel synthetic data set that is difficult to solve for almost\nall existing nearest neighbor search approaches, and for which PUFFINN\nsignificantly outperform previous methods.</p>\n", "tags": ["Locality-Sensitive-Hashing"], "tsne_embedding": [32.24900817871094, 25.798385620117188], "cluster": 2}, {"key": "aum\u00fcller2020differentially", "year": "2020", "citations": "5", "title": "Differentially Private Sketches For Jaccard Similarity Estimation", "abstract": "<p>This paper describes two locally-differential private algorithms for\nreleasing user vectors such that the Jaccard similarity between these vectors\ncan be efficiently estimated. The basic building block is the well known\nMinHash method. To achieve a privacy-utility trade-off, MinHash is extended in\ntwo ways using variants of Generalized Randomized Response and the Laplace\nMechanism. A theoretical analysis provides bounds on the absolute error and\nexperiments show the utility-privacy trade-off on synthetic and real-world\ndata. The paper ends with a critical discussion of related work.</p>\n", "tags": ["Locality-Sensitive-Hashing"], "tsne_embedding": [-17.402902603149414, 27.88528823852539], "cluster": 8}, {"key": "aum\u00fcller2021sampling", "year": "2022", "citations": "4", "title": "Sampling A Near Neighbor In High Dimensions -- Who Is The Fairest Of Them All?", "abstract": "<p>Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. Given a set of points \\(S\\) and a radius parameter\n\\(r&gt;0\\), the \\(r\\)-near neighbor (\\(r\\)-NN) problem asks for a data structure that,\ngiven any query point \\(q\\), returns a point \\(p\\) within distance at most \\(r\\) from\n\\(q\\). In this paper, we study the \\(r\\)-NN problem in the light of individual\nfairness and providing equal opportunities: all points that are within distance\n\\(r\\) from the query should have the same probability to be returned. In the\nlow-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS\n2014). Locality sensitive hashing (LSH), the theoretically strongest approach\nto similarity search in high dimensions, does not provide such a fairness\nguarantee. In this work, we show that LSH based algorithms can be made fair,\nwithout a significant loss in efficiency. We propose several efficient data\nstructures for the exact and approximate variants of the fair NN problem. Our\napproach works more generally for sampling uniformly from a sub-collection of\nsets of a given collection and can be used in a few other applications. We also\ndevelop a data structure for fair similarity search under inner product that\nrequires nearly-linear space and exploits locality sensitive filters. The paper\nconcludes with an experimental evaluation that highlights the inherent\nunfairness of NN data structures and shows the performance of our algorithms on\nreal-world datasets.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [27.768760681152344, 48.9765739440918], "cluster": 4}, {"key": "avgoustinakis2020audio", "year": "2021", "citations": "8", "title": "Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning", "abstract": "<p>In this work, we address the problem of audio-based near-duplicate video\nretrieval. We propose the Audio Similarity Learning (AuSiL) approach that\neffectively captures temporal patterns of audio similarity between video pairs.\nFor the robust similarity calculation between two videos, we first extract\nrepresentative audio-based video descriptors by leveraging transfer learning\nbased on a Convolutional Neural Network (CNN) trained on a large scale dataset\nof audio events, and then we calculate the similarity matrix derived from the\npairwise similarity of these descriptors. The similarity matrix is subsequently\nfed to a CNN network that captures the temporal structures existing within its\ncontent. We train our network following a triplet generation process and\noptimizing the triplet loss function. To evaluate the effectiveness of the\nproposed approach, we have manually annotated two publicly available video\ndatasets based on the audio duplicity between their videos. The proposed\napproach achieves very competitive results compared to three state-of-the-art\nmethods. Also, unlike the competing methods, it is very robust to the retrieval\nof audio duplicates generated with speed transformations.</p>\n", "tags": ["Distance-Metric-Learning", "Video-Retrieval", "Datasets"], "tsne_embedding": [-49.100624084472656, -6.631173133850098], "cluster": 0}, {"key": "badamdorj2019fast", "year": "2019", "citations": "1", "title": "Fast Search With Poor OCR", "abstract": "<p>The indexing and searching of historical documents have garnered attention in\nrecent years due to massive digitization efforts of important collections\nworldwide. Pure textual search in these corpora is a problem since optical\ncharacter recognition (OCR) is infamous for performing poorly on such\nhistorical material, which often suffer from poor preservation. We propose a\nnovel text-based method for searching through noisy text. Our system represents\nwords as vectors, projects queries and candidates obtained from the OCR into a\ncommon space, and ranks the candidates using a metric suited to\nnearest-neighbor search. We demonstrate the practicality of our method on\ntypewritten German documents from the WWII era.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [9.789202690124512, -16.698986053466797], "cluster": 7}, {"key": "baek2023direct", "year": "2023", "citations": "4", "title": "Direct Fact Retrieval From Knowledge Graphs Without Entity Linking", "abstract": "<p>There has been a surge of interest in utilizing Knowledge Graphs (KGs) for\nvarious natural language processing/understanding tasks. The conventional\nmechanism to retrieve facts in KGs usually involves three steps: entity span\ndetection, entity disambiguation, and relation classification. However, this\napproach requires additional labels for training each of the three\nsubcomponents in addition to pairs of input texts and facts, and also may\naccumulate errors propagated from failures in previous steps. To tackle these\nlimitations, we propose a simple knowledge retrieval framework, which directly\nretrieves facts from the KGs given the input text based on their\nrepresentational similarities, which we refer to as Direct Fact Retrieval\n(DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding\nspace by using a language model trained by only pairs of input texts and facts,\nand then provide the nearest facts in response to the input text. Since the\nfact, consisting of only two entities and one relation, has little context to\nencode, we propose to further refine ranks of top-k retrieved facts with a\nreranker that contextualizes the input text and the fact jointly. We validate\nour DiFaR framework on multiple fact retrieval tasks, showing that it\nsignificantly outperforms relevant baselines that use the three-step approach.</p>\n", "tags": ["Tools-&-Libraries"], "tsne_embedding": [-7.709319114685059, -14.451745986938477], "cluster": 1}, {"key": "bai2017deep", "year": "2019", "citations": "220", "title": "Deep-person: Learning Discriminative Deep Features For Person Re-identification", "abstract": "<p>Recently, many methods of person re-identification (Re-ID) rely on part-based\nfeature representation to learn a discriminative pedestrian descriptor.\nHowever, the spatial context between these parts is ignored for the independent\nextractor to each separate part. In this paper, we propose to apply Long\nShort-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as\na sequence of body parts from head to foot. Integrating the contextual\ninformation strengthens the discriminative ability of local representation. We\nalso leverage the complementary information between local and global feature.\nFurthermore, we integrate both identification task and ranking task in one\nnetwork, where a discriminative embedding and a similarity measurement are\nlearned concurrently. This results in a novel three-branch framework named\nDeep-Person, which learns highly discriminative features for person Re-ID.\nExperimental results demonstrate that Deep-Person outperforms the\nstate-of-the-art methods by a large margin on three challenging datasets\nincluding Market-1501, CUHK03, and DukeMTMC-reID. Specifically, combining with\na re-ranking approach, we achieve a 90.84% mAP on Market-1501 under single\nquery setting.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Tools-&-Libraries", "Hybrid-Ann-Methods", "Re-Ranking"], "tsne_embedding": [-31.404090881347656, -43.678192138671875], "cluster": 5}, {"key": "bai2017incorporating", "year": "2017", "citations": "61", "title": "Incorporating Intra-class Variance To Fine-grained Visual Recognition", "abstract": "<p>Fine-grained visual recognition aims to capture discriminative\ncharacteristics amongst visually similar categories. The state-of-the-art\nresearch work has significantly improved the fine-grained recognition\nperformance by deep metric learning using triplet network. However, the impact\nof intra-category variance on the performance of recognition and robust feature\nrepresentation has not been well studied. In this paper, we propose to leverage\nintra-class variance in metric learning of triplet network to improve the\nperformance of fine-grained recognition. Through partitioning training images\nwithin each category into a few groups, we form the triplet samples across\ndifferent categories as well as different groups, which is called Group\nSensitive TRiplet Sampling (GS-TRS). Accordingly, the triplet loss function is\nstrengthened by incorporating intra-class variance with GS-TRS, which may\ncontribute to the optimization objective of triplet network. Extensive\nexperiments over benchmark datasets CompCar and VehicleID show that the\nproposed GS-TRS has significantly outperformed state-of-the-art approaches in\nboth classification and retrieval tasks.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-10.055670738220215, 8.014376640319824], "cluster": 8}, {"key": "bai2018learning", "year": "2020", "citations": "64", "title": "Learning-based Efficient Graph Similarity Computation Via Multi-scale Convolutional Set Matching", "abstract": "<p>Graph similarity computation is one of the core operations in many\ngraph-based applications, such as graph similarity search, graph database\nanalysis, graph clustering, etc. Since computing the exact distance/similarity\nbetween two graphs is typically NP-hard, a series of approximate methods have\nbeen proposed with a trade-off between accuracy and speed. Recently, several\ndata-driven approaches based on neural networks have been proposed, most of\nwhich model the graph-graph similarity as the inner product of their\ngraph-level representations, with different techniques proposed for generating\none embedding per graph. However, using one fixed-dimensional embedding per\ngraph may fail to fully capture graphs in varying sizes and link structures, a\nlimitation that is especially problematic for the task of graph similarity\ncomputation, where the goal is to find the fine-grained difference between two\ngraphs. In this paper, we address the problem of graph similarity computation\nfrom another perspective, by directly matching two sets of node embeddings\nwithout the need to use fixed-dimensional vectors to represent whole graphs for\ntheir similarity computation. The model, GraphSim, achieves the\nstate-of-the-art performance on four real-world graph datasets under six out of\neight settings (here we count a specific dataset and metric combination as one\nsetting), compared to existing popular methods for approximate Graph Edit\nDistance (GED) and Maximum Common Subgraph (MCS) computation.</p>\n", "tags": ["Graph-Based-Ann", "Similarity-Search", "AAAI", "Datasets", "Evaluation"], "tsne_embedding": [56.2890739440918, 4.0006794929504395], "cluster": 9}, {"key": "bai2020targeted", "year": "2020", "citations": "68", "title": "Targeted Attack For Deep Hashing Based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the \u2113\u221e restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Robustness", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-16.88530921936035, 33.374698638916016], "cluster": 8}, {"key": "bai2025targeted", "year": "2020", "citations": "68", "title": "Targeted Attack For Deep Hashing Based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the \u2113\u221e restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Robustness", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-16.88521385192871, 33.37471389770508], "cluster": 8}, {"key": "bain2020condensed", "year": "2021", "citations": "68", "title": "Condensed Movies: Story Based Retrieval With Contextual Embeddings", "abstract": "<p>Our objective in this work is long range understanding of the narrative\nstructure of movies. Instead of considering the entire movie, we propose to\nlearn from the `key scenes\u2019 of the movie, providing a condensed look at the\nfull storyline. To this end, we make the following three contributions: (i) We\ncreate the Condensed Movies Dataset (CMD) consisting of the key scenes from\nover 3K movies: each key scene is accompanied by a high level semantic\ndescription of the scene, character face-tracks, and metadata about the movie.\nThe dataset is scalable, obtained automatically from YouTube, and is freely\navailable for anybody to download and use. It is also an order of magnitude\nlarger than existing movie datasets in the number of movies; (ii) We provide a\ndeep network baseline for text-to-video retrieval on our dataset, combining\ncharacter, speech and visual cues into a single video embedding; and finally\n(iii) We demonstrate how the addition of context from other video clips\nimproves retrieval performance.</p>\n", "tags": ["Evaluation", "Video-Retrieval", "Datasets"], "tsne_embedding": [-37.66089630126953, -33.71455764770508], "cluster": 5}, {"key": "bajaj2019relemb", "year": "2019", "citations": "5", "title": "Relemb: A Relevance-based Application Embedding For Mobile App Retrieval And Categorization", "abstract": "<p>Information Retrieval Systems have revolutionized the organization and\nextraction of Information. In recent years, mobile applications (apps) have\nbecome primary tools of collecting and disseminating information. However,\nlimited research is available on how to retrieve and organize mobile apps on\nusers\u2019 devices. In this paper, authors propose a novel method to estimate\napp-embeddings which are then applied to tasks like app clustering,\nclassification, and retrieval. Usage of app-embedding for query expansion,\nnearest neighbor analysis enables unique and interesting use cases to enhance\nend-user experience with mobile apps.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [29.253429412841797, -27.888330459594727], "cluster": 7}, {"key": "baldrati2023composed", "year": "2023", "citations": "13", "title": "Composed Image Retrieval Using Contrastive Learning And Task-oriented Clip-based Features", "abstract": "<p>Given a query composed of a reference image and a relative caption, the\nComposed Image Retrieval goal is to retrieve images visually similar to the\nreference one that integrates the modifications expressed by the caption. Given\nthat recent research has demonstrated the efficacy of large-scale vision and\nlanguage pre-trained (VLP) models in various tasks, we rely on features from\nthe OpenAI CLIP model to tackle the considered task. We initially perform a\ntask-oriented fine-tuning of both CLIP encoders using the element-wise sum of\nvisual and textual features. Then, in the second stage, we train a Combiner\nnetwork that learns to combine the image-text features integrating the bimodal\ninformation and providing combined features used to perform the retrieval. We\nuse contrastive learning in both stages of training. Starting from the bare\nCLIP features as a baseline, experimental results show that the task-oriented\nfine-tuning and the carefully crafted Combiner network are highly effective and\noutperform more complex state-of-the-art approaches on FashionIQ and CIRR, two\npopular and challenging datasets for composed image retrieval. Code and\npre-trained models are available at https://github.com/ABaldrati/CLIP4Cir</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [-18.624677658081055, -28.42365837097168], "cluster": 5}, {"key": "bansal2016extraction", "year": "2016", "citations": "0", "title": "Extraction Of Layout Entities And Sub-layout Query-based Retrieval Of Document Images", "abstract": "<p>Layouts and sub-layouts constitute an important clue while searching a\ndocument on the basis of its structure, or when textual content is\nunknown/irrelevant. A sub-layout specifies the arrangement of document entities\nwithin a smaller portion of the document. We propose an efficient graph-based\nmatching algorithm, integrated with hash-based indexing, to prune a possibly\nlarge search space. A user can specify a combination of sub-layouts of interest\nusing sketch-based queries. The system supports partial matching for\nunspecified layout entities. We handle cases of segmentation pre-processing\nerrors (for text/non-text blocks) with a symmetry maximization-based strategy,\nand accounting for multiple domain-specific plausible segmentation hypotheses.\nWe show promising results of our system on a database of unstructured entities,\ncontaining 4776 newspaper images.</p>\n", "tags": ["Graph-Based-Ann"], "tsne_embedding": [40.0388298034668, -19.61980628967285], "cluster": 7}, {"key": "bao2022mmfl", "year": "2022", "citations": "5", "title": "Mmfl-net: Multi-scale And Multi-granularity Feature Learning For Cross-domain Fashion Retrieval", "abstract": "<p>Instance-level image retrieval in fashion is a challenging issue owing to its\nincreasing importance in real-scenario visual fashion search. Cross-domain\nfashion retrieval aims to match the unconstrained customer images as queries\nfor photographs provided by retailers; however, it is a difficult task due to a\nwide range of consumer-to-shop (C2S) domain discrepancies and also considering\nthat clothing image is vulnerable to various non-rigid deformations. To this\nend, we propose a novel multi-scale and multi-granularity feature learning\nnetwork (MMFL-Net), which can jointly learn global-local aggregation feature\nrepresentations of clothing images in a unified framework, aiming to train a\ncross-domain model for C2S fashion visual similarity. First, a new\nsemantic-spatial feature fusion part is designed to bridge the semantic-spatial\ngap by applying top-down and bottom-up bidirectional multi-scale feature\nfusion. Next, a multi-branch deep network architecture is introduced to capture\nglobal salient, part-informed, and local detailed information, and extracting\nrobust and discrimination feature embedding by integrating the similarity\nlearning of coarse-to-fine embedding with the multiple granularities. Finally,\nthe improved trihard loss, center loss, and multi-task classification loss are\nadopted for our MMFL-Net, which can jointly optimize intra-class and\ninter-class distance and thus explicitly improve intra-class compactness and\ninter-class discriminability between its visual representations for feature\nlearning. Furthermore, our proposed model also combines the multi-task\nattribute recognition and classification module with multi-label semantic\nattributes and product ID labels. Experimental results demonstrate that our\nproposed MMFL-Net achieves significant improvement over the state-of-the-art\nmethods on the two datasets, DeepFashion-C2S and Street2Shop.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-18.023677825927734, -49.658790588378906], "cluster": 3}, {"key": "bao2024average", "year": "2024", "citations": "0", "title": "Average-distortion Sketching", "abstract": "<p>We introduce average-distortion sketching for metric spaces. As in\n(worst-case) sketching, these algorithms compress points in a metric space\nwhile approximately recovering pairwise distances. The novelty is studying\naverage-distortion: for any fixed (yet, arbitrary) distribution \\(\\mu\\) over the\nmetric, the sketch should not over-estimate distances, and it should\n(approximately) preserve the average distance with respect to draws from \\(\\mu\\).\nThe notion generalizes average-distortion embeddings into \\(\\ell_1\\) [Rabinovich\n\u201803, Kush-Nikolov-Tang \u201821] as well as data-dependent locality-sensitive\nhashing [Andoni-Razenshteyn \u201815, Andoni-Naor-Nikolov-et-al. \u201818], which have\nbeen recently studied in the context of nearest neighbor search.\n  \\(\\bullet\\) For all \\(p \\in (2, \\infty)\\) and any \\(c\\) larger than a fixed\nconstant, we give an average-distortion sketch for \\(([\\Delta]^d, \\ell_p)\\) with\napproximation \\(c\\) and bit-complexity \\(\\text{poly}(2^{p/c} \\cdot\nlog(d\\Delta))\\), which is provably impossible in (worst-case) sketching.\n  \\(\\bullet\\) As an application, we improve on the approximation of\nsublinear-time data structures for nearest neighbor search over \\(\\ell_p\\) (for\nlarge \\(p &gt; 2\\)). The prior best approximation was \\(O(p)\\)\n[Andoni-Naor-Nikolov-et-al. \u201818, Kush-Nikolov-Tang \u201821], and we show it can be\nany \\(c\\) larger than a fixed constant (irrespective of \\(p\\)) by using\n\\(n^{O(p/c)}\\) space.\n  We give some evidence that \\(2^{\u03a9(p/c)}\\) space may be necessary by giving\na lower bound on average-distortion sketches which produce a certain\nprobabilistic certificate of farness (which our sketches crucially rely on).</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [21.282243728637695, 51.588050842285156], "cluster": 4}, {"key": "baranchuk2018revisiting", "year": "2018", "citations": "67", "title": "Revisiting The Inverted Indices For Billion-scale Approximate Nearest Neighbors", "abstract": "<p>This work addresses the problem of billion-scale nearest neighbor search. The\nstate-of-the-art retrieval systems for billion-scale databases are currently\nbased on the inverted multi-index, the recently proposed generalization of the\ninverted index structure. The multi-index provides a very fine-grained\npartition of the feature space that allows extracting concise and accurate\nshort-lists of candidates for the search queries. In this paper, we argue that\nthe potential of the simple inverted index was not fully exploited in previous\nworks and advocate its usage both for the highly-entangled deep descriptors and\nrelatively disentangled SIFT descriptors. We introduce a new retrieval system\nthat is based on the inverted index and outperforms the multi-index by a large\nmargin for the same memory consumption and construction complexity. For\nexample, our system achieves the state-of-the-art recall rates several times\nfaster on the dataset of one billion deep descriptors compared to the efficient\nimplementation of the inverted multi-index from the FAISS library.</p>\n", "tags": ["Vector-Indexing", "Scalability", "Tools-&-Libraries", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [1.0691055059432983, 32.278377532958984], "cluster": 4}, {"key": "baranchuk2019learning", "year": "2019", "citations": "8", "title": "Learning To Route In Similarity Graphs", "abstract": "<p>Recently similarity graphs became the leading paradigm for efficient nearest\nneighbor search, outperforming traditional tree-based and LSH-based methods.\nSimilarity graphs perform the search via greedy routing: a query traverses the\ngraph and in each vertex moves to the adjacent vertex that is the closest to\nthis query. In practice, similarity graphs are often susceptible to local\nminima, when queries do not reach its nearest neighbors, getting stuck in\nsuboptimal vertices. In this paper we propose to learn the routing function\nthat overcomes local minima via incorporating information about the graph\nglobal structure. In particular, we augment the vertices of a given graph with\nadditional representations that are learned to provide the optimal routing from\nthe start vertex to the query nearest neighbor. By thorough experiments, we\ndemonstrate that the proposed learnable routing successfully diminishes the\nlocal minima problem and significantly improves the overall search performance.</p>\n", "tags": ["Tree-Based-Ann", "Evaluation", "Locality-Sensitive-Hashing"], "tsne_embedding": [54.9542121887207, 4.818283557891846], "cluster": 9}, {"key": "baranchuk2023dedrift", "year": "2023", "citations": "2", "title": "Dedrift: Robust Similarity Search Under Content Drift", "abstract": "<p>The statistical distribution of content uploaded and searched on media\nsharing sites changes over time due to seasonal, sociological and technical\nfactors. We investigate the impact of this \u201ccontent drift\u201d for large-scale\nsimilarity search tools, based on nearest neighbor search in embedding space.\nUnless a costly index reconstruction is performed frequently, content drift\ndegrades the search accuracy and efficiency. The degradation is especially\nsevere since, in general, both the query and database distributions change.\n  We introduce and analyze real-world image and video datasets for which\ntemporal information is available over a long time period. Based on the\nlearnings, we devise DeDrift, a method that updates embedding quantizers to\ncontinuously adapt large-scale indexing structures on-the-fly. DeDrift almost\neliminates the accuracy degradation due to the query and database content drift\nwhile being up to 100x faster than a full index reconstruction.</p>\n", "tags": ["ICCV", "Efficiency", "Similarity-Search", "Scalability", "Datasets"], "tsne_embedding": [32.572898864746094, 0.7431716322898865], "cluster": 9}, {"key": "barbany2023procsim", "year": "2024", "citations": "0", "title": "Procsim: Proxy-based Confidence For Robust Similarity Learning", "abstract": "<p>Deep Metric Learning (DML) methods aim at learning an embedding space in\nwhich distances are closely related to the inherent semantic similarity of the\ninputs. Previous studies have shown that popular benchmark datasets often\ncontain numerous wrong labels, and DML methods are susceptible to them.\nIntending to study the effect of realistic noise, we create an ontology of the\nclasses in a dataset and use it to simulate semantically coherent labeling\nmistakes. To train robust DML models, we propose ProcSim, a simple framework\nthat assigns a confidence score to each sample using the normalized distance to\nits class representative. The experimental results show that the proposed\nmethod achieves state-of-the-art performance on the DML benchmark datasets\ninjected with uniform and the proposed semantically coherent noise.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-14.200352668762207, -17.430587768554688], "cluster": 1}, {"key": "barz2017automatic", "year": "2018", "citations": "2", "title": "Automatic Query Image Disambiguation For Content-based Image Retrieval", "abstract": "<p>Query images presented to content-based image retrieval systems often have\nvarious different interpretations, making it difficult to identify the search\nobjective pursued by the user. We propose a technique for overcoming this\nambiguity, while keeping the amount of required user interaction at a minimum.\nTo achieve this, the neighborhood of the query image is divided into coherent\nclusters from which the user may choose the relevant ones. A novel feedback\nintegration technique is then employed to re-rank the entire database with\nregard to both the user feedback and the original query. We evaluate our\napproach on the publicly available MIRFLICKR-25K dataset, where it leads to a\nrelative improvement of average precision by 23% over the baseline retrieval,\nwhich does not distinguish between different image senses.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-12.349881172180176, -11.86911678314209], "cluster": 1}, {"key": "barz2018hierarchy", "year": "2019", "citations": "52", "title": "Hierarchy-based Image Embeddings For Semantic Image Retrieval", "abstract": "<p>Deep neural networks trained for classification have been found to learn\npowerful image representations, which are also often used for other tasks such\nas comparing images w.r.t. their visual similarity. However, visual similarity\ndoes not imply semantic similarity. In order to learn semantically\ndiscriminative features, we propose to map images onto class embeddings whose\npair-wise dot products correspond to a measure of semantic similarity between\nclasses. Such an embedding does not only improve image retrieval results, but\ncould also facilitate integrating semantics for other tasks, e.g., novelty\ndetection or few-shot learning. We introduce a deterministic algorithm for\ncomputing the class centroids directly based on prior world-knowledge encoded\nin a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,\nand ImageNet show that our learned semantic image embeddings improve the\nsemantic consistency of image retrieval results by a large margin.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Image-Retrieval"], "tsne_embedding": [-12.40145206451416, -16.741058349609375], "cluster": 1}, {"key": "bastan2016multi", "year": "2016", "citations": "0", "title": "Multi-view Product Image Search Using Deep Convnets Representations", "abstract": "<p>Multi-view product image queries can improve retrieval performance over\nsingle view queries significantly. In this paper, we investigated the\nperformance of deep convolutional neural networks (ConvNets) on multi-view\nproduct image search. First, we trained a VGG-like network to learn deep\nConvNets representations of product images. Then, we computed the deep ConvNets\nrepresentations of database and query images and performed single view queries,\nand multi-view queries using several early and late fusion approaches.\n  We performed extensive experiments on the publicly available Multi-View\nObject Image Dataset (MVOD 5K) with both clean background queries from the\nInternet and cluttered background queries from a mobile phone. We compared the\nperformance of ConvNets to the classical bag-of-visual-words (BoWs). We\nconcluded that (1) multi-view queries with deep ConvNets representations\nperform significantly better than single view queries, (2) ConvNets perform\nmuch better than BoWs and have room for further improvement, (3) pre-training\nof ConvNets on a different image dataset with background clutter is needed to\nobtain good performance on cluttered product image queries obtained with a\nmobile phone.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-6.514022350311279, -29.99629020690918], "cluster": 3}, {"key": "beck2019distributed", "year": "2019", "citations": "29", "title": "A Distributed And Approximated Nearest Neighbors Algorithm For An Efficient Large Scale Mean Shift Clustering", "abstract": "<p>In this paper we target the class of modal clustering methods where clusters\nare defined in terms of the local modes of the probability density function\nwhich generates the data. The most well-known modal clustering method is the\nk-means clustering. Mean Shift clustering is a generalization of the k-means\nclustering which computes arbitrarily shaped clusters as defined as the basins\nof attraction to the local modes created by the density gradient ascent paths.\nDespite its potential, the Mean Shift approach is a computationally expensive\nmethod for unsupervised learning. Thus, we introduce two contributions aiming\nto provide clustering algorithms with a linear time complexity, as opposed to\nthe quadratic time complexity for the exact Mean Shift clustering. Firstly we\npropose a scalable procedure to approximate the density gradient ascent.\nSecond, our proposed scalable cluster labeling technique is presented. Both\npropositions are based on Locality Sensitive Hashing (LSH) to approximate\nnearest neighbors. These two techniques may be used for moderate sized\ndatasets. Furthermore, we show that using our proposed approximations of the\ndensity gradient ascent as a pre-processing step in other clustering methods\ncan also improve dedicated classification metrics. For the latter, a\ndistributed implementation, written for the Spark/Scala ecosystem is proposed.\nFor all these considered clustering methods, we present experimental results\nillustrating their labeling accuracy and their potential to solve concrete\nproblems.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Unsupervised", "Datasets"], "tsne_embedding": [44.71063232421875, 6.918314456939697], "cluster": 9}, {"key": "belyy2018memoir", "year": "2018", "citations": "0", "title": "MEMOIR: Multi-class Extreme Classification With Inexact Margin", "abstract": "<p>Multi-class classification with a very large number of classes, or extreme\nclassification, is a challenging problem from both statistical and\ncomputational perspectives. Most of the classical approaches to multi-class\nclassification, including one-vs-rest or multi-class support vector machines,\nrequire the exact estimation of the classifier\u2019s margin, at both the training\nand the prediction steps making them intractable in extreme classification\nscenarios. In this paper, we study the impact of computing an approximate\nmargin using nearest neighbor (ANN) search structures combined with\nlocality-sensitive hashing (LSH). This approximation allows to dramatically\nreduce both the training and the prediction time without a significant loss in\nperformance. We theoretically prove that this approximation does not lead to a\nsignificant loss of the risk of the model and provide empirical evidence over\nfive publicly available large scale datasets, showing that the proposed\napproach is highly competitive with respect to state-of-the-art approaches on\ntime, memory and performance measures.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [36.670352935791016, 27.747325897216797], "cluster": 2}, {"key": "bencohen2021semantic", "year": "2021", "citations": "29", "title": "Semantic Diversity Learning For Zero-shot Multi-label Classification", "abstract": "<p>Training a neural network model for recognizing multiple labels associated\nwith an image, including identifying unseen labels, is challenging, especially\nfor images that portray numerous semantically diverse labels. As challenging as\nthis task is, it is an essential task to tackle since it represents many\nreal-world cases, such as image retrieval of natural images. We argue that\nusing a single embedding vector to represent an image, as commonly practiced,\nis not sufficient to rank both relevant seen and unseen labels accurately. This\nstudy introduces an end-to-end model training for multi-label zero-shot\nlearning that supports semantic diversity of the images and labels. We propose\nto use an embedding matrix having principal embedding vectors trained using a\ntailored loss function. In addition, during training, we suggest up-weighting\nin the loss function image samples presenting higher semantic diversity to\nencourage the diversity of the embedding matrix. Extensive experiments show\nthat our proposed method improves the zero-shot model\u2019s quality in tag-based\nimage retrieval achieving SoTA results on several common datasets (NUS-Wide,\nCOCO, Open Images).</p>\n", "tags": ["ICCV", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets"], "tsne_embedding": [-23.063854217529297, -17.1309814453125], "cluster": 5}, {"key": "bergman2025leveraging", "year": "2025", "citations": "0", "title": "Leveraging Approximate Caching For Faster Retrieval-augmented Generation", "abstract": "<p>Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [4.459934711456299, -20.24173355102539], "cluster": 7}, {"key": "berman2018supermodular", "year": "2018", "citations": "1", "title": "Supermodular Locality Sensitive Hashes", "abstract": "<p>In this work, we show deep connections between Locality Sensitive Hashability\nand submodular analysis. We show that the LSHablility of the most commonly\nanalyzed set similarities is in one-to-one correspondance with the\nsupermodularity of these similarities when taken with respect to the symmetric\ndifference of their arguments. We find that the supermodularity of equivalent\nLSHable similarities can be dependent on the set encoding. While monotonicity\nand supermodularity does not imply the metric condition necessary for\nsupermodularity, this condition is guaranteed for the more restricted class of\nsupermodular Hamming similarities that we introduce. We show moreover that LSH\npreserving transformations are also supermodular-preserving, yielding a way to\ngenerate families of similarities both LSHable and supermodular. Finally, we\nshow that even the more restricted family of cardinality-based supermodular\nHamming similarities presents promising aspects for the study of the link\nbetween LSHability and supermodularity. We hope that the several bridges that\nwe introduce between LSHability and supermodularity paves the way to a better\nunderstanding both of supermodular analysis and LSHability, notably in the\ncontext of large-scale supermodular optimization.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Scalability"], "tsne_embedding": [16.053483963012695, 35.61354446411133], "cluster": 4}, {"key": "berman2019multigrain", "year": "2019", "citations": "46", "title": "Multigrain: A Unified Image Embedding For Classes And Instances", "abstract": "<p>MultiGrain is a network architecture producing compact vector representations\nthat are suited both for image classification and particular object retrieval.\nIt builds on a standard classification trunk. The top of the network produces\nan embedding containing coarse and fine-grained information, so that images can\nbe recognized based on the object class, particular object, or if they are\ndistorted copies. Our joint training is simple: we minimize a cross-entropy\nloss for classification and a ranking loss that determines if two images are\nidentical up to data augmentation, with no need for additional labels. A key\ncomponent of MultiGrain is a pooling layer that takes advantage of\nhigh-resolution images with a network trained at a lower resolution.\n  When fed to a linear classifier, the learned embeddings provide\nstate-of-the-art classification accuracy. For instance, we obtain 79.4% top-1\naccuracy with a ResNet-50 learned on Imagenet, which is a +1.8% absolute\nimprovement over the AutoAugment method. When compared with the cosine\nsimilarity, the same embeddings perform on par with the state-of-the-art for\nimage retrieval at moderate resolutions.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-20.438161849975586, 3.2184507846832275], "cluster": 1}, {"key": "bernabeu2022multi", "year": "2022", "citations": "2", "title": "Multi-label Logo Recognition And Retrieval Based On Weighted Fusion Of Neural Features", "abstract": "<p>Classifying logo images is a challenging task as they contain elements such\nas text or shapes that can represent anything from known objects to abstract\nshapes. While the current state of the art for logo classification addresses\nthe problem as a multi-class task focusing on a single characteristic, logos\ncan have several simultaneous labels, such as different colors. This work\nproposes a method that allows visually similar logos to be classified and\nsearched from a set of data according to their shape, color, commercial sector,\nsemantics, general characteristics, or a combination of features selected by\nthe user. Unlike previous approaches, the proposal employs a series of\nmulti-label deep neural networks specialized in specific attributes and\ncombines the obtained features to perform the similarity search. To delve into\nthe classification system, different existing logo topologies are compared and\nsome of their problems are analyzed, such as the incomplete labeling that\ntrademark registration databases usually contain. The proposal is evaluated\nconsidering 76,000 logos (7 times more than previous approaches) from the\nEuropean Union Trademarks dataset, which is organized hierarchically using the\nVienna ontology. Overall, experimentation attains reliable quantitative and\nqualitative results, reducing the normalized average rank error of the\nstate-of-the-art from 0.040 to 0.018 for the Trademark Image Retrieval task.\nFinally, given that the semantics of logos can often be subjective, graphic\ndesign students and professionals were surveyed. Results show that the proposed\nmethodology provides better labeling than a human expert operator, improving\nthe label ranking average precision from 0.53 to 0.68.</p>\n", "tags": ["Evaluation", "Similarity-Search", "Image-Retrieval", "Datasets"], "tsne_embedding": [-53.00416564941406, -14.106291770935059], "cluster": 5}, {"key": "berriche2024leveraging", "year": "2025", "citations": "0", "title": "Leveraging High-resolution Features For Improved Deep Hashing-based Image Retrieval", "abstract": "<p>Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-2.8852627277374268, 2.0973901748657227], "cluster": 1}, {"key": "berton2023eigenplaces", "year": "2023", "citations": "33", "title": "Eigenplaces: Training Viewpoint Robust Models For Visual Place Recognition", "abstract": "<p>Visual Place Recognition is a task that aims to predict the place of an image\n(called query) based solely on its visual features. This is typically done\nthrough image retrieval, where the query is matched to the most similar images\nfrom a large database of geotagged photos, using learned global descriptors. A\nmajor challenge in this task is recognizing places seen from different\nviewpoints. To overcome this limitation, we propose a new method, called\nEigenPlaces, to train our neural network on images from different point of\nviews, which embeds viewpoint robustness into the learned global descriptors.\nThe underlying idea is to cluster the training data so as to explicitly present\nthe model with different views of the same points of interest. The selection of\nthis points of interest is done without the need for extra supervision. We then\npresent experiments on the most comprehensive set of datasets in literature,\nfinding that EigenPlaces is able to outperform previous state of the art on the\nmajority of datasets, while requiring 60% less GPU memory for training and\nusing 50% smaller descriptors. The code and trained models for EigenPlaces are\navailable at {\\small{https://github.com/gmberton/EigenPlaces}}, while\nresults with any other baseline can be computed with the codebase at\n{\\small{https://github.com/gmberton/auto_VPR}}.</p>\n", "tags": ["Robustness", "ICCV", "Image-Retrieval", "Datasets"], "tsne_embedding": [-22.496353149414062, -12.062847137451172], "cluster": 1}, {"key": "bessa2023weighted", "year": "2023", "citations": "2", "title": "Weighted Minwise Hashing Beats Linear Sketching For Inner Product Estimation", "abstract": "<p>We present a new approach for computing compact sketches that can be used to\napproximate the inner product between pairs of high-dimensional vectors. Based\non the Weighted MinHash algorithm, our approach admits strong accuracy\nguarantees that improve on the guarantees of popular linear sketching\napproaches for inner product estimation, such as CountSketch and\nJohnson-Lindenstrauss projection. Specifically, while our method admits\nguarantees that exactly match linear sketching for dense vectors, it yields\nsignificantly lower error for sparse vectors with limited overlap between\nnon-zero entries. Such vectors arise in many applications involving sparse\ndata. They are also important in increasingly popular dataset search\napplications, where inner product sketches are used to estimate data\ncovariance, conditional means, and other quantities involving columns in\nunjoined tables. We complement our theoretical results by showing that our\napproach empirically outperforms existing linear sketches and unweighted\nhashing-based sketches for sparse vectors.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [24.638959884643555, 23.401594161987305], "cluster": 2}, {"key": "bhatnagar2018fine", "year": "2018", "citations": "2", "title": "Fine-grained Apparel Classification And Retrieval Without Rich Annotations", "abstract": "<p>The ability to correctly classify and retrieve apparel images has a variety\nof applications important to e-commerce, online advertising and internet\nsearch. In this work, we propose a robust framework for fine-grained apparel\nclassification, in-shop and cross-domain retrieval which eliminates the\nrequirement of rich annotations like bounding boxes and human-joints or\nclothing landmarks, and training of bounding box/ key-landmark detector for the\nsame. Factors such as subtle appearance differences, variations in human poses,\ndifferent shooting angles, apparel deformations, and self-occlusion add to the\nchallenges in classification and retrieval of apparel items. Cross-domain\nretrieval is even harder due to the presence of large variation between online\nshopping images, usually taken in ideal lighting, pose, positive angle and\nclean background as compared with street photos captured by users in\ncomplicated conditions with poor lighting and cluttered scenes. Our framework\nuses compact bilinear CNN with tensor sketch algorithm to generate embeddings\nthat capture local pairwise feature interactions in a translationally invariant\nmanner. For apparel classification, we pass the feature embeddings through a\nsoftmax classifier, while, the in-shop and cross-domain retrieval pipelines use\na triplet-loss based optimization approach, such that squared Euclidean\ndistance between embeddings measures the dissimilarity between the images.\nUnlike previous works that relied on bounding box, key clothing landmarks or\nhuman joint detectors to assist the final deep classifier, proposed framework\ncan be trained directly on the provided category labels or generated triplets\nfor triplet loss optimization. Lastly, Experimental results on the DeepFashion\nfine-grained categorization, and in-shop and consumer-to-shop retrieval\ndatasets provide a comparative analysis with previous work performed in the\ndomain.</p>\n", "tags": ["Tools-&-Libraries", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-17.88468360900879, -48.75070571899414], "cluster": 3}, {"key": "bhatnagar2024piecewise", "year": "2024", "citations": "0", "title": "Piecewise-linear Manifolds For Deep Metric Learning", "abstract": "<p>Unsupervised deep metric learning (UDML) focuses on learning a semantic\nrepresentation space using only unlabeled data. This challenging problem\nrequires accurately estimating the similarity between data points, which is\nused to supervise a deep network. For this purpose, we propose to model the\nhigh-dimensional data manifold using a piecewise-linear approximation, with\neach low-dimensional linear piece approximating the data manifold in a small\nneighborhood of a point. These neighborhoods are used to estimate similarity\nbetween data points. We empirically show that this similarity estimate\ncorrelates better with the ground truth than the similarity estimates of\ncurrent state-of-the-art techniques. We also show that proxies, commonly used\nin supervised metric learning, can be used to model the piecewise-linear\nmanifold in an unsupervised setting, helping improve performance. Our method\noutperforms existing unsupervised metric learning approaches on standard\nzero-shot image retrieval benchmarks.</p>\n", "tags": ["Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-10.636509895324707, -14.950512886047363], "cluster": 1}, {"key": "bhattacharyya2022visual", "year": "2022", "citations": "2", "title": "Visual Representation Learning With Self-supervised Attention For Low-label High-data Regime", "abstract": "<p>Self-supervision has shown outstanding results for natural language\nprocessing, and more recently, for image recognition. Simultaneously, vision\ntransformers and its variants have emerged as a promising and scalable\nalternative to convolutions on various computer vision tasks. In this paper, we\nare the first to question if self-supervised vision transformers (SSL-ViTs) can\nbe adapted to two important computer vision tasks in the low-label, high-data\nregime: few-shot image classification and zero-shot image retrieval. The\nmotivation is to reduce the number of manual annotations required to train a\nvisual embedder, and to produce generalizable and semantically meaningful\nembeddings. For few-shot image classification we train SSL-ViTs without any\nsupervision, on external data, and use this trained embedder to adapt quickly\nto novel classes with limited number of labels. For zero-shot image retrieval,\nwe use SSL-ViTs pre-trained on a large dataset without any labels and fine-tune\nthem with several metric learning objectives. Our self-supervised attention\nrepresentations outperforms the state-of-the-art on several public benchmarks\nfor both tasks, namely miniImageNet and CUB200 for few-shot image\nclassification by up-to 6%-10%, and Stanford Online Products, Cars196 and\nCUB200 for zero-shot image retrieval by up-to 4%-11%. Code is available at\nhttps://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "ICASSP", "Datasets", "Supervised"], "tsne_embedding": [-34.783992767333984, -2.580077886581421], "cluster": 0}, {"key": "bhunia2018texture", "year": "2019", "citations": "17", "title": "Texture Synthesis Guided Deep Hashing For Texture Image Retrieval", "abstract": "<p>With the large-scale explosion of images and videos over the internet,\nefficient hashing methods have been developed to facilitate memory and time\nefficient retrieval of similar images. However, none of the existing works uses\nhashing to address texture image retrieval mostly because of the lack of\nsufficiently large texture image databases. Our work addresses this problem by\ndeveloping a novel deep learning architecture that generates binary hash codes\nfor input texture images. For this, we first pre-train a Texture Synthesis\nNetwork (TSN) which takes a texture patch as input and outputs an enlarged view\nof the texture by injecting newer texture content. Thus it signifies that the\nTSN encodes the learnt texture specific information in its intermediate layers.\nIn the next stage, a second network gathers the multi-scale feature\nrepresentations from the TSN\u2019s intermediate layers using channel-wise\nattention, combines them in a progressive manner to a dense continuous\nrepresentation which is finally converted into a binary hash code with the help\nof individual and pairwise label information. The new enlarged texture patches\nalso help in data augmentation to alleviate the problem of insufficient texture\ndata and are used to train the second stage of the network. Experiments on\nthree public texture image retrieval datasets indicate the superiority of our\ntexture synthesis guided hashing approach over current state-of-the-art\nmethods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Similarity-Search", "Scalability", "Datasets", "Neural-Hashing"], "tsne_embedding": [-1.9414395093917847, -0.31698328256607056], "cluster": 1}, {"key": "bhunia2022adaptive", "year": "2022", "citations": "20", "title": "Adaptive Fine-grained Sketch-based Image Retrieval", "abstract": "<p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has\nshifted towards generalising a model to new categories without any training\ndata from them. In real-world applications, however, a trained FG-SBIR model is\noften applied to both new categories and different human sketchers, i.e.,\ndifferent drawing styles. Although this complicates the generalisation problem,\nfortunately, a handful of examples are typically available, enabling the model\nto adapt to the new category/style. In this paper, we offer a novel perspective\n\u2013 instead of asking for a model that generalises, we advocate for one that\nquickly adapts, with just very few samples during testing (in a few-shot\nmanner). To solve this new problem, we introduce a novel model-agnostic\nmeta-learning (MAML) based framework with several key modifications: (1) As a\nretrieval task with a margin-based contrastive loss, we simplify the MAML\ntraining in the inner loop to make it more stable and tractable. (2) The margin\nin our contrastive loss is also meta-learned with the rest of the model. (3)\nThree additional regularisation losses are introduced in the outer loop, to\nmake the meta-learned FG-SBIR model more effective for category/style\nadaptation. Extensive experiments on public datasets suggest a large gain over\ngeneralisation and zero-shot based approaches, and a few strong few-shot\nbaselines.</p>\n", "tags": ["Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-41.606449127197266, -16.451566696166992], "cluster": 5}, {"key": "bianchi2021query2prod2vec", "year": "2021", "citations": "11", "title": "Query2prod2vec Grounded Word Embeddings For Ecommerce", "abstract": "<p>We present Query2Prod2Vec, a model that grounds lexical representations for\nproduct search in product embeddings: in our model, meaning is a mapping\nbetween words and a latent space of products in a digital shop. We leverage\nshopping sessions to learn the underlying space and use merchandising\nannotations to build lexical analogies for evaluation: our experiments show\nthat our model is more accurate than known techniques from the NLP and IR\nliterature. Finally, we stress the importance of data efficiency for product\nsearch outside of retail giants, and highlight how Query2Prod2Vec fits with\npractical constraints faced by most practitioners.</p>\n", "tags": ["Evaluation", "Efficiency"], "tsne_embedding": [-13.736848831176758, -45.78921890258789], "cluster": 3}, {"key": "bica2024improving", "year": "2024", "citations": "1", "title": "Improving Fine-grained Understanding In Image-text Pre-training", "abstract": "<p>We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple\nmethod for pretraining more fine-grained multimodal representations from\nimage-text pairs. Given that multiple image patches often correspond to single\nwords, we propose to learn a grouping of image patches for every token in the\ncaption. To achieve this, we use a sparse similarity metric between image\npatches and language tokens and compute for each token a language-grouped\nvision embedding as the weighted average of patches. The token and\nlanguage-grouped vision embeddings are then contrasted through a fine-grained\nsequence-wise loss that only depends on individual samples and does not require\nother batch samples as negatives. This enables more detailed information to be\nlearned in a computationally inexpensive manner. SPARC combines this\nfine-grained loss with a contrastive loss between global image and text\nembeddings to learn representations that simultaneously encode global and local\ninformation. We thoroughly evaluate our proposed method and show improved\nperformance over competing approaches both on image-level tasks relying on\ncoarse-grained information, e.g. classification, as well as region-level tasks\nrelying on fine-grained information, e.g. retrieval, object detection, and\nsegmentation. Moreover, SPARC improves model faithfulness and captioning in\nfoundational vision-language models.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-43.43380355834961, -11.954513549804688], "cluster": 5}, {"key": "billings2018gradient", "year": "2018", "citations": "0", "title": "Gradient Augmented Information Retrieval With Autoencoders And Semantic Hashing", "abstract": "<p>This paper will explore the use of autoencoders for semantic hashing in the\ncontext of Information Retrieval. This paper will summarize how to efficiently\ntrain an autoencoder in order to create meaningful and low-dimensional\nencodings of data. This paper will demonstrate how computing and storing the\nclosest encodings to an input query can help speed up search time and improve\nthe quality of our search results. The novel contributions of this paper\ninvolve using the representation of the data learned by an auto-encoder in\norder to augment our search query in various ways. I present and evaluate the\nnew gradient search augmentation (GSA) approach, as well as the more well-known\npseudo-relevance-feedback (PRF) adjustment. I find that GSA helps to improve\nthe performance of the TF-IDF based information retrieval system, and PRF\ncombined with GSA works best overall for the systems compared in this paper.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Text-Retrieval"], "tsne_embedding": [-12.288225173950195, 9.49341106414795], "cluster": 8}, {"key": "bin2023unifying", "year": "2023", "citations": "17", "title": "Unifying Two-stream Encoders With Transformers For Cross-modal Retrieval", "abstract": "<p>Most existing cross-modal retrieval methods employ two-stream encoders with\ndifferent architectures for images and texts, \\textit{e.g.}, CNN for images and\nRNN/Transformer for texts. Such discrepancy in architectures may induce\ndifferent semantic distribution spaces and limit the interactions between\nimages and texts, and further result in inferior alignment between images and\ntexts. To fill this research gap, inspired by recent advances of Transformers\nin vision tasks, we propose to unify the encoder architectures with\nTransformers for both modalities. Specifically, we design a cross-modal\nretrieval framework purely based on two-stream Transformers, dubbed\n\\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image\nTransformer, a text Transformer, and a hierarchical alignment module. With such\nidentical architectures, the encoders could produce representations with more\nsimilar characteristics for images and texts, and make the interactions and\nalignments between them much easier. Besides, to leverage the rich semantics,\nwe devise a hierarchical alignment scheme to explore multi-level\ncorrespondences of different layers between images and texts. To evaluate the\neffectiveness of the proposed HAT, we conduct extensive experiments on two\nbenchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that\nHAT outperforms SOTA baselines by a large margin. Specifically, on two key\ntasks, \\textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves\n7.6% and 16.7% relative score improvement of Recall@1 on MSCOCO, and 4.4%\nand 11.6% on Flickr30k respectively. The code is available at\nhttps://github.com/LuminosityX/HAT.</p>\n", "tags": ["Image-Retrieval", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-32.39487838745117, -21.15693473815918], "cluster": 5}, {"key": "biswas2020approximate", "year": "2021", "citations": "1", "title": "Approximate Nearest Neighbour Search On Privacy-aware Encoding Of User Locations To Identify Susceptible Infections In Simulated Epidemics", "abstract": "<p>Amidst an increasing number of infected cases during the Covid-19 pandemic,\nit is essential to trace, as early as possible, the susceptible people who\nmight have been infected by the disease due to their close proximity with\npeople who were tested positive for the virus. This early contact tracing is\nlikely to limit the rate of spread of the infection within a locality. In this\npaper, we investigate how effectively and efficiently can such a list of\nsusceptible people be found given a list of infected persons and their\nlocations. To address this problem from an information retrieval (search)\nperspective, we represent the location of each person at each time instant as a\npoint in a vector space. By using the locations of the given list of infected\npersons as queries, we investigate the feasibility of applying approximate\nnearest neighbour (ANN) based indexing and retrieval approaches to obtain a\nlist of top-k suspected users in real-time. Since leveraging information from\ntrue user location data can lead to security and privacy concerns, we also\ninvestigate what effects does distance-preserving encoding methods have on the\neffectiveness of the ANN methods. Experiments conducted on real and synthetic\ndatasets demonstrate that the top-k retrieved lists of susceptible users\nretrieved with existing ANN approaches (KD-tree and HNSW) yield satisfactory\nprecision and recall values, thus indicating that ANN approaches can\npotentially be applied in practice to facilitate real-time contact tracing even\nunder the presence of imposed privacy constraints.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Similarity-Search", "Tree-Based-Ann", "Datasets", "Evaluation"], "tsne_embedding": [-20.367431640625, 43.22474670410156], "cluster": 8}, {"key": "biswas2021state", "year": "2021", "citations": "4", "title": "State Of The Art: Image Hashing", "abstract": "<p>Perceptual image hashing methods are often applied in various objectives,\nsuch as image retrieval, finding duplicate or near-duplicate images, and\nfinding similar images from large-scale image content. The main challenge in\nimage hashing techniques is robust feature extraction, which generates the same\nor similar hashes in images that are visually identical. In this article, we\npresent a short review of the state-of-the-art traditional perceptual hashing\nand deep learning-based perceptual hashing methods, identifying the best\napproaches.</p>\n", "tags": ["Survey-Paper", "Image-Retrieval", "Hashing-Methods", "Scalability"], "tsne_embedding": [-17.360774993896484, 9.68575382232666], "cluster": 8}, {"key": "black2021compositional", "year": "2021", "citations": "2", "title": "Compositional Sketch Search", "abstract": "<p>We present an algorithm for searching image collections using free-hand\nsketches that describe the appearance and relative positions of multiple\nobjects. Sketch based image retrieval (SBIR) methods predominantly match\nqueries containing a single, dominant object invariant to its position within\nan image. Our work exploits drawings as a concise and intuitive representation\nfor specifying entire scene compositions. We train a convolutional neural\nnetwork (CNN) to encode masked visual features from sketched objects, pooling\nthese into a spatial descriptor encoding the spatial relationships and\nappearances of objects in the composition. Training the CNN backbone as a\nSiamese network under triplet loss yields a metric search embedding for\nmeasuring compositional similarity which may be efficiently leveraged for\nvisual search by applying product quantization.</p>\n", "tags": ["Quantization", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-43.433265686035156, -21.432327270507812], "cluster": 5}, {"key": "black2022simprov", "year": "2022", "citations": "0", "title": "Simprov: Scalable Image Provenance Framework For Robust Content Attribution", "abstract": "<p>We present SImProv - a scalable image provenance framework to match a query\nimage back to a trusted database of originals and identify possible\nmanipulations on the query. SImProv consists of three stages: a scalable search\nstage for retrieving top-k most similar images; a re-ranking and\nnear-duplicated detection stage for identifying the original among the\ncandidates; and finally a manipulation detection and visualization stage for\nlocalizing regions within the query that may have been manipulated to differ\nfrom the original. SImProv is robust to benign image transformations that\ncommonly occur during online redistribution, such as artifacts due to noise and\nrecompression degradation, as well as out-of-place transformations due to image\npadding, warping, and changes in size and shape. Robustness towards\nout-of-place transformations is achieved via the end-to-end training of a\ndifferentiable warping module within the comparator architecture. We\ndemonstrate effective retrieval and manipulation detection over a dataset of\n100 million images.</p>\n", "tags": ["Scalability", "Robustness", "Tools-&-Libraries", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [-19.218542098999023, 10.964624404907227], "cluster": 8}, {"key": "blumenstiel2024multi", "year": "2024", "citations": "2", "title": "Multi-spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models", "abstract": "<p>Image retrieval enables an efficient search through vast amounts of satellite\nimagery and returns similar images to a query. Deep learning models can\nidentify images across various semantic concepts without the need for\nannotations. This work proposes to use Geospatial Foundation Models, like\nPrithvi, for remote sensing image retrieval with multiple benefits: i) the\nmodels encode multi-spectral satellite data and ii) generalize without further\nfine-tuning. We introduce two datasets to the retrieval task and observe a\nstrong performance: Prithvi processes six bands and achieves a mean Average\nPrecision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming\nother RGB-based models. Further, we evaluate three compression methods with\nbinarized embeddings balancing retrieval speed and accuracy. They match the\nretrieval speed of much shorter hash codes while maintaining the same accuracy\nas floating-point embeddings but with a 32-fold compression. The code is\navailable at https://github.com/IBM/remote-sensing-image-retrieval.</p>\n", "tags": ["Datasets", "Evaluation", "Image-Retrieval", "Hashing-Methods"], "tsne_embedding": [-26.234085083007812, 17.681652069091797], "cluster": 8}, {"key": "bogolin2021cross", "year": "2022", "citations": "58", "title": "Cross Modal Retrieval With Querybank Normalisation", "abstract": "<p>Profiting from large-scale training datasets, advances in neural architecture\ndesign and efficient inference, joint embeddings have become the dominant\napproach for tackling cross-modal retrieval. In this work we first show that,\ndespite their effectiveness, state-of-the-art joint embeddings suffer\nsignificantly from the longstanding \u201chubness problem\u201d in which a small number\nof gallery embeddings form the nearest neighbours of many queries. Drawing\ninspiration from the NLP literature, we formulate a simple but effective\nframework called Querybank Normalisation (QB-Norm) that re-normalises query\nsimilarities to account for hubs in the embedding space. QB-Norm improves\nretrieval performance without requiring retraining. Differently from prior\nwork, we show that QB-Norm works effectively without concurrent access to any\ntest set queries. Within the QB-Norm framework, we also propose a novel\nsimilarity normalisation method, the Dynamic Inverted Softmax, that is\nsignificantly more robust than existing approaches. We showcase QB-Norm across\na range of cross modal retrieval models and benchmarks where it consistently\nenhances strong baselines beyond the state of the art. Code is available at\nhttps://vladbogo.github.io/QB-Norm/.</p>\n", "tags": ["CVPR", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [3.944305181503296, -18.081527709960938], "cluster": 7}, {"key": "borgersen2023corrembed", "year": "2023", "citations": "3", "title": "Corrembed: Evaluating Pre-trained Model Image Similarity Efficacy With A Novel Metric", "abstract": "<p>Detecting visually similar images is a particularly useful attribute to look\nto when calculating product recommendations. Embedding similarity, which\nutilizes pre-trained computer vision models to extract high-level image\nfeatures, has demonstrated remarkable efficacy in identifying images with\nsimilar compositions. However, there is a lack of methods for evaluating the\nembeddings generated by these models, as conventional loss and performance\nmetrics do not adequately capture their performance in image similarity search\ntasks.\n  In this paper, we evaluate the viability of the image embeddings from\nnumerous pre-trained computer vision models using a novel approach named\nCorrEmbed. Our approach computes the correlation between distances in image\nembeddings and distances in human-generated tag vectors. We extensively\nevaluate numerous pre-trained Torchvision models using this metric, revealing\nan intuitive relationship of linear scaling between ImageNet1k accuracy scores\nand tag-correlation scores. Importantly, our method also identifies deviations\nfrom this pattern, providing insights into how different models capture\nhigh-level image features.\n  By offering a robust performance evaluation of these pre-trained models,\nCorrEmbed serves as a valuable tool for researchers and practitioners seeking\nto develop effective, data-driven approaches to similar item recommendations in\nfashion retail.</p>\n", "tags": ["Evaluation", "Similarity-Search"], "tsne_embedding": [-18.917232513427734, -43.251861572265625], "cluster": 3}, {"key": "borthwick2020scalable", "year": "2020", "citations": "3", "title": "Scalable Blocking For Very Large Databases", "abstract": "<p>In the field of database deduplication, the goal is to find approximately\nmatching records within a database. Blocking is a typical stage in this process\nthat involves cheaply finding candidate pairs of records that are potential\nmatches for further processing. We present here Hashed Dynamic Blocking, a new\napproach to blocking designed to address datasets larger than those studied in\nmost prior work. Hashed Dynamic Blocking (HDB) extends Dynamic Blocking, which\nleverages the insight that rare matching values and rare intersections of\nvalues are predictive of a matching relationship. We also present a novel use\nof Locality Sensitive Hashing (LSH) to build blocking key values for huge\ndatabases with a convenient configuration to control the trade-off between\nprecision and recall. HDB achieves massive scale by minimizing data movement,\nusing compact block representation, and greedily pruning ineffective candidate\nblocks using a Count-min Sketch approximate counting data structure. We\nbenchmark the algorithm by focusing on real-world datasets in excess of one\nmillion rows, demonstrating that the algorithm displays linear time complexity\nscaling in this range. Furthermore, we execute HDB on a 530 million row\nindustrial dataset, detecting 68 billion candidate pairs in less than three\nhours at a cost of $307 on a major cloud service.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [26.57035255432129, 16.094257354736328], "cluster": 2}, {"key": "boschin2023self", "year": "2023", "citations": "0", "title": "A Self-encoder For Learning Nearest Neighbors", "abstract": "<p>We present the self-encoder, a neural network trained to guess the identity\nof each data sample. Despite its simplicity, it learns a very useful\nrepresentation of data, in a self-supervised way. Specifically, the\nself-encoder learns to distribute the data samples in the embedding space so\nthat they are linearly separable from one another. This induces a geometry\nwhere two samples are close in the embedding space when they are not easy to\ndifferentiate. The self-encoder can then be combined with a nearest-neighbor\nclassifier or regressor for any subsequent supervised task. Unlike regular\nnearest neighbors, the predictions resulting from this encoding of data are\ninvariant to any scaling of features, making any preprocessing like min-max\nscaling not necessary. The experiments show the efficiency of the approach,\nespecially on heterogeneous data mixing numerical features and categorical\nfeatures.</p>\n", "tags": ["Supervised", "Self-Supervised", "Efficiency"], "tsne_embedding": [20.5570011138916, 9.98692798614502], "cluster": 6}, {"key": "bouma2018individual", "year": "2018", "citations": "36", "title": "Individual Common Dolphin Identification Via Metric Embedding Learning", "abstract": "<p>Photo-identification (photo-id) of dolphin individuals is a commonly used\ntechnique in ecological sciences to monitor state and health of individuals, as\nwell as to study the social structure and distribution of a population.\nTraditional photo-id involves a laborious manual process of matching each\ndolphin fin photograph captured in the field to a catalogue of known\nindividuals.\n  We examine this problem in the context of open-set recognition and utilise a\ntriplet loss function to learn a compact representation of fin images in a\nEuclidean embedding, where the Euclidean distance metric represents fin\nsimilarity. We show that this compact representation can be successfully learnt\nfrom a fairly small (in deep learning context) training set and still\ngeneralise well to out-of-sample identities (completely new dolphin\nindividuals), with top-1 and top-5 test set (37 individuals) accuracy of\n\\(90.5\\pm2\\) and \\(93.6\\pm1\\) percent. In the presence of 1200 distractors, top-1\naccuracy dropped by \\(12%\\); however, top-5 accuracy saw only a \\(2.8%\\) drop</p>\n", "tags": ["Distance-Metric-Learning"], "tsne_embedding": [-41.01839065551758, 16.529438018798828], "cluster": 0}, {"key": "bouma2019individual", "year": "2018", "citations": "36", "title": "Individual Common Dolphin Identification Via Metric Embedding Learning", "abstract": "<p>Photo-identification (photo-id) of dolphin individuals is a commonly used\ntechnique in ecological sciences to monitor state and health of individuals, as\nwell as to study the social structure and distribution of a population.\nTraditional photo-id involves a laborious manual process of matching each\ndolphin fin photograph captured in the field to a catalogue of known\nindividuals.\n  We examine this problem in the context of open-set recognition and utilise a\ntriplet loss function to learn a compact representation of fin images in a\nEuclidean embedding, where the Euclidean distance metric represents fin\nsimilarity. We show that this compact representation can be successfully learnt\nfrom a fairly small (in deep learning context) training set and still\ngeneralise well to out-of-sample identities (completely new dolphin\nindividuals), with top-1 and top-5 test set (37 individuals) accuracy of\n\\(90.5\\pm2\\) and \\(93.6\\pm1\\) percent. In the presence of 1200 distractors, top-1\naccuracy dropped by \\(12%\\); however, top-5 accuracy saw only a \\(2.8%\\) drop</p>\n", "tags": ["Distance-Metric-Learning"], "tsne_embedding": [-41.01839065551758, 16.529438018798828], "cluster": 0}, {"key": "boytsov2019accurate", "year": "2019", "citations": "3", "title": "Accurate And Fast Retrieval For Complex Non-metric Data Via Neighborhood Graphs", "abstract": "<p>We demonstrate that a graph-based search algorithm-relying on the\nconstruction of an approximate neighborhood graph-can directly work with\nchallenging non-metric and/or non-symmetric distances without resorting to\nmetric-space mapping and/or distance symmetrization, which, in turn, lead to\nsubstantial performance degradation. Although the straightforward metrization\nand symmetrization is usually ineffective, we find that constructing an index\nusing a modified, e.g., symmetrized, distance can improve performance. This\nobservation paves a way to a new line of research of designing index-specific\ngraph-construction distance functions.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Evaluation"], "tsne_embedding": [52.768672943115234, 6.879924774169922], "cluster": 9}, {"key": "boytsov2019pruning", "year": "2019", "citations": "4", "title": "Pruning Algorithms For Low-dimensional Non-metric K-nn Search: A Case Study", "abstract": "<p>We focus on low-dimensional non-metric search, where tree-based approaches\npermit efficient and accurate retrieval while having short indexing time. These\nmethods rely on space partitioning and require a pruning rule to avoid visiting\nunpromising parts. We consider two known data-driven approaches to extend these\nrules to non-metric spaces: TriGen and a piece-wise linear approximation of the\npruning rule. We propose and evaluate two adaptations of TriGen to\nnon-symmetric similarities (TriGen does not support non-symmetric distances).\nWe also evaluate a hybrid of TriGen and the piece-wise linear approximation\npruning. We find that this hybrid approach is often more effective than either\nof the pruning rules. We make our software publicly available.</p>\n", "tags": ["Tree-Based-Ann"], "tsne_embedding": [42.597530364990234, 12.380374908447266], "cluster": 9}, {"key": "breznik2022cross", "year": "2022", "citations": "3", "title": "Cross-modality Sub-image Retrieval Using Contrastive Multimodal Image Representations", "abstract": "<p>In tissue characterization and cancer diagnostics, multimodal imaging has\nemerged as a powerful technique. Thanks to computational advances, large\ndatasets can be exploited to discover patterns in pathologies and improve\ndiagnosis. However, this requires efficient and scalable image retrieval\nmethods. Cross-modality image retrieval is particularly challenging, since\nimages of similar (or even the same) content captured by different modalities\nmight share few common structures. We propose a new application-independent\ncontent-based image retrieval (CBIR) system for reverse (sub-)image search\nacross modalities, which combines deep learning to generate representations\n(embedding the different modalities in a common space) with classical feature\nextraction and bag-of-words models for efficient and reliable retrieval. We\nillustrate its advantages through a replacement study, exploring a number of\nfeature extractors and learned representations, as well as through comparison\nto recent (cross-modality) CBIR methods. For the task of (sub-)image retrieval\non a (publicly available) dataset of brightfield and second harmonic generation\nmicroscopy images, the results show that our approach is superior to all tested\nalternatives. We discuss the shortcomings of the compared methods and observe\nthe importance of equivariance and invariance properties of the learned\nrepresentations and feature extractors in the CBIR pipeline. Code is available\nat: https://github.com/MIDA-group/CrossModal_ImgRetrieval.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-50.86491012573242, 15.006589889526367], "cluster": 0}, {"key": "brooks2017multi", "year": "2017", "citations": "1", "title": "Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors", "abstract": "<p>This paper introduces \u201cMulti-Level Spherical LSH\u201d: parameter-free, a\nmulti-level, data-dependant Locality Sensitive Hashing data structure for\nsolving the Approximate Near Neighbors Problem (ANN). This data structure uses\na modified version of a multi-probe adaptive querying algorithm, with the\npotential of achieving a \\(O(n^p + t)\\) query run time, for all inputs n where \\(t\n&lt;= n\\).</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [25.817604064941406, 41.502986907958984], "cluster": 4}, {"key": "bruch2023approximate", "year": "2023", "citations": "6", "title": "An Approximate Algorithm For Maximum Inner Product Search Over Streaming Sparse Vectors", "abstract": "<p>Maximum Inner Product Search or top-k retrieval on sparse vectors is\nwell-understood in information retrieval, with a number of mature algorithms\nthat solve it exactly. However, all existing algorithms are tailored to text\nand frequency-based similarity measures. To achieve optimal memory footprint\nand query latency, they rely on the near stationarity of documents and on laws\ngoverning natural languages. We consider, instead, a setup in which collections\nare streaming \u2013 necessitating dynamic indexing \u2013 and where indexing and\nretrieval must work with arbitrarily distributed real-valued vectors. As we\nshow, existing algorithms are no longer competitive in this setup, even against\nnaive solutions. We investigate this gap and present a novel approximate\nsolution, called Sinnamon, that can efficiently retrieve the top-k results for\nsparse real valued vectors drawn from arbitrary distributions. Notably,\nSinnamon offers levers to trade-off memory consumption, latency, and accuracy,\nmaking the algorithm suitable for constrained applications and systems. We give\ntheoretical results on the error introduced by the approximate nature of the\nalgorithm, and present an empirical evaluation of its performance on two\nhardware platforms and synthetic and real-valued datasets. We conclude by\nlaying out concrete directions for future research on this general top-k\nretrieval problem over sparse vectors.</p>\n", "tags": ["Memory-Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [32.880645751953125, 24.996183395385742], "cluster": 2}, {"key": "bruch2024efficient", "year": "2024", "citations": "14", "title": "Efficient Inverted Indexes For Approximate Retrieval Over Learned Sparse Representations", "abstract": "<p>Learned sparse representations form an attractive class of contextual\nembeddings for text retrieval. That is so because they are effective models of\nrelevance and are interpretable by design. Despite their apparent compatibility\nwith inverted indexes, however, retrieval over sparse embeddings remains\nchallenging. That is due to the distributional differences between learned\nembeddings and term frequency-based lexical models of relevance such as BM25.\nRecognizing this challenge, a great deal of research has gone into, among other\nthings, designing retrieval algorithms tailored to the properties of learned\nsparse representations, including approximate retrieval systems. In fact, this\ntask featured prominently in the latest BigANN Challenge at NeurIPS 2023, where\napproximate algorithms were evaluated on a large benchmark dataset by\nthroughput and recall. In this work, we propose a novel organization of the\ninverted index that enables fast yet effective approximate retrieval over\nlearned sparse embeddings. Our approach organizes inverted lists into\ngeometrically-cohesive blocks, each equipped with a summary vector. During\nquery processing, we quickly determine if a block must be evaluated using the\nsummaries. As we show experimentally, single-threaded query processing using\nour method, Seismic, reaches sub-millisecond per-query latency on various\nsparse embeddings of the MS MARCO dataset while maintaining high recall. Our\nresults indicate that Seismic is one to two orders of magnitude faster than\nstate-of-the-art inverted index-based solutions and further outperforms the\nwinning (graph-based) submissions to the BigANN Challenge by a significant\nmargin.</p>\n", "tags": ["Graph-Based-Ann", "Text-Retrieval", "SIGIR", "Datasets", "Evaluation"], "tsne_embedding": [23.422517776489258, 15.269770622253418], "cluster": 2}, {"key": "bruch2024optimistic", "year": "2024", "citations": "1", "title": "Optimistic Query Routing In Clustering-based Approximate Maximum Inner Product Search", "abstract": "<p>Clustering-based nearest neighbor search is an effective method in which\npoints are partitioned into geometric shards to form an index, with only a few\nshards searched during query processing to find a set of top-\\(k\\) vectors. Even\nthough the search efficacy is heavily influenced by the algorithm that\nidentifies the shards to probe, it has received little attention in the\nliterature. This work bridges that gap by studying routing in clustering-based\nmaximum inner product search. We unpack existing routers and notice the\nsurprising contribution of optimism. We then take a page from the sequential\ndecision making literature and formalize that insight following the principle\nof ``optimism in the face of uncertainty.\u2019\u2019 In particular, we present a\nframework that incorporates the moments of the distribution of inner products\nwithin each shard to estimate the maximum inner product. We then present an\ninstance of our algorithm that uses only the first two moments to reach the\nsame accuracy as state-of-the-art routers such as ScaNN by probing up to \\(50%\\)\nfewer points on benchmark datasets. Our algorithm is also space-efficient: we\ndesign a sketch of the second moment whose size is independent of the number of\npoints and requires \\(\\mathcal{O}(1)\\) vectors per shard.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [36.00764083862305, 25.81801986694336], "cluster": 2}, {"key": "bruch2024pairing", "year": "2024", "citations": "0", "title": "Pairing Clustered Inverted Indexes With Knn Graphs For Fast Approximate Retrieval Over Learned Sparse Representations", "abstract": "<p>Learned sparse representations form an effective and interpretable class of\nembeddings for text retrieval. While exact top-k retrieval over such embeddings\nfaces efficiency challenges, a recent algorithm called Seismic has enabled\nremarkably fast, highly-accurate approximate retrieval. Seismic statically\nprunes inverted lists, organizes each list into geometrically-cohesive blocks,\nand augments each block with a summary vector. At query time, each inverted\nlist associated with a query term is traversed one block at a time in an\narbitrary order, with the inner product between the query and summaries\ndetermining if a block must be evaluated. When a block is deemed promising, its\ndocuments are fully evaluated with a forward index. Seismic is one to two\norders of magnitude faster than state-of-the-art inverted index-based solutions\nand significantly outperforms the winning graph-based submissions to the BigANN\n2023 Challenge. In this work, we speed up Seismic further by introducing two\ninnovations to its query processing subroutine. First, we traverse blocks in\norder of importance, rather than arbitrarily. Second, we take the list of\ndocuments retrieved by Seismic and expand it to include the neighbors of each\ndocument using an offline k-regular nearest neighbor graph; the expanded list\nis then ranked to produce the final top-k set. Experiments on two public\ndatasets show that our extension, named SeismicWave, can reach almost-exact\naccuracy levels and is up to 2.2x faster than Seismic.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Text-Retrieval", "Datasets"], "tsne_embedding": [23.749778747558594, 15.743124008178711], "cluster": 2}, {"key": "bruch2025investigating", "year": "2025", "citations": "1", "title": "Investigating The Scalability Of Approximate Sparse Retrieval Algorithms To Massive Datasets", "abstract": "<p>Learned sparse text embeddings have gained popularity due to their\neffectiveness in top-k retrieval and inherent interpretability. Their\ndistributional idiosyncrasies, however, have long hindered their use in\nreal-world retrieval systems. That changed with the recent development of\napproximate algorithms that leverage the distributional properties of sparse\nembeddings to speed up retrieval. Nonetheless, in much of the existing\nliterature, evaluation has been limited to datasets with only a few million\ndocuments such as MSMARCO. It remains unclear how these systems behave on much\nlarger datasets and what challenges lurk in larger scales. To bridge that gap,\nwe investigate the behavior of state-of-the-art retrieval algorithms on massive\ndatasets. We compare and contrast the recently-proposed Seismic and graph-based\nsolutions adapted from dense retrieval. We extensively evaluate Splade\nembeddings of 138M passages from MsMarco-v2 and report indexing time and other\nefficiency and effectiveness metrics.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [31.9514217376709, 24.396678924560547], "cluster": 2}, {"key": "bury2016efficient", "year": "2016", "citations": "0", "title": "Efficient Similarity Search In Dynamic Data Streams", "abstract": "<p>The Jaccard index is an important similarity measure for item sets and\nBoolean data. On large datasets, an exact similarity computation is often\ninfeasible for all item pairs both due to time and space constraints, giving\nrise to faster approximate methods. The algorithm of choice used to quickly\ncompute the Jaccard index \\(\\frac{\\vert A \\cap B \\vert}{\\vert A\\cup B\\vert}\\) of\ntwo item sets \\(A\\) and \\(B\\) is usually a form of min-hashing. Most min-hashing\nschemes are maintainable in data streams processing only additions, but none\nare known to work when facing item-wise deletions. In this paper, we\ninvestigate scalable approximation algorithms for rational set similarities, a\nbroad class of similarity measures including Jaccard. Motivated by a result of\nChierichetti and Kumar [J. ACM 2015] who showed any rational set similarity \\(S\\)\nadmits a locality sensitive hashing (LSH) scheme if and only if the\ncorresponding distance \\(1-S\\) is a metric, we can show that there exists a space\nefficient summary maintaining a \\((1\\pm \\epsilon)\\) multiplicative\napproximation to \\(1-S\\) in dynamic data streams. This in turn also yields a\n\\(\\epsilon\\) additive approximation of the similarity. The existence of these\napproximations hints at, but does not directly imply a LSH scheme in dynamic\ndata streams. Our second and main contribution now lies in the design of such a\nLSH scheme maintainable in dynamic data streams. The scheme is space efficient,\neasy to implement and to the best of our knowledge the first of its kind able\nto process deletions.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Datasets"], "tsne_embedding": [19.302490234375, 35.92558670043945], "cluster": 4}, {"key": "busolin2024early", "year": "2024", "citations": "2", "title": "Early Exit Strategies For Approximate K-nn Search In Dense Retrieval", "abstract": "<p>Learned dense representations are a popular family of techniques for encoding\nqueries and documents using high-dimensional embeddings, which enable retrieval\nby performing approximate k nearest-neighbors search (A-kNN). A popular\ntechnique for making A-kNN search efficient is based on a two-level index,\nwhere the embeddings of documents are clustered offline and, at query\nprocessing, a fixed number N of clusters closest to the query is visited\nexhaustively to compute the result set. In this paper, we build upon\nstate-of-the-art for early exit A-kNN and propose an unsupervised method based\non the notion of patience, which can reach competitive effectiveness with large\nefficiency gains. Moreover, we discuss a cascade approach where we first\nidentify queries that find their nearest neighbor within the closest t \u00ab\u00a0N\nclusters, and then we decide how many more to visit based on our patience\napproach or other state-of-the-art strategies. Reproducible experiments\nemploying state-of-the-art dense retrieval models and publicly available\nresources show that our techniques improve the A-kNN efficiency with up to 5x\nspeedups while achieving negligible effectiveness losses. All the code used is\navailable at https://github.com/francescobusolin/faiss_pEE</p>\n", "tags": ["Efficiency", "CIKM", "Similarity-Search", "Unsupervised"], "tsne_embedding": [34.20930862426758, 26.15410614013672], "cluster": 2}, {"key": "b\u00f6hm2020massively", "year": "2020", "citations": "4", "title": "Massively Parallel Graph Drawing And Representation Learning", "abstract": "<p>To fully exploit the performance potential of modern multi-core processors,\nmachine learning and data mining algorithms for big data must be parallelized\nin multiple ways. Today\u2019s CPUs consist of multiple cores, each following an\nindependent thread of control, and each equipped with multiple arithmetic units\nwhich can perform the same operation on a vector of multiple data objects.\nGraph embedding, i.e. converting the vertices of a graph into numerical vectors\nis a data mining task of high importance and is useful for graph drawing\n(low-dimensional vectors) and graph representation learning (high-dimensional\nvectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by\nMinimizing the Predictive Entropy), an information-theoretic method which can\ngenerate low and high-dimensional vectors. MulticoreGEMPE applies MIMD\n(Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single\nInstructions Multiple Data, using AVX-512) parallelism. We propose general\nideas applicable in other graph-based algorithms like <em>vectorized hashing</em>\nand <em>vectorized reduction</em>. Our experimental evaluation demonstrates the\nsuperiority of our approach.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Hashing-Methods"], "tsne_embedding": [54.93842697143555, 0.38391897082328796], "cluster": 9}, {"key": "b\u00f6nisch2025finding", "year": "2025", "citations": "0", "title": "Finding Needles In Emb(a)dding Haystacks: Legal Document Retrieval Via Bagging And SVR Ensembles", "abstract": "<p>We introduce a retrieval approach leveraging Support Vector Regression (SVR)\nensembles, bootstrap aggregation (bagging), and embedding spaces on the German\nDataset for Legal Information Retrieval (GerDaLIR). By conceptualizing the\nretrieval task in terms of multiple binary needle-in-a-haystack subtasks, we\nshow improved recall over the baselines (0.849 &gt; 0.803 | 0.829) using our\nvoting ensemble, suggesting promising initial results, without training or\nfine-tuning any deep learning models. Our approach holds potential for further\nenhancement, particularly through refining the encoding models and optimizing\nhyperparameters.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [9.321660041809082, -15.02223014831543], "cluster": 7}, {"key": "b\u0142asiok2016adagio", "year": "2016", "citations": "1", "title": "ADAGIO: Fast Data-aware Near-isometric Linear Embeddings", "abstract": "<p>Many important applications, including signal reconstruction, parameter\nestimation, and signal processing in a compressed domain, rely on a\nlow-dimensional representation of the dataset that preserves {\\em all} pairwise\ndistances between the data points and leverages the inherent geometric\nstructure that is typically present. Recently Hedge, Sankaranarayanan, Yin and\nBaraniuk \\cite{hedge2015} proposed the first data-aware near-isometric linear\nembedding which achieves the best of both worlds. However, their method NuMax\ndoes not scale to large-scale datasets.\n  Our main contribution is a simple, data-aware, near-isometric linear\ndimensionality reduction method which significantly outperforms a\nstate-of-the-art method \\cite{hedge2015} with respect to scalability while\nachieving high quality near-isometries. Furthermore, our method comes with\nstrong worst-case theoretical guarantees that allow us to guarantee the quality\nof the obtained near-isometry. We verify experimentally the efficiency of our\nmethod on numerous real-world datasets, where we find that our method (\\(&lt;\\)10\nsecs) is more than 3\\,000\\(\\times\\) faster than the state-of-the-art method\n\\cite{hedge2015} (\\(&gt;\\)9 hours) on medium scale datasets with 60\\,000 data points\nin 784 dimensions. Finally, we use our method as a preprocessing step to\nincrease the computational efficiency of a classification application and for\nspeeding up approximate nearest neighbor queries.</p>\n", "tags": ["Efficiency", "Scalability", "Datasets"], "tsne_embedding": [13.253852844238281, 28.31761360168457], "cluster": 4}, {"key": "cadar2024leveraging", "year": "2024", "citations": "0", "title": "Leveraging Semantic Cues From Foundation Vision Models For Enhanced Local Feature Correspondence", "abstract": "<p>Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24</p>\n", "tags": ["Evaluation", "Similarity-Search"], "tsne_embedding": [-18.860864639282227, -11.155328750610352], "cluster": 1}, {"key": "cai2016revisit", "year": "2019", "citations": "30", "title": "A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many\nareas of machine learning and data mining. During the past decade, numerous\nhashing algorithms are proposed to solve this problem. Every proposed algorithm\nclaims outperform other state-of-the-art hashing methods. However, the\nevaluation of these hashing papers was not thorough enough, and those claims\nshould be re-examined. The ultimate goal of an ANNS method is returning the\nmost accurate answers (nearest neighbors) in the shortest time. If implemented\ncorrectly, almost all the hashing methods will have their performance improved\nas the code length increases. However, many existing hashing papers only report\nthe performance with the code length shorter than 128. In this paper, we\ncarefully revisit the problem of search with a hash index, and analyze the pros\nand cons of two popular hash index search procedures. Then we proposed a very\nsimple but effective two level index structures and make a thorough comparison\nof eleven popular hashing algorithms. Surprisingly, the random-projection-based\nLocality Sensitive Hashing (LSH) is the best performed algorithm, which is in\ncontradiction to the claims in all the other ten hashing papers. Despite the\nextreme simplicity of random-projection-based LSH, our results show that the\ncapability of this algorithm has been far underestimated. For the sake of\nreproducibility, all the codes used in the paper are released on GitHub, which\ncan be used as a testing platform for a fair comparison between various hashing\nalgorithms.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Vector-Indexing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [12.206136703491211, 24.788103103637695], "cluster": 4}, {"key": "cai2017revisit", "year": "2017", "citations": "13", "title": "A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval", "abstract": "<p>There is a growing trend in studying deep hashing methods for content-based\nimage retrieval (CBIR), where hash functions and binary codes are learnt using\ndeep convolutional neural networks and then the binary codes can be used to do\napproximate nearest neighbor (ANN) search. All the existing deep hashing papers\nreport their methods\u2019 superior performance over the traditional hashing methods\naccording to their experimental results. However, there are serious flaws in\nthe evaluations of existing deep hashing papers: (1) The datasets they used are\ntoo small and simple to simulate the real CBIR situation. (2) They did not\ncorrectly include the search time in their evaluation criteria, while the\nsearch time is crucial in real CBIR systems. (3) The performance of some\nunsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses\nmultiple hash tables, which is an important factor should be considered in the\nevaluation while most of the deep hashing papers failed to do so.\n  We re-evaluate several state-of-the-art deep hashing methods with a carefully\ndesigned experimental setting. Empirical results reveal that the performance of\nthese deep hashing methods are inferior to multi-table IsoH, a very simple\nunsupervised hashing method. Thus, the conclusions in all the deep hashing\npapers should be carefully re-examined.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Compact-Codes", "Neural-Hashing", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [0.5436460971832275, 16.236608505249023], "cluster": 8}, {"key": "cai2024recall", "year": "2024", "citations": "0", "title": "Recall: Empowering Multimodal Embedding For Edge Devices", "abstract": "<p>Human memory is inherently prone to forgetting. To address this, multimodal\nembedding models have been introduced, which transform diverse real-world data\ninto a unified embedding space. These embeddings can be retrieved efficiently,\naiding mobile users in recalling past information. However, as model complexity\ngrows, so do its resource demands, leading to reduced throughput and heavy\ncomputational requirements that limit mobile device implementation. In this\npaper, we introduce RECALL, a novel on-device multimodal embedding system\noptimized for resource-limited mobile environments. RECALL achieves\nhigh-throughput, accurate retrieval by generating coarse-grained embeddings and\nleveraging query-based filtering for refined retrieval. Experimental results\ndemonstrate that RECALL delivers high-quality embeddings with superior\nthroughput, all while operating unobtrusively with minimal memory and energy\nconsumption.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [42.905799865722656, -22.515350341796875], "cluster": 9}, {"key": "cakaloglu2018text", "year": "2018", "citations": "6", "title": "Text Embeddings For Retrieval From A Large Knowledge Base", "abstract": "<p>Text embedding representing natural language documents in a semantic vector\nspace can be used for document retrieval using nearest neighbor lookup. In\norder to study the feasibility of neural models specialized for retrieval in a\nsemantically meaningful way, we suggest the use of the Stanford Question\nAnswering Dataset (SQuAD) in an open-domain question answering context, where\nthe first task is to find paragraphs useful for answering a given question.\nFirst, we compare the quality of various text-embedding methods on the\nperformance of retrieval and give an extensive empirical comparison on the\nperformance of various non-augmented base embedding with, and without IDF\nweighting. Our main results are that by training deep residual neural models,\nspecifically for retrieval purposes, can yield significant gains when it is\nused to augment existing embeddings. We also establish that deeper models are\nsuperior to this task. The best base baseline embeddings augmented by our\nlearned neural approach improves the top-1 paragraph recall of the system by\n14%.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-6.284581184387207, -28.752687454223633], "cluster": 3}, {"key": "cakir2015adaptive", "year": "2015", "citations": "88", "title": "Adaptive Hashing For Fast Similarity Search", "abstract": "<p>With the staggering growth in image and video datasets,\nalgorithms that provide fast similarity search and compact\nstorage are crucial. Hashing methods that map the\ndata into Hamming space have shown promise; however,\nmany of these methods employ a batch-learning strategy\nin which the computational cost and memory requirements\nmay become intractable and infeasible with larger and\nlarger datasets. To overcome these challenges, we propose\nan online learning algorithm based on stochastic gradient\ndescent in which the hash functions are updated iteratively\nwith streaming data. In experiments with three image retrieval\nbenchmarks, our online algorithm attains retrieval\naccuracy that is comparable to competing state-of-the-art\nbatch-learning solutions, while our formulation is orders\nof magnitude faster and being online it is adaptable to the\nvariations of the data. Moreover, our formulation yields improved\nretrieval performance over a recently reported online\nhashing technique, Online Kernel Hashing.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Similarity-Search", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [31.660511016845703, 0.9309489727020264], "cluster": 9}, {"key": "cakir2017mihash", "year": "2017", "citations": "99", "title": "Mihash: Online Hashing With Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for\nnearest neighbor retrieval, and recently, online hashing\nmethods have demonstrated good performance-complexity\ntrade-offs by learning hash functions from streaming data.\nIn this paper, we first address a key challenge for online\nhashing: the binary codes for indexed data must be recomputed\nto keep pace with updates to the hash functions.\nWe propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information,\nand use it successfully as a criterion to eliminate\nunnecessary hash table updates. Next, we also show how to\noptimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method,\nMIHash, that can be used in both online and batch settings.\nExperiments on image retrieval benchmarks (including a\n2.5M image dataset) confirm the effectiveness of our formulation,\nboth in reducing hash table recomputations and\nin learning high-quality hash functions.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [31.708786010742188, 3.0745933055877686], "cluster": 2}, {"key": "cakir2018hashing", "year": "2019", "citations": "91", "title": "Hashing With Mutual Information", "abstract": "<p>Binary vector embeddings enable fast nearest neighbor retrieval in large\ndatabases of high-dimensional objects, and play an important role in many\npractical applications, such as image and video retrieval. We study the problem\nof learning binary vector embeddings under a supervised setting, also known as\nhashing. We propose a novel supervised hashing method based on optimizing an\ninformation-theoretic quantity: mutual information. We show that optimizing\nmutual information can reduce ambiguity in the induced neighborhood structure\nin the learned Hamming space, which is essential in obtaining high retrieval\nperformance. To this end, we optimize mutual information in deep neural\nnetworks with minibatch stochastic gradient descent, with a formulation that\nmaximally and efficiently utilizes available supervision. Experiments on four\nimage retrieval benchmarks, including ImageNet, confirm the effectiveness of\nour method in learning high-quality binary embeddings for nearest neighbor\nretrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-6.375158309936523, -17.494943618774414], "cluster": 1}, {"key": "cakir2019hashing", "year": "2019", "citations": "91", "title": "Hashing With Binary Matrix Pursuit", "abstract": "<p>We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that, under mild assumptions, a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly, we show that with high-capacity hash functions such as CNNs, binary code inference can be greatly simplified for many standard neighborhood definitions, yielding smaller optimization problems and more robust codes. Incorporating our findings, we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-9.750833511352539, 23.97315216064453], "cluster": 8}, {"key": "cakir2025adaptive", "year": "2015", "citations": "88", "title": "Adaptive Hashing For Fast Similarity Search", "abstract": "<p>With the staggering growth in image and video datasets,\nalgorithms that provide fast similarity search and compact\nstorage are crucial. Hashing methods that map the\ndata into Hamming space have shown promise; however,\nmany of these methods employ a batch-learning strategy\nin which the computational cost and memory requirements\nmay become intractable and infeasible with larger and\nlarger datasets. To overcome these challenges, we propose\nan online learning algorithm based on stochastic gradient\ndescent in which the hash functions are updated iteratively\nwith streaming data. In experiments with three image retrieval\nbenchmarks, our online algorithm attains retrieval\naccuracy that is comparable to competing state-of-the-art\nbatch-learning solutions, while our formulation is orders\nof magnitude faster and being online it is adaptable to the\nvariations of the data. Moreover, our formulation yields improved\nretrieval performance over a recently reported online\nhashing technique, Online Kernel Hashing.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Similarity-Search", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [31.660381317138672, 0.9309507012367249], "cluster": 9}, {"key": "cakir2025hashing", "year": "2019", "citations": "91", "title": "Hashing With Mutual Information", "abstract": "<p>Binary vector embeddings enable fast nearest neighbor retrieval in large databases of high-dimensional objects, and play an important role in many practical applications, such as image and video retrieval. We study the problem of learning binary vector embeddings under a supervised setting, also known as hashing. We propose a novel supervised hashing method based on optimizing an information-theoretic quantity: mutual information. We show that optimizing mutual information can reduce ambiguity in the induced neighborhood structure in the learned Hamming space, which is essential in obtaining high retrieval performance. To this end, we optimize mutual information in deep neural networks with minibatch stochastic gradient descent, with a formulation that maximally and efficiently utilizes available supervision. Experiments on four image retrieval benchmarks, including ImageNet, confirm the effectiveness of our method in learning high-quality binary embeddings for nearest neighbor retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-6.375176906585693, -17.494932174682617], "cluster": 1}, {"key": "cakir2025mihash", "year": "2017", "citations": "99", "title": "Mihash: Online Hashing With Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for\nnearest neighbor retrieval, and recently, online hashing\nmethods have demonstrated good performance-complexity\ntrade-offs by learning hash functions from streaming data.\nIn this paper, we first address a key challenge for online\nhashing: the binary codes for indexed data must be recomputed\nto keep pace with updates to the hash functions.\nWe propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information,\nand use it successfully as a criterion to eliminate\nunnecessary hash table updates. Next, we also show how to\noptimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method,\nMIHash, that can be used in both online and batch settings.\nExperiments on image retrieval benchmarks (including a\n2.5M image dataset) confirm the effectiveness of our formulation,\nboth in reducing hash table recomputations and\nin learning high-quality hash functions.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [31.708786010742188, 3.0745933055877686], "cluster": 2}, {"key": "calixto2017multilingual", "year": "2017", "citations": "15", "title": "Multilingual Multi-modal Embeddings For Natural Language Processing", "abstract": "<p>We propose a novel discriminative model that learns embeddings from\nmultilingual and multi-modal data, meaning that our model can take advantage of\nimages and descriptions in multiple languages to improve embedding quality. To\nthat end, we introduce a modification of a pairwise contrastive estimation\noptimisation function as our training objective. We evaluate our embeddings on\nan image-sentence ranking (ISR), a semantic textual similarity (STS), and a\nneural machine translation (NMT) task. We find that the additional multilingual\nsignals lead to improvements on both the ISR and STS tasks, and the\ndiscriminative cost can also be used in re-ranking \\(n\\)-best lists produced by\nNMT models, yielding strong improvements.</p>\n", "tags": ["Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [-24.160255432128906, -26.747163772583008], "cluster": 5}, {"key": "camara2019spatio", "year": "2019", "citations": "25", "title": "Spatio-semantic Convnet-based Visual Place Recognition", "abstract": "<p>We present a Visual Place Recognition system that follows the two-stage\nformat common to image retrieval pipelines. The system encodes images of places\nby employing the activations of different layers of a pre-trained,\noff-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. In the\nfirst stage of our method and given a query image of a place, a number of top\ncandidate images is retrieved from a previously stored database of places. In\nthe second stage, we propose an exhaustive comparison of the query image\nagainst these candidates by encoding semantic and spatial information in the\nform of CNN features. Results from our approach outperform by a large margin\nstate-of-the-art visual place recognition methods on five of the most commonly\nused benchmark datasets. The performance gain is especially remarkable on the\nmost challenging datasets, with more than a twofold recognition improvement\nwith respect to the latest published work.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-51.860477447509766, -1.3015332221984863], "cluster": 0}, {"key": "can2019deep", "year": "2021", "citations": "4", "title": "Deep Metric Learning With Alternating Projections Onto Feasible Sets", "abstract": "<p>During the training of networks for distance metric learning, minimizers of\nthe typical loss functions can be considered as \u201cfeasible points\u201d satisfying a\nset of constraints imposed by the training data. To this end, we reformulate\ndistance metric learning problem as finding a feasible point of a constraint\nset where the embedding vectors of the training data satisfy desired\nintra-class and inter-class proximity. The feasible set induced by the\nconstraint set is expressed as the intersection of the relaxed feasible sets\nwhich enforce the proximity constraints only for particular samples (a sample\nfrom each class) of the training data. Then, the feasible point problem is to\nbe approximately solved by performing alternating projections onto those\nfeasible sets. Such an approach introduces a regularization term and results in\nminimizing a typical loss function with a systematic batch set construction\nwhere these batches are constrained to contain the same sample from each class\nfor a certain number of iterations. Moreover, these particular samples can be\nconsidered as the class representatives, allowing efficient utilization of hard\nclass mining during batch construction. The proposed technique is applied with\nthe well-accepted losses and evaluated on Stanford Online Products, CAR196 and\nCUB200-2011 datasets for image retrieval and clustering. Outperforming\nstate-of-the-art, the proposed approach consistently improves the performance\nof the integrated loss functions with no additional computational cost and\nboosts the performance further by hard negative class mining.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-10.622441291809082, -22.81300926208496], "cluster": 3}, {"key": "cao2016correlation", "year": "2016", "citations": "98", "title": "Correlation Autoencoder Hashing For Supervised Cross-modal Search", "abstract": "<p>Due to its storage and query efficiency, hashing has been widely\napplied to approximate nearest neighbor search from large-scale\ndatasets. While there is increasing interest in cross-modal hashing\nwhich facilitates cross-media retrieval by embedding data from different modalities into a common Hamming space, how to distill the\ncross-modal correlation structure effectively remains a challenging\nproblem. In this paper, we propose a novel supervised cross-modal\nhashing method, Correlation Autoencoder Hashing (CAH), to learn\ndiscriminative and compact binary codes based on deep autoencoders. Specifically, CAH jointly maximizes the feature correlation\nrevealed by bimodal data and the semantic correlation conveyed in\nsimilarity labels, while embeds them into hash codes by nonlinear\ndeep autoencoders. Extensive experiments clearly show the superior effectiveness and efficiency of CAH against the state-of-the-art\nhashing methods on standard cross-modal retrieval benchmarks.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Scalability", "Multimodal-Retrieval", "Datasets", "Supervised"], "tsne_embedding": [10.120100021362305, 6.846174240112305], "cluster": 6}, {"key": "cao2016deep", "year": "2016", "citations": "264", "title": "Deep Cauchy Hashing For Hamming Space Retrieval", "abstract": "<p>Due to its computation efficiency and retrieval quality,\nhashing has been widely applied to approximate nearest\nneighbor search for large-scale image retrieval, while deep\nhashing further improves the retrieval quality by end-toend representation learning and hash coding. With compact\nhash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a\ngiven Hamming radius to each query, by hash table lookups\ninstead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small\nHamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming\nspace retrieval.  This work presents Deep Cauchy Hashing\n(DCH), a novel deep hashing model that generates compact\nand concentrated binary hash codes to enable efficient and\neffective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs\nwith Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate\nthat DCH can generate highly concentrated hash codes and\nyield state-of-the-art Hamming space retrieval performance\non three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Image-Retrieval", "Datasets", "Evaluation", "KDD"], "tsne_embedding": [-5.842432975769043, 8.03560733795166], "cluster": 8}, {"key": "cao2016transitive", "year": "2017", "citations": "25", "title": "Transitive Hashing Network For Heterogeneous Multimedia Retrieval", "abstract": "<p>Hashing has been widely applied to large-scale multimedia retrieval due to\nthe storage and retrieval efficiency. Cross-modal hashing enables efficient\nretrieval from database of one modality in response to a query of another\nmodality. Existing work on cross-modal hashing assumes heterogeneous\nrelationship across modalities for hash function learning. In this paper, we\nrelax the strong assumption by only requiring such heterogeneous relationship\nin an auxiliary dataset different from the query/database domain. We craft a\nhybrid deep architecture to simultaneously learn the cross-modal correlation\nfrom the auxiliary dataset, and align the dataset distributions between the\nauxiliary dataset and the query/database domain, which generates transitive\nhash codes for heterogeneous multimedia retrieval. Extensive experiments\nexhibit that the proposed approach yields state of the art multimedia retrieval\nperformance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "AAAI", "Datasets", "Evaluation"], "tsne_embedding": [13.683847427368164, -35.462947845458984], "cluster": 7}, {"key": "cao2016where", "year": "2016", "citations": "19", "title": "Where To Focus: Query Adaptive Matching For Instance Retrieval Using Convolutional Feature Maps", "abstract": "<p>Instance retrieval requires one to search for images that contain a\nparticular object within a large corpus. Recent studies show that using image\nfeatures generated by pooling convolutional layer feature maps (CFMs) of a\npretrained convolutional neural network (CNN) leads to promising performance\nfor this task. However, due to the global pooling strategy adopted in those\nworks, the generated image feature is less robust to image clutter and tends to\nbe contaminated by the irrelevant image patterns. In this article, we alleviate\nthis drawback by proposing a novel reranking algorithm using CFMs to refine the\nretrieval result obtained by existing methods. Our key idea, called query\nadaptive matching (QAM), is to first represent the CFMs of each image by a set\nof base regions which can be freely combined into larger regions-of-interest.\nThen the similarity between the query and a candidate image is measured by the\nbest similarity score that can be attained by comparing the query feature and\nthe feature pooled from a combined region. We show that the above procedure can\nbe cast as an optimization problem and it can be solved efficiently with an\noff-the-shelf solver. Besides this general framework, we also propose two\npractical ways to create the base regions. One is based on the property of the\nCFM and the other one is based on a multi-scale spatial pyramid scheme. Through\nextensive experiments, we show that our reranking approaches bring substantial\nperformance improvement and by applying them we can outperform the state of the\nart on several instance retrieval benchmarks.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation"], "tsne_embedding": [-26.364652633666992, 9.999807357788086], "cluster": 0}, {"key": "cao2017collective", "year": "2017", "citations": "97", "title": "Collective Deep Quantization For Efficient Cross-modal Retrieval", "abstract": "<p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across\ncontent modalities, e.g., using an image to retrieve for\ntexts. This paper presents a compact coding solution for\nefficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior\nperformance over the hashing solutions in single-modal\nsimilarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to\nintroduce quantization in end-to-end deep architecture\nfor cross-modal retrieval. The major contribution lies in\njointly learning deep representations and the quantizers\nfor both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our\napproach simultaneously learns the common quantizer\ncodebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ\nenables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art\ncross-modal retrieval results on standard benchmarks.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Similarity-Search", "AAAI", "Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-3.8847341537475586, -20.255329132080078], "cluster": 1}, {"key": "cao2017hashnet", "year": "2017", "citations": "643", "title": "Hashnet: Deep Learning To Hash By Continuation", "abstract": "<p>Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality\nby end-to-end representation learning and hash encoding,\nhas received increasing attention recently. Subject to the illposed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first\nlearn continuous representations and then generate binary\nhash codes in a separated binarization step, which suffer\nfrom substantial loss of retrieval quality.  This work presents\nHashNet, a novel deep architecture for deep learning to\nhash by continuation method with convergence guarantees,\nwhich learns exactly binary hash codes from imbalanced\nsimilarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth\nbinary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it\neventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art\nmultimedia retrieval performance on standard benchmarks.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Evaluation"], "tsne_embedding": [16.67544174194336, -0.1938001960515976], "cluster": 6}, {"key": "cao2017transfer", "year": "2018", "citations": "15", "title": "Transfer Adversarial Hashing For Hamming Space Retrieval", "abstract": "<p>Hashing is widely applied to large-scale image retrieval due to the storage\nand retrieval efficiency. Existing work on deep hashing assumes that the\ndatabase in the target domain is identically distributed with the training set\nin the source domain. This paper relaxes this assumption to a transfer\nretrieval setting, which allows the database and the training set to come from\ndifferent but relevant domains. However, the transfer retrieval setting will\nintroduce two technical difficulties: first, the hash model trained on the\nsource domain cannot work well on the target domain due to the large\ndistribution gap; second, the domain gap makes it difficult to concentrate the\ndatabase points to be within a small Hamming ball. As a consequence, transfer\nretrieval performance within Hamming Radius 2 degrades significantly in\nexisting hashing methods. This paper presents Transfer Adversarial Hashing\n(TAH), a new hybrid deep architecture that incorporates a pairwise\n\\(t\\)-distribution cross-entropy loss to learn concentrated hash codes and an\nadversarial network to align the data distributions between the source and\ntarget domains. TAH can generate compact transfer hash codes for efficient\nimage retrieval on both source and target domains. Comprehensive experiments\nvalidate that TAH yields state of the art Hamming space retrieval performance\non standard datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Image-Retrieval", "Robustness", "AAAI", "Datasets", "Evaluation"], "tsne_embedding": [-2.129655599594116, 16.568540573120117], "cluster": 8}, {"key": "cao2018deep", "year": "2018", "citations": "33", "title": "Deep Priority Hashing", "abstract": "<p>Deep hashing enables image retrieval by end-to-end learning of deep\nrepresentations and hash codes from training data with pairwise similarity\ninformation. Subject to the distribution skewness underlying the similarity\ninformation, most existing deep hashing methods may underperform for imbalanced\ndata due to misspecified loss functions. This paper presents Deep Priority\nHashing (DPH), an end-to-end architecture that generates compact and balanced\nhash codes in a Bayesian learning framework. The main idea is to reshape the\nstandard cross-entropy loss for similarity-preserving learning such that it\ndown-weighs the loss associated to highly-confident pairs. This idea leads to a\nnovel priority cross-entropy loss, which prioritizes the training on uncertain\npairs over confident pairs. Also, we propose another priority quantization\nloss, which prioritizes hard-to-quantize examples for generation of nearly\nlossless hash codes. Extensive experiments demonstrate that DPH can generate\nhigh-quality hash codes and yield state-of-the-art image retrieval results on\nthree datasets, ImageNet, NUS-WIDE, and MS-COCO.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Neural-Hashing"], "tsne_embedding": [-7.768232822418213, 8.608345985412598], "cluster": 8}, {"key": "cao2018end", "year": "2019", "citations": "65", "title": "End-to-end Latent Fingerprint Search", "abstract": "<p>Latent fingerprints are one of the most important and widely used sources of\nevidence in law enforcement and forensic agencies. Yet the performance of the\nstate-of-the-art latent recognition systems is far from satisfactory, and they\noften require manual markups to boost the latent search performance. Further,\nthe COTS systems are proprietary and do not output the true comparison scores\nbetween a latent and reference prints to conduct quantitative evidential\nanalysis. We present an end-to-end latent fingerprint search system, including\nautomated region of interest (ROI) cropping, latent image preprocessing,\nfeature extraction, feature comparison , and outputs a candidate list. Two\nseparate minutiae extraction models provide complementary minutiae templates.\nTo compensate for the small number of minutiae in small area and poor quality\nlatents, a virtual minutiae set is generated to construct a texture template. A\n96-dimensional descriptor is extracted for each minutia from its neighborhood.\nFor computational efficiency, the descriptor length for virtual minutiae is\nfurther reduced to 16 using product quantization. Our end-to-end system is\nevaluated on three latent databases: NIST SD27 (258 latents); MSP (1,200\nlatents), WVU (449 latents) and N2N (10,000 latents) against a background set\nof 100K rolled prints, which includes the true rolled mates of the latents with\nrank-1 retrieval rates of 65.7%, 69.4%, 65.5%, and 7.6% respectively. A\nmulti-core solution implemented on 24 cores obtains 1ms per latent to rolled\ncomparison.</p>\n", "tags": ["Efficiency", "Quantization", "Evaluation"], "tsne_embedding": [-24.96843147277832, 15.116283416748047], "cluster": 8}, {"key": "cao2018hashgan", "year": "2018", "citations": "117", "title": "Hashgan: Deep Learning To Hash With Pair Conditional Wasserstein GAN", "abstract": "<p>Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information.\nSubject to the scarcity of similarity information that is often\nexpensive to collect for many application domains, existing\ndeep learning to hash methods may overfit the training data\nand result in substantial loss of retrieval quality. This paper\npresents HashGAN, a novel architecture for deep learning\nto hash, which learns compact binary hash codes from both\nreal images and diverse images synthesized by generative\nmodels. The main idea is to augment the training data with\nnearly real images synthesized from a new Pair Conditional\nWasserstein GAN (PC-WGAN) conditioned on the pairwise\nsimilarity information. Extensive experiments demonstrate\nthat HashGAN can generate high-quality binary hash codes\nand yield state-of-the-art image retrieval performance on\nthree benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["CVPR", "Evaluation", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-7.183071136474609, 9.146236419677734], "cluster": 8}, {"key": "cao2019enhancing", "year": "2019", "citations": "94", "title": "Enhancing Remote Sensing Image Retrieval With Triplet Deep Metric Learning Network", "abstract": "<p>With the rapid growing of remotely sensed imagery data, there is a high\ndemand for effective and efficient image retrieval tools to manage and exploit\nsuch data. In this letter, we present a novel content-based remote sensing\nimage retrieval method based on Triplet deep metric learning convolutional\nneural network (CNN). By constructing a Triplet network with metric learning\nobjective function, we extract the representative features of the images in a\nsemantic space in which images from the same class are close to each other\nwhile those from different classes are far apart. In such a semantic space,\nsimple metric measures such as Euclidean distance can be used directly to\ncompare the similarity of images and effectively retrieve images of the same\nclass. We also investigate a supervised and an unsupervised learning methods\nfor reducing the dimensionality of the learned semantic features. We present\ncomprehensive experimental results on two publicly available remote sensing\nimage retrieval datasets and show that our method significantly outperforms\nstate-of-the-art.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [-44.617618560791016, -4.246984481811523], "cluster": 0}, {"key": "cao2019image", "year": "2018", "citations": "4", "title": "Image Retrieval Method Based On CNN And Dimension Reduction", "abstract": "<p>An image retrieval method based on convolution neural network and dimension\nreduction is proposed in this paper. Convolution neural network is used to\nextract high-level features of images, and to solve the problem that the\nextracted feature dimensions are too high and have strong correlation,\nmultilinear principal component analysis is used to reduce the dimension of\nfeatures. The features after dimension reduction are binary hash coded for fast\nimage retrieval. Experiments show that the method proposed in this paper has\nbetter retrieval effect than the retrieval method based on principal component\nanalysis on the e-commerce image datasets.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Datasets"], "tsne_embedding": [-22.21862030029297, 7.558241844177246], "cluster": 1}, {"key": "cao2019unsupervised", "year": "2019", "citations": "12", "title": "Unsupervised Deep Metric Learning Via Auxiliary Rotation Loss", "abstract": "<p>Deep metric learning is an important area due to its applicability to many\ndomains such as image retrieval and person re-identification. The main drawback\nof such models is the necessity for labeled data. In this work, we propose to\ngenerate pseudo-labels for deep metric learning directly from clustering\nassignment and we introduce unsupervised deep metric learning (UDML)\nregularized by a self-supervision (SS) task. In particular, we propose to\nregularize the training process by predicting image rotations. Our method\n(UDML-SS) jointly learns discriminative embeddings, unsupervised clustering\nassignments of the embeddings, as well as a self-supervised pretext task.\nUDML-SS iteratively cluster embeddings using traditional clustering algorithm\n(e.g., k-means), and sampling training pairs based on the cluster assignment\nfor metric learning, while optimizing self-supervised pretext task in a\nmulti-task fashion. The role of self-supervision is to stabilize the training\nprocess and encourages the model to learn meaningful feature representations\nthat are not distorted due to unreliable clustering assignments. The proposed\nmethod performs well on standard benchmarks for metric learning, where it\noutperforms current state-of-the-art approaches by a large margin and it also\nshows competitive performance with various metric learning loss functions.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Image-Retrieval", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-8.881752014160156, -21.10401725769043], "cluster": 1}, {"key": "cao2020unifying", "year": "2020", "citations": "293", "title": "Unifying Deep Local And Global Features For Image Search", "abstract": "<p>Image retrieval is the problem of searching an image database for items that\nare similar to a query image. To address this task, two main types of image\nrepresentations have been studied: global and local image features. In this\nwork, our key contribution is to unify global and local features into a single\ndeep model, enabling accurate retrieval with efficient feature extraction. We\nrefer to the new model as DELG, standing for DEep Local and Global features. We\nleverage lessons from recent feature learning work and propose a model that\ncombines generalized mean pooling for global features and attentive selection\nfor local features. The entire network can be learned end-to-end by carefully\nbalancing the gradient flow between two heads \u2013 requiring only image-level\nlabels. We also introduce an autoencoder-based dimensionality reduction\ntechnique for local features, which is integrated into the model, improving\ntraining efficiency and matching performance. Comprehensive experiments show\nthat our model achieves state-of-the-art image retrieval on the Revisited\nOxford and Paris datasets, and state-of-the-art single-model instance-level\nrecognition on the Google Landmarks dataset v2. Code and models are available\nat https://github.com/tensorflow/models/tree/master/research/delf .</p>\n", "tags": ["Efficiency", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-28.826370239257812, -4.163826942443848], "cluster": 1}, {"key": "cao2022context", "year": "2024", "citations": "14", "title": "Context Recovery And Knowledge Retrieval: A Novel Two-stream Framework For Video Anomaly Detection", "abstract": "<p>Video anomaly detection aims to find the events in a video that do not\nconform to the expected behavior. The prevalent methods mainly detect anomalies\nby snippet reconstruction or future frame prediction error. However, the error\nis highly dependent on the local context of the current snippet and lacks the\nunderstanding of normality. To address this issue, we propose to detect\nanomalous events not only by the local context, but also according to the\nconsistency between the testing event and the knowledge about normality from\nthe training data. Concretely, we propose a novel two-stream framework based on\ncontext recovery and knowledge retrieval, where the two streams can complement\neach other. For the context recovery stream, we propose a spatiotemporal U-Net\nwhich can fully utilize the motion information to predict the future frame.\nFurthermore, we propose a maximum local error mechanism to alleviate the\nproblem of large recovery errors caused by complex foreground objects. For the\nknowledge retrieval stream, we propose an improved learnable locality-sensitive\nhashing, which optimizes hash functions via a Siamese network and a mutual\ndifference loss. The knowledge about normality is encoded and stored in hash\ntables, and the distance between the testing event and the knowledge\nrepresentation is used to reveal the probability of anomaly. Finally, we fuse\nthe anomaly scores from the two streams to detect anomalies. Extensive\nexperiments demonstrate the effectiveness and complementarity of the two\nstreams, whereby the proposed two-stream framework achieves state-of-the-art\nperformance on four datasets.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [-12.451906204223633, 2.8585832118988037], "cluster": 1}, {"key": "cao2025collective", "year": "2017", "citations": "97", "title": "Collective Deep Quantization For Efficient Cross-modal Retrieval", "abstract": "<p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across\ncontent modalities, e.g., using an image to retrieve for\ntexts. This paper presents a compact coding solution for\nefficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior\nperformance over the hashing solutions in single-modal\nsimilarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to\nintroduce quantization in end-to-end deep architecture\nfor cross-modal retrieval. The major contribution lies in\njointly learning deep representations and the quantizers\nfor both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our\napproach simultaneously learns the common quantizer\ncodebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ\nenables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art\ncross-modal retrieval results on standard benchmarks.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Similarity-Search", "AAAI", "Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-3.8847341537475586, -20.255329132080078], "cluster": 1}, {"key": "cao2025correlation", "year": "2016", "citations": "98", "title": "Correlation Autoencoder Hashing For Supervised Cross-modal Search", "abstract": "<p>Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors, and then generate binary codes by another separate quantization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes.\nThis paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perception for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Quantization", "Scalability", "Multimodal-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [-4.774612903594971, 6.194045543670654], "cluster": 1}, {"key": "cao2025deep", "year": "2016", "citations": "264", "title": "Deep Visual-semantic Hashing For Cross-modal Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been\nwidely applied to approximate nearest neighbor search for\nlarge-scale multimedia retrieval. Cross-modal hashing, which\nenables efficient retrieval of images in response to text queries\nor vice versa, has received increasing attention recently. Most\nexisting work on cross-modal hashing does not capture the\nspatial dependency of images and temporal dynamics of text\nsentences for learning powerful feature representations and\ncross-modal embeddings that mitigate the heterogeneity of\ndifferent modalities. This paper presents a new Deep Visual Semantic Hashing (DVSH) model that generates compact\nhash codes of images and sentences in an end-to-end deep\nlearning architecture, which capture the intrinsic cross-modal\ncorrespondences between visual data and natural language.\nDVSH is a hybrid deep architecture that constitutes a visual semantic fusion network for learning joint embedding space\nof images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact\nbinary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based\non a novel combination of Convolutional Neural Networks\nover images, Recurrent Neural Networks over sentences, and\na structured max-margin objective that integrates all things\ntogether to enable learning of similarity-preserving and highquality hash codes. Extensive empirical evidence shows that\nour DVSH approach yields state of the art results in crossmodal retrieval experiments on image-sentences datasets,\ni.e. standard IAPR TC-12 and large-scale Microsoft COCO.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Efficiency", "Scalability", "Similarity-Search", "Multimodal-Retrieval", "Datasets", "Compact-Codes", "KDD"], "tsne_embedding": [-4.240342140197754, 4.936724662780762], "cluster": 1}, {"key": "cao2025hashgan", "year": "2018", "citations": "117", "title": "Hashgan: Deep Learning To Hash With Pair Conditional Wasserstein GAN", "abstract": "<p>Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information.\nSubject to the scarcity of similarity information that is often\nexpensive to collect for many application domains, existing\ndeep learning to hash methods may overfit the training data\nand result in substantial loss of retrieval quality. This paper\npresents HashGAN, a novel architecture for deep learning\nto hash, which learns compact binary hash codes from both\nreal images and diverse images synthesized by generative\nmodels. The main idea is to augment the training data with\nnearly real images synthesized from a new Pair Conditional\nWasserstein GAN (PC-WGAN) conditioned on the pairwise\nsimilarity information. Extensive experiments demonstrate\nthat HashGAN can generate high-quality binary hash codes\nand yield state-of-the-art image retrieval performance on\nthree benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["CVPR", "Evaluation", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-7.183089256286621, 9.146251678466797], "cluster": 8}, {"key": "cao2025hashnet", "year": "2017", "citations": "643", "title": "Hashnet: Deep Learning To Hash By Continuation", "abstract": "<p>Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality\nby end-to-end representation learning and hash encoding,\nhas received increasing attention recently. Subject to the illposed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first\nlearn continuous representations and then generate binary\nhash codes in a separated binarization step, which suffer\nfrom substantial loss of retrieval quality.  This work presents\nHashNet, a novel deep architecture for deep learning to\nhash by continuation method with convergence guarantees,\nwhich learns exactly binary hash codes from imbalanced\nsimilarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth\nbinary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it\neventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art\nmultimedia retrieval performance on standard benchmarks.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Evaluation"], "tsne_embedding": [16.677183151245117, -0.19384297728538513], "cluster": 6}, {"key": "caron2024generative", "year": "2024", "citations": "1", "title": "A Generative Approach For Wikipedia-scale Visual Entity Recognition", "abstract": "<p>In this paper, we address web-scale visual entity recognition, specifically\nthe task of mapping a given query image to one of the 6 million existing\nentities in Wikipedia. One way of approaching a problem of such scale is using\ndual-encoder models (eg CLIP), where all the entity names and query images are\nembedded into a unified space, paving the way for an approximate k-NN search.\nAlternatively, it is also possible to re-purpose a captioning model to directly\ngenerate the entity names for a given image. In contrast, we introduce a novel\nGenerative Entity Recognition (GER) framework, which given an input image\nlearns to auto-regressively decode a semantic and discriminative ``code\u2019\u2019\nidentifying the target entity. Our experiments demonstrate the efficacy of this\nGER paradigm, showcasing state-of-the-art performance on the challenging OVEN\nbenchmark. GER surpasses strong captioning, dual-encoder, visual matching and\nhierarchical classification baselines, affirming its advantage in tackling the\ncomplexities of web-scale recognition.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Large-Scale-Search", "Scalability"], "tsne_embedding": [21.784955978393555, -16.695547103881836], "cluster": 7}, {"key": "carreiraperpinan2015hashing", "year": "2015", "citations": "142", "title": "Hashing With Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image\ndatabases is binary hashing, where each high-dimensional,\nreal-valued image is mapped onto a low-dimensional, binary\nvector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves\nbinary constraints, and most approaches approximate\nthe optimization by relaxing the constraints and then\nbinarizing the result. Here, we focus on the binary autoencoder\nmodel, which seeks to reconstruct an image from the\nbinary code produced by the hash function. We show that\nthe optimization can be simplified with the method of auxiliary\ncoordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder\nand decoder separately, and one that optimizes the code for\neach image. Image retrieval experiments show the resulting\nhash function outperforms or is competitive with state-ofthe-art\nmethods for binary hashing.</p>\n", "tags": ["Compact-Codes", "CVPR", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-10.937865257263184, 20.125473022460938], "cluster": 8}, {"key": "carreiraperpinan2025hashing", "year": "2015", "citations": "142", "title": "Hashing With Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image\ndatabases is binary hashing, where each high-dimensional,\nreal-valued image is mapped onto a low-dimensional, binary\nvector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves\nbinary constraints, and most approaches approximate\nthe optimization by relaxing the constraints and then\nbinarizing the result. Here, we focus on the binary autoencoder\nmodel, which seeks to reconstruct an image from the\nbinary code produced by the hash function. We show that\nthe optimization can be simplified with the method of auxiliary\ncoordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder\nand decoder separately, and one that optimizes the code for\neach image. Image retrieval experiments show the resulting\nhash function outperforms or is competitive with state-ofthe-art\nmethods for binary hashing.</p>\n", "tags": ["Compact-Codes", "CVPR", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-10.937865257263184, 20.125473022460938], "cluster": 8}, {"key": "carreiraperpi\u00f1\u00e1n2016ensemble", "year": "2016", "citations": "8", "title": "An Ensemble Diversity Approach To Supervised Binary Hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [-9.081851959228516, 20.006664276123047], "cluster": 8}, {"key": "carvalho2018cross", "year": "2018", "citations": "39", "title": "Cross-modal Retrieval In The Cooking Context: Learning Semantic Text-image Embeddings", "abstract": "<p>Designing powerful tools that support cooking activities has rapidly gained\npopularity due to the massive amounts of available data, as well as recent\nadvances in machine learning that are capable of analyzing them. In this paper,\nwe propose a cross-modal retrieval model aligning visual and textual data (like\npictures of dishes and their recipes) in a shared representation space. We\ndescribe an effective learning scheme, capable of tackling large-scale\nproblems, and validate it on the Recipe1M dataset containing nearly 1 million\npicture-recipe pairs. We show the effectiveness of our approach regarding\nprevious state-of-the-art models and present qualitative results over\ncomputational cooking use cases.</p>\n", "tags": ["Multimodal-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [-43.64115524291992, 31.804790496826172], "cluster": 0}, {"key": "carvalho2018images", "year": "2018", "citations": "4", "title": "Images & Recipes: Retrieval In The Cooking Context", "abstract": "<p>Recent advances in the machine learning community allowed different use cases\nto emerge, as its association to domains like cooking which created the\ncomputational cuisine. In this paper, we tackle the picture-recipe alignment\nproblem, having as target application the large-scale retrieval task (finding a\nrecipe given a picture, and vice versa). Our approach is validated on the\nRecipe1M dataset, composed of one million image-recipe pairs and additional\nclass information, for which we achieve state-of-the-art results.</p>\n", "tags": ["Scalability", "Datasets"], "tsne_embedding": [-43.50374221801758, 31.946866989135742], "cluster": 0}, {"key": "carvalho2023self", "year": "2023", "citations": "2", "title": "Self-supervised Contrastive Learning For Robust Audio-sheet Music Retrieval Systems", "abstract": "<p>Linking sheet music images to audio recordings remains a key problem for the\ndevelopment of efficient cross-modal music retrieval systems. One of the\nfundamental approaches toward this task is to learn a cross-modal embedding\nspace via deep neural networks that is able to connect short snippets of audio\nand sheet music. However, the scarcity of annotated data from real musical\ncontent affects the capability of such methods to generalize to real retrieval\nscenarios. In this work, we investigate whether we can mitigate this limitation\nwith self-supervised contrastive learning, by exposing a network to a large\namount of real music data as a pre-training step, by contrasting randomly\naugmented views of snippets of both modalities, namely audio and sheet images.\nThrough a number of experiments on synthetic and real piano data, we show that\npre-trained models are able to retrieve snippets with better precision in all\nscenarios and pre-training configurations. Encouraged by these results, we\nemploy the snippet embeddings in the higher-level task of cross-modal piece\nidentification and conduct more experiments on several retrieval\nconfigurations. In this task, we observe that the retrieval quality improves\nfrom 30% up to 100% when real music data is present. We then conclude by\narguing for the potential of self-supervised contrastive learning for\nalleviating the annotated data scarcity in multi-modal music retrieval models.</p>\n", "tags": ["Supervised", "Self-Supervised", "Evaluation"], "tsne_embedding": [9.79991626739502, -44.1107177734375], "cluster": 3}, {"key": "caspari2024beyond", "year": "2024", "citations": "1", "title": "Beyond Benchmarks: Evaluating Embedding Model Similarity For Retrieval Augmented Generation Systems", "abstract": "<p>The choice of embedding model is a crucial step in the design of Retrieval\nAugmented Generation (RAG) systems. Given the sheer volume of available\noptions, identifying clusters of similar models streamlines this model\nselection process. Relying solely on benchmark performance scores only allows\nfor a weak assessment of model similarity. Thus, in this study, we evaluate the\nsimilarity of embedding models within the context of RAG systems. Our\nassessment is two-fold: We use Centered Kernel Alignment to compare embeddings\non a pair-wise level. Additionally, as it is especially pertinent to RAG\nsystems, we evaluate the similarity of retrieval results between these models\nusing Jaccard and rank similarity. We compare different families of embedding\nmodels, including proprietary ones, across five datasets from the popular\nBenchmark Information Retrieval (BEIR). Through our experiments we identify\nclusters of models corresponding to model families, but interestingly, also\nsome inter-family clusters. Furthermore, our analysis of top-k retrieval\nsimilarity reveals high-variance at low k values. We also identify possible\nopen-source alternatives to proprietary models, with Mistral exhibiting the\nhighest similarity to OpenAI models.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [3.597686529159546, -4.088597297668457], "cluster": 6}, {"key": "castellano2020visual", "year": "2020", "citations": "40", "title": "Visual Link Retrieval And Knowledge Discovery In Painting Datasets", "abstract": "<p>Visual arts are of inestimable importance for the cultural, historic and\neconomic growth of our society. One of the building blocks of most analysis in\nvisual arts is to find similarity relationships among paintings of different\nartists and painting schools. To help art historians better understand visual\narts, this paper presents a framework for visual link retrieval and knowledge\ndiscovery in digital painting datasets. Visual link retrieval is accomplished\nby using a deep convolutional neural network to perform feature extraction and\na fully unsupervised nearest neighbor mechanism to retrieve links among\ndigitized paintings. Historical knowledge discovery is achieved by performing a\ngraph analysis that makes it possible to study influences among artists. An\nexperimental evaluation on a database collecting paintings by very popular\nartists shows the effectiveness of the method. The unsupervised strategy makes\nthe method interesting especially in cases where metadata are scarce,\nunavailable or difficult to collect.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Unsupervised", "Datasets"], "tsne_embedding": [-4.24312686920166, -48.55497360229492], "cluster": 3}, {"key": "castrejon2016learning", "year": "2016", "citations": "199", "title": "Learning Aligned Cross-modal Representations From Weakly Aligned Data", "abstract": "<p>People can recognize scenes across many different modalities beyond natural\nimages. In this paper, we investigate how to learn cross-modal scene\nrepresentations that transfer across modalities. To study this problem, we\nintroduce a new cross-modal scene dataset. While convolutional neural networks\ncan categorize cross-modal scenes well, they also learn an intermediate\nrepresentation not aligned across modalities, which is undesirable for\ncross-modal transfer applications. We present methods to regularize cross-modal\nconvolutional neural networks so that they have a shared representation that is\nagnostic of the modality. Our experiments suggest that our scene representation\ncan help transfer representations across modalities for retrieval. Moreover,\nour visualizations suggest that units emerge in the shared representation that\ntend to activate on consistent concepts independently of the modality.</p>\n", "tags": ["CVPR", "Datasets"], "tsne_embedding": [-33.12117004394531, -38.24272537231445], "cluster": 5}, {"key": "ceccarello2018fresh", "year": "2018", "citations": "3", "title": "FRESH: Fr\\'echet Similarity With Hashing", "abstract": "<p>This paper studies the \\(r\\)-range search problem for curves under the\ncontinuous Fr'echet distance: given a dataset \\(S\\) of \\(n\\) polygonal curves and\na threshold \\(r&gt;0\\), construct a data structure that, for any query curve \\(q\\),\nefficiently returns all entries in \\(S\\) with distance at most \\(r\\) from \\(q\\). We\npropose FRESH, an approximate and randomized approach for \\(r\\)-range search,\nthat leverages on a locality sensitive hashing scheme for detecting candidate\nnear neighbors of the query curve, and on a subsequent pruning step based on a\ncascade of curve simplifications. We experimentally compare \\fresh to exact and\ndeterministic solutions, and we show that high performance can be reached by\nsuitably relaxing precision and recall.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [23.710657119750977, 43.07137680053711], "cluster": 4}, {"key": "chadha2016voronoi", "year": "2017", "citations": "29", "title": "Voronoi-based Compact Image Descriptors: Efficient Region-of-interest Retrieval With VLAD And Deep-learning-based Descriptors", "abstract": "<p>We investigate the problem of image retrieval based on visual queries when\nthe latter comprise arbitrary regions-of-interest (ROI) rather than entire\nimages. Our proposal is a compact image descriptor that combines the\nstate-of-the-art in content-based descriptor extraction with a multi-level,\nVoronoi-based spatial partitioning of each dataset image. The proposed\nmulti-level Voronoi-based encoding uses a spatial hierarchical K-means over\ninterest-point locations, and computes a content-based descriptor over each\ncell. In order to reduce the matching complexity with minimal or no sacrifice\nin retrieval performance: (i) we utilize the tree structure of the spatial\nhierarchical K-means to perform a top-to-bottom pruning for local similarity\nmaxima; (ii) we propose a new image similarity score that combines relevant\ninformation from all partition levels into a single measure for similarity;\n(iii) we combine our proposal with a novel and efficient approach for optimal\nbit allocation within quantized descriptor representations. By deriving both a\nVoronoi-based VLAD descriptor (termed as Fast-VVLAD) and a Voronoi-based deep\nconvolutional neural network (CNN) descriptor (termed as Fast-VDCNN), we\ndemonstrate that our Voronoi-based framework is agnostic to the descriptor\nbasis, and can easily be slotted into existing frameworks. Via a range of ROI\nqueries in two standard datasets, it is shown that the Voronoi-based\ndescriptors achieve comparable or higher mean Average Precision against\nconventional grid-based spatial search, while offering more than two-fold\nreduction in complexity. Finally, beyond ROI queries, we show that Voronoi\npartitioning improves the geometric invariance of compact CNN descriptors,\nthereby resulting in competitive performance to the current state-of-the-art on\nwhole image retrieval.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-26.727256774902344, 10.951568603515625], "cluster": 0}, {"key": "chaidaroon2017variational", "year": "2017", "citations": "71", "title": "Variational Deep Semantic Hashing For Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over\nthe past decade, efficient similarity search methods have become\na crucial component of large-scale information retrieval systems.\nA popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The\nrecent advances of deep learning in a wide range of applications\nhas demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative\nmodels naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,\nwhich is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Compact-Codes", "Scalability", "Similarity-Search", "SIGIR", "Tools-&-Libraries", "Supervised", "Unsupervised"], "tsne_embedding": [16.80988883972168, -15.159482955932617], "cluster": 7}, {"key": "chaidaroon2018deep", "year": "2018", "citations": "29", "title": "Deep Semantic Text Hashing With Weak Supervision", "abstract": "<p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Similarity-Search", "SIGIR", "Datasets", "Compact-Codes", "Unsupervised"], "tsne_embedding": [20.67222785949707, -12.688721656799316], "cluster": 7}, {"key": "chaidaroon2025deep", "year": "2018", "citations": "29", "title": "Deep Semantic Text Hashing With Weak Supervision", "abstract": "<p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Similarity-Search", "SIGIR", "Datasets", "Compact-Codes", "Unsupervised"], "tsne_embedding": [20.672222137451172, -12.688774108886719], "cluster": 7}, {"key": "chaidaroon2025variational", "year": "2017", "citations": "71", "title": "Variational Deep Semantic Hashing For Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over\nthe past decade, efficient similarity search methods have become\na crucial component of large-scale information retrieval systems.\nA popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The\nrecent advances of deep learning in a wide range of applications\nhas demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative\nmodels naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,\nwhich is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Compact-Codes", "Scalability", "Similarity-Search", "SIGIR", "Tools-&-Libraries", "Supervised", "Unsupervised"], "tsne_embedding": [16.80937385559082, -15.15922737121582], "cluster": 7}, {"key": "chakrabarti2020efficient", "year": "2020", "citations": "3", "title": "Efficient Image Retrieval Using Multi Neural Hash Codes And Bloom Filters", "abstract": "<p>This paper aims to deliver an efficient and modified approach for image\nretrieval using multiple neural hash codes and limiting the number of queries\nusing bloom filters by identifying false positives beforehand. Traditional\napproaches involving neural networks for image retrieval tasks tend to use\nhigher layers for feature extraction. But it has been seen that the activations\nof lower layers have proven to be more effective in a number of scenarios. In\nour approach, we have leveraged the use of local deep convolutional neural\nnetworks which combines the powers of both the features of lower and higher\nlayers for creating feature maps which are then compressed using PCA and fed to\na bloom filter after binary sequencing using a modified multi k-means approach.\nThe feature maps obtained are further used in the image retrieval process in a\nhierarchical coarse-to-fine manner by first comparing the images in the higher\nlayers for semantically similar images and then gradually moving towards the\nlower layers searching for structural similarities. While searching, the neural\nhashes for the query image are again calculated and queried in the bloom filter\nwhich tells us whether the query image is absent in the set or maybe present.\nIf the bloom filter doesn\u2019t necessarily rule out the query, then it goes into\nthe image retrieval process. This approach can be particularly helpful in cases\nwhere the image store is distributed since the approach supports parallel\nquerying.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-22.99633026123047, 7.678854942321777], "cluster": 1}, {"key": "chakraborty2017improved", "year": "2017", "citations": "0", "title": "An Improved Video Analysis Using Context Based Extension Of LSH", "abstract": "<p>Locality Sensitive Hashing (LSH) based algorithms have already shown their\npromise in finding approximate nearest neighbors in high dimen- sional data\nspace. However, there are certain scenarios, as in sequential data, where the\nproximity of a pair of points cannot be captured without considering their\nsurroundings or context. In videos, as for example, a particular frame is\nmeaningful only when it is seen in the context of its preceding and following\nframes. LSH has no mechanism to handle the con- texts of the data points. In\nthis article, a novel scheme of Context based Locality Sensitive Hashing\n(conLSH) has been introduced, in which points are hashed together not only\nbased on their closeness, but also because of similar context. The contribution\nmade in this article is three fold. First, conLSH is integrated with a recently\nproposed fast optimal sequence alignment algorithm (FOGSAA) using a layered\napproach. The resultant method is applied to video retrieval for extracting\nsimilar sequences. The pro- posed algorithm yields more than 80% accuracy on an\naverage in different datasets. It has been found to save 36.3% of the total\ntime, consumed by the exhaustive search. conLSH reduces the search space to\napproximately 42% of the entire dataset, when compared with an exhaustive\nsearch by the aforementioned FOGSAA, Bag of Words method and the standard LSH\nimplementations. Secondly, the effectiveness of conLSH is demon- strated in\naction recognition of the video clips, which yields an average gain of 12.83%\nin terms of classification accuracy over the state of the art methods using\nSTIP descriptors. The last but of great significance is that this article\nprovides a way of automatically annotating long and composite real life videos.\nThe source code of conLSH is made available at\nhttp://www.isical.ac.in/~bioinfo_miu/conLSH/conLSH.html</p>\n", "tags": ["Locality-Sensitive-Hashing", "Hashing-Methods", "Video-Retrieval", "Datasets"], "tsne_embedding": [6.730353355407715, 11.84488296508789], "cluster": 6}, {"key": "chakraborty2019conlsh", "year": "2020", "citations": "13", "title": "Conlsh: Context Based Locality Sensitive Hashing For Mapping Of Noisy SMRT Reads", "abstract": "<p>Single Molecule Real-Time (SMRT) sequencing is a recent advancement of Next\nGen technology developed by Pacific Bio (PacBio). It comes with an explosion of\nlong and noisy reads demanding cutting edge research to get most out of it. To\ndeal with the high error probability of SMRT data, a novel contextual Locality\nSensitive Hashing (conLSH) based algorithm is proposed in this article, which\ncan effectively align the noisy SMRT reads to the reference genome. Here,\nsequences are hashed together based not only on their closeness, but also on\nsimilarity of context. The algorithm has \\(\\mathcal{O}(n^{\\rho+1})\\) space\nrequirement, where \\(n\\) is the number of sequences in the corpus and \\(\\rho\\) is a\nconstant. The indexing time and querying time are bounded by \\(\\mathcal{O}(\n\\frac{n^{\\rho+1} \\cdot \\ln n}{\\ln \\frac{1}{P_2}})\\) and \\(\\mathcal{O}(n^\\rho)\\)\nrespectively, where \\(P_2 &gt; 0\\), is a probability value. This algorithm is\nparticularly useful for retrieving similar sequences, a widely used task in\nbiology. The proposed conLSH based aligner is compared with rHAT, popularly\nused for aligning SMRT reads, and is found to comprehensively beat it in speed\nas well as in memory requirements. In particular, it takes approximately\n\\(24.2%\\) less processing time, while saving about \\(70.3%\\) in peak memory\nrequirement for H.sapiens PacBio dataset.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [4.64623498916626, 52.11080551147461], "cluster": 4}, {"key": "chandrasekaran2017lattice", "year": "2017", "citations": "1", "title": "Lattice-based Locality Sensitive Hashing Is Optimal", "abstract": "<p>Locality sensitive hashing (LSH) was introduced by Indyk and Motwani (STOC\n<code class=\"language-plaintext highlighter-rouge\">98) to give the first sublinear time algorithm for the c-approximate nearest\nneighbor (ANN) problem using only polynomial space. At a high level, an LSH\nfamily hashes \"nearby\" points to the same bucket and \"far away\" points to\ndifferent buckets. The quality of measure of an LSH family is its LSH exponent,\nwhich helps determine both query time and space usage.\n  In a seminal work, Andoni and Indyk (FOCS </code>06) constructed an LSH family\nbased on random ball partitioning of space that achieves an LSH exponent of\n1/c^2 for the l_2 norm, which was later shown to be optimal by Motwani, Naor\nand Panigrahy (SIDMA <code class=\"language-plaintext highlighter-rouge\">07) and O'Donnell, Wu and Zhou (TOCT </code>14). Although\noptimal in the LSH exponent, the ball partitioning approach is computationally\nexpensive. So, in the same work, Andoni and Indyk proposed a simpler and more\npractical hashing scheme based on Euclidean lattices and provided computational\nresults using the 24-dimensional Leech lattice. However, no theoretical\nanalysis of the scheme was given, thus leaving open the question of finding the\nexponent of lattice based LSH.\n  In this work, we resolve this question by showing the existence of lattices\nachieving the optimal LSH exponent of 1/c^2 using techniques from the geometry\nof numbers. At a more conceptual level, our results show that optimal LSH space\npartitions can have periodic structure. Understanding the extent to which\nadditional structure can be imposed on these partitions, e.g. to yield low\nspace and query complexity, remains an important open problem.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [24.83052635192871, 47.00957489013672], "cluster": 4}, {"key": "chandrasekhar2017compression", "year": "2017", "citations": "21", "title": "Compression Of Deep Neural Networks For Image Instance Retrieval", "abstract": "<p>Image instance retrieval is the problem of retrieving images from a database\nwhich contain the same object. Convolutional Neural Network (CNN) based\ndescriptors are becoming the dominant approach for generating {\\it global image\ndescriptors} for the instance retrieval problem. One major drawback of\nCNN-based {\\it global descriptors} is that uncompressed deep neural network\nmodels require hundreds of megabytes of storage making them inconvenient to\ndeploy in mobile applications or in custom hardware. In this work, we study the\nproblem of neural network model compression focusing on the image instance\nretrieval task. We study quantization, coding, pruning and weight sharing\ntechniques for reducing model size for the instance retrieval problem. We\nprovide extensive experimental results on the trade-off between retrieval\nperformance and model size for different types of networks on several data sets\nproviding the most comprehensive study on this topic. We compress models to the\norder of a few MBs: two orders of magnitude smaller than the uncompressed\nmodels while achieving negligible loss in retrieval performance.</p>\n", "tags": ["Quantization", "Evaluation"], "tsne_embedding": [-45.55960464477539, 0.2981758415699005], "cluster": 0}, {"key": "chang2019semi", "year": "2019", "citations": "3", "title": "Semi-supervised Exploration In Image Retrieval", "abstract": "<p>We present our solution to Landmark Image Retrieval Challenge 2019. This\nchallenge was based on the large Google Landmarks Dataset V2[9]. The goal was\nto retrieve all database images containing the same landmark for every provided\nquery image. Our solution is a combination of global and local models to form\nan initial KNN graph. We then use a novel extension of the recently proposed\ngraph traversal method EGT [1] referred to as semi-supervised EGT to refine the\ngraph and retrieve better candidates.</p>\n", "tags": ["Supervised", "Image-Retrieval", "Datasets"], "tsne_embedding": [-16.054243087768555, -57.82756805419922], "cluster": 3}, {"key": "charikar2018hashing", "year": "2017", "citations": "52", "title": "Hashing-based-estimators For Kernel Density In High Dimensions", "abstract": "<p>Given a set of points \\(P\\subset \\mathbb{R}^{d}\\) and a kernel \\(k\\), the Kernel\nDensity Estimate at a point \\(x\\in\\mathbb{R}^{d}\\) is defined as\n\\(\\mathrm{KDE}<em>{P}(x)=\\frac{1}{|P|}\\sum</em>{y\\in P} k(x,y)\\). We study the problem\nof designing a data structure that given a data set \\(P\\) and a kernel function,\nreturns <em>approximations to the kernel density</em> of a query point in <em>sublinear\ntime</em>. We introduce a class of unbiased estimators for kernel density\nimplemented through locality-sensitive hashing, and give general theorems\nbounding the variance of such estimators. These estimators give rise to\nefficient data structures for estimating the kernel density in high dimensions\nfor a variety of commonly used kernels. Our work is the first to provide\ndata-structures with theoretical guarantees that improve upon simple random\nsampling in high dimensions.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [17.471420288085938, 47.92017364501953], "cluster": 4}, {"key": "charikar2020kernel", "year": "2020", "citations": "8", "title": "Kernel Density Estimation Through Density Constrained Near Neighbor Search", "abstract": "<p>In this paper we revisit the kernel density estimation problem: given a\nkernel \\(K(x, y)\\) and a dataset of \\(n\\) points in high dimensional Euclidean\nspace, prepare a data structure that can quickly output, given a query \\(q\\), a\n\\((1+\\epsilon)\\)-approximation to \\(\\mu:=\\frac1{|P|}\\sum_{p\\in P} K(p, q)\\). First,\nwe give a single data structure based on classical near neighbor search\ntechniques that improves upon or essentially matches the query time and space\ncomplexity for all radial kernels considered in the literature so far. We then\nshow how to improve both the query complexity and runtime by using recent\nadvances in data-dependent near neighbor search.\n  We achieve our results by giving a new implementation of the natural\nimportance sampling scheme. Unlike previous approaches, our algorithm first\nsamples the dataset uniformly (considering a geometric sequence of sampling\nrates), and then uses existing approximate near neighbor search techniques on\nthe resulting smaller dataset to retrieve the sampled points that lie at an\nappropriate distance from the query. We show that the resulting sampled dataset\nhas strong geometric structure, making approximate near neighbor search return\nthe required samples much more efficiently than for worst case datasets of the\nsame size. As an example application, we show that this approach yields a data\nstructure that achieves query time \\(\\mu^{-(1+o(1))/4}\\) and space complexity\n\\(\\mu^{-(1+o(1))}\\) for the Gaussian kernel. Our data dependent approach achieves\nquery time \\(\\mu^{-0.173-o(1)}\\) and space \\(\\mu^{-(1+o(1))}\\) for the Gaussian\nkernel. The data dependent analysis relies on new techniques for tracking the\ngeometric structure of the input datasets in a recursive hashing process that\nwe hope will be of interest in other applications in near neighbor search.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Datasets"], "tsne_embedding": [19.987071990966797, 47.341949462890625], "cluster": 4}, {"key": "chaudhuri2021crossatnet", "year": "2020", "citations": "28", "title": "Crossatnet - A Novel Cross-attention Based Framework For Sketch-based Image Retrieval", "abstract": "<p>We propose a novel framework for cross-modal zero-shot learning (ZSL) in the\ncontext of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema\nmainly considers simultaneous mappings among the two image views and the\nsemantic side information. Therefore, it is desirable to consider fine-grained\nclasses mainly in the sketch domain using highly discriminative and\nsemantically rich feature space. However, the existing deep generative\nmodeling-based SBIR approaches majorly focus on bridging the gaps between the\nseen and unseen classes by generating pseudo-unseen-class samples. Besides,\nviolating the ZSL protocol by not utilizing any unseen-class information during\ntraining, such techniques do not pay explicit attention to modeling the\ndiscriminative nature of the shared space. Also, we note that learning a\nunified feature space for both the multi-view visual data is a tedious task\nconsidering the significant domain difference between sketches and color\nimages. In this respect, as a remedy, we introduce a novel framework for\nzero-shot SBIR. While we define a cross-modal triplet loss to ensure the\ndiscriminative nature of the shared space, an innovative cross-modal attention\nlearning strategy is also proposed to guide feature extraction from the image\ndomain exploiting information from the respective sketch counterpart. In order\nto preserve the semantic consistency of the shared space, we consider a graph\nCNN-based module that propagates the semantic class topology to the shared\nspace. To ensure an improved response time during inference, we further explore\nthe possibility of representing the shared space in terms of hash codes.\nExperimental results obtained on the benchmark TU-Berlin and the Sketchy\ndatasets confirm the superiority of CrossATNet in yielding state-of-the-art\nresults.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [2.95926833152771, 10.107851028442383], "cluster": 6}, {"key": "chaudhuri2022cross", "year": "2022", "citations": "2", "title": "Cross-modal Fusion Distillation For Fine-grained Sketch-based Image Retrieval", "abstract": "<p>Representation learning for sketch-based image retrieval has mostly been\ntackled by learning embeddings that discard modality-specific information. As\ninstances from different modalities can often provide complementary information\ndescribing the underlying concept, we propose a cross-attention framework for\nVision Transformers (XModalViT) that fuses modality-specific information\ninstead of discarding them. Our framework first maps paired datapoints from the\nindividual photo and sketch modalities to fused representations that unify\ninformation from both modalities. We then decouple the input space of the\naforementioned modality fusion network into independent encoders of the\nindividual modalities via contrastive and relational cross-modal knowledge\ndistillation. Such encoders can then be applied to downstream tasks like\ncross-modal retrieval. We demonstrate the expressive capacity of the learned\nrepresentations by performing a wide range of experiments and achieving\nstate-of-the-art results on three fine-grained sketch-based image retrieval\nbenchmarks: Shoe-V2, Chair-V2 and Sketchy. Implementation is available at\nhttps://github.com/abhrac/xmodal-vit.</p>\n", "tags": ["Multimodal-Retrieval", "Tools-&-Libraries", "Image-Retrieval"], "tsne_embedding": [-18.779823303222656, -28.0599308013916], "cluster": 5}, {"key": "chen2016revisiting", "year": "2016", "citations": "3", "title": "Revisiting Winner Take All (WTA) Hashing For Sparse Datasets", "abstract": "<p>WTA (Winner Take All) hashing has been successfully applied in many large\nscale vision applications. This hashing scheme was tailored to take advantage\nof the comparative reasoning (or order based information), which showed\nsignificant accuracy improvements. In this paper, we identify a subtle issue\nwith WTA, which grows with the sparsity of the datasets. This issue limits the\ndiscriminative power of WTA. We then propose a solution for this problem based\non the idea of Densification which provably fixes the issue. Our experiments\nshow that Densified WTA Hashing outperforms Vanilla WTA both in image\nclassification and retrieval tasks consistently and significantly.</p>\n", "tags": ["Hashing-Methods", "Datasets"], "tsne_embedding": [6.657668113708496, 31.80270004272461], "cluster": 4}, {"key": "chen2017darkrank", "year": "2018", "citations": "219", "title": "Darkrank: Accelerating Deep Metric Learning Via Cross Sample Similarities Transfer", "abstract": "<p>We have witnessed rapid evolution of deep neural network architecture design\nin the past years. These latest progresses greatly facilitate the developments\nin various areas such as computer vision and natural language processing.\nHowever, along with the extraordinary performance, these state-of-the-art\nmodels also bring in expensive computational cost. Directly deploying these\nmodels into applications with real-time requirement is still infeasible.\nRecently, Hinton etal. have shown that the dark knowledge within a powerful\nteacher model can significantly help the training of a smaller and faster\nstudent network. These knowledge are vastly beneficial to improve the\ngeneralization ability of the student model. Inspired by their work, we\nintroduce a new type of knowledge \u2013 cross sample similarities for model\ncompression and acceleration. This knowledge can be naturally derived from deep\nmetric learning model. To transfer them, we bring the \u201clearning to rank\u201d\ntechnique into deep metric learning formulation. We test our proposed DarkRank\nmethod on various metric learning tasks including pedestrian re-identification,\nimage retrieval and image clustering. The results are quite encouraging. Our\nmethod can improve over the baseline method by a large margin. Moreover, it is\nfully compatible with other existing methods. When combined, the performance\ncan be further boosted.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Image-Retrieval", "AAAI", "Evaluation"], "tsne_embedding": [26.58761978149414, -34.676483154296875], "cluster": 7}, {"key": "chen2017doctag2vec", "year": "2017", "citations": "21", "title": "Doctag2vec: An Embedding Based Multi-label Learning Approach For Document Tagging", "abstract": "<p>Tagging news articles or blog posts with relevant tags from a collection of\npredefined ones is coined as document tagging in this work. Accurate tagging of\narticles can benefit several downstream applications such as recommendation and\nsearch. In this work, we propose a novel yet simple approach called DocTag2Vec\nto accomplish this task. We substantially extend Word2Vec and Doc2Vec\u2014two\npopular models for learning distributed representation of words and documents.\nIn DocTag2Vec, we simultaneously learn the representation of words, documents,\nand tags in a joint vector space during training, and employ the simple\n\\(k\\)-nearest neighbor search to predict tags for unseen documents. In contrast\nto previous multi-label learning methods, DocTag2Vec directly deals with raw\ntext instead of provided feature vector, and in addition, enjoys advantages\nlike the learning of tag representation, and the ability of handling newly\ncreated tags. To demonstrate the effectiveness of our approach, we conduct\nexperiments on several datasets and show promising results against\nstate-of-the-art methods.</p>\n", "tags": ["Recommender-Systems", "Datasets"], "tsne_embedding": [8.66073989868164, -8.71023941040039], "cluster": 6}, {"key": "chen2017vertex", "year": "2017", "citations": "4", "title": "Vertex-context Sampling For Weighted Network Embedding", "abstract": "<p>In recent years, network embedding methods have garnered increasing attention\nbecause of their effectiveness in various information retrieval tasks. The goal\nis to learn low-dimensional representations of vertexes in an information\nnetwork and simultaneously capture and preserve the network structure. Critical\nto the performance of a network embedding method is how the edges/vertexes of\nthe network is sampled for the learning process. Many existing methods adopt a\nuniform sampling method to reduce learning complexity, but when the network is\nnon-uniform (i.e. a weighted network) such uniform sampling incurs information\nloss. The goal of this paper is to present a generalized vertex sampling\nframework that works seamlessly with most existing network embedding methods to\nsupport weighted instead of uniform vertex/edge sampling. For efficiency, we\npropose a delicate sequential vertex-to-context graph data structure, such that\nsampling a training pair for learning takes only constant time. For scalability\nand memory efficiency, we design the graph data structure in a way that keeps\nspace consumption low without requiring additional space. In addition to\nimplementing existing network embedding methods, the proposed framework can be\nused to implement extensions that feature high-order proximity modeling and\nweighted relation modeling. Experiments conducted on three datasets, including\na commercial large-scale one, verify the effectiveness and efficiency of the\nproposed weighted network embedding methods on a variety of tasks, including\nword similarity search, multi-label classification, and item recommendation.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Similarity-Search", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [50.44938659667969, 1.3987295627593994], "cluster": 9}, {"key": "chen2018almn", "year": "2018", "citations": "4", "title": "ALMN: Deep Embedding Learning With Geometrical Virtual Point Generating", "abstract": "<p>Deep embedding learning becomes more attractive for discriminative feature\nlearning, but many methods still require hard-class mining, which is\ncomputationally complex and performance-sensitive. To this end, we propose\nAdaptive Large Margin N-Pair loss (ALMN) to address the aforementioned issues.\nInstead of exploring hard example-mining strategy, we introduce the concept of\nlarge margin constraint. This constraint aims at encouraging local-adaptive\nlarge angular decision margin among dissimilar samples in multimodal feature\nspace so as to significantly encourage intraclass compactness and interclass\nseparability. And it is mainly achieved by a simple yet novel geometrical\nVirtual Point Generating (VPG) method, which converts artificially setting a\nfixed margin into automatically generating a boundary training sample in\nfeature space and is an open question. We demonstrate the effectiveness of our\nmethod on several popular datasets for image retrieval and clustering tasks.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-20.369718551635742, -13.794156074523926], "cluster": 1}, {"key": "chen2018blazingly", "year": "2018", "citations": "283", "title": "Blazingly Fast Video Object Segmentation With Pixel-wise Metric Learning", "abstract": "<p>This paper tackles the problem of video object segmentation, given some user\nannotation which indicates the object of interest. The problem is formulated as\npixel-wise retrieval in a learned embedding space: we embed pixels of the same\nobject instance into the vicinity of each other, using a fully convolutional\nnetwork trained by a modified triplet loss as the embedding model. Then the\nannotated pixels are set as reference and the rest of the pixels are classified\nusing a nearest-neighbor approach. The proposed method supports different kinds\nof user input such as segmentation mask in the first frame (semi-supervised\nscenario), or a sparse set of clicked points (interactive scenario). In the\nsemi-supervised scenario, we achieve results competitive with the state of the\nart but at a fraction of computation cost (275 milliseconds per frame). In the\ninteractive scenario where the user is able to refine their input iteratively,\nthe proposed method provides instant response to each input, and reaches\ncomparable quality to competing methods with much less interaction.</p>\n", "tags": ["Supervised", "CVPR", "Distance-Metric-Learning"], "tsne_embedding": [-20.00821876525879, 11.641473770141602], "cluster": 8}, {"key": "chen2018deep", "year": "2018", "citations": "60", "title": "Deep Supervised Hashing With Anchor Graph", "abstract": "<p>Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware\u2019s memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [24.419090270996094, 0.6417078375816345], "cluster": 6}, {"key": "chen2018distributed", "year": "2018", "citations": "7", "title": "Distributed Collaborative Hashing And Its Applications In Ant Financial", "abstract": "<p>Collaborative filtering, especially latent factor model, has been popularly\nused in personalized recommendation. Latent factor model aims to learn user and\nitem latent factors from user-item historic behaviors. To apply it into real\nbig data scenarios, efficiency becomes the first concern, including offline\nmodel training efficiency and online recommendation efficiency. In this paper,\nwe propose a Distributed Collaborative Hashing (DCH) model which can\nsignificantly improve both efficiencies. Specifically, we first propose a\ndistributed learning framework, following the state-of-the-art parameter server\nparadigm, to learn the offline collaborative model. Our model can be learnt\nefficiently by distributedly computing subgradients in minibatches on workers\nand updating model parameters on servers asynchronously. We then adopt hashing\ntechnique to speedup the online recommendation procedure. Recommendation can be\nquickly made through exploiting lookup hash tables. We conduct thorough\nexperiments on two real large-scale datasets. The experimental results\ndemonstrate that, comparing with the classic and state-of-the-art (distributed)\nlatent factor models, DCH has comparable performance in terms of recommendation\naccuracy but has both fast convergence speed in offline model training\nprocedure and realtime efficiency in online recommendation procedure.\nFurthermore, the encouraging performance of DCH is also shown for several\nreal-world applications in Ant Financial.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation", "KDD"], "tsne_embedding": [25.64358901977539, -27.01923942565918], "cluster": 7}, {"key": "chen2018improving", "year": "2019", "citations": "12", "title": "Improving Deep Binary Embedding Networks By Order-aware Reweighting Of Triplets", "abstract": "<p>In this paper, we focus on triplet-based deep binary embedding networks for\nimage retrieval task. The triplet loss has been shown to be most effective for\nthe ranking problem. However, most of the previous works treat the triplets\nequally or select the hard triplets based on the loss. Such strategies do not\nconsider the order relations, which is important for retrieval task. To this\nend, we propose an order-aware reweighting method to effectively train the\ntriplet-based deep networks, which up-weights the important triplets and\ndown-weights the uninformative triplets. First, we present the order-aware\nweighting factors to indicate the importance of the triplets, which depend on\nthe rank order of binary codes. Then, we reshape the triplet loss to the\nsquared triplet loss such that the loss function will put more weights on the\nimportant triplets. Extensive evaluations on four benchmark datasets show that\nthe proposed method achieves significant performance compared with the\nstate-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-9.510337829589844, 8.39380168914795], "cluster": 8}, {"key": "chen2019differentiable", "year": "2019", "citations": "13", "title": "Differentiable Product Quantization For End-to-end Embedding Compression", "abstract": "<p>Embedding layers are commonly used to map discrete symbols into continuous\nembedding vectors that reflect their semantic meanings. Despite their\neffectiveness, the number of parameters in an embedding layer increases\nlinearly with the number of symbols and poses a critical challenge on memory\nand storage constraints. In this work, we propose a generic and end-to-end\nlearnable compression framework termed differentiable product quantization\n(DPQ). We present two instantiations of DPQ that leverage different\napproximation techniques to enable differentiability in end-to-end learning.\nOur method can readily serve as a drop-in alternative for any existing\nembedding layer. Empirically, DPQ offers significant compression ratios\n(14-238\\(\\times\\)) at negligible or no performance cost on 10 datasets across\nthree different language tasks.</p>\n", "tags": ["Quantization", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [22.878602981567383, 11.189547538757324], "cluster": 2}, {"key": "chen2019efficient", "year": "2021", "citations": "12", "title": "Efficient Object Embedding For Spliced Image Retrieval", "abstract": "<p>Detecting spliced images is one of the emerging challenges in computer\nvision. Unlike prior methods that focus on detecting low-level artifacts\ngenerated during the manipulation process, we use an image retrieval approach\nto tackle this problem. When given a spliced query image, our goal is to\nretrieve the original image from a database of authentic images. To achieve\nthis goal, we propose representing an image by its constituent objects based on\nthe intuition that the finest granularity of manipulations is oftentimes at the\nobject-level. We introduce a framework, object embeddings for spliced image\nretrieval (OE-SIR), that utilizes modern object detectors to localize object\nregions. Each region is then embedded and collectively used to represent the\nimage. Further, we propose a student-teacher training paradigm for learning\ndiscriminative embeddings within object regions to avoid expensive multiple\nforward passes. Detailed analysis of the efficacy of different feature\nembedding models is also provided in this study. Extensive experimental results\nshow that the OE-SIR achieves state-of-the-art performance in spliced image\nretrieval.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval", "CVPR"], "tsne_embedding": [-21.94504737854004, -25.549135208129883], "cluster": 5}, {"key": "chen2019hadamard", "year": "2019", "citations": "3", "title": "Hadamard Codebook Based Deep Hashing", "abstract": "<p>As an approximate nearest neighbor search technique, hashing has been widely\napplied in large-scale image retrieval due to its excellent efficiency. Most\nsupervised deep hashing methods have similar loss designs with embedding\nlearning, while quantizing the continuous high-dim feature into compact binary\nspace. We argue that the existing deep hashing schemes are defective in two\nissues that seriously affect the performance, i.e., bit independence and bit\nbalance. The former refers to hash codes of different classes should be\nindependent of each other, while the latter means each bit should have a\nbalanced distribution of +1s and -1s. In this paper, we propose a novel\nsupervised deep hashing method, termed Hadamard Codebook based Deep Hashing\n(HCDH), which solves the above two problems in a unified formulation.\nSpecifically, we utilize an off-the-shelf algorithm to generate a binary\nHadamard codebook to satisfy the requirement of bit independence and bit\nbalance, which subsequently serves as the desired outputs of the hash functions\nlearning. We also introduce a projection matrix to solve the inconsistency\nbetween the order of Hadamard matrix and the number of classes. Besides, the\nproposed HCDH further exploits the supervised labels by constructing a\nclassifier on top of the outputs of hash functions. Extensive experiments\ndemonstrate that HCDH can yield discriminative and balanced binary codes, which\nwell outperforms many state-of-the-arts on three widely-used benchmarks.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [2.161611318588257, 14.812777519226074], "cluster": 8}, {"key": "chen2019hybrid", "year": "2019", "citations": "66", "title": "Hybrid-attention Based Decoupled Metric Learning For Zero-shot Image Retrieval", "abstract": "<p>In zero-shot image retrieval (ZSIR) task, embedding learning becomes more\nattractive, however, many methods follow the traditional metric learning idea\nand omit the problems behind zero-shot settings. In this paper, we first\nemphasize the importance of learning visual discriminative metric and\npreventing the partial/selective learning behavior of learner in ZSIR, and then\npropose the Decoupled Metric Learning (DeML) framework to achieve these\nindividually. Instead of coarsely optimizing an unified metric, we decouple it\ninto multiple attention-specific parts so as to recurrently induce the\ndiscrimination and explicitly enhance the generalization. And they are mainly\nachieved by our object-attention module based on random walk graph propagation\nand the channel-attention module based on the adversary constraint,\nrespectively. We demonstrate the necessity of addressing the vital problems in\nZSIR on the popular benchmarks, outperforming the state-of-theart methods by a\nsignificant margin. Code is available at http://www.bhchen.cn</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Tools-&-Libraries"], "tsne_embedding": [-12.53963851928711, -18.822463989257812], "cluster": 1}, {"key": "chen2019locality", "year": "2019", "citations": "4", "title": "Locality-sensitive Hashing For F-divergences: Mutual Information Loss And Beyond", "abstract": "<p>Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.</p>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability"], "tsne_embedding": [14.96341323852539, 45.04170227050781], "cluster": 4}, {"key": "chen2019two", "year": "2019", "citations": "45", "title": "A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps", "abstract": "<p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Compact-Codes", "Multimodal-Retrieval", "Hashing-Methods"], "tsne_embedding": [8.468842506408691, 2.3781192302703857], "cluster": 6}, {"key": "chen2019vector", "year": "2019", "citations": "7", "title": "Vector And Line Quantization For Billion-scale Similarity Search On Gpus", "abstract": "<p>Billion-scale high-dimensional approximate nearest neighbour (ANN) search has\nbecome an important problem for searching similar objects among the vast amount\nof images and videos available online. The existing ANN methods are usually\ncharacterized by their specific indexing structures, including the inverted\nindex and the inverted multi-index structure. The inverted index structure is\namenable to GPU-based implementations, and the state-of-the-art systems such as\nFaiss are able to exploit the massive parallelism offered by GPUs. However, the\ninverted index requires high memory overhead to index the dataset effectively.\nThe inverted multi-index structure is difficult to implement for GPUs, and also\nineffective in dealing with database with different data distributions. In this\npaper we propose a novel hierarchical inverted index structure generated by\nvector and line quantization methods. Our quantization method improves both\nsearch efficiency and accuracy, while maintaining comparable memory\nconsumption. This is achieved by reducing search space and increasing the\nnumber of indexed regions. We introduce a new ANN search system, VLQ-ADC, that\nis based on the proposed inverted index, and perform extensive evaluation on\ntwo public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation\nshows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and\nCPU-based systems in terms of both accuracy and search speed. The source code\nof VLQ-ADC is available at\nhttps://github.com/zjuchenwei/vector-line-quantization.</p>\n", "tags": ["Efficiency", "Quantization", "Vector-Indexing", "Scalability", "Similarity-Search", "Tools-&-Libraries", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [39.64567565917969, 20.314579010009766], "cluster": 2}, {"key": "chen2020enhanced", "year": "2020", "citations": "27", "title": "Enhanced Discrete Multi-modal Hashing: More Constraints Yet Less Time To Learn", "abstract": "<p>Due to the exponential growth of multimedia data, multi-modal hashing as a promising technique to make cross-view retrieval scalable is attracting more and more attention. However, most of the existing multi-modal hashing methods either divide the learning process unnaturally into two separate stages or treat the discrete optimization problem simplistically as a continuous one, which leads to suboptimal results. Recently, a few discrete multi-modal hashing methods that try to address such issues have emerged, but they still ignore several important discrete constraints (such as the balance and decorrelation of hash bits). In this paper, we overcome those limitations by proposing a novel method named \u201cEnhanced Discrete Multi-modal Hashing (EDMH)\u201d which learns binary codes and hashing functions simultaneously from the pairwise similarity matrix of data, under the aforementioned discrete constraints. Although the model of EDMH looks a lot more complex than the other models for multi-modal hashing, we are actually able to develop a fast iterative learning algorithm for it, since the subproblems of its optimization all have closed-form solutions after introducing two auxiliary variables. Our experimental results on three real-world datasets have demonstrated that EDMH not only performs much better than state-of-the-art competitors but also runs much faster than them.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Datasets"], "tsne_embedding": [16.54281234741211, -10.373635292053223], "cluster": 7}, {"key": "chen2020expressing", "year": "2020", "citations": "58", "title": "Expressing Objects Just Like Words: Recurrent Visual Embedding For Image-text Matching", "abstract": "<p>Existing image-text matching approaches typically infer the similarity of an\nimage-text pair by capturing and aggregating the affinities between the text\nand each independent object of the image. However, they ignore the connections\nbetween the objects that are semantically related. These objects may\ncollectively determine whether the image corresponds to a text or not. To\naddress this problem, we propose a Dual Path Recurrent Neural Network (DP-RNN)\nwhich processes images and sentences symmetrically by recurrent neural networks\n(RNN). In particular, given an input image-text pair, our model reorders the\nimage objects based on the positions of their most related words in the text.\nIn the same way as extracting the hidden features from word embeddings, the\nmodel leverages RNN to extract high-level object features from the reordered\nobject inputs. We validate that the high-level object features contain useful\njoint information of semantically related objects, which benefit the retrieval\ntask. To compute the image-text similarity, we incorporate a Multi-attention\nCross Matching Model into DP-RNN. It aggregates the affinity between objects\nand words with cross-modality guided attention and self-attention. Our model\nachieves the state-of-the-art performance on Flickr30K dataset and competitive\nperformance on MS-COCO dataset. Extensive experiments demonstrate the\neffectiveness of our model.</p>\n", "tags": ["AAAI", "Evaluation", "Datasets"], "tsne_embedding": [-32.83186721801758, -24.798171997070312], "cluster": 5}, {"key": "chen2020fine", "year": "2020", "citations": "272", "title": "Fine-grained Video-text Retrieval With Hierarchical Graph Reasoning", "abstract": "<p>Cross-modal retrieval between videos and texts has attracted growing\nattentions due to the rapid emergence of videos on the web. The current\ndominant approach for this problem is to learn a joint embedding space to\nmeasure cross-modal similarities. However, simple joint embeddings are\ninsufficient to represent complicated visual and textual details, such as\nscenes, objects, actions and their compositions. To improve fine-grained\nvideo-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model,\nwhich decomposes video-text matching into global-to-local levels. To be\nspecific, the model disentangles texts into hierarchical semantic graph\nincluding three levels of events, actions, entities and relationships across\nlevels. Attention-based graph reasoning is utilized to generate hierarchical\ntextual embeddings, which can guide the learning of diverse and hierarchical\nvideo representations. The HGR model aggregates matchings from different\nvideo-text levels to capture both global and local details. Experimental\nresults on three video-text datasets demonstrate the advantages of our model.\nSuch hierarchical decomposition also enables better generalization across\ndatasets and improves the ability to distinguish fine-grained semantic\ndifferences.</p>\n", "tags": ["Multimodal-Retrieval", "CVPR", "Text-Retrieval", "Datasets"], "tsne_embedding": [-39.4826774597168, -33.60750198364258], "cluster": 5}, {"key": "chen2020imram", "year": "2020", "citations": "337", "title": "IMRAM: Iterative Matching With Recurrent Attention Memory For Cross-modal Image-text Retrieval", "abstract": "<p>Enabling bi-directional retrieval of images and texts is important for\nunderstanding the correspondence between vision and language. Existing methods\nleverage the attention mechanism to explore such correspondence in a\nfine-grained manner. However, most of them consider all semantics equally and\nthus align them uniformly, regardless of their diverse complexities. In fact,\nsemantics are diverse (i.e. involving different kinds of semantic concepts),\nand humans usually follow a latent structure to combine them into\nunderstandable languages. It may be difficult to optimally capture such\nsophisticated correspondences in existing methods. In this paper, to address\nsuch a deficiency, we propose an Iterative Matching with Recurrent Attention\nMemory (IMRAM) method, in which correspondences between images and texts are\ncaptured with multiple steps of alignments. Specifically, we introduce an\niterative matching scheme to explore such fine-grained correspondence\nprogressively. A memory distillation unit is used to refine alignment knowledge\nfrom early steps to later ones. Experiment results on three benchmark datasets,\ni.e. Flickr8K, Flickr30K, and MS COCO, show that our IMRAM achieves\nstate-of-the-art performance, well demonstrating its effectiveness. Experiments\non a practical business advertisement dataset, named \\Ads{}, further validates\nthe applicability of our method in practical scenarios.</p>\n", "tags": ["CVPR", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-33.67443084716797, -27.37139892578125], "cluster": 5}, {"key": "chen2020making", "year": "2019", "citations": "21", "title": "Making Online Sketching Hashing Even Faster", "abstract": "<p>Data-dependent hashing methods have demonstrated good performance in various\nmachine learning applications to learn a low-dimensional representation from\nthe original data. However, they still suffer from several obstacles: First,\nmost of existing hashing methods are trained in a batch mode, yielding\ninefficiency for training streaming data. Second, the computational cost and\nthe memory consumption increase extraordinarily in the big data setting, which\nperplexes the training procedure. Third, the lack of labeled data hinders the\nimprovement of the model performance. To address these difficulties, we utilize\nonline sketching hashing (OSH) and present a FasteR Online Sketching Hashing\n(FROSH) algorithm to sketch the data in a more compact form via an independent\ntransformation. We provide theoretical justification to guarantee that our\nproposed FROSH consumes less time and achieves a comparable sketching precision\nunder the same memory cost of OSH. We also extend FROSH to its distributed\nimplementation, namely DFROSH, to further reduce the training time cost of\nFROSH while deriving the theoretical bound of the sketching precision. Finally,\nwe conduct extensive experiments on both synthetic and real datasets to\ndemonstrate the attractive merits of FROSH and DFROSH.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [26.278820037841797, 8.350680351257324], "cluster": 2}, {"key": "chen2020strongly", "year": "2020", "citations": "50", "title": "Strongly Constrained Discrete Hashing", "abstract": "<p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [21.5539608001709, 1.978505253791809], "cluster": 6}, {"key": "chen2021augnet", "year": "2021", "citations": "5", "title": "Augnet: End-to-end Unsupervised Visual Representation Learning With Image Augmentation", "abstract": "<p>Most of the achievements in artificial intelligence so far were accomplished\nby supervised learning which requires numerous annotated training data and thus\ncosts innumerable manpower for labeling. Unsupervised learning is one of the\neffective solutions to overcome such difficulties. In our work, we propose\nAugNet, a new deep learning training paradigm to learn image features from a\ncollection of unlabeled pictures. We develop a method to construct the\nsimilarities between pictures as distance metrics in the embedding space by\nleveraging the inter-correlation between augmented versions of samples. Our\nexperiments demonstrate that the method is able to represent the image in low\ndimensional space and performs competitively in downstream tasks such as image\nclassification and image similarity comparison. Specifically, we achieved over\n60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised\nclustering, respectively. Moreover, unlike many deep-learning-based image\nretrieval algorithms, our approach does not require access to external\nannotated datasets to train the feature extractor, but still shows comparable\nor even better feature representation ability and easy-to-use characteristics.\nIn our evaluations, the method outperforms all the state-of-the-art image\nretrieval algorithms on some out-of-domain image datasets. The code for the\nmodel implementation is available at\nhttps://github.com/chenmingxiang110/AugNet.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-22.21771812438965, -12.235696792602539], "cluster": 1}, {"key": "chen2021deep", "year": "2022", "citations": "122", "title": "Deep Learning For Instance Retrieval: A Survey", "abstract": "<p>In recent years a vast amount of visual content has been generated and shared\nfrom many fields, such as social media platforms, medical imaging, and\nrobotics. This abundance of content creation and sharing has introduced new\nchallenges, particularly that of searching databases for similar\ncontent-Content Based Image Retrieval (CBIR)-a long-established research area\nin which improved efficiency and accuracy are needed for real-time retrieval.\nArtificial intelligence has made progress in CBIR and has significantly\nfacilitated the process of instance search. In this survey we review recent\ninstance retrieval works that are developed based on deep learning algorithms\nand techniques, with the survey organized by deep network architecture types,\ndeep features, feature embedding and aggregation methods, and network\nfine-tuning strategies. Our survey considers a wide variety of recent methods,\nwhereby we identify milestone work, reveal connections among various methods\nand present the commonly used benchmarks, evaluation results, common\nchallenges, and propose promising future directions.</p>\n", "tags": ["Efficiency", "Evaluation", "Image-Retrieval", "Survey-Paper"], "tsne_embedding": [-21.784496307373047, -38.12737274169922], "cluster": 3}, {"key": "chen2021dvhn", "year": "2023", "citations": "3", "title": "DVHN: A Deep Hashing Framework For Large-scale Vehicle Re-identification", "abstract": "<p>In this paper, we make the very first attempt to investigate the integration\nof deep hash learning with vehicle re-identification. We propose a deep\nhash-based vehicle re-identification framework, dubbed DVHN, which\nsubstantially reduces memory usage and promotes retrieval efficiency while\nreserving nearest neighbor search accuracy. Concretely,~DVHN directly learns\ndiscrete compact binary hash codes for each image by jointly optimizing the\nfeature learning network and the hash code generating module. Specifically, we\ndirectly constrain the output from the convolutional neural network to be\ndiscrete binary codes and ensure the learned binary codes are optimal for\nclassification. To optimize the deep discrete hashing framework, we further\npropose an alternating minimization method for learning binary\nsimilarity-preserved hashing codes. Extensive experiments on two widely-studied\nvehicle re-identification datasets- \\textbf{VehicleID} and \\textbf{VeRi}-~have\ndemonstrated the superiority of our method against the state-of-the-art deep\nhash methods. \\textbf{DVHN} of \\(2048\\) bits can achieve 13.94% and 10.21%\naccuracy improvement in terms of \\textbf{mAP} and \\textbf{Rank@1} for\n\\textbf{VehicleID (800)} dataset. For \\textbf{VeRi}, we achieve 35.45% and\n32.72% performance gains for \\textbf{Rank@1} and \\textbf{mAP}, respectively.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-19.32855224609375, 6.554800510406494], "cluster": 1}, {"key": "chen2021fuzzy", "year": "2022", "citations": "40", "title": "Fuzzy Logic Based Logical Query Answering On Knowledge Graphs", "abstract": "<p>Answering complex First-Order Logical (FOL) queries on large-scale incomplete\nknowledge graphs (KGs) is an important yet challenging task. Recent advances\nembed logical queries and KG entities in the same space and conduct query\nanswering via dense similarity search. However, most logical operators designed\nin previous studies do not satisfy the axiomatic system of classical logic,\nlimiting their performance. Moreover, these logical operators are parameterized\nand thus require many complex FOL queries as training data, which are often\narduous to collect or even inaccessible in most real-world KGs. We thus present\nFuzzQE, a fuzzy logic based logical query embedding framework for answering FOL\nqueries over KGs. FuzzQE follows fuzzy logic to define logical operators in a\nprincipled and learning-free manner, where only entity and relation embeddings\nrequire learning. FuzzQE can further benefit from labeled complex logical\nqueries for training. Extensive experiments on two benchmark datasets\ndemonstrate that FuzzQE provides significantly better performance in answering\nFOL queries compared to state-of-the-art methods. In addition, FuzzQE trained\nwith only KG link prediction can achieve comparable performance to those\ntrained with extra complex query data.</p>\n", "tags": ["Similarity-Search", "Scalability", "AAAI", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [21.282522201538086, -11.144058227539062], "cluster": 7}, {"key": "chen2021ideal", "year": "2021", "citations": "1", "title": "IDEAL: Independent Domain Embedding Augmentation Learning", "abstract": "<p>Many efforts have been devoted to designing sampling, mining, and weighting\nstrategies in high-level deep metric learning (DML) loss objectives. However,\nlittle attention has been paid to low-level but essential data transformation.\nIn this paper, we develop a novel mechanism, the independent domain embedding\naugmentation learning ({IDEAL}) method. It can simultaneously learn multiple\nindependent embedding spaces for multiple domains generated by predefined data\ntransformations. Our IDEAL is orthogonal to existing DML techniques and can be\nseamlessly combined with prior DML approaches for enhanced performance.\nEmpirical results on visual retrieval tasks demonstrate the superiority of the\nproposed method. For example, the IDEAL improves the performance of MS loss by\na large margin, 84.5% \\(\\rightarrow\\) 87.1% on Cars-196, and 65.8%\n\\(\\rightarrow\\) 69.5% on CUB-200 at Recall\\(@1\\). Our IDEAL with MS loss also\nachieves the new state-of-the-art performance on three image retrieval\nbenchmarks, \\ie, <em>Cars-196</em>, <em>CUB-200</em>, and <em>SOP</em>. It\noutperforms the most recent DML approaches, such as Circle loss and XBM,\nsignificantly. The source code and pre-trained models of our method will be\navailable at<em>https://github.com/emdata-ailab/IDEAL</em>.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-35.60295104980469, 5.2726850509643555], "cluster": 0}, {"key": "chen2021learning", "year": "2021", "citations": "14", "title": "Learning Context-aware Embedding For Person Search", "abstract": "<p>Person Search is a relevant task that aims to jointly solve Person Detection\nand Person Re-identification(re-ID). Though most previous methods focus on\nlearning robust individual features for retrieval, it\u2019s still hard to\ndistinguish confusing persons because of illumination, large pose variance, and\nocclusion. Contextual information is practically available in person search\ntask which benefits searching in terms of reducing confusion. To this end, we\npresent a novel contextual feature head named Attention Context-Aware\nEmbedding(ACAE) which enhances contextual information. ACAE repeatedly reviews\nthe person features within and across images to find similar pedestrian\npatterns, allowing it to implicitly learn to find possible co-travelers and\nefficiently model contextual relevant instances\u2019 relations. Moreover, we\npropose Image Memory Bank to improve the training efficiency. Experimentally,\nACAE shows extensive promotion when built on different one-step methods. Our\noverall methods achieve state-of-the-art results compared with previous\none-step methods.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [-30.75969886779785, -43.463096618652344], "cluster": 5}, {"key": "chen2021long", "year": "2021", "citations": "9", "title": "Long-tail Hashing", "abstract": "<p>Hashing, which represents data items as compact binary codes, has\nbeen becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well\nas its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially\nbalanced datasets which are not commonly available in real-world\nscenarios. In this paper, we propose Long-Tail Hashing Network\n(LTHNet), a novel two-stage deep hashing approach that addresses\nthe problem of learning to hash for more realistic datasets where\nthe data labels roughly exhibit a long-tail distribution. Specifically,\nthe first stage is to learn relaxed embeddings of the given dataset\nwith its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those\nobtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual\nknowledge transfer between head and tail classes, and thus enrich\nimage representations for hashing. Our experiments have shown\nthat LTHNet achieves dramatic performance improvements over all\nstate-of-the-art competitors on long-tail datasets, with no or little\nsacrifice on balanced datasets. Further analyses reveal that while to\nour surprise directly manipulating class weights in the loss function\nhas little effect, the extended dynamic meta-embedding module, the\nusage of cross-entropy loss instead of square loss, and the relatively\nsmall batch-size for training all contribute to LTHNet\u2019s success.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "Image-Retrieval", "SIGIR", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [4.962960243225098, 18.886892318725586], "cluster": 8}, {"key": "chen2021multimodal", "year": "2021", "citations": "52", "title": "Multimodal Clustering Networks For Self-supervised Learning From Unlabeled Videos", "abstract": "<p>Multimodal self-supervised learning is getting more and more attention as it\nallows not only to train large networks without human supervision but also to\nsearch and retrieve data across various modalities. In this context, this paper\nproposes a self-supervised training framework that learns a common multimodal\nembedding space that, in addition to sharing representations across different\nmodalities, enforces a grouping of semantically similar instances. To this end,\nwe extend the concept of instance-level contrastive learning with a multimodal\nclustering step in the training pipeline to capture semantic similarities\nacross modalities. The resulting embedding space enables retrieval of samples\nacross all modalities, even from unseen datasets and different domains. To\nevaluate our approach, we train our model on the HowTo100M dataset and evaluate\nits zero-shot retrieval capabilities in two challenging domains, namely\ntext-to-video retrieval, and temporal action localization, showing\nstate-of-the-art results on four different datasets.</p>\n", "tags": ["Self-Supervised", "ICCV", "Few-Shot-&-Zero-Shot", "Tools-&-Libraries", "Datasets", "Supervised", "Video-Retrieval"], "tsne_embedding": [-7.267380714416504, -22.622528076171875], "cluster": 3}, {"key": "chen2021spann", "year": "2021", "citations": "4", "title": "SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search", "abstract": "<p>The in-memory algorithms for approximate nearest neighbor search (ANNS) have\nachieved great success for fast high-recall search, but are extremely expensive\nwhen handling very large scale database. Thus, there is an increasing request\nfor the hybrid ANNS solutions with small memory and inexpensive solid-state\ndrive (SSD). In this paper, we present a simple but efficient memory-disk\nhybrid indexing and search system, named SPANN, that follows the inverted index\nmethodology. It stores the centroid points of the posting lists in the memory\nand the large posting lists in the disk. We guarantee both disk-access\nefficiency (low latency) and high recall by effectively reducing the\ndisk-access number and retrieving high-quality posting lists. In the\nindex-building stage, we adopt a hierarchical balanced clustering algorithm to\nbalance the length of posting lists and augment the posting list by adding the\npoints in the closure of the corresponding clusters. In the search stage, we\nuse a query-aware scheme to dynamically prune the access of unnecessary posting\nlists. Experiment results demonstrate that SPANN is 2\\(\\times\\) faster than the\nstate-of-the-art ANNS solution DiskANN to reach the same recall quality \\(90%\\)\nwith same memory cost in three billion-scale datasets. It can reach \\(90%\\)\nrecall@1 and recall@10 in just around one millisecond with only 32GB memory\ncost. Code is available at:\n{\\footnotesize\\color{blue}{https://github.com/microsoft/SPTAG}}.</p>\n", "tags": ["Efficiency", "Scalability", "Large-Scale-Search", "Datasets", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [35.75282287597656, 20.231746673583984], "cluster": 2}, {"key": "chen2021towards", "year": "2021", "citations": "1", "title": "Towards Low-loss 1-bit Quantization Of User-item Representations For Top-k Recommendation", "abstract": "<p>Due to the promising advantages in space compression and inference\nacceleration, quantized representation learning for recommender systems has\nbecome an emerging research direction recently. As the target is to embed\nlatent features in the discrete embedding space, developing quantization for\nuser-item representations with a few low-precision integers confronts the\nchallenge of high information loss, thus leading to unsatisfactory performance\nin Top-K recommendation.\n  In this work, we study the problem of representation learning for\nrecommendation with 1-bit quantization. We propose a model named Low-loss\nQuantized Graph Convolutional Network (L^2Q-GCN). Different from previous work\nthat plugs quantization as the final encoder of user-item embeddings, L^2Q-GCN\nlearns the quantized representations whilst capturing the structural\ninformation of user-item interaction graphs at different semantic levels. This\nachieves the substantial retention of intermediate interactive information,\nalleviating the feature smoothing issue for ranking caused by numerical\nquantization. To further improve the model performance, we also present an\nadvanced solution named L^2Q-GCN-anl with quantization approximation and\nannealing training strategy. We conduct extensive experiments on four\nbenchmarks over Top-K recommendation task. The experimental results show that,\nwith nearly 9x representation storage compression, L^2Q-GCN-anl attains about\n90~99% performance recovery compared to the state-of-the-art model.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Evaluation"], "tsne_embedding": [51.26422882080078, -2.71386456489563], "cluster": 9}, {"key": "chen2021transhash", "year": "2022", "citations": "38", "title": "Transhash: Transformer-based Hamming Hashing For Efficient Image Retrieval", "abstract": "<p>Deep hamming hashing has gained growing popularity in approximate nearest\nneighbour search for large-scale image retrieval. Until now, the deep hashing\nfor the image retrieval community has been dominated by convolutional neural\nnetwork architectures, e.g. \\texttt{Resnet}\\cite{he2016deep}. In this paper,\ninspired by the recent advancements of vision transformers, we present\n\\textbf{Transhash}, a pure transformer-based framework for deep hashing\nlearning. Concretely, our framework is composed of two major modules: (1) Based\non \\textit{Vision Transformer} (ViT), we design a siamese vision transformer\nbackbone for image feature extraction. To learn fine-grained features, we\ninnovate a dual-stream feature learning on top of the transformer to learn\ndiscriminative global and local features. (2) Besides, we adopt a Bayesian\nlearning scheme with a dynamically constructed similarity matrix to learn\ncompact binary hash codes. The entire framework is jointly trained in an\nend-to-end manner.~To the best of our knowledge, this is the first work to\ntackle deep hashing learning problems without convolutional neural networks\n(\\textit{CNNs}). We perform comprehensive experiments on three widely-studied\ndatasets: \\textbf{CIFAR-10}, \\textbf{NUSWIDE} and \\textbf{IMAGENET}. The\nexperiments have evidenced our superiority against the existing\nstate-of-the-art deep hashing methods. Specifically, we achieve 8.2%, 2.6%,\n12.7% performance gains in terms of average \\textit{mAP} for different hash\nbit lengths on three public datasets, respectively.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Tools-&-Libraries", "Image-Retrieval", "Hashing-Methods", "Multimodal-Retrieval", "Scalability"], "tsne_embedding": [-2.3116586208343506, 5.765270709991455], "cluster": 6}, {"key": "chen2022approximate", "year": "2022", "citations": "12", "title": "Approximate Nearest Neighbor Search Under Neural Similarity Metric For Large-scale Recommendation", "abstract": "<p>Model-based methods for recommender systems have been studied extensively for\nyears. Modern recommender systems usually resort to 1) representation learning\nmodels which define user-item preference as the distance between their\nembedding representations, and 2) embedding-based Approximate Nearest Neighbor\n(ANN) search to tackle the efficiency problem introduced by large-scale corpus.\nWhile providing efficient retrieval, the embedding-based retrieval pattern also\nlimits the model capacity since the form of user-item preference measure is\nrestricted to the distance between their embedding representations. However,\nfor other more precise user-item preference measures, e.g., preference scores\ndirectly derived from a deep neural network, they are computationally\nintractable because of the lack of an efficient retrieval method, and an\nexhaustive search for all user-item pairs is impractical. In this paper, we\npropose a novel method to extend ANN search to arbitrary matching functions,\ne.g., a deep neural network. Our main idea is to perform a greedy walk with a\nmatching function in a similarity graph constructed from all items. To solve\nthe problem that the similarity measures of graph construction and user-item\nmatching function are heterogeneous, we propose a pluggable adversarial\ntraining task to ensure the graph search with arbitrary matching function can\nachieve fairly high precision. Experimental results in both open source and\nindustry datasets demonstrate the effectiveness of our method. The proposed\nmethod has been fully deployed in the Taobao display advertising platform and\nbrings a considerable advertising revenue increase. We also summarize our\ndetailed experiences in deployment in this paper.</p>\n", "tags": ["Datasets", "Evaluation", "Graph-Based-Ann", "CIKM", "Efficiency", "Recommender-Systems", "Robustness", "Similarity-Search", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [42.321083068847656, 4.626150608062744], "cluster": 9}, {"key": "chen2022deep", "year": "2022", "citations": "122", "title": "Deep Learning To Ternary Hash Codes By Continuation", "abstract": "<p>Recently, it has been observed that {0,1,-1}-ternary codes which are simply\ngenerated from deep features by hard thresholding, tend to outperform\n{-1,1}-binary codes in image retrieval. To obtain better ternary codes, we for\nthe first time propose to jointly learn the features with the codes by\nappending a smoothed function to the networks. During training, the function\ncould evolve into a non-smoothed ternary function by a continuation method. The\nmethod circumvents the difficulty of directly training discrete functions and\nreduces the quantization errors of ternary codes. Experiments show that the\ngenerated codes indeed could achieve higher retrieval accuracy.</p>\n", "tags": ["Compact-Codes", "Quantization", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-5.567004680633545, -4.685736179351807], "cluster": 1}, {"key": "chen2022finger", "year": "2023", "citations": "13", "title": "FINGER: Fast Inference For Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate K-Nearest Neighbor Search (AKNNS) has now become ubiquitous in\nmodern applications, for example, as a fast search procedure with two tower\ndeep learning models. Graph-based methods for AKNNS in particular have received\ngreat attention due to their superior performance. These methods rely on greedy\ngraph search to traverse the data points as embedding vectors in a database.\nUnder this greedy search scheme, we make a key observation: many distance\ncomputations do not influence search updates so these computations can be\napproximated without hurting performance. As a result, we propose FINGER, a\nfast inference method to achieve efficient graph search. FINGER approximates\nthe distance function by estimating angles between neighboring residual vectors\nwith low-rank bases and distribution matching. The approximated distance can be\nused to bypass unnecessary computations, which leads to faster searches.\nEmpirically, accelerating a popular graph-based method named HNSW by FINGER is\nshown to outperform existing graph-based methods by 20%-60% across different\nbenchmark datasets.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Datasets"], "tsne_embedding": [54.64308166503906, 7.667208671569824], "cluster": 9}, {"key": "chen2022hivlp", "year": "2022", "citations": "4", "title": "Hivlp: Hierarchical Vision-language Pre-training For Fast Image-text Retrieval", "abstract": "<p>In the past few years, the emergence of vision-language pre-training (VLP)\nhas brought cross-modal retrieval to a new era. However, due to the latency and\ncomputation demand, it is commonly challenging to apply VLP in a real-time\nonline retrieval system. To alleviate the defect, this paper proposes a\n\\textbf{Hi}erarchical \\textbf{V}ision-\\textbf{}Language \\textbf{P}re-Training\n(\\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a\nnovel hierarchical retrieval objective, which uses the representation of\ndifferent dimensions for coarse-to-fine ITR, i.e., using low-dimensional\nrepresentation for large-scale coarse retrieval and high-dimensional\nrepresentation for small-scale fine retrieval. We evaluate our proposed HiVLP\non two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.\nExtensive experiments demonstrate that our HiVLP not only has fast inference\nspeed but also can be easily scaled to large-scale ITR scenarios. The detailed\nresults show that HiVLP is \\(1,427\\)\\(\\sim\\)\\(120,649\\times\\) faster than the\nfusion-based model UNITER and 2\\(\\sim\\)5 faster than the fastest embedding-based\nmodel LightingDot in different candidate scenarios. It also achieves about +4.9\nAR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable\nperformance with the state-of-the-art (SOTA) fusion-based model METER.</p>\n", "tags": ["Text-Retrieval", "Efficiency", "Scalability", "Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-38.6346549987793, 8.54305362701416], "cluster": 0}, {"key": "chen2022intra", "year": "2022", "citations": "6", "title": "Intra-modal Constraint Loss For Image-text Retrieval", "abstract": "<p>Cross-modal retrieval has drawn much attention in both computer vision and\nnatural language processing domains. With the development of convolutional and\nrecurrent neural networks, the bottleneck of retrieval across image-text\nmodalities is no longer the extraction of image and text features but an\nefficient loss function learning in embedding space. Many loss functions try to\ncloser pairwise features from heterogeneous modalities. This paper proposes a\nmethod for learning joint embedding of images and texts using an intra-modal\nconstraint loss function to reduce the violation of negative pairs from the\nsame homogeneous modality. Experimental results show that our approach\noutperforms state-of-the-art bi-directional image-text retrieval methods on\nFlickr30K and Microsoft COCO datasets. Our code is publicly available:\nhttps://github.com/CanonChen/IMC.</p>\n", "tags": ["Multimodal-Retrieval", "Text-Retrieval", "Datasets"], "tsne_embedding": [-9.68291187286377, -10.288497924804688], "cluster": 1}, {"key": "chen2022learning", "year": "2022", "citations": "24", "title": "Learning Binarized Graph Representations With Multi-faceted Quantization Reinforcement For Top-k Recommendation", "abstract": "<p>Learning vectorized embeddings is at the core of various recommender systems\nfor user-item matching. To perform efficient online inference, representation\nquantization, aiming to embed the latent features by a compact sequence of\ndiscrete numbers, recently shows the promising potentiality in optimizing both\nmemory and computation overheads. However, existing work merely focuses on\nnumerical quantization whilst ignoring the concomitant information loss issue,\nwhich, consequently, leads to conspicuous performance degradation. In this\npaper, we propose a novel quantization framework to learn Binarized Graph\nRepresentations for Top-K Recommendation (BiGeaR). BiGeaR introduces\nmulti-faceted quantization reinforcement at the pre-, mid-, and post-stage of\nbinarized representation learning, which substantially retains the\nrepresentation informativeness against embedding binarization. In addition to\nsaving the memory footprint, BiGeaR further develops solid online inference\nacceleration with bitwise operations, providing alternative flexibility for the\nrealistic deployment. The empirical results over five large real-world\nbenchmarks show that BiGeaR achieves about 22%~40% performance improvement over\nthe state-of-the-art quantization-based recommender system, and recovers about\n95%~102% of the performance capability of the best full-precision counterpart\nwith over 8x time and space reduction.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Memory-Efficiency", "Tools-&-Libraries", "Evaluation", "KDD"], "tsne_embedding": [51.33017349243164, -3.196359634399414], "cluster": 9}, {"key": "chen2022multi", "year": "2022", "citations": "19", "title": "Multi-level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-tagged Photos", "abstract": "<p>Geo-tagged photo based tourist attraction recommendation can discover users\u2019\ntravel preferences from their taken photos, so as to recommend suitable tourist\nattractions to them. However, existing visual content based methods cannot\nfully exploit the user and tourist attraction information of photos to extract\nvisual features, and do not differentiate the significances of different\nphotos. In this paper, we propose multi-level visual similarity based\npersonalized tourist attraction recommendation using geo-tagged photos (MEAL).\nMEAL utilizes the visual contents of photos and interaction behavior data to\nobtain the final embeddings of users and tourist attractions, which are then\nused to predict the visit probabilities. Specifically, by crossing the user and\ntourist attraction information of photos, we define four visual similarity\nlevels and introduce a corresponding quintuplet loss to embed the visual\ncontents of photos. In addition, to capture the significances of different\nphotos, we exploit the self-attention mechanism to obtain the visual\nrepresentations of users and tourist attractions. We conducted experiments on a\ndataset crawled from Flickr, and the experimental results proved the advantage\nof this method.</p>\n", "tags": ["Recommender-Systems", "Datasets"], "tsne_embedding": [-40.51207733154297, 27.202083587646484], "cluster": 0}, {"key": "chen2022place", "year": "2023", "citations": "0", "title": "Place Recognition Under Occlusion And Changing Appearance Via Disentangled Representations", "abstract": "<p>Place recognition is a critical and challenging task for mobile robots,\naiming to retrieve an image captured at the same place as a query image from a\ndatabase. Existing methods tend to fail while robots move autonomously under\nocclusion (e.g., car, bus, truck) and changing appearance (e.g., illumination\nchanges, seasonal variation). Because they encode the image into only one code,\nentangling place features with appearance and occlusion features. To overcome\nthis limitation, we propose PROCA, an unsupervised approach to decompose the\nimage representation into three codes: a place code used as a descriptor to\nretrieve images, an appearance code that captures appearance properties, and an\nocclusion code that encodes occlusion content. Extensive experiments show that\nour model outperforms the state-of-the-art methods. Our code and data are\navailable at https://github.com/rover-xingyu/PROCA.</p>\n", "tags": ["Unsupervised"], "tsne_embedding": [-37.16470718383789, -8.805158615112305], "cluster": 5}, {"key": "chen2022sedr", "year": "2022", "citations": "4", "title": "Sedr: Segment Representation Learning For Long Documents Dense Retrieval", "abstract": "<p>Recently, Dense Retrieval (DR) has become a promising solution to document\nretrieval, where document representations are used to perform effective and\nefficient semantic search. However, DR remains challenging on long documents,\ndue to the quadratic complexity of its Transformer-based encoder and the finite\ncapacity of a low-dimension embedding. Current DR models use suboptimal\nstrategies such as truncating or splitting-and-pooling to long documents\nleading to poor utilization of whole document information. In this work, to\ntackle this problem, we propose Segment representation learning for long\ndocuments Dense Retrieval (SeDR). In SeDR, Segment-Interaction Transformer is\nproposed to encode long documents into document-aware and segment-sensitive\nrepresentations, while it holds the complexity of splitting-and-pooling and\noutperforms other segment-interaction patterns on DR. Since GPU memory\nrequirements for long document encoding causes insufficient negatives for DR\ntraining, Late-Cache Negative is further proposed to provide additional cache\nnegatives for optimizing representation learning. Experiments on MS MARCO and\nTREC-DL datasets show that SeDR achieves superior performance among DR models,\nand confirm the effectiveness of SeDR on long document retrieval.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [6.9550347328186035, -19.095067977905273], "cluster": 7}, {"key": "chen2023bipartite", "year": "2023", "citations": "15", "title": "Bipartite Graph Convolutional Hashing For Effective And Efficient Top-n Search In Hamming Space", "abstract": "<p>Searching on bipartite graphs is basal and versatile to many real-world Web\napplications, e.g., online recommendation, database retrieval, and\nquery-document searching. Given a query node, the conventional approaches rely\non the similarity matching with the vectorized node embeddings in the\ncontinuous Euclidean space. To efficiently manage intensive similarity\ncomputation, developing hashing techniques for graph structured data has\nrecently become an emerging research direction. Despite the retrieval\nefficiency in Hamming space, prior work is however confronted with catastrophic\nperformance decay. In this work, we investigate the problem of hashing with\nGraph Convolutional Network on bipartite graphs for effective Top-N search. We\npropose an end-to-end Bipartite Graph Convolutional Hashing approach, namely\nBGCH, which consists of three novel and effective modules: (1) adaptive graph\nconvolutional hashing, (2) latent feature dispersion, and (3) Fourier\nserialized gradient estimation. Specifically, the former two modules achieve\nthe substantial retention of the structural information against the inevitable\ninformation loss in hash encoding; the last module develops Fourier Series\ndecomposition to the hashing function in the frequency domain mainly for more\naccurate gradient estimation. The extensive experiments on six real-world\ndatasets not only show the performance superiority over the competing\nhashing-based counterparts, but also demonstrate the effectiveness of all\nproposed model components contained therein.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Recommender-Systems", "Datasets", "Evaluation"], "tsne_embedding": [51.92206954956055, -0.3148801326751709], "cluster": 9}, {"key": "chen2023ca", "year": "2024", "citations": "6", "title": "Ca-jaccard: Camera-aware Jaccard Distance For Person Re-identification", "abstract": "<p>Person re-identification (re-ID) is a challenging task that aims to learn\ndiscriminative features for person retrieval. In person re-ID, Jaccard distance\nis a widely used distance metric, especially in re-ranking and clustering\nscenarios. However, we discover that camera variation has a significant\nnegative impact on the reliability of Jaccard distance. In particular, Jaccard\ndistance calculates the distance based on the overlap of relevant neighbors.\nDue to camera variation, intra-camera samples dominate the relevant neighbors,\nwhich reduces the reliability of the neighbors by introducing intra-camera\nnegative samples and excluding inter-camera positive samples. To overcome this\nproblem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that\nleverages camera information to enhance the reliability of Jaccard distance.\nSpecifically, we design camera-aware k-reciprocal nearest neighbors (CKRNNs) to\nfind k-reciprocal nearest neighbors on the intra-camera and inter-camera\nranking lists, which improves the reliability of relevant neighbors and\nguarantees the contribution of inter-camera samples in the overlap. Moreover,\nwe propose a camera-aware local query expansion (CLQE) to mine reliable samples\nin relevant neighbors by exploiting camera variation as a strong constraint and\nassign these samples higher weights in overlap, further improving the\nreliability. Our CA-Jaccard distance is simple yet effective and can serve as a\ngeneral distance metric for person re-ID methods with high reliability and low\ncomputational cost. Extensive experiments demonstrate the effectiveness of our\nmethod.</p>\n", "tags": ["Re-Ranking", "CVPR", "Distance-Metric-Learning", "Hybrid-Ann-Methods"], "tsne_embedding": [-30.334402084350586, 9.40182876586914], "cluster": 0}, {"key": "chen2023end", "year": "2023", "citations": "0", "title": "End-to-end Retrieval With Learned Dense And Sparse Representations Using Lucene", "abstract": "<p>The bi-encoder architecture provides a framework for understanding\nmachine-learned retrieval models based on dense and sparse vector\nrepresentations. Although these representations capture parametric realizations\nof the same underlying conceptual framework, their respective implementations\nof top-\\(k\\) similarity search require the coordination of different software\ncomponents (e.g., inverted indexes, HNSW indexes, and toolkits for neural\ninference), often knitted together in complex architectures. In this work, we\nask the following question: What\u2019s the simplest design, in terms of requiring\nthe fewest changes to existing infrastructure, that can support end-to-end\nretrieval with modern dense and sparse representations? The answer appears to\nbe that Lucene is sufficient, as we demonstrate in Anserini, a toolkit for\nreproducible information retrieval research. That is, effective retrieval with\nmodern single-vector neural models can be efficiently performed directly in\nJava on the CPU. We examine the implications of this design for information\nretrieval researchers pushing the state of the art as well as for software\nengineers building production search systems.</p>\n", "tags": ["Graph-Based-Ann", "Tools-&-Libraries", "Similarity-Search"], "tsne_embedding": [35.15209197998047, -18.832901000976562], "cluster": 7}, {"key": "chen2023hessian", "year": "2023", "citations": "0", "title": "Hessian-aware Quantized Node Embeddings For Recommendation", "abstract": "<p>Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nrecommender systems. Nevertheless, the process of searching and ranking from a\nlarge item corpus usually requires high latency, which limits the widespread\ndeployment of GNNs in industry-scale applications. To address this issue, many\nmethods compress user/item representations into the binary embedding space to\nreduce space requirements and accelerate inference. Also, they use the\nStraight-through Estimator (STE) to prevent vanishing gradients during\nback-propagation. However, the STE often causes the gradient mismatch problem,\nleading to sub-optimal results.\n  In this work, we present the Hessian-aware Quantized GNN (HQ-GNN) as an\neffective solution for discrete representations of users/items that enable fast\nretrieval. HQ-GNN is composed of two components: a GNN encoder for learning\ncontinuous node embeddings and a quantized module for compressing\nfull-precision embeddings into low-bit ones. Consequently, HQ-GNN benefits from\nboth lower memory requirements and faster inference speeds compared to vanilla\nGNNs. To address the gradient mismatch problem in STE, we further consider the\nquantized errors and its second-order derivatives for better stability. The\nexperimental results on several large-scale datasets show that HQ-GNN achieves\na good balance between latency and performance.</p>\n", "tags": ["Hashing-Methods", "Recommender-Systems", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [52.681888580322266, 1.2287381887435913], "cluster": 9}, {"key": "chen2023stair", "year": "2023", "citations": "8", "title": "STAIR: Learning Sparse Text And Image Representation In Grounded Tokens", "abstract": "<p>Image and text retrieval is one of the foundational tasks in the vision and\nlanguage domain with multiple real-world applications. State-of-the-art\napproaches, e.g. CLIP, ALIGN, represent images and texts as dense embeddings\nand calculate the similarity in the dense embedding space as the matching\nscore. On the other hand, sparse semantic features like bag-of-words models are\nmore interpretable, but believed to suffer from inferior accuracy than dense\nrepresentations. In this work, we show that it is possible to build a sparse\nsemantic representation that is as powerful as, or even better than, dense\npresentations. We extend the CLIP model and build a sparse text and image\nrepresentation (STAIR), where the image and text are mapped to a sparse token\nspace. Each token in the space is a (sub-)word in the vocabulary, which is not\nonly interpretable but also easy to integrate with existing information\nretrieval systems. STAIR model significantly outperforms a CLIP model with\n+\\(4.9%\\) and +\\(4.3%\\) absolute Recall@1 improvement on COCO-5k\ntext\\(\\rightarrow\\)image and image\\(\\rightarrow\\)text retrieval respectively. It\nalso achieved better performance on both of ImageNet zero-shot and linear\nprobing compared to CLIP.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Text-Retrieval", "EMNLP"], "tsne_embedding": [6.299210548400879, -15.117951393127441], "cluster": 7}, {"key": "chen2023supervised", "year": "2023", "citations": "0", "title": "Supervised Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Deep hashing has shown to be a complexity-efficient solution for the\nApproximate Nearest Neighbor search problem in high dimensional space. Many\nmethods usually build the loss function from pairwise or triplet data points to\ncapture the local similarity structure. Other existing methods construct the\nsimilarity graph and consider all points simultaneously. Auto-encoding\nTwin-bottleneck Hashing is one such method that dynamically builds the graph.\nSpecifically, each input data is encoded into a binary code and a continuous\nvariable, or the so-called twin bottlenecks. The similarity graph is then\ncomputed from these binary codes, which get updated consistently during the\ntraining. In this work, we generalize the original model into a supervised deep\nhashing network by incorporating the label information. In addition, we examine\nthe differences of codes structure between these two networks and consider the\nclass imbalance problem especially in multi-labeled datasets. Experiments on\nthree datasets yield statistically significant improvement against the original\nmodel. Results are also comparable and competitive to other supervised methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [11.361698150634766, -5.696520805358887], "cluster": 6}, {"key": "chen2024deep", "year": "2025", "citations": "1", "title": "Deep Class-guided Hashing For Multi-label Cross-modal Retrieval", "abstract": "<p>Deep hashing, due to its low cost and efficient retrieval advantages, is\nwidely valued in cross-modal retrieval. However, existing cross-modal hashing\nmethods either explore the relationships between data points, which inevitably\nleads to intra-class dispersion, or explore the relationships between data\npoints and categories while ignoring the preservation of inter-class structural\nrelationships, resulting in the generation of suboptimal hash codes. How to\nmaintain both intra-class aggregation and inter-class structural relationships,\nIn response to this issue, this paper proposes a DCGH method. Specifically, we\nuse proxy loss as the mainstay to maintain intra-class aggregation of data,\ncombined with pairwise loss to maintain inter-class structural relationships,\nand on this basis, further propose a variance constraint to address the\nsemantic bias issue caused by the combination. A large number of comparative\nexperiments on three benchmark datasets show that the DCGH method has\ncomparable or even better performance compared to existing cross-modal\nretrieval methods. The code for the implementation of our DCGH framework is\navailable at https://github.com/donnotnormal/DCGH.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Similarity-Search", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [9.024234771728516, 6.977506637573242], "cluster": 6}, {"key": "chen2024digital", "year": "2024", "citations": "0", "title": "Digital Fingerprinting On Multimedia: A Survey", "abstract": "<p>The explosive growth of multimedia content in the digital economy era has\nbrought challenges in content recognition, copyright protection, and data\nmanagement. As an emerging content management technology, perceptual hash-based\ndigital fingerprints, serving as compact summaries of multimedia content, have\nbeen widely adopted for efficient multimedia content identification and\nretrieval across different modalities (e.g., text, image, video, audio),\nattracting significant attention from both academia and industry. Despite the\nincreasing applications of digital fingerprints, there is a lack of systematic\nand comprehensive literature review on multimedia digital fingerprints. This\nsurvey aims to fill this gap and provide an important resource for researchers\nstudying the details and related advancements of multimedia digital\nfingerprints. The survey first introduces the definition, characteristics, and\nrelated concepts (including hash functions, granularity, similarity measures,\netc.) of digital fingerprints. It then focuses on analyzing and summarizing the\nalgorithms for extracting unimodal fingerprints of different types of digital\ncontent, including text fingerprints, image fingerprints, video fingerprints,\nand audio fingerprints. Particularly, it provides an in-depth review and\nsummary of deep learning-based fingerprints. Additionally, the survey\nelaborates on the various practical applications of digital fingerprints and\noutlines the challenges and potential future research directions. The goal is\nto promote the continued development of multimedia digital fingerprint\nresearch.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods"], "tsne_embedding": [16.011117935180664, -33.51089859008789], "cluster": 7}, {"key": "chen2024efficient", "year": "2024", "citations": "0", "title": "Efficient Ternary Weight Embedding Model: Bridging Scalability And Performance", "abstract": "<p>Embedding models have become essential tools in both natural language\nprocessing and computer vision, enabling efficient semantic search,\nrecommendation, clustering, and more. However, the high memory and\ncomputational demands of full-precision embeddings pose challenges for\ndeployment in resource-constrained environments, such as real-time\nrecommendation systems. In this work, we propose a novel finetuning framework\nto ternary-weight embedding models, which reduces memory and computational\noverhead while maintaining high performance. To apply ternarization to\npre-trained embedding models, we introduce self-taught knowledge distillation\nto finalize the ternary-weights of the linear layers. With extensive\nexperiments on public text and vision datasets, we demonstrated that without\nsacrificing effectiveness, the ternarized model consumes low memory usage and\nhas low latency in the inference stage with great efficiency. In practical\nimplementations, embedding models are typically integrated with Approximate\nNearest Neighbor (ANN) search. Our experiments combining ternary embedding with\nANN search yielded impressive improvement in both accuracy and computational\nefficiency. The repository is available at here.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Similarity-Search", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-2.190892219543457, -28.50828742980957], "cluster": 3}, {"key": "chen2024exploring", "year": "2024", "citations": "0", "title": "Exploring The Meaningfulness Of Nearest Neighbor Search In High-dimensional Space", "abstract": "<p>Dense high dimensional vectors are becoming increasingly vital in fields such\nas computer vision, machine learning, and large language models (LLMs), serving\nas standard representations for multimodal data. Now the dimensionality of\nthese vector can exceed several thousands easily. Despite the nearest neighbor\nsearch (NNS) over these dense high dimensional vectors have been widely used\nfor retrieval augmented generation (RAG) and many other applications, the\neffectiveness of NNS in such a high-dimensional space remains uncertain, given\nthe possible challenge caused by the \u201ccurse of dimensionality.\u201d To address\nabove question, in this paper, we conduct extensive NNS studies with different\ndistance functions, such as \\(L_1\\) distance, \\(L_2\\) distance and\nangular-distance, across diverse embedding datasets, of varied types,\ndimensionality and modality. Our aim is to investigate factors influencing the\nmeaningfulness of NNS. Our experiments reveal that high-dimensional text\nembeddings exhibit increased resilience as dimensionality rises to higher\nlevels when compared to random vectors. This resilience suggests that text\nembeddings are less affected to the \u201ccurse of dimensionality,\u201d resulting in\nmore meaningful NNS outcomes for practical use. Additionally, the choice of\ndistance function has minimal impact on the relevance of NNS. Our study shows\nthe effectiveness of the embedding-based data representation method and can\noffer opportunity for further optimization of dense vector-related\napplications.</p>\n", "tags": ["Datasets"], "tsne_embedding": [7.782464981079102, 29.073776245117188], "cluster": 4}, {"key": "chen2024how", "year": "2024", "citations": "1", "title": "How To Make Cross Encoder A Good Teacher For Efficient Image-text Retrieval?", "abstract": "<p>Dominant dual-encoder models enable efficient image-text retrieval but suffer\nfrom limited accuracy while the cross-encoder models offer higher accuracy at\nthe expense of efficiency. Distilling cross-modality matching knowledge from\ncross-encoder to dual-encoder provides a natural approach to harness their\nstrengths. Thus we investigate the following valuable question: how to make\ncross-encoder a good teacher for dual-encoder? Our findings are threefold:(1)\nCross-modal similarity score distribution of cross-encoder is more concentrated\nwhile the result of dual-encoder is nearly normal making vanilla logit\ndistillation less effective. However ranking distillation remains practical as\nit is not affected by the score distribution.(2) Only the relative order\nbetween hard negatives conveys valid knowledge while the order information\nbetween easy negatives has little significance.(3) Maintaining the coordination\nbetween distillation loss and dual-encoder training loss is beneficial for\nknowledge transfer. Based on these findings we propose a novel Contrastive\nPartial Ranking Distillation (CPRD) method which implements the objective of\nmimicking relative order between hard negative samples with contrastive\nlearning. This approach coordinates with the training of the dual-encoder\neffectively transferring valid knowledge from the cross-encoder to the\ndual-encoder. Extensive experiments on image-text retrieval and ranking tasks\nshow that our method surpasses other distillation methods and significantly\nimproves the accuracy of dual-encoder.</p>\n", "tags": ["Efficiency", "CVPR", "Text-Retrieval"], "tsne_embedding": [4.475945472717285, 20.853424072265625], "cluster": 8}, {"key": "chen2024magicpig", "year": "2024", "citations": "1", "title": "Magicpig: LSH Sampling For Efficient LLM Generation", "abstract": "<p>Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to \\(5\\times\\) across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [31.893325805664062, 18.68514060974121], "cluster": 2}, {"key": "chen2024roargraph", "year": "2024", "citations": "4", "title": "Roargraph: A Projected Bipartite Graph For Efficient Cross-modal Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is a fundamental and critical\ncomponent in many applications, including recommendation systems and large\nlanguage model-based applications. With the advancement of multimodal neural\nmodels, which transform data from different modalities into a shared\nhigh-dimensional space as feature vectors, cross-modal ANNS aims to use the\ndata vector from one modality (e.g., texts) as the query to retrieve the most\nsimilar items from another (e.g., images or videos). However, there is an\ninherent distribution gap between embeddings from different modalities, and\ncross-modal queries become Out-of-Distribution (OOD) to the base data.\nConsequently, state-of-the-art ANNS approaches suffer poor performance for OOD\nworkloads. In this paper, we quantitatively analyze the properties of the OOD\nworkloads to gain an understanding of their ANNS efficiency. Unlike\nsingle-modal workloads, we reveal OOD queries spatially deviate from base data,\nand the k-nearest neighbors of an OOD query are distant from each other in the\nembedding space. The property breaks the assumptions of existing ANNS\napproaches and mismatches their design for efficient search. With insights from\nthe OOD workloads, we propose pRojected bipartite Graph (RoarGraph), an\nefficient ANNS graph index built under the guidance of query distribution.\nExtensive experiments show that RoarGraph significantly outperforms\nstate-of-the-art approaches on modern cross-modal datasets, achieving up to\n3.56x faster search speed at a 90% recall rate for OOD queries.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [21.717397689819336, 35.0555419921875], "cluster": 4}, {"key": "chen2024supervised", "year": "2024", "citations": "2", "title": "Supervised Consensus Anchor Graph Hashing For Cross Modal Retrieval", "abstract": "<p>The target of cross-modal hashing is to embed heterogeneous multimedia data into a common low-dimensional Hamming space, which plays a pivotal part in multimedia retrieval due to the emergence of big multimodal data. Recently, matrix factorization has achieved great success in cross-modal hashing. However, how to effectively use label information and local geometric structure is still a challenging problem for these approaches. To address this issue, we propose a cross-modal hashing method based on collective matrix factorization, which considers both the label consistency across different modalities and the local geometric consistency in each modality. These two elements are formulated as a graph Laplacian term in the objective function, leading to a substantial improvement on the discriminative power of latent semantic features obtained by collective matrix factorization. Moreover, the proposed method learns unified hash codes for different modalities of an instance to facilitate cross-modal search, and the objective function is solved using an iterative strategy. The experimental results on two benchmark data sets show the effectiveness of the proposed method and its superiority over state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Supervised", "Multimodal-Retrieval", "Evaluation", "Hashing-Methods"], "tsne_embedding": [51.88595199584961, -6.804538726806641], "cluster": 9}, {"key": "chen2024towards", "year": "2024", "citations": "2", "title": "Towards Effective Top-n Hamming Search Via Bipartite Graph Contrastive Hashing", "abstract": "<p>Searching on bipartite graphs serves as a fundamental task for various\nreal-world applications, such as recommendation systems, database retrieval,\nand document querying. Conventional approaches rely on similarity matching in\ncontinuous Euclidean space of vectorized node embeddings. To handle intensive\nsimilarity computation efficiently, hashing techniques for graph-structured\ndata have emerged as a prominent research direction. However, despite the\nretrieval efficiency in Hamming space, previous studies have encountered\ncatastrophic performance decay. To address this challenge, we investigate the\nproblem of hashing with Graph Convolutional Network for effective Top-N search.\nOur findings indicate the learning effectiveness of incorporating hashing\ntechniques within the exploration of bipartite graph reception fields, as\nopposed to simply treating hashing as post-processing to output embeddings. To\nfurther enhance the model performance, we advance upon these findings and\npropose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel\ndual augmentation approach to both intermediate information and hash code\noutputs in the latent feature spaces, thereby producing more expressive and\nrobust hash codes within a dual self-supervised learning paradigm.\nComprehensive empirical analyses on six real-world benchmarks validate the\neffectiveness of our dual feature contrastive learning in boosting the\nperformance of BGCH+ compared to existing approaches.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Recommender-Systems", "Supervised", "Re-Ranking", "Evaluation"], "tsne_embedding": [51.61913299560547, -0.5878013968467712], "cluster": 9}, {"key": "chen2025deep", "year": "2018", "citations": "60", "title": "Deep Hashing Via Discrepancy Minimization", "abstract": "<p>This paper presents a discrepancy minimizing model to\naddress the discrete optimization problem in hashing learning. The discrete optimization introduced by binary constraint is an NP-hard mixed integer programming problem.\nIt is usually addressed by relaxing the binary variables into\ncontinuous variables to adapt to the gradient based learning of hashing functions, especially the training of deep\nneural networks. To deal with the objective discrepancy\ncaused by relaxation, we transform the original binary optimization into differentiable optimization problem over hash\nfunctions through series expansion. This transformation decouples the binary constraint and the similarity preserving\nhashing function optimization. The transformed objective\nis optimized in a tractable alternating optimization framework with gradual discrepancy minimization. Extensive experimental results on three benchmark datasets validate the\nefficacy of the proposed discrepancy minimizing hashing.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Tools-&-Libraries", "Datasets", "Evaluation", "Neural-Hashing"], "tsne_embedding": [16.877710342407227, -4.608850479125977], "cluster": 6}, {"key": "chen2025enhanced", "year": "2020", "citations": "27", "title": "Enhanced Discrete Multi-modal Hashing: More Constraints Yet Less Time To Learn", "abstract": "<p>Due to the exponential growth of multimedia data, multi-modal hashing as a promising technique to make cross-view retrieval scalable is attracting more and more attention. However, most of the existing multi-modal hashing methods either divide the learning process unnaturally into two separate stages or treat the discrete optimization problem simplistically as a continuous one, which leads to suboptimal results. Recently, a few discrete multi-modal hashing methods that try to address such issues have emerged, but they still ignore several important discrete constraints (such as the balance and decorrelation of hash bits). In this paper, we overcome those limitations by proposing a novel method named \u201cEnhanced Discrete Multi-modal Hashing (EDMH)\u201d which learns binary codes and hashing functions simultaneously from the pairwise similarity matrix of data, under the aforementioned discrete constraints. Although the model of EDMH looks a lot more complex than the other models for multi-modal hashing, we are actually able to develop a fast iterative learning algorithm for it, since the subproblems of its optimization all have closed-form solutions after introducing two auxiliary variables. Our experimental results on three real-world datasets have demonstrated that EDMH not only performs much better than state-of-the-art competitors but also runs much faster than them.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Datasets"], "tsne_embedding": [16.542814254760742, -10.373635292053223], "cluster": 7}, {"key": "chen2025learning", "year": "2025", "citations": "0", "title": "Learning Binarized Representations With Pseudo-positive Sample Enhancement For Efficient Graph Collaborative Filtering", "abstract": "<p>Learning vectorized embeddings is fundamental to many recommender systems for user-item matching. To enable efficient online inference, representation binarization, which embeds latent features into compact binary sequences, has recently shown significant promise in optimizing both memory usage and computational overhead. However, existing approaches primarily focus on numerical quantization, neglecting the associated information loss, which often results in noticeable performance degradation. To address these issues, we study the problem of graph representation binarization for efficient collaborative filtering. Our findings indicate that explicitly mitigating information loss at various stages of embedding binarization has a significant positive impact on performance. Building on these insights, we propose an enhanced framework, BiGeaR++, which specifically leverages supervisory signals from pseudo-positive samples, incorporating both real item data and latent embedding samples. Compared to its predecessor BiGeaR, BiGeaR++ introduces a fine-grained inference distillation mechanism and an effective embedding sample synthesis approach. Empirical evaluations across five real-world datasets demonstrate that the new designs in BiGeaR++ work seamlessly well with other modules, delivering substantial improvements of around 1%-10% over BiGeaR and thus achieving state-of-the-art performance compared to the competing methods. Our implementation is available at https://github.com/QueYork/BiGeaR-SS.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [51.60322952270508, -3.479734182357788], "cluster": 9}, {"key": "chen2025locality", "year": "2019", "citations": "4", "title": "Locality-sensitive Hashing For F-divergences: Mutual Information Loss And Beyond", "abstract": "<p>Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.</p>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability"], "tsne_embedding": [14.963644981384277, 45.04180145263672], "cluster": 4}, {"key": "chen2025long", "year": "2021", "citations": "9", "title": "Long-tail Hashing", "abstract": "<p>Hashing, which represents data items as compact binary codes, has\nbeen becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well\nas its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially\nbalanced datasets which are not commonly available in real-world\nscenarios. In this paper, we propose Long-Tail Hashing Network\n(LTHNet), a novel two-stage deep hashing approach that addresses\nthe problem of learning to hash for more realistic datasets where\nthe data labels roughly exhibit a long-tail distribution. Specifically,\nthe first stage is to learn relaxed embeddings of the given dataset\nwith its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those\nobtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual\nknowledge transfer between head and tail classes, and thus enrich\nimage representations for hashing. Our experiments have shown\nthat LTHNet achieves dramatic performance improvements over all\nstate-of-the-art competitors on long-tail datasets, with no or little\nsacrifice on balanced datasets. Further analyses reveal that while to\nour surprise directly manipulating class weights in the loss function\nhas little effect, the extended dynamic meta-embedding module, the\nusage of cross-entropy loss instead of square loss, and the relatively\nsmall batch-size for training all contribute to LTHNet\u2019s success.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "Image-Retrieval", "SIGIR", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [4.963158130645752, 18.886978149414062], "cluster": 8}, {"key": "chen2025strongly", "year": "2020", "citations": "50", "title": "Strongly Constrained Discrete Hashing", "abstract": "<p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [21.5539608001709, 1.978505253791809], "cluster": 6}, {"key": "chen2025supervised", "year": "2024", "citations": "2", "title": "Supervised Consensus Anchor Graph Hashing For Cross Modal Retrieval", "abstract": "<p>The target of cross-modal hashing is to embed heterogeneous multimedia data into a common low-dimensional Hamming space, which plays a pivotal part in multimedia retrieval due to the emergence of big multimodal data. Recently, matrix factorization has achieved great success in cross-modal hashing. However, how to effectively use label information and local geometric structure is still a challenging problem for these approaches. To address this issue, we propose a cross-modal hashing method based on collective matrix factorization, which considers both the label consistency across different modalities and the local geometric consistency in each modality. These two elements are formulated as a graph Laplacian term in the objective function, leading to a substantial improvement on the discriminative power of latent semantic features obtained by collective matrix factorization. Moreover, the proposed method learns unified hash codes for different modalities of an instance to facilitate cross-modal search, and the objective function is solved using an iterative strategy. The experimental results on two benchmark data sets show the effectiveness of the proposed method and its superiority over state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Supervised", "Multimodal-Retrieval", "Evaluation", "Hashing-Methods"], "tsne_embedding": [51.88595199584961, -6.804538726806641], "cluster": 9}, {"key": "chen2025two", "year": "2019", "citations": "45", "title": "A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps", "abstract": "<p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Compact-Codes", "Multimodal-Retrieval", "Hashing-Methods"], "tsne_embedding": [8.468891143798828, 2.378227710723877], "cluster": 6}, {"key": "cheng2016adaptive", "year": "2016", "citations": "0", "title": "Adaptive Training Of Random Mapping For Data Quantization", "abstract": "<p>Data quantization learns encoding results of data with certain requirements,\nand provides a broad perspective of many real-world applications to data\nhandling. Nevertheless, the results of encoder is usually limited to\nmultivariate inputs with the random mapping, and side information of binary\ncodes are hardly to mostly depict the original data patterns as possible. In\nthe literature, cosine based random quantization has attracted much attentions\ndue to its intrinsic bounded results. Nevertheless, it usually suffers from the\nuncertain outputs, and information of original data fails to be fully preserved\nin the reduced codes. In this work, a novel binary embedding method, termed\nadaptive training quantization (ATQ), is proposed to learn the ideal transform\nof random encoder, where the limitation of cosine random mapping is tackled. As\nan adaptive learning idea, the reduced mapping is adaptively calculated with\nidea of data group, while the bias of random transform is to be improved to\nhold most matching information. Experimental results show that the proposed\nmethod is able to obtain outstanding performance compared with other random\nquantization methods.</p>\n", "tags": ["Quantization", "Evaluation", "Hashing-Methods"], "tsne_embedding": [19.801191329956055, -5.970598220825195], "cluster": 6}, {"key": "cheng2020robust", "year": "2020", "citations": "48", "title": "Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval", "abstract": "<p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [5.070319175720215, 8.739265441894531], "cluster": 6}, {"key": "cheng2021cnn", "year": "2021", "citations": "4", "title": "CNN Retrieval Based Unsupervised Metric Learning For Near-duplicated Video Retrieval", "abstract": "<p>As important data carriers, the drastically increasing number of multimedia\nvideos often brings many duplicate and near-duplicate videos in the top results\nof search. Near-duplicate video retrieval (NDVR) can cluster and filter out the\nredundant contents. In this paper, the proposed NDVR approach extracts the\nframe-level video representation based on convolutional neural network (CNN)\nfeatures from fully-connected layer and aggregated intermediate convolutional\nlayers. Unsupervised metric learning is used for similarity measurement and\nfeature matching. An efficient re-ranking algorithm combined with k-nearest\nneighborhood fuses the retrieval results from two levels of features and\nfurther improves the retrieval performance. Extensive experiments on the widely\nused CC_WEB_VIDEO dataset shows that the proposed approach exhibits superior\nperformance over the state-of-the-art.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation", "Video-Retrieval", "Unsupervised"], "tsne_embedding": [-10.700414657592773, -3.486687183380127], "cluster": 1}, {"key": "cheng2021computationally", "year": "2021", "citations": "0", "title": "Computationally Efficient Learning Of Statistical Manifolds", "abstract": "<p>Analyzing high-dimensional data with manifold learning algorithms often\nrequires searching for the nearest neighbors of all observations. This presents\na computational bottleneck in statistical manifold learning when observations\nof probability distributions rather than vector-valued variables are available\nor when data size is large. We resolve this problem by proposing a new method\nfor approximation in statistical manifold learning. The novelty of our\napproximation is the strongly consistent distance estimators based on\nindependent and identically distributed samples from probability distributions.\nBy exploiting the connection between Hellinger/total variation distance for\ndiscrete distributions and the L2/L1 norm, we demonstrate that the proposed\ndistance estimators, combined with approximate nearest neighbor searching,\ncould largely improve the computational efficiency with little to no loss in\nthe accuracy of manifold embedding. The result is robust to different manifold\nlearning algorithms and different approximate nearest neighbor algorithms. The\nproposed method is applied to learning statistical manifolds of electricity\nusage. This application demonstrates how underlying structures in high\ndimensional data, including anomalies, can be visualized and identified, in a\nway that is scalable to large datasets.</p>\n", "tags": ["Efficiency", "Datasets"], "tsne_embedding": [17.376379013061523, 30.97187614440918], "cluster": 4}, {"key": "cheng2024adapting", "year": "2024", "citations": "0", "title": "Adapting Dual-encoder Vision-language Models For Paraphrased Retrieval", "abstract": "<p>In the recent years, the dual-encoder vision-language models (\\eg CLIP) have\nachieved remarkable text-to-image retrieval performance. However, we discover\nthat these models usually results in very different retrievals for a pair of\nparaphrased queries. Such behavior might render the retrieval system less\npredictable and lead to user frustration. In this work, we consider the task of\nparaphrased text-to-image retrieval where a model aims to return similar\nresults given a pair of paraphrased queries. To start with, we collect a\ndataset of paraphrased image descriptions to facilitate quantitative evaluation\nfor this task. We then hypothesize that the undesired behavior of existing\ndual-encoder model is due to their text towers which are trained on\nimage-sentence pairs and lack the ability to capture the semantic similarity\nbetween paraphrased queries. To improve on this, we investigate multiple\nstrategies for training a dual-encoder model starting from a language model\npretrained on a large text corpus. Compared to public dual-encoder models such\nas CLIP and OpenCLIP, the model trained with our best adaptation strategy\nachieves a significantly higher ranking similarity for paraphrased queries\nwhile maintaining similar zero-shot classification and retrieval accuracy.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Image-Retrieval", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-20.632081985473633, -0.6296623945236206], "cluster": 1}, {"key": "cheng2025robust", "year": "2020", "citations": "48", "title": "Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval", "abstract": "<p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [5.070087432861328, 8.739259719848633], "cluster": 6}, {"key": "chiu2018learning", "year": "2019", "citations": "24", "title": "Learning To Index For Nearest Neighbor Search", "abstract": "<p>In this study, we present a novel ranking model based on learning\nneighborhood relationships embedded in the index space. Given a query point,\nconventional approximate nearest neighbor search calculates the distances to\nthe cluster centroids, before ranking the clusters from near to far based on\nthe distances. The data indexed in the top-ranked clusters are retrieved and\ntreated as the nearest neighbor candidates for the query. However, the loss of\nquantization between the data and cluster centroids will inevitably harm the\nsearch accuracy. To address this problem, the proposed model ranks clusters\nbased on their nearest neighbor probabilities rather than the query-centroid\ndistances. The nearest neighbor probabilities are estimated by employing neural\nnetworks to characterize the neighborhood relationships, i.e., the density\nfunction of nearest neighbors with respect to the query. The proposed\nprobability-based ranking can replace the conventional distance-based ranking\nfor finding candidate clusters, and the predicted probability can be used to\ndetermine the data quantity to be retrieved from the candidate cluster. Our\nexperimental results demonstrated that the proposed ranking model could boost\nthe search performance effectively in billion-scale datasets.</p>\n", "tags": ["Quantization", "Scalability", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [45.00748062133789, 8.246111869812012], "cluster": 9}, {"key": "choi2023is", "year": "2023", "citations": "0", "title": "Is Cross-modal Information Retrieval Possible Without Training?", "abstract": "<p>Encoded representations from a pretrained deep learning model (e.g., BERT\ntext embeddings, penultimate CNN layer activations of an image) convey a rich\nset of features beneficial for information retrieval. Embeddings for a\nparticular modality of data occupy a high-dimensional space of its own, but it\ncan be semantically aligned to another by a simple mapping without training a\ndeep neural net. In this paper, we take a simple mapping computed from the\nleast squares and singular value decomposition (SVD) for a solution to the\nProcrustes problem to serve a means to cross-modal information retrieval. That\nis, given information in one modality such as text, the mapping helps us locate\na semantically equivalent data item in another modality such as image. Using\noff-the-shelf pretrained deep learning models, we have experimented the\naforementioned simple cross-modal mappings in tasks of text-to-image and\nimage-to-text retrieval. Despite simplicity, our mappings perform reasonably\nwell reaching the highest accuracy of 77% on recall@10, which is comparable to\nthose requiring costly neural net training and fine-tuning. We have improved\nthe simple mappings by contrastive learning on the pretrained models.\nContrastive learning can be thought as properly biasing the pretrained encoders\nto enhance the cross-modal mapping quality. We have further improved the\nperformance by multilayer perceptron with gating (gMLP), a simple neural\narchitecture.</p>\n", "tags": ["Self-Supervised", "Text-Retrieval", "Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-31.097890853881836, -15.34912395477295], "cluster": 5}, {"key": "chowdhury2018instance", "year": "2019", "citations": "5", "title": "Instance-based Inductive Deep Transfer Learning By Cross-dataset Querying With Locality Sensitive Hashing", "abstract": "<p>Supervised learning models are typically trained on a single dataset and the\nperformance of these models rely heavily on the size of the dataset, i.e.,\namount of data available with the ground truth. Learning algorithms try to\ngeneralize solely based on the data that is presented with during the training.\nIn this work, we propose an inductive transfer learning method that can augment\nlearning models by infusing similar instances from different learning tasks in\nthe Natural Language Processing (NLP) domain. We propose to use instance\nrepresentations from a source dataset, \\textit{without inheriting anything}\nfrom the source learning model. Representations of the instances of\n\\textit{source} \\&amp; \\textit{target} datasets are learned, retrieval of relevant\nsource instances is performed using soft-attention mechanism and\n\\textit{locality sensitive hashing}, and then, augmented into the model during\ntraining on the target dataset. Our approach simultaneously exploits the local\n\\textit{instance level information} as well as the macro statistical viewpoint\nof the dataset. Using this approach we have shown significant improvements for\nthree major news classification datasets over the baseline. Experimental\nevaluations also show that the proposed approach reduces dependency on labeled\ndata by a significant margin for comparable performance. With our proposed\ncross dataset learning procedure we show that one can achieve\ncompetitive/better performance than learning from a single dataset.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-8.097264289855957, -25.330291748046875], "cluster": 3}, {"key": "chowdhury2024nearest", "year": "2024", "citations": "0", "title": "Nearest Neighbor Normalization Improves Multimodal Retrieval", "abstract": "<p>Multimodal models leverage large-scale pre-training to achieve strong but\nstill imperfect performance on tasks such as image captioning, visual question\nanswering, and cross-modal retrieval. In this paper, we present a simple and\nefficient method for correcting errors in trained contrastive image-text\nretrieval models with no additional training, called Nearest Neighbor\nNormalization (NNN). We show an improvement on retrieval metrics in both text\nretrieval and image retrieval for all of the contrastive models that we tested\n(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used\n(MS-COCO and Flickr30k). NNN requires a reference database, but does not\nrequire any training on this database, and can even increase the retrieval\naccuracy of a model after finetuning.</p>\n", "tags": ["EMNLP", "Image-Retrieval", "Scalability", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-20.284278869628906, -19.7329044342041], "cluster": 5}, {"key": "christiani2016framework", "year": "2017", "citations": "21", "title": "A Framework For Similarity Search With Space-time Tradeoffs Using Locality-sensitive Filtering", "abstract": "<p>We present a framework for similarity search based on Locality-Sensitive\nFiltering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive\nHashing (LSH) framework to support space-time tradeoffs. Given a family of\nfilters, defined as a distribution over pairs of subsets of space with certain\nlocality-sensitivity properties, we can solve the approximate near neighbor\nproblem in \\(d\\)-dimensional space for an \\(n\\)-point data set with query time\n\\(dn^{\\rho_q+o(1)}\\), update time \\(dn^{\\rho_u+o(1)}\\), and space usage \\(dn + n^{1</p>\n<ul>\n  <li>\\rho_u + o(1)}\\). The space-time tradeoff is tied to the tradeoff between\nquery time and update time, controlled by the exponents \\(\\rho_q, \\rho_u\\) that\nare determined by the filter family. Locality-sensitive filtering was\nintroduced by Becker et al. (SODA 2016) together with a framework yielding a\nsingle, balanced, tradeoff between query time and space, further relying on the\nassumption of an efficient oracle for the filter evaluation algorithm. We\nextend the LSF framework to support space-time tradeoffs and through a\ncombination of existing techniques we remove the oracle assumption.\nBuilding on a filter family for the unit sphere by Laarhoven (arXiv 2015) we\nuse a kernel embedding technique by Rahimi &amp; Recht (NIPS 2007) to show a\nsolution to the \\((r,cr)\\)-near neighbor problem in \\(\\ell_s^d\\)-space for \\(0 &lt; s\n\\leq 2\\) with query and update exponents\n\\(\\rho_q=\\frac{c^s(1+\\lambda)^2}{(c^s+\\lambda)^2}\\) and\n\\(\\rho_u=\\frac{c^s(1-\\lambda)^2}{(c^s+\\lambda)^2}\\) where \\(\\lambda\\in[-1,1]\\) is a\ntradeoff parameter. This result improves upon the space-time tradeoff of\nKapralov (PODS 2015) and is shown to be optimal in the case of a balanced\ntradeoff. Finally, we show a lower bound for the space-time tradeoff on the\nunit sphere that matches Laarhoven\u2019s and our own upper bound in the case of\nrandom data.</li>\n</ul>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [19.604692459106445, 51.899566650390625], "cluster": 4}, {"key": "christiani2016set", "year": "2017", "citations": "45", "title": "Set Similarity Search Beyond Minhash", "abstract": "<p>We consider the problem of approximate set similarity search under\nBraun-Blanquet similarity \\(B(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} \\cap\n\\mathbf{y}| / \\max(|\\mathbf{x}|, |\\mathbf{y}|)\\). The \\((b_2, b_2)\\)-approximate\nBraun-Blanquet similarity search problem is to preprocess a collection of sets\n\\(P\\) such that, given a query set \\(\\mathbf{q}\\), if there exists \\(\\mathbf{x} \\in\nP\\) with \\(B(\\mathbf{q}, \\mathbf{x}) \\geq b_1\\), then we can efficiently return\n\\(\\mathbf{x}\u2019 \\in P\\) with \\(B(\\mathbf{q}, \\mathbf{x}\u2019) &gt; b_2\\).\n  We present a simple data structure that solves this problem with space usage\n\\(O(n^{1+\\rho}log n + \\sum_{\\mathbf{x} \\in P}|\\mathbf{x}|)\\) and query time\n\\(O(|\\mathbf{q}|n^{\\rho} log n)\\) where \\(n = |P|\\) and \\(\\rho =\nlog(1/b_1)/log(1/b_2)\\). Making use of existing lower bounds for\nlocality-sensitive hashing by O\u2019Donnell et al. (TOCT 2014) we show that this\nvalue of \\(\\rho\\) is tight across the parameter space, i.e., for every choice of\nconstants \\(0 &lt; b_2 &lt; b_1 &lt; 1\\).\n  In the case where all sets have the same size our solution strictly improves\nupon the value of \\(\\rho\\) that can be obtained through the use of\nstate-of-the-art data-independent techniques in the Indyk-Motwani\nlocality-sensitive hashing framework (STOC 1998) such as Broder\u2019s MinHash (CCS\n1997) for Jaccard similarity and Andoni et al.\u2019s cross-polytope LSH (NIPS 2015)\nfor cosine similarity. Surprisingly, even though our solution is\ndata-independent, for a large part of the parameter space we outperform the\ncurrently best data-dependent method by Andoni and Razenshteyn (STOC 2015).</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Efficiency", "Similarity-Search", "Tools-&-Libraries"], "tsne_embedding": [21.96640968322754, 50.136932373046875], "cluster": 4}, {"key": "christiani2017fast", "year": "2019", "citations": "14", "title": "Fast Locality-sensitive Hashing Frameworks For Approximate Near Neighbor Search", "abstract": "<p>The Indyk-Motwani Locality-Sensitive Hashing (LSH) framework (STOC 1998) is a\ngeneral technique for constructing a data structure to answer approximate near\nneighbor queries by using a distribution \\(\\mathcal{H}\\) over locality-sensitive\nhash functions that partition space. For a collection of \\(n\\) points, after\npreprocessing, the query time is dominated by \\(O(n^{\\rho} log n)\\) evaluations\nof hash functions from \\(\\mathcal{H}\\) and \\(O(n^{\\rho})\\) hash table lookups and\ndistance computations where \\(\\rho \\in (0,1)\\) is determined by the\nlocality-sensitivity properties of \\(\\mathcal{H}\\). It follows from a recent\nresult by Dahlgaard et al. (FOCS 2017) that the number of locality-sensitive\nhash functions can be reduced to \\(O(log^2 n)\\), leaving the query time to be\ndominated by \\(O(n^{\\rho})\\) distance computations and \\(O(n^{\\rho} log n)\\)\nadditional word-RAM operations. We state this result as a general framework and\nprovide a simpler analysis showing that the number of lookups and distance\ncomputations closely match the Indyk-Motwani framework, making it a viable\nreplacement in practice. Using ideas from another locality-sensitive hashing\nframework by Andoni and Indyk (SODA 2006) we are able to reduce the number of\nadditional word-RAM operations to \\(O(n^\\rho)\\).</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [18.433094024658203, 50.17049026489258], "cluster": 4}, {"key": "christiani2018confirmation", "year": "2018", "citations": "6", "title": "Confirmation Sampling For Exact Nearest Neighbor Search", "abstract": "<p>Locality-sensitive hashing (LSH), introduced by Indyk and Motwani in STOC\n\u201898, has been an extremely influential framework for nearest neighbor search in\nhigh-dimensional data sets. While theoretical work has focused on the\napproximate nearest neighbor problems, in practice LSH data structures with\nsuitably chosen parameters are used to solve the exact nearest neighbor problem\n(with some error probability). Sublinear query time is often possible in\npractice even for exact nearest neighbor search, intuitively because the\nnearest neighbor tends to be significantly closer than other data points.\nHowever, theory offers little advice on how to choose LSH parameters outside of\npre-specified worst-case settings.\n  We introduce the technique of confirmation sampling for solving the exact\nnearest neighbor problem using LSH. First, we give a general reduction that\ntransforms a sequence of data structures that each find the nearest neighbor\nwith a small, unknown probability, into a data structure that returns the\nnearest neighbor with probability \\(1-\\delta\\), using as few queries as possible.\nSecond, we present a new query algorithm for the LSH Forest data structure with\n\\(L\\) trees that is able to return the exact nearest neighbor of a query point\nwithin the same time bound as an LSH Forest of \\(\u03a9(L)\\) trees with internal\nparameters specifically tuned to the query and data.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [22.471044540405273, 40.63888931274414], "cluster": 4}, {"key": "christiani2019algorithms", "year": "2019", "citations": "0", "title": "Algorithms For Similarity Search And Pseudorandomness", "abstract": "<p>We study the problem of approximate near neighbor (ANN) search and show the\nfollowing results:</p>\n<ul>\n  <li>An improved framework for solving the ANN problem using locality-sensitive\nhashing, reducing the number of evaluations of locality-sensitive hash\nfunctions and the word-RAM complexity compared to the standard framework.</li>\n  <li>A framework for solving the ANN problem with space-time tradeoffs as well\nas tight upper and lower bounds for the space-time tradeoff of framework\nsolutions to the ANN problem under cosine similarity.</li>\n  <li>A novel approach to solving the ANN problem on sets along with a matching\nlower bound, improving the state of the art.</li>\n  <li>A self-tuning version of the algorithm is shown through experiments to\noutperform existing similarity join algorithms.</li>\n  <li>Tight lower bounds for asymmetric locality-sensitive hashing which has\napplications to the approximate furthest neighbor problem, orthogonal vector\nsearch, and annulus queries.</li>\n  <li>A proof of the optimality of a well-known Boolean locality-sensitive\nhashing scheme.\n  We study the problem of efficient algorithms for producing high-quality\npseudorandom numbers and obtain the following results:</li>\n  <li>A deterministic algorithm for generating pseudorandom numbers of\narbitrarily high quality in constant time using near-optimal space.</li>\n  <li>A randomized construction of a family of hash functions that outputs\npseudorandom numbers of arbitrarily high quality with space usage and running\ntime nearly matching known cell-probe lower bounds.</li>\n</ul>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods", "Distance-Metric-Learning", "Similarity-Search"], "tsne_embedding": [19.60883140563965, 45.15397644042969], "cluster": 4}, {"key": "christiani2020dartminhash", "year": "2020", "citations": "4", "title": "Dartminhash: Fast Sketching For Weighted Sets", "abstract": "<p>Weighted minwise hashing is a standard dimensionality reduction technique\nwith applications to similarity search and large-scale kernel machines. We\nintroduce a simple algorithm that takes a weighted set \\(x \\in \\mathbb{R}<em>{\\geq\n0}^{d}\\) and computes \\(k\\) independent minhashes in expected time \\(O(k log k +\n\\Vert x \\Vert</em>{0}log( \\Vert x \\Vert_1 + 1/\\Vert x \\Vert_1))\\), improving upon\nthe state-of-the-art BagMinHash algorithm (KDD \u201818) and representing the\nfastest weighted minhash algorithm for sparse data. Our experiments show\nrunning times that scale better with \\(k\\) and \\(\\Vert x \\Vert_0\\) compared to ICWS\n(ICDM \u201810) and BagMinhash, obtaining \\(10\\)x speedups in common use cases. Our\napproach also gives rise to a technique for computing fully independent\nlocality-sensitive hash values for \\((L, K)\\)-parameterized approximate near\nneighbor search under weighted Jaccard similarity in optimal expected time\n\\(O(LK + \\Vert x \\Vert_0)\\), improving on prior work even in the case of\nunweighted sets.</p>\n", "tags": ["Similarity-Search", "Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability"], "tsne_embedding": [18.972110748291016, 37.457061767578125], "cluster": 4}, {"key": "chumbalov2019scalable", "year": "2020", "citations": "1", "title": "Scalable And Efficient Comparison-based Search Without Features", "abstract": "<p>We consider the problem of finding a target object \\(t\\) using pairwise\ncomparisons, by asking an oracle questions of the form <em>\u201cWhich object from\nthe pair \\((i,j)\\) is more similar to \\(t\\)?\u201d</em>. Objects live in a space of latent\nfeatures, from which the oracle generates noisy answers. First, we consider the\n{\\em non-blind} setting where these features are accessible. We propose a new\nBayesian comparison-based search algorithm with noisy answers; it has low\ncomputational complexity yet is efficient in the number of queries. We provide\ntheoretical guarantees, deriving the form of the optimal query and proving\nalmost sure convergence to the target \\(t\\). Second, we consider the <em>blind</em>\nsetting, where the object features are hidden from the search algorithm. In\nthis setting, we combine our search method and a new distributional triplet\nembedding algorithm into one scalable learning framework called\n\\textsc{Learn2Search}. We show that the query complexity of our approach on two\nreal-world datasets is on par with the non-blind setting, which is not\nachievable using any of the current state-of-the-art embedding methods.\nFinally, we demonstrate the efficacy of our framework by conducting an\nexperiment with users searching for movie actors.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [25.241409301757812, 14.071223258972168], "cluster": 2}, {"key": "chun2021probabilistic", "year": "2021", "citations": "169", "title": "Probabilistic Embeddings For Cross-modal Retrieval", "abstract": "<p>Cross-modal retrieval methods build a common representation space for samples\nfrom multiple modalities, typically from the vision and the language domains.\nFor images and their captions, the multiplicity of the correspondences makes\nthe task particularly challenging. Given an image (respectively a caption),\nthere are multiple captions (respectively images) that equally make sense. In\nthis paper, we argue that deterministic functions are not sufficiently powerful\nto capture such one-to-many correspondences. Instead, we propose to use\nProbabilistic Cross-Modal Embedding (PCME), where samples from the different\nmodalities are represented as probabilistic distributions in the common\nembedding space. Since common benchmarks such as COCO suffer from\nnon-exhaustive annotations for cross-modal matches, we propose to additionally\nevaluate retrieval on the CUB dataset, a smaller yet clean database where all\npossible image-caption pairs are annotated. We extensively ablate PCME and\ndemonstrate that it not only improves the retrieval performance over its\ndeterministic counterpart but also provides uncertainty estimates that render\nthe embeddings more interpretable. Code is available at\nhttps://github.com/naver-ai/pcme</p>\n", "tags": ["Multimodal-Retrieval", "CVPR", "Evaluation", "Datasets"], "tsne_embedding": [-23.195396423339844, -6.8286285400390625], "cluster": 1}, {"key": "chung2017learning", "year": "2017", "citations": "63", "title": "Learning Deep Representations Of Medical Images Using Siamese Cnns With Application To Content-based Image Retrieval", "abstract": "<p>Deep neural networks have been investigated in learning latent\nrepresentations of medical images, yet most of the studies limit their approach\nin a single supervised convolutional neural network (CNN), which usually rely\nheavily on a large scale annotated dataset for training. To learn image\nrepresentations with less supervision involved, we propose a deep Siamese CNN\n(SCNN) architecture that can be trained with only binary image pair\ninformation. We evaluated the learned image representations on a task of\ncontent-based medical image retrieval using a publicly available multiclass\ndiabetic retinopathy fundus image dataset. The experimental results show that\nour proposed deep SCNN is comparable to the state-of-the-art single supervised\nCNN, and requires much less supervision for training.</p>\n", "tags": ["Supervised", "Image-Retrieval", "Datasets"], "tsne_embedding": [-41.694828033447266, -0.007825342938303947], "cluster": 0}, {"key": "cicconet2018image", "year": "2018", "citations": "20", "title": "Image Forensics: Detecting Duplication Of Scientific Images With Manipulation-invariant Image Similarity", "abstract": "<p>Manipulation and re-use of images in scientific publications is a concerning\nproblem that currently lacks a scalable solution. Current tools for detecting\nimage duplication are mostly manual or semi-automated, despite the availability\nof an overwhelming target dataset for a learning-based approach. This paper\naddresses the problem of determining if, given two images, one is a manipulated\nversion of the other by means of copy, rotation, translation, scale,\nperspective transform, histogram adjustment, or partial erasing. We propose a\ndata-driven solution based on a 3-branch Siamese Convolutional Neural Network.\nThe ConvNet model is trained to map images into a 128-dimensional space, where\nthe Euclidean distance between duplicate images is smaller than or equal to 1,\nand the distance between unique images is greater than 1. Our results suggest\nthat such an approach has the potential to improve surveillance of the\npublished and in-peer-review literature for image manipulation.</p>\n", "tags": ["Survey-Paper", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-19.909099578857422, 20.971858978271484], "cluster": 8}, {"key": "ciro2021lsh", "year": "2021", "citations": "0", "title": "LSH Methods For Data Deduplication In A Wikipedia Artificial Dataset", "abstract": "<p>This paper illustrates locality sensitive hasing (LSH) models for the\nidentification and removal of nearly redundant data in a text dataset. To\nevaluate the different models, we create an artificial dataset for data\ndeduplication using English Wikipedia articles. Area-Under-Curve (AUC) over 0.9\nwere observed for most models, with the best model reaching 0.96. Deduplication\nenables more effective model training by preventing the model from learning a\ndistribution that differs from the real one as a result of the repeated data.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [4.217134475708008, -30.302040100097656], "cluster": 3}, {"key": "clavi\u00e92024reducing", "year": "2024", "citations": "0", "title": "Reducing The Footprint Of Multi-vector Retrieval With Minimal Performance Impact Via Token Pooling", "abstract": "<p>Over the last few years, multi-vector retrieval methods, spearheaded by\nColBERT, have become an increasingly popular approach to Neural IR. By storing\nrepresentations at the token level rather than at the document level, these\nmethods have demonstrated very strong retrieval performance, especially in\nout-of-domain settings. However, the storage and memory requirements necessary\nto store the large number of associated vectors remain an important drawback,\nhindering practical adoption. In this paper, we introduce a simple\nclustering-based token pooling approach to aggressively reduce the number of\nvectors that need to be stored. This method can reduce the space &amp; memory\nfootprint of ColBERT indexes by 50% with virtually no retrieval performance\ndegradation. This method also allows for further reductions, reducing the\nvector count by 66%-to-75% , with degradation remaining below 5% on a vast\nmajority of datasets. Importantly, this approach requires no architectural\nchange nor query-time processing, and can be used as a simple drop-in during\nindexation with any ColBERT-like model.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [17.40239906311035, 23.81623649597168], "cluster": 2}, {"key": "cleveland2020content", "year": "2020", "citations": "2", "title": "Content-based Music Similarity With Triplet Networks", "abstract": "<p>We explore the feasibility of using triplet neural networks to embed songs\nbased on content-based music similarity. Our network is trained using triplets\nof songs such that two songs by the same artist are embedded closer to one\nanother than to a third song by a different artist. We compare two models that\nare trained using different ways of picking this third song: at random vs.\nbased on shared genre labels. Our experiments are conducted using songs from\nthe Free Music Archive and use standard audio features. The initial results\nshow that shallow Siamese networks can be used to embed music for a simple\nartist retrieval task.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [7.308446884155273, -49.303558349609375], "cluster": 3}, {"key": "coleman2019sub", "year": "2019", "citations": "3", "title": "Sub-linear Memory Sketches For Near Neighbor Search On Streaming Data", "abstract": "<p>We present the first sublinear memory sketch that can be queried to find the\nnearest neighbors in a dataset. Our online sketching algorithm compresses an N\nelement dataset to a sketch of size \\(O(N^b log^3 N)\\) in \\(O(N^{(b+1)} log^3\nN)\\) time, where \\(b &lt; 1\\). This sketch can correctly report the nearest neighbors\nof any query that satisfies a stability condition parameterized by \\(b\\). We\nachieve sublinear memory performance on stable queries by combining recent\nadvances in locality sensitive hash (LSH)-based estimators, online kernel\ndensity estimation, and compressed sensing. Our theoretical results shed new\nlight on the memory-accuracy tradeoff for nearest neighbor search, and our\nsketch, which consists entirely of short integer arrays, has a variety of\nattractive features in practice. We evaluate the memory-recall tradeoff of our\nmethod on a friend recommendation task in the Google Plus social media network.\nWe obtain orders of magnitude better compression than the random projection\nbased alternative while retaining the ability to report the nearest neighbors\nof practical queries.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [18.347326278686523, 11.508220672607422], "cluster": 6}, {"key": "coleman2020similarity", "year": "2022", "citations": "16", "title": "Similarity Search For Efficient Active Learning And Search Of Rare Concepts", "abstract": "<p>Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.</p>\n", "tags": ["Efficiency", "Similarity-Search", "Scalability", "AAAI", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [37.12214279174805, -2.3515079021453857], "cluster": 9}, {"key": "coleman2021graph", "year": "2021", "citations": "5", "title": "Graph Reordering For Cache-efficient Near Neighbor Search", "abstract": "<p>Graph search is one of the most successful algorithmic trends in near\nneighbor search. Several of the most popular and empirically successful\nalgorithms are, at their core, a simple walk along a pruned near neighbor\ngraph. Such algorithms consistently perform at the top of industrial speed\nbenchmarks for applications such as embedding search. However, graph traversal\napplications often suffer from poor memory access patterns, and near neighbor\nsearch is no exception to this rule. Our measurements show that popular search\nindices such as the hierarchical navigable small-world graph (HNSW) can have\npoor cache miss performance. To address this problem, we apply graph reordering\nalgorithms to near neighbor graphs. Graph reordering is a memory layout\noptimization that groups commonly-accessed nodes together in memory. We present\nexhaustive experiments applying several reordering algorithms to a leading\ngraph-based near neighbor method based on the HNSW index. We find that\nreordering improves the query time by up to 40%, and we demonstrate that the\ntime needed to reorder the graph is negligible compared to the time required to\nconstruct the index.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Evaluation"], "tsne_embedding": [56.69379425048828, 6.603604316711426], "cluster": 9}, {"key": "collomosse2019livesketch", "year": "2019", "citations": "55", "title": "Livesketch: Query Perturbations For Guided Sketch-based Visual Search", "abstract": "<p>LiveSketch is a novel algorithm for searching large image collections using\nhand-sketched queries. LiveSketch tackles the inherent ambiguity of sketch\nsearch by creating visual suggestions that augment the query as it is drawn,\nmaking query specification an iterative rather than one-shot process that helps\ndisambiguate users\u2019 search intent. Our technical contributions are: a triplet\nconvnet architecture that incorporates an RNN based variational autoencoder to\nsearch for images using vector (stroke-based) queries; real-time clustering to\nidentify likely search intents (and so, targets within the search embedding);\nand the use of backpropagation from those targets to perturb the input stroke\nsequence, so suggesting alterations to the query in order to guide the search.\nWe show improvements in accuracy and time-to-task over contemporary baselines\nusing a 67M image corpus.</p>\n", "tags": ["Efficiency", "CVPR", "Image-Retrieval"], "tsne_embedding": [-46.58315658569336, -21.82468605041504], "cluster": 5}, {"key": "conjeti2016deep", "year": "2016", "citations": "3", "title": "Deep Residual Hashing", "abstract": "<p>Hashing aims at generating highly compact similarity preserving code words\nwhich are well suited for large-scale image retrieval tasks.\n  Most existing hashing methods first encode the images as a vector of\nhand-crafted features followed by a separate binarization step to generate hash\ncodes. This two-stage process may produce sub-optimal encoding. In this paper,\nfor the first time, we propose a deep architecture for supervised hashing\nthrough residual learning, termed Deep Residual Hashing (DRH), for an\nend-to-end simultaneous representation learning and hash coding. The DRH model\nconstitutes four key elements: (1) a sub-network with multiple stacked residual\nblocks; (2) hashing layer for binarization; (3) supervised retrieval loss\nfunction based on neighbourhood component analysis for similarity preserving\nembedding; and (4) hashing related losses and regularisation to control the\nquantization error and improve the quality of hash coding. We present results\nof extensive experiments on a large public chest x-ray image database with\nco-morbidities and discuss the outcome showing substantial improvements over\nthe latest state-of-the art methods.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Scalability", "Image-Retrieval", "Supervised", "Neural-Hashing"], "tsne_embedding": [-7.771548271179199, 4.343750953674316], "cluster": 1}, {"key": "conjeti2017learning", "year": "2017", "citations": "1", "title": "Learning Robust Hash Codes For Multiple Instance Image Retrieval", "abstract": "<p>In this paper, for the first time, we introduce a multiple instance (MI) deep\nhashing technique for learning discriminative hash codes with weak bag-level\nsupervision suited for large-scale retrieval. We learn such hash codes by\naggregating deeply learnt hierarchical representations across bag members\nthrough a dedicated MI pool layer. For better trainability and retrieval\nquality, we propose a two-pronged approach that includes robust optimization\nand training with an auxiliary single instance hashing arm which is\ndown-regulated gradually. We pose retrieval for tumor assessment as an MI\nproblem because tumors often coexist with benign masses and could exhibit\ncomplementary signatures when scanned from different anatomical views.\nExperimental validations on benchmark mammography and histology datasets\ndemonstrate improved retrieval performance over the state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-45.58479309082031, 16.37469482421875], "cluster": 0}, {"key": "connor2016hilbert", "year": "2016", "citations": "18", "title": "Hilbert Exclusion: Improved Metric Search Through Finite Isometric Embeddings", "abstract": "<p>Most research into similarity search in metric spaces relies upon the\ntriangle inequality property. This property allows the space to be arranged\naccording to relative distances to avoid searching some subspaces. We show that\nmany common metric spaces, notably including those using Euclidean and\nJensen-Shannon distances, also have a stronger property, sometimes called the\nfour-point property: in essence, these spaces allow an isometric embedding of\nany four points in three-dimensional Euclidean space, as well as any three\npoints in two-dimensional Euclidean space. In fact, we show that any space\nwhich is isometrically embeddable in Hilbert space has the stronger property.\nThis property gives stronger geometric guarantees, and one in particular, which\nwe name the Hilbert Exclusion property, allows any indexing mechanism which\nuses hyperplane partitioning to perform better. One outcome of this observation\nis that a number of state-of-the-art indexing mechanisms over high dimensional\nspaces can be easily extended to give a significant increase in performance;\nfurthermore, the improvement given is greater in higher dimensions. This\ntherefore leads to a significant improvement in the cost of metric search in\nthese spaces.</p>\n", "tags": ["Evaluation", "Similarity-Search"], "tsne_embedding": [10.867630004882812, 48.22124481201172], "cluster": 4}, {"key": "connor2017high", "year": "2017", "citations": "19", "title": "High-dimensional Simplexes For Supermetric Search", "abstract": "<p>In 1953, Blumenthal showed that every semi-metric space that is isometrically\nembeddable in a Hilbert space has the n-point property; we have previously\ncalled such spaces supermetric spaces. Although this is a strictly stronger\nproperty than triangle inequality, it is nonetheless closely related and many\nuseful metric spaces possess it. These include Euclidean, Cosine and\nJensen-Shannon spaces of any dimension. A simple corollary of the n-point\nproperty is that, for any (n+1) objects sampled from the space, there exists an\nn-dimensional simplex in Euclidean space whose edge lengths correspond to the\ndistances among the objects. We show how the construction of such simplexes in\nhigher dimensions can be used to give arbitrarily tight lower and upper bounds\non distances within the original space. This allows the construction of an\nn-dimensional Euclidean space, from which lower and upper bounds of the\noriginal space can be calculated, and which is itself an indexable space with\nthe n-point property. For similarity search, the engineering tradeoffs are\ngood: we show significant reductions in data size and metric cost with little\nloss of accuracy, leading to a significant overall improvement in search\nperformance.</p>\n", "tags": ["Evaluation", "Similarity-Search"], "tsne_embedding": [10.890580177307129, 48.242835998535156], "cluster": 4}, {"key": "corbi\u00e8re2017leveraging", "year": "2017", "citations": "59", "title": "Leveraging Weakly Annotated Data For Fashion Image Retrieval And Label Prediction", "abstract": "<p>In this paper, we present a method to learn a visual representation adapted\nfor e-commerce products. Based on weakly supervised learning, our model learns\nfrom noisy datasets crawled on e-commerce website catalogs and does not require\nany manual labeling. We show that our representation can be used for downward\nclassification tasks over clothing categories with different levels of\ngranularity. We also demonstrate that the learnt representation is suitable for\nimage retrieval. We achieve nearly state-of-art results on the DeepFashion\nIn-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without\nusing the provided training set.</p>\n", "tags": ["Supervised", "ICCV", "Image-Retrieval", "Datasets"], "tsne_embedding": [-12.862508773803711, -43.928802490234375], "cluster": 3}, {"key": "couairon2021embedding", "year": "2022", "citations": "13", "title": "Embedding Arithmetic Of Multimodal Queries For Image Retrieval", "abstract": "<p>Latent text representations exhibit geometric regularities, such as the\nfamous analogy: queen is to king what woman is to man. Such structured semantic\nrelations were not demonstrated on image representations. Recent works aiming\nat bridging this semantic gap embed images and text into a multimodal space,\nenabling the transfer of text-defined transformations to the image modality. We\nintroduce the SIMAT dataset to evaluate the task of Image Retrieval with\nMultimodal queries. SIMAT contains 6k images and 18k textual transformation\nqueries that aim at either replacing scene elements or changing pairwise\nrelationships between scene elements. The goal is to retrieve an image\nconsistent with the (source image, text transformation) query. We use an\nimage/text matching oracle (OSCAR) to assess whether the image transformation\nis successful. The SIMAT dataset will be publicly available. We use SIMAT to\nevaluate the geometric properties of multimodal embedding spaces trained with\nan image/text matching objective, like CLIP. We show that vanilla CLIP\nembeddings are not very well suited to transform images with delta vectors, but\nthat a simple finetuning on the COCO dataset can bring dramatic improvements.\nWe also study whether it is beneficial to leverage pretrained universal\nsentence encoders (FastText, LASER and LaBSE).</p>\n", "tags": ["CVPR", "Image-Retrieval", "Datasets"], "tsne_embedding": [-28.55164337158203, 3.7483322620391846], "cluster": 0}, {"key": "croitoru2021teachtext", "year": "2021", "citations": "108", "title": "TEACHTEXT: Crossmodal Generalized Distillation For Text-video Retrieval", "abstract": "<p>In recent years, considerable progress on the task of text-video retrieval\nhas been achieved by leveraging large-scale pretraining on visual and audio\ndatasets to construct powerful video encoders. By contrast, despite the natural\nsymmetry, the design of effective algorithms for exploiting large-scale\nlanguage pretraining remains under-explored. In this work, we are the first to\ninvestigate the design of such algorithms and propose a novel generalized\ndistillation method, TeachText, which leverages complementary cues from\nmultiple text encoders to provide an enhanced supervisory signal to the\nretrieval model. Moreover, we extend our method to video side modalities and\nshow that we can effectively reduce the number of used modalities at test time\nwithout compromising performance. Our approach advances the state of the art on\nseveral video retrieval benchmarks by a significant margin and adds no\ncomputational overhead at test time. Last but not least, we show an effective\napplication of our method for eliminating noise from retrieval datasets. Code\nand data can be found at https://www.robots.ox.ac.uk/~vgg/research/teachtext/.</p>\n", "tags": ["ICCV", "Scalability", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-8.841955184936523, -29.609846115112305], "cluster": 3}, {"key": "csurka2014unsupervised", "year": "2014", "citations": "6", "title": "Unsupervised Visual And Textual Information Fusion In Multimedia Retrieval - A Graph-based Point Of View", "abstract": "<p>Multimedia collections are more than ever growing in size and diversity.\nEffective multimedia retrieval systems are thus critical to access these\ndatasets from the end-user perspective and in a scalable way. We are interested\nin repositories of image/text multimedia objects and we study multimodal\ninformation fusion techniques in the context of content based multimedia\ninformation retrieval. We focus on graph based methods which have proven to\nprovide state-of-the-art performances. We particularly examine two of such\nmethods : cross-media similarities and random walk based scores. From a\ntheoretical viewpoint, we propose a unifying graph based framework which\nencompasses the two aforementioned approaches. Our proposal allows us to\nhighlight the core features one should consider when using a graph based\ntechnique for the combination of visual and textual information. We compare\ncross-media and random walk based results using three different real-world\ndatasets. From a practical standpoint, our extended empirical analysis allow us\nto provide insights and guidelines about the use of graph based methods for\nmultimodal information fusion in content based multimedia information\nretrieval.</p>\n", "tags": ["Graph-Based-Ann", "Tools-&-Libraries", "Unsupervised", "Datasets"], "tsne_embedding": [52.90088653564453, -6.735172271728516], "cluster": 9}, {"key": "cui2020exchnet", "year": "2020", "citations": "32", "title": "Exchnet: A Unified Hashing Network For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Retrieving content relevant images from a large-scale fine grained dataset could suffer from intolerably slow query speed and highly\nredundant storage cost, due to high-dimensional real-valued embeddings\nwhich aim to distinguish subtle visual differences of fine-grained objects.\nIn this paper, we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images, leveraging the search\nand storage efficiency of hash learning to alleviate the aforementioned\nproblems. Specifically, we propose a unified end-to-end trainable network,\ntermed as ExchNet. Based on attention mechanisms and proposed attention constraints, it can firstly obtain both local and global features\nto represent object parts and whole fine-grained objects, respectively.\nFurthermore, to ensure the discriminative ability and semantic meaning\u2019s\nconsistency of these part-level features across images, we design a local\nfeature alignment approach by performing a feature exchanging operation. Later, an alternative learning algorithm is employed to optimize\nthe whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments, our proposal consistently outperforms\nstate-of-the-art generic hashing methods on five fine-grained datasets,\nwhich shows our effectiveness. Moreover, compared with other approximate nearest neighbor methods, ExchNet achieves the best speed-up and\nstorage reduction, revealing its efficiency and practicality.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Image-Retrieval", "Memory-Efficiency", "Datasets", "Compact-Codes"], "tsne_embedding": [-24.355178833007812, 4.581604480743408], "cluster": 1}, {"key": "cui2025exchnet", "year": "2020", "citations": "32", "title": "Exchnet: A Unified Hashing Network For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Retrieving content relevant images from a large-scale fine grained dataset could suffer from intolerably slow query speed and highly\nredundant storage cost, due to high-dimensional real-valued embeddings\nwhich aim to distinguish subtle visual differences of fine-grained objects.\nIn this paper, we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images, leveraging the search\nand storage efficiency of hash learning to alleviate the aforementioned\nproblems. Specifically, we propose a unified end-to-end trainable network,\ntermed as ExchNet. Based on attention mechanisms and proposed attention constraints, it can firstly obtain both local and global features\nto represent object parts and whole fine-grained objects, respectively.\nFurthermore, to ensure the discriminative ability and semantic meaning\u2019s\nconsistency of these part-level features across images, we design a local\nfeature alignment approach by performing a feature exchanging operation. Later, an alternative learning algorithm is employed to optimize\nthe whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments, our proposal consistently outperforms\nstate-of-the-art generic hashing methods on five fine-grained datasets,\nwhich shows our effectiveness. Moreover, compared with other approximate nearest neighbor methods, ExchNet achieves the best speed-up and\nstorage reduction, revealing its efficiency and practicality.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Image-Retrieval", "Memory-Efficiency", "Datasets", "Compact-Codes"], "tsne_embedding": [-24.35496711730957, 4.581740856170654], "cluster": 1}, {"key": "curtin2016fast", "year": "2016", "citations": "0", "title": "Fast Approximate Furthest Neighbors With Data-dependent Hashing", "abstract": "<p>We present a novel hashing strategy for approximate furthest neighbor search\nthat selects projection bases using the data distribution. This strategy leads\nto an algorithm, which we call DrusillaHash, that is able to outperform\nexisting approximate furthest neighbor strategies. Our strategy is motivated by\nan empirical study of the behavior of the furthest neighbor search problem,\nwhich lends intuition for where our algorithm is most useful. We also present a\nvariant of the algorithm that gives an absolute approximation guarantee; to our\nknowledge, this is the first such approximate furthest neighbor hashing\napproach to give such a guarantee. Performance studies indicate that\nDrusillaHash can achieve comparable levels of approximation to other algorithms\nwhile giving up to an order of magnitude speedup. An implementation is\navailable in the mlpack machine learning library (found at\nhttp://www.mlpack.org).</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Evaluation", "Hashing-Methods"], "tsne_embedding": [21.273618698120117, 38.26637649536133], "cluster": 4}, {"key": "curt\u00f32017segmentation", "year": "2017", "citations": "0", "title": "Segmentation Of Objects By Hashing", "abstract": "<p>We propose a novel approach to address the problem of Simultaneous Detection\nand Segmentation introduced in [Hariharan et al 2014]. Using the hierarchical\nstructures first presented in [Arbel'aez et al 2011] we use an efficient and\naccurate procedure that exploits the feature information of the hierarchy using\nLocality Sensitive Hashing. We build on recent work that utilizes convolutional\nneural networks to detect bounding boxes in an image [Ren et al 2015] and then\nuse the top similar hierarchical region that best fits each bounding box after\nhashing, we call this approach C&amp;Z Segmentation. We then refine our final\nsegmentation results by automatic hierarchical pruning. C&amp;Z Segmentation\nintroduces a train-free alternative to Hypercolumns [Hariharan et al 2015]. We\nconduct extensive experiments on PASCAL VOC 2012 segmentation dataset, showing\nthat C&amp;Z gives competitive state-of-the-art segmentations of objects.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [15.09219741821289, -18.74991226196289], "cluster": 7}, {"key": "dadaneh2020pairwise", "year": "2020", "citations": "11", "title": "Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator", "abstract": "<p>Semantic hashing has become a crucial component of fast similarity search in\nmany large-scale information retrieval systems, in particular, for text data.\nVariational auto-encoders (VAEs) with binary latent variables as hashing codes\nprovide state-of-the-art performance in terms of precision for document\nretrieval. We propose a pairwise loss function with discrete latent VAE to\nreward within-class similarity and between-class dissimilarity for supervised\nhashing. Instead of solving the optimization relying on existing biased\ngradient estimators, an unbiased low-variance gradient estimator is adopted to\noptimize the hashing function by evaluating the non-differentiable loss\nfunction over two correlated sets of binary hashing codes to control the\nvariance of gradient estimates. This new semantic hashing framework achieves\nsuperior performance compared to the state-of-the-arts, as demonstrated by our\ncomprehensive experiments.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Text-Retrieval", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Supervised", "Evaluation"], "tsne_embedding": [4.526247978210449, 0.8630954027175903], "cluster": 6}, {"key": "dadas2023opi", "year": "2023", "citations": "5", "title": "OPI At Semeval 2023 Task 1: Image-text Embeddings And Multimodal Information Retrieval For Visual Word Sense Disambiguation", "abstract": "<p>The goal of visual word sense disambiguation is to find the image that best\nmatches the provided description of the word\u2019s meaning. It is a challenging\nproblem, requiring approaches that combine language and image understanding. In\nthis paper, we present our submission to SemEval 2023 visual word sense\ndisambiguation shared task. The proposed system integrates multimodal\nembeddings, learning to rank methods, and knowledge-based approaches. We build\na classifier based on the CLIP model, whose results are enriched with\nadditional information retrieved from Wikipedia and lexical databases. Our\nsolution was ranked third in the multilingual task and won in the Persian\ntrack, one of the three language subtasks.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-28.013212203979492, -36.836978912353516], "cluster": 5}, {"key": "dahlgaard2017fast", "year": "2017", "citations": "30", "title": "Fast Similarity Sketching", "abstract": "<p>We consider the \\(\\textit{Similarity Sketching}\\) problem: Given a universe\n\\([u] = \\{0,\\ldots, u-1\\}\\) we want a random function \\(S\\) mapping subsets\n\\(A\\subseteq [u]\\) into vectors \\(S(A)\\) of size \\(t\\), such that the Jaccard\nsimilarity \\(J(A,B) = |A\\cap B|/|A\\cup B|\\) between sets \\(A\\) and \\(B\\) is\npreserved. More precisely, define \\(X_i = [S(A)[i] =\n  S(B)[i]]\\) and \\(X = \\sum_{i\\in [t]} X_i\\). We want \\(E[X_i]=J(A,B)\\), and we want\n\\(X\\) to be strongly concentrated around \\(E[X] = t \\cdot J(A,B)\\) (i.e.\nChernoff-style bounds). This is a fundamental problem which has found numerous\napplications in data mining, large-scale classification, computer vision,\nsimilarity search, etc. via the classic MinHash algorithm. The vectors \\(S(A)\\)\nare also called \\(\\textit{sketches}\\). Strong concentration is critical, for\noften we want to sketch many sets \\(B_1,\\ldots,B_n\\) so that we later, for a\nquery set \\(A\\), can find (one of) the most similar \\(B_i\\). It is then critical\nthat no \\(B_i\\) looks much more similar to \\(A\\) due to errors in the sketch.\n  The seminal \\(t\\times\\textit{MinHash}\\) algorithm uses \\(t\\) random hash\nfunctions \\(h_1,\\ldots, h_t\\), and stores \\(\\left ( \\min_{a\\in A} h_1(A),\\ldots,\n\\min_{a\\in A} h_t(A) \\right )\\) as the sketch of \\(A\\). The main drawback of\nMinHash is, however, its \\(O(t\\cdot |A|)\\) running time, and finding a sketch\nwith similar properties and faster running time has been the subject of several\npapers. (continued\u2026)</p>\n", "tags": ["Similarity-Search", "Locality-Sensitive-Hashing", "Scalability"], "tsne_embedding": [21.386972427368164, 50.62807846069336], "cluster": 4}, {"key": "dahlgaard2017practical", "year": "2017", "citations": "16", "title": "Practical Hash Functions For Similarity Estimation And Dimensionality Reduction", "abstract": "<p>Hashing is a basic tool for dimensionality reduction employed in several\naspects of machine learning. However, the perfomance analysis is often carried\nout under the abstract assumption that a truly random unit cost hash function\nis used, without concern for which concrete hash function is employed. The\nconcrete hash function may work fine on sufficiently random input. The question\nis if it can be trusted in the real world when faced with more structured\ninput.\n  In this paper we focus on two prominent applications of hashing, namely\nsimilarity estimation with the one permutation hashing (OPH) scheme of Li et\nal. [NIPS\u201912] and feature hashing (FH) of Weinberger et al. [ICML\u201909], both of\nwhich have found numerous applications, i.e. in approximate near-neighbour\nsearch with LSH and large-scale classification with SVM.\n  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS\u201915] which was\nproved to perform like a truly random hash function in many applications,\nincluding OPH. Here we first show improved concentration bounds for FH with\ntruly random hashing and then argue that mixed tabulation performs similar for\nsparse input. Our main contribution, however, is an experimental comparison of\ndifferent hashing schemes when used inside FH, OPH, and LSH.\n  We find that mixed tabulation hashing is almost as fast as the\nmultiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work\nwell on sufficiently random data, but we demonstrate that in the above\napplications, it can lead to bias and poor concentration on both real-world and\nsynthetic data. We also compare with the popular MurmurHash3, which has no\nproven guarantees. Mixed tabulation and MurmurHash3 both perform similar to\ntruly random hashing in our experiments. However, mixed tabulation is 40%\nfaster than MurmurHash3, and it has the proven guarantee of good performance on\nall possible input.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Scalability"], "tsne_embedding": [11.739969253540039, 23.12374496459961], "cluster": 4}, {"key": "dai2017stochastic", "year": "2017", "citations": "73", "title": "Stochastic Generative Hashing", "abstract": "<p>Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [23.083688735961914, 3.792290449142456], "cluster": 6}, {"key": "dai2020convolutional", "year": "2020", "citations": "7", "title": "Convolutional Embedding For Edit Distance", "abstract": "<p>Edit-distance-based string similarity search has many applications such as\nspell correction, data de-duplication, and sequence alignment. However,\ncomputing edit distance is known to have high complexity, which makes string\nsimilarity search challenging for large datasets. In this paper, we propose a\ndeep learning pipeline (called CNN-ED) that embeds edit distance into Euclidean\ndistance for fast approximate similarity search. A convolutional neural network\n(CNN) is used to generate fixed-length vector embeddings for a dataset of\nstrings and the loss function is a combination of the triplet loss and the\napproximation error. To justify our choice of using CNN instead of other\nstructures (e.g., RNN) as the model, theoretical analysis is conducted to show\nthat some basic operations in our CNN model preserve edit distance.\nExperimental results show that CNN-ED outperforms data-independent CGK\nembedding and RNN-based GRU embedding in terms of both accuracy and efficiency\nby a large margin. We also show that string similarity search can be\nsignificantly accelerated using CNN-based embeddings, sometimes by orders of\nmagnitude.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Similarity-Search", "SIGIR", "Datasets"], "tsne_embedding": [-48.61744689941406, -3.2266547679901123], "cluster": 0}, {"key": "dai2022multi", "year": "2022", "citations": "1", "title": "Multi-granularity Association Learning Framework For On-the-fly Fine-grained Sketch-based Image Retrieval", "abstract": "<p>Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of\nretrieving a particular photo in a given query sketch. However, its widespread\napplicability is limited by the fact that it is difficult to draw a complete\nsketch for most people, and the drawing process often takes time. In this\nstudy, we aim to retrieve the target photo with the least number of strokes\npossible (incomplete sketch), named on-the-fly FG-SBIR (Bhunia et al. 2020),\nwhich starts retrieving at each stroke as soon as the drawing begins. We\nconsider that there is a significant correlation among these incomplete\nsketches in the sketch drawing episode of each photo. To learn more efficient\njoint embedding space shared between the photo and its incomplete sketches, we\npropose a multi-granularity association learning framework that further\noptimizes the embedding space of all incomplete sketches. Specifically, based\non the integrity of the sketch, we can divide a complete sketch episode into\nseveral stages, each of which corresponds to a simple linear mapping layer.\nMoreover, our framework guides the vector space representation of the current\nsketch to approximate that of its later sketches to realize the retrieval\nperformance of the sketch with fewer strokes to approach that of the sketch\nwith more strokes. In the experiments, we proposed more realistic challenges,\nand our method achieved superior early retrieval efficiency over the\nstate-of-the-art methods and alternative baselines on two publicly available\nfine-grained sketch retrieval datasets.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-45.440711975097656, -18.12393569946289], "cluster": 5}, {"key": "dai2023fine", "year": "2023", "citations": "2", "title": "Fine-grained Text-video Retrieval With Frozen Image Encoders", "abstract": "<p>State-of-the-art text-video retrieval (TVR) methods typically utilize CLIP\nand cosine similarity for efficient retrieval. Meanwhile, cross attention\nmethods, which employ a transformer decoder to compute attention between each\ntext query and all frames in a video, offer a more comprehensive interaction\nbetween text and videos. However, these methods lack important fine-grained\nspatial information as they directly compute attention between text and\nvideo-level tokens. To address this issue, we propose CrossTVR, a two-stage\ntext-video retrieval architecture. In the first stage, we leverage existing TVR\nmethods with cosine similarity network for efficient text/video candidate\nselection. In the second stage, we propose a novel decoupled video text cross\nattention module to capture fine-grained multimodal information in spatial and\ntemporal dimensions. Additionally, we employ the frozen CLIP model strategy in\nfine-grained retrieval, enabling scalability to larger pre-trained vision\nmodels like ViT-G, resulting in improved retrieval performance. Experiments on\ntext video retrieval datasets demonstrate the effectiveness and scalability of\nour proposed CrossTVR compared to state-of-the-art approaches.</p>\n", "tags": ["Distance-Metric-Learning", "Similarity-Search", "Scalability", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-37.10258102416992, -29.704513549804688], "cluster": 5}, {"key": "dai2023sketch", "year": "2023", "citations": "4", "title": "Sketch Less Face Image Retrieval: A New Challenge", "abstract": "<p>In some specific scenarios, face sketch was used to identify a person.\nHowever, drawing a complete face sketch often needs skills and takes time,\nwhich hinder its widespread applicability in the practice. In this study, we\nproposed a new task named sketch less face image retrieval (SLFIR), in which\nthe retrieval was carried out at each stroke and aim to retrieve the target\nface photo using a partial sketch with as few strokes as possible (see Fig.1).\nFirstly, we developed a method to generate the data of sketch with drawing\nprocess, and opened such dataset; Secondly, we proposed a two-stage method as\nthe baseline for SLFIR that (1) A triplet network, was first adopt to learn the\njoint embedding space shared between the complete sketch and its target face\nphoto; (2) Regarding the sketch drawing episode as a sequence, we designed a\nLSTM module to optimize the representation of the incomplete face sketch.\nExperiments indicate that the new framework can finish the retrieval using a\npartial or pool drawing sketch.</p>\n", "tags": ["Datasets", "Tools-&-Libraries", "Image-Retrieval", "ICASSP"], "tsne_embedding": [-46.03652572631836, -19.067129135131836], "cluster": 5}, {"key": "dalins2019pdq", "year": "2019", "citations": "5", "title": "PDQ & TMK + PDQF -- A Test Drive Of Facebook's Perceptual Hashing Algorithms", "abstract": "<p>Efficient and reliable automated detection of modified image and multimedia\nfiles has long been a challenge for law enforcement, compounded by the harm\ncaused by repeated exposure to psychologically harmful materials. In August\n2019 Facebook open-sourced their PDQ and TMK + PDQF algorithms for image and\nvideo similarity measurement, respectively. In this report, we review the\nalgorithms\u2019 performance on detecting commonly encountered transformations on\nreal-world case data, sourced from contemporary investigations. We also provide\na reference implementation to demonstrate the potential application and\nintegration of such algorithms within existing law enforcement systems.</p>\n", "tags": ["Survey-Paper", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-21.737836837768555, -40.218624114990234], "cluster": 3}, {"key": "daras2020smyrf", "year": "2020", "citations": "12", "title": "SMYRF: Efficient Attention Using Asymmetric Clustering", "abstract": "<p>We propose a novel type of balanced clustering algorithm to approximate\nattention. Attention complexity is reduced from \\(O(N^2)\\) to \\(O(N log N)\\),\nwhere \\(N\\) is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive\nHashing (LSH) in a novel way by defining new Asymmetric transformations and an\nadaptive scheme that produces balanced clusters. The biggest advantage of SMYRF\nis that it can be used as a drop-in replacement for dense attention layers\nwithout any retraining. On the contrary, prior fast attention methods impose\nconstraints (e.g. queries and keys share the same vector representations) and\nrequire re-training from scratch. We apply our method to pre-trained\nstate-of-the-art Natural Language Processing and Computer Vision models and we\nreport significant memory and speed benefits. Notably, SMYRF-BERT outperforms\n(slightly) BERT on GLUE, while using \\(50%\\) less memory. We also show that\nSMYRF can be used interchangeably with dense attention before and after\ntraining. Finally, we use SMYRF to train GANs with attention in high\nresolutions. Using a single TPU, we were able to scale attention to 128x128=16k\nand 256x256=65k tokens on BigGAN on CelebA-HQ.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [28.818092346191406, 9.752325057983398], "cluster": 2}, {"key": "das2019semi", "year": "2019", "citations": "1", "title": "Semi Supervised Phrase Localization In A Bidirectional Caption-image Retrieval Framework", "abstract": "<p>We introduce a novel deep neural network architecture that links visual\nregions to corresponding textual segments including phrases and words. To\naccomplish this task, our architecture makes use of the rich semantic\ninformation available in a joint embedding space of multi-modal data. From this\njoint embedding space, we extract the associative localization maps that\ndevelop naturally, without explicitly providing supervision during training for\nthe localization task. The joint space is learned using a bidirectional ranking\nobjective that is optimized using a \\(N\\)-Pair loss formulation. This training\nmechanism demonstrates the idea that localization information is learned\ninherently while optimizing a Bidirectional Retrieval objective. The model\u2019s\nretrieval and localization performance is evaluated on MSCOCO and Flickr30K\nEntities datasets. This architecture outperforms the state of the art results\nin the semi-supervised phrase localization setting.</p>\n", "tags": ["Supervised", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [-6.459554195404053, -22.939727783203125], "cluster": 3}, {"key": "dash2020open", "year": "2021", "citations": "12", "title": "Open Knowledge Graphs Canonicalization Using Variational Autoencoders", "abstract": "<p>Noun phrases and Relation phrases in open knowledge graphs are not\ncanonicalized, leading to an explosion of redundant and ambiguous\nsubject-relation-object triples. Existing approaches to solve this problem take\na two-step approach. First, they generate embedding representations for both\nnoun and relation phrases, then a clustering algorithm is used to group them\nusing the embeddings as features. In this work, we propose Canonicalizing Using\nVariational Autoencoders (CUVA), a joint model to learn both embeddings and\ncluster assignments in an end-to-end approach, which leads to a better vector\nrepresentation for the noun and relation phrases. Our evaluation over multiple\nbenchmarks shows that CUVA outperforms the existing state-of-the-art\napproaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate\nentity canonicalization systems.</p>\n", "tags": ["Evaluation", "EMNLP", "Datasets"], "tsne_embedding": [4.047835826873779, -28.661184310913086], "cluster": 3}, {"key": "datar2004locality", "year": "2004", "citations": "2887", "title": "Locality-sensitive Hashing Scheme Based On P-stable Distributions", "abstract": "<p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain \u201cbounded growth\u201d condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>\n", "tags": ["Efficiency", "Tree-Based-Ann", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [32.63098907470703, 32.79261016845703], "cluster": 2}, {"key": "datar2025locality", "year": "2004", "citations": "2887", "title": "Locality-sensitive Hashing Scheme Based On P-stable Distributions", "abstract": "<p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain \u201cbounded growth\u201d condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>\n", "tags": ["Efficiency", "Tree-Based-Ann", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [32.63098907470703, 32.79261016845703], "cluster": 2}, {"key": "davoodi2019forestdsh", "year": "2021", "citations": "1", "title": "Forestdsh: A Universal Hash Design For Discrete Probability Distributions", "abstract": "<p>In this paper, we consider the problem of classification of \\(M\\) high\ndimensional queries \\(y^1,\\cdots,y^M\\in B^S\\) to \\(N\\) high dimensional classes\n\\(x^1,\\cdots,x^N\\in A^S\\) where \\(A\\) and \\(B\\) are discrete alphabets and the\nprobabilistic model that relates data to the classes \\(P(x,y)\\) is known. This\nproblem has applications in various fields including the database search\nproblem in mass spectrometry. The problem is analogous to the nearest neighbor\nsearch problem, where the goal is to find the data point in a database that is\nthe most similar to a query point. The state of the art method for solving an\napproximate version of the nearest neighbor search problem in high dimensions\nis locality sensitive hashing (LSH). LSH is based on designing hash functions\nthat map near points to the same buckets with a probability higher than random\n(far) points. To solve our high dimensional classification problem, we\nintroduce distribution sensitive hashes that map jointly generated pairs\n\\((x,y)\\sim P\\) to the same bucket with probability higher than random pairs\n\\(x\\sim P^A\\) and \\(y\\sim P^B\\), where \\(P^A\\) and \\(P^B\\) are the marginal probability\ndistributions of \\(P\\). We design distribution sensitive hashes using a forest of\ndecision trees and we show that the complexity of search grows with\n\\(O(N^{\\lambda^<em>(P)})\\) where \\(\\lambda^</em>(P)\\) is expressed in an analytical form.\nWe further show that the proposed hashes perform faster than state of the art\napproximate nearest neighbor search methods for a range of probability\ndistributions, in both theory and simulations. Finally, we apply our method to\nthe spectral library search problem in mass spectrometry, and show that it is\nan order of magnitude faster than the state of the art methods.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Tools-&-Libraries", "Evaluation", "Hashing-Methods"], "tsne_embedding": [19.205127716064453, 49.385719299316406], "cluster": 4}, {"key": "delfino2025kannolo", "year": "2025", "citations": "1", "title": "Kannolo: Sweet And Smooth Approximate K-nearest Neighbors Search", "abstract": "<p>Approximate Nearest Neighbors (ANN) search is a crucial task in several\napplications like recommender systems and information retrieval. Current\nstate-of-the-art ANN libraries, although being performance-oriented, often lack\nmodularity and ease of use. This translates into them not being fully suitable\nfor easy prototyping and testing of research ideas, an important feature to\nenable. We address these limitations by introducing kANNolo, a novel\nresearch-oriented ANN library written in Rust and explicitly designed to\ncombine usability with performance effectively. kANNolo is the first ANN\nlibrary that supports dense and sparse vector representations made available on\ntop of different similarity measures, e.g., euclidean distance and inner\nproduct. Moreover, it also supports vector quantization techniques, e.g.,\nProduct Quantization, on top of the indexing strategies implemented. These\nfunctionalities are managed through Rust traits, allowing shared behaviors to\nbe handled abstractly. This abstraction ensures flexibility and facilitates an\neasy integration of new components. In this work, we detail the architecture of\nkANNolo and demonstrate that its flexibility does not compromise performance.\nThe experimental analysis shows that kANNolo achieves state-of-the-art\nperformance in terms of speed-accuracy trade-off while allowing fast and easy\nprototyping, thus making kANNolo a valuable tool for advancing ANN research.\nSource code available on GitHub: https://github.com/TusKANNy/kannolo.</p>\n", "tags": ["Distance-Metric-Learning", "Quantization", "Recommender-Systems", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [27.629560470581055, -21.287376403808594], "cluster": 7}, {"key": "delmas2022artemis", "year": "2022", "citations": "26", "title": "ARTEMIS: Attention-based Retrieval With Text-explicit Matching And Implicit Similarity", "abstract": "<p>An intuitive way to search for images is to use queries composed of an\nexample image and a complementary text. While the first provides rich and\nimplicit context for the search, the latter explicitly calls for new traits, or\nspecifies how some elements of the example image should be changed to retrieve\nthe desired target image. Current approaches typically combine the features of\neach of the two elements of the query into a single representation, which can\nthen be compared to the ones of the potential target images. Our work aims at\nshedding new light on the task by looking at it through the prism of two\nfamiliar and related frameworks: text-to-image and image-to-image retrieval.\nTaking inspiration from them, we exploit the specific relation of each query\nelement with the targeted image and derive light-weight attention mechanisms\nwhich enable to mediate between the two complementary modalities. We validate\nour approach on several retrieval benchmarks, querying with images and their\nassociated free-form text modifiers. Our method obtains state-of-the-art\nresults without resorting to side information, multi-level features, heavy\npre-training nor large architectures as in previous works.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-24.77313232421875, -30.111360549926758], "cluster": 5}, {"key": "deng2017learning", "year": "2017", "citations": "37", "title": "Learning Deep Similarity Models With Focus Ranking For Fabric Image Retrieval", "abstract": "<p>Fabric image retrieval is beneficial to many applications including clothing\nsearching, online shopping and cloth modeling. Learning pairwise image\nsimilarity is of great importance to an image retrieval task. With the\nresurgence of Convolutional Neural Networks (CNNs), recent works have achieved\nsignificant progresses via deep representation learning with metric embedding,\nwhich drives similar examples close to each other in a feature space, and\ndissimilar ones apart from each other. In this paper, we propose a novel\nembedding method termed focus ranking that can be easily unified into a CNN for\njointly learning image representations and metrics in the context of\nfine-grained fabric image retrieval. Focus ranking aims to rank similar\nexamples higher than all dissimilar ones by penalizing ranking disorders via\nthe minimization of the overall cost attributed to similar samples being ranked\nbelow dissimilar ones. At the training stage, training samples are organized\ninto focus ranking units for efficient optimization. We build a large-scale\nfabric image retrieval dataset (FIRD) with about 25,000 images of 4,300\nfabrics, and test the proposed model on the FIRD dataset. Experimental results\nshow the superiority of the proposed model over existing metric embedding\nmodels.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [-54.901248931884766, 4.7893805503845215], "cluster": 0}, {"key": "deng2019triplet", "year": "2018", "citations": "378", "title": "Triplet-based Deep Hashing Network For Cross-modal Retrieval", "abstract": "<p>Given the benefits of its low storage requirements and high retrieval\nefficiency, hashing has recently received increasing attention. In\nparticular,cross-modal hashing has been widely and successfully used in\nmultimedia similarity search applications. However, almost all existing methods\nemploying cross-modal hashing cannot obtain powerful hash codes due to their\nignoring the relative similarity between heterogeneous data that contains\nricher semantic information, leading to unsatisfactory retrieval performance.\nIn this paper, we propose a triplet-based deep hashing (TDH) network for\ncross-modal retrieval. First, we utilize the triplet labels, which describes\nthe relative relationships among three instances as supervision in order to\ncapture more general semantic correlations between cross-modal instances. We\nthen establish a loss function from the inter-modal view and the intra-modal\nview to boost the discriminative abilities of the hash codes. Finally, graph\nregularization is introduced into our proposed TDH method to preserve the\noriginal semantic similarity between hash codes in Hamming space. Experimental\nresults show that our proposed method outperforms several state-of-the-art\napproaches on two popular cross-modal datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Similarity-Search", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [6.294556140899658, -1.2221299409866333], "cluster": 6}, {"key": "deng2019two", "year": "2019", "citations": "76", "title": "Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Supervised", "Neural-Hashing"], "tsne_embedding": [-4.720762252807617, -0.225666806101799], "cluster": 1}, {"key": "deng2021insclr", "year": "2022", "citations": "11", "title": "Insclr: Improving Instance Retrieval With Self-supervision", "abstract": "<p>This work aims at improving instance retrieval with self-supervision. We find\nthat fine-tuning using the recently developed self-supervised (SSL) learning\nmethods, such as SimCLR and MoCo, fails to improve the performance of instance\nretrieval. In this work, we identify that the learnt representations for\ninstance retrieval should be invariant to large variations in viewpoint and\nbackground etc., whereas self-augmented positives applied by the current SSL\nmethods can not provide strong enough signals for learning robust\ninstance-level representations. To overcome this problem, we propose InsCLR, a\nnew SSL method that builds on the \\textit{instance-level} contrast, to learn\nthe intra-class invariance by dynamically mining meaningful pseudo positive\nsamples from both mini-batches and a memory bank during training. Extensive\nexperiments demonstrate that InsCLR achieves similar or even better performance\nthan the state-of-the-art SSL methods on instance retrieval. Code is available\nat https://github.com/zeludeng/insclr.</p>\n", "tags": ["Supervised", "AAAI", "Self-Supervised", "Evaluation"], "tsne_embedding": [10.386075019836426, -1.0635361671447754], "cluster": 6}, {"key": "deng2025large", "year": "2025", "citations": "0", "title": "Large Vision-language Models For Knowledge-grounded Data Annotation Of Memes", "abstract": "<p>Memes have emerged as a powerful form of communication, integrating visual\nand textual elements to convey humor, satire, and cultural messages. Existing\nresearch has focused primarily on aspects such as emotion classification, meme\ngeneration, propagation, interpretation, figurative language, and\nsociolinguistics, but has often overlooked deeper meme comprehension and\nmeme-text retrieval. To address these gaps, this study introduces\nClassicMemes-50-templates (CM50), a large-scale dataset consisting of over\n33,000 memes, centered around 50 popular meme templates. We also present an\nautomated knowledge-grounded annotation pipeline leveraging large\nvision-language models to produce high-quality image captions, meme captions,\nand literary device labels overcoming the labor intensive demands of manual\nannotation. Additionally, we propose a meme-text retrieval CLIP model (mtrCLIP)\nthat utilizes cross-modal embedding to enhance meme analysis, significantly\nimproving retrieval performance. Our contributions include:(1) a novel dataset\nfor large-scale meme study, (2) a scalable meme annotation framework, and (3) a\nfine-tuned CLIP for meme-text retrieval, all aimed at advancing the\nunderstanding and analysis of memes at scale.</p>\n", "tags": ["Text-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-32.17359924316406, -33.276458740234375], "cluster": 5}, {"key": "deng2025two", "year": "2019", "citations": "76", "title": "Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Supervised", "Neural-Hashing"], "tsne_embedding": [-4.720656871795654, -0.2256719321012497], "cluster": 1}, {"key": "depalma2017distributed", "year": "2017", "citations": "1", "title": "Distributed Stratified Locality Sensitive Hashing For Critical Event Prediction In The Cloud", "abstract": "<p>The availability of massive healthcare data repositories calls for efficient\ntools for data-driven medicine. We introduce a distributed system for\nStratified Locality Sensitive Hashing to perform fast similarity-based\nprediction on large medical waveform datasets. Our implementation, for an ICU\nuse case, prioritizes latency over throughput and is targeted at a cloud\nenvironment. We demonstrate our system on Acute Hypotensive Episode prediction\nfrom Arterial Blood Pressure waveforms. On a dataset of \\(1.37\\) million points,\nwe show scaling up to \\(40\\) processors and a \\(21\\times\\) speedup in number of\ncomparisons to parallel exhaustive search at the price of a \\(10%\\) Matthews\ncorrelation coefficient (MCC) loss. Furthermore, if additional MCC loss can be\ntolerated, our system achieves speedups up to two orders of magnitude.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [-53.697509765625, 18.956035614013672], "cluster": 0}, {"key": "derasmo2024eclipse", "year": "2024", "citations": "0", "title": "ECLIPSE: Contrastive Dimension Importance Estimation With Pseudo-irrelevance Feedback For Dense Retrieval", "abstract": "<p>Recent advances in Information Retrieval have leveraged high-dimensional\nembedding spaces to improve the retrieval of relevant documents. Moreover, the\nManifold Clustering Hypothesis suggests that despite these high-dimensional\nrepresentations, documents relevant to a query reside on a lower-dimensional,\nquery-dependent manifold. While this hypothesis has inspired new retrieval\nmethods, existing approaches still face challenges in effectively separating\nnon-relevant information from relevant signals. We propose a novel methodology\nthat addresses these limitations by leveraging information from both relevant\nand non-relevant documents. Our method, ECLIPSE, computes a centroid based on\nirrelevant documents as a reference to estimate noisy dimensions present in\nrelevant ones, enhancing retrieval performance. Extensive experiments on three\nin-domain and one out-of-domain benchmarks demonstrate an average improvement\nof up to 19.50% (resp. 22.35%) in mAP(AP) and 11.42% (resp. 13.10%) in nDCG@10\nw.r.t. the DIME-based baseline (resp. the baseline using all dimensions). Our\nresults pave the way for more robust, pseudo-irrelevance-based retrieval\nsystems in future IR research.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [10.537399291992188, -27.27950096130371], "cluster": 7}, {"key": "desai2021semantically", "year": "2021", "citations": "2", "title": "Semantically Constrained Memory Allocation (SCMA) For Embedding In Efficient Recommendation Systems", "abstract": "<p>Deep learning-based models are utilized to achieve state-of-the-art\nperformance for recommendation systems. A key challenge for these models is to\nwork with millions of categorical classes or tokens. The standard approach is\nto learn end-to-end, dense latent representations or embeddings for each token.\nThe resulting embeddings require large amounts of memory that blow up with the\nnumber of tokens. Training and inference with these models create storage, and\nmemory bandwidth bottlenecks leading to significant computing and energy\nconsumption when deployed in practice. To this end, we present the problem of\n\\textit{Memory Allocation} under budget for embeddings and propose a novel\nformulation of memory shared embedding, where memory is shared in proportion to\nthe overlap in semantic information. Our formulation admits a practical and\nefficient randomized solution with Locality sensitive hashing based Memory\nAllocation (LMA). We demonstrate a significant reduction in the memory\nfootprint while maintaining performance. In particular, our LMA embeddings\nachieve the same performance compared to standard embeddings with a 16\\(\\times\\)\nreduction in memory footprint. Moreover, LMA achieves an average improvement of\nover 0.003 AUC across different memory regimes than standard DLRM models on\nCriteo and Avazu datasets</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [30.441299438476562, 15.095929145812988], "cluster": 2}, {"key": "desai2023heterogeneous", "year": "2023", "citations": "0", "title": "Heterogeneous Federated Collaborative Filtering Using FAIR: Federated Averaging In Random Subspaces", "abstract": "<p>Recommendation systems (RS) for items (e.g., movies, books) and ads are\nwidely used to tailor content to users on various internet platforms.\nTraditionally, recommendation models are trained on a central server. However,\ndue to rising concerns for data privacy and regulations like the GDPR,\nfederated learning is an increasingly popular paradigm in which data never\nleaves the client device. Applying federated learning to recommendation models\nis non-trivial due to large embedding tables, which often exceed the memory\nconstraints of most user devices. To include data from all devices in federated\nlearning, we must enable collective training of embedding tables on devices\nwith heterogeneous memory capacities. Current solutions to heterogeneous\nfederated learning can only accommodate a small range of capacities and thus\nlimit the number of devices that can participate in training. We present\nFederated Averaging in Random subspaces (FAIR), which allows arbitrary\ncompression of embedding tables based on device capacity and ensures the\nparticipation of all devices in training. FAIR uses what we call consistent and\ncollapsible subspaces defined by hashing-based random projections to jointly\ntrain large embedding tables while using varying amounts of compression on user\ndevices. We evaluate FAIR on Neural Collaborative Filtering tasks with multiple\ndatasets and verify that FAIR can gather and share information from a wide\nrange of devices with varying capacities, allowing for seamless collaboration.\nWe prove the convergence of FAIR in the homogeneous setting with non-i.i.d data\ndistribution. Our code is open source at {https://github.com/apd10/FLCF}</p>\n", "tags": ["Locality-Sensitive-Hashing", "Recommender-Systems", "Hashing-Methods", "Datasets"], "tsne_embedding": [36.394432067871094, 0.6128571033477783], "cluster": 9}, {"key": "desai2023hyperbolic", "year": "2023", "citations": "0", "title": "Hyperbolic Image-text Representations", "abstract": "<p>Visual and linguistic concepts naturally organize themselves in a hierarchy,\nwhere a textual concept \u201cdog\u201d entails all images that contain dogs. Despite\nbeing intuitive, current large-scale vision and language models such as CLIP do\nnot explicitly capture such hierarchy. We propose MERU, a contrastive model\nthat yields hyperbolic representations of images and text. Hyperbolic spaces\nhave suitable geometric properties to embed tree-like data, so MERU can better\ncapture the underlying hierarchy in image-text datasets. Our results show that\nMERU learns a highly interpretable and structured representation space while\nbeing competitive with CLIP\u2019s performance on standard multi-modal tasks like\nimage classification and image-text retrieval. Our code and models are\navailable at https://www.github.com/facebookresearch/meru</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [-34.94866943359375, -19.4299259185791], "cluster": 5}, {"key": "desai2024hashattention", "year": "2024", "citations": "0", "title": "Hashattention: Semantic Sparsity For Faster Inference", "abstract": "<p>Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to \\(16\\times\\) with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to \\(32\\times\\) through task-specific fine-tuning. On A100 GPU, at \\(32\\times\\) sparsity, incorporating HashAttention reduces attention latency by up to \\(4.3\\times\\) in GPT-FAST and \\(2.54\\times\\) in FlashDecode, and achieves up to \\(3.12\\times\\) higher throughput for GPT-FAST.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Scalability"], "tsne_embedding": [10.30693244934082, 16.209096908569336], "cluster": 6}, {"key": "desai2024identity", "year": "2025", "citations": "0", "title": "Identity With Locality: An Ideal Hash For Gene Sequence Search", "abstract": "<p>Gene sequence search is a fundamental operation in computational genomics.\nDue to the petabyte scale of genome archives, most gene search systems now use\nhashing-based data structures such as Bloom Filters (BF). The state-of-the-art\nsystems such as Compact bit-slicing signature index (COBS) and Repeated And\nMerged Bloom filters (RAMBO) use BF with Random Hash (RH) functions for gene\nrepresentation and identification. The standard recipe is to cast the gene\nsearch problem as a sequence of membership problems testing if each subsequent\ngene substring (called kmer) of Q is present in the set of kmers of the entire\ngene database D. We observe that RH functions, which are crucial to the memory\nand the computational advantage of BF, are also detrimental to the system\nperformance of gene-search systems. While subsequent kmers being queried are\nlikely very similar, RH, oblivious to any similarity, uniformly distributes the\nkmers to different parts of potentially large BF, thus triggering excessive\ncache misses and causing system slowdown. We propose a novel hash function\ncalled the Identity with Locality (IDL) hash family, which co-locates the keys\nclose in input space without causing collisions. This approach ensures both\ncache locality and key preservation. IDL functions can be a drop-in replacement\nfor RH functions and help improve the performance of information retrieval\nsystems. We give a simple but practical construction of IDL function families\nand show that replacing the RH with IDL functions reduces cache misses by a\nfactor of 5x, thus improving query and indexing times of SOTA methods such as\nCOBS and RAMBO by factors up to 2x without compromising their quality. We also\nprovide a theoretical analysis of the false positive rate of BF with IDL\nfunctions. Our hash function is the first study that bridges Locality Sensitive\nHash (LSH) and RH to obtain cache efficiency.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Evaluation", "KDD"], "tsne_embedding": [3.597130298614502, 52.657169342041016], "cluster": 4}, {"key": "dey2018learning", "year": "2018", "citations": "24", "title": "Learning Cross-modal Deep Embeddings For Multi-object Image Retrieval Using Text And Sketch", "abstract": "<p>In this work we introduce a cross modal image retrieval system that allows\nboth text and sketch as input modalities for the query. A cross-modal deep\nnetwork architecture is formulated to jointly model the sketch and text input\nmodalities as well as the the image output modality, learning a common\nembedding between text and images and between sketches and images. In addition,\nan attention model is used to selectively focus the attention on the different\nobjects of the image, allowing for retrieval with multiple objects in the\nquery. Experiments show that the proposed method performs the best in both\nsingle and multiple object image retrieval in standard datasets.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [-24.034189224243164, -32.674522399902344], "cluster": 5}, {"key": "dey2019beyond", "year": "2021", "citations": "23", "title": "Beyond Visual Semantics: Exploring The Role Of Scene Text In Image Understanding", "abstract": "<p>Images with visual and scene text content are ubiquitous in everyday life.\nHowever, current image interpretation systems are mostly limited to using only\nthe visual features, neglecting to leverage the scene text content. In this\npaper, we propose to jointly use scene text and visual channels for robust\nsemantic interpretation of images. We do not only extract and encode visual and\nscene text cues, but also model their interplay to generate a contextual joint\nembedding with richer semantics. The contextual embedding thus generated is\napplied to retrieval and classification tasks on multimedia images, with scene\ntext content, to demonstrate its effectiveness. In the retrieval framework, we\naugment our learned text-visual semantic representation with scene text cues,\nto mitigate vocabulary misses that may have occurred during the semantic\nembedding. To deal with irrelevant or erroneous recognition of scene text, we\nalso apply query-based attention to our text channel. We show how the\nmulti-channel approach, involving visual semantics and scene text, improves\nupon state of the art.</p>\n", "tags": ["Tools-&-Libraries"], "tsne_embedding": [-32.550537109375, -37.714691162109375], "cluster": 5}, {"key": "dey2019doodle", "year": "2019", "citations": "74", "title": "Doodle To Search: Practical Zero-shot Sketch-based Image Retrieval", "abstract": "<p>In this paper, we investigate the problem of zero-shot sketch-based image\nretrieval (ZS-SBIR), where human sketches are used as queries to conduct\nretrieval of photos from unseen categories. We importantly advance prior arts\nby proposing a novel ZS-SBIR scenario that represents a firm step forward in\nits practical application. The new setting uniquely recognizes two important\nyet often neglected challenges of practical ZS-SBIR, (i) the large domain gap\nbetween amateur sketch and photo, and (ii) the necessity for moving towards\nlarge-scale retrieval. We first contribute to the community a novel ZS-SBIR\ndataset, QuickDraw-Extended, that consists of 330,000 sketches and 204,000\nphotos spanning across 110 categories. Highly abstract amateur human sketches\nare purposefully sourced to maximize the domain gap, instead of ones included\nin existing datasets that can often be semi-photorealistic. We then formulate a\nZS-SBIR framework to jointly model sketches and photos into a common embedding\nspace. A novel strategy to mine the mutual information among domains is\nspecifically engineered to alleviate the domain gap. External semantic\nknowledge is further embedded to aid semantic transfer. We show that, rather\nsurprisingly, retrieval performance significantly outperforms that of\nstate-of-the-art on existing datasets that can already be achieved using a\nreduced version of our model. We further demonstrate the superior performance\nof our full model by comparing with a number of alternatives on the newly\nproposed dataset. The new dataset, plus all training and testing code of our\nmodel, will be publicly released to facilitate future research</p>\n", "tags": ["CVPR", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-46.268165588378906, -17.035343170166016], "cluster": 5}, {"key": "dhamala2018multivariate", "year": "2018", "citations": "4", "title": "Multivariate Time-series Similarity Assessment Via Unsupervised Representation Learning And Stratified Locality Sensitive Hashing: Application To Early Acute Hypotensive Episode Detection", "abstract": "<p>Timely prediction of clinically critical events in Intensive Care Unit (ICU)\nis important for improving care and survival rate. Most of the existing\napproaches are based on the application of various classification methods on\nexplicitly extracted statistical features from vital signals. In this work, we\npropose to eliminate the high cost of engineering hand-crafted features from\nmultivariate time-series of physiologic signals by learning their\nrepresentation with a sequence-to-sequence auto-encoder. We then propose to\nhash the learned representations to enable signal similarity assessment for the\nprediction of critical events. We apply this methodological framework to\npredict Acute Hypotensive Episodes (AHE) on a large and diverse dataset of\nvital signal recordings. Experiments demonstrate the ability of the presented\nframework in accurately predicting an upcoming AHE.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Tools-&-Libraries", "Datasets", "Unsupervised"], "tsne_embedding": [-54.030364990234375, 17.509639739990234], "cluster": 0}, {"key": "dhulipala2024muvera", "year": "2024", "citations": "0", "title": "MUVERA: Multi-vector Retrieval Via Fixed Dimensional Encodings", "abstract": "<p>Neural embedding models have become a fundamental component of modern\ninformation retrieval (IR) pipelines. These models produce a single embedding\n\\(x \\in \\mathbb{R}^d\\) per data-point, allowing for fast retrieval via highly\noptimized maximum inner product search (MIPS) algorithms. Recently, beginning\nwith the landmark ColBERT paper, multi-vector models, which produce a set of\nembedding per data point, have achieved markedly superior performance for IR\ntasks. Unfortunately, using these models for IR is computationally expensive\ndue to the increased complexity of multi-vector retrieval and scoring.\n  In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a\nretrieval mechanism which reduces multi-vector similarity search to\nsingle-vector similarity search. This enables the usage of off-the-shelf MIPS\nsolvers for multi-vector retrieval. MUVERA asymmetrically generates Fixed\nDimensional Encodings (FDEs) of queries and documents, which are vectors whose\ninner product approximates multi-vector similarity. We prove that FDEs give\nhigh-quality \\(\\epsilon\\)-approximations, thus providing the first single-vector\nproxy for multi-vector similarity with theoretical guarantees. Empirically, we\nfind that FDEs achieve the same recall as prior state-of-the-art heuristics\nwhile retrieving 2-5\\(\\times\\) fewer candidates. Compared to prior state of the\nart implementations, MUVERA achieves consistently good end-to-end recall and\nlatency across a diverse set of the BEIR retrieval datasets, achieving an\naverage of 10\\(%\\) improved recall with \\(90%\\) lower latency.</p>\n", "tags": ["Efficiency", "Evaluation", "Similarity-Search", "Datasets"], "tsne_embedding": [10.789649963378906, 19.1514949798584], "cluster": 6}, {"key": "diao2024deep", "year": "2024", "citations": "1", "title": "Deep Boosting Learning: A Brand-new Cooperative Approach For Image-text Matching", "abstract": "<p>Image-text matching remains a challenging task due to heterogeneous semantic\ndiversity across modalities and insufficient distance separability within\ntriplets. Different from previous approaches focusing on enhancing multi-modal\nrepresentations or exploiting cross-modal correspondence for more accurate\nretrieval, in this paper we aim to leverage the knowledge transfer between peer\nbranches in a boosting manner to seek a more powerful matching model.\nSpecifically, we propose a brand-new Deep Boosting Learning (DBL) algorithm,\nwhere an anchor branch is first trained to provide insights into the data\nproperties, with a target branch gaining more advanced knowledge to develop\noptimal features and distance metrics. Concretely, an anchor branch initially\nlearns the absolute or relative distance between positive and negative pairs,\nproviding a foundational understanding of the particular network and data\ndistribution. Building upon this knowledge, a target branch is concurrently\ntasked with more adaptive margin constraints to further enlarge the relative\ndistance between matched and unmatched samples. Extensive experiments validate\nthat our DBL can achieve impressive and consistent improvements based on\nvarious recent state-of-the-art models in the image-text matching field, and\noutperform related popular cooperative strategies, e.g., Conventional\nDistillation, Mutual Learning, and Contrastive Learning. Beyond the above, we\nconfirm that DBL can be seamlessly integrated into their training scenarios and\nachieve superior performance under the same computational costs, demonstrating\nthe flexibility and broad applicability of our proposed method. Our code is\npublicly available at: https://github.com/Paranioar/DBL.</p>\n", "tags": ["Self-Supervised", "Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-24.744226455688477, -12.50114917755127], "cluster": 5}, {"key": "dias2022pattern", "year": "2022", "citations": "0", "title": "Pattern Spotting And Image Retrieval In Historical Documents Using Deep Hashing", "abstract": "<p>This paper presents a deep learning approach for image retrieval and pattern\nspotting in digital collections of historical documents. First, a region\nproposal algorithm detects object candidates in the document page images. Next,\ndeep learning models are used for feature extraction, considering two distinct\nvariants, which provide either real-valued or binary code representations.\nFinally, candidate images are ranked by computing the feature similarity with a\ngiven input query. A robust experimental protocol evaluates the proposed\napproach considering each representation scheme (real-valued and binary code)\non the DocExplore image database. The experimental results show that the\nproposed deep models compare favorably to the state-of-the-art image retrieval\napproaches for images of historical documents, outperforming other deep models\nby 2.56 percentage points using the same techniques for pattern spotting.\nBesides, the proposed approach also reduces the search time by up to 200x and\nthe storage cost up to 6,000x when compared to related works based on\nreal-valued representations.</p>\n", "tags": ["Compact-Codes", "Memory-Efficiency", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-14.035051345825195, -12.309589385986328], "cluster": 1}, {"key": "dimitrov2020combining", "year": "2020", "citations": "1", "title": "Combining Word Embeddings And Convolutional Neural Networks To Detect Duplicated Questions", "abstract": "<p>Detecting semantic similarities between sentences is still a challenge today\ndue to the ambiguity of natural languages. In this work, we propose a simple\napproach to identifying semantically similar questions by combining the\nstrengths of word embeddings and Convolutional Neural Networks (CNNs). In\naddition, we demonstrate how the cosine similarity metric can be used to\neffectively compare feature vectors. Our network is trained on the Quora\ndataset, which contains over 400k question pairs. We experiment with different\nembedding approaches such as Word2Vec, Fasttext, and Doc2Vec and investigate\nthe effects these approaches have on model performance. Our model achieves\ncompetitive results on the Quora dataset and complements the well-established\nevidence that CNNs can be utilized for paraphrase detection tasks.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-46.87394714355469, -5.795051097869873], "cluster": 0}, {"key": "ding2014collective", "year": "2014", "citations": "641", "title": "Collective Matrix Factorization Hashing For Multimodal Data", "abstract": "<p>Nearest neighbor search methods based on hashing have\nattracted considerable attention for effective and efficient\nlarge-scale similarity search in computer vision and information\nretrieval community. In this paper, we study the\nproblems of learning hash functions in the context of multimodal\ndata for cross-view similarity search. We put forward\na novel hashing method, which is referred to Collective\nMatrix Factorization Hashing (CMFH). CMFH learns unified\nhash codes by collective matrix factorization with latent\nfactor model from different modalities of one instance,\nwhich can not only supports cross-view search but also increases\nthe search accuracy by merging multiple view information\nsources. We also prove that CMFH, a similaritypreserving\nhashing learning method, has upper and lower\nboundaries. Extensive experiments verify that CMFH significantly\noutperforms several state-of-the-art methods on\nthree different datasets.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Similarity-Search", "Scalability", "Datasets"], "tsne_embedding": [10.954248428344727, 8.155810356140137], "cluster": 6}, {"key": "ding2015knn", "year": "2015", "citations": "14", "title": "Knn Hashing With Factorized Neighborhood Representation", "abstract": "<p>Hashing is very effective for many tasks in reducing the\nprocessing time and in compressing massive databases. Although lots of approaches have been developed to learn\ndata-dependent hash functions in recent years, how to learn\nhash functions to yield good performance with acceptable\ncomputational and memory cost is still a challenging problem. Based on the observation that retrieval precision is\nhighly related to the kNN classification accuracy, this paper\nproposes a novel kNN-based supervised hashing method,\nwhich learns hash functions by directly maximizing the kNN\naccuracy of the Hamming-embedded training data. To make\nit scalable well to large problem, we propose a factorized\nneighborhood representation to parsimoniously model the\nneighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,\nwe further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn\naccurate hashing functions with tolerable computation and\nstorage cost. Experiments on four benchmarks demonstrate\nthat our method outperforms the state-of-the-arts.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Memory-Efficiency", "Supervised", "Evaluation"], "tsne_embedding": [25.38205337524414, -6.450376510620117], "cluster": 6}, {"key": "ding2018mean", "year": "2018", "citations": "4", "title": "Mean Local Group Average Precision (mlgap): A New Performance Metric For Hashing-based Retrieval", "abstract": "<p>The research on hashing techniques for visual data is gaining increased\nattention in recent years due to the need for compact representations\nsupporting efficient search/retrieval in large-scale databases such as online\nimages. Among many possibilities, Mean Average Precision(mAP) has emerged as\nthe dominant performance metric for hashing-based retrieval. One glaring\nshortcoming of mAP is its inability in balancing retrieval accuracy and\nutilization of hash codes: pushing a system to attain higher mAP will\ninevitably lead to poorer utilization of the hash codes. Poor utilization of\nthe hash codes hinders good retrieval because of increased collision of samples\nin the hash space. This means that a model giving a higher mAP values does not\nnecessarily do a better job in retrieval. In this paper, we introduce a new\nmetric named Mean Local Group Average Precision (mLGAP) for better evaluation\nof the performance of hashing-based retrieval. The new metric provides a\nretrieval performance measure that also reconciles the utilization of hash\ncodes, leading to a more practically meaningful performance metric than\nconventional ones like mAP. To this end, we start by mathematical analysis of\nthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and\nshow why it is more appropriate for hashing-based retrieval. Experiments on\nimage retrieval are used to demonstrate the effectiveness of the proposed\nmetric.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Hashing-Methods", "Scalability"], "tsne_embedding": [-1.8071268796920776, 24.11923599243164], "cluster": 8}, {"key": "ding2019bilinear", "year": "2019", "citations": "14", "title": "Bilinear Supervised Hashing Based On 2D Image Features", "abstract": "<p>Hashing has been recognized as an efficient representation learning method to\neffectively handle big data due to its low computational complexity and memory\ncost. Most of the existing hashing methods focus on learning the\nlow-dimensional vectorized binary features based on the high-dimensional raw\nvectorized features. However, studies on how to obtain preferable binary codes\nfrom the original 2D image features for retrieval is very limited. This paper\nproposes a bilinear supervised discrete hashing (BSDH) method based on 2D image\nfeatures which utilizes bilinear projections to binarize the image matrix\nfeatures such that the intrinsic characteristics in the 2D image space are\npreserved in the learned binary codes. Meanwhile, the bilinear projection\napproximation and vectorization binary codes regression are seamlessly\nintegrated together to formulate the final robust learning framework.\nFurthermore, a discrete optimization strategy is developed to alternatively\nupdate each variable for obtaining the high-quality binary codes. In addition,\ntwo 2D image features, traditional SURF-based FVLAD feature and CNN-based\nAlexConv5 feature are designed for further improving the performance of the\nproposed BSDH method. Results of extensive experiments conducted on four\nbenchmark datasets show that the proposed BSDH method almost outperforms all\ncompeting hashing methods with different input features by different evaluation\nprotocols.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [3.986510753631592, 20.02105712890625], "cluster": 8}, {"key": "ding2021dynamic", "year": "2022", "citations": "3", "title": "Dynamic Texture Recognition Using PDV Hashing And Dictionary Learning On Multi-scale Volume Local Binary Pattern", "abstract": "<p>Spatial-temporal local binary pattern (STLBP) has been widely used in dynamic\ntexture recognition. STLBP often encounters the high-dimension problem as its\ndimension increases exponentially, so that STLBP could only utilize a small\nneighborhood. To tackle this problem, we propose a method for dynamic texture\nrecognition using PDV hashing and dictionary learning on multi-scale volume\nlocal binary pattern (PHD-MVLBP). Instead of forming very high-dimensional LBP\nhistogram features, it first uses hash functions to map the pixel difference\nvectors (PDVs) to binary vectors, then forms a dictionary using the derived\nbinary vector, and encodes them using the derived dictionary. In such a way,\nthe PDVs are mapped to feature vectors of the size of dictionary, instead of\nLBP histograms of very high dimension. Such an encoding scheme could extract\nthe discriminant information from videos in a much larger neighborhood\neffectively. The experimental results on two widely-used dynamic textures\ndatasets, DynTex++ and UCLA, show the superiority performance of the proposed\napproach over the state-of-the-art methods.</p>\n", "tags": ["ICASSP", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [-23.332063674926758, 14.954044342041016], "cluster": 8}, {"key": "ding2025collective", "year": "2014", "citations": "641", "title": "Collective Matrix Factorization Hashing For Multimodal Data", "abstract": "<p>Nearest neighbor search methods based on hashing have\nattracted considerable attention for effective and efficient\nlarge-scale similarity search in computer vision and information\nretrieval community. In this paper, we study the\nproblems of learning hash functions in the context of multimodal\ndata for cross-view similarity search. We put forward\na novel hashing method, which is referred to Collective\nMatrix Factorization Hashing (CMFH). CMFH learns unified\nhash codes by collective matrix factorization with latent\nfactor model from different modalities of one instance,\nwhich can not only supports cross-view search but also increases\nthe search accuracy by merging multiple view information\nsources. We also prove that CMFH, a similaritypreserving\nhashing learning method, has upper and lower\nboundaries. Extensive experiments verify that CMFH significantly\noutperforms several state-of-the-art methods on\nthree different datasets.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Similarity-Search", "Scalability", "Datasets"], "tsne_embedding": [10.954176902770996, 8.1563138961792], "cluster": 6}, {"key": "ding2025knn", "year": "2015", "citations": "14", "title": "Knn Hashing With Factorized Neighborhood Representation", "abstract": "<p>Hashing is very effective for many tasks in reducing the\nprocessing time and in compressing massive databases. Although lots of approaches have been developed to learn\ndata-dependent hash functions in recent years, how to learn\nhash functions to yield good performance with acceptable\ncomputational and memory cost is still a challenging problem. Based on the observation that retrieval precision is\nhighly related to the kNN classification accuracy, this paper\nproposes a novel kNN-based supervised hashing method,\nwhich learns hash functions by directly maximizing the kNN\naccuracy of the Hamming-embedded training data. To make\nit scalable well to large problem, we propose a factorized\nneighborhood representation to parsimoniously model the\nneighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,\nwe further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn\naccurate hashing functions with tolerable computation and\nstorage cost. Experiments on four benchmarks demonstrate\nthat our method outperforms the state-of-the-arts.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Memory-Efficiency", "Supervised", "Evaluation"], "tsne_embedding": [25.3819637298584, -6.4504218101501465], "cluster": 6}, {"key": "dirksen2016fast", "year": "2018", "citations": "13", "title": "Fast Binary Embeddings With Gaussian Circulant Matrices: Improved Bounds", "abstract": "<p>We consider the problem of encoding a finite set of vectors into a small\nnumber of bits while approximately retaining information on the angular\ndistances between the vectors. By deriving improved variance bounds related to\nbinary Gaussian circulant embeddings, we largely fix a gap in the proof of the\nbest known fast binary embedding method. Our bounds also show that\nwell-spreadness assumptions on the data vectors, which were needed in earlier\nwork on variance bounds, are unnecessary. In addition, we propose a new binary\nembedding with a faster running time on sparse data.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [2.661461591720581, 40.8873291015625], "cluster": 4}, {"key": "dirksen2018fast", "year": "2018", "citations": "13", "title": "Fast Binary Embeddings With Gaussian Circulant Matrices: Improved Bounds", "abstract": "<p>We consider the problem of encoding a finite set of vectors into a small\nnumber of bits while approximately retaining information on the angular\ndistances between the vectors. By deriving improved variance bounds related to\nbinary Gaussian circulant embeddings, we largely fix a gap in the proof of the\nbest known fast binary embedding method. Our bounds also show that\nwell-spreadness assumptions on the data vectors, which were needed in earlier\nwork on variance bounds, are unnecessary. In addition, we propose a new binary\nembedding with a faster running time on sparse data.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [2.661461591720581, 40.8873291015625], "cluster": 4}, {"key": "dirksen2020binarized", "year": "2020", "citations": "3", "title": "Binarized Johnson-lindenstrauss Embeddings", "abstract": "<p>We consider the problem of encoding a set of vectors into a minimal number of\nbits while preserving information on their Euclidean geometry. We show that\nthis task can be accomplished by applying a Johnson-Lindenstrauss embedding and\nsubsequently binarizing each vector by comparing each entry of the vector to a\nuniformly random threshold. Using this simple construction we produce two\nencodings of a dataset such that one can query Euclidean information for a pair\nof points using a small number of bit operations up to a desired additive error</p>\n<ul>\n  <li>Euclidean distances in the first case and inner products and squared\nEuclidean distances in the second. In the latter case, each point is encoded in\nnear-linear time. The number of bits required for these encodings is quantified\nin terms of two natural complexity parameters of the dataset - its covering\nnumbers and localized Gaussian complexity - and shown to be near-optimal.</li>\n</ul>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [4.401471138000488, 40.88090133666992], "cluster": 4}, {"key": "do2016binary", "year": "2016", "citations": "20", "title": "Binary Hashing With Semidefinite Relaxation And Augmented Lagrangian", "abstract": "<p>This paper proposes two approaches for inferencing binary codes in two-step\n(supervised, unsupervised) hashing. We first introduce an unified formulation\nfor both supervised and unsupervised hashing. Then, we cast the learning of one\nbit as a Binary Quadratic Problem (BQP). We propose two approaches to solve\nBQP. In the first approach, we relax BQP as a semidefinite programming problem\nwhich its global optimum can be achieved. We theoretically prove that the\nobjective value of the binary solution achieved by this approach is well\nbounded. In the second approach, we propose an augmented Lagrangian based\napproach to solve BQP directly without relaxing the binary constraint.\nExperimental results on three benchmark datasets show that our proposed methods\ncompare favorably with the state of the art.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [21.694202423095703, -1.801300048828125], "cluster": 6}, {"key": "do2016embedding", "year": "2017", "citations": "37", "title": "Embedding Based On Function Approximation For Large Scale Image Search", "abstract": "<p>The objective of this paper is to design an embedding method that maps local\nfeatures describing an image (e.g. SIFT) to a higher dimensional representation\nuseful for the image retrieval problem. First, motivated by the relationship\nbetween the linear approximation of a nonlinear function in high dimensional\nspace and the stateof-the-art feature representation used in image retrieval,\ni.e., VLAD, we propose a new approach for the approximation. The embedded\nvectors resulted by the function approximation process are then aggregated to\nform a single representation for image retrieval. Second, in order to make the\nproposed embedding method applicable to large scale problem, we further derive\nits fast version in which the embedded vectors can be efficiently computed,\ni.e., in the closed-form. We compare the proposed embedding methods with the\nstate of the art in the context of image search under various settings: when\nthe images are represented by medium length vectors, short vectors, or binary\nvectors. The experimental results show that the proposed embedding methods\noutperform existing the state of the art on the standard public image retrieval\nbenchmarks.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-37.025238037109375, 11.152031898498535], "cluster": 0}, {"key": "do2016learning", "year": "2016", "citations": "169", "title": "Learning To Hash With Binary Deep Neural Network", "abstract": "<p>This work proposes deep network models and learning algorithms for\nunsupervised and supervised binary hashing. Our novel network design constrains\none hidden layer to directly output the binary codes. This addresses a\nchallenging issue in some previous works: optimizing non-smooth objective\nfunctions due to binarization. Moreover, we incorporate independence and\nbalance properties in the direct and strict forms in the learning. Furthermore,\nwe include similarity preserving property in our objective function. Our\nresulting optimization with these binary, independence, and balance constraints\nis difficult to solve. We propose to attack it with alternating optimization\nand careful relaxation. Experimental results on three benchmark datasets show\nthat our proposed methods compare favorably with the state of the art.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [22.830055236816406, -2.6323866844177246], "cluster": 6}, {"key": "do2017compact", "year": "2019", "citations": "23", "title": "Compact Hash Code Learning With Binary Deep Neural Network", "abstract": "<p>Learning compact binary codes for image retrieval problem using deep neural\nnetworks has recently attracted increasing attention. However, training deep\nhashing networks is challenging due to the binary constraints on the hash\ncodes. In this paper, we propose deep network models and learning algorithms\nfor learning binary hash codes given image representations under both\nunsupervised and supervised manners. The novelty of our network design is that\nwe constrain one hidden layer to directly output the binary codes. This design\nhas overcome a challenging problem in some previous works: optimizing\nnon-smooth objective functions because of binarization. In addition, we propose\nto incorporate independence and balance properties in the direct and strict\nforms into the learning schemes. We also include a similarity preserving\nproperty in our objective functions. The resulting optimizations involving\nthese binary, independence, and balance constraints are difficult to solve. To\ntackle this difficulty, we propose to learn the networks with alternating\noptimization and careful relaxation. Furthermore, by leveraging the powerful\ncapacity of convolutional neural networks, we propose an end-to-end\narchitecture that jointly learns to extract visual features and produce binary\nhash codes. Experimental results for the benchmark datasets show that the\nproposed methods compare favorably or outperform the state of the art.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [12.255436897277832, -2.600348711013794], "cluster": 6}, {"key": "do2017simultaneous", "year": "2017", "citations": "38", "title": "Simultaneous Feature Aggregating And Hashing For Large-scale Image Search", "abstract": "<p>In most state-of-the-art hashing-based visual search systems, local image\ndescriptors of an image are first aggregated as a single feature vector. This\nfeature vector is then subjected to a hashing function that produces a binary\nhash code. In previous work, the aggregating and the hashing processes are\ndesigned independently. In this paper, we propose a novel framework where\nfeature aggregating and hashing are designed simultaneously and optimized\njointly. Specifically, our joint optimization produces aggregated\nrepresentations that can be better reconstructed by some binary codes. This\nleads to more discriminative binary hash codes and improved retrieval accuracy.\nIn addition, we also propose a fast version of the recently-proposed Binary\nAutoencoder to be used in our proposed framework. We perform extensive\nretrieval experiments on several benchmark datasets with both SIFT and\nconvolutional features. Our results suggest that the proposed framework\nachieves significant improvements over the state of the art.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-10.674474716186523, 17.96477508544922], "cluster": 8}, {"key": "do2018binary", "year": "2019", "citations": "9", "title": "Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation", "abstract": "<p>Learning compact binary codes for image retrieval task using deep neural\nnetworks has attracted increasing attention recently. However, training deep\nhashing networks for the task is challenging due to the binary constraints on\nthe hash codes, the similarity preserving property, and the requirement for a\nvast amount of labelled images. To the best of our knowledge, none of the\nexisting methods has tackled all of these challenges completely in a unified\nframework. In this work, we propose a novel end-to-end deep learning approach\nfor the task, in which the network is trained to produce binary codes directly\nfrom image pixels without the need of manual annotation. In particular, to deal\nwith the non-smoothness of binary constraints, we propose a novel pairwise\nconstrained loss function, which simultaneously encodes the distances between\npairs of hash codes, and the binary quantization error. In order to train the\nnetwork with the proposed loss function, we propose an efficient parameter\nlearning algorithm. In addition, to provide similar / dissimilar training\nimages to train the network, we exploit 3D models reconstructed from unlabelled\nimages for automatic generation of enormous training image pairs. The extensive\nexperiments on image retrieval benchmark datasets demonstrate the improvements\nof the proposed method over the state-of-the-art compact representation methods\non the image retrieval problem.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [12.112776756286621, -2.8317790031433105], "cluster": 6}, {"key": "do2018selective", "year": "2019", "citations": "30", "title": "From Selective Deep Convolutional Features To Compact Binary Representations For Image Retrieval", "abstract": "<p>In the large-scale image retrieval task, the two most important requirements\nare the discriminability of image representations and the efficiency in\ncomputation and storage of representations. Regarding the former requirement,\nConvolutional Neural Network (CNN) is proven to be a very powerful tool to\nextract highly discriminative local descriptors for effective image search.\nAdditionally, in order to further improve the discriminative power of the\ndescriptors, recent works adopt fine-tuned strategies. In this paper, taking a\ndifferent approach, we propose a novel, computationally efficient, and\ncompetitive framework. Specifically, we firstly propose various strategies to\ncompute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and eliminate redundant\nfeatures. Our in-depth analyses demonstrate that proposed masking schemes are\neffective to address the burstiness drawback and improve retrieval accuracy.\nSecondly, we propose to employ recent embedding and aggregating methods which\ncan significantly boost the feature discriminability. Regarding the computation\nand storage efficiency, we include a hashing module to produce very compact\nbinary image representations. Extensive experiments on six image retrieval\nbenchmarks demonstrate that our proposed framework achieves the\nstate-of-the-art retrieval performances.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Scalability", "Tools-&-Libraries"], "tsne_embedding": [-24.314010620117188, 6.121153354644775], "cluster": 0}, {"key": "do2019simultaneous", "year": "2019", "citations": "14", "title": "Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning", "abstract": "<p>Representing images by compact hash codes is an attractive approach for\nlarge-scale content-based image retrieval. In most state-of-the-art\nhashing-based image retrieval systems, for each image, local descriptors are\nfirst aggregated as a global representation vector. This global vector is then\nsubjected to a hashing function to generate a binary hash code. In previous\nworks, the aggregating and the hashing processes are designed independently.\nHence these frameworks may generate suboptimal hash codes. In this paper, we\nfirst propose a novel unsupervised hashing framework in which feature\naggregating and hashing are designed simultaneously and optimized jointly.\nSpecifically, our joint optimization generates aggregated representations that\ncan be better reconstructed by some binary codes. This leads to more\ndiscriminative binary hash codes and improved retrieval accuracy. In addition,\nthe proposed method is flexible. It can be extended for supervised hashing.\nWhen the data label is available, the framework can be adapted to learn binary\ncodes which minimize the reconstruction loss w.r.t. label vectors. Furthermore,\nwe also propose a fast version of the state-of-the-art hashing method Binary\nAutoencoder to be used in our proposed frameworks. Extensive experiments on\nbenchmark datasets under various settings show that the proposed methods\noutperform state-of-the-art unsupervised and supervised hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-10.525411605834961, 17.856277465820312], "cluster": 8}, {"key": "doan2020hm4", "year": "2020", "citations": "0", "title": "HM4: Hidden Markov Model With Memory Management For Visual Place Recognition", "abstract": "<p>Visual place recognition needs to be robust against appearance variability\ndue to natural and man-made causes. Training data collection should thus be an\nongoing process to allow continuous appearance changes to be recorded. However,\nthis creates an unboundedly-growing database that poses time and memory\nscalability challenges for place recognition methods. To tackle the scalability\nissue for visual place recognition in autonomous driving, we develop a Hidden\nMarkov Model approach with a two-tiered memory management. Our algorithm,\ndubbed HM\\(^4\\), exploits temporal look-ahead to transfer promising candidate\nimages between passive storage and active memory when needed. The inference\nprocess takes into account both promising images and a coarse representations\nof the full database. We show that this allows constant time and space\ninference for a fixed coverage area. The coarse representations can also be\nupdated incrementally to absorb new data. To further reduce the memory\nrequirements, we derive a compact image representation inspired by Locality\nSensitive Hashing (LSH). Through experiments on real world data, we demonstrate\nthe excellent scalability and accuracy of the approach under appearance changes\nand provide comparisons against state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "Datasets"], "tsne_embedding": [-12.493576049804688, -8.765911102294922], "cluster": 1}, {"key": "doan2020image", "year": "2020", "citations": "4", "title": "Image Hashing By Minimizing Discrete Component-wise Wasserstein Distance", "abstract": "<p>Image hashing is one of the fundamental problems that demand both efficient\nand effective solutions for various practical scenarios. Adversarial\nautoencoders are shown to be able to implicitly learn a robust,\nlocality-preserving hash function that generates balanced and high-quality hash\ncodes. However, the existing adversarial hashing methods are inefficient to be\nemployed for large-scale image retrieval applications. Specifically, they\nrequire an exponential number of samples to be able to generate optimal hash\ncodes and a significantly high computational cost to train. In this paper, we\nshow that the high sample-complexity requirement often results in sub-optimal\nretrieval performance of the adversarial hashing methods. To address this\nchallenge, we propose a new adversarial-autoencoder hashing approach that has a\nmuch lower sample requirement and computational cost. Specifically, by\nexploiting the desired properties of the hash function in the low-dimensional,\ndiscrete space, our method efficiently estimates a better variant of\nWasserstein distance by averaging a set of easy-to-compute one-dimensional\nWasserstein distances. The resulting hashing approach has an order-of-magnitude\nbetter sample complexity, thus better generalization property, compared to the\nother adversarial hashing methods. In addition, the computational cost is\nsignificantly reduced using our approach. We conduct experiments on several\nreal-world datasets and show that the proposed method outperforms the competing\nhashing methods, achieving up to 10% improvement over the current\nstate-of-the-art image hashing methods. The code accompanying this paper is\navailable on Github (https://github.com/khoadoan/adversarial-hashing).</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [0.23299263417720795, 20.970319747924805], "cluster": 8}, {"key": "doan2022asymmetric", "year": "2023", "citations": "3", "title": "Asymmetric Hashing For Fast Ranking Via Neural Network Measures", "abstract": "<p>Fast item ranking is an important task in recommender systems. In previous\nworks, graph-based Approximate Nearest Neighbor (ANN) approaches have\ndemonstrated good performance on item ranking tasks with generic\nsearching/matching measures (including complex measures such as neural network\nmeasures). However, since these ANN approaches must go through the neural\nmeasures several times during ranking, the computation is not practical if the\nneural measure is a large network. On the other hand, fast item ranking using\nexisting hashing-based approaches, such as Locality Sensitive Hashing (LSH),\nonly works with a limited set of measures. Previous learning-to-hash approaches\nare also not suitable to solve the fast item ranking problem since they can\ntake a significant amount of time and computation to train the hash functions.\nHashing approaches, however, are attractive because they provide a principle\nand efficient way to retrieve candidate items. In this paper, we propose a\nsimple and effective learning-to-hash approach for the fast item ranking\nproblem that can be used for any type of measure, including neural network\nmeasures. Specifically, we solve this problem with an asymmetric hashing\nframework based on discrete inner product fitting. We learn a pair of related\nhash functions that map heterogeneous objects (e.g., users and items) into a\ncommon discrete space where the inner product of their binary codes reveals\ntheir true similarity defined via the original searching measure. The fast\nranking problem is reduced to an ANN search via this asymmetric hashing scheme.\nThen, we propose a sampling strategy to efficiently select relevant and\ncontrastive samples to train the hashing model. We empirically validate the\nproposed method against the existing state-of-the-art fast item ranking methods\nin several combinations of non-linear searching functions and prominent\ndatasets.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "Similarity-Search", "SIGIR", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [42.19944763183594, 3.742222785949707], "cluster": 9}, {"key": "doan2022coophash", "year": "2022", "citations": "2", "title": "Coophash: Cooperative Learning Of Multipurpose Descriptor And Contrastive Pair Generator Via Variational MCMC Teaching For Supervised Image Hashing", "abstract": "<p>Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Robustness", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [2.652132272720337, 5.787222862243652], "cluster": 6}, {"key": "doan2022one", "year": "2022", "citations": "38", "title": "One Loss For Quantization: Deep Hashing With Discrete Wasserstein Distributional Matching", "abstract": "<p>Image hashing is a principled approximate nearest neighbor approach to find\nsimilar items to a query in a large collection of images. Hashing aims to learn\na binary-output function that maps an image to a binary vector. For optimal\nretrieval performance, producing balanced hash codes with low-quantization\nerror to bridge the gap between the learning stage\u2019s continuous relaxation and\nthe inference stage\u2019s discrete quantization is important. However, in the\nexisting deep supervised hashing methods, coding balance and low-quantization\nerror are difficult to achieve and involve several losses. We argue that this\nis because the existing quantization approaches in these methods are\nheuristically constructed and not effective to achieve these objectives. This\npaper considers an alternative approach to learning the quantization\nconstraints. The task of learning balanced codes with low quantization error is\nre-formulated as matching the learned distribution of the continuous codes to a\npre-defined discrete, uniform distribution. This is equivalent to minimizing\nthe distance between two distributions. We then propose a computationally\nefficient distributional distance by leveraging the discrete property of the\nhash functions. This distributional distance is a valid distance and enjoys\nlower time and sample complexities. The proposed single-loss quantization\nobjective can be integrated into any existing supervised hashing method to\nimprove code balance and quantization error. Experiments confirm that the\nproposed approach substantially improves the performance of several\nrepresentative hashing~methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "CVPR", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [-5.535898208618164, 17.616636276245117], "cluster": 8}, {"key": "dodds2018learning", "year": "2018", "citations": "4", "title": "Learning Embeddings For Product Visual Search With Triplet Loss And Online Sampling", "abstract": "<p>In this paper, we propose learning an embedding function for content-based\nimage retrieval within the e-commerce domain using the triplet loss and an\nonline sampling method that constructs triplets from within a minibatch. We\ncompare our method to several strong baselines as well as recent works on the\nDeepFashion and Stanford Online Product datasets. Our approach significantly\noutperforms the state-of-the-art on the DeepFashion dataset. With a\nmodification to favor sampling minibatches from a single product category, the\nsame approach demonstrates competitive results when compared to the\nstate-of-the-art for the Stanford Online Products dataset.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-14.294763565063477, -31.66921043395996], "cluster": 3}, {"key": "doh2024enriching", "year": "2024", "citations": "1", "title": "Enriching Music Descriptions With A Finetuned-llm And Metadata For Text-to-music Retrieval", "abstract": "<p>Text-to-Music Retrieval, finding music based on a given natural language\nquery, plays a pivotal role in content discovery within extensive music\ndatabases. To address this challenge, prior research has predominantly focused\non a joint embedding of music audio and text, utilizing it to retrieve music\ntracks that exactly match descriptive queries related to musical attributes\n(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,\nusers also articulate a need to explore music that shares similarities with\ntheir favorite tracks or artists, such as \\textit{I need a similar track to\nSuperstition by Stevie Wonder}. To address these concerns, this paper proposes\nan improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes\nrich text descriptions generated with a finetuned large language model and\nmetadata. To accomplish this, we obtained various types of seed text from\nseveral existing music tag and caption datasets and a knowledge graph dataset\nof artists and tracks. The experimental results show the effectiveness of\nTTMR++ in comparison to state-of-the-art music-text joint embedding models\nthrough a comprehensive evaluation involving various musical text queries.</p>\n", "tags": ["ICASSP", "Evaluation", "Datasets"], "tsne_embedding": [11.255029678344727, -44.851707458496094], "cluster": 3}, {"key": "dolhansky2020adversarial", "year": "2020", "citations": "9", "title": "Adversarial Collision Attacks On Image Hashing Functions", "abstract": "<p>Hashing images with a perceptual algorithm is a common approach to solving\nduplicate image detection problems. However, perceptual image hashing\nalgorithms are differentiable, and are thus vulnerable to gradient-based\nadversarial attacks. We demonstrate that not only is it possible to modify an\nimage to produce an unrelated hash, but an exact image hash collision between a\nsource and target image can be produced via minuscule adversarial\nperturbations. In a white box setting, these collisions can be replicated\nacross nearly every image pair and hash type (including both deep and\nnon-learned hashes). Furthermore, by attacking points other than the output of\na hashing function, an attacker can avoid having to know the details of a\nparticular algorithm, resulting in collisions that transfer across different\nhash sizes or model architectures. Using these techniques, an adversary can\npoison the image lookup table of a duplicate image detection service, resulting\nin undefined or unwanted behavior. Finally, we offer several potential\nmitigations to gradient-based image hash attacks.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Robustness"], "tsne_embedding": [-9.210886001586914, 16.13580894470215], "cluster": 8}, {"key": "dong2017cross", "year": "2018", "citations": "26", "title": "Cross-media Similarity Evaluation For Web Image Retrieval In The Wild", "abstract": "<p>In order to retrieve unlabeled images by textual queries, cross-media\nsimilarity computation is a key ingredient. Although novel methods are\ncontinuously introduced, little has been done to evaluate these methods\ntogether with large-scale query log analysis. Consequently, how far have these\nmethods brought us in answering real-user queries is unclear. Given baseline\nmethods that compute cross-media similarity using relatively simple text/image\nmatching, how much progress have advanced models made is also unclear. This\npaper takes a pragmatic approach to answering the two questions. Queries are\nautomatically categorized according to the proposed query visualness measure,\nand later connected to the evaluation of multiple cross-media similarity models\non three test sets. Such a connection reveals that the success of the\nstate-of-the-art is mainly attributed to their good performance on\nvisual-oriented queries, while these queries account for only a small part of\nreal-user queries. To quantify the current progress, we propose a simple\ntext2image method, representing a novel test query by a set of images selected\nfrom large-scale query log. Consequently, computing cross-media similarity\nbetween the test query and a given image boils down to comparing the visual\nsimilarity between the given image and the selected images. Image retrieval\nexperiments on the challenging Clickture dataset show that the proposed\ntext2image compares favorably to recent deep learning based alternatives.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-45.251182556152344, -16.139652252197266], "cluster": 5}, {"key": "dong2017video", "year": "2018", "citations": "17", "title": "Video Retrieval Based On Deep Convolutional Neural Network", "abstract": "<p>Recently, with the enormous growth of online videos, fast video retrieval\nresearch has received increasing attention. As an extension of image hashing\ntechniques, traditional video hashing methods mainly depend on hand-crafted\nfeatures and transform the real-valued features into binary hash codes. As\nvideos provide far more diverse and complex visual information than images,\nextracting features from videos is much more challenging than that from images.\nTherefore, high-level semantic features to represent videos are needed rather\nthan low-level hand-crafted methods. In this paper, a deep convolutional neural\nnetwork is proposed to extract high-level semantic features and a binary hash\nfunction is then integrated into this framework to achieve an end-to-end\noptimization. Particularly, our approach also combines triplet loss function\nwhich preserves the relative similarity and difference of videos and\nclassification loss function as the optimization objective. Experiments have\nbeen performed on two public datasets and the results demonstrate the\nsuperiority of our proposed method compared with other state-of-the-art video\nretrieval methods.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Video-Retrieval"], "tsne_embedding": [-10.79893684387207, -3.41605806350708], "cluster": 1}, {"key": "dong2019document", "year": "2019", "citations": "19", "title": "Document Hashing With Mixture-prior Generative Models", "abstract": "<p>Hashing is promising for large-scale information retrieval tasks thanks to\nthe efficiency of distance evaluation between binary codes. Generative hashing\nis often used to generate hashing codes in an unsupervised way. However,\nexisting generative hashing methods only considered the use of simple priors,\nlike Gaussian and Bernoulli priors, which limits these methods to further\nimprove their performance. In this paper, two mixture-prior generative models\nare proposed, under the objective to produce high-quality hashing codes for\ndocuments. Specifically, a Gaussian mixture prior is first imposed onto the\nvariational auto-encoder (VAE), followed by a separate step to cast the\ncontinuous latent representation of VAE into binary code. To avoid the\nperformance loss caused by the separate casting, a model using a Bernoulli\nmixture prior is further developed, in which an end-to-end training is admitted\nby resorting to the straight-through (ST) discrete gradient estimator.\nExperimental results on several benchmark datasets demonstrate that the\nproposed methods, especially the one using Bernoulli mixture priors,\nconsistently outperform existing ones by a substantial margin.</p>\n", "tags": ["Hashing-Methods", "EMNLP", "Efficiency", "Scalability", "Datasets", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [-5.828453540802002, 20.38613510131836], "cluster": 8}, {"key": "dong2019learning", "year": "2020", "citations": "26", "title": "Learning Space Partitions For Nearest Neighbor Search", "abstract": "<p>Space partitions of \\(\\mathbb{R}^d\\) underlie a vast and important class of\nfast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for building space\npartitions reducing the problem to balanced graph partitioning followed by\nsupervised classification. We instantiate this general approach with the KaHIP\ngraph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,\nto obtain a new partitioning procedure called Neural Locality-Sensitive Hashing\n(Neural LSH). On several standard benchmarks for NNS, our experiments show that\nthe partitions obtained by Neural LSH consistently outperform partitions found\nby quantization-based and tree-based methods as well as classic, data-oblivious\nLSH.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization", "Tree-Based-Ann", "Tools-&-Libraries", "Supervised"], "tsne_embedding": [47.05221939086914, 10.56781005859375], "cluster": 9}, {"key": "dong2020learning", "year": "2020", "citations": "26", "title": "Learning Space Partitions For Nearest Neighbor Search", "abstract": "<p>Space partitions of underlie a vast and important\nclass of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.\nWe instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization", "Tree-Based-Ann", "Tools-&-Libraries", "Supervised"], "tsne_embedding": [47.12167739868164, 10.70120906829834], "cluster": 9}, {"key": "dong2020using", "year": "2021", "citations": "5", "title": "Using Text To Teach Image Retrieval", "abstract": "<p>Image retrieval relies heavily on the quality of the data modeling and the\ndistance measurement in the feature space. Building on the concept of image\nmanifold, we first propose to represent the feature space of images, learned\nvia neural networks, as a graph. Neighborhoods in the feature space are now\ndefined by the geodesic distance between images, represented as graph vertices\nor manifold samples. When limited images are available, this manifold is\nsparsely sampled, making the geodesic computation and the corresponding\nretrieval harder. To address this, we augment the manifold samples with\ngeometrically aligned text, thereby using a plethora of sentences to teach us\nabout images. In addition to extensive results on standard datasets\nillustrating the power of text to help in image retrieval, a new public dataset\nbased on CLEVR is introduced to quantify the semantic similarity between visual\ndata and text data. The experimental results show that the joint embedding\nmanifold is a robust representation, allowing it to be a better basis to\nperform image retrieval given only an image and a textual instruction on the\ndesired modifications over the image</p>\n", "tags": ["CVPR", "Image-Retrieval", "Datasets"], "tsne_embedding": [54.80786895751953, -7.407494068145752], "cluster": 9}, {"key": "dong2022generalized", "year": "2018", "citations": "3", "title": "A Generalized Approach For Cancellable Template And Its Realization For Minutia Cylinder-code", "abstract": "<p>Hashing technology gains much attention in protecting the biometric template\nlately. For instance, Index-of-Max (IoM), a recent reported hashing technique,\nis a ranking-based locality sensitive hashing technique, which illustrates the\nfeasibility to protect the ordered and fixed-length biometric template.\nHowever, biometric templates are not always in the form of ordered and\nfixed-length, rather it may be an unordered and variable size point set e.g.\nfingerprint minutiae, which restricts the usage of the traditional hashing\ntechnology. In this paper, we proposed a generalized version of IoM hashing\nnamely gIoM, and therefore the unordered and variable size biometric template\ncan be used. We demonstrate a realization using a well-known variable size\nfeature vector, fingerprint Minutia Cylinder-Code (MCC). The gIoM transforms\nMCC into index domain to form indexing-based feature representation.\nConsequently, the inversion of MCC from the transformed representation is\ncomputational infeasible, thus to achieve non-invertibility while the\nperformance is preserved. Public fingerprint databases FVC2002 and FVC2004 are\nemployed for experiment as benchmark to demonstrate a fair comparison with\nother methods. Moreover, the security and privacy analysis suggest that gIoM\nmeets the criteria of template protection: non-invertibility, revocability, and\nnon-linkability.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-13.450950622558594, 29.29764747619629], "cluster": 8}, {"key": "dong2022learning", "year": "2023", "citations": "4", "title": "Learning-based Dimensionality Reduction For Computing Compact And Effective Local Feature Descriptors", "abstract": "<p>A distinctive representation of image patches in form of features is a key\ncomponent of many computer vision and robotics tasks, such as image matching,\nimage retrieval, and visual localization. State-of-the-art descriptors, from\nhand-crafted descriptors such as SIFT to learned ones such as HardNet, are\nusually high dimensional; 128 dimensions or even more. The higher the\ndimensionality, the larger the memory consumption and computational time for\napproaches using such descriptors. In this paper, we investigate multi-layer\nperceptrons (MLPs) to extract low-dimensional but high-quality descriptors. We\nthoroughly analyze our method in unsupervised, self-supervised, and supervised\nsettings, and evaluate the dimensionality reduction results on four\nrepresentative descriptors. We consider different applications, including\nvisual localization, patch verification, image matching and retrieval. The\nexperiments show that our lightweight MLPs achieve better dimensionality\nreduction than PCA. The lower-dimensional descriptors generated by our approach\noutperform the original higher-dimensional descriptors in downstream tasks,\nespecially for the hand-crafted ones. The code will be available at\nhttps://github.com/PRBonn/descriptor-dr.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Supervised", "Unsupervised"], "tsne_embedding": [-35.50433349609375, -2.9679195880889893], "cluster": 0}, {"key": "dong2023region", "year": "2023", "citations": "9", "title": "From Region To Patch: Attribute-aware Foreground-background Contrastive Learning For Fine-grained Fashion Retrieval", "abstract": "<p>Attribute-specific fashion retrieval (ASFR) is a challenging information\nretrieval task, which has attracted increasing attention in recent years.\nDifferent from traditional fashion retrieval which mainly focuses on optimizing\nholistic similarity, the ASFR task concentrates on attribute-specific\nsimilarity, resulting in more fine-grained and interpretable retrieval results.\nAs the attribute-specific similarity typically corresponds to the specific\nsubtle regions of images, we propose a Region-to-Patch Framework (RPF) that\nconsists of a region-aware branch and a patch-aware branch to extract\nfine-grained attribute-related visual features for precise retrieval in a\ncoarse-to-fine manner. In particular, the region-aware branch is first to be\nutilized to locate the potential regions related to the semantic of the given\nattribute. Then, considering that the located region is coarse and still\ncontains the background visual contents, the patch-aware branch is proposed to\ncapture patch-wise attribute-related details from the previous amplified\nregion. Such a hybrid architecture strikes a proper balance between region\nlocalization and feature extraction. Besides, different from previous works\nthat solely focus on discriminating the attribute-relevant foreground visual\nfeatures, we argue that the attribute-irrelevant background features are also\ncrucial for distinguishing the detailed visual contexts in a contrastive\nmanner. Therefore, a novel E-InfoNCE loss based on the foreground and\nbackground representations is further proposed to improve the discrimination of\nattribute-specific representation. Extensive experiments on three datasets\ndemonstrate the effectiveness of our proposed framework, and also show a decent\ngeneralization of our RPF on out-of-domain fashion images. Our source code is\navailable at https://github.com/HuiGuanLab/RPF.</p>\n", "tags": ["SIGIR", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-26.887073516845703, 3.2606918811798096], "cluster": 0}, {"key": "dong2023seine", "year": "2023", "citations": "0", "title": "SEINE: Segment-based Indexing For Neural Information Retrieval", "abstract": "<p>Many early neural Information Retrieval (NeurIR) methods are re-rankers that\nrely on a traditional first-stage retriever due to expensive query time\ncomputations. Recently, representation-based retrievers have gained much\nattention, which learns query representation and document representation\nseparately, making it possible to pre-compute document representations offline\nand reduce the workload at query time. Both dense and sparse\nrepresentation-based retrievers have been explored. However, these methods\nfocus on finding the representation that best represents a text (aka metric\nlearning) and the actual retrieval function that is responsible for similarity\nmatching between query and document is kept at a minimum by using dot product.\nOne drawback is that unlike traditional term-level inverted index, the index\nformed by these embeddings cannot be easily re-used by another retrieval\nmethod. Another drawback is that keeping the interaction at minimum hurts\nretrieval effectiveness. On the contrary, interaction-based retrievers are\nknown for their better retrieval effectiveness. In this paper, we propose a\nnovel SEgment-based Neural Indexing method, SEINE, which provides a general\nindexing framework that can flexibly support a variety of interaction-based\nneural retrieval methods. We emphasize on a careful decomposition of common\ncomponents in existing neural retrieval methods and propose to use\nsegment-level inverted index to store the atomic query-document interaction\nvalues. Experiments on LETOR MQ2007 and MQ2008 datasets show that our indexing\nmethod can accelerate multiple neural retrieval methods up to 28-times faster\nwithout sacrificing much effectiveness.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [6.624911308288574, -19.88425636291504], "cluster": 7}, {"key": "dong2024ervd", "year": "2024", "citations": "0", "title": "ERVD: An Efficient And Robust Vit-based Distillation Framework For Remote Sensing Image Retrieval", "abstract": "<p>ERVD: An Efficient and Robust ViT-Based Distillation Framework for Remote\nSensing Image Retrieval</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval"], "tsne_embedding": [-11.898554801940918, -30.517478942871094], "cluster": 3}, {"key": "dong2025learning", "year": "2020", "citations": "26", "title": "Learning Space Partitions For Nearest Neighbor Search", "abstract": "<p>Space partitions of underlie a vast and important\nclass of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.\nWe instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization", "Tree-Based-Ann", "Tools-&-Libraries", "Supervised"], "tsne_embedding": [47.12184143066406, 10.70113754272461], "cluster": 9}, {"key": "doras2019cover", "year": "2019", "citations": "22", "title": "Cover Detection Using Dominant Melody Embeddings", "abstract": "<p>Automatic cover detection \u2013 the task of finding in an audio database all the\ncovers of one or several query tracks \u2013 has long been seen as a challenging\ntheoretical problem in the MIR community and as an acute practical problem for\nauthors and composers societies. Original algorithms proposed for this task\nhave proven their accuracy on small datasets, but are unable to scale up to\nmodern real-life audio corpora. On the other hand, faster approaches designed\nto process thousands of pairwise comparisons resulted in lower accuracy, making\nthem unsuitable for practical use.\n  In this work, we propose a neural network architecture that is trained to\nrepresent each track as a single embedding vector. The computation burden is\ntherefore left to the embedding extraction \u2013 that can be conducted offline and\nstored, while the pairwise comparison task reduces to a simple Euclidean\ndistance computation. We further propose to extract each track\u2019s embedding out\nof its dominant melody representation, obtained by another neural network\ntrained for this task. We then show that this architecture improves\nstate-of-the-art accuracy both on small and large datasets, and is able to\nscale to query databases of thousands of tracks in a few seconds.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [39.975704193115234, -12.034600257873535], "cluster": 9}, {"key": "dordevic2024evidential", "year": "2024", "citations": "0", "title": "Evidential Transformers For Improved Image Retrieval", "abstract": "<p>We introduce the Evidential Transformer, an uncertainty-driven transformer\nmodel for improved and robust image retrieval. In this paper, we make several\ncontributions to content-based image retrieval (CBIR). We incorporate\nprobabilistic methods into image retrieval, achieving robust and reliable\nresults, with evidential classification surpassing traditional training based\non multiclass classification as a baseline for deep metric learning.\nFurthermore, we improve the state-of-the-art retrieval results on several\ndatasets by leveraging the Global Context Vision Transformer (GC ViT)\narchitecture. Our experimental results consistently demonstrate the reliability\nof our approach, setting a new benchmark in CBIR in all test settings on the\nStanford Online Products (SOP) and CUB-200-2011 datasets.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-14.362765312194824, -29.62983512878418], "cluster": 3}, {"key": "dorfer2017end", "year": "2018", "citations": "32", "title": "End-to-end Cross-modality Retrieval With CCA Projections And Pairwise Ranking Loss", "abstract": "<p>Cross-modality retrieval encompasses retrieval tasks where the fetched items\nare of a different type than the search query, e.g., retrieving pictures\nrelevant to a given text query. The state-of-the-art approach to cross-modality\nretrieval relies on learning a joint embedding space of the two modalities,\nwhere items from either modality are retrieved using nearest-neighbor search.\nIn this work, we introduce a neural network layer based on Canonical\nCorrelation Analysis (CCA) that learns better embedding spaces by analytically\ncomputing projections that maximize correlation. In contrast to previous\napproaches, the CCA Layer (CCAL) allows us to combine existing objectives for\nembedding space learning, such as pairwise ranking losses, with the optimal\nprojections of CCA. We show the effectiveness of our approach for\ncross-modality retrieval on three different scenarios (text-to-image,\naudio-sheet-music and zero-shot retrieval), surpassing both Deep CCA and a\nmulti-view network using freely learned projections optimized by a pairwise\nranking loss, especially when little training data is available (the code for\nall three methods is released at: https://github.com/CPJKU/cca_layer).</p>\n", "tags": ["Text-Retrieval", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [-19.109609603881836, -4.158453464508057], "cluster": 1}, {"key": "doshi2020lanns", "year": "2020", "citations": "0", "title": "LANNS: A Web-scale Approximate Nearest Neighbor Lookup System", "abstract": "<p>Nearest neighbor search (NNS) has a wide range of applications in information\nretrieval, computer vision, machine learning, databases, and other areas.\nExisting state-of-the-art algorithm for nearest neighbor search, Hierarchical\nNavigable Small World Networks(HNSW), is unable to scale to large datasets of\n100M records in high dimensions. In this paper, we propose LANNS, an end-to-end\nplatform for Approximate Nearest Neighbor Search, which scales for web-scale\ndatasets. Library for Large Scale Approximate Nearest Neighbor Search (LANNS)\nis deployed in multiple production systems for identifying topK (\\(100 \\leq topK\n\\leq 200\\)) approximate nearest neighbors with a latency of a few milliseconds\nper query, high throughput of 2.5k Queries Per Second (QPS) on a single node,\non large (\\(\\sim\\)180M data points) high dimensional (50-2048 dimensional)\ndatasets.</p>\n", "tags": ["Graph-Based-Ann", "Scalability", "Tools-&-Libraries", "Large-Scale-Search", "Datasets"], "tsne_embedding": [49.75649642944336, 19.612619400024414], "cluster": 9}, {"key": "dou2020learning", "year": "2020", "citations": "7", "title": "Learning Global And Local Consistent Representations For Unsupervised Image Retrieval Via Deep Graph Diffusion Networks", "abstract": "<p>Diffusion has shown great success in improving accuracy of unsupervised image\nretrieval systems by utilizing high-order structures of image manifold.\nHowever, existing diffusion methods suffer from three major limitations: 1)\nthey usually rely on local structures without considering global manifold\ninformation; 2) they focus on improving pair-wise similarities within existing\nimages input output transductively while lacking flexibility to learn\nrepresentations for novel unseen instances inductively; 3) they fail to scale\nto large datasets due to prohibitive memory consumption and computational\nburden due to intrinsic high-order operations on the whole graph. In this\npaper, to address these limitations, we propose a novel method, Graph Diffusion\nNetworks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant\nof deep learning algorithms on irregular graphs. GRAD-Net learns semantic\nrepresentations by exploiting both local and global structures of image\nmanifold in an unsupervised fashion. By utilizing sparse coding techniques,\nGRAD-Net not only preserves global information on the image manifold, but also\nenables scalable training and efficient querying. Experiments on several large\nbenchmark datasets demonstrate effectiveness of our method over\nstate-of-the-art diffusion algorithms for unsupervised image retrieval.</p>\n", "tags": ["Unsupervised", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [53.24470901489258, 1.8121061325073242], "cluster": 9}, {"key": "dourado2019fusion", "year": "2019", "citations": "2", "title": "Fusion Vectors: Embedding Graph Fusions For Efficient Unsupervised Rank Aggregation", "abstract": "<p>The vast increase in amount and complexity of digital content led to a wide\ninterest in ad-hoc retrieval systems in recent years. Complementary, the\nexistence of heterogeneous data sources and retrieval models stimulated the\nproliferation of increasingly ingenious and effective rank aggregation\nfunctions. Although recently proposed rank aggregation functions are promising\nwith respect to effectiveness, existing proposals in the area usually overlook\nefficiency aspects. We propose an innovative rank aggregation function that is\nunsupervised, intrinsically multimodal, and targeted for fast retrieval and top\neffectiveness performance. We introduce the concepts of embedding and indexing\nof graph-based rank-aggregation representation models, and their application\nfor search tasks. Embedding formulations are also proposed for graph-based rank\nrepresentations. We introduce the concept of fusion vectors, a late-fusion\nrepresentation of objects based on ranks, from which an intrinsically\nrank-aggregation retrieval model is defined. Next, we present an approach for\nfast retrieval based on fusion vectors, thus promoting an efficient rank\naggregation system. Our method presents top effectiveness performance among\nstate-of-the-art related work, while bringing novel aspects of multimodality\nand effectiveness. Consistent speedups are achieved against the recent\nbaselines in all datasets considered.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [41.9697265625, 9.22381591796875], "cluster": 9}, {"key": "douze2016polysemous", "year": "2016", "citations": "45", "title": "Polysemous Codes", "abstract": "<p>This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90\u2019s to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine.</p>\n", "tags": ["Efficiency", "Quantization", "Vector-Indexing", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [27.36966323852539, 28.866453170776367], "cluster": 2}, {"key": "douze2018link", "year": "2018", "citations": "31", "title": "Link And Code: Fast Indexing With Graphs And Compact Regression Codes", "abstract": "<p>Similarity search approaches based on graph walks have recently attained\noutstanding speed-accuracy trade-offs, taking aside the memory requirements. In\nthis paper, we revisit these approaches by considering, additionally, the\nmemory constraint required to index billions of images on a single server. This\nleads us to propose a method based both on graph traversal and compact\nrepresentations. We encode the indexed vectors using quantization and exploit\nthe graph structure to refine the similarity estimation.\n  In essence, our method takes the best of these two worlds: the search\nstrategy is based on nested graphs, thereby providing high precision with a\nrelatively small set of comparisons. At the same time it offers a significant\nmemory compression. As a result, our approach outperforms the state of the art\non operating points considering 64-128 bytes per vector, as demonstrated by our\nresults on two billion-scale public benchmarks.</p>\n", "tags": ["Quantization", "CVPR", "Similarity-Search", "Scalability", "Large-Scale-Search", "Evaluation"], "tsne_embedding": [56.39972686767578, 6.1090216636657715], "cluster": 9}, {"key": "douze2024faiss", "year": "2024", "citations": "17", "title": "The Faiss Library", "abstract": "<p>Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Similarity-Search"], "tsne_embedding": [-21.257381439208984, -36.589332580566406], "cluster": 3}, {"key": "driemel2017locality", "year": "2017", "citations": "31", "title": "Locality-sensitive Hashing Of Curves", "abstract": "<p>We study data structures for storing a set of polygonal curves in \\({\\rm R}^d\\)\nsuch that, given a query curve, we can efficiently retrieve similar curves from\nthe set, where similarity is measured using the discrete Fr'echet distance or\nthe dynamic time warping distance. To this end we devise the first\nlocality-sensitive hashing schemes for these distance measures. A major\nchallenge is posed by the fact that these distance measures internally optimize\nthe alignment between the curves. We give solutions for different types of\nalignments including constrained and unconstrained versions. For unconstrained\nalignments, we improve over a result by Indyk from 2002 for short curves. Let\n\\(n\\) be the number of input curves and let \\(m\\) be the maximum complexity of a\ncurve in the input. In the particular case where \\(m \\leq \\frac{\\alpha}{4d} log\nn\\), for some fixed \\(\\alpha&gt;0\\), our solutions imply an approximate near-neighbor\ndata structure for the discrete Fr'echet distance that uses space in\n\\(O(n^{1+\\alpha}log n)\\) and achieves query time in \\(O(n^{\\alpha}log^2 n)\\) and\nconstant approximation factor. Furthermore, our solutions provide a trade-off\nbetween approximation quality and computational performance: for any parameter\n\\(k \\in [m]\\), we can give a data structure that uses space in \\(O(2^{2k}m^{k-1} n\nlog n + nm)\\), answers queries in \\(O( 2^{2k} m^{k}log n)\\) time and achieves\napproximation factor in \\(O(m/k)\\).</p>\n", "tags": ["Efficiency", "Evaluation", "Hashing-Methods"], "tsne_embedding": [22.33474349975586, 46.89558410644531], "cluster": 4}, {"key": "duan2020slade", "year": "2021", "citations": "8", "title": "SLADE: A Self-training Framework For Distance Metric Learning", "abstract": "<p>Most existing distance metric learning approaches use fully labeled data to\nlearn the sample similarities in an embedding space. We present a self-training\nframework, SLADE, to improve retrieval performance by leveraging additional\nunlabeled data. We first train a teacher model on the labeled data and use it\nto generate pseudo labels for the unlabeled data. We then train a student model\non both labels and pseudo labels to generate final feature embeddings. We use\nself-supervised representation learning to initialize the teacher model. To\nbetter deal with noisy pseudo labels generated by the teacher network, we\ndesign a new feature basis learning component for the student network, which\nlearns basis functions of feature representations for unlabeled data. The\nlearned basis vectors better measure the pairwise similarity and are used to\nselect high-confident samples for training the student network. We evaluate our\nmethod on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop.\nExperimental results demonstrate that our approach significantly improves the\nperformance over the state-of-the-art methods.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "CVPR", "Tools-&-Libraries", "Supervised", "Evaluation"], "tsne_embedding": [26.507741928100586, -39.89030075073242], "cluster": 7}, {"key": "dubey2018ldop", "year": "2019", "citations": "38", "title": "LDOP: Local Directional Order Pattern For Robust Face Retrieval", "abstract": "<p>The local descriptors have gained wide range of attention due to their\nenhanced discriminative abilities. It has been proved that the consideration of\nmulti-scale local neighborhood improves the performance of the descriptor,\nthough at the cost of increased dimension. This paper proposes a novel method\nto construct a local descriptor using multi-scale neighborhood by finding the\nlocal directional order among the intensity values at different scales in a\nparticular direction. Local directional order is the multi-radius relationship\nfactor in a particular direction. The proposed local directional order pattern\n(LDOP) for a particular pixel is computed by finding the relationship between\nthe center pixel and local directional order indexes. It is required to\ntransform the center value into the range of neighboring orders. Finally, the\nhistogram of LDOP is computed over whole image to construct the descriptor. In\ncontrast to the state-of-the-art descriptors, the dimension of the proposed\ndescriptor does not depend upon the number of neighbors involved to compute the\norder; it only depends upon the number of directions. The introduced descriptor\nis evaluated over the image retrieval framework and compared with the\nstate-of-the-art descriptors over challenging face databases such as PaSC, LFW,\nPubFig, FERET, AR, AT&amp;T, and ExtendedYale. The experimental results confirm the\nsuperiority and robustness of the LDOP descriptor.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval", "Robustness"], "tsne_embedding": [-22.8369197845459, 17.421167373657227], "cluster": 8}, {"key": "dubey2020decade", "year": "2021", "citations": "242", "title": "A Decade Survey Of Content Based Image Retrieval Using Deep Learning", "abstract": "<p>The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.</p>\n", "tags": ["Survey-Paper", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-21.97968101501465, -26.09378433227539], "cluster": 5}, {"key": "dubey2021vision", "year": "2022", "citations": "47", "title": "Vision Transformer Hashing For Image Retrieval", "abstract": "<p>Deep learning has shown a tremendous growth in hashing techniques for image\nretrieval. Recently, Transformer has emerged as a new architecture by utilizing\nself-attention without convolution. Transformer is also extended to Vision\nTransformer (ViT) for the visual recognition with a promising performance on\nImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)\nfor image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone\nnetwork and add the hashing head. The proposed VTS model is fine tuned for\nhashing under six different image retrieval frameworks, including Deep\nSupervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network\n(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)\nwith their objective functions. We perform the extensive experiments on\nCIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image\nretrieval outperforms the recent state-of-the-art hashing techniques with a\ngreat margin. We also find the proposed VTS model as the backbone network is\nbetter than the existing networks, such as AlexNet and ResNet. The code is\nreleased at https://github.com/shivram1987/VisionTransformerHashing.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-1.663720965385437, 1.8159226179122925], "cluster": 1}, {"key": "dubey2024transformer", "year": "2024", "citations": "1", "title": "Transformer-based Clipped Contrastive Quantization Learning For Unsupervised Image Retrieval", "abstract": "<p>Unsupervised image retrieval aims to learn the important visual\ncharacteristics without any given level to retrieve the similar images for a\ngiven query image. The Convolutional Neural Network (CNN)-based approaches have\nbeen extensively exploited with self-supervised contrastive learning for image\nhashing. However, the existing approaches suffer due to lack of effective\nutilization of global features by CNNs and biased-ness created by false\nnegative pairs in the contrastive learning. In this paper, we propose a\nTransClippedCLR model by encoding the global context of an image using\nTransformer having local context through patch based processing, by generating\nthe hash codes through product quantization and by avoiding the potential false\nnegative pairs through clipped contrastive learning. The proposed model is\ntested with superior performance for unsupervised image retrieval on benchmark\ndatasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent\nstate-of-the-art deep models. The results using the proposed clipped\ncontrastive learning are greatly improved on all datasets as compared to same\nbackbone network with vanilla contrastive learning.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Quantization", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-37.58220672607422, -0.5174964666366577], "cluster": 0}, {"key": "ducau2019automatic", "year": "2019", "citations": "11", "title": "Automatic Malware Description Via Attribute Tagging And Similarity Embedding", "abstract": "<p>With the rapid proliferation and increased sophistication of malicious\nsoftware (malware), detection methods no longer rely only on manually generated\nsignatures but have also incorporated more general approaches like machine\nlearning detection. Although powerful for conviction of malicious artifacts,\nthese methods do not produce any further information about the type of threat\nthat has been detected neither allows for identifying relationships between\nmalware samples. In this work, we address the information gap between machine\nlearning and signature-based detection methods by learning a representation\nspace for malware samples in which files with similar malicious behaviors\nappear close to each other. We do so by introducing a deep learning based\ntagging model trained to generate human-interpretable semantic descriptions of\nmalicious software, which, at the same time provides potentially more useful\nand flexible information than malware family names.\n  We show that the malware descriptions generated with the proposed approach\ncorrectly identify more than 95% of eleven possible tag descriptions for a\ngiven sample, at a deployable false positive rate of 1% per tag. Furthermore,\nwe use the learned representation space to introduce a similarity index between\nmalware files, and empirically demonstrate using dynamic traces from files\u2019\nexecution, that is not only more effective at identifying samples from the same\nfamilies, but also 32 times smaller than those based on raw feature vectors.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [26.339313507080078, -17.139137268066406], "cluster": 7}, {"key": "duhan2024semantic", "year": "2024", "citations": "0", "title": "Semantic Search And Recommendation Algorithm", "abstract": "<p>This paper introduces a new semantic search algorithm that uses Word2Vec and\nAnnoy Index to improve the efficiency of information retrieval from large\ndatasets. The proposed approach addresses the limitations of traditional search\nmethods by offering enhanced speed, accuracy, and scalability. Testing on\ndatasets up to 100GB demonstrates the method\u2019s effectiveness in processing vast\namounts of data while maintaining high precision and performance.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [4.5552449226379395, -35.68640899658203], "cluster": 3}, {"key": "dutta2017stochastic", "year": "2018", "citations": "19", "title": "Stochastic Graphlet Embedding", "abstract": "<p>Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of \u2013\nexplicit/implicit \u2013 graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Hashing-Methods"], "tsne_embedding": [56.026004791259766, -0.8365158438682556], "cluster": 9}, {"key": "dutta2018graph", "year": "2018", "citations": "0", "title": "Graph Kernels Based On High Order Graphlet Parsing And Hashing", "abstract": "<p>Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of \u2013\nexplicit/implicit \u2013 graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Hashing-Methods"], "tsne_embedding": [56.026004791259766, -0.8365158438682556], "cluster": 9}, {"key": "dutta2018when", "year": "2021", "citations": "5", "title": "When Hashing Met Matching: Efficient Spatio-temporal Search For Ridesharing", "abstract": "<p>Carpooling, or sharing a ride with other passengers, holds immense potential\nfor urban transportation. Ridesharing platforms enable such sharing of rides\nusing real-time data. Finding ride matches in real-time at urban scale is a\ndifficult combinatorial optimization task and mostly heuristic approaches are\napplied. In this work, we mathematically model the problem as that of finding\nnear-neighbors and devise a novel efficient spatio-temporal search algorithm\nbased on the theory of locality sensitive hashing for Maximum Inner Product\nSearch (MIPS). The proposed algorithm can find \\(k\\) near-optimal potential\nmatches for every ride from a pool of \\(n\\) rides in time \\(O(n^{1 + \\rho} (k +\nlog n) log k)\\) and space \\(O(n^{1 + \\rho} log k)\\) for a small \\(\\rho &lt; 1\\). Our\nalgorithm can be extended in several useful and interesting ways increasing its\npractical appeal. Experiments with large NY yellow taxi trip datasets show that\nour algorithm consistently outperforms state-of-the-art heuristic methods\nthereby proving its practical applicability.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "AAAI", "Datasets"], "tsne_embedding": [30.106948852539062, 36.03418731689453], "cluster": 4}, {"key": "dutta2019probabilistic", "year": "2019", "citations": "1", "title": "A Probabilistic Approach For Learning Embeddings Without Supervision", "abstract": "<p>For challenging machine learning problems such as zero-shot learning and\nfine-grained categorization, embedding learning is the machinery of choice\nbecause of its ability to learn generic notions of similarity, as opposed to\nclass-specific concepts in standard classification models. Embedding learning\naims at learning discriminative representations of data such that similar\nexamples are pulled closer, while pushing away dissimilar ones. Despite their\nexemplary performances, supervised embedding learning approaches require huge\nnumber of annotations for training. This restricts their applicability for\nlarge datasets in new applications where obtaining labels require extensive\nmanual efforts and domain knowledge. In this paper, we propose to learn an\nembedding in a completely unsupervised manner without using any class labels.\nUsing a graph-based clustering approach to obtain pseudo-labels, we form\ntriplet-based constraints following a metric learning paradigm. Our novel\nembedding learning approach uses a probabilistic notion, that intuitively\nminimizes the chances of each triplet violating a geometric constraint. Due to\nnature of the search space, we learn the parameters of our approach using\nRiemannian geometry. Our proposed approach performs competitive to\nstate-of-the-art approaches.</p>\n", "tags": ["Graph-Based-Ann", "Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [46.43633270263672, 2.369903087615967], "cluster": 9}, {"key": "dutta2022fuse", "year": "2023", "citations": "0", "title": "Fuse And Attend: Generalized Embedding Learning For Art And Sketches", "abstract": "<p>While deep Embedding Learning approaches have witnessed widespread success in\nmultiple computer vision tasks, the state-of-the-art methods for representing\nnatural images need not necessarily perform well on images from other domains,\nsuch as paintings, cartoons, and sketch. This is because of the huge shift in\nthe distribution of data from across these domains, as compared to natural\nimages. Domains like sketch often contain sparse informative pixels. However,\nrecognizing objects in such domains is crucial, given multiple relevant\napplications leveraging such data, for instance, sketch to image retrieval.\nThus, achieving an Embedding Learning model that could perform well across\nmultiple domains is not only challenging, but plays a pivotal role in computer\nvision. To this end, in this paper, we propose a novel Embedding Learning\napproach with the goal of generalizing across different domains. During\ntraining, given a query image from a domain, we employ gated fusion and\nattention to generate a positive example, which carries a broad notion of the\nsemantics of the query object category (from across multiple domains). By\nvirtue of Contrastive Learning, we pull the embeddings of the query and\npositive, in order to learn a representation which is robust across domains. At\nthe same time, to teach the model to be discriminative against examples from\ndifferent semantic categories (across domains), we also maintain a pool of\nnegative embeddings (from different categories). We show the prowess of our\nmethod using the DomainBed framework, on the popular PACS (Photo, Art painting,\nCartoon, and Sketch) dataset.</p>\n", "tags": ["Self-Supervised", "Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-24.464929580688477, -15.79485034942627], "cluster": 5}, {"key": "dutto2024collaborative", "year": "2024", "citations": "1", "title": "Collaborative Visual Place Recognition Through Federated Learning", "abstract": "<p>Visual Place Recognition (VPR) aims to estimate the location of an image by\ntreating it as a retrieval problem. VPR uses a database of geo-tagged images\nand leverages deep neural networks to extract a global representation, called\ndescriptor, from each image. While the training data for VPR models often\noriginates from diverse, geographically scattered sources (geo-tagged images),\nthe training process itself is typically assumed to be centralized. This\nresearch revisits the task of VPR through the lens of Federated Learning (FL),\naddressing several key challenges associated with this adaptation. VPR data\ninherently lacks well-defined classes, and models are typically trained using\ncontrastive learning, which necessitates a data mining step on a centralized\ndatabase. Additionally, client devices in federated systems can be highly\nheterogeneous in terms of their processing capabilities. The proposed FedVPR\nframework not only presents a novel approach for VPR but also introduces a new,\nchallenging, and realistic task for FL research, paving the way to other image\nretrieval tasks in FL.</p>\n", "tags": ["Self-Supervised", "CVPR", "Tools-&-Libraries"], "tsne_embedding": [-30.247047424316406, -1.9719222784042358], "cluster": 0}, {"key": "dwibedi2021little", "year": "2021", "citations": "297", "title": "With A Little Help From My Friends: Nearest-neighbor Contrastive Learning Of Visual Representations", "abstract": "<p>Self-supervised learning algorithms based on instance discrimination train\nencoders to be invariant to pre-defined transformations of the same instance.\nWhile most methods treat different views of the same image as positives for a\ncontrastive loss, we are interested in using positives from other instances in\nthe dataset. Our method, Nearest-Neighbor Contrastive Learning of visual\nRepresentations (NNCLR), samples the nearest neighbors from the dataset in the\nlatent space, and treats them as positives. This provides more semantic\nvariations than pre-defined transformations.\n  We find that using the nearest-neighbor as positive in contrastive losses\nimproves performance significantly on ImageNet classification, from 71.7% to\n75.6%, outperforming previous state-of-the-art methods. On semi-supervised\nlearning benchmarks we improve performance significantly when only 1% ImageNet\nlabels are available, from 53.8% to 56.5%. On transfer learning benchmarks our\nmethod outperforms state-of-the-art methods (including supervised learning with\nImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate\nempirically that our method is less reliant on complex data augmentations. We\nsee a relative reduction of only 2.1% ImageNet Top-1 accuracy when we train\nusing only random crops.</p>\n", "tags": ["Self-Supervised", "ICCV", "Distance-Metric-Learning", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-42.016571044921875, 11.267982482910156], "cluster": 0}, {"key": "efremenko2019fast", "year": "2020", "citations": "1", "title": "Fast And Bayes-consistent Nearest Neighbors", "abstract": "<p>Research on nearest-neighbor methods tends to focus somewhat dichotomously\neither on the statistical or the computational aspects \u2013 either on, say, Bayes\nconsistency and rates of convergence or on techniques for speeding up the\nproximity search. This paper aims at bridging these realms: to reap the\nadvantages of fast evaluation time while maintaining Bayes consistency, and\nfurther without sacrificing too much in the risk decay rate. We combine the\nlocality-sensitive hashing (LSH) technique with a novel missing-mass argument\nto obtain a fast and Bayes-consistent classifier. Our algorithm\u2019s prediction\nruntime compares favorably against state of the art approximate NN methods,\nwhile maintaining Bayes-consistency and attaining rates comparable to minimax.\nOn samples of size \\(n\\) in \\(\\R^d\\), our pre-processing phase has runtime \\(O(d n\nlog n)\\), while the evaluation phase has runtime \\(O(dlog n)\\) per query point.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [24.73675537109375, 39.05699920654297], "cluster": 4}, {"key": "eghbali2016fast", "year": "2018", "citations": "15", "title": "Fast Cosine Similarity Search In Binary Space With Angular Multi-index Hashing", "abstract": "<p>Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find \\(K\\) codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular \\(K\\) nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Vector-Indexing", "Similarity-Search", "Datasets", "Compact-Codes"], "tsne_embedding": [-1.0845537185668945, 30.201942443847656], "cluster": 8}, {"key": "eghbali2019deep", "year": "2019", "citations": "27", "title": "Deep Spherical Quantization For Image Search", "abstract": "<p>Hashing methods, which encode high-dimensional images with compact discrete\ncodes, have been widely applied to enhance large-scale image retrieval. In this\npaper, we put forward Deep Spherical Quantization (DSQ), a novel method to make\ndeep convolutional neural networks generate supervised and compact binary codes\nfor efficient image search. Our approach simultaneously learns a mapping that\ntransforms the input images into a low-dimensional discriminative space, and\nquantizes the transformed data points using multi-codebook quantization. To\neliminate the negative effect of norm variance on codebook learning, we force\nthe network to L_2 normalize the extracted features and then quantize the\nresulting vectors using a new supervised quantization technique specifically\ndesigned for points lying on a unit hypersphere. Furthermore, we introduce an\neasy-to-implement extension of our quantization technique that enforces\nsparsity on the codebooks. Extensive experiments demonstrate that DSQ and its\nsparse variant can generate semantically separable compact binary codes\noutperforming many state-of-the-art image retrieval methods on three\nbenchmarks.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Quantization", "CVPR", "Scalability", "Image-Retrieval", "Supervised"], "tsne_embedding": [-18.814762115478516, 20.673370361328125], "cluster": 8}, {"key": "elezi2019group", "year": "2020", "citations": "48", "title": "The Group Loss For Deep Metric Learning", "abstract": "<p>Deep metric learning has yielded impressive results in tasks such as\nclustering and image retrieval by leveraging neural networks to obtain highly\ndiscriminative feature embeddings, which can be used to group samples into\ndifferent classes. Much research has been devoted to the design of smart loss\nfunctions or data mining strategies for training such networks. Most methods\nconsider only pairs or triplets of samples within a mini-batch to compute the\nloss function, which is commonly based on the distance between embeddings. We\npropose Group Loss, a loss function based on a differentiable label-propagation\nmethod that enforces embedding similarity across all samples of a group while\npromoting, at the same time, low-density regions amongst data points belonging\nto different groups. Guided by the smoothness assumption that \u201csimilar objects\nshould belong to the same group\u201d, the proposed loss trains the neural network\nfor a classification task, enforcing a consistent labelling amongst samples\nwithin a class. We show state-of-the-art results on clustering and image\nretrieval on several datasets, and show the potential of our method when\ncombined with other techniques such as ensembles</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-10.07435417175293, -19.475439071655273], "cluster": 1}, {"key": "elezi2022group", "year": "2022", "citations": "13", "title": "The Group Loss++: A Deeper Look Into Group Loss For Deep Metric Learning", "abstract": "<p>Deep metric learning has yielded impressive results in tasks such as\nclustering and image retrieval by leveraging neural networks to obtain highly\ndiscriminative feature embeddings, which can be used to group samples into\ndifferent classes. Much research has been devoted to the design of smart loss\nfunctions or data mining strategies for training such networks. Most methods\nconsider only pairs or triplets of samples within a mini-batch to compute the\nloss function, which is commonly based on the distance between embeddings. We\npropose Group Loss, a loss function based on a differentiable label-propagation\nmethod that enforces embedding similarity across all samples of a group while\npromoting, at the same time, low-density regions amongst data points belonging\nto different groups. Guided by the smoothness assumption that \u201csimilar objects\nshould belong to the same group\u201d, the proposed loss trains the neural network\nfor a classification task, enforcing a consistent labelling amongst samples\nwithin a class. We design a set of inference strategies tailored towards our\nalgorithm, named Group Loss++ that further improve the results of our model. We\nshow state-of-the-art results on clustering and image retrieval on four\nretrieval datasets, and present competitive results on two person\nre-identification datasets, providing a unified framework for retrieval and\nre-identification.</p>\n", "tags": ["Tools-&-Libraries", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-10.064002990722656, -19.55640411376953], "cluster": 1}, {"key": "elkin2021new", "year": "2021", "citations": "4", "title": "A New Near-linear Time Algorithm For K-nearest Neighbor Search Using A Compressed Cover Tree", "abstract": "<p>Given a reference set \\(R\\) of \\(n\\) points and a query set \\(Q\\) of \\(m\\) points in\na metric space, this paper studies an important problem of finding \\(k\\)-nearest\nneighbors of every point \\(q \\in Q\\) in the set \\(R\\) in a near-linear time. In the\npaper at ICML 2006, Beygelzimer, Kakade, and Langford introduced a cover tree\non \\(R\\) and attempted to prove that this tree can be built in \\(O(nlog n)\\) time\nwhile the nearest neighbor search can be done in \\(O(nlog m)\\) time with a\nhidden dimensionality factor. This paper fills a substantial gap in the past\nproofs of time complexity by defining a simpler compressed cover tree on the\nreference set \\(R\\). The first new algorithm constructs a compressed cover tree\nin \\(O(n log n)\\) time. The second new algorithm finds all \\(k\\)-nearest neighbors\nof all points from \\(Q\\) using a compressed cover tree in time \\(O(m(k+log n)log\nk)\\) with a hidden dimensionality factor depending on point distributions of the\ngiven sets \\(R,Q\\) but not on their sizes.</p>\n", "tags": ["Tree-Based-Ann"], "tsne_embedding": [31.553180694580078, 40.88092803955078], "cluster": 4}, {"key": "elkin2022paired", "year": "2022", "citations": "0", "title": "Paired Compressed Cover Trees Guarantee A Near Linear Parametrized Complexity For All \\(k\\)-nearest Neighbors Search In An Arbitrary Metric Space", "abstract": "<p>This paper studies the important problem of finding all \\(k\\)-nearest neighbors\nto points of a query set \\(Q\\) in another reference set \\(R\\) within any metric\nspace. Our previous work defined compressed cover trees and corrected the key\narguments in several past papers for challenging datasets. In 2009 Ram, Lee,\nMarch, and Gray attempted to improve the time complexity by using pairs of\ncover trees on the query and reference sets. In 2015 Curtin with the above\nco-authors used extra parameters to finally prove a time complexity for \\(k=1\\).\nThe current work fills all previous gaps and improves the nearest neighbor\nsearch based on pairs of new compressed cover trees. The novel imbalance\nparameter of paired trees allowed us to prove a better time complexity for any\nnumber of neighbors \\(k\\geq 1\\).</p>\n", "tags": ["Tree-Based-Ann", "Datasets"], "tsne_embedding": [31.36532974243164, 40.838436126708984], "cluster": 4}, {"key": "elkishky2022knn", "year": "2022", "citations": "1", "title": "Knn-embed: Locally Smoothed Embedding Mixtures For Multi-interest Candidate Retrieval", "abstract": "<p>Candidate retrieval is the first stage in recommendation systems, where a\nlight-weight system is used to retrieve potentially relevant items for an input\nuser. These candidate items are then ranked and pruned in later stages of\nrecommender systems using a more complex ranking model. As the top of the\nrecommendation funnel, it is important to retrieve a high-recall candidate set\nto feed into downstream ranking models. A common approach is to leverage\napproximate nearest neighbor (ANN) search from a single dense query embedding;\nhowever, this approach this can yield a low-diversity result set with many near\nduplicates. As users often have multiple interests, candidate retrieval should\nideally return a diverse set of candidates reflective of the user\u2019s multiple\ninterests. To this end, we introduce kNN-Embed, a general approach to improving\ndiversity in dense ANN-based retrieval. kNN-Embed represents each user as a\nsmoothed mixture over learned item clusters that represent distinct \u201cinterests\u201d\nof the user. By querying each of a user\u2019s mixture component in proportion to\ntheir mixture weights, we retrieve a high-diversity set of candidates\nreflecting elements from each of a user\u2019s interests. We experimentally compare\nkNN-Embed to standard ANN candidate retrieval, and show significant\nimprovements in overall recall and improved diversity across three datasets.\nAccompanying this work, we open source a large Twitter follow-graph dataset\n(https://huggingface.co/datasets/Twitter/TwitterFollowGraph), to spur further\nresearch in graph-mining and representation learning for recommender systems.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [21.017549514770508, -26.689950942993164], "cluster": 7}, {"key": "elnouby2021training", "year": "2021", "citations": "120", "title": "Training Vision Transformers For Image Retrieval", "abstract": "<p>Transformers have shown outstanding results for natural language\nunderstanding and, more recently, for image classification. We here extend this\nwork and propose a transformer-based approach for image retrieval: we adopt\nvision transformers for generating image descriptors and train the resulting\nmodel with a metric learning objective, which combines a contrastive loss with\na differential entropy regularizer. Our results show consistent and significant\nimprovements of transformers over convolution-based approaches. In particular,\nour method outperforms the state of the art on several public benchmarks for\ncategory-level retrieval, namely Stanford Online Product, In-Shop and CUB-200.\nFurthermore, our experiments on ROxford and RParis also show that, in\ncomparable settings, transformers are competitive for particular object\nretrieval, especially in the regime of short vector representations and\nlow-resolution images.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-37.40582275390625, 18.428958892822266], "cluster": 0}, {"key": "emanuilov2025billion", "year": "2024", "citations": "0", "title": "Billion-scale Similarity Search Using A Hybrid Indexing Approach With Advanced Filtering", "abstract": "<p>This paper presents a novel approach for similarity search with complex\nfiltering capabilities on billion-scale datasets, optimized for CPU inference.\nOur method extends the classical IVF-Flat index structure to integrate\nmulti-dimensional filters. The proposed algorithm combines dense embeddings\nwith discrete filtering attributes, enabling fast retrieval in high-dimensional\nspaces. Designed specifically for CPU-based systems, our disk-based approach\noffers a cost-effective solution for large-scale similarity search. We\ndemonstrate the effectiveness of our method through a case study, showcasing\nits potential for various practical uses.</p>\n", "tags": ["Efficiency", "Vector-Indexing", "Scalability", "Similarity-Search", "Large-Scale-Search", "Datasets", "Hybrid-Ann-Methods"], "tsne_embedding": [29.667125701904297, 12.1795072555542], "cluster": 2}, {"key": "engels2021practical", "year": "2021", "citations": "3", "title": "Practical Near Neighbor Search Via Group Testing", "abstract": "<p>We present a new algorithm for the approximate near neighbor problem that\ncombines classical ideas from group testing with locality-sensitive hashing\n(LSH). We reduce the near neighbor search problem to a group testing problem by\ndesignating neighbors as \u201cpositives,\u201d non-neighbors as \u201cnegatives,\u201d and\napproximate membership queries as group tests. We instantiate this framework\nusing distance-sensitive Bloom Filters to Identify Near-Neighbor Groups\n(FLINNG). We prove that FLINNG has sub-linear query time and show that our\nalgorithm comes with a variety of practical advantages. For example, FLINNG can\nbe constructed in a single pass through the data, consists entirely of\nefficient integer operations, and does not require any distance computations.\nWe conduct large-scale experiments on high-dimensional search tasks such as\ngenome search, URL similarity search, and embedding search over the massive\nYFCC100M dataset. In our comparison with leading algorithms such as HNSW and\nFAISS, we find that FLINNG can provide up to a 10x query speedup with\nsubstantially smaller indexing time and memory.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [31.875699996948242, 32.266998291015625], "cluster": 2}, {"key": "engels2022dessert", "year": "2022", "citations": "1", "title": "DESSERT: An Efficient Algorithm For Vector Set Search With Vector Set Queries", "abstract": "<p>We study the problem of \\(\\textit{vector set search}\\) with \\(\\textit{vector set\nqueries}\\). This task is analogous to traditional near-neighbor search, with the\nexception that both the query and each element in the collection are\n\\(\\textit{sets}\\) of vectors. We identify this problem as a core subroutine for\nsemantic search applications and find that existing solutions are unacceptably\nslow. Towards this end, we present a new approximate search algorithm, DESSERT\n(\\({\\bf D}\\)ESSERT \\({\\bf E}\\)ffeciently \\({\\bf S}\\)earches \\({\\bf S}\\)ets of \\({\\bf\nE}\\)mbeddings via \\({\\bf R}\\)etrieval \\({\\bf T}\\)ables). DESSERT is a general tool\nwith strong theoretical guarantees and excellent empirical performance. When we\nintegrate DESSERT into ColBERT, a state-of-the-art semantic search model, we\nfind a 2-5x speedup on the MS MARCO and LoTTE retrieval benchmarks with minimal\nloss in recall, underscoring the effectiveness and practical applicability of\nour proposal.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [15.878223419189453, -26.512531280517578], "cluster": 7}, {"key": "engels2024approximate", "year": "2024", "citations": "0", "title": "Approximate Nearest Neighbor Search With Window Filters", "abstract": "<p>We define and investigate the problem of \\(\\textit{c-approximate window\nsearch}\\): approximate nearest neighbor search where each point in the dataset\nhas a numeric label, and the goal is to find nearest neighbors to queries\nwithin arbitrary label ranges. Many semantic search problems, such as image and\ndocument search with timestamp filters, or product search with cost filters,\nare natural examples of this problem. We propose and theoretically analyze a\nmodular tree-based framework for transforming an index that solves the\ntraditional c-approximate nearest neighbor problem into a data structure that\nsolves window search. On standard nearest neighbor benchmark datasets equipped\nwith random label values, adversarially constructed embeddings, and image\nsearch embeddings with real timestamps, we obtain up to a \\(75\\times\\) speedup\nover existing solutions at the same level of recall.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Tree-Based-Ann", "Datasets", "Evaluation"], "tsne_embedding": [26.241464614868164, 38.79050064086914], "cluster": 4}, {"key": "engelsma2019fingerprints", "year": "2019", "citations": "10", "title": "Fingerprints: Fixed Length Representation Via Deep Networks And Domain Knowledge", "abstract": "<p>We learn a discriminative fixed length feature representation of fingerprints\nwhich stands in contrast to commonly used unordered, variable length sets of\nminutiae points. To arrive at this fixed length representation, we embed\nfingerprint domain knowledge into a multitask deep convolutional neural network\narchitecture. Empirical results, on two public-domain fingerprint databases\n(NIST SD4 and FVC 2004 DB1) show that compared to minutiae representations,\nextracted by two state-of-the-art commercial matchers (Verifinger v6.3 and\nInnovatrics v2.0.3), our fixed-length representations provide (i) higher search\naccuracy: Rank-1 accuracy of 97.9% vs. 97.3% on NIST SD4 against a gallery size\nof 2000 and (ii) significantly faster, large scale search: 682,594 matches per\nsecond vs. 22 matches per second for commercial matchers on an i5 3.3 GHz\nprocessor with 8 GB of RAM.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [3.7601544857025146, 30.358240127563477], "cluster": 4}, {"key": "engelsma2020hers", "year": "2022", "citations": "34", "title": "HERS: Homomorphically Encrypted Representation Search", "abstract": "<p>We present a method to search for a probe (or query) image representation\nagainst a large gallery in the encrypted domain. We require that the probe and\ngallery images be represented in terms of a fixed-length representation, which\nis typical for representations obtained from learned networks. Our encryption\nscheme is agnostic to how the fixed-length representation is obtained and can\ntherefore be applied to any fixed-length representation in any application\ndomain. Our method, dubbed HERS (Homomorphically Encrypted Representation\nSearch), operates by (i) compressing the representation towards its estimated\nintrinsic dimensionality with minimal loss of accuracy (ii) encrypting the\ncompressed representation using the proposed fully homomorphic encryption\nscheme, and (iii) efficiently searching against a gallery of encrypted\nrepresentations directly in the encrypted domain, without decrypting them.\nNumerical results on large galleries of face, fingerprint, and object datasets\nsuch as ImageNet show that, for the first time, accurate and fast image search\nwithin the encrypted domain is feasible at scale (500 seconds; \\(275\\times\\)\nspeed up over state-of-the-art for encrypted search against a gallery of 100\nmillion). Code is available at\nhttps://github.com/human-analysis/hers-encrypted-image-search</p>\n", "tags": ["Compact-Codes", "Privacy-&-Security", "Image-Retrieval", "Datasets"], "tsne_embedding": [-21.16456413269043, 37.471195220947266], "cluster": 8}, {"key": "engelsma2022hers", "year": "2022", "citations": "34", "title": "HERS: Homomorphically Encrypted Representation Search", "abstract": "<p>We present a method to search for a probe (or query) image representation\nagainst a large gallery in the encrypted domain. We require that the probe and\ngallery images be represented in terms of a fixed-length representation, which\nis typical for representations obtained from learned networks. Our encryption\nscheme is agnostic to how the fixed-length representation is obtained and can\ntherefore be applied to any fixed-length representation in any application\ndomain. Our method, dubbed HERS (Homomorphically Encrypted Representation\nSearch), operates by (i) compressing the representation towards its estimated\nintrinsic dimensionality with minimal loss of accuracy (ii) encrypting the\ncompressed representation using the proposed fully homomorphic encryption\nscheme, and (iii) efficiently searching against a gallery of encrypted\nrepresentations directly in the encrypted domain, without decrypting them.\nNumerical results on large galleries of face, fingerprint, and object datasets\nsuch as ImageNet show that, for the first time, accurate and fast image search\nwithin the encrypted domain is feasible at scale (500 seconds; \\(275\\times\\)\nspeed up over state-of-the-art for encrypted search against a gallery of 100\nmillion). Code is available at\nhttps://github.com/human-analysis/hers-encrypted-image-search</p>\n", "tags": ["Compact-Codes", "Privacy-&-Security", "Image-Retrieval", "Datasets"], "tsne_embedding": [-21.16456413269043, 37.471195220947266], "cluster": 8}, {"key": "engilberge2018finding", "year": "2018", "citations": "86", "title": "Finding Beans In Burgers: Deep Semantic-visual Embedding With Localization", "abstract": "<p>Several works have proposed to learn a two-path neural network that maps\nimages and texts, respectively, to a same shared Euclidean space where geometry\ncaptures useful semantic relationships. Such a multi-modal embedding can be\ntrained and used for various tasks, notably image captioning. In the present\nwork, we introduce a new architecture of this type, with a visual path that\nleverages recent space-aware pooling mechanisms. Combined with a textual path\nwhich is jointly trained from scratch, our semantic-visual embedding offers a\nversatile model. Once trained under the supervision of captioned images, it\nyields new state-of-the-art performance on cross-modal retrieval. It also\nallows the localization of new concepts from the embedding space into any input\nimage, delivering state-of-the-art result on the visual grounding of phrases.</p>\n", "tags": ["Multimodal-Retrieval", "CVPR", "Evaluation"], "tsne_embedding": [-17.511985778808594, -26.947113037109375], "cluster": 5}, {"key": "ercoli2016compact", "year": "2017", "citations": "42", "title": "Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases", "abstract": "<p>In this paper we present an efficient method for visual descriptors retrieval\nbased on compact hash codes computed using a multiple k-means assignment. The\nmethod has been applied to the problem of approximate nearest neighbor (ANN)\nsearch of local and global visual content descriptors, and it has been tested\non different datasets: three large scale public datasets of up to one billion\ndescriptors (BIGANN) and, supported by recent progress in convolutional neural\nnetworks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental results\nshow that, despite its simplicity, the proposed method obtains a very high\nperformance that makes it superior to more complex state-of-the-art methods.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [-4.630819797515869, 31.5946044921875], "cluster": 8}, {"key": "ertl2017superminhash", "year": "2017", "citations": "13", "title": "Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation", "abstract": "<p>This paper presents a new algorithm for calculating hash signatures of sets\nwhich can be directly used for Jaccard similarity estimation. The new approach\nis an improvement over the MinHash algorithm, because it has a better runtime\nbehavior and the resulting signatures allow a more precise estimation of the\nJaccard index.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [5.011973857879639, 36.379417419433594], "cluster": 4}, {"key": "ertl2018bagminhash", "year": "2018", "citations": "15", "title": "Bagminhash - Minwise Hashing Algorithm For Weighted Sets", "abstract": "<p>Minwise hashing has become a standard tool to calculate signatures which\nallow direct estimation of Jaccard similarities. While very efficient\nalgorithms already exist for the unweighted case, the calculation of signatures\nfor weighted sets is still a time consuming task. BagMinHash is a new algorithm\nthat can be orders of magnitude faster than current state of the art without\nany particular restrictions or assumptions on weights or data dimensionality.\nApplied to the special case of unweighted sets, it represents the first\nefficient algorithm producing independent signature components. A series of\ntests finally verifies the new algorithm and also reveals limitations of other\napproaches published in the recent past.</p>\n", "tags": ["Hashing-Methods", "KDD"], "tsne_embedding": [27.28558349609375, -1.8004361391067505], "cluster": 6}, {"key": "ertl2019probminhash", "year": "2020", "citations": "23", "title": "Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity", "abstract": "<p>The probability Jaccard similarity was recently proposed as a natural\ngeneralization of the Jaccard similarity to measure the proximity of sets whose\nelements are associated with relative frequencies or probabilities. In\ncombination with a hash algorithm that maps those weighted sets to compact\nsignatures which allow fast estimation of pairwise similarities, it constitutes\na valuable method for big data applications such as near-duplicate detection,\nnearest neighbor search, or clustering. This paper introduces a class of\none-pass locality-sensitive hash algorithms that are orders of magnitude faster\nthan the original approach. The performance gain is achieved by calculating\nsignature components not independently, but collectively. Four different\nalgorithms are proposed based on this idea. Two of them are statistically\nequivalent to the original approach and can be used as drop-in replacements.\nThe other two may even improve the estimation error by introducing statistical\ndependence between signature components. Moreover, the presented techniques can\nbe specialized for the conventional Jaccard similarity, resulting in highly\nefficient algorithms that outperform traditional minwise hashing and that are\nable to compete with the state of the art.</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [5.643671035766602, 36.85270309448242], "cluster": 4}, {"key": "esen2016large", "year": "2016", "citations": "1", "title": "Large-scale Video Search With Efficient Temporal Voting Structure", "abstract": "<p>In this work, we propose a fast content-based video querying system for\nlarge-scale video search. The proposed system is distinguished from similar\nworks with two major contributions. First contribution is superiority of joint\nusage of repeated content representation and efficient hashing mechanisms.\nRepeated content representation is utilized with a simple yet robust feature,\nwhich is based on edge energy of frames. Each of the representation is\nconverted into hash code with Hamming Embedding method for further queries.\nSecond contribution is novel queue-based voting scheme that leads to modest\nmemory requirements with gradual memory allocation capability, contrary to\ncomplete brute-force temporal voting schemes. This aspect enables us to make\nqueries on large video databases conveniently, even on commodity computers with\nlimited memory capacity. Our results show that the system can respond to video\nqueries on a large video database with fast query times, high recall rate and\nvery low memory and disk requirements.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Evaluation", "Video-Retrieval"], "tsne_embedding": [34.010658264160156, 17.70291519165039], "cluster": 2}, {"key": "esser2021faster", "year": "2021", "citations": "1", "title": "A Faster Algorithm For Finding Closest Pairs In Hamming Metric", "abstract": "<p>We study the Closest Pair Problem in Hamming metric, which asks to find the\npair with the smallest Hamming distance in a collection of binary vectors. We\ngive a new randomized algorithm for the problem on uniformly random input\noutperforming previous approaches whenever the dimension of input points is\nsmall compared to the dataset size. For moderate to large dimensions, our\nalgorithm matches the time complexity of the previously best-known locality\nsensitive hashing based algorithms. Technically our algorithm follows similar\ndesign principles as Dubiner (IEEE Trans. Inf. Theory 2010) and May-Ozerov\n(Eurocrypt 2015). Besides improving the time complexity in the aforementioned\nareas, we significantly simplify the analysis of these previous works. We give\na modular analysis, which allows us to investigate the performance of the\nalgorithm also on non-uniform input distributions. Furthermore, we give a proof\nof concept implementation of our algorithm which performs well in comparison to\na quadratic search baseline. This is the first step towards answering an open\nquestion raised by May and Ozerov regarding the practicability of algorithms\nfollowing these design principles.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [15.758675575256348, 33.71091079711914], "cluster": 4}, {"key": "faghri2017vse", "year": "2017", "citations": "562", "title": "VSE++: Improving Visual-semantic Embeddings With Hard Negatives", "abstract": "<p>We present a new technique for learning visual-semantic embeddings for\ncross-modal retrieval. Inspired by hard negative mining, the use of hard\nnegatives in structured prediction, and ranking loss functions, we introduce a\nsimple change to common loss functions used for multi-modal embeddings. That,\ncombined with fine-tuning and use of augmented data, yields significant gains\nin retrieval performance. We showcase our approach, VSE++, on MS-COCO and\nFlickr30K datasets, using ablation studies and comparisons with existing\nmethods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8%\nin caption retrieval and 11.3% in image retrieval (at R@1).</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-25.38486099243164, -25.075881958007812], "cluster": 5}, {"key": "fahim2022unsupervised", "year": "2022", "citations": "0", "title": "Unsupervised Space Partitioning For Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) in high dimensional spaces is\ncrucial for many real-life applications (e.g., e-commerce, web, multimedia,\netc.) dealing with an abundance of data. This paper proposes an end-to-end\nlearning framework that couples the partitioning (one critical step of ANNS)\nand learning-to-search steps using a custom loss function. A key advantage of\nour proposed solution is that it does not require any expensive pre-processing\nof the dataset, which is one of the critical limitations of the\nstate-of-the-art approach. We achieve the above edge by formulating a\nmulti-objective custom loss function that does not need ground truth labels to\nquantify the quality of a given data-space partition, making it entirely\nunsupervised. We also propose an ensembling technique by adding varying input\nweights to the loss function to train an ensemble of models to enhance the\nsearch quality. On several standard benchmarks for ANNS, we show that our\nmethod beats the state-of-the-art space partitioning method and the ubiquitous\nK-means clustering method while using fewer parameters and shorter offline\ntraining times. We also show that incorporating our space-partitioning strategy\ninto state-of-the-art ANNS techniques such as ScaNN can improve their\nperformance significantly. Finally, we present our unsupervised partitioning\napproach as a promising alternative to many widely used clustering methods,\nsuch as K-means clustering and DBSCAN.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Unsupervised", "Datasets"], "tsne_embedding": [27.03822898864746, 27.018871307373047], "cluster": 2}, {"key": "fain2019dividing", "year": "2019", "citations": "16", "title": "Dividing And Conquering Cross-modal Recipe Retrieval: From Nearest Neighbours Baselines To Sota", "abstract": "<p>We propose a novel non-parametric method for cross-modal recipe retrieval\nwhich is applied on top of precomputed image and text embeddings. By combining\nour method with standard approaches for building image and text encoders,\ntrained independently with a self-supervised classification objective, we\ncreate a baseline model which outperforms most existing methods on a\nchallenging image-to-recipe task. We also use our method for comparing image\nand text encoders trained using different modern approaches, thus addressing\nthe issues hindering the development of novel methods for cross-modal recipe\nretrieval. We demonstrate how to use the insights from model comparison and\nextend our baseline model with standard triplet loss that improves\nstate-of-the-art on the Recipe1M dataset by a large margin, while using only\nprecomputed features and with much less complexity than existing methods.\nFurther, our approach readily generalizes beyond recipe retrieval to other\nchallenging domains, achieving state-of-the-art performance on Politics and\nGoodNews cross-modal retrieval tasks.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-40.216941833496094, 32.69051742553711], "cluster": 0}, {"key": "falcon2022learning", "year": "2022", "citations": "2", "title": "Learning Video Retrieval Models With Relevance-aware Online Mining", "abstract": "<p>Due to the amount of videos and related captions uploaded every hour, deep\nlearning-based solutions for cross-modal video retrieval are attracting more\nand more attention. A typical approach consists in learning a joint text-video\nembedding space, where the similarity of a video and its associated caption is\nmaximized, whereas a lower similarity is enforced with all the other captions,\ncalled negatives. This approach assumes that only the video and caption pairs\nin the dataset are valid, but different captions - positives - may also\ndescribe its visual contents, hence some of them may be wrongly penalized. To\naddress this shortcoming, we propose the Relevance-Aware Negatives and\nPositives mining (RANP) which, based on the semantics of the negatives,\nimproves their selection while also increasing the similarity of other valid\npositives. We explore the influence of these techniques on two video-text\ndatasets: EPIC-Kitchens-100 and MSR-VTT. By using the proposed techniques, we\nachieve considerable improvements in terms of nDCG and mAP, leading to\nstate-of-the-art results, e.g. +5.3% nDCG and +3.0% mAP on EPIC-Kitchens-100.\nWe share code and pretrained models at\nhttps://github.com/aranciokov/ranp.</p>\n", "tags": ["Evaluation", "Video-Retrieval", "Datasets"], "tsne_embedding": [-33.518497467041016, -16.973880767822266], "cluster": 5}, {"key": "fan2013supervised", "year": "2013", "citations": "15", "title": "Supervised Binary Hash Code Learning With Jensen Shannon Divergence", "abstract": "<p>This paper proposes to learn binary hash codes within\na statistical learning framework, in which an upper bound\nof the probability of Bayes decision errors is derived for\ndifferent forms of hash functions and a rigorous proof of\nthe convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent\nperformance improvements of existing hash code learning\nalgorithms, regardless of whether original algorithms are\nunsupervised or supervised. This paper also illustrates a\nfast hash coding method that exploits simple binary tests to\nachieve orders of magnitude improvement in coding speed\nas compared to projection based methods.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [19.728132247924805, -2.3682782649993896], "cluster": 6}, {"key": "fan2020deep", "year": "2020", "citations": "81", "title": "Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes", "abstract": "<p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away\nfrom zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,\n2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as\ndisclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from\nthe target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple \u2014 even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>\n", "tags": ["Datasets", "Compact-Codes", "Tools-&-Libraries", "AAAI", "Hashing-Methods", "Supervised", "IJCAI", "Distance-Metric-Learning"], "tsne_embedding": [19.273122787475586, 8.347014427185059], "cluster": 6}, {"key": "fan2022tree", "year": "2022", "citations": "2", "title": "Tree-based Search Graph For Approximate Nearest Neighbor Search", "abstract": "<p>Nearest neighbor search supports important applications in many domains, such\nas database, machine learning, computer vision. Since the computational cost\nfor accurate search is too high, the community turned to the research of\napproximate nearest neighbor search (ANNS). Among them, graph-based algorithm\nis one of the most important branches. Research by Fu et al. shows that the\nalgorithms based on Monotonic Search Network (MSNET), such as NSG and NSSG,\nhave achieved the state-of-the-art search performance in efficiency. The MSNET\nis dedicated to achieving monotonic search with minimal out-degree of nodes to\npursue high efficiency. However, the current MSNET designs did not optimize the\nprobability of the monotonic search, and the lower bound of the probability is\nonly 50%. If they fail in monotonic search stage, they have to suffer\ntremendous backtracking cost to achieve the required accuracy. This will cause\nperformance problems in search efficiency. To address this problem, we propose\n(r,p)-MSNET, which achieves guaranteed probability on monotonic search. Due to\nthe high building complexity of a strict (r,p)-MSNET, we propose TBSG, which is\nan approximation with low complexity. Experiment conducted on four\nmillion-scaled datasets show that TBSG outperforms existing state-of-the-art\ngraph-based algorithms in search efficiency. Our code has been released on\nGithub.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Tree-Based-Ann", "Datasets", "Evaluation"], "tsne_embedding": [58.254173278808594, 9.17659854888916], "cluster": 9}, {"key": "fan2025deep", "year": "2020", "citations": "81", "title": "Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes", "abstract": "<p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away\nfrom zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,\n2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as\ndisclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from\nthe target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple \u2014 even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>\n", "tags": ["Datasets", "Compact-Codes", "Tools-&-Libraries", "AAAI", "Hashing-Methods", "Supervised", "IJCAI", "Distance-Metric-Learning"], "tsne_embedding": [19.27305030822754, 8.346988677978516], "cluster": 6}, {"key": "fan2025supervised", "year": "2013", "citations": "15", "title": "Supervised Binary Hash Code Learning With Jensen Shannon Divergence", "abstract": "<p>This paper proposes to learn binary hash codes within\na statistical learning framework, in which an upper bound\nof the probability of Bayes decision errors is derived for\ndifferent forms of hash functions and a rigorous proof of\nthe convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent\nperformance improvements of existing hash code learning\nalgorithms, regardless of whether original algorithms are\nunsupervised or supervised. This paper also illustrates a\nfast hash coding method that exploits simple binary tests to\nachieve orders of magnitude improvement in coding speed\nas compared to projection based methods.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [19.728132247924805, -2.3682782649993896], "cluster": 6}, {"key": "fang2020attention", "year": "2020", "citations": "12", "title": "Attention-based Saliency Hashing For Ophthalmic Image Retrieval", "abstract": "<p>Deep hashing methods have been proved to be effective for the large-scale\nmedical image search assisting reference-based diagnosis for clinicians.\nHowever, when the salient region plays a maximal discriminative role in\nophthalmic image, existing deep hashing methods do not fully exploit the\nlearning ability of the deep network to capture the features of salient regions\npointedly. The different grades or classes of ophthalmic images may be share\nsimilar overall performance but have subtle differences that can be\ndifferentiated by mining salient regions. To address this issue, we propose a\nnovel end-to-end network, named Attention-based Saliency Hashing (ASH), for\nlearning compact hash-code to represent ophthalmic images. ASH embeds a\nspatial-attention module to focus more on the representation of salient regions\nand highlights their essential role in differentiating ophthalmic images.\nBenefiting from the spatial-attention module, the information of salient\nregions can be mapped into the hash-code for similarity calculation. In the\ntraining stage, we input the image pairs to share the weights of the network,\nand a pairwise loss is designed to maximize the discriminability of the\nhash-code. In the retrieval stage, ASH obtains the hash-code by inputting an\nimage with an end-to-end manner, then the hash-code is used to similarity\ncalculation to return the most similar images. Extensive experiments on two\ndifferent modalities of ophthalmic image datasets demonstrate that the proposed\nASH can further improve the retrieval performance compared to the\nstate-of-the-art deep hashing methods due to the huge contributions of the\nspatial-attention module.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-49.48387908935547, 18.219507217407227], "cluster": 0}, {"key": "fang2020beyond", "year": "2020", "citations": "1", "title": "Beyond Lexical: A Semantic Retrieval Framework For Textual Searchengine", "abstract": "<p>Search engine has become a fundamental component in various web and mobile\napplications. Retrieving relevant documents from the massive datasets is\nchallenging for a search engine system, especially when faced with verbose or\ntail queries. In this paper, we explore a vector space search framework for\ndocument retrieval. Specifically, we trained a deep semantic matching model so\nthat each query and document can be encoded as a low dimensional embedding. Our\nmodel was trained based on BERT architecture. We deployed a fast\nk-nearest-neighbor index service for online serving. Both offline and online\nmetrics demonstrate that our method improved retrieval performance and search\nquality considerably, particularly for tail</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [8.315975189208984, -27.9140567779541], "cluster": 7}, {"key": "fang2021combating", "year": "2021", "citations": "10", "title": "Combating Ambiguity For Hash-code Learning In Medical Instance Retrieval", "abstract": "<p>When encountering a dubious diagnostic case, medical instance retrieval can\nhelp radiologists make evidence-based diagnoses by finding images containing\ninstances similar to a query case from a large image database. The similarity\nbetween the query case and retrieved similar cases is determined by visual\nfeatures extracted from pathologically abnormal regions. However, the\nmanifestation of these regions often lacks specificity, i.e., different\ndiseases can have the same manifestation, and different manifestations may\noccur at different stages of the same disease. To combat the manifestation\nambiguity in medical instance retrieval, we propose a novel deep framework\ncalled Y-Net, encoding images into compact hash-codes generated from\nconvolutional features by feature aggregation. Y-Net can learn highly\ndiscriminative convolutional features by unifying the pixel-wise segmentation\nloss and classification loss. The segmentation loss allows exploring subtle\nspatial differences for good spatial-discriminability while the classification\nloss utilizes class-aware semantic information for good semantic-separability.\nAs a result, Y-Net can enhance the visual features in pathologically abnormal\nregions and suppress the disturbing of the background during model training,\nwhich could effectively embed discriminative features into the hash-codes in\nthe retrieval stage. Extensive experiments on two medical image datasets\ndemonstrate that Y-Net can alleviate the ambiguity of pathologically abnormal\nregions and its retrieval performance outperforms the state-of-the-art method\nby an average of 9.27% on the returned list of 10.</p>\n", "tags": ["Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-50.17015075683594, 18.440977096557617], "cluster": 0}, {"key": "fang2021deep", "year": "2021", "citations": "52", "title": "Deep Triplet Hashing Network For Case-based Medical Image Retrieval", "abstract": "<p>Deep hashing methods have been shown to be the most efficient approximate\nnearest neighbor search techniques for large-scale image retrieval. However,\nexisting deep hashing methods have a poor small-sample ranking performance for\ncase-based medical image retrieval. The top-ranked images in the returned query\nresults may be as a different class than the query image. This ranking problem\nis caused by classification, regions of interest (ROI), and small-sample\ninformation loss in the hashing space. To address the ranking problem, we\npropose an end-to-end framework, called Attention-based Triplet Hashing (ATH)\nnetwork, to learn low-dimensional hash codes that preserve the classification,\nROI, and small-sample information. We embed a spatial-attention module into the\nnetwork structure of our ATH to focus on ROI information. The spatial-attention\nmodule aggregates the spatial information of feature maps by utilizing\nmax-pooling, element-wise maximum, and element-wise mean operations jointly\nalong the channel axis. The triplet cross-entropy loss can help to map the\nclassification information of images and similarity between images into the\nhash codes. Extensive experiments on two case-based medical datasets\ndemonstrate that our proposed ATH can further improve the retrieval performance\ncompared to the state-of-the-art deep hashing methods and boost the ranking\nperformance for small samples. Compared to the other loss methods, the triplet\ncross-entropy loss can enhance the classification performance and hash\ncode-discriminability</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-49.383819580078125, 18.497486114501953], "cluster": 0}, {"key": "fang2022adversarial", "year": "2022", "citations": "0", "title": "Adversarial Learning Of Hard Positives For Place Recognition", "abstract": "<p>Image retrieval methods for place recognition learn global image descriptors\nthat are used for fetching geo-tagged images at inference time. Recent works\nhave suggested employing weak and self-supervision for mining hard positives\nand hard negatives in order to improve localization accuracy and robustness to\nvisibility changes (e.g. in illumination or view point). However, generating\nhard positives, which is essential for obtaining robustness, is still limited\nto hard-coded or global augmentations. In this work we propose an adversarial\nmethod to guide the creation of hard positives for training image retrieval\nnetworks. Our method learns local and global augmentation policies which will\nincrease the training loss, while the image retrieval network is forced to\nlearn more powerful features for discriminating increasingly difficult\nexamples. This approach allows the image retrieval network to generalize beyond\nthe hard examples presented in the data and learn features that are robust to a\nwide range of variations. Our method achieves state-of-the-art recalls on the\nPitts250 and Tokyo 24/7 benchmarks and outperforms recent image retrieval\nmethods on the rOxford and rParis datasets by a noticeable margin.</p>\n", "tags": ["Robustness", "Image-Retrieval", "Datasets"], "tsne_embedding": [-29.93875503540039, -3.387087106704712], "cluster": 0}, {"key": "faysse2024colpali", "year": "2024", "citations": "0", "title": "Colpali: Efficient Document Retrieval With Vision Language Models", "abstract": "<p>Documents are visually rich structures that convey information through text,\nbut also figures, page layouts, tables, or even fonts. Since modern retrieval\nsystems mainly rely on the textual information they extract from document pages\nto index documents -often through lengthy and brittle processes-, they struggle\nto exploit key visual cues efficiently. This limits their capabilities in many\npractical document retrieval applications such as Retrieval Augmented\nGeneration (RAG). To benchmark current systems on visually rich document\nretrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,\ncomposed of various page-level retrieval tasks spanning multiple domains,\nlanguages, and practical settings. The inherent complexity and performance\nshortcomings of modern systems motivate a new concept; doing document retrieval\nby directly embedding the images of the document pages. We release ColPali, a\nVision Language Model trained to produce high-quality multi-vector embeddings\nfrom images of document pages. Combined with a late interaction matching\nmechanism, ColPali largely outperforms modern document retrieval pipelines\nwhile being drastically simpler, faster and end-to-end trainable. We release\nmodels, data, code and benchmarks under open licenses at https://hf.co/vidore.</p>\n", "tags": ["Evaluation", "Text-Retrieval"], "tsne_embedding": [-23.32455825805664, -2.403316020965576], "cluster": 1}, {"key": "fehervari2020adaptive", "year": "2020", "citations": "0", "title": "Adaptive Additive Classification-based Loss For Deep Metric Learning", "abstract": "<p>Recent works have shown that deep metric learning algorithms can benefit from\nweak supervision from another input modality. This additional modality can be\nincorporated directly into the popular triplet-based loss function as\ndistances. Also recently, classification loss and proxy-based metric learning\nhave been observed to lead to faster convergence as well as better retrieval\nresults, all the while without requiring complex and costly sampling\nstrategies. In this paper we propose an extension to the existing adaptive\nmargin for classification-based deep metric learning. Our extension introduces\na separate margin for each negative proxy per sample. These margins are\ncomputed during training from precomputed distances of the classes in the other\nmodality. Our results set a new state-of-the-art on both on the Amazon fashion\nretrieval dataset as well as on the public DeepFashion dataset. This was\nobserved with both fastText- and BERT-based embeddings for the additional\ntextual modality. Our results were achieved with faster convergence and lower\ncode complexity than the prior state-of-the-art.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-3.4926648139953613, -28.202625274658203], "cluster": 3}, {"key": "feng2016deep", "year": "2017", "citations": "12", "title": "Deep Image Set Hashing", "abstract": "<p>In applications involving matching of image sets, the information from\nmultiple images must be effectively exploited to represent each set.\nState-of-the-art methods use probabilistic distribution or subspace to model a\nset and use specific distance measure to compare two sets. These methods are\nslow to compute and not compact to use in a large scale scenario.\nLearning-based hashing is often used in large scale image retrieval as they\nprovide a compact representation of each sample and the Hamming distance can be\nused to efficiently compare two samples. However, most hashing methods encode\neach image separately and discard knowledge that multiple images in the same\nset represent the same object or person. We investigate the set hashing problem\nby combining both set representation and hashing in a single deep neural\nnetwork. An image set is first passed to a CNN module to extract image\nfeatures, then these features are aggregated using two types of set feature to\ncapture both set specific and database-wide distribution information. The\ncomputed set feature is then fed into a multilayer perceptron to learn a\ncompact binary embedding. Triplet loss is used to train the network by forming\nset similarity relations using class labels. We extensively evaluate our\napproach on datasets used for image matching and show highly competitive\nperformance compared to state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-6.135646820068359, 0.6915687918663025], "cluster": 1}, {"key": "feng2020adversarial", "year": "2020", "citations": "43", "title": "Adversarial Attack On Deep Product Quantization Network For Image Retrieval", "abstract": "<p>Deep product quantization network (DPQN) has recently received much attention\nin fast image retrieval tasks due to its efficiency of encoding\nhigh-dimensional visual features especially when dealing with large-scale\ndatasets. Recent studies show that deep neural networks (DNNs) are vulnerable\nto input with small and maliciously designed perturbations (a.k.a., adversarial\nexamples). This phenomenon raises the concern of security issues for DPQN in\nthe testing/deploying stage as well. However, little effort has been devoted to\ninvestigating how adversarial examples affect DPQN. To this end, we propose\nproduct quantization adversarial generation (PQ-AG), a simple yet effective\nmethod to generate adversarial examples for product quantization based\nretrieval systems. PQ-AG aims to generate imperceptible adversarial\nperturbations for query images to form adversarial queries, whose nearest\nneighbors from a targeted product quantizaiton model are not semantically\nrelated to those from the original queries. Extensive experiments show that our\nPQ-AQ successfully creates adversarial examples to mislead targeted product\nquantization retrieval models. Besides, we found that our PQ-AG significantly\ndegrades retrieval performance in both white-box and black-box settings.</p>\n", "tags": ["Efficiency", "Quantization", "Image-Retrieval", "Scalability", "Robustness", "AAAI", "Datasets", "Evaluation"], "tsne_embedding": [-26.117080688476562, -0.96244215965271], "cluster": 1}, {"key": "feng2020unifying", "year": "2020", "citations": "5", "title": "Unifying Specialist Image Embedding Into Universal Image Embedding", "abstract": "<p>Deep image embedding provides a way to measure the semantic similarity of two\nimages. It plays a central role in many applications such as image search, face\nverification, and zero-shot learning. It is desirable to have a universal deep\nembedding model applicable to various domains of images. However, existing\nmethods mainly rely on training specialist embedding models each of which is\napplicable to images from a single domain. In this paper, we study an important\nbut unexplored task: how to train a single universal image embedding model to\nmatch the performance of several specialists on each specialist\u2019s domain.\nSimply fusing the training data from multiple domains cannot solve this problem\nbecause some domains become overfitted sooner when trained together using\nexisting methods. Therefore, we propose to distill the knowledge in multiple\nspecialists into a universal embedding to solve this problem. In contrast to\nexisting embedding distillation methods that distill the absolute distances\nbetween images, we transform the absolute distances between images into a\nprobabilistic distribution and minimize the KL-divergence between the\ndistributions of the specialists and the universal embedding. Using several\npublic datasets, we validate that our proposed method accomplishes the goal of\nuniversal image embedding.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets"], "tsne_embedding": [-25.92658042907715, -14.761698722839355], "cluster": 5}, {"key": "feng2022evit", "year": "2024", "citations": "14", "title": "Evit: Privacy-preserving Image Retrieval Via Encrypted Vision Transformer In Cloud Computing", "abstract": "<p>Image retrieval systems help users to browse and search among extensive\nimages in real-time. With the rise of cloud computing, retrieval tasks are\nusually outsourced to cloud servers. However, the cloud scenario brings a\ndaunting challenge of privacy protection as cloud servers cannot be fully\ntrusted. To this end, image-encryption-based privacy-preserving image retrieval\nschemes have been developed, which first extract features from cipher-images,\nand then build retrieval models based on these features. Yet, most existing\napproaches extract shallow features and design trivial retrieval models,\nresulting in insufficient expressiveness for the cipher-images. In this paper,\nwe propose a novel paradigm named Encrypted Vision Transformer (EViT), which\nadvances the discriminative representations capability of cipher-images. First,\nin order to capture comprehensive ruled information, we extract multi-level\nlocal length sequence and global Huffman-code frequency features from the\ncipher-images which are encrypted by stream cipher during JPEG compression\nprocess. Second, we design the Vision Transformer-based retrieval model to\ncouple with the multi-level features, and propose two adaptive data\naugmentation methods to improve representation power of the retrieval model.\nOur proposal can be easily adapted to unsupervised and supervised settings via\nself-supervised contrastive learning manner. Extensive experiments reveal that\nEViT achieves both excellent encryption and retrieval performance,\noutperforming current schemes in terms of retrieval accuracy by large margins\nwhile protecting image privacy effectively. Code is publicly available at\nhttps://github.com/onlinehuazai/EViT.</p>\n", "tags": ["Self-Supervised", "Efficiency", "Image-Retrieval", "Privacy-&-Security", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-2.530658006668091, 12.010248184204102], "cluster": 8}, {"key": "feng2023towards", "year": "2025", "citations": "0", "title": "Towards Efficient Deep Hashing Retrieval: Condensing Your Data Via Feature-embedding Matching", "abstract": "<p>Deep hashing retrieval has gained widespread use in big data retrieval due to\nits robust feature extraction and efficient hashing process. However, training\nadvanced deep hashing models has become more expensive due to complex\noptimizations and large datasets. Coreset selection and Dataset Condensation\nlower overall training costs by reducing the volume of training data without\nsignificantly compromising model accuracy for classification task. In this\npaper, we explore the effect of mainstream dataset condensation methods for\ndeep hashing retrieval and propose IEM (Information-intensive feature Embedding\nMatching), which is centered on distribution matching and incorporates model\nand data augmentation techniques to further enhance the feature of hashing\nspace. Extensive experiments demonstrate the superior performance and\nefficiency of our approach.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "ICASSP", "Datasets", "Evaluation"], "tsne_embedding": [28.15066909790039, -5.117218494415283], "cluster": 6}, {"key": "feng2024improving", "year": "2024", "citations": "2", "title": "Improving Composed Image Retrieval Via Contrastive Learning With Scaling Positives And Negatives", "abstract": "<p>The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethod also performs well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario. Our code and data are released\nat https://github.com/BUAADreamer/SPN4CIR.</p>\n", "tags": ["Self-Supervised", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-23.113346099853516, 2.0859341621398926], "cluster": 1}, {"key": "ferdowsi2017multi", "year": "2017", "citations": "2", "title": "A Multi-layer Network Based On Sparse Ternary Codes For Universal Vector Compression", "abstract": "<p>We present the multi-layer extension of the Sparse Ternary Codes (STC) for\nfast similarity search where we focus on the reconstruction of the database\nvectors from the ternary codes. To consider the trade-offs between the\ncompactness of the STC and the quality of the reconstructed vectors, we study\nthe rate-distortion behavior of these codes under different setups. We show\nthat a single-layer code cannot achieve satisfactory results at high rates.\nTherefore, we extend the concept of STC to multiple layers and design the\nML-STC, a codebook-free system that successively refines the reconstruction of\nthe residuals of previous layers. While the ML-STC keeps the sparse ternary\nstructure of the single-layer STC and hence is suitable for fast similarity\nsearch in large-scale databases, we show its superior rate-distortion\nperformance on both model-based synthetic data and public large-scale\ndatabases, as compared to several binary hashing methods.</p>\n", "tags": ["Similarity-Search", "Evaluation", "Hashing-Methods", "Scalability"], "tsne_embedding": [24.49501609802246, 18.937471389770508], "cluster": 2}, {"key": "ferdowsi2017sparse", "year": "2017", "citations": "12", "title": "Sparse Ternary Codes For Similarity Search Have Higher Coding Gain Than Dense Binary Codes", "abstract": "<p>This paper addresses the problem of Approximate Nearest Neighbor (ANN) search\nin pattern recognition where feature vectors in a database are encoded as\ncompact codes in order to speed-up the similarity search in large-scale\ndatabases. Considering the ANN problem from an information-theoretic\nperspective, we interpret it as an encoding, which maps the original feature\nvectors to a less entropic sparse representation while requiring them to be as\ninformative as possible. We then define the coding gain for ANN search using\ninformation-theoretic measures. We next show that the classical approach to\nthis problem, which consists of binarization of the projected vectors is\nsub-optimal. Instead, a properly designed ternary encoding achieves higher\ncoding gains and lower complexity.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Similarity-Search", "Scalability"], "tsne_embedding": [10.219646453857422, 44.2836799621582], "cluster": 4}, {"key": "fernandes2020locality", "year": "2021", "citations": "7", "title": "Locality Sensitive Hashing With Extended Differential Privacy", "abstract": "<p>Extended differential privacy, a generalization of standard differential\nprivacy (DP) using a general metric, has been widely studied to provide\nrigorous privacy guarantees while keeping high utility. However, existing works\non extended DP are limited to few metrics, such as the Euclidean metric.\nConsequently, they have only a small number of applications, such as\nlocation-based services and document processing. In this paper, we propose a\ncouple of mechanisms providing extended DP with a different metric: angular\ndistance (or cosine distance). Our mechanisms are based on locality sensitive\nhashing (LSH), which can be applied to the angular distance and work well for\npersonal data in a high-dimensional space. We theoretically analyze the privacy\nproperties of our mechanisms, and prove extended DP for input data by taking\ninto account that LSH preserves the original metric only approximately. We\napply our mechanisms to friend matching based on high-dimensional personal data\nwith angular distance in the local model, and evaluate our mechanisms using two\nreal datasets. We show that LDP requires a very large privacy budget and that\nRAPPOR does not work in this application. Then we show that our mechanisms\nenable friend matching with high utility and rigorous privacy guarantees based\non extended DP.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Privacy-&-Security", "Datasets"], "tsne_embedding": [-15.854109764099121, 29.581083297729492], "cluster": 8}, {"key": "ferraro2023contrastive", "year": "2023", "citations": "1", "title": "Contrastive Learning For Cross-modal Artist Retrieval", "abstract": "<p>Music retrieval and recommendation applications often rely on content\nfeatures encoded as embeddings, which provide vector representations of items\nin a music dataset. Numerous complementary embeddings can be derived from\nprocessing items originally represented in several modalities, e.g., audio\nsignals, user interaction data, or editorial data. However, data of any given\nmodality might not be available for all items in any music dataset. In this\nwork, we propose a method based on contrastive learning to combine embeddings\nfrom multiple modalities and explore the impact of the presence or absence of\nembeddings from diverse modalities in an artist similarity task. Experiments on\ntwo datasets suggest that our contrastive method outperforms single-modality\nembeddings and baseline algorithms for combining modalities, both in terms of\nartist retrieval accuracy and coverage. Improvements with respect to other\nmethods are particularly significant for less popular query artists. We\ndemonstrate our method successfully combines complementary information from\ndiverse modalities, and is more robust to missing modality data (i.e., it\nbetter handles the retrieval of artists with different modality embeddings than\nthe query artist\u2019s).</p>\n", "tags": ["Self-Supervised", "Recommender-Systems", "Datasets"], "tsne_embedding": [7.822402000427246, -43.7638053894043], "cluster": 3}, {"key": "fervers2024statewide", "year": "2024", "citations": "0", "title": "Statewide Visual Geolocalization In The Wild", "abstract": "<p>This work presents a method that is able to predict the geolocation of a\nstreet-view photo taken in the wild within a state-sized search region by\nmatching against a database of aerial reference imagery. We partition the\nsearch region into geographical cells and train a model to map cells and\ncorresponding photos into a joint embedding space that is used to perform\nretrieval at test time. The model utilizes aerial images for each cell at\nmultiple levels-of-detail to provide sufficient information about the\nsurrounding scene. We propose a novel layout of the search region with\nconsistent cell resolutions that allows scaling to large geographical regions.\nExperiments demonstrate that the method successfully localizes 60.6% of all\nnon-panoramic street-view photos uploaded to the crowd-sourcing platform\nMapillary in the state of Massachusetts to within 50m of their ground-truth\nlocation. Source code is available at\nhttps://github.com/fferflo/statewide-visual-geolocalization.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-35.29623031616211, 23.15160369873047], "cluster": 0}, {"key": "fitzgerald2021moleman", "year": "2021", "citations": "8", "title": "MOLEMAN: Mention-only Linking Of Entities With A Mention Annotation Network", "abstract": "<p>We present an instance-based nearest neighbor approach to entity linking. In\ncontrast to most prior entity retrieval systems which represent each entity\nwith a single vector, we build a contextualized mention-encoder that learns to\nplace similar mentions of the same entity closer in vector space than mentions\nof different entities. This approach allows all mentions of an entity to serve\nas \u201cclass prototypes\u201d as inference involves retrieving from the full set of\nlabeled entity mentions in the training set and applying the nearest mention\nneighbor\u2019s entity label. Our model is trained on a large multilingual corpus of\nmention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor\ninference on an index of 700 million mentions. It is simpler to train, gives\nmore interpretable predictions, and outperforms all other systems on two\nmultilingual entity linking benchmarks.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [35.256980895996094, -24.44207000732422], "cluster": 7}, {"key": "florek2024efficient", "year": "2025", "citations": "0", "title": "Efficient And Discriminative Image Feature Extraction For Universal Image Retrieval", "abstract": "<p>Current image retrieval systems often face domain specificity and\ngeneralization issues. This study aims to overcome these limitations by\ndeveloping a computationally efficient training framework for a universal\nfeature extractor that provides strong semantic image representations across\nvarious domains. To this end, we curated a multi-domain training dataset,\ncalled M4D-35k, which allows for resource-efficient training. Additionally, we\nconduct an extensive evaluation and comparison of various state-of-the-art\nvisual-semantic foundation models and margin-based metric learning loss\nfunctions regarding their suitability for efficient universal feature\nextraction. Despite constrained computational resources, we achieve near\nstate-of-the-art results on the Google Universal Image Embedding Challenge,\nwith a mMP@5 of 0.721. This places our method at the second rank on the\nleaderboard, just 0.7 percentage points behind the best performing method.\nHowever, our model has 32% fewer overall parameters and 289 times fewer\ntrainable parameters. Compared to methods with similar computational\nrequirements, we outperform the previous state of the art by 3.3 percentage\npoints. We release our code and M4D-35k training set annotations at\nhttps://github.com/morrisfl/UniFEx.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-31.73748779296875, -9.83688735961914], "cluster": 5}, {"key": "forcen2020co", "year": "2020", "citations": "19", "title": "Co-occurrence Of Deep Convolutional Features For Image Search", "abstract": "<p>Image search can be tackled using deep features from pre-trained\nConvolutional Neural Networks (CNN). The feature map from the last\nconvolutional layer of a CNN encodes descriptive information from which a\ndiscriminative global descriptor can be obtained. We propose a new\nrepresentation of co-occurrences from deep convolutional features to extract\nadditional relevant information from this last convolutional layer. Combining\nthis co-occurrence map with the feature map, we achieve an improved image\nrepresentation. We present two different methods to get the co-occurrence\nrepresentation, the first one based on direct aggregation of activations, and\nthe second one, based on a trainable co-occurrence representation. The image\ndescriptors derived from our methodology improve the performance in very\nwell-known image retrieval datasets as we prove in the experiments.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.275943756103516, 1.1330506801605225], "cluster": 0}, {"key": "fosset2022docent", "year": "2022", "citations": "2", "title": "Docent: A Content-based Recommendation System To Discover Contemporary Art", "abstract": "<p>Recommendation systems have been widely used in various domains such as\nmusic, films, e-shopping etc. After mostly avoiding digitization, the art world\nhas recently reached a technological turning point due to the pandemic, making\nonline sales grow significantly as well as providing quantitative online data\nabout artists and artworks. In this work, we present a content-based\nrecommendation system on contemporary art relying on images of artworks and\ncontextual metadata of artists. We gathered and annotated artworks with\nadvanced and art-specific information to create a completely unique database\nthat was used to train our models. With this information, we built a proximity\ngraph between artworks. Similarly, we used NLP techniques to characterize the\npractices of the artists and we extracted information from exhibitions and\nother event history to create a proximity graph between artists. The power of\ngraph analysis enables us to provide an artwork recommendation system based on\na combination of visual and contextual information from artworks and artists.\nAfter an assessment by a team of art specialists, we get an average final\nrating of 75% of meaningful artworks when compared to their professional\nevaluations.</p>\n", "tags": ["Graph-Based-Ann", "Recommender-Systems"], "tsne_embedding": [-4.269857406616211, -48.54375076293945], "cluster": 3}, {"key": "foster2022generalized", "year": "2022", "citations": "2", "title": "Generalized Relative Neighborhood Graph (GRNG) For Similarity Search", "abstract": "<p>Similarity search is a fundamental building block for information retrieval\non a variety of datasets. The notion of a neighbor is often based on binary\nconsiderations, such as the k nearest neighbors. However, considering that data\nis often organized as a manifold with low intrinsic dimension, the notion of a\nneighbor must recognize higher-order relationship, to capture neighbors in all\ndirections. Proximity graphs, such as the Relative Neighbor Graphs (RNG), use\ntrinary relationships which capture the notion of direction and have been\nsuccessfully used in a number of applications. However, the current algorithms\nfor computing the RNG, despite widespread use, are approximate and not\nscalable. This paper proposes a novel type of graph, the Generalized Relative\nNeighborhood Graph (GRNG) for use in a pivot layer that then guides the\nefficient and exact construction of the RNG of a set of exemplars. It also\nshows how to extend this to a multi-layer hierarchy which significantly\nimproves over the state-of-the-art methods which can only construct an\napproximate RNG.</p>\n", "tags": ["Graph-Based-Ann", "Similarity-Search", "Datasets"], "tsne_embedding": [51.158817291259766, 6.289369106292725], "cluster": 9}, {"key": "frady2020neuromorphic", "year": "2020", "citations": "47", "title": "Neuromorphic Nearest-neighbor Search Using Intel's Pohoiki Springs", "abstract": "<p>Neuromorphic computing applies insights from neuroscience to uncover\ninnovations in computing technology. In the brain, billions of interconnected\nneurons perform rapid computations at extremely low energy levels by leveraging\nproperties that are foreign to conventional computing systems, such as temporal\nspiking codes and finely parallelized processing units integrating both memory\nand computation. Here, we showcase the Pohoiki Springs neuromorphic system, a\nmesh of 768 interconnected Loihi chips that collectively implement 100 million\nspiking neurons in silicon. We demonstrate a scalable approximate k-nearest\nneighbor (k-NN) algorithm for searching large databases that exploits\nneuromorphic principles. Compared to state-of-the-art conventional CPU-based\nimplementations, we achieve superior latency, index build time, and energy\nefficiency when evaluated on several standard datasets containing over 1\nmillion high-dimensional patterns. Further, the system supports adding new data\npoints to the indexed database online in O(1) time unlike all but brute force\nconventional k-NN implementations.</p>\n", "tags": ["Efficiency", "Datasets"], "tsne_embedding": [43.73541259765625, 18.957286834716797], "cluster": 2}, {"key": "fredriksson2016geometric", "year": "2016", "citations": "6", "title": "Geometric Near-neighbor Access Tree (GNAT) Revisited", "abstract": "<p>Geometric Near-neighbor Access Tree (GNAT) is a metric space indexing method\nbased on hierarchical hyperplane partitioning of the space. While GNAT is very\nefficient in proximity searching, it has a bad reputation of being a memory\nhog. We show that this is partially based on too coarse analysis, and that the\nmemory requirements can be lowered while at the same time improving the search\nefficiency. We also show how to make GNAT memory adaptive in a smooth way, and\nthat the hyperplane partitioning can be replaced with ball partitioning, which\ncan further improve the search performance. We conclude with experimental\nresults showing the new methods can give significant performance boost.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [19.182172775268555, 39.312660217285156], "cluster": 4}, {"key": "fu2016auto", "year": "2016", "citations": "3", "title": "Auto-jacobin: Auto-encoder Jacobian Binary Hashing", "abstract": "<p>Binary codes can be used to speed up nearest neighbor search tasks in large\nscale data sets as they are efficient for both storage and retrieval. In this\npaper, we propose a robust auto-encoder model that preserves the geometric\nrelationships of high-dimensional data sets in Hamming space. This is done by\nconsidering a noise-removing function in a region surrounding the manifold\nwhere the training data points lie. This function is defined with the property\nthat it projects the data points near the manifold into the manifold wisely,\nand we approximate this function by its first order approximation. Experimental\nresults show that the proposed method achieves better than state-of-the-art\nresults on three large scale high dimensional data sets.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods"], "tsne_embedding": [1.5957571268081665, 37.86829376220703], "cluster": 4}, {"key": "fu2016efanna", "year": "2016", "citations": "66", "title": "EFANNA : An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based On Knn Graph", "abstract": "<p>Approximate nearest neighbor (ANN) search is a fundamental problem in many\nareas of data mining, machine learning and computer vision. The performance of\ntraditional hierarchical structure (tree) based methods decreases as the\ndimensionality of data grows, while hashing based methods usually lack\nefficiency in practice. Recently, the graph based methods have drawn\nconsiderable attention. The main idea is that <em>a neighbor of a neighbor is\nalso likely to be a neighbor</em>, which we refer as <em>NN-expansion</em>. These\nmethods construct a \\(k\\)-nearest neighbor (\\(k\\)NN) graph offline. And at online\nsearch stage, these methods find candidate neighbors of a query point in some\nway (\\eg, random selection), and then check the neighbors of these candidate\nneighbors for closer ones iteratively. Despite some promising results, there\nare mainly two problems with these approaches: 1) These approaches tend to\nconverge to local optima. 2) Constructing a \\(k\\)NN graph is time consuming. We\nfind that these two problems can be nicely solved when we provide a good\ninitialization for NN-expansion. In this paper, we propose EFANNA, an extremely\nfast approximate nearest neighbor search algorithm based on \\(k\\)NN Graph. Efanna\nnicely combines the advantages of hierarchical structure based methods and\nnearest-neighbor-graph based methods. Extensive experiments have shown that\nEFANNA outperforms the state-of-art algorithms both on approximate nearest\nneighbor search and approximate nearest neighbor graph construction. To the\nbest of our knowledge, EFANNA is the fastest algorithm so far both on\napproximate nearest neighbor graph construction and approximate nearest\nneighbor search. A library EFANNA based on this research is released on Github.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Efficiency", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [55.77367401123047, 8.680859565734863], "cluster": 9}, {"key": "fu2016improved", "year": "2015", "citations": "5", "title": "An Improved System For Sentence-level Novelty Detection In Textual Streams", "abstract": "<p>Novelty detection in news events has long been a difficult problem. A number\nof models performed well on specific data streams but certain issues are far\nfrom being solved, particularly in large data streams from the WWW where\nunpredictability of new terms requires adaptation in the vector space model. We\npresent a novel event detection system based on the Incremental Term\nFrequency-Inverse Document Frequency (TF-IDF) weighting incorporated with\nLocality Sensitive Hashing (LSH). Our system could efficiently and effectively\nadapt to the changes within the data streams of any new terms with continual\nupdates to the vector space model. Regarding miss probability, our proposed\nnovelty detection framework outperforms a recognised baseline system by\napproximately 16% when evaluating a benchmark dataset from Google News.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [11.163003921508789, -27.88510513305664], "cluster": 7}, {"key": "fu2017fast", "year": "2019", "citations": "222", "title": "Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph", "abstract": "<p>Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial search scenario of Taobao (Alibaba Group) and has been integrated into their search engine at billion-node scale.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [57.01401901245117, 8.462059020996094], "cluster": 9}, {"key": "fu2018neurons", "year": "2019", "citations": "6", "title": "Neurons Merging Layer: Towards Progressive Redundancy Reduction For Deep Supervised Hashing", "abstract": "<p>Deep supervised hashing has become an active topic in information retrieval.\nIt generates hashing bits by the output neurons of a deep hashing network.\nDuring binary discretization, there often exists much redundancy between\nhashing bits that degenerates retrieval performance in terms of both storage\nand accuracy. This paper proposes a simple yet effective Neurons Merging Layer\n(NMLayer) for deep supervised hashing. A graph is constructed to represent the\nredundancy relationship between hashing bits that is used to guide the learning\nof a hashing network. Specifically, it is dynamically learned by a novel\nmechanism defined in our active and frozen phases. According to the learned\nrelationship, the NMLayer merges the redundant neurons together to balance the\nimportance of each output neuron. Moreover, multiple NMLayers are progressively\ntrained for a deep hashing network to learn a more compact hashing code from a\nlong redundant code. Extensive experiments on four datasets demonstrate that\nour proposed method outperforms state-of-the-art hashing methods.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "AAAI", "Hashing-Methods", "Supervised", "IJCAI"], "tsne_embedding": [30.652996063232422, -4.454356670379639], "cluster": 9}, {"key": "fu2020deep", "year": "2021", "citations": "15", "title": "Deep Momentum Uncertainty Hashing", "abstract": "<p>Combinatorial optimization (CO) has been a hot research topic because of its\ntheoretic and practical importance. As a classic CO problem, deep hashing aims\nto find an optimal code for each data from finite discrete possibilities, while\nthe discrete nature brings a big challenge to the optimization process.\nPrevious methods usually mitigate this challenge by binary approximation,\nsubstituting binary codes for real-values via activation functions or\nregularizations. However, such approximation leads to uncertainty between\nreal-values and binary ones, degrading retrieval performance. In this paper, we\npropose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly\nestimates the uncertainty during training and leverages the uncertainty\ninformation to guide the approximation process. Specifically, we model\nbit-level uncertainty via measuring the discrepancy between the output of a\nhashing network and that of a momentum-updated network. The discrepancy of each\nbit indicates the uncertainty of the hashing network to the approximate output\nof that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can\nbe regarded as image-level uncertainty. It embodies the uncertainty of the\nhashing network to the corresponding input image. The hashing bit and image\nwith higher uncertainty are paid more attention during optimization. To the\nbest of our knowledge, this is the first work to study the uncertainty in\nhashing bits. Extensive experiments are conducted on four datasets to verify\nthe superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a\nmillion-scale dataset Clothing1M. Our method achieves the best performance on\nall of the datasets and surpasses existing state-of-the-art methods by a large\nmargin.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Neural-Hashing", "Compact-Codes", "Hashing-Methods"], "tsne_embedding": [7.454200744628906, 17.470449447631836], "cluster": 6}, {"key": "fu2020hard", "year": "2020", "citations": "6", "title": "Hard Example Generation By Texture Synthesis For Cross-domain Shape Similarity Learning", "abstract": "<p>Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape\nof a given 2D image from a large 3D shape database. The common routine is to\nmap 2D images and 3D shapes into an embedding space and define (or learn) a\nshape similarity measure. While metric learning with some adaptation techniques\nseems to be a natural solution to shape similarity learning, the performance is\noften unsatisfactory for fine-grained shape retrieval. In the paper, we\nidentify the source of the poor performance and propose a practical solution to\nthis problem. We find that the shape difference between a negative pair is\nentangled with the texture gap, making metric learning ineffective in pushing\naway negative pairs. To tackle this issue, we develop a geometry-focused\nmulti-view metric learning framework empowered by texture synthesis. The\nsynthesis of textures for 3D shape models creates hard triplets, which suppress\nthe adverse effects of rich texture in 2D images, thereby push the network to\nfocus more on discovering geometric characteristics. Our approach shows\nstate-of-the-art performance on a recently released large-scale 3D-FUTURE[1]\nrepository, as well as three widely studied benchmarks, including Pix3D[2],\nStanford Cars[3], and Comp Cars[4]. Codes will be made publicly available at:\nhttps://github.com/3D-FRONT-FUTURE/IBSR-texture</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [-39.50484848022461, -14.228255271911621], "cluster": 5}, {"key": "fuentes2021sketch", "year": "2021", "citations": "14", "title": "Sketch-qnet: A Quadruplet Convnet For Color Sketch-based Image Retrieval", "abstract": "<p>Architectures based on siamese networks with triplet loss have shown\noutstanding performance on the image-based similarity search problem. This\napproach attempts to discriminate between positive (relevant) and negative\n(irrelevant) items. However, it undergoes a critical weakness. Given a query,\nit cannot discriminate weakly relevant items, for instance, items of the same\ntype but different color or texture as the given query, which could be a\nserious limitation for many real-world search applications. Therefore, in this\nwork, we present a quadruplet-based architecture that overcomes the\naforementioned weakness. Moreover, we present an instance of this quadruplet\nnetwork, which we call Sketch-QNet, to deal with the color sketch-based image\nretrieval (CSBIR) problem, achieving new state-of-the-art results.</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Similarity-Search", "Image-Retrieval", "Evaluation"], "tsne_embedding": [-16.943201065063477, 7.390200614929199], "cluster": 1}, {"key": "furuya2021deepdiffusion", "year": "2022", "citations": "3", "title": "Deepdiffusion: Unsupervised Learning Of Retrieval-adapted Representations Via Diffusion-based Ranking On Latent Feature Manifold", "abstract": "<p>Unsupervised learning of feature representations is a challenging yet\nimportant problem for analyzing a large collection of multimedia data that do\nnot have semantic labels. Recently proposed neural network-based unsupervised\nlearning approaches have succeeded in obtaining features appropriate for\nclassification of multimedia data. However, unsupervised learning of feature\nrepresentations adapted to content-based matching, comparison, or retrieval of\nmultimedia data has not been explored well. To obtain such retrieval-adapted\nfeatures, we introduce the idea of combining diffusion distance on a feature\nmanifold with neural network-based unsupervised feature learning. This idea is\nrealized as a novel algorithm called DeepDiffusion (DD). DD simultaneously\noptimizes two components, a feature embedding by a deep neural network and a\ndistance metric that leverages diffusion on a latent feature manifold,\ntogether. DD relies on its loss function but not encoder architecture. It can\nthus be applied to diverse multimedia data types with their respective encoder\narchitectures. Experimental evaluation using 3D shapes and 2D images\ndemonstrates versatility as well as high accuracy of the DD algorithm. Code is\navailable at https://github.com/takahikof/DeepDiffusion</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Unsupervised"], "tsne_embedding": [1.5113260746002197, -9.540173530578613], "cluster": 1}, {"key": "gabeur2020multi", "year": "2020", "citations": "492", "title": "Multi-modal Transformer For Video Retrieval", "abstract": "<p>The task of retrieving video content relevant to natural language queries\nplays a critical role in effectively handling internet-scale datasets. Most of\nthe existing methods for this caption-to-video retrieval problem do not fully\nexploit cross-modal cues present in video. Furthermore, they aggregate\nper-frame visual features with limited or no temporal information. In this\npaper, we present a multi-modal transformer to jointly encode the different\nmodalities in video, which allows each of them to attend to the others. The\ntransformer architecture is also leveraged to encode and model the temporal\ninformation. On the natural language side, we investigate the best practices to\njointly optimize the language embedding together with the multi-modal\ntransformer. This novel framework allows us to establish state-of-the-art\nresults for video retrieval on three datasets. More details are available at\nhttp://thoth.inrialpes.fr/research/MMT.</p>\n", "tags": ["Tools-&-Libraries", "Video-Retrieval", "Datasets"], "tsne_embedding": [-37.8533935546875, -32.21270751953125], "cluster": 5}, {"key": "gajic2019bag", "year": "2019", "citations": "2", "title": "Bag Of Negatives For Siamese Architectures", "abstract": "<p>Training a Siamese architecture for re-identification with a large number of\nidentities is a challenging task due to the difficulty of finding relevant\nnegative samples efficiently. In this work we present Bag of Negatives (BoN), a\nmethod for accelerated and improved training of Siamese networks that scales\nwell on datasets with a very large number of identities. BoN is an efficient\nand loss-independent method, able to select a bag of high quality negatives,\nbased on a novel online hashing strategy.</p>\n", "tags": ["Hashing-Methods", "Datasets"], "tsne_embedding": [22.050365447998047, -14.201085090637207], "cluster": 7}, {"key": "galanopoulos2022are", "year": "2023", "citations": "11", "title": "Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval", "abstract": "<p>In this paper we tackle the cross-modal video retrieval problem and, more\nspecifically, we focus on text-to-video retrieval. We investigate how to\noptimally combine multiple diverse textual and visual features into feature\npairs that lead to generating multiple joint feature spaces, which encode\ntext-video pairs into comparable representations. To learn these\nrepresentations our proposed network architecture is trained by following a\nmultiple space learning procedure. Moreover, at the retrieval stage, we\nintroduce additional softmax operations for revising the inferred query-video\nsimilarities. Extensive experiments in several setups based on three\nlarge-scale datasets (IACC.3, V3C1, and MSR-VTT) lead to conclusions on how to\nbest combine text-visual features and document the performance of the proposed\nnetwork. Source code is made publicly available at:\nhttps://github.com/bmezaris/TextToVideoRetrieval-TtimesV</p>\n", "tags": ["Datasets", "Scalability", "Video-Retrieval", "Evaluation"], "tsne_embedding": [-25.8809871673584, -38.83823776245117], "cluster": 5}, {"key": "galanopoulos2023are", "year": "2023", "citations": "11", "title": "Are All Combinations Equal? Combining Textual And Visual Features With Multiple Space Learning For Text-based Video Retrieval", "abstract": "<p>In this paper we tackle the cross-modal video retrieval problem and, more\nspecifically, we focus on text-to-video retrieval. We investigate how to\noptimally combine multiple diverse textual and visual features into feature\npairs that lead to generating multiple joint feature spaces, which encode\ntext-video pairs into comparable representations. To learn these\nrepresentations our proposed network architecture is trained by following a\nmultiple space learning procedure. Moreover, at the retrieval stage, we\nintroduce additional softmax operations for revising the inferred query-video\nsimilarities. Extensive experiments in several setups based on three\nlarge-scale datasets (IACC.3, V3C1, and MSR-VTT) lead to conclusions on how to\nbest combine text-visual features and document the performance of the proposed\nnetwork. Source code is made publicly available at:\nhttps://github.com/bmezaris/TextToVideoRetrieval-TtimesV</p>\n", "tags": ["Evaluation", "Video-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [-25.8809871673584, -38.83823776245117], "cluster": 5}, {"key": "gan2023binary", "year": "2023", "citations": "7", "title": "Binary Embedding-based Retrieval At Tencent", "abstract": "<p>Large-scale embedding-based retrieval (EBR) is the cornerstone of\nsearch-related industrial applications. Given a user query, the system of EBR\naims to identify relevant information from a large corpus of documents that may\nbe tens or hundreds of billions in size. The storage and computation turn out\nto be expensive and inefficient with massive documents and high concurrent\nqueries, making it difficult to further scale up. To tackle the challenge, we\npropose a binary embedding-based retrieval (BEBR) engine equipped with a\nrecurrent binarization algorithm that enables customized bits per dimension.\nSpecifically, we compress the full-precision query and document embeddings,\nformulated as float vectors in general, into a composition of multiple binary\nvectors using a lightweight transformation model with residual multilayer\nperception (MLP) blocks. We can therefore tailor the number of bits for\ndifferent applications to trade off accuracy loss and cost savings.\nImportantly, we enable task-agnostic efficient training of the binarization\nmodel using a new embedding-to-embedding strategy. We also exploit the\ncompatible training of binary embeddings so that the BEBR engine can support\nindexing among multiple embedding versions within a unified system. To further\nrealize efficient search, we propose Symmetric Distance Calculation (SDC) to\nachieve lower response time than Hamming codes. We successfully employed the\nintroduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World,\netc. The binarization algorithm can be seamlessly generalized to various tasks\nwith multiple modalities. Extensive experiments on offline benchmarks and\nonline A/B tests demonstrate the efficiency and effectiveness of our method,\nsignificantly saving 30%~50% index costs with almost no loss of accuracy at the\nsystem level.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Evaluation", "KDD"], "tsne_embedding": [20.390783309936523, 16.356456756591797], "cluster": 2}, {"key": "ganea2021incremental", "year": "2021", "citations": "55", "title": "Incremental Few-shot Instance Segmentation", "abstract": "<p>Few-shot instance segmentation methods are promising when labeled training\ndata for novel classes is scarce. However, current approaches do not facilitate\nflexible addition of novel classes. They also require that examples of each\nclass are provided at train and test time, which is memory intensive. In this\npaper, we address these limitations by presenting the first incremental\napproach to few-shot instance segmentation: iMTFA. We learn discriminative\nembeddings for object instances that are merged into class representatives.\nStoring embedding vectors rather than images effectively solves the memory\noverhead problem. We match these class embeddings at the RoI-level using cosine\nsimilarity. This allows us to add new classes without the need for further\ntraining or access to previous training data. In a series of experiments, we\nconsistently outperform the current state-of-the-art. Moreover, the reduced\nmemory requirements allow us to evaluate, for the first time, few-shot instance\nsegmentation performance on all classes in COCO jointly.</p>\n", "tags": ["CVPR", "Evaluation", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [37.79644775390625, -9.242405891418457], "cluster": 9}, {"key": "gao2019beyond", "year": "2019", "citations": "28", "title": "Beyond Product Quantization: Deep Progressive Quantization For Image Retrieval", "abstract": "<p>Product Quantization (PQ) has long been a mainstream for generating an\nexponentially large codebook at very low memory/time cost. Despite its success,\nPQ is still tricky for the decomposition of high-dimensional vector space, and\nthe retraining of model is usually unavoidable when the code length changes. In\nthis work, we propose a deep progressive quantization (DPQ) model, as an\nalternative to PQ, for large scale image retrieval. DPQ learns the quantization\ncodes sequentially and approximates the original feature space progressively.\nTherefore, we can train the quantization codes with different code lengths\nsimultaneously. Specifically, we first utilize the label information for\nguiding the learning of visual features, and then apply several quantization\nblocks to progressively approach the visual features. Each quantization block\nis designed to be a layer of a convolutional neural network, and the whole\nframework can be trained in an end-to-end manner. Experimental results on the\nbenchmark datasets show that our model significantly outperforms the\nstate-of-the-art for image retrieval. Our model is trained once for different\ncode lengths and therefore requires less computation time. Additional ablation\nstudy demonstrates the effect of each component of our proposed model. Our code\nis released at https://github.com/cfm-uestc/DPQ.</p>\n", "tags": ["Datasets", "Evaluation", "AAAI", "Quantization", "Tools-&-Libraries", "Image-Retrieval", "IJCAI"], "tsne_embedding": [-20.2386417388916, -16.918996810913086], "cluster": 1}, {"key": "gao2020complementing", "year": "2020", "citations": "61", "title": "Complementing Lexical Retrieval With Semantic Residual Embedding", "abstract": "<p>This paper presents CLEAR, a retrieval model that seeks to complement\nclassical lexical exact-match models such as BM25 with semantic matching\nsignals from a neural embedding matching model. CLEAR explicitly trains the\nneural embedding to encode language structures and semantics that lexical\nretrieval fails to capture with a novel residual-based embedding learning\nmethod. Empirical evaluations demonstrate the advantages of CLEAR over\nstate-of-the-art retrieval models, and that it can substantially improve the\nend-to-end accuracy and efficiency of reranking pipelines.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [3.8378188610076904, -26.454113006591797], "cluster": 7}, {"key": "gao2020fashionbert", "year": "2020", "citations": "102", "title": "Fashionbert: Text And Image Matching With Adaptive Loss For Cross-modal Retrieval", "abstract": "<p>In this paper, we address the text and image matching in cross-modal\nretrieval of the fashion industry. Different from the matching in the general\ndomain, the fashion matching is required to pay much more attention to the\nfine-grained information in the fashion images and texts. Pioneer approaches\ndetect the region of interests (i.e., RoIs) from images and use the RoI\nembeddings as image representations. In general, RoIs tend to represent the\n\u201cobject-level\u201d information in the fashion images, while fashion texts are prone\nto describe more detailed information, e.g. styles, attributes. RoIs are thus\nnot fine-grained enough for fashion text and image matching. To this end, we\npropose FashionBERT, which leverages patches as image features. With the\npre-trained BERT model as the backbone network, FashionBERT learns high level\nrepresentations of texts and images. Meanwhile, we propose an adaptive loss to\ntrade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text\nand image matching and cross-modal retrieval) are incorporated to evaluate\nFashionBERT. On the public dataset, experiments demonstrate FashionBERT\nachieves significant improvements in performances than the baseline and\nstate-of-the-art approaches. In practice, FashionBERT is applied in a concrete\ncross-modal retrieval application. We provide the detailed matching performance\nand inference efficiency analysis.</p>\n", "tags": ["Efficiency", "SIGIR", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-22.262374877929688, -47.54658889770508], "cluster": 3}, {"key": "gao2021backdoor", "year": "2021", "citations": "3", "title": "Backdoor Attack On Hash-based Image Retrieval Via Clean-label Data Poisoning", "abstract": "<p>A backdoored deep hashing model is expected to behave normally on original\nquery images and return the images with the target label when a specific\ntrigger pattern presents. To this end, we propose the confusing\nperturbations-induced backdoor attack (CIBA). It injects a small number of\npoisoned images with the correct label into the training data, which makes the\nattack hard to be detected. To craft the poisoned images, we first propose the\nconfusing perturbations to disturb the hashing code learning. As such, the\nhashing model can learn more about the trigger. The confusing perturbations are\nimperceptible and generated by optimizing the intra-class dispersion and\ninter-class shift in the Hamming space. We then employ the targeted adversarial\npatch as the backdoor trigger to improve the attack performance. We have\nconducted extensive experiments to verify the effectiveness of our proposed\nCIBA. Our code is available at https://github.com/KuofengGao/CIBA.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Robustness", "Evaluation"], "tsne_embedding": [-7.836631774902344, 11.666449546813965], "cluster": 8}, {"key": "gao2022clusterea", "year": "2022", "citations": "29", "title": "Clusterea: Scalable Entity Alignment With Stochastic Training And Normalized Mini-batch Similarities", "abstract": "<p>Entity alignment (EA) aims at finding equivalent entities in different\nknowledge graphs (KGs). Embedding-based approaches have dominated the EA task\nin recent years. Those methods face problems that come from the geometric\nproperties of embedding vectors, including hubness and isolation. To solve\nthese geometric problems, many normalization approaches have been adopted for\nEA. However, the increasing scale of KGs renders it hard for EA models to adopt\nthe normalization processes, thus limiting their usage in real-world\napplications. To tackle this challenge, we present ClusterEA, a general\nframework that is capable of scaling up EA models and enhancing their results\nby leveraging normalization methods on mini-batches with a high entity\nequivalent rate. ClusterEA contains three components to align entities between\nlarge-scale KGs, including stochastic training, ClusterSampler, and\nSparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic\nfashion to produce entity embeddings. Based on the embeddings, a novel\nClusterSampler strategy is proposed for sampling highly overlapped\nmini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes\nlocal and global similarity and then fuses all similarity matrices to obtain\nthe final similarity matrix. Extensive experiments with real-life datasets on\nEA benchmarks offer insight into the proposed framework, and suggest that it is\ncapable of outperforming the state-of-the-art scalable EA framework by up to 8\ntimes in terms of Hits@1.</p>\n", "tags": ["KDD", "Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [44.73188400268555, -7.5542778968811035], "cluster": 9}, {"key": "gao2022long", "year": "2023", "citations": "5", "title": "Long-tail Cross Modal Hashing", "abstract": "<p>Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced\ndata, while imbalanced data with long-tail distribution is more general in\nreal-world. Several long-tail hashing methods have been proposed but they can\nnot adapt for multi-modal data, due to the complex interplay between labels and\nindividuality and commonality information of multi-modal data. Furthermore, CMH\nmethods mostly mine the commonality of multi-modal data to learn hash codes,\nwhich may override tail labels encoded by the individuality of respective\nmodalities. In this paper, we propose LtCMH (Long-tail CMH) to handle\nimbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the\nindividuality and commonality of different modalities by minimizing the\ndependency between the individuality of respective modalities and by enhancing\nthe commonality of these modalities. Then it dynamically combines the\nindividuality and commonality with direct features extracted from respective\nmodalities to create meta features that enrich the representation of tail\nlabels, and binaries meta features to generate hash codes. LtCMH significantly\noutperforms state-of-the-art baselines on long-tail datasets and holds a better\n(or comparable) performance on datasets with balanced labels.</p>\n", "tags": ["AAAI", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [10.956509590148926, 13.07170295715332], "cluster": 6}, {"key": "gao2022precise", "year": "2023", "citations": "81", "title": "Precise Zero-shot Dense Retrieval Without Relevance Labels", "abstract": "<p>While dense retrieval has been shown effective and efficient across tasks and\nlanguages, it remains difficult to create effective fully zero-shot dense\nretrieval systems when no relevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning and encoding relevance. Instead,\nwe propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a\nquery, HyDE first zero-shot instructs an instruction-following language model\n(e.g. InstructGPT) to generate a hypothetical document. The document captures\nrelevance patterns but is unreal and may contain false details. Then, an\nunsupervised contrastively learned encoder~(e.g. Contriever) encodes the\ndocument into an embedding vector. This vector identifies a neighborhood in the\ncorpus embedding space, where similar real documents are retrieved based on\nvector similarity. This second step ground the generated document to the actual\ncorpus, with the encoder\u2019s dense bottleneck filtering out the incorrect\ndetails. Our experiments show that HyDE significantly outperforms the\nstate-of-the-art unsupervised dense retriever Contriever and shows strong\nperformance comparable to fine-tuned retrievers, across various tasks (e.g. web\nsearch, QA, fact verification) and languages~(e.g. sw, ko, ja).</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Unsupervised"], "tsne_embedding": [9.278231620788574, -19.04831886291504], "cluster": 7}, {"key": "gao2023high", "year": "2023", "citations": "24", "title": "High-dimensional Approximate Nearest Neighbor Search: With Reliable And Efficient Distance Comparison Operations", "abstract": "<p>Approximate K nearest neighbor (AKNN) search is a fundamental and challenging\nproblem. We observe that in high-dimensional space, the time consumption of\nnearly all AKNN algorithms is dominated by that of the distance comparison\noperations (DCOs). For each operation, it scans full dimensions of an object\nand thus, runs in linear time wrt the dimensionality. To speed it up, we\npropose a randomized algorithm named ADSampling which runs in logarithmic time\nwrt to the dimensionality for the majority of DCOs and succeeds with high\nprobability. In addition, based on ADSampling we develop one general and two\nalgorithm-specific techniques as plugins to enhance existing AKNN algorithms.\nBoth theoretical and empirical studies confirm that: (1) our techniques\nintroduce nearly no accuracy loss and (2) they consistently improve the\nefficiency.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [34.56782913208008, 29.32808494567871], "cluster": 2}, {"key": "gao2024practical", "year": "2025", "citations": "0", "title": "Practical And Asymptotically Optimal Quantization Of High-dimensional Vectors In Euclidean Space For Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor (ANN) query in high-dimensional Euclidean space\nis a key operator in database systems. For this query, quantization is a\npopular family of methods developed for compressing vectors and reducing memory\nconsumption. Recently, a method called RaBitQ achieves the state-of-the-art\nperformance among these methods. It produces better empirical performance in\nboth accuracy and efficiency when using the same compression rate and provides\nrigorous theoretical guarantees. However, the method is only designed for\ncompressing vectors at high compression rates (32x) and lacks support for\nachieving higher accuracy by using more space. In this paper, we introduce a\nnew quantization method to address this limitation by extending RaBitQ. The new\nmethod inherits the theoretical guarantees of RaBitQ and achieves the\nasymptotic optimality in terms of the trade-off between space and error bounds\nas to be proven in this study. Additionally, we present efficient\nimplementations of the method, enabling its application to ANN queries to\nreduce both space and time consumption. Extensive experiments on real-world\ndatasets confirm that our method consistently outperforms the state-of-the-art\nbaselines in both accuracy and efficiency when using the same amount of memory.</p>\n", "tags": ["Efficiency", "Quantization", "Evaluation", "Datasets"], "tsne_embedding": [12.545416831970215, 34.23017883300781], "cluster": 4}, {"key": "gao2024rabitq", "year": "2024", "citations": "15", "title": "Rabitq: Quantizing High-dimensional Vectors With A Theoretical Error Bound For Approximate Nearest Neighbor Search", "abstract": "<p>Searching for approximate nearest neighbors (ANN) in the high-dimensional\nEuclidean space is a pivotal problem. Recently, with the help of fast\nSIMD-based implementations, Product Quantization (PQ) and its variants can\noften efficiently and accurately estimate the distances between the vectors and\nhave achieved great success in the in-memory ANN search. Despite their\nempirical success, we note that these methods do not have a theoretical error\nbound and are observed to fail disastrously on some real-world datasets.\nMotivated by this, we propose a new randomized quantization method named\nRaBitQ, which quantizes \\(D\\)-dimensional vectors into \\(D\\)-bit strings. RaBitQ\nguarantees a sharp theoretical error bound and provides good empirical accuracy\nat the same time. In addition, we introduce efficient implementations of\nRaBitQ, supporting to estimate the distances with bitwise operations or\nSIMD-based operations. Extensive experiments on real-world datasets confirm\nthat (1) our method outperforms PQ and its variants in terms of\naccuracy-efficiency trade-off by a clear margin and (2) its empirical\nperformance is well-aligned with our theoretical analysis.</p>\n", "tags": ["Efficiency", "Quantization", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [12.326891899108887, 34.20165252685547], "cluster": 4}, {"key": "garcia2017learning", "year": "2019", "citations": "40", "title": "Learning Non-metric Visual Similarity For Image Retrieval", "abstract": "<p>Measuring visual similarity between two or more instances within a data\ndistribution is a fundamental task in image retrieval. Theoretically,\nnon-metric distances are able to generate a more complex and accurate\nsimilarity model than metric distances, provided that the non-linear data\ndistribution is precisely captured by the system. In this work, we explore\nneural networks models for learning a non-metric similarity function for\ninstance search. We argue that non-metric similarity functions based on neural\nnetworks can build a better model of human visual perception than standard\nmetric distances. As our proposed similarity function is differentiable, we\nexplore a real end-to-end trainable approach for image retrieval, i.e. we learn\nthe weights from the input image pixels to the final similarity score.\nExperimental evaluation shows that non-metric similarity networks are able to\nlearn visual similarities between images and improve performance on top of\nstate-of-the-art image representations, boosting results in standard image\nretrieval datasets with respect standard metric distances.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-17.05176544189453, -9.968172073364258], "cluster": 1}, {"key": "garcia2019context", "year": "2019", "citations": "26", "title": "Context-aware Embeddings For Automatic Art Analysis", "abstract": "<p>Automatic art analysis aims to classify and retrieve artistic representations\nfrom a collection of images by using computer vision and machine learning\ntechniques. In this work, we propose to enhance visual representations from\nneural networks with contextual artistic information. Whereas visual\nrepresentations are able to capture information about the content and the style\nof an artwork, our proposed context-aware embeddings additionally encode\nrelationships between different artistic attributes, such as author, school, or\nhistorical period. We design two different approaches for using context in\nautomatic art analysis. In the first one, contextual data is obtained through a\nmulti-task learning model, in which several attributes are trained together to\nfind visual relationships between elements. In the second approach, context is\nobtained through an art-specific knowledge graph, which encodes relationships\nbetween artistic attributes. An exhaustive evaluation of both of our models in\nseveral art analysis problems, such as author identification, type\nclassification, or cross-modal retrieval, show that performance is improved by\nup to 7.3% in art classification and 37.24% in retrieval when context-aware\nembeddings are used.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-20.045604705810547, -28.06098175048828], "cluster": 5}, {"key": "garg2019nearly", "year": "2019", "citations": "3", "title": "Nearly-unsupervised Hashcode Representations For Relation Extraction", "abstract": "<p>Recently, kernelized locality sensitive hashcodes have been successfully\nemployed as representations of natural language text, especially showing high\nrelevance to biomedical relation extraction tasks. In this paper, we propose to\noptimize the hashcode representations in a nearly unsupervised manner, in which\nwe only use data points, but not their class labels, for learning. The\noptimized hashcode representations are then fed to a supervised classifier\nfollowing the prior work. This nearly unsupervised approach allows fine-grained\noptimization of each hash function, which is particularly suitable for building\nhashcode representations generalizing from a training set to a test set. We\nempirically evaluate the proposed approach for biomedical relation extraction\ntasks, obtaining significant accuracy improvements w.r.t. state-of-the-art\nsupervised and semi-supervised approaches.</p>\n", "tags": ["Supervised", "Hashing-Methods", "Unsupervised"], "tsne_embedding": [-47.30756378173828, 22.505023956298828], "cluster": 0}, {"key": "garg2024revisit", "year": "2024", "citations": "1", "title": "Revisit Anything: Visual Place Recognition Via Image Segment Retrieval", "abstract": "<p>Accurately recognizing a revisited place is crucial for embodied agents to\nlocalize and navigate. This requires visual representations to be distinct,\ndespite strong variations in camera viewpoint and scene appearance. Existing\nvisual place recognition pipelines encode the \u201cwhole\u201d image and search for\nmatches. This poses a fundamental challenge in matching two images of the same\nplace captured from different camera viewpoints: \u201cthe similarity of what\noverlaps can be dominated by the dissimilarity of what does not overlap\u201d. We\naddress this by encoding and searching for \u201cimage segments\u201d instead of the\nwhole images. We propose to use open-set image segmentation to decompose an\nimage into <code class=\"language-plaintext highlighter-rouge\">meaningful' entities (i.e., things and stuff). This enables us to\ncreate a novel image representation as a collection of multiple overlapping\nsubgraphs connecting a segment with its neighboring segments, dubbed\nSuperSegment. Furthermore, to efficiently encode these SuperSegments into\ncompact vector representations, we propose a novel factorized representation of\nfeature aggregation. We show that retrieving these partial representations\nleads to significantly higher recognition recall than the typical whole image\nbased retrieval. Our segments-based approach, dubbed SegVLAD, sets a new\nstate-of-the-art in place recognition on a diverse selection of benchmark\ndatasets, while being applicable to both generic and task-specialized image\nencoders. Finally, we demonstrate the potential of our method to </code>`revisit\nanything\u2019\u2019 by evaluating our method on an object instance retrieval task, which\nbridges the two disparate areas of research: visual place recognition and\nobject-goal navigation, through their common aim of recognizing goal objects\nspecific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-24.17229461669922, -5.768805503845215], "cluster": 1}, {"key": "gaskill2019bitwise", "year": "2019", "citations": "1", "title": "The Bitwise Hashing Trick For Personalized Search", "abstract": "<p>Many real world problems require fast and efficient lexical comparison of\nlarge numbers of short text strings. Search personalization is one such domain.\nWe introduce the use of feature bit vectors using the hashing trick for\nimproving relevance in personalized search and other personalization\napplications. We present results of several lexical hashing and comparison\nmethods. These methods are applied to a user\u2019s historical behavior and are used\nto predict future behavior. Using a single bit per dimension instead of\nfloating point results in an order of magnitude decrease in data structure\nsize, while preserving or even improving quality. We use real data to simulate\na search personalization task. A simple method for combining bit vectors\ndemonstrates an order of magnitude improvement in compute time on the task with\nonly a small decrease in accuracy.</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [28.619468688964844, 18.61851692199707], "cluster": 2}, {"key": "gasser2019towards", "year": "2019", "citations": "4", "title": "Towards An All-purpose Content-based Multimedia Information Retrieval System", "abstract": "<p>The growth of multimedia collections - in terms of size, heterogeneity, and\nvariety of media types - necessitates systems that are able to conjointly deal\nwith several forms of media, especially when it comes to searching for\nparticular objects. However, existing retrieval systems are organized in silos\nand treat different media types separately. As a consequence, retrieval across\nmedia types is either not supported at all or subject to major limitations. In\nthis paper, we present vitrivr, a content-based multimedia information\nretrieval stack. As opposed to the keyword search approach implemented by most\nmedia management systems, vitrivr makes direct use of the object\u2019s content to\nfacilitate different types of similarity search, such as Query-by-Example or\nQuery-by-Sketch, for and, most importantly, across different media types -\nnamely, images, audio, videos, and 3D models. Furthermore, we introduce a new\nweb-based user interface that enables easy-to-use, multimodal retrieval from\nand browsing in mixed media collections. The effectiveness of vitrivr is shown\non the basis of a user study that involves different query and media types. To\nthe best of our knowledge, the full vitrivr stack is unique in that it is the\nfirst multimedia retrieval system that seamlessly integrates support for four\ndifferent types of media. As such, it paves the way towards an all-purpose,\ncontent-based multimedia information retrieval system.</p>\n", "tags": ["Multimodal-Retrieval", "Similarity-Search"], "tsne_embedding": [11.910616874694824, -36.284584045410156], "cluster": 7}, {"key": "gatti2025composite", "year": "2024", "citations": "1", "title": "Composite Sketch+text Queries For Retrieving Objects With Elusive Names And Complex Interactions", "abstract": "<p>Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "AAAI", "Datasets", "Evaluation"], "tsne_embedding": [-47.119354248046875, -15.387134552001953], "cluster": 5}, {"key": "gattupalli2018weakly", "year": "2019", "citations": "42", "title": "Weakly Supervised Deep Image Hashing Through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised\nlearning problems that utilize images and label information to learn the binary\nhash codes. However, large-scale labeled image data is expensive to obtain,\nthus imposing a restriction on the usage of such algorithms. On the other hand,\nunlabelled image data is abundant due to the existence of many Web image\nrepositories. Such Web images may often come with images tags that contain\nuseful information, although raw tags, in general, do not readily lead to\nsemantic labels. Motivated by this scenario, we formulate the problem of\nsemantic image hashing as a weakly-supervised learning problem. We utilize the\ninformation contained in the user-generated tags associated with the images to\nlearn the hash codes. More specifically, we extract the word2vec semantic\nembeddings of the tags and use the information contained in them for\nconstraining the learning. Accordingly, we name our model Weakly Supervised\nDeep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of\nsemantic image retrieval and is compared against several state-of-art models.\nResults show that our approach sets a new state-of-art in the area of weekly\nsupervised image hashing.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Scalability", "Image-Retrieval", "Supervised", "Neural-Hashing"], "tsne_embedding": [6.579267978668213, -7.659816741943359], "cluster": 6}, {"key": "gattupalli2019weakly", "year": "2019", "citations": "42", "title": "Weakly Supervised Deep Image Hashing Through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.\nMotivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.\nAccordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Scalability", "Image-Retrieval", "Supervised", "Neural-Hashing"], "tsne_embedding": [6.579506874084473, -7.659499168395996], "cluster": 6}, {"key": "gattupalli2025weakly", "year": "2019", "citations": "42", "title": "Weakly Supervised Deep Image Hashing Through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.\nMotivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.\nAccordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Scalability", "Image-Retrieval", "Supervised", "Neural-Hashing"], "tsne_embedding": [6.579471111297607, -7.659450054168701], "cluster": 6}, {"key": "ge2014graph", "year": "2014", "citations": "69", "title": "Graph Cuts For Supervised Binary Coding", "abstract": "<p>Learning short binary codes is challenged by the inherent discrete\nnature of the problem. The graph cuts algorithm is a well-studied\ndiscrete label assignment solution in computer vision, but has not yet\nbeen applied to solve the binary coding problems. This is partially because\nit was unclear how to use it to learn the encoding (hashing) functions\nfor out-of-sample generalization. In this paper, we formulate supervised\nbinary coding as a single optimization problem that involves both\nthe encoding functions and the binary label assignment. Then we apply\nthe graph cuts algorithm to address the discrete optimization problem\ninvolved, with no continuous relaxation. This method, named as Graph\nCuts Coding (GCC), shows competitive results in various datasets.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Supervised", "Datasets"], "tsne_embedding": [62.49699783325195, 7.492180347442627], "cluster": 9}, {"key": "ge2020self", "year": "2020", "citations": "107", "title": "Self-supervising Fine-grained Region Similarities For Large-scale Image Localization", "abstract": "<p>The task of large-scale retrieval-based image localization is to estimate the\ngeographical location of a query image by recognizing its nearest reference\nimages from a city-scale dataset. However, the general public benchmarks only\nprovide noisy GPS labels associated with the training images, which act as weak\nsupervisions for learning image-to-image similarities. Such label noise\nprevents deep neural networks from learning discriminative features for\naccurate localization. To tackle this challenge, we propose to self-supervise\nimage-to-region similarities in order to fully explore the potential of\ndifficult positive images alongside their sub-regions. The estimated\nimage-to-region similarities can serve as extra training supervision for\nimproving the network in generations, which could in turn gradually refine the\nfine-grained similarities to achieve optimal performance. Our proposed\nself-enhanced image-to-region similarity labels effectively deal with the\ntraining bottleneck in the state-of-the-art pipelines without any additional\nparameters or manual annotations in both training and inference. Our method\noutperforms state-of-the-arts on the standard localization benchmarks by\nnoticeable margins and shows excellent generalization capability on multiple\nimage retrieval datasets.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-31.05757713317871, -5.512063503265381], "cluster": 0}, {"key": "ge2021structured", "year": "2021", "citations": "43", "title": "Structured Multi-modal Feature Embedding And Alignment For Image-sentence Retrieval", "abstract": "<p>The current state-of-the-art image-sentence retrieval methods implicitly\nalign the visual-textual fragments, like regions in images and words in\nsentences, and adopt attention modules to highlight the relevance of\ncross-modal semantic correspondences. However, the retrieval performance\nremains unsatisfactory due to a lack of consistent representation in both\nsemantics and structural spaces. In this work, we propose to address the above\nissue from two aspects: (i) constructing intrinsic structure (along with\nrelations) among the fragments of respective modalities, e.g., \u201cdog \\(\\to\\) play\n\\(\\to\\) ball\u201d in semantic structure for an image, and (ii) seeking explicit\ninter-modal structural and semantic correspondence between the visual and\ntextual modalities. In this paper, we propose a novel Structured Multi-modal\nFeature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In\norder to jointly and explicitly learn the visual-textual embedding and the\ncross-modal alignment, SMFEA creates a novel multi-modal structured module with\na shared context-aware referral tree. In particular, the relations of the\nvisual and textual fragments are modeled by constructing Visual Context-aware\nStructured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree\nencoder (TCS-Tree) with shared labels, from which visual and textual features\ncan be jointly learned and optimized. We utilize the multi-modal tree structure\nto explicitly align the heterogeneous image-sentence data by maximizing the\nsemantic and structural similarity between corresponding inter-modal tree\nnodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks\ndemonstrate the superiority of the proposed model in comparison to the\nstate-of-the-art methods.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-33.53983688354492, -19.6552791595459], "cluster": 5}, {"key": "ge2022cross", "year": "2023", "citations": "29", "title": "Cross-modal Semantic Enhanced Interaction For Image-sentence Retrieval", "abstract": "<p>Image-sentence retrieval has attracted extensive research attention in\nmultimedia and computer vision due to its promising application. The key issue\nlies in jointly learning the visual and textual representation to accurately\nestimate their similarity. To this end, the mainstream schema adopts an\nobject-word based attention to calculate their relevance scores and refine\ntheir interactive representations with the attention features, which, however,\nneglects the context of the object representation on the inter-object\nrelationship that matches the predicates in sentences. In this paper, we\npropose a Cross-modal Semantic Enhanced Interaction method, termed CMSEI for\nimage-sentence retrieval, which correlates the intra- and inter-modal semantics\nbetween objects and words. In particular, we first design the intra-modal\nspatial and semantic graphs based reasoning to enhance the semantic\nrepresentations of objects guided by the explicit relationships of the objects\u2019\nspatial positions and their scene graph. Then the visual and textual semantic\nrepresentations are refined jointly via the inter-modal interactive attention\nand the cross-modal alignment. To correlate the context of objects with the\ntextual context, we further refine the visual semantic representation via the\ncross-level object-sentence and word-image based interactive attention.\nExperimental results on seven standard evaluation metrics show that the\nproposed CMSEI outperforms the state-of-the-art and the alternative approaches\non MS-COCO and Flickr30K benchmarks.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-30.871021270751953, -22.474512100219727], "cluster": 5}, {"key": "ge2022miles", "year": "2022", "citations": "26", "title": "MILES: Visual BERT Pre-training With Injected Language Semantics For Video-text Retrieval", "abstract": "<p>Dominant pre-training work for video-text retrieval mainly adopt the\n\u201cdual-encoder\u201d architectures to enable efficient retrieval, where two separate\nencoders are used to contrast global video and text representations, but ignore\ndetailed local semantics. The recent success of image BERT pre-training with\nmasked visual modeling that promotes the learning of local visual context,\nmotivates a possible solution to address the above limitation. In this work, we\nfor the first time investigate masked visual modeling in video-text\npre-training with the \u201cdual-encoder\u201d architecture. We perform Masked visual\nmodeling with Injected LanguagE Semantics (MILES) by employing an extra\nsnapshot video encoder as an evolving \u201ctokenizer\u201d to produce reconstruction\ntargets for masked video patch prediction. Given the corrupted video, the video\nencoder is trained to recover text-aligned features of the masked patches via\nreasoning with the visible regions along the spatial and temporal dimensions,\nwhich enhances the discriminativeness of local visual features and the\nfine-grained cross-modality alignment. Our method outperforms state-of-the-art\nmethods for text-to-video retrieval on four datasets with both zero-shot and\nfine-tune evaluation protocols. Our approach also surpasses the baseline models\nsignificantly on zero-shot action recognition, which can be cast as\nvideo-to-text retrieval.</p>\n", "tags": ["Text-Retrieval", "Few-Shot-&-Zero-Shot", "Similarity-Search", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-16.20022964477539, 4.82976770401001], "cluster": 1}, {"key": "ge20243shnet", "year": "2024", "citations": "12", "title": "3shnet: Boosting Image-sentence Retrieval Via Visual Semantic-spatial Self-highlighting", "abstract": "<p>In this paper, we propose a novel visual Semantic-Spatial Self-Highlighting\nNetwork (termed 3SHNet) for high-precision, high-efficiency and\nhigh-generalization image-sentence retrieval. 3SHNet highlights the salient\nidentification of prominent objects and their spatial locations within the\nvisual modality, thus allowing the integration of visual semantics-spatial\ninteractions and maintaining independence between two modalities. This\nintegration effectively combines object regions with the corresponding semantic\nand position layouts derived from segmentation to enhance the visual\nrepresentation. And the modality-independence guarantees efficiency and\ngeneralization. Additionally, 3SHNet utilizes the structured contextual visual\nscene information from segmentation to conduct the local (region-based) or\nglobal (grid-based) guidance and achieve accurate hybrid-level retrieval.\nExtensive experiments conducted on MS-COCO and Flickr30K benchmarks\nsubstantiate the superior performances, inference efficiency and generalization\nof the proposed 3SHNet when juxtaposed with contemporary state-of-the-art\nmethodologies. Specifically, on the larger MS-COCO 5K test set, we achieve\n16.3%, 24.8%, and 18.3% improvements in terms of rSum score, respectively,\ncompared with the state-of-the-art methods using different image\nrepresentations, while maintaining optimal retrieval efficiency. Moreover, our\nperformance on cross-dataset generalization improves by 18.6%. Data and code\nare available at https://github.com/XuriGe1995/3SHNet.</p>\n", "tags": ["Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [-16.097673416137695, -0.2359180599451065], "cluster": 1}, {"key": "ge2025graph", "year": "2014", "citations": "69", "title": "Graph Cuts For Supervised Binary Coding", "abstract": "<p>Learning short binary codes is challenged by the inherent discrete\nnature of the problem. The graph cuts algorithm is a well-studied\ndiscrete label assignment solution in computer vision, but has not yet\nbeen applied to solve the binary coding problems. This is partially because\nit was unclear how to use it to learn the encoding (hashing) functions\nfor out-of-sample generalization. In this paper, we formulate supervised\nbinary coding as a single optimization problem that involves both\nthe encoding functions and the binary label assignment. Then we apply\nthe graph cuts algorithm to address the discrete optimization problem\ninvolved, with no continuous relaxation. This method, named as Graph\nCuts Coding (GCC), shows competitive results in various datasets.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Supervised", "Datasets"], "tsne_embedding": [62.49690628051758, 7.492050647735596], "cluster": 9}, {"key": "gebre2024pfeed", "year": "2024", "citations": "0", "title": "Pfeed: Generating Near Real-time Personalized Feeds Using Precomputed Embedding Similarities", "abstract": "<p>In personalized recommender systems, embeddings are often used to encode\ncustomer actions and items, and retrieval is then performed in the embedding\nspace using approximate nearest neighbor search. However, this approach can\nlead to two challenges: 1) user embeddings can restrict the diversity of\ninterests captured and 2) the need to keep them up-to-date requires an\nexpensive, real-time infrastructure. In this paper, we propose a method that\novercomes these challenges in a practical, industrial setting. The method\ndynamically updates customer profiles and composes a feed every two minutes,\nemploying precomputed embeddings and their respective similarities. We tested\nand deployed this method to personalise promotional items at Bol, one of the\nlargest e-commerce platforms of the Netherlands and Belgium. The method\nenhanced customer engagement and experience, leading to a significant 4.9%\nuplift in conversions.</p>\n", "tags": ["Efficiency", "Recommender-Systems"], "tsne_embedding": [-14.240118980407715, -46.99919891357422], "cluster": 3}, {"key": "gella2017image", "year": "2017", "citations": "72", "title": "Image Pivoting For Learning Multilingual Multimodal Representations", "abstract": "<p>In this paper we propose a model to learn multimodal multilingual\nrepresentations for matching images and sentences in different languages, with\nthe aim of advancing multilingual versions of image search and image\nunderstanding. Our model learns a common representation for images and their\ndescriptions in two different languages (which need not be parallel) by\nconsidering the image as a pivot between two languages. We introduce a new\npairwise ranking loss function which can handle both symmetric and asymmetric\nsimilarity between the two modalities. We evaluate our models on\nimage-description ranking for German and English, and on semantic textual\nsimilarity of image descriptions in English. In both cases we achieve\nstate-of-the-art performance.</p>\n", "tags": ["EMNLP", "Image-Retrieval", "Evaluation"], "tsne_embedding": [-26.924388885498047, -34.210880279541016], "cluster": 5}, {"key": "geng2018regularizing", "year": "2018", "citations": "0", "title": "Regularizing Deep Hashing Networks Using GAN Generated Fake Images", "abstract": "<p>Recently, deep-networks-based hashing (deep hashing) has become a leading\napproach for large-scale image retrieval. It aims to learn a compact bitwise\nrepresentation for images via deep networks, so that similar images are mapped\nto nearby hash codes. Since a deep network model usually has a large number of\nparameters, it may probably be too complicated for the training data we have,\nleading to model over-fitting. To address this issue, in this paper, we propose\na simple two-stage pipeline to learn deep hashing models, by regularizing the\ndeep hashing networks using fake images. The first stage is to generate fake\nimages from the original training set without extra data, via a generative\nadversarial network (GAN). In the second stage, we propose a deep architec-\nture to learn hash functions, in which we use a maximum-entropy based loss to\nincorporate the newly created fake images by the GAN. We show that this loss\nacts as a strong regularizer of the deep architecture, by penalizing\nlow-entropy output hash codes. This loss can also be interpreted as a model\nensemble by simultaneously training many network models with massive weight\nsharing but over different training sets. Empirical evaluation results on\nseveral benchmark datasets show that the proposed method has superior\nperformance gains over state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-0.3098279535770416, 10.3369779586792], "cluster": 8}, {"key": "gennaro2016large", "year": "2016", "citations": "1", "title": "Large Scale Deep Convolutional Neural Network Features Search With Lucene", "abstract": "<p>In this work, we propose an approach to index Deep Convolutional Neural\nNetwork Features to support efficient content-based retrieval on large image\ndatabases. To this aim, we have converted the these features into a textual\nform, to index them into an inverted index by means of Lucene. In this way, we\nwere able to set up a robust retrieval system that combines full-text search\nwith content-based image retrieval capabilities. We evaluated different\nstrategies of textual representation in order to optimize the index occupation\nand the query response time. In order to show that our approach is able to\nhandle large datasets, we have developed a web-based prototype that provides an\ninterface for combined textual and visual searching into a dataset of about 100\nmillion of images.</p>\n", "tags": ["Image-Retrieval", "Text-Retrieval", "Datasets"], "tsne_embedding": [-17.492595672607422, -38.293853759765625], "cluster": 3}, {"key": "gerritse2020graph", "year": "2020", "citations": "25", "title": "Graph-embedding Empowered Entity Retrieval", "abstract": "<p>In this research, we improve upon the current state of the art in entity\nretrieval by re-ranking the result list using graph embeddings. The paper shows\nthat graph embeddings are useful for entity-oriented search tasks. We\ndemonstrate empirically that encoding information from the knowledge graph into\n(graph) embeddings contributes to a higher increase in effectiveness of entity\nretrieval results than using plain word embeddings. We analyze the impact of\nthe accuracy of the entity linker on the overall retrieval effectiveness. Our\nanalysis further deploys the cluster hypothesis to explain the observed\nadvantages of graph embeddings over the more widely used word embeddings, for\nuser tasks involving ranking entities.</p>\n", "tags": ["Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [57.9239616394043, -5.09035587310791], "cluster": 9}, {"key": "gerritse2025graph", "year": "2020", "citations": "25", "title": "Graph-embedding Empowered Entity Retrieval", "abstract": "<p>In this research, we investigate methods for entity retrieval using graph embeddings. While various methods have been proposed over the years, most utilize a single graph embedding and entity linking approach. This hinders our understanding of how different graph embedding and entity linking methods impact entity retrieval. To address this gap, we investigate the effects of three different categories of graph embedding techniques and five different entity linking methods. We perform a reranking of entities using the distance between the embeddings of annotated entities and the entities we wish to rerank. We conclude that the selection of both graph embeddings and entity linkers significantly impacts the effectiveness of entity retrieval. For graph embeddings, methods that incorporate both graph structure and textual descriptions of entities are the most effective. For entity linking, both precision and recall concerning concepts are important for optimal retrieval performance. Additionally, it is essential for the graph to encompass as many entities as possible.</p>\n", "tags": ["Re-Ranking", "Evaluation"], "tsne_embedding": [57.47874450683594, -5.600897312164307], "cluster": 9}, {"key": "ghaemmaghami2022learning", "year": "2022", "citations": "0", "title": "Learning To Collide: Recommendation System Model Compression With Learned Hash Functions", "abstract": "<p>A key characteristic of deep recommendation models is the immense memory\nrequirements of their embedding tables. These embedding tables can often reach\nhundreds of gigabytes which increases hardware requirements and training cost.\nA common technique to reduce model size is to hash all of the categorical\nvariable identifiers (ids) into a smaller space. This hashing reduces the\nnumber of unique representations that must be stored in the embedding table;\nthus decreasing its size. However, this approach introduces collisions between\nsemantically dissimilar ids that degrade model quality. We introduce an\nalternative approach, Learned Hash Functions, which instead learns a new\nmapping function that encourages collisions between semantically similar ids.\nWe derive this learned mapping from historical data and embedding access\npatterns. We experiment with this technique on a production model and find that\na mapping informed by the combination of access frequency and a learned low\ndimension embedding is the most effective. We demonstrate a small improvement\nrelative to the hashing trick and other collision related compression\ntechniques. This is ongoing work that explores the impact of categorical id\ncollisions on recommendation model quality and how those collisions may be\ncontrolled to improve model performance.</p>\n", "tags": ["Evaluation", "Recommender-Systems", "Hashing-Methods"], "tsne_embedding": [29.285505294799805, 15.199055671691895], "cluster": 2}, {"key": "ghita2023class", "year": "2023", "citations": "1", "title": "Class Anchor Margin Loss For Content-based Image Retrieval", "abstract": "<p>The performance of neural networks in content-based image retrieval (CBIR) is\nhighly influenced by the chosen loss (objective) function. The majority of\nobjective functions for neural models can be divided into metric learning and\nstatistical learning. Metric learning approaches require a pair mining strategy\nthat often lacks efficiency, while statistical learning approaches are not\ngenerating highly compact features due to their indirect feature optimization.\nTo this end, we propose a novel repeller-attractor loss that falls in the\nmetric learning paradigm, yet directly optimizes for the L2 metric without the\nneed of generating pairs. Our loss is formed of three components. One leading\nobjective ensures that the learned features are attracted to each designated\nlearnable class anchor. The second loss component regulates the anchors and\nforces them to be separable by a margin, while the third objective ensures that\nthe anchors do not collapse to zero. Furthermore, we develop a more efficient\ntwo-stage retrieval system by harnessing the learned class anchors during the\nfirst stage of the retrieval process, eliminating the need of comparing the\nquery with every image in the database. We establish a set of four datasets\n(CIFAR-100, Food-101, SVHN, and Tiny ImageNet) and evaluate the proposed\nobjective in the context of few-shot and full-set training on the CBIR task, by\nusing both convolutional and transformer architectures. Compared to existing\nobjective functions, our empirical evidence shows that the proposed objective\nis generating superior and more consistent results.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [2.907562255859375, -11.270094871520996], "cluster": 1}, {"key": "giakkoupis2020cluster", "year": "2021", "citations": "2", "title": "Cluster-and-conquer: When Randomness Meets Graph Locality", "abstract": "<p>K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining\nand machine-learning applications. Some of the most efficient KNN graph\nalgorithms are incremental and local: they start from a random graph, which\nthey incrementally improve by traversing neighbors-of-neighbors links.\nParadoxically, this random start is also one of the key weaknesses of these\nalgorithms: nodes are initially connected to dissimilar neighbors, that lie far\naway according to the similarity metric. As a result, incremental algorithms\nmust first laboriously explore spurious potential neighbors before they can\nidentify similar nodes, and start converging. In this paper, we remove this\ndrawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts\nthe starting configuration of greedy algorithms thanks to a novel lightweight\nclustering mechanism, dubbed FastRandomHash. FastRandomHash leverages\nrandom-ness and recursion to pre-cluster similar nodes at a very low cost. Our\nextensive evaluation on real datasets shows that Cluster-and-Conquer\nsignificantly outperforms existing approaches, including LSH, yielding\nspeed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN\nquality.</p>\n", "tags": ["Evaluation", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [46.3853874206543, 6.758442401885986], "cluster": 9}, {"key": "gildenblat2019self", "year": "2019", "citations": "36", "title": "Self-supervised Similarity Learning For Digital Pathology", "abstract": "<p>Using features extracted from networks pretrained on ImageNet is a common\npractice in applications of deep learning for digital pathology. However it\npresents the downside of missing domain specific image information. In digital\npathology, supervised training data is expensive and difficult to collect. We\npropose a self-supervised method for feature extraction by similarity learning\non whole slide images (WSI) that is simple to implement and allows creation of\nrobust and compact image descriptors. We train a siamese network, exploiting\nimage spatial continuity and assuming spatially adjacent tiles in the image are\nmore similar to each other than distant tiles. Our network outputs feature\nvectors of length 128, which allows dramatically lower memory storage and\nfaster processing than networks pretrained on ImageNet. We apply the method on\ndigital pathology WSIs from the Camelyon16 train set and assess and compare our\nmethod by measuring image retrieval of tumor tiles and descriptor pair distance\nratio for distant/near tiles in the Camelyon16 test set. We show that our\nmethod yields better retrieval task results than existing ImageNet based and\ngeneric self-supervised feature extraction methods. To the best of our\nknowledge, this is also the first published method for self-supervised learning\ntailored for digital pathology.</p>\n", "tags": ["Supervised", "Self-Supervised", "Image-Retrieval"], "tsne_embedding": [-47.04669189453125, 16.689918518066406], "cluster": 0}, {"key": "gill2025advancing", "year": "2025", "citations": "0", "title": "Advancing Semantic Caching For Llms With Domain-specific Embeddings And Synthetic Data", "abstract": "<p>This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.</p>\n", "tags": ["Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [3.324932336807251, -20.972835540771484], "cluster": 7}, {"key": "gillick2018end", "year": "2018", "citations": "81", "title": "End-to-end Retrieval In Continuous Space", "abstract": "<p>Most text-based information retrieval (IR) systems index objects by words or\nphrases. These discrete systems have been augmented by models that use\nembeddings to measure similarity in continuous space. But continuous-space\nmodels are typically used just to re-rank the top candidates. We consider the\nproblem of end-to-end continuous retrieval, where standard approximate nearest\nneighbor (ANN) search replaces the usual discrete inverted index, and rely\nentirely on distances between learned embeddings. By training simple models\nspecifically for retrieval, with an appropriate model architecture, we improve\non a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval\ntasks. We also discuss the problem of evaluation for retrieval systems, and\nshow how to modify existing pairwise similarity datasets for this purpose.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-32.59302520751953, -25.998926162719727], "cluster": 5}, {"key": "gillick2019learning", "year": "2019", "citations": "189", "title": "Learning Dense Representations For Entity Retrieval", "abstract": "<p>We show that it is feasible to perform entity linking by training a dual\nencoder (two-tower) model that encodes mentions and entities in the same dense\nvector space, where candidate entities are retrieved by approximate nearest\nneighbor search. Unlike prior work, this setup does not rely on an alias table\nfollowed by a re-ranker, and is thus the first fully learned entity retrieval\nmodel. We show that our dual encoder, trained using only anchor-text links in\nWikipedia, outperforms discrete alias table and BM25 baselines, and is\ncompetitive with the best comparable results on the standard TACKBP-2010\ndataset. In addition, it can retrieve candidates extremely fast, and\ngeneralizes well to a new dataset derived from Wikinews. On the modeling side,\nwe demonstrate the dramatic value of an unsupervised negative mining algorithm\nfor this task.</p>\n", "tags": ["Unsupervised", "Datasets"], "tsne_embedding": [22.379493713378906, -14.814644813537598], "cluster": 7}, {"key": "gionis1999similarity", "year": "1999", "citations": "3205", "title": "Similarity Search In High Dimensions Via Hashing", "abstract": "<p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,\nall known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;\nin fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our\nmethod gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.\nExperimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Vector-Indexing", "Similarity-Search", "Tree-Based-Ann"], "tsne_embedding": [23.217933654785156, 31.484806060791016], "cluster": 4}, {"key": "gionis2025similarity", "year": "1999", "citations": "3205", "title": "Similarity Search In High Dimensions Via Hashing", "abstract": "<p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,\nall known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;\nin fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our\nmethod gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.\nExperimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Vector-Indexing", "Similarity-Search", "Tree-Based-Ann"], "tsne_embedding": [23.217975616455078, 31.484745025634766], "cluster": 4}, {"key": "giraud2019superpixel", "year": "2017", "citations": "6", "title": "Superpixel-based Color Transfer", "abstract": "<p>In this work, we propose a fast superpixel-based color transfer method (SCT)\nbetween two images. Superpixels enable to decrease the image dimension and to\nextract a reduced set of color candidates. We propose to use a fast approximate\nnearest neighbor matching algorithm in which we enforce the match diversity by\nlimiting the selection of the same superpixels. A fusion framework is designed\nto transfer the matched colors, and we demonstrate the improvement obtained\nover exact matching results. Finally, we show that SCT is visually competitive\ncompared to state-of-the-art methods.</p>\n", "tags": ["Tools-&-Libraries"], "tsne_embedding": [-23.30807113647461, 12.431279182434082], "cluster": 8}, {"key": "girdhar2023imagebind", "year": "2023", "citations": "332", "title": "Imagebind: One Embedding Space To Bind Them All", "abstract": "<p>We present ImageBind, an approach to learn a joint embedding across six\ndifferent modalities - images, text, audio, depth, thermal, and IMU data. We\nshow that all combinations of paired data are not necessary to train such a\njoint embedding, and only image-paired data is sufficient to bind the\nmodalities together. ImageBind can leverage recent large scale vision-language\nmodels, and extends their zero-shot capabilities to new modalities just by\nusing their natural pairing with images. It enables novel emergent applications\n\u2018out-of-the-box\u2019 including cross-modal retrieval, composing modalities with\narithmetic, cross-modal detection and generation. The emergent capabilities\nimprove with the strength of the image encoder and we set a new\nstate-of-the-art on emergent zero-shot recognition tasks across modalities,\noutperforming specialist supervised models. Finally, we show strong few-shot\nrecognition results outperforming prior work, and that ImageBind serves as a\nnew way to evaluate vision models for visual and non-visual tasks.</p>\n", "tags": ["Supervised", "Multimodal-Retrieval", "CVPR", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [-18.805788040161133, -20.05996322631836], "cluster": 5}, {"key": "gkelios2021investigating", "year": "2021", "citations": "30", "title": "Investigating The Vision Transformer Model For Image Retrieval Tasks", "abstract": "<p>This paper introduces a plug-and-play descriptor that can be effectively\nadopted for image retrieval tasks without prior initialization or preparation.\nThe description method utilizes the recently proposed Vision Transformer\nnetwork while it does not require any training data to adjust parameters. In\nimage retrieval tasks, the use of Handcrafted global and local descriptors has\nbeen very successfully replaced, over the last years, by the Convolutional\nNeural Networks (CNN)-based methods. However, the experimental evaluation\nconducted in this paper on several benchmarking datasets against 36\nstate-of-the-art descriptors from the literature demonstrates that a neural\nnetwork that contains no convolutional layer, such as Vision Transformer, can\nshape a global descriptor and achieve competitive results. As fine-tuning is\nnot required, the presented methodology\u2019s low complexity encourages adoption of\nthe architecture as an image retrieval baseline model, replacing the\ntraditional and well adopted CNN-based approaches and inaugurating a new era in\nimage retrieval approaches.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-18.001789093017578, 1.3437761068344116], "cluster": 1}, {"key": "godil2011retrieval", "year": "2006", "citations": "19", "title": "Retrieval And Clustering From A 3D Human Database Based On Body And Head Shape", "abstract": "<p>In this paper, we describe a framework for similarity based retrieval and\nclustering from a 3D human database. Our technique is based on both body and\nhead shape representation and the retrieval is based on similarity of both of\nthem. The 3D human database used in our study is the CAESAR anthropometric\ndatabase which contains approximately 5000 bodies. We have developed a\nweb-based interface for specifying the queries to interact with the retrieval\nsystem. Our approach performs the similarity based retrieval in a reasonable\namount of time and is a practical approach.</p>\n", "tags": ["Tools-&-Libraries"], "tsne_embedding": [-15.014673233032227, -3.9735896587371826], "cluster": 1}, {"key": "gomez2018learning", "year": "2019", "citations": "23", "title": "Learning To Learn From Web Data Through Deep Semantic Embeddings", "abstract": "<p>In this paper we propose to learn a multimodal image and text embedding from\nWeb and Social Media data, aiming to leverage the semantic knowledge learnt in\nthe text domain and transfer it to a visual model for semantic image retrieval.\nWe demonstrate that the pipeline can learn from images with associated text\nwithout supervision and perform a thourough analysis of five different text\nembeddings in three different benchmarks. We show that the embeddings learnt\nwith Web and Social Media data have competitive performances over supervised\nmethods in the text based image retrieval task, and we clearly outperform state\nof the art in the MIRFlickr dataset when training in the target data. Further\nwe demonstrate how semantic multimodal image retrieval can be performed using\nthe learnt embeddings, going beyond classical instance-level retrieval\nproblems. Finally, we present a new dataset, InstaCities1M, composed by\nInstagram images and their associated texts that can be used for fair\ncomparison of image-text embeddings.</p>\n", "tags": ["Supervised", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-8.592046737670898, -32.41556930541992], "cluster": 3}, {"key": "gomez2019self", "year": "2019", "citations": "13", "title": "Self-supervised Learning From Web Data For Multimodal Retrieval", "abstract": "<p>Self-Supervised learning from multimodal image and text data allows deep\nneural networks to learn powerful features with no need of human annotated\ndata. Web and Social Media platforms provide a virtually unlimited amount of\nthis multimodal data. In this work we propose to exploit this free available\ndata to learn a multimodal image and text embedding, aiming to leverage the\nsemantic knowledge learnt in the text domain and transfer it to a visual model\nfor semantic image retrieval. We demonstrate that the proposed pipeline can\nlearn from images with associated textwithout supervision and analyze the\nsemantic structure of the learnt joint image and text embedding space. We\nperform a thorough analysis and performance comparison of five different state\nof the art text embeddings in three different benchmarks. We show that the\nembeddings learnt with Web and Social Media data have competitive performances\nover supervised methods in the text based image retrieval task, and we clearly\noutperform state of the art in the MIRFlickr dataset when training in the\ntarget data. Further, we demonstrate how semantic multimodal image retrieval\ncan be performed using the learnt embeddings, going beyond classical\ninstance-level retrieval problems. Finally, we present a new dataset,\nInstaCities1M, composed by Instagram images and their associated texts that can\nbe used for fair comparison of image-text embeddings.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-8.53844165802002, -32.1103515625], "cluster": 3}, {"key": "gomez2020retrieval", "year": "2020", "citations": "8", "title": "Retrieval Guided Unsupervised Multi-domain Image-to-image Translation", "abstract": "<p>Image to image translation aims to learn a mapping that transforms an image\nfrom one visual domain to another. Recent works assume that images descriptors\ncan be disentangled into a domain-invariant content representation and a\ndomain-specific style representation. Thus, translation models seek to preserve\nthe content of source images while changing the style to a target visual\ndomain. However, synthesizing new images is extremely challenging especially in\nmulti-domain translations, as the network has to compose content and style to\ngenerate reliable and diverse images in multiple domains. In this paper we\npropose the use of an image retrieval system to assist the image-to-image\ntranslation task. First, we train an image-to-image translation model to map\nimages to multiple domains. Then, we train an image retrieval model using real\nand generated images to find images similar to a query one in content but in a\ndifferent domain. Finally, we exploit the image retrieval system to fine-tune\nthe image-to-image translation model and generate higher quality images. Our\nexperiments show the effectiveness of the proposed solution and highlight the\ncontribution of the retrieval network, which can benefit from additional\nunlabeled data and help image-to-image translation models in the presence of\nscarce data.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Unsupervised"], "tsne_embedding": [-23.004487991333008, -27.286916732788086], "cluster": 5}, {"key": "gominski2019challenging", "year": "2019", "citations": "7", "title": "Challenging Deep Image Descriptors For Retrieval In Heterogeneous Iconographic Collections", "abstract": "<p>This article proposes to study the behavior of recent and efficient\nstate-of-the-art deep-learning based image descriptors for content-based image\nretrieval, facing a panel of complex variations appearing in heterogeneous\nimage datasets, in particular in cultural collections that may involve\nmulti-source, multi-date and multi-view Permission to make digital</p>\n", "tags": ["Datasets"], "tsne_embedding": [-21.621644973754883, -27.090930938720703], "cluster": 5}, {"key": "goncalves2023geometric", "year": "2023", "citations": "0", "title": "Geometric Covering Using Random Fields", "abstract": "<p>A set of vectors \\(S \\subseteq \\mathbb{R}^d\\) is\n\\((k_1,\\epsilon)\\)-clusterable if there are \\(k_1\\) balls of radius\n\\(\\epsilon\\) that cover \\(S\\). A set of vectors \\(S \\subseteq \\mathbb{R}^d\\) is\n\\((k_2,\\delta)\\)-far from being clusterable if there are at least \\(k_2\\) vectors\nin \\(S\\), with all pairwise distances at least \\(\\delta\\). We propose a\nprobabilistic algorithm to distinguish between these two cases. Our algorithm\nreaches a decision by only looking at the extreme values of a scalar valued\nhash function, defined by a random field, on \\(S\\); hence, it is especially\nsuitable in distributed and online settings. An important feature of our method\nis that the algorithm is oblivious to the number of vectors: in the online\nsetting, for example, the algorithm stores only a constant number of scalars,\nwhich is independent of the stream length.\n  We introduce random field hash functions, which are a key ingredient in our\nparadigm. Random field hash functions generalize locality-sensitive hashing\n(LSH). In addition to the LSH requirement that <code class=\"language-plaintext highlighter-rouge\">nearby vectors are hashed to\nsimilar values\", our hash function also guarantees that the</code>hash values are\n(nearly) independent random variables for distant vectors\u201d. We formulate\nnecessary conditions for the kernels which define the random fields applied to\nour problem, as well as a measure of kernel optimality, for which we provide a\nbound. Then, we propose a method to construct kernels which approximate the\noptimal one.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [24.53057861328125, 47.71351623535156], "cluster": 4}, {"key": "gong2011iterative", "year": "2011", "citations": "1094", "title": "Iterative Quantization: A Procrustean Approach To Learning Binary Codes", "abstract": "<p>This paper addresses the problem of learning similarity preserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zerocentered data so as to minimize the quantization error of\nmapping this data to the vertices of a zero-centered binary\nhypercube. This method, dubbed iterative quantization\n(ITQ), has connections to multi-class spectral clustering\nand to the orthogonal Procrustes problem, and it can be\nused both with unsupervised data embeddings such as PCA\nand supervised embeddings such as canonical correlation\nanalysis (CCA). Our experiments show that the resulting\nbinary coding schemes decisively outperform several other\nstate-of-the-art methods.</p>\n", "tags": ["Compact-Codes", "Quantization", "CVPR", "Scalability", "Similarity-Search", "Supervised", "Unsupervised"], "tsne_embedding": [-13.469077110290527, 22.617042541503906], "cluster": 8}, {"key": "gong2013learning", "year": "2013", "citations": "183", "title": "Learning Binary Codes For High-dimensional Data Using Bilinear Projections", "abstract": "<p>Recent advances in visual recognition indicate that to\nachieve good retrieval and classification accuracy on largescale\ndatasets like ImageNet, extremely high-dimensional\nvisual descriptors, e.g., Fisher Vectors, are needed. We\npresent a novel method for converting such descriptors to\ncompact similarity-preserving binary codes that exploits\ntheir natural matrix structure to reduce their dimensionality\nusing compact bilinear projections instead of a single\nlarge projection matrix. This method achieves comparable\nretrieval and classification accuracy to the original descriptors\nand to the state-of-the-art Product Quantization\napproach while having orders of magnitude faster code generation\ntime and smaller memory footprint.</p>\n", "tags": ["Quantization", "CVPR", "Memory-Efficiency", "Datasets", "Compact-Codes"], "tsne_embedding": [-23.105026245117188, 21.2233943939209], "cluster": 8}, {"key": "gong2022improving", "year": "2022", "citations": "13", "title": "Improving Visual-semantic Embeddings By Learning Semantically-enhanced Hard Negatives For Cross-modal Information Retrieval", "abstract": "<p>Visual Semantic Embedding (VSE) aims to extract the semantics of images and\ntheir descriptions, and embed them into the same latent space for cross-modal\ninformation retrieval. Most existing VSE networks are trained by adopting a\nhard negatives loss function which learns an objective margin between the\nsimilarity of relevant and irrelevant image-description embedding pairs.\nHowever, the objective margin in the hard negatives loss function is set as a\nfixed hyperparameter that ignores the semantic differences of the irrelevant\nimage-description pairs. To address the challenge of measuring the optimal\nsimilarities between image-description pairs before obtaining the trained VSE\nnetworks, this paper presents a novel approach that comprises two main parts:\n(1) finds the underlying semantics of image descriptions; and (2) proposes a\nnovel semantically enhanced hard negatives loss function, where the learning\nobjective is dynamically determined based on the optimal similarity scores\nbetween irrelevant image-description pairs. Extensive experiments were carried\nout by integrating the proposed methods into five state-of-the-art VSE networks\nthat were applied to three benchmark datasets for cross-modal information\nretrieval tasks. The results revealed that the proposed methods achieved the\nbest performance and can also be adopted by existing and future VSE networks.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation"], "tsne_embedding": [-29.132110595703125, -19.643817901611328], "cluster": 5}, {"key": "gong2022vit2hash", "year": "2022", "citations": "1", "title": "Vit2hash: Unsupervised Information-preserving Hashing", "abstract": "<p>Unsupervised image hashing, which maps images into binary codes without\nsupervision, is a compressor with a high compression rate. Hence, how to\npreserving meaningful information of the original data is a critical problem.\nInspired by the large-scale vision pre-training model, known as ViT, which has\nshown significant progress for learning visual representations, in this paper,\nwe propose a simple information-preserving compressor to finetune the ViT model\nfor the target unsupervised hashing task. Specifically, from pixels to\ncontinuous features, we first propose a feature-preserving module, using the\ncorrupted image as input to reconstruct the original feature from the\npre-trained ViT model and the complete image, so that the feature extractor can\nfocus on preserving the meaningful information of original data. Secondly, from\ncontinuous features to hash codes, we propose a hashing-preserving module,\nwhich aims to keep the semantic information from the pre-trained ViT model by\nusing the proposed Kullback-Leibler divergence loss. Besides, the quantization\nloss and the similarity loss are added to minimize the quantization error. Our\nmethod is very simple and achieves a significantly higher degree of MAP on\nthree benchmark image datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-14.562758445739746, 15.09506607055664], "cluster": 8}, {"key": "gong2025iterative", "year": "2011", "citations": "1094", "title": "Iterative Quantization: A Procrustean Approach To Learning Binary Codes", "abstract": "<p>This paper addresses the problem of learning similarity preserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zerocentered data so as to minimize the quantization error of\nmapping this data to the vertices of a zero-centered binary\nhypercube. This method, dubbed iterative quantization\n(ITQ), has connections to multi-class spectral clustering\nand to the orthogonal Procrustes problem, and it can be\nused both with unsupervised data embeddings such as PCA\nand supervised embeddings such as canonical correlation\nanalysis (CCA). Our experiments show that the resulting\nbinary coding schemes decisively outperform several other\nstate-of-the-art methods.</p>\n", "tags": ["Compact-Codes", "Quantization", "CVPR", "Scalability", "Similarity-Search", "Supervised", "Unsupervised"], "tsne_embedding": [-13.469082832336426, 22.61676597595215], "cluster": 8}, {"key": "gong2025learning", "year": "2013", "citations": "183", "title": "Learning Binary Codes For High-dimensional Data Using Bilinear Projections", "abstract": "<p>Recent advances in visual recognition indicate that to\nachieve good retrieval and classification accuracy on largescale\ndatasets like ImageNet, extremely high-dimensional\nvisual descriptors, e.g., Fisher Vectors, are needed. We\npresent a novel method for converting such descriptors to\ncompact similarity-preserving binary codes that exploits\ntheir natural matrix structure to reduce their dimensionality\nusing compact bilinear projections instead of a single\nlarge projection matrix. This method achieves comparable\nretrieval and classification accuracy to the original descriptors\nand to the state-of-the-art Product Quantization\napproach while having orders of magnitude faster code generation\ntime and smaller memory footprint.</p>\n", "tags": ["Quantization", "CVPR", "Memory-Efficiency", "Datasets", "Compact-Codes"], "tsne_embedding": [-23.104751586914062, 21.223636627197266], "cluster": 8}, {"key": "gordo2016deep", "year": "2016", "citations": "753", "title": "Deep Image Retrieval: Learning Global Representations For Image Search", "abstract": "<p>We propose a novel approach for instance-level image retrieval. It produces a\nglobal and compact fixed-length representation for each image by aggregating\nmany region-wise descriptors. In contrast to previous works employing\npre-trained deep networks as a black box to produce features, our method\nleverages a deep architecture trained for the specific task of image retrieval.\nOur contribution is twofold: (i) we leverage a ranking framework to learn\nconvolution and projection weights that are used to build the region features;\nand (ii) we employ a region proposal network to learn which regions should be\npooled to form the final global descriptor. We show that using clean training\ndata is key to the success of our approach. To that aim, we use a large scale\nbut noisy landmark dataset and develop an automatic cleaning approach. The\nproposed architecture produces a global image representation in a single\nforward pass. Our approach significantly outperforms previous approaches based\non global descriptors on standard datasets. It even surpasses most prior works\nbased on costly local descriptor indexing and spatial verification. Additional\nmaterial is available at www.xrce.xerox.com/Deep-Image-Retrieval.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-41.15776824951172, -4.001343250274658], "cluster": 0}, {"key": "gordo2016end", "year": "2017", "citations": "532", "title": "End-to-end Learning Of Deep Visual Representations For Image Retrieval", "abstract": "<p>While deep learning has become a key ingredient in the top performing methods\nfor many computer vision tasks, it has failed so far to bring similar\nimprovements to instance-level image retrieval. In this article, we argue that\nreasons for the underwhelming results of deep methods on image retrieval are\nthreefold: i) noisy training data, ii) inappropriate deep architecture, and\niii) suboptimal training procedure. We address all three issues.\n  First, we leverage a large-scale but noisy landmark dataset and develop an\nautomatic cleaning method that produces a suitable training set for deep\nretrieval. Second, we build on the recent R-MAC descriptor, show that it can be\ninterpreted as a deep and differentiable architecture, and present improvements\nto enhance it. Last, we train this network with a siamese architecture that\ncombines three streams with a triplet loss. At the end of the training process,\nthe proposed architecture produces a global image representation in a single\nforward pass that is well suited for image retrieval. Extensive experiments\nshow that our approach significantly outperforms previous retrieval approaches,\nincluding state-of-the-art methods based on costly local descriptor indexing\nand spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively\nreport 94.7, 96.6, and 94.8 mean average precision. Our representations can\nalso be heavily compressed using product quantization with little loss in\naccuracy. For additional material, please see\nwww.xrce.xerox.com/Deep-Image-Retrieval.</p>\n", "tags": ["Distance-Metric-Learning", "Quantization", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-42.06403350830078, -4.843114376068115], "cluster": 0}, {"key": "gottesb\u00fcren2024unleashing", "year": "2024", "citations": "1", "title": "Unleashing Graph Partitioning For Large-scale Nearest Neighbor Search", "abstract": "<p>We consider the fundamental problem of decomposing a large-scale approximate\nnearest neighbor search (ANNS) problem into smaller sub-problems. The goal is\nto partition the input points into neighborhood-preserving shards, so that the\nnearest neighbors of any point are contained in only a few shards. When a query\narrives, a routing algorithm is used to identify the shards which should be\nsearched for its nearest neighbors. This approach forms the backbone of\ndistributed ANNS, where the dataset is so large that it must be split across\nmultiple machines.\n  In this paper, we design simple and highly efficient routing methods, and\nprove strong theoretical guarantees on their performance. A crucial\ncharacteristic of our routing algorithms is that they are inherently modular,\nand can be used with any partitioning method. This addresses a key drawback of\nprior approaches, where the routing algorithms are inextricably linked to their\nassociated partitioning method. In particular, our new routing methods enable\nthe use of balanced graph partitioning, which is a high-quality partitioning\nmethod without a naturally associated routing algorithm. Thus, we provide the\nfirst methods for routing using balanced graph partitioning that are extremely\nfast to train, admit low latency, and achieve high recall. We provide a\ncomprehensive evaluation of our full partitioning and routing pipeline on\nbillion-scale datasets, where it outperforms existing scalable partitioning\nmethods by significant margins, achieving up to 2.14x higher QPS at 90%\nrecall\\(@10\\) than the best competitor.</p>\n", "tags": ["Efficiency", "Scalability", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [44.80120086669922, 12.694648742675781], "cluster": 9}, {"key": "grauman2012learning", "year": "2012", "citations": "102", "title": "Learning Binary Hash Codes For Large-scale Image Search", "abstract": "<p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>\n\n<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Compact-Codes", "Image-Retrieval", "Scalability", "Large-Scale-Search", "Supervised", "Unsupervised"], "tsne_embedding": [-4.078320503234863, -11.815852165222168], "cluster": 1}, {"key": "grauman2025learning", "year": "2012", "citations": "102", "title": "Learning Binary Hash Codes For Large-scale Image Search", "abstract": "<p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>\n\n<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Compact-Codes", "Image-Retrieval", "Scalability", "Large-Scale-Search", "Supervised", "Unsupervised"], "tsne_embedding": [-4.0786051750183105, -11.815855979919434], "cluster": 1}, {"key": "gripon2016associative", "year": "2018", "citations": "9", "title": "Associative Memories To Accelerate Approximate Nearest Neighbor Search", "abstract": "<p>Nearest neighbor search is a very active field in machine learning for it\nappears in many application cases, including classification and object\nretrieval. In its canonical version, the complexity of the search is linear\nwith both the dimension and the cardinal of the collection of vectors the\nsearch is performed in. Recently many works have focused on reducing the\ndimension of vectors using quantization techniques or hashing, while providing\nan approximate result. In this paper we focus instead on tackling the cardinal\nof the collection of vectors. Namely, we introduce a technique that partitions\nthe collection of vectors and stores each part in its own associative memory.\nWhen a query vector is given to the system, associative memories are polled to\nidentify which one contain the closest match. Then an exhaustive search is\nconducted only on the part of vectors stored in the selected associative\nmemory. We study the effectiveness of the system when messages to store are\ngenerated from i.i.d. uniform \\(\\pm\\)1 random variables or 0-1 sparse i.i.d.\nrandom variables. We also conduct experiment on both synthetic data and real\ndata and show it is possible to achieve interesting trade-offs between\ncomplexity and accuracy.</p>\n", "tags": ["Quantization", "Hashing-Methods"], "tsne_embedding": [27.960948944091797, 12.470338821411133], "cluster": 2}, {"key": "groh2019ggnn", "year": "2022", "citations": "34", "title": "GGNN: Graph-based GPU Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor (ANN) search in high dimensions is an integral\npart of several computer vision systems and gains importance in deep learning\nwith explicit memory representations. Since PQT, FAISS, and SONG started to\nleverage the massive parallelism offered by GPUs, GPU-based implementations are\na crucial resource for today\u2019s state-of-the-art ANN methods. While most of\nthese methods allow for faster queries, less emphasis is devoted to\naccelerating the construction of the underlying index structures. In this\npaper, we propose a novel GPU-friendly search structure based on nearest\nneighbor graphs and information propagation on graphs. Our method is designed\nto take advantage of GPU architectures to accelerate the hierarchical\nconstruction of the index structure and for performing the query. Empirical\nevaluation shows that GGNN significantly surpasses the state-of-the-art CPU-\nand GPU-based systems in terms of build-time, accuracy and search speed.</p>\n", "tags": ["Graph-Based-Ann", "Vector-Indexing", "Evaluation", "Tools-&-Libraries"], "tsne_embedding": [46.34844207763672, 16.93394660949707], "cluster": 9}, {"key": "grover2022contextclip", "year": "2022", "citations": "2", "title": "Contextclip: Contextual Alignment Of Image-text Pairs On CLIP Visual Representations", "abstract": "<p>State-of-the-art empirical work has shown that visual representations learned\nby deep neural networks are robust in nature and capable of performing\nclassification tasks on diverse datasets. For example, CLIP demonstrated\nzero-shot transfer performance on multiple datasets for classification tasks in\na joint embedding space of image and text pairs. However, it showed negative\ntransfer performance on standard datasets, e.g., BirdsNAP, RESISC45, and MNIST.\nIn this paper, we propose ContextCLIP, a contextual and contrastive learning\nframework for the contextual alignment of image-text pairs by learning robust\nvisual representations on Conceptual Captions dataset. Our framework was\nobserved to improve the image-text alignment by aligning text and image\nrepresentations contextually in the joint embedding space. ContextCLIP showed\ngood qualitative performance for text-to-image retrieval tasks and enhanced\nclassification accuracy. We evaluated our model quantitatively with zero-shot\ntransfer and fine-tuning experiments on CIFAR-10, CIFAR-100, Birdsnap,\nRESISC45, and MNIST datasets for classification task.</p>\n", "tags": ["Self-Supervised", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-26.460693359375, -21.402738571166992], "cluster": 5}, {"key": "grzegorczyk2016binary", "year": "2017", "citations": "5", "title": "Binary Paragraph Vectors", "abstract": "<p>Recently Le &amp; Mikolov described two log-linear models, called Paragraph\nVector, that can be used to learn state-of-the-art distributed representations\nof documents. Inspired by this work, we present Binary Paragraph Vector models:\nsimple neural networks that learn short binary codes for fast information\nretrieval. We show that binary paragraph vectors outperform autoencoder-based\nbinary codes, despite using fewer bits. We also evaluate their precision in\ntransfer learning settings, where binary codes are inferred for documents\nunrelated to the training corpus. Results from these experiments indicate that\nbinary paragraph vectors can capture semantics relevant for various\ndomain-specific documents. Finally, we present a model that simultaneously\nlearns short binary codes and longer, real-valued representations. This model\ncan be used to rapidly retrieve a short list of highly relevant documents from\na large document collection.</p>\n", "tags": ["Compact-Codes", "Evaluation"], "tsne_embedding": [13.309652328491211, -17.1374568939209], "cluster": 7}, {"key": "gu2018attention", "year": "2018", "citations": "28", "title": "Attention-aware Generalized Mean Pooling For Image Retrieval", "abstract": "<p>It has been shown that image descriptors extracted by convolutional neural\nnetworks (CNNs) achieve remarkable results for retrieval problems. In this\npaper, we apply attention mechanism to CNN, which aims at enhancing more\nrelevant features that correspond to important keypoints in the input image.\nThe generated attention-aware features are then aggregated by the previous\nstate-of-the-art generalized mean (GeM) pooling followed by normalization to\nproduce a compact global descriptor, which can be efficiently compared to other\nimage descriptors by the dot product. An extensive comparison of our proposed\napproach with state-of-the-art methods is performed on the new challenging\nROxford5k and RParis6k retrieval benchmarks. Results indicate significant\nimprovement over previous work. In particular, our attention-aware GeM (AGeM)\ndescriptor outperforms state-of-the-art method on ROxford5k under the `Hard\u2019\nevaluation protocal.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-50.64418029785156, 1.5491182804107666], "cluster": 0}, {"key": "gu2020symmetrical", "year": "2020", "citations": "21", "title": "Symmetrical Synthesis For Deep Metric Learning", "abstract": "<p>Deep metric learning aims to learn embeddings that contain semantic\nsimilarity information among data points. To learn better embeddings, methods\nto generate synthetic hard samples have been proposed. Existing methods of\nsynthetic hard sample generation are adopting autoencoders or generative\nadversarial networks, but this leads to more hyper-parameters, harder\noptimization, and slower training speed. In this paper, we address these\nproblems by proposing a novel method of synthetic hard sample generation called\nsymmetrical synthesis. Given two original feature points from the same class,\nthe proposed method firstly generates synthetic points with each other as an\naxis of symmetry. Secondly, it performs hard negative pair mining within the\noriginal and synthetic points to select a more informative negative pair for\ncomputing the metric learning loss. Our proposed method is hyper-parameter free\nand plug-and-play for existing metric learning losses without network\nmodification. We demonstrate the superiority of our proposed method over\nexisting methods for a variety of loss functions on clustering and image\nretrieval tasks. Our implementations is publicly available.</p>\n", "tags": ["AAAI", "Distance-Metric-Learning", "Robustness"], "tsne_embedding": [-19.612491607666016, -13.19718074798584], "cluster": 1}, {"key": "gu2021cross", "year": "2022", "citations": "13", "title": "Cross-modal Image Retrieval With Deep Mutual Information Maximization", "abstract": "<p>In this paper, we study the cross-modal image retrieval, where the inputs\ncontain a source image plus some text that describes certain modifications to\nthis image and the desired image. Prior work usually uses a three-stage\nstrategy to tackle this task: 1) extract the features of the inputs; 2) fuse\nthe feature of the source image and its modified text to obtain fusion feature;\n3) learn a similarity metric between the desired image and the source image +\nmodified text by using deep metric learning. Since classical image/text\nencoders can learn the useful representation and common pair-based loss\nfunctions of distance metric learning are enough for cross-modal retrieval,\npeople usually improve retrieval accuracy by designing new fusion networks.\nHowever, these methods do not successfully handle the modality gap caused by\nthe inconsistent distribution and representation of the features of different\nmodalities, which greatly influences the feature fusion and similarity\nlearning. To alleviate this problem, we adopt the contrastive self-supervised\nlearning method Deep InforMax (DIM) to our approach to bridge this gap by\nenhancing the dependence between the text, the image, and their fusion.\nSpecifically, our method narrows the modality gap between the text modality and\nthe image modality by maximizing mutual information between their not exactly\nsemantically identical representation. Moreover, we seek an effective common\nsubspace for the semantically same fusion feature and desired image\u2019s feature\nby utilizing Deep InforMax between the low-level layer of the image encoder and\nthe high-level layer of the fusion network. Extensive experiments on three\nlarge-scale benchmark datasets show that we have bridged the modality gap\nbetween different modalities and achieve state-of-the-art retrieval\nperformance.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Scalability", "Image-Retrieval", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-24.87734603881836, -10.77474308013916], "cluster": 1}, {"key": "gu2021local", "year": "2022", "citations": "14", "title": "Local Citation Recommendation With Hierarchical-attention Text Encoder And Scibert-based Reranking", "abstract": "<p>The goal of local citation recommendation is to recommend a missing reference\nfrom the local citation context and optionally also from the global context. To\nbalance the tradeoff between speed and accuracy of citation recommendation in\nthe context of a large-scale paper database, a viable approach is to first\nprefetch a limited number of relevant documents using efficient ranking methods\nand then to perform a fine-grained reranking using more sophisticated models.\nIn that vein, BM25 has been found to be a tough-to-beat approach to\nprefetching, which is why recent work has focused mainly on the reranking step.\nEven so, we explore prefetching with nearest neighbor search among text\nembeddings constructed by a hierarchical attention network. When coupled with a\nSciBERT reranker fine-tuned on local citation recommendation tasks, our\nhierarchical Attention encoder (HAtten) achieves high prefetch recall for a\ngiven number of candidates to be reranked. Consequently, our reranker requires\nfewer prefetch candidates to rerank, yet still achieves state-of-the-art\nperformance on various local citation recommendation datasets such as ACL-200,\nFullTextPeerRead, RefSeer, and arXiv.</p>\n", "tags": ["Recommender-Systems", "Scalability", "Datasets", "Re-Ranking", "Evaluation"], "tsne_embedding": [19.57673454284668, 23.25654411315918], "cluster": 2}, {"key": "gu2021multimodal", "year": "2021", "citations": "32", "title": "Multimodal Representation For Neural Code Search", "abstract": "<p>Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of code\nsearch. Last, we define intuitive quantification metrics oriented to the\ncompleteness of semantic and syntactic information of the code data, to help\nunderstand the experimental findings.</p>\n", "tags": ["Evaluation", "Scalability"], "tsne_embedding": [19.486492156982422, -17.36974334716797], "cluster": 7}, {"key": "gu2022accelerating", "year": "2022", "citations": "10", "title": "Accelerating Code Search With Deep Hashing And Code Classification", "abstract": "<p>Code search is to search reusable code snippets from source code corpus based\non natural languages queries. Deep learning-based methods of code search have\nshown promising results. However, previous methods focus on retrieval accuracy\nbut lacked attention to the efficiency of the retrieval process. We propose a\nnovel method CoSHC to accelerate code search with deep hashing and code\nclassification, aiming to perform an efficient code search without sacrificing\ntoo much accuracy. To evaluate the effectiveness of CoSHC, we apply our method\nto five code search models. Extensive experimental results indicate that\ncompared with previous code search baselines, CoSHC can save more than 90% of\nretrieval time meanwhile preserving at least 99% of retrieval accuracy.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency"], "tsne_embedding": [2.8543903827667236, -35.94982147216797], "cluster": 3}, {"key": "gu2022tevis", "year": "2023", "citations": "1", "title": "Tevis:translating Text Synopses To Video Storyboards", "abstract": "<p>A video storyboard is a roadmap for video creation which consists of\nshot-by-shot images to visualize key plots in a text synopsis. Creating video\nstoryboards, however, remains challenging which not only requires cross-modal\nassociation between high-level texts and images but also demands long-term\nreasoning to make transitions smooth across shots. In this paper, we propose a\nnew task called Text synopsis to Video Storyboard (TeViS) which aims to\nretrieve an ordered sequence of images as the video storyboard to visualize the\ntext synopsis. We construct a MovieNet-TeViS dataset based on the public\nMovieNet dataset. It contains 10K text synopses each paired with keyframes\nmanually selected from corresponding movies by considering both relevance and\ncinematic coherence. To benchmark the task, we present strong CLIP-based\nbaselines and a novel VQ-Trans. VQ-Trans first encodes text synopsis and images\ninto a joint embedding space and uses vector quantization (VQ) to improve the\nvisual representation. Then, it auto-regressively generates a sequence of\nvisual features for retrieval and ordering. Experimental results demonstrate\nthat VQ-Trans significantly outperforms prior methods and the CLIP-based\nbaselines. Nevertheless, there is still a large gap compared to human\nperformance suggesting room for promising future work. The code and data are\navailable at: https://ruc-aimind.github.io/projects/TeViS/</p>\n", "tags": ["Quantization", "Evaluation", "Datasets"], "tsne_embedding": [-36.03324890136719, -32.57094192504883], "cluster": 5}, {"key": "guan2019post", "year": "2019", "citations": "16", "title": "Post-training 4-bit Quantization On Embedding Tables", "abstract": "<p>Continuous representations have been widely adopted in recommender systems\nwhere a large number of entities are represented using embedding vectors. As\nthe cardinality of the entities increases, the embedding components can easily\ncontain millions of parameters and become the bottleneck in both storage and\ninference due to large memory consumption. This work focuses on post-training\n4-bit quantization on the continuous embeddings. We propose row-wise uniform\nquantization with greedy search and codebook-based quantization that\nconsistently outperforms state-of-the-art quantization approaches on reducing\naccuracy degradation. We deploy our uniform quantization technique on a\nproduction model in Facebook and demonstrate that it can reduce the model size\nto only 13.89% of the single-precision version while the model quality stays\nneutral.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Evaluation"], "tsne_embedding": [37.82670593261719, -2.145153045654297], "cluster": 9}, {"key": "guerrero2020cross", "year": "2020", "citations": "1", "title": "Cross-modal Retrieval And Synthesis (X-MRS): Closing The Modality Gap In Shared Representation Learning", "abstract": "<p>Computational food analysis (CFA) naturally requires multi-modal evidence of\na particular food, e.g., images, recipe text, etc. A key to making CFA possible\nis multi-modal shared representation learning, which aims to create a joint\nrepresentation of the multiple views (text and image) of the data. In this work\nwe propose a method for food domain cross-modal shared representation learning\nthat preserves the vast semantic richness present in the food data. Our\nproposed method employs an effective transformer-based multilingual recipe\nencoder coupled with a traditional image embedding architecture. Here, we\npropose the use of imperfect multilingual translations to effectively\nregularize the model while at the same time adding functionality across\nmultiple languages and alphabets. Experimental analysis on the public Recipe1M\ndataset shows that the representation learned via the proposed method\nsignificantly outperforms the current state-of-the-arts (SOTA) on retrieval\ntasks. Furthermore, the representational power of the learned representation is\ndemonstrated through a generative food image synthesis model conditioned on\nrecipe embeddings. Synthesized images can effectively reproduce the visual\nappearance of paired samples, indicating that the learned representation\ncaptures the joint semantics of both the textual recipe and its visual content,\nthus narrowing the modality gap.</p>\n", "tags": ["Multimodal-Retrieval", "Datasets"], "tsne_embedding": [-41.450172424316406, 30.679044723510742], "cluster": 0}, {"key": "gui2019fast", "year": "2017", "citations": "265", "title": "Fast Supervised Discrete Hashing", "abstract": "<p>Learning-based hashing algorithms are <code class=\"language-plaintext highlighter-rouge\">hot topics\" because they can greatly\nincrease the scale at which existing methods operate. In this paper, we propose\na new learning-based hashing method called</code>fast supervised discrete hashing\u201d\n(FSDH) based on ``supervised discrete hashing\u201d (SDH). Regressing the training\nexamples (or hash code) to the corresponding class labels is widely used in\nordinary least squares regression. Rather than adopting this method, FSDH uses\na very simple yet effective regression of the class labels of training examples\nto the corresponding hash code to accelerate the algorithm. To the best of our\nknowledge, this strategy has not previously been used for hashing. Traditional\nSDH decomposes the optimization into three sub-problems, with the most critical\nsub-problem - discrete optimization for binary hash codes - solved using\niterative discrete cyclic coordinate descent (DCC), which is time-consuming.\nHowever, FSDH has a closed-form solution and only requires a single rather than\niterative hash code-solving step, which is highly efficient. Furthermore, FSDH\nis usually faster than SDH for solving the projection matrix for least squares\nregression, making FSDH generally faster than SDH. For example, our results\nshow that FSDH is about 12-times faster than SDH when the number of hashing\nbits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than\nFastHash when the number of hashing bits is 64 on the MNIST data-base. Our\nexperimental results show that FSDH is not only fast, but also outperforms\nother comparative methods.</p>\n", "tags": ["Supervised", "Hashing-Methods"], "tsne_embedding": [24.52252960205078, 2.461958169937134], "cluster": 6}, {"key": "gui2019supervised", "year": "2016", "citations": "92", "title": "Supervised Discrete Hashing With Relaxation", "abstract": "<p>Data-dependent hashing has recently attracted attention due to being able to\nsupport efficient retrieval and storage of high-dimensional data such as\ndocuments, images, and videos. In this paper, we propose a novel learning-based\nhashing method called \u201cSupervised Discrete Hashing with Relaxation\u201d (SDHR)\nbased on \u201cSupervised Discrete Hashing\u201d (SDH). SDH uses ordinary least squares\nregression and traditional zero-one matrix encoding of class label information\nas the regression target (code words), thus fixing the regression target. In\nSDHR, the regression target is instead optimized. The optimized regression\ntarget matrix satisfies a large margin constraint for correct classification of\neach example. Compared with SDH, which uses the traditional zero-one matrix,\nSDHR utilizes the learned regression target matrix and, therefore, more\naccurately measures the classification error of the regression model and is\nmore flexible. As expected, SDHR generally outperforms SDH. Experimental\nresults on two large-scale image datasets (CIFAR-10 and MNIST) and a\nlarge-scale and challenging face dataset (FRGC) demonstrate the effectiveness\nand efficiency of SDHR.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Similarity-Search", "Scalability", "Datasets", "Supervised"], "tsne_embedding": [10.630059242248535, 2.206364631652832], "cluster": 6}, {"key": "guo2019accelerating", "year": "2019", "citations": "91", "title": "Accelerating Large-scale Inference With Anisotropic Vector Quantization", "abstract": "<p>Quantization based techniques are the current state-of-the-art for scaling\nmaximum inner product search to massive databases. Traditional approaches to\nquantization aim to minimize the reconstruction error of the database points.\nBased on the observation that for a given query, the database points that have\nthe largest inner products are more relevant, we develop a family of\nanisotropic quantization loss functions. Under natural statistical assumptions,\nwe show that quantization with these loss functions leads to a new variant of\nvector quantization that more greatly penalizes the parallel component of a\ndatapoint\u2019s residual relative to its orthogonal component. The proposed\napproach achieves state-of-the-art results on the public benchmarks available\nat \\url{ann-benchmarks.com}.</p>\n", "tags": ["Quantization", "Scalability"], "tsne_embedding": [24.26264762878418, 19.403261184692383], "cluster": 2}, {"key": "guo2019hierarchical", "year": "2019", "citations": "22", "title": "Hierarchical Document Encoder For Parallel Corpus Mining", "abstract": "<p>We explore using multilingual document embeddings for nearest neighbor mining\nof parallel data. Three document-level representations are investigated: (i)\ndocument embeddings generated by simply averaging multilingual sentence\nembeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a\nhierarchical multilingual document encoder (HiDE) that builds on our\nsentence-level model. The results show document embeddings derived from\nsentence-level averaging are surprisingly effective for clean datasets, but\nsuggest models trained hierarchically at the document-level are more effective\non noisy data. Analysis experiments demonstrate our hierarchical models are\nvery robust to variations in the underlying sentence embedding quality. Using\ndocument embeddings trained with HiDE achieves state-of-the-art performance on\nUnited Nations (UN) parallel document mining, 94.9% P@1 for en-fr and 97.3% P@1\nfor en-es.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [3.6093459129333496, -13.33682632446289], "cluster": 7}, {"key": "guo2020deep", "year": "2021", "citations": "5", "title": "Deep Kernel Supervised Hashing For Node Classification In Structural Networks", "abstract": "<p>Node classification in structural networks has been proven to be useful in\nmany real world applications. With the development of network embedding, the\nperformance of node classification has been greatly improved. However, nearly\nall the existing network embedding based methods are hard to capture the actual\ncategory features of a node because of the linearly inseparable problem in\nlow-dimensional space; meanwhile they cannot incorporate simultaneously network\nstructure information and node label information into network embedding. To\naddress the above problems, in this paper, we propose a novel Deep Kernel\nSupervised Hashing (DKSH) method to learn the hashing representations of nodes\nfor node classification. Specifically, a deep multiple kernel learning is first\nproposed to map nodes into suitable Hilbert space to deal with linearly\ninseparable problem. Then, instead of only considering structural similarity\nbetween two nodes, a novel similarity matrix is designed to merge both network\nstructure information and node label information. Supervised by the similarity\nmatrix, the learned hashing representations of nodes simultaneously preserve\nthe two kinds of information well from the learned Hilbert space. Extensive\nexperiments show that the proposed method significantly outperforms the\nstate-of-the-art baselines over three real world benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [34.66252136230469, -5.606978416442871], "cluster": 9}, {"key": "guo2025gpu", "year": "2025", "citations": "0", "title": "Gpu-accelerated Multi-relational Parallel Graph Retrieval For Web-scale Recommendations", "abstract": "<p>Web recommendations provide personalized items from massive catalogs for\nusers, which rely heavily on retrieval stages to trade off the effectiveness\nand efficiency of selecting a small relevant set from billion-scale candidates\nin online digital platforms. As one of the largest Chinese search engine and\nnews feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based\nApproximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance\nestimation and efficient search for relevant items. However, current retrieval\nat Baidu fails in comprehensive user-item relational understanding due to\ndissected interaction modeling, and performs inefficiently in large-scale\ngraph-based ANNS because of suboptimal traversal navigation and the GPU\ncomputational bottleneck under high concurrency. To this end, we propose a\nGPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to\nachieve effective yet efficient retrieval in web-scale recommendations. First,\nwe propose a multi-relational user-item relevance metric learning method that\nunifies diverse user behaviors through multi-objective optimization and employs\na self-covariant loss to enhance pathfinding performance. Second, we develop a\nhierarchical parallel graph-based ANNS to boost graph retrieval throughput,\nwhich conducts breadth-depth-balanced searches on a large-scale item graph and\ncost-effectively handles irregular neural computation via adaptive aggregation\non GPUs. In addition, we integrate system optimization strategies in the\ndeployment of GMP-GR in Baidu. Extensive experiments demonstrate the\nsuperiority of GMP-GR in retrieval accuracy and efficiency. Deployed across\nmore than twenty applications at Baidu, GMP-GR serves hundreds of millions of\nusers with a throughput exceeding one hundred million requests per second.</p>\n", "tags": ["Graph-Based-Ann", "Distance-Metric-Learning", "Efficiency", "Scalability", "Similarity-Search", "Tools-&-Libraries", "Large-Scale-Search", "Evaluation"], "tsne_embedding": [42.24335861206055, 5.585280895233154], "cluster": 9}, {"key": "gupta2021irli", "year": "2021", "citations": "0", "title": "IRLI: Iterative Re-partitioning For Learning To Index", "abstract": "<p>Neural models have transformed the fundamental information retrieval problem\nof mapping a query to a giant set of items. However, the need for efficient and\nlow latency inference forces the community to reconsider efficient approximate\nnear-neighbor search in the item space. To this end, learning to index is\ngaining much interest in recent times. Methods have to trade between obtaining\nhigh accuracy while maintaining load balance and scalability in distributed\nsettings. We propose a novel approach called IRLI (pronounced `early\u2019), which\niteratively partitions the items by learning the relevant buckets directly from\nthe query-item relevance data. Furthermore, IRLI employs a superior\npower-of-\\(k\\)-choices based load balancing strategy. We mathematically show that\nIRLI retrieves the correct item with high probability under very natural\nassumptions and provides superior load balancing. IRLI surpasses the best\nbaseline\u2019s precision on multi-label classification while being \\(5x\\) faster on\ninference. For near-neighbor search tasks, the same method outperforms the\nstate-of-the-art Learned Hashing approach NeuralLSH by requiring only ~\n{1/6}^th of the candidates for the same recall. IRLI is both data and model\nparallel, making it ideal for distributed GPU implementation. We demonstrate\nthis advantage by indexing 100 million dense vectors and surpassing the popular\nFAISS library by &gt;10% on recall.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [33.504798889160156, 25.63810920715332], "cluster": 2}, {"key": "gupta2022faircop", "year": "2022", "citations": "1", "title": "Faircop: Facial Image Retrieval Using Contrastive Personalization", "abstract": "<p>Retrieving facial images from attributes plays a vital role in various\nsystems such as face recognition and suspect identification. Compared to other\nimage retrieval tasks, facial image retrieval is more challenging due to the\nhigh subjectivity involved in describing a person\u2019s facial features. Existing\nmethods do so by comparing specific characteristics from the user\u2019s mental\nimage against the suggested images via high-level supervision such as using\nnatural language. In contrast, we propose a method that uses a relatively\nsimpler form of binary supervision by utilizing the user\u2019s feedback to label\nimages as either similar or dissimilar to the target image. Such supervision\nenables us to exploit the contrastive learning paradigm for encapsulating each\nuser\u2019s personalized notion of similarity. For this, we propose a novel loss\nfunction optimized online via user feedback. We validate the efficacy of our\nproposed approach using a carefully designed testbed to simulate user feedback\nand a large-scale user study. Our experiments demonstrate that our method\niteratively improves personalization, leading to faster convergence and\nenhanced recommendation relevance, thereby, improving user satisfaction. Our\nproposed framework is also equipped with a user-friendly web interface with a\nreal-time experience for facial image retrieval.</p>\n", "tags": ["Self-Supervised", "Efficiency", "Recommender-Systems", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-17.684995651245117, -7.37254524230957], "cluster": 1}, {"key": "gupta2022medical", "year": "2023", "citations": "7", "title": "Medical Image Retrieval Via Nearest Neighbor Search On Pre-trained Image Features", "abstract": "<p>Nearest neighbor search (NNS) aims to locate the points in high-dimensional\nspace that is closest to the query point. The brute-force approach for finding\nthe nearest neighbor becomes computationally infeasible when the number of\npoints is large. The NNS has multiple applications in medicine, such as\nsearching large medical imaging databases, disease classification, diagnosis,\netc. With a focus on medical imaging, this paper proposes DenseLinkSearch an\neffective and efficient algorithm that searches and retrieves the relevant\nimages from heterogeneous sources of medical images. Towards this, given a\nmedical database, the proposed algorithm builds the index that consists of\npre-computed links of each point in the database. The search algorithm utilizes\nthe index to efficiently traverse the database in search of the nearest\nneighbor. We extensively tested the proposed NNS approach and compared the\nperformance with state-of-the-art NNS approaches on benchmark datasets and our\ncreated medical image datasets. The proposed approach outperformed the existing\napproach in terms of retrieving accurate neighbors and retrieval speed. We also\nexplore the role of medical image feature representation in content-based\nmedical image retrieval tasks. We propose a Transformer-based feature\nrepresentation technique that outperformed the existing pre-trained Transformer\napproach on CLEF 2011 medical image retrieval task. The source code of our\nexperiments are available at https://github.com/deepaknlp/DLS.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-51.660438537597656, 16.48640251159668], "cluster": 0}, {"key": "gupta2023caps", "year": "2023", "citations": "0", "title": "CAPS: A Practical Partition Index For Filtered Similarity Search", "abstract": "<p>With the surging popularity of approximate near-neighbor search (ANNS),\ndriven by advances in neural representation learning, the ability to serve\nqueries accompanied by a set of constraints has become an area of intense\ninterest. While the community has recently proposed several algorithms for\nconstrained ANNS, almost all of these methods focus on integration with\ngraph-based indexes, the predominant class of algorithms achieving\nstate-of-the-art performance in latency-recall tradeoffs. In this work, we take\na different approach and focus on developing a constrained ANNS algorithm via\nspace partitioning as opposed to graphs. To that end, we introduce Constrained\nApproximate Partitioned Search (CAPS), an index for ANNS with filters via space\npartitions that not only retains the benefits of a partition-based algorithm\nbut also outperforms state-of-the-art graph-based constrained search techniques\nin recall-latency tradeoffs, with only 10% of the index size.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Similarity-Search"], "tsne_embedding": [48.1702766418457, 10.940728187561035], "cluster": 9}, {"key": "gupta2025retreever", "year": "2025", "citations": "0", "title": "Retreever: Tree-based Coarse-to-fine Representations For Retrieval", "abstract": "<p>Document retrieval is a core component of question-answering systems, as it\nenables conditioning answer generation on new and large-scale corpora. While\neffective, the standard practice of encoding documents into high-dimensional\nembeddings for similarity search entails large memory and compute footprints,\nand also makes it hard to inspect the inner workings of the system. In this\npaper, we propose a tree-based method for organizing and representing reference\ndocuments at various granular levels, which offers the flexibility to balance\ncost and utility, and eases the inspection of the corpus content and retrieval\noperations. Our method, called ReTreever, jointly learns a routing function per\ninternal node of a binary tree such that query and reference documents are\nassigned to similar tree branches, hence directly optimizing for retrieval\nperformance. Our evaluations show that ReTreever generally preserves full\nrepresentation accuracy. Its hierarchical structure further provides strong\ncoarse representations and enhances transparency by indirectly learning\nmeaningful semantic groupings. Among hierarchical retrieval methods, ReTreever\nachieves the best retrieval accuracy at the lowest latency, proving that this\nfamily of techniques can be viable in practical applications.</p>\n", "tags": ["Text-Retrieval", "Scalability", "Similarity-Search", "Tree-Based-Ann", "Evaluation"], "tsne_embedding": [7.436561584472656, -23.192771911621094], "cluster": 7}, {"key": "gusak2024rece", "year": "2024", "citations": "2", "title": "RECE: Reduced Cross-entropy Loss For Large-catalogue Sequential Recommenders", "abstract": "<p>Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.</p>\n", "tags": ["Hashing-Methods", "CIKM", "Recommender-Systems", "Scalability", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [34.5554084777832, 13.836310386657715], "cluster": 2}, {"key": "gusev2022evolution", "year": "2020", "citations": "2", "title": "Evolution Of A Web-scale Near Duplicate Image Detection System", "abstract": "<p>Detecting near duplicate images is fundamental to the content ecosystem of\nphoto sharing web applications. However, such a task is challenging when\ninvolving a web-scale image corpus containing billions of images. In this\npaper, we present an efficient system for detecting near duplicate images\nacross 8 billion images. Our system consists of three stages: candidate\ngeneration, candidate selection, and clustering. We also demonstrate that this\nsystem can be used to greatly improve the quality of recommendations and search\nresults across a number of real-world applications.\n  In addition, we include the evolution of the system over the course of six\nyears, bringing out experiences and lessons on how new systems are designed to\naccommodate organic content growth as well as the latest technology. Finally,\nwe are releasing a human-labeled dataset of ~53,000 pairs of images introduced\nin this paper.</p>\n", "tags": ["Large-Scale-Search", "Scalability", "Datasets"], "tsne_embedding": [-20.885963439941406, -39.03927993774414], "cluster": 3}, {"key": "g\u00f3mez2018single", "year": "2018", "citations": "54", "title": "Single Shot Scene Text Retrieval", "abstract": "<p>Textual information found in scene images provides high level semantic\ninformation about the image and its context and it can be leveraged for better\nscene understanding. In this paper we address the problem of scene text\nretrieval: given a text query, the system must return all images containing the\nqueried text. The novelty of the proposed model consists in the usage of a\nsingle shot CNN architecture that predicts at the same time bounding boxes and\na compact text representation of the words in them. In this way, the text based\nimage retrieval task can be casted as a simple nearest neighbor search of the\nquery text representation over the outputs of the CNN over the entire image\ndatabase. Our experiments demonstrate that the proposed architecture\noutperforms previous state-of-the-art while it offers a significant increase in\nprocessing speed.</p>\n", "tags": ["Image-Retrieval", "Text-Retrieval"], "tsne_embedding": [-51.34471893310547, -0.664121687412262], "cluster": 0}, {"key": "g\u00fcnther2024late", "year": "2024", "citations": "0", "title": "Late Chunking: Contextual Chunk Embeddings Using Long-context Embedding Models", "abstract": "<p>Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be over-compressed in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term late in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks. The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [16.2738037109375, -20.67195701599121], "cluster": 7}, {"key": "hajiaghayi2021unbiased", "year": "2021", "citations": "0", "title": "Unbiased Sentence Encoder For Large-scale Multi-lingual Search Engines", "abstract": "<p>In this paper, we present a multi-lingual sentence encoder that can be used\nin search engines as a query and document encoder. This embedding enables a\nsemantic similarity score between queries and documents that can be an\nimportant feature in document ranking and relevancy. To train such a customized\nsentence encoder, it is beneficial to leverage users search data in the form of\nquery-document clicked pairs however, we must avoid relying too much on search\nclick data as it is biased and does not cover many unseen cases. The search\ndata is heavily skewed towards short queries and for long queries is small and\noften noisy. The goal is to design a universal multi-lingual encoder that works\nfor all cases and covers both short and long queries. We select a number of\npublic NLI datasets in different languages and translation data and together\nwith user search data we train a language model using a multi-task approach. A\nchallenge is that these datasets are not homogeneous in terms of content, size\nand the balance ratio. While the public NLI datasets are usually two-sentence\nbased with the same portion of positive and negative pairs, the user search\ndata can contain multi-sentence documents and only positive pairs. We show how\nmulti-task training enables us to leverage all these datasets and exploit\nknowledge sharing across these tasks.</p>\n", "tags": ["Scalability", "Datasets"], "tsne_embedding": [-20.266714096069336, 0.6881412267684937], "cluster": 1}, {"key": "hamann2019hamming", "year": "2019", "citations": "0", "title": "Hamming Sentence Embeddings For Information Retrieval", "abstract": "<p>In retrieval applications, binary hashes are known to offer significant\nimprovements in terms of both memory and speed. We investigate the compression\nof sentence embeddings using a neural encoder-decoder architecture, which is\ntrained by minimizing reconstruction error. Instead of employing the original\nreal-valued embeddings, we use latent representations in Hamming space produced\nby the encoder for similarity calculations.\n  In quantitative experiments on several benchmarks for semantic similarity\ntasks, we show that our compressed hamming embeddings yield a comparable\nperformance to uncompressed embeddings (Sent2Vec, InferSent, Glove-BoW), at\ncompression ratios of up to 256:1. We further demonstrate that our model\nstrongly decorrelates input features, and that the compressor generalizes well\nwhen pre-trained on Wikipedia sentences. We publish the source code on Github\nand all experimental results.</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [3.656123161315918, -15.171454429626465], "cluster": 7}, {"key": "hamara2024latent", "year": "2024", "citations": "0", "title": "From Latent To Engine Manifolds: Analyzing Imagebind's Multimodal Embedding Space", "abstract": "<p>This study investigates ImageBind\u2019s ability to generate meaningful fused\nmultimodal embeddings for online auto parts listings. We propose a simplistic\nembedding fusion workflow that aims to capture the overlapping information of\nimage/text pairs, ultimately combining the semantics of a post into a joint\nembedding. After storing such fused embeddings in a vector database, we\nexperiment with dimensionality reduction and provide empirical evidence to\nconvey the semantic quality of the joint embeddings by clustering and examining\nthe posts nearest to each cluster centroid. Additionally, our initial findings\nwith ImageBind\u2019s emergent zero-shot cross-modal retrieval suggest that pure\naudio embeddings can correlate with semantically similar marketplace listings,\nindicating potential avenues for future research.</p>\n", "tags": ["Multimodal-Retrieval", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [8.812156677246094, -49.4838752746582], "cluster": 3}, {"key": "hamster2023rediscovering", "year": "2023", "citations": "0", "title": "Rediscovering Hashed Random Projections For Efficient Quantization Of Contextualized Sentence Embeddings", "abstract": "<p>Training and inference on edge devices often requires an efficient setup due\nto computational limitations. While pre-computing data representations and\ncaching them on a server can mitigate extensive edge device computation, this\nleads to two challenges. First, the amount of storage required on the server\nthat scales linearly with the number of instances. Second, the bandwidth\nrequired to send extensively large amounts of data to an edge device. To reduce\nthe memory footprint of pre-computed data representations, we propose a simple,\nyet effective approach that uses randomly initialized hyperplane projections.\nTo further reduce their size by up to 98.96%, we quantize the resulting\nfloating-point representations into binary vectors. Despite the greatly reduced\nsize, we show that the embeddings remain effective for training models across\nvarious English and German sentence classification tasks that retain 94%\u201399%\nof their floating-point.</p>\n", "tags": ["Quantization", "Memory-Efficiency", "Locality-Sensitive-Hashing"], "tsne_embedding": [25.106128692626953, 4.4750542640686035], "cluster": 6}, {"key": "han2017beyond", "year": "2017", "citations": "10", "title": "Beyond SIFT Using Binary Features For Loop Closure Detection", "abstract": "<p>In this paper a binary feature based Loop Closure Detection (LCD) method is\nproposed, which for the first time achieves higher precision-recall (PR)\nperformance compared with state-of-the-art SIFT feature based approaches. The\nproposed system originates from our previous work Multi-Index hashing for Loop\nclosure Detection (MILD), which employs Multi-Index Hashing\n(MIH)~\\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of\nbinary features. As the accuracy of MILD is limited by repeating textures and\ninaccurate image similarity measurement, burstiness handling is introduced to\nsolve this problem and achieves considerable accuracy improvement.\nAdditionally, a comprehensive theoretical analysis on MIH used in MILD is\nconducted to further explore the potentials of hashing methods for ANN search\nof binary features from probabilistic perspective. This analysis provides more\nfreedom on best parameter choosing in MIH for different application scenarios.\nExperiments on popular public datasets show that the proposed approach achieved\nthe highest accuracy compared with state-of-the-art while running at 30Hz for\ndatabases containing thousands of images.</p>\n", "tags": ["Hashing-Methods", "Vector-Indexing", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [-7.635015487670898, 25.36146354675293], "cluster": 8}, {"key": "han2017mild", "year": "2017", "citations": "4", "title": "MILD: Multi-index Hashing For Loop Closure Detection", "abstract": "<p>Loop Closure Detection (LCD) has been proved to be extremely useful in global\nconsistent visual Simultaneously Localization and Mapping (SLAM) and\nappearance-based robot relocalization. Methods exploiting binary features in\nbag of words representation have recently gained a lot of popularity for their\nefficiency, but suffer from low recall due to the inherent drawback that high\ndimensional binary feature descriptors lack well-defined centroids. In this\npaper, we propose a realtime LCD approach called MILD (Multi-Index Hashing for\nLoop closure Detection), in which image similarity is measured by feature\nmatching directly to achieve high recall without introducing extra\ncomputational complexity with the aid of Multi-Index Hashing (MIH). A\ntheoretical analysis of the approximate image similarity measurement using MIH\nis presented, which reveals the trade-off between efficiency and accuracy from\na probabilistic perspective. Extensive comparisons with state-of-the-art LCD\nmethods demonstrate the superiority of MILD in both efficiency and accuracy.</p>\n", "tags": ["Efficiency", "Vector-Indexing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-7.693398475646973, 25.426029205322266], "cluster": 8}, {"key": "han2021text", "year": "2021", "citations": "36", "title": "Text-based Person Search With Limited Data", "abstract": "<p>Text-based person search (TBPS) aims at retrieving a target person from an\nimage gallery with a descriptive text query. Solving such a fine-grained\ncross-modal retrieval task is challenging, which is further hampered by the\nlack of large-scale datasets. In this paper, we present a framework with two\nnovel components to handle the problems brought by limited data. Firstly, to\nfully utilize the existing small-scale benchmarking datasets for more\ndiscriminative feature learning, we introduce a cross-modal momentum\ncontrastive learning framework to enrich the training data for a given\nmini-batch. Secondly, we propose to transfer knowledge learned from existing\ncoarse-grained large-scale datasets containing image-text pairs from\ndrastically different problem domains to compensate for the lack of TBPS\ntraining data. A transfer learning method is designed so that useful\ninformation can be transferred despite the large domain gap. Armed with these\ncomponents, our method achieves new state of the art on the CUHK-PEDES dataset\nwith significant improvements over the prior art in terms of Rank-1 and mAP.\nOur code is available at https://github.com/BrandonHanx/TextReID.</p>\n", "tags": ["Self-Supervised", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-26.54111099243164, -15.961060523986816], "cluster": 5}, {"key": "han2022large", "year": "2022", "citations": "0", "title": "Large-scale Product Retrieval With Weakly Supervised Representation Learning", "abstract": "<p>Large-scale weakly supervised product retrieval is a practically useful yet\ncomputationally challenging problem. This paper introduces a novel solution for\nthe eBay Visual Search Challenge (eProduct) held at the Ninth Workshop on\nFine-Grained Visual Categorisation workshop (FGVC9) of CVPR 2022. This\ncompetition presents two challenges: (a) E-commerce is a drastically\nfine-grained domain including many products with subtle visual differences; (b)\nA lacking of target instance-level labels for model training, with only coarse\ncategory labels and product titles available. To overcome these obstacles, we\nformulate a strong solution by a set of dedicated designs: (a) Instead of using\ntext training data directly, we mine thousands of pseudo-attributes from\nproduct titles and use them as the ground truths for multi-label\nclassification. (b) We incorporate several strong backbones with advanced\ntraining recipes for more discriminative representation learning. (c) We\nfurther introduce a number of post-processing techniques including whitening,\nre-ranking and model ensemble for retrieval enhancement. By achieving 71.53%\nMAR, our solution \u201cInvolution King\u201d achieves the second position on the\nleaderboard.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Hybrid-Ann-Methods", "Datasets", "Supervised", "Re-Ranking"], "tsne_embedding": [-9.831009864807129, -44.450584411621094], "cluster": 3}, {"key": "han2023noisy", "year": "2023", "citations": "16", "title": "Noisy Correspondence Learning With Meta Similarity Correction", "abstract": "<p>Despite the success of multimodal learning in cross-modal retrieval task, the\nremarkable progress relies on the correct correspondence among multimedia data.\nHowever, collecting such ideal data is expensive and time-consuming. In\npractice, most widely used datasets are harvested from the Internet and\ninevitably contain mismatched pairs. Training on such noisy correspondence\ndatasets causes performance degradation because the cross-modal retrieval\nmethods can wrongly enforce the mismatched data to be similar. To tackle this\nproblem, we propose a Meta Similarity Correction Network (MSCN) to provide\nreliable similarity scores. We view a binary classification task as the\nmeta-process that encourages the MSCN to learn discrimination from positive and\nnegative meta-data. To further alleviate the influence of noise, we design an\neffective data purification strategy using meta-data as prior knowledge to\nremove the noisy samples. Extensive experiments are conducted to demonstrate\nthe strengths of our method in both synthetic and real-world noises, including\nFlickr30K, MS-COCO, and Conceptual Captions.</p>\n", "tags": ["Multimodal-Retrieval", "CVPR", "Evaluation", "Datasets"], "tsne_embedding": [-9.63364315032959, 11.15429401397705], "cluster": 8}, {"key": "han2024hashing", "year": "2024", "citations": "0", "title": "Hashing For Protein Structure Similarity Search", "abstract": "<p>Protein structure similarity search (PSSS), which tries to search proteins\nwith similar structures, plays a crucial role across diverse domains from drug\ndesign to protein function prediction and molecular evolution. Traditional\nalignment-based PSSS methods, which directly calculate alignment on the protein\nstructures, are highly time-consuming with high memory cost. Recently,\nalignment-free methods, which represent protein structures as fixed-length\nreal-valued vectors, are proposed for PSSS. Although these methods have lower\ntime and memory cost than alignment-based methods, their time and memory cost\nis still too high for large-scale PSSS, and their accuracy is unsatisfactory.\nIn this paper, we propose a novel method, called\n\\(\\underline{\\text{p}}\\)r\\(\\underline{\\text{o}}\\)tein\n\\(\\underline{\\text{s}}\\)tructure \\(\\underline{\\text{h}}\\)ashing (POSH), for PSSS.\nPOSH learns a binary vector representation for each protein structure, which\ncan dramatically reduce the time and memory cost for PSSS compared with\nreal-valued vector representation based methods. Furthermore, in POSH we also\npropose expressive hand-crafted features and a structure encoder to well model\nboth node and edge interactions in proteins. Experimental results on real\ndatasets show that POSH can outperform other methods to achieve\nstate-of-the-art accuracy. Furthermore, POSH achieves a memory saving of more\nthan six times and speed improvement of more than four times, compared with\nother methods.</p>\n", "tags": ["Hashing-Methods", "Similarity-Search", "Scalability", "Datasets"], "tsne_embedding": [0.8073187470436096, 54.09549331665039], "cluster": 4}, {"key": "hansen2019unsupervised", "year": "2019", "citations": "25", "title": "Unsupervised Semantic Hashing With Pairwise Reconstruction", "abstract": "<p>Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing, documents are encoded as short binary vectors (i.e., hash codes), such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this, we present Semantic Hashing with Pairwise Reconstruction (PairRec), which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes, and then learns to reconstruct the same query document from both of these hash codes (i.e., pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches, and obtain significant performance improvements in the task of document similarity search.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Similarity-Search", "SIGIR", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [10.01210880279541, -7.525675296783447], "cluster": 6}, {"key": "hansen2020content", "year": "2020", "citations": "36", "title": "Content-aware Neural Hashing For Cold-start Recommendation", "abstract": "<p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item\u2019s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Text-Retrieval", "Recommender-Systems", "SIGIR", "Evaluation"], "tsne_embedding": [19.90630340576172, -22.920974731445312], "cluster": 7}, {"key": "hansen2020unsupervised", "year": "2020", "citations": "22", "title": "Unsupervised Semantic Hashing With Pairwise Reconstruction", "abstract": "<p>Semantic Hashing is a popular family of methods for efficient similarity\nsearch in large-scale datasets. In Semantic Hashing, documents are encoded as\nshort binary vectors (i.e., hash codes), such that semantic similarity can be\nefficiently computed using the Hamming distance. Recent state-of-the-art\napproaches have utilized weak supervision to train better performing hashing\nmodels. Inspired by this, we present Semantic Hashing with Pairwise\nReconstruction (PairRec), which is a discrete variational autoencoder based\nhashing model. PairRec first encodes weakly supervised training pairs (a query\ndocument and a semantically similar document) into two hash codes, and then\nlearns to reconstruct the same query document from both of these hash codes\n(i.e., pairwise reconstruction). This pairwise reconstruction enables our model\nto encode local neighbourhood structures within the hash code directly through\nthe decoder. We experimentally compare PairRec to traditional and\nstate-of-the-art approaches, and obtain significant performance improvements in\nthe task of document similarity search.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Similarity-Search", "SIGIR", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [10.012140274047852, -7.525728702545166], "cluster": 6}, {"key": "hansen2021projected", "year": "2021", "citations": "4", "title": "Projected Hamming Dissimilarity For Bit-level Importance Coding In Collaborative Filtering", "abstract": "<p>When reasoning about tasks that involve large amounts of data, a common\napproach is to represent data items as objects in the Hamming space where\noperations can be done efficiently and effectively. Object similarity can then\nbe computed by learning binary representations (hash codes) of the objects and\ncomputing their Hamming distance. While this is highly efficient, each bit\ndimension is equally weighted, which means that potentially discriminative\ninformation of the data is lost. A more expressive alternative is to use\nreal-valued vector representations and compute their inner product; this allows\nvarying the weight of each dimension but is many magnitudes slower. To fix\nthis, we derive a new way of measuring the dissimilarity between two objects in\nthe Hamming space with binary weighting of each dimension (i.e., disabling\nbits): we consider a field-agnostic dissimilarity that projects the vector of\none object onto the vector of the other. When working in the Hamming space,\nthis results in a novel projected Hamming dissimilarity, which by choice of\nprojection, effectively allows a binary importance weighting of the hash code\nof one object through the hash code of the other. We propose a variational\nhashing model for learning hash codes optimized for this projected Hamming\ndissimilarity, and experimentally evaluate it in collaborative filtering\nexperiments. The resultant hash codes lead to effectiveness gains of up to +7%\nin NDCG and +14% in MRR compared to state-of-the-art hashing-based\ncollaborative filtering baselines, while requiring no additional storage and no\ncomputational overhead compared to using the Hamming distance.</p>\n", "tags": ["Recommender-Systems", "Hashing-Methods"], "tsne_embedding": [1.585991621017456, 25.305519104003906], "cluster": 8}, {"key": "hansen2021representation", "year": "2021", "citations": "0", "title": "Representation Learning For Efficient And Effective Similarity Search And Recommendation", "abstract": "<p>How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [21.401878356933594, 7.380964756011963], "cluster": 6}, {"key": "hansen2021unsupervised", "year": "2021", "citations": "8", "title": "Unsupervised Multi-index Semantic Hashing", "abstract": "<p>Semantic hashing represents documents as compact binary vectors (hash codes)\nand allows both efficient and effective similarity search in large-scale\ninformation retrieval. The state of the art has primarily focused on learning\nhash codes that improve similarity search effectiveness, while assuming a\nbrute-force linear scan strategy for searching over all the hash codes, even\nthough much faster alternatives exist. One such alternative is multi-index\nhashing, an approach that constructs a smaller candidate set to search over,\nwhich depending on the distribution of the hash codes can lead to sub-linear\nsearch time. In this work, we propose Multi-Index Semantic Hashing (MISH), an\nunsupervised hashing model that learns hash codes that are both effective and\nhighly efficient by being optimized for multi-index hashing. We derive novel\ntraining objectives, which enable to learn hash codes that reduce the candidate\nsets produced by multi-index hashing, while being end-to-end trainable. In\nfact, our proposed training objectives are model agnostic, i.e., not tied to\nhow the hash codes are generated specifically in MISH, and are straight-forward\nto include in existing and future semantic hashing models. We experimentally\ncompare MISH to state-of-the-art semantic hashing baselines in the task of\ndocument similarity search. We find that even though multi-index hashing also\nimproves the efficiency of the baselines compared to a linear scan, they are\nstill upwards of 33% slower than MISH, while MISH is still able to obtain\nstate-of-the-art effectiveness.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Efficiency", "Vector-Indexing", "Similarity-Search", "Scalability", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [16.5999813079834, 1.3162682056427002], "cluster": 6}, {"key": "hansen2025content", "year": "2020", "citations": "36", "title": "Content-aware Neural Hashing For Cold-start Recommendation", "abstract": "<p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item\u2019s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Text-Retrieval", "Recommender-Systems", "SIGIR", "Evaluation"], "tsne_embedding": [19.906301498413086, -22.920974731445312], "cluster": 7}, {"key": "hansen2025unsupervised", "year": "2019", "citations": "25", "title": "Unsupervised Neural Generative Semantic Hashing", "abstract": "<p>Fast similarity search is a key component in large-scale information retrieval, where semantic hashing has become a popular strategy for representing documents as binary hash codes. Recent advances in this area have been obtained through neural network based models: generative models trained by learning to reconstruct the original documents. We present a novel unsupervised generative semantic hashing approach, \\textit{Ranking based Semantic Hashing} (RBSH) that consists of both a variational and a ranking based component. Similarly to variational autoencoders, the variational component is trained to reconstruct the original document conditioned on its generated hash code, and as in prior work, it only considers documents individually. The ranking component solves this limitation by incorporating inter-document similarity into the hash code generation, modelling document ranking through a hinge loss. To circumvent the need for labelled data to compute the hinge loss, we use a weak labeller and thus keep the approach fully unsupervised.\nExtensive experimental evaluation on four publicly available datasets against traditional baselines and recent state-of-the-art methods for semantic hashing shows that RBSH significantly outperforms all other methods across all evaluated hash code lengths. In fact, RBSH hash codes are able to perform similarly to state-of-the-art hash codes while using 2-4x fewer bits.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Similarity-Search", "SIGIR", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [12.234959602355957, -10.815037727355957], "cluster": 7}, {"key": "hao2016what", "year": "2016", "citations": "9", "title": "What Is The Best Practice For Cnns Applied To Visual Instance Retrieval?", "abstract": "<p>Previous work has shown that feature maps of deep convolutional neural\nnetworks (CNNs) can be interpreted as feature representation of a particular\nimage region. Features aggregated from these feature maps have been exploited\nfor image retrieval tasks and achieved state-of-the-art performances in recent\nyears. The key to the success of such methods is the feature representation.\nHowever, the different factors that impact the effectiveness of features are\nstill not explored thoroughly. There are much less discussion about the best\ncombination of them.\n  The main contribution of our paper is the thorough evaluations of the various\nfactors that affect the discriminative ability of the features extracted from\nCNNs. Based on the evaluation results, we also identify the best choices for\ndifferent factors and propose a new multi-scale image feature representation\nmethod to encode the image effectively. Finally, we show that the proposed\nmethod generalises well and outperforms the state-of-the-art methods on four\ntypical datasets used for visual instance retrieval.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-51.24106216430664, 2.7276597023010254], "cluster": 0}, {"key": "harpeled2019near", "year": "2019", "citations": "8", "title": "Near Neighbor: Who Is The Fairest Of Them All?", "abstract": "<p>\\(\\newcommand{\\ball}{\\mathbb{B}}\\newcommand{\\dsQ}{{\\mathcal{Q}}}\\newcommand{\\dsS}{{\\mathcal{S}}}\\)In\nthis work we study a fair variant of the near neighbor problem. Namely, given a\nset of \\(n\\) points \\(P\\) and a parameter \\(r\\), the goal is to preprocess the\npoints, such that given a query point \\(q\\), any point in the \\(r\\)-neighborhood of\nthe query, i.e., \\(\\ball(q,r)\\), have the same probability of being reported as\nthe near neighbor.\n  We show that LSH based algorithms can be made fair, without a significant\nloss in efficiency. Specifically, we show an algorithm that reports a point in\nthe \\(r\\)-neighborhood of a query \\(q\\) with almost uniform probability. The query\ntime is proportional to \\(O\\bigl( \\mathrm{dns}(q.r) \\dsQ(n,c) \\bigr)\\), and its\nspace is \\(O(\\dsS(n,c))\\), where \\(\\dsQ(n,c)\\) and \\(\\dsS(n,c)\\) are the query time\nand space of an LSH algorithm for \\(c\\)-approximate near neighbor, and\n\\(\\mathrm{dns}(q,r)\\) is a function of the local density around \\(q\\).\n  Our approach works more generally for sampling uniformly from a\nsub-collection of sets of a given collection and can be used in a few other\napplications. Finally, we run experiments to show performance of our approach\non real data.</p>\n", "tags": ["Efficiency", "Evaluation", "Locality-Sensitive-Hashing"], "tsne_embedding": [21.533802032470703, 47.910858154296875], "cluster": 4}, {"key": "hashimoto2021case", "year": "2023", "citations": "20", "title": "Case-based Similar Image Retrieval For Weakly Annotated Large Histopathological Images Of Malignant Lymphoma Using Deep Metric Learning", "abstract": "<p>In the present study, we propose a novel case-based similar image retrieval\n(SIR) method for hematoxylin and eosin (H&amp;E)-stained histopathological images\nof malignant lymphoma. When a whole slide image (WSI) is used as an input\nquery, it is desirable to be able to retrieve similar cases by focusing on\nimage patches in pathologically important regions such as tumor cells. To\naddress this problem, we employ attention-based multiple instance learning,\nwhich enables us to focus on tumor-specific regions when the similarity between\ncases is computed. Moreover, we employ contrastive distance metric learning to\nincorporate immunohistochemical (IHC) staining patterns as useful supervised\ninformation for defining appropriate similarity between heterogeneous malignant\nlymphoma cases. In the experiment with 249 malignant lymphoma patients, we\nconfirmed that the proposed method exhibited higher evaluation measures than\nthe baseline case-based SIR methods. Furthermore, the subjective evaluation by\npathologists revealed that our similarity measure using IHC staining patterns\nis appropriate for representing the similarity of H&amp;E-stained tissue images for\nmalignant lymphoma.</p>\n", "tags": ["Supervised", "Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-52.81682205200195, 15.398664474487305], "cluster": 0}, {"key": "he2016fashionista", "year": "2016", "citations": "1", "title": "Fashionista: A Fashion-aware Graphical System For Exploring Visually Similar Items", "abstract": "<p>To build a fashion recommendation system, we need to help users retrieve\nfashionable items that are visually similar to a particular query, for reasons\nranging from searching alternatives (i.e., substitutes), to generating stylish\noutfits that are visually consistent, among other applications. In domains like\nclothing and accessories, such considerations are particularly paramount as the\nvisual appearance of items is a critical feature that guides users\u2019 decisions.\nHowever, existing systems like Amazon and eBay still rely mainly on keyword\nsearch and recommending loosely consistent items (e.g. based on co-purchasing\nor browsing data), without an interface that makes use of visual information to\nserve the above needs. In this paper, we attempt to fill this gap by designing\nand implementing an image-based query system, called Fashionista, which\nprovides a graphical interface to help users efficiently explore those items\nthat are not only visually similar to a given query, but which are also\nfashionable, as determined by visually-aware recommendation approaches.\nMethodologically, Fashionista learns a low-dimensional visual space as well as\nthe evolution of fashion trends from large corpora of binary feedback data such\nas purchase histories of Women\u2019s Clothing &amp; Accessories from Amazon, which we\nuse for this demonstration.</p>\n", "tags": ["Recommender-Systems"], "tsne_embedding": [-19.915639877319336, -50.70457458496094], "cluster": 3}, {"key": "he2017hashing", "year": "2018", "citations": "89", "title": "Hashing As Tie-aware Learning To Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest\nneighbor retrieval. In this paper, we develop learning to rank formulations for\nhashing, aimed at directly optimizing ranking-based evaluation metrics such as\nAverage Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We\nfirst observe that the integer-valued Hamming distance often leads to tied\nrankings, and propose to use tie-aware versions of AP and NDCG to evaluate\nhashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive\ntheir continuous relaxations, and perform gradient-based optimization with deep\nneural networks. Our results establish the new state-of-the-art for image\nretrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["CVPR", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-4.9586663246154785, -15.809935569763184], "cluster": 1}, {"key": "he2018hashing", "year": "2018", "citations": "89", "title": "Hashing As Tie-aware Learning To Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["CVPR", "Evaluation", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-4.958887100219727, -15.809955596923828], "cluster": 1}, {"key": "he2018local", "year": "2018", "citations": "209", "title": "Local Descriptors Optimized For Average Precision", "abstract": "<p>Extraction of local feature descriptors is a vital stage in the solution\npipelines for numerous computer vision tasks. Learning-based approaches improve\nperformance in certain tasks, but still cannot replace handcrafted features in\ngeneral. In this paper, we improve the learning of local feature descriptors by\noptimizing the performance of descriptor matching, which is a common stage that\nfollows descriptor extraction in local feature based pipelines, and can be\nformulated as nearest neighbor retrieval. Specifically, we directly optimize a\nranking-based retrieval performance metric, Average Precision, using deep\nneural networks. This general-purpose solution can also be viewed as a listwise\nlearning to rank approach, which is advantageous compared to recent local\nranking approaches. On standard benchmarks, descriptors learned with our\nformulation achieve state-of-the-art results in patch verification, patch\nretrieval, and image matching.</p>\n", "tags": ["CVPR", "Evaluation"], "tsne_embedding": [-32.450809478759766, -5.4052228927612305], "cluster": 0}, {"key": "he2019k", "year": "2019", "citations": "31", "title": "K-nearest Neighbors Hashing", "abstract": "<p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which\nenables efficient similarity search and storage. However,\nthe non-isometry sign(\u00b7) function makes it hard to project\nthe nearest neighbors in continuous data space into the\nclosest codewords in discrete Hamming space. In this work,\nwe revisit the sign(\u00b7) function from the perspective of space partitioning.\nIn specific, we bridge the gap between\nk-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(\u00b7).\nTheoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Similarity-Search", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-2.2612078189849854, 32.02456283569336], "cluster": 8}, {"key": "he2019one", "year": "2019", "citations": "9", "title": "One Network For Multi-domains: Domain Adaptive Hashing With Intersectant Generative Adversarial Network", "abstract": "<p>With the recent explosive increase of digital data, image recognition and\nretrieval become a critical practical application. Hashing is an effective\nsolution to this problem, due to its low storage requirement and high query\nspeed. However, most of past works focus on hashing in a single (source)\ndomain. Thus, the learned hash function may not adapt well in a new (target)\ndomain that has a large distributional difference with the source domain. In\nthis paper, we explore an end-to-end domain adaptive learning framework that\nsimultaneously and precisely generates discriminative hash codes and classifies\ntarget domain images. Our method encodes two domains images into a semantic\ncommon space, followed by two independent generative adversarial networks\narming at crosswise reconstructing two domains\u2019 images, reducing domain\ndisparity and improving alignment in the shared space. We evaluate our\nframework on {four} public benchmark datasets, all of which show that our\nmethod is superior to the other state-of-the-art methods on the tasks of object\nrecognition and image retrieval.</p>\n", "tags": ["Datasets", "Evaluation", "AAAI", "Tools-&-Libraries", "Image-Retrieval", "Hashing-Methods", "IJCAI", "Robustness"], "tsne_embedding": [-5.296802043914795, 11.54836654663086], "cluster": 8}, {"key": "he2019view", "year": "2019", "citations": "57", "title": "View N-gram Network For 3D Object Retrieval", "abstract": "<p>How to aggregate multi-view representations of a 3D object into an\ninformative and discriminative one remains a key challenge for multi-view 3D\nobject retrieval. Existing methods either use view-wise pooling strategies\nwhich neglect the spatial information across different views or employ\nrecurrent neural networks which may face the efficiency problem. To address\nthese issues, we propose an effective and efficient framework called View\nN-gram Network (VNN). Inspired by n-gram models in natural language processing,\nVNN divides the view sequence into a set of visual n-grams, which involve\noverlapping consecutive view sub-sequences. By doing so, spatial information\nacross multiple views is captured, which helps to learn a discriminative global\nembedding for each 3D object. Experiments on 3D shape retrieval benchmarks,\nincluding ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the\nsuperiority of our proposed method.</p>\n", "tags": ["Efficiency", "ICCV", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-37.591732025146484, -13.41657829284668], "cluster": 5}, {"key": "he2021self", "year": "2021", "citations": "3", "title": "Self-supervised Video Retrieval Transformer Network", "abstract": "<p>Content-based video retrieval aims to find videos from a large video database\nthat are similar to or even near-duplicate of a given query video. Video\nrepresentation and similarity search algorithms are crucial to any video\nretrieval system. To derive effective video representation, most video\nretrieval systems require a large amount of manually annotated data for\ntraining, making it costly inefficient. In addition, most retrieval systems are\nbased on frame-level features for video similarity searching, making it\nexpensive both storage wise and search wise. We propose a novel video retrieval\nsystem, termed SVRTN, that effectively addresses the above shortcomings. It\nfirst applies self-supervised training to effectively learn video\nrepresentation from unlabeled data to avoid the expensive cost of manual\nannotation. Then, it exploits transformer structure to aggregate frame-level\nfeatures into clip-level to reduce both storage space and search complexity. It\ncan learn the complementary and discriminative information from the\ninteractions among clip frames, as well as acquire the frame permutation and\nmissing invariant ability to support more flexible retrieval manners.\nComprehensive experiments on two challenging video retrieval datasets, namely\nFIVR-200K and SVD, verify the effectiveness of our proposed SVRTN method, which\nachieves the best performance of video retrieval on accuracy and efficiency.</p>\n", "tags": ["Self-Supervised", "Efficiency", "Similarity-Search", "Datasets", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-39.59023666381836, -30.63790512084961], "cluster": 5}, {"key": "he2021unsupervised", "year": "2021", "citations": "0", "title": "Unsupervised Domain-adaptive Hash For Networks", "abstract": "<p>Abundant real-world data can be naturally represented by large-scale\nnetworks, which demands efficient and effective learning algorithms. At the\nsame time, labels may only be available for some networks, which demands these\nalgorithms to be able to adapt to unlabeled networks. Domain-adaptive hash\nlearning has enjoyed considerable success in the computer vision community in\nmany practical tasks due to its lower cost in both retrieval time and storage\nfootprint. However, it has not been applied to multiple-domain networks. In\nthis work, we bridge this gap by developing an unsupervised domain-adaptive\nhash learning method for networks, dubbed UDAH. Specifically, we develop four\n{task-specific yet correlated} components: (1) network structure preservation\nvia a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,\n(3) cross-domain intersected discriminators, and (4) semantic center alignment.\nWe conduct a wide range of experiments to evaluate the effectiveness and\nefficiency of our method on a range of tasks including link prediction, node\nclassification, and neighbor recommendation. Our evaluation results demonstrate\nthat our model achieves better performance than the state-of-the-art\nconventional discrete embedding methods over all the tasks.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Neural-Hashing", "Efficiency", "Recommender-Systems", "Scalability", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [33.68583679199219, -3.90696120262146], "cluster": 9}, {"key": "he2024bit", "year": "2024", "citations": "2", "title": "Bit-mask Robust Contrastive Knowledge Distillation For Unsupervised Semantic Hashing", "abstract": "<p>Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Image-Retrieval", "Evaluation", "Unsupervised"], "tsne_embedding": [24.432281494140625, -36.86466979980469], "cluster": 7}, {"key": "he2024hybridhash", "year": "2024", "citations": "4", "title": "Hybridhash: Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval", "abstract": "<p>Deep image hashing aims to map input images into simple binary hash codes via\ndeep neural networks and thus enable effective large-scale image retrieval.\nRecently, hybrid networks that combine convolution and Transformer have\nachieved superior performance on various computer tasks and have attracted\nextensive attention from researchers. Nevertheless, the potential benefits of\nsuch hybrid networks in image retrieval still need to be verified. To this end,\nwe propose a hybrid convolutional and self-attention deep hashing method known\nas HybridHash. Specifically, we propose a backbone network with stage-wise\narchitecture in which the block aggregation function is introduced to achieve\nthe effect of local self-attention and reduce the computational complexity. The\ninteraction module has been elaborately designed to promote the communication\nof information between image blocks and to enhance the visual representations.\nWe have conducted comprehensive experiments on three widely used datasets:\nCIFAR-10, NUS-WIDE and IMAGENET. The experimental results demonstrate that the\nmethod proposed in this paper has superior performance with respect to\nstate-of-the-art deep hashing methods. Source code is available\nhttps://github.com/shuaichaochao/HybridHash.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Image-Retrieval", "Hashing-Methods", "Multimodal-Retrieval", "Scalability"], "tsne_embedding": [-15.786617279052734, 0.7677788138389587], "cluster": 1}, {"key": "he2025hashing", "year": "2018", "citations": "89", "title": "Hashing As Tie-aware Learning To Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["CVPR", "Evaluation", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-4.958716869354248, -15.809820175170898], "cluster": 1}, {"key": "he2025k", "year": "2019", "citations": "31", "title": "K-nearest Neighbors Hashing", "abstract": "<p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which\nenables efficient similarity search and storage. However,\nthe non-isometry sign(\u00b7) function makes it hard to project\nthe nearest neighbors in continuous data space into the\nclosest codewords in discrete Hamming space. In this work,\nwe revisit the sign(\u00b7) function from the perspective of space partitioning.\nIn specific, we bridge the gap between\nk-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(\u00b7).\nTheoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Similarity-Search", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-2.2612078189849854, 32.024566650390625], "cluster": 8}, {"key": "he2025mambahash", "year": "2025", "citations": "0", "title": "Mambahash: Visual State Space Deep Hashing Model For Large-scale Image Retrieval", "abstract": "<p>Deep image hashing aims to enable effective large-scale image retrieval by mapping the input images into simple binary hash codes through deep neural networks. More recently, Vision Mamba with linear time complexity has attracted extensive attention from researchers by achieving outstanding performance on various computer tasks. Nevertheless, the suitability of Mamba for large-scale image retrieval tasks still needs to be explored. Towards this end, we propose a visual state space hashing model, called MambaHash. Concretely, we propose a backbone network with stage-wise architecture, in which grouped Mamba operation is introduced to model local and global information by utilizing Mamba to perform multi-directional scanning along different groups of the channel. Subsequently, the proposed channel interaction attention module is used to enhance information communication across channels. Finally, we meticulously design an adaptive feature enhancement module to increase feature diversity and enhance the visual representation capability of the model. We have conducted comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and IMAGENET. The experimental results demonstrate that compared with the state-of-the-art deep hashing methods, our proposed MambaHash has well efficiency and superior performance to effectively accomplish large-scale image retrieval tasks. Source code is available https://github.com/shuaichaochao/MambaHash.git</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Hashing-Methods", "Multimodal-Retrieval", "Scalability"], "tsne_embedding": [-15.659968376159668, 0.7102973461151123], "cluster": 1}, {"key": "hegde2019similar", "year": "2019", "citations": "138", "title": "Similar Image Search For Histopathology: SMILY", "abstract": "<p>The increasing availability of large institutional and public histopathology\nimage datasets is enabling the searching of these datasets for diagnosis,\nresearch, and education. Though these datasets typically have associated\nmetadata such as diagnosis or clinical notes, even carefully curated datasets\nrarely contain annotations of the location of regions of interest on each\nimage. Because pathology images are extremely large (up to 100,000 pixels in\neach dimension), further laborious visual search of each image may be needed to\nfind the feature of interest. In this paper, we introduce a deep learning based\nreverse image search tool for histopathology images: Similar Medical Images\nLike Yours (SMILY). We assessed SMILY\u2019s ability to retrieve search results in\ntwo ways: using pathologist-provided annotations, and via prospective studies\nwhere pathologists evaluated the quality of SMILY search results. As a negative\ncontrol in the second evaluation, pathologists were blinded to whether search\nresults were retrieved by SMILY or randomly. In both types of assessments,\nSMILY was able to retrieve search results with similar histologic features,\norgan site, and prostate cancer Gleason grade compared with the original query.\nSMILY may be a useful general-purpose tool in the pathologist\u2019s arsenal, to\nimprove the efficiency of searching large archives of histopathology images,\nwithout the need to develop and implement specific tools for each application.</p>\n", "tags": ["Efficiency", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-51.89775466918945, 15.716231346130371], "cluster": 0}, {"key": "heimerl2019embcomp", "year": "2020", "citations": "19", "title": "Embcomp: Visual Interactive Comparison Of Vector Embeddings", "abstract": "<p>This paper introduces embComp, a novel approach for comparing two embeddings\nthat capture the similarity between objects, such as word and document\nembeddings. We survey scenarios where comparing these embedding spaces is\nuseful. From those scenarios, we derive common tasks, introduce visual analysis\nmethods that support these tasks, and combine them into a comprehensive system.\nOne of embComp\u2019s central features are overview visualizations that are based on\nmetrics for measuring differences in the local structure around objects.\nSummarizing these local metrics over the embeddings provides global overviews\nof similarities and differences. Detail views allow comparison of the local\nstructure around selected objects and relating this local information to the\nglobal views. Integrating and connecting all of these components, embComp\nsupports a range of analysis workflows that help understand similarities and\ndifferences between embedding spaces. We assess our approach by applying it in\nseveral use cases, including understanding corpora differences via word vector\nembeddings, and understanding algorithmic differences in generating embeddings.</p>\n", "tags": ["Survey-Paper", "Evaluation"], "tsne_embedding": [-35.590579986572266, -22.399320602416992], "cluster": 5}, {"key": "hekmatfar2020embedding", "year": "2021", "citations": "8", "title": "Embedding Ranking-oriented Recommender System Graphs", "abstract": "<p>Graph-based recommender systems (GRSs) analyze the structural information in\nthe graphical representation of data to make better recommendations, especially\nwhen the direct user-item relation data is sparse. Ranking-oriented GRSs that\nform a major class of recommendation systems, mostly use the graphical\nrepresentation of preference (or rank) data for measuring node similarities,\nfrom which they can infer a recommendation list using a neighborhood-based\nmechanism. In this paper, we propose PGRec, a novel graph-based\nranking-oriented recommendation framework. PGRec models the preferences of the\nusers over items, by a novel graph structure called PrefGraph. This graph is\nthen exploited by an improved embedding approach, taking advantage of both\nfactorization and deep learning methods, to extract vectors representing users,\nitems, and preferences. The resulting embedding are then used for predicting\nusers\u2019 unknown pairwise preferences from which the final recommendation lists\nare inferred. We have evaluated the performance of the proposed method against\nthe state of the art model-based and neighborhood-based recommendation methods,\nand our experiments show that PGRec outperforms the baseline algorithms up to\n3.2% in terms of NDCG@10 in different MovieLens datasets.</p>\n", "tags": ["Graph-Based-Ann", "Recommender-Systems", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [59.391414642333984, -9.51250171661377], "cluster": 9}, {"key": "hemati2020non", "year": "2022", "citations": "3", "title": "A Non-alternating Graph Hashing Algorithm For Large Scale Image Search", "abstract": "<p>In the era of big data, methods for improving memory and computational\nefficiency have become crucial for successful deployment of technologies.\nHashing is one of the most effective approaches to deal with computational\nlimitations that come with big data. One natural way for formulating this\nproblem is spectral hashing that directly incorporates affinity to learn binary\ncodes. However, due to binary constraints, the optimization becomes\nintractable. To mitigate this challenge, different relaxation approaches have\nbeen proposed to reduce the computational load of obtaining binary codes and\nstill attain a good solution. The problem with all existing relaxation methods\nis resorting to one or more additional auxiliary variables to attain high\nquality binary codes while relaxing the problem. The existence of auxiliary\nvariables leads to coordinate descent approach which increases the\ncomputational complexity. We argue that introducing these variables is\nunnecessary. To this end, we propose a novel relaxed formulation for spectral\nhashing that adds no additional variables to the problem. Furthermore, instead\nof solving the problem in original space where number of variables is equal to\nthe data points, we solve the problem in a much smaller space and retrieve the\nbinary codes from this solution. This trick reduces both the memory and\ncomputational complexity at the same time. We apply two optimization\ntechniques, namely projected gradient and optimization on manifold, to obtain\nthe solution. Using comprehensive experiments on four public datasets, we show\nthat the proposed efficient spectral hashing (ESH) algorithm achieves highly\ncompetitive retrieval performance compared with state of the art at low\ncomplexity.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [8.780659675598145, 22.87749481201172], "cluster": 4}, {"key": "hemati2021beyond", "year": "2021", "citations": "2", "title": "Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing", "abstract": "<p>An effective unsupervised hashing algorithm leads to compact binary codes preserving the neighborhood structure of data as much as possible. One of the most established schemes for unsupervised hashing is to reduce the dimensionality of data and then find a rigid (neighbourhood-preserving) transformation that reduces the quantization error. Although employing rigid transformations is effective, we may not reduce quantization loss to the ultimate limits. As well, reducing dimensionality and quantization loss in two separate steps seems to be sub-optimal. Motivated by these shortcomings, we propose to employ both rigid and non-rigid transformations to reduce quantization error and dimensionality simultaneously. We relax the orthogonality constraint on the projection in a PCA-formulation and regularize this by a quantization term. We show that both the non-rigid projection matrix and rotation matrix contribute towards minimizing quantization loss but in different ways. A scalable nested coordinate descent approach is proposed to optimize this mixed-integer optimization problem. We evaluate the proposed method on five public benchmark datasets providing almost half a million images. Comparative results indicate that the proposed method mostly outperforms state-of-art linear methods and competes with end-to-end deep solutions.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-8.565006256103516, 36.945899963378906], "cluster": 8}, {"key": "hemati2022learning", "year": "2023", "citations": "9", "title": "Learning Binary And Sparse Permutation-invariant Representations For Fast And Memory Efficient Whole Slide Image Search", "abstract": "<p>Learning suitable Whole slide images (WSIs) representations for efficient\nretrieval systems is a non-trivial task. The WSI embeddings obtained from\ncurrent methods are in Euclidean space not ideal for efficient WSI retrieval.\nFurthermore, most of the current methods require high GPU memory due to the\nsimultaneous processing of multiple sets of patches. To address these\nchallenges, we propose a novel framework for learning binary and sparse WSI\nrepresentations utilizing a deep generative modelling and the Fisher Vector. We\nintroduce new loss functions for learning sparse and binary\npermutation-invariant WSI representations that employ instance-based training\nachieving better memory efficiency. The learned WSI representations are\nvalidated on The Cancer Genomic Atlas (TCGA) and Liver-Kidney-Stomach (LKS)\ndatasets. The proposed method outperforms Yottixel (a recent search engine for\nhistopathology images) both in terms of retrieval accuracy and speed. Further,\nwe achieve competitive performance against SOTA on the public benchmark LKS\ndataset for WSI classification.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-46.423526763916016, 16.892955780029297], "cluster": 0}, {"key": "hemati2022non", "year": "2022", "citations": "3", "title": "A Non-alternating Graph Hashing Algorithm For Large Scale Image Search", "abstract": "<p>In the era of big data, methods for improving memory and computational efficiency have become crucial for successful deployment of technologies. Hashing is one of the most effective approaches to deal with computational limitations that come with big data. One natural way for formulating this problem is spectral hashing that directly incorporates affinity to learn binary codes. However, due to binary constraints, the optimization becomes intractable. To mitigate this challenge, different relaxation approaches have been proposed to reduce the computational load of obtaining binary codes and still attain a good solution. The problem with all existing relaxation methods is resorting to one or more additional auxiliary variables to attain high quality binary codes while relaxing the problem. The existence of auxiliary variables leads to coordinate descent approach which increases the computational complexity. We argue that introducing these variables is unnecessary. To this end, we propose a novel relaxed formulation for spectral hashing that adds no additional variables to the problem. Furthermore, instead of solving the problem in original space where number of variables is equal to the data points, we solve the problem in a much smaller space and retrieve the binary codes from this solution. This trick reduces both the memory and computational complexity at the same time. We apply two optimization techniques, namely projected gradient and optimization on manifold, to obtain the solution. Using comprehensive experiments on four public datasets, we show that the proposed efficient spectral hashing (ESH) algorithm achieves highly competitive retrieval performance compared with state of the art at low complexity.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [8.780367851257324, 22.879621505737305], "cluster": 4}, {"key": "hemati2025beyond", "year": "2021", "citations": "2", "title": "Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing", "abstract": "<p>An effective unsupervised hashing algorithm leads to compact binary codes preserving the neighborhood structure of data as much as possible. One of the most established schemes for unsupervised hashing is to reduce the dimensionality of data and then find a rigid (neighbourhood-preserving) transformation that reduces the quantization error. Although employing rigid transformations is effective, we may not reduce quantization loss to the ultimate limits. As well, reducing dimensionality and quantization loss in two separate steps seems to be sub-optimal. Motivated by these shortcomings, we propose to employ both rigid and non-rigid transformations to reduce quantization error and dimensionality simultaneously. We relax the orthogonality constraint on the projection in a PCA-formulation and regularize this by a quantization term. We show that both the non-rigid projection matrix and rotation matrix contribute towards minimizing quantization loss but in different ways. A scalable nested coordinate descent approach is proposed to optimize this mixed-integer optimization problem. We evaluate the proposed method on five public benchmark datasets providing almost half a million images. Comparative results indicate that the proposed method mostly outperforms state-of-art linear methods and competes with end-to-end deep solutions.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-8.564926147460938, 36.946537017822266], "cluster": 8}, {"key": "hemati2025non", "year": "2022", "citations": "3", "title": "A Non-alternating Graph Hashing Algorithm For Large Scale Image Search", "abstract": "<p>In the era of big data, methods for improving memory and computational efficiency have become crucial for successful deployment of technologies. Hashing is one of the most effective approaches to deal with computational limitations that come with big data. One natural way for formulating this problem is spectral hashing that directly incorporates affinity to learn binary codes. However, due to binary constraints, the optimization becomes intractable. To mitigate this challenge, different relaxation approaches have been proposed to reduce the computational load of obtaining binary codes and still attain a good solution. The problem with all existing relaxation methods is resorting to one or more additional auxiliary variables to attain high quality binary codes while relaxing the problem. The existence of auxiliary variables leads to coordinate descent approach which increases the computational complexity. We argue that introducing these variables is unnecessary. To this end, we propose a novel relaxed formulation for spectral hashing that adds no additional variables to the problem. Furthermore, instead of solving the problem in original space where number of variables is equal to the data points, we solve the problem in a much smaller space and retrieve the binary codes from this solution. This trick reduces both the memory and computational complexity at the same time. We apply two optimization techniques, namely projected gradient and optimization on manifold, to obtain the solution. Using comprehensive experiments on four public datasets, we show that the proposed efficient spectral hashing (ESH) algorithm achieves highly competitive retrieval performance compared with state of the art at low complexity.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [8.781505584716797, 22.87795639038086], "cluster": 4}, {"key": "hendriksen2021extending", "year": "2022", "citations": "14", "title": "Extending CLIP For Category-to-image Retrieval In E-commerce", "abstract": "<p>E-commerce provides rich multimodal data that is barely leveraged in\npractice. One aspect of this data is a category tree that is being used in\nsearch and recommendation. However, in practice, during a user\u2019s session there\nis often a mismatch between a textual and a visual representation of a given\ncategory. Motivated by the problem, we introduce the task of category-to-image\nretrieval in e-commerce and propose a model for the task, CLIP-ITA. The model\nleverages information from multiple modalities (textual, visual, and attribute\nmodality) to create product representations. We explore how adding information\nfrom multiple modalities (textual, visual, and attribute modality) impacts the\nmodel\u2019s performance. In particular, we observe that CLIP-ITA significantly\noutperforms a comparable model that leverages only the visual modality and a\ncomparable model that leverages the visual and attribute modality.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-16.681228637695312, -46.9493293762207], "cluster": 3}, {"key": "henkel2020supporting", "year": "2020", "citations": "5", "title": "Supporting Large-scale Image Recognition With Out-of-domain Samples", "abstract": "<p>This article presents an efficient end-to-end method to perform\ninstance-level recognition employed to the task of labeling and ranking\nlandmark images. In a first step, we embed images in a high dimensional feature\nspace using convolutional neural networks trained with an additive angular\nmargin loss and classify images using visual similarity. We then efficiently\nre-rank predictions and filter noise utilizing similarity to out-of-domain\nimages. Using this approach we achieved the 1st place in the 2020 edition of\nthe Google Landmark Recognition challenge.</p>\n", "tags": ["Scalability"], "tsne_embedding": [-15.549246788024902, -57.35251235961914], "cluster": 3}, {"key": "henkel2021efficient", "year": "2021", "citations": "4", "title": "Efficient Large-scale Image Retrieval With Deep Feature Orthogonality And Hybrid-swin-transformers", "abstract": "<p>We present an efficient end-to-end pipeline for largescale landmark\nrecognition and retrieval. We show how to combine and enhance concepts from\nrecent research in image retrieval and introduce two architectures especially\nsuited for large-scale landmark identification. A model with deep orthogonal\nfusion of local and global features (DOLG) using an EfficientNet backbone as\nwell as a novel Hybrid-Swin-Transformer is discussed and details how to train\nboth architectures efficiently using a step-wise approach and a sub-center\narcface loss with dynamic margins are provided. Furthermore, we elaborate a\nnovel discriminative re-ranking methodology for image retrieval. The\nsuperiority of our approach was demonstrated by winning the recognition and\nretrieval track of the Google Landmark Competition 2021.</p>\n", "tags": ["Re-Ranking", "Hybrid-Ann-Methods", "Image-Retrieval", "Scalability"], "tsne_embedding": [-15.236236572265625, -57.194087982177734], "cluster": 3}, {"key": "henkel2023annotation", "year": "2023", "citations": "1", "title": "Annotation Cost Efficient Active Learning For Content Based Image Retrieval", "abstract": "<p>Deep metric learning (DML) based methods have been found very effective for\ncontent-based image retrieval (CBIR) in remote sensing (RS). For accurately\nlearning the model parameters of deep neural networks, most of the DML methods\nrequire a high number of annotated training images, which can be costly to\ngather. To address this problem, in this paper we present an annotation cost\nefficient active learning (AL) method (denoted as ANNEAL). The proposed method\naims to iteratively enrich the training set by annotating the most informative\nimage pairs as similar or dissimilar, while accurately modelling a deep metric\nspace. This is achieved by two consecutive steps. In the first step the\npairwise image similarity is modelled based on the available training set.\nThen, in the second step the most uncertain and diverse (i.e., informative)\nimage pairs are selected to be annotated. Unlike the existing AL methods for\nCBIR, at each AL iteration of ANNEAL a human expert is asked to annotate the\nmost informative image pairs as similar/dissimilar. This significantly reduces\nthe annotation cost compared to annotating images with land-use/land cover\nclass labels. Experimental results show the effectiveness of our method. The\ncode of ANNEAL is publicly available at https://git.tu-berlin.de/rsim/ANNEAL.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-24.204402923583984, -12.23019790649414], "cluster": 1}, {"key": "heo2012spherical", "year": "2012", "citations": "380", "title": "Spherical Hashing", "abstract": "<p>Many binary code encoding schemes based on hashing\nhave been actively studied recently, since they can provide\nefficient similarity search, especially nearest neighbor\nsearch, and compact data representations suitable for handling\nlarge scale image databases in many computer vision\nproblems. Existing hashing techniques encode highdimensional\ndata points by using hyperplane-based hashing\nfunctions. In this paper we propose a novel hyperspherebased\nhashing function, spherical hashing, to map more\nspatially coherent data points into a binary code compared\nto hyperplane-based hashing functions. Furthermore, we\npropose a new binary code distance function, spherical\nHamming distance, that is tailored to our hyperspherebased\nbinary coding scheme, and design an efficient iterative\noptimization process to achieve balanced partitioning\nof data points for each hash function and independence between\nhashing functions. Our extensive experiments show\nthat our spherical hashing technique significantly outperforms\nsix state-of-the-art hashing techniques based on hyperplanes\nacross various image benchmarks of sizes ranging\nfrom one to 75 million of GIST descriptors. The performance\ngains are consistent and large, up to 100% improvements.\nThe excellent results confirm the unique merits of\nthe proposed idea in using hyperspheres to encode proximity\nregions in high-dimensional spaces. Finally, our method\nis intuitive and easy to implement.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Similarity-Search", "Compact-Codes", "Evaluation"], "tsne_embedding": [-2.0610952377319336, 38.38843536376953], "cluster": 4}, {"key": "heo2025spherical", "year": "2012", "citations": "380", "title": "Spherical Hashing", "abstract": "<p>Many binary code encoding schemes based on hashing\nhave been actively studied recently, since they can provide\nefficient similarity search, especially nearest neighbor\nsearch, and compact data representations suitable for handling\nlarge scale image databases in many computer vision\nproblems. Existing hashing techniques encode highdimensional\ndata points by using hyperplane-based hashing\nfunctions. In this paper we propose a novel hyperspherebased\nhashing function, spherical hashing, to map more\nspatially coherent data points into a binary code compared\nto hyperplane-based hashing functions. Furthermore, we\npropose a new binary code distance function, spherical\nHamming distance, that is tailored to our hyperspherebased\nbinary coding scheme, and design an efficient iterative\noptimization process to achieve balanced partitioning\nof data points for each hash function and independence between\nhashing functions. Our extensive experiments show\nthat our spherical hashing technique significantly outperforms\nsix state-of-the-art hashing techniques based on hyperplanes\nacross various image benchmarks of sizes ranging\nfrom one to 75 million of GIST descriptors. The performance\ngains are consistent and large, up to 100% improvements.\nThe excellent results confirm the unique merits of\nthe proposed idea in using hyperspheres to encode proximity\nregions in high-dimensional spaces. Finally, our method\nis intuitive and easy to implement.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Similarity-Search", "Compact-Codes", "Evaluation"], "tsne_embedding": [-2.060939073562622, 38.38865661621094], "cluster": 4}, {"key": "herath2021em", "year": "2021", "citations": "0", "title": "Em-k Indexing For Approximate Query Matching In Large-scale ER", "abstract": "<p>Accurate and efficient entity resolution (ER) is a significant challenge in\nmany data mining and analysis projects requiring integrating and processing\nmassive data collections. It is becoming increasingly important in real-world\napplications to develop ER solutions that produce prompt responses for entity\nqueries on large-scale databases. Some of these applications demand entity\nquery matching against large-scale reference databases within a short time. We\ndefine this as the query matching problem in ER in this work. Indexing or\nblocking techniques reduce the search space and execution time in the ER\nprocess. However, approximate indexing techniques that scale to very\nlarge-scale datasets remain open to research. In this paper, we investigate the\nquery matching problem in ER to propose an indexing method suitable for\napproximate and efficient query matching.\n  We first use spatial mappings to embed records in a multidimensional\nEuclidean space that preserves the domain-specific similarity. Among the\nvarious mapping techniques, we choose multidimensional scaling. Then using a\nKd-tree and the nearest neighbour search, the method returns a block of records\nthat includes potential matches for a query. Our method can process queries\nagainst a large-scale dataset using only a fraction of the data \\(L\\) (given the\ndataset size is \\(N\\)), with a \\(O(L^2)\\) complexity where \\(L \\ll N\\). The\nexperiments conducted on several datasets showed the effectiveness of the\nproposed method.</p>\n", "tags": ["Tree-Based-Ann", "Similarity-Search", "Scalability", "Datasets"], "tsne_embedding": [26.34591293334961, 34.34025192260742], "cluster": 4}, {"key": "hirota2019emu", "year": "2020", "citations": "5", "title": "Emu: Enhancing Multilingual Sentence Embeddings With Semantic Specialization", "abstract": "<p>We present Emu, a system that semantically enhances multilingual sentence\nembeddings. Our framework fine-tunes pre-trained multilingual sentence\nembeddings using two main components: a semantic classifier and a language\ndiscriminator. The semantic classifier improves the semantic similarity of\nrelated sentences, whereas the language discriminator enhances the\nmultilinguality of the embeddings via multilingual adversarial training. Our\nexperimental results based on several language pairs show that our specialized\nembeddings outperform the state-of-the-art multilingual sentence embedding\nmodel on the task of cross-lingual intent classification using only monolingual\nlabeled data.</p>\n", "tags": ["AAAI", "Tools-&-Libraries", "Robustness"], "tsne_embedding": [-3.6049320697784424, -38.09602355957031], "cluster": 3}, {"key": "hoang2017enhance", "year": "2017", "citations": "4", "title": "Enhance Feature Discrimination For Unsupervised Hashing", "abstract": "<p>We introduce a novel approach to improve unsupervised hashing. Specifically,\nwe propose a very efficient embedding method: Gaussian Mixture Model embedding\n(Gemb). The proposed method, using Gaussian Mixture Model, embeds feature\nvector into a low-dimensional vector and, simultaneously, enhances the\ndiscriminative property of features before passing them into hashing. Our\nexperiment shows that the proposed method boosts the hashing performance of\nmany state-of-the-art, e.g. Binary Autoencoder (BA) [1], Iterative Quantization\n(ITQ) [2], in standard evaluation metrics for the three main benchmark\ndatasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-12.53622817993164, 11.42314338684082], "cluster": 8}, {"key": "hoang2017selective", "year": "2017", "citations": "58", "title": "Selective Deep Convolutional Features For Image Retrieval", "abstract": "<p>Convolutional Neural Network (CNN) is a very powerful approach to extract\ndiscriminative local descriptors for effective image search. Recent work adopts\nfine-tuned strategies to further improve the discriminative power of the\ndescriptors. Taking a different approach, in this paper, we propose a novel\nframework to achieve competitive retrieval performance. Firstly, we propose\nvarious masking schemes, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and remove a large number\nof redundant features. We demonstrate that this can effectively address the\nburstiness issue and improve retrieval accuracy. Secondly, we propose to employ\nrecent embedding and aggregating methods to further enhance feature\ndiscriminability. Extensive experiments demonstrate that our proposed framework\nachieves state-of-the-art retrieval accuracy.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-50.80150604248047, 0.4669431447982788], "cluster": 0}, {"key": "hoang2018simultaneous", "year": "2019", "citations": "11", "title": "Simultaneous Compression And Quantization: A Joint Approach For Efficient Unsupervised Hashing", "abstract": "<p>For unsupervised data-dependent hashing, the two most important requirements\nare to preserve similarity in the low-dimensional feature space and to minimize\nthe binary quantization loss. A well-established hashing approach is Iterative\nQuantization (ITQ), which addresses these two requirements in separate steps.\nIn this paper, we revisit the ITQ approach and propose novel formulations and\nalgorithms to the problem. Specifically, we propose a novel approach, named\nSimultaneous Compression and Quantization (SCQ), to jointly learn to compress\n(reduce dimensionality) and binarize input data in a single formulation under\nstrict orthogonal constraint. With this approach, we introduce a loss function\nand its relaxed version, termed Orthonormal Encoder (OnE) and Orthogonal\nEncoder (OgE) respectively, which involve challenging binary and orthogonal\nconstraints. We propose to attack the optimization using novel algorithms based\non recent advances in cyclic coordinate descent approach. Comprehensive\nexperiments on unsupervised image retrieval demonstrate that our proposed\nmethods consistently outperform other state-of-the-art hashing methods.\nNotably, our proposed methods outperform recent deep neural networks and GAN\nbased hashing in accuracy, while being very computationally-efficient.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-6.693157196044922, 16.830625534057617], "cluster": 8}, {"key": "hoang2020unsupervised", "year": "2020", "citations": "28", "title": "Unsupervised Deep Cross-modality Spectral Hashing", "abstract": "<p>This paper presents a novel framework, namely Deep Cross-modality Spectral\nHashing (DCSH), to tackle the unsupervised learning problem of binary hash\ncodes for efficient cross-modal retrieval. The framework is a two-step hashing\napproach which decouples the optimization into (1) binary optimization and (2)\nhashing function learning. In the first step, we propose a novel spectral\nembedding-based algorithm to simultaneously learn single-modality and binary\ncross-modality representations. While the former is capable of well preserving\nthe local structure of each modality, the latter reveals the hidden patterns\nfrom all modalities. In the second step, to learn mapping functions from\ninformative data inputs (images and word embeddings) to binary codes obtained\nfrom the first step, we leverage the powerful CNN for images and propose a\nCNN-based deep architecture to learn text modality. Quantitative evaluations on\nthree standard benchmark datasets demonstrate that the proposed DCSH method\nconsistently outperforms other state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [-48.84317398071289, -2.363879919052124], "cluster": 0}, {"key": "hoang2021multi", "year": "2021", "citations": "3", "title": "Multi-modal Mutual Information Maximization: A Novel Approach For Unsupervised Deep Cross-modal Hashing", "abstract": "<p>In this paper, we adopt the maximizing mutual information (MI) approach to\ntackle the problem of unsupervised learning of binary hash codes for efficient\ncross-modal retrieval. We proposed a novel method, dubbed Cross-Modal Info-Max\nHashing (CMIMH). First, to learn informative representations that can preserve\nboth intra- and inter-modal similarities, we leverage the recent advances in\nestimating variational lower-bound of MI to maximize the MI between the binary\nrepresentations and input features and between binary representations of\ndifferent modalities. By jointly maximizing these MIs under the assumption that\nthe binary representations are modelled by multivariate Bernoulli\ndistributions, we can learn binary representations, which can preserve both\nintra- and inter-modal similarities, effectively in a mini-batch manner with\ngradient descent. Furthermore, we find out that trying to minimize the modality\ngap by learning similar binary representations for the same instance from\ndifferent modalities could result in less informative representations. Hence,\nbalancing between reducing the modality gap and losing modality-private\ninformation is important for the cross-modal retrieval tasks. Quantitative\nevaluations on standard benchmark datasets demonstrate that the proposed method\nconsistently outperforms other state-of-the-art cross-modal retrieval methods.</p>\n", "tags": ["Hashing-Methods", "Multimodal-Retrieval", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [-6.747231960296631, 18.123003005981445], "cluster": 8}, {"key": "hoe2021one", "year": "2021", "citations": "46", "title": "One Loss For All: Deep Hashing With A Single Cosine Similarity Based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives: to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality, it is not uncommon for existing models to employ a large number (&gt;4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work, we propose a novel deep hashing model with only a single learning objective. Specifically, we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further, with this learning objective, code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly, extensive experiments show that our model is highly effective, outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Quantization", "Scalability", "Neural-Hashing"], "tsne_embedding": [16.01714515686035, 4.1635050773620605], "cluster": 6}, {"key": "hoe2025one", "year": "2021", "citations": "46", "title": "One Loss For All: Deep Hashing With A Single Cosine Similarity Based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives: to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality, it is not uncommon for existing models to employ a large number (&gt;4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work, we propose a novel deep hashing model with only a single learning objective. Specifically, we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further, with this learning objective, code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly, extensive experiments show that our model is highly effective, outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Quantization", "Scalability", "Neural-Hashing"], "tsne_embedding": [16.017189025878906, 4.16347074508667], "cluster": 6}, {"key": "hoffer2016semi", "year": "2016", "citations": "28", "title": "Semi-supervised Deep Learning By Metric Embedding", "abstract": "<p>Deep networks are successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are usually much less suited for semi-supervised\nproblems because of their tendency to overfit easily when trained on small\namounts of data. In this work we will explore a new training objective that is\ntargeting a semi-supervised regime with only a small subset of labeled data.\nThis criterion is based on a deep metric embedding over distance relations\nwithin the set of labeled samples, together with constraints over the\nembeddings of the unlabeled set. The final learned representations are\ndiscriminative in euclidean space, and hence can be used with subsequent\nnearest-neighbor classification using the labeled samples.</p>\n", "tags": ["Supervised"], "tsne_embedding": [-10.326730728149414, -16.086721420288086], "cluster": 1}, {"key": "hong2017content", "year": "2017", "citations": "10", "title": "Content-based Video-music Retrieval Using Soft Intra-modal Structure Constraint", "abstract": "<p>Up to now, only limited research has been conducted on cross-modal retrieval\nof suitable music for a specified video or vice versa. Moreover, much of the\nexisting research relies on metadata such as keywords, tags, or associated\ndescription that must be individually produced and attached posterior. This\npaper introduces a new content-based, cross-modal retrieval method for video\nand music that is implemented through deep neural networks. We train the\nnetwork via inter-modal ranking loss such that videos and music with similar\nsemantics end up close together in the embedding space. However, if only the\ninter-modal ranking constraint is used for embedding, modality-specific\ncharacteristics can be lost. To address this problem, we propose a novel soft\nintra-modal structure loss that leverages the relative distance relationship\nbetween intra-modal samples before embedding. We also introduce reasonable\nquantitative and qualitative experimental protocols to solve the lack of\nstandard protocols for less-mature video-music related tasks. Finally, we\nconstruct a large-scale 200K video-music pair benchmark. All the datasets and\nsource code can be found in our online repository\n(https://github.com/csehong/VM-NET).</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [3.1431169509887695, -48.621273040771484], "cluster": 3}, {"key": "horiuchi2022similarity", "year": "2022", "citations": "0", "title": "Similarity Search On Computational Notebooks", "abstract": "<p>Computational notebook software such as Jupyter Notebook is popular for data\nscience tasks. Numerous computational notebooks are available on the Web and\nreusable; however, searching for computational notebooks manually is a tedious\ntask, and so far, there are no tools to search for computational notebooks\neffectively and efficiently. In this paper, we propose a similarity search on\ncomputational notebooks and develop a new framework for the similarity search.\nGiven contents (i.e., source codes, tabular data, libraries, and outputs\nformats) in computational notebooks as a query, the similarity search problem\naims to find top-k computational notebooks with the most similar contents. We\ndefine two similarity measures; set-based and graph-based similarities.\nSet-based similarity handles each content independently, while graph-based\nsimilarity captures the relationships between contents. Our framework can\neffectively prune the candidates of computational notebooks that should not be\nin the top-k results. Furthermore, we develop optimization techniques such as\ncaching and indexing to accelerate the search. Experiments using Kaggle\nnotebooks show that our method, in particular graph-based similarity, can\nachieve high accuracy and high efficiency.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Tools-&-Libraries", "Similarity-Search"], "tsne_embedding": [59.41726303100586, 4.627188205718994], "cluster": 9}, {"key": "hou2023semstamp", "year": "2024", "citations": "7", "title": "Semstamp: A Semantic Watermark With Paraphrastic Robustness For Text Generation", "abstract": "<p>Existing watermarking algorithms are vulnerable to paraphrase attacks because\nof their token-level design. To address this issue, we propose SemStamp, a\nrobust sentence-level semantic watermarking algorithm based on\nlocality-sensitive hashing (LSH), which partitions the semantic space of\nsentences. The algorithm encodes and LSH-hashes a candidate sentence generated\nby an LLM, and conducts sentence-level rejection sampling until the sampled\nsentence falls in watermarked partitions in the semantic embedding space. A\nmargin-based constraint is used to enhance its robustness. To show the\nadvantages of our algorithm, we propose a \u201cbigram\u201d paraphrase attack using the\nparaphrase that has the fewest bigram overlaps with the original sentence. This\nattack is shown to be effective against the existing token-level watermarking\nmethod. Experimental results show that our novel semantic watermark algorithm\nis not only more robust than the previous state-of-the-art method on both\ncommon and bigram paraphrase attacks, but also is better at preserving the\nquality of generation.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Robustness"], "tsne_embedding": [11.78041934967041, -21.396026611328125], "cluster": 7}, {"key": "hou2024bridging", "year": "2024", "citations": "11", "title": "Bridging Language And Items For Retrieval And Recommendation", "abstract": "<p>This paper introduces BLaIR, a series of pretrained sentence embedding models\nspecialized for recommendation scenarios. BLaIR is trained to learn\ncorrelations between item metadata and potential natural language context,\nwhich is useful for retrieving and recommending items. To pretrain BLaIR, we\ncollect Amazon Reviews 2023, a new dataset comprising over 570 million reviews\nand 48 million items from 33 categories, significantly expanding beyond the\nscope of previous versions. We evaluate the generalization ability of BLaIR\nacross multiple domains and tasks, including a new task named complex product\nsearch, referring to retrieving relevant items given long, complex natural\nlanguage contexts. Leveraging large language models like ChatGPT, we\ncorrespondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical\nresults on the new task, as well as conventional retrieval and recommendation\ntasks, demonstrate that BLaIR exhibit strong text and item representation\ncapacity. Our datasets, code, and checkpoints are available at:\nhttps://github.com/hyp1231/AmazonReviews2023.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [-15.802125930786133, -34.37608337402344], "cluster": 3}, {"key": "hou2024k", "year": "2024", "citations": "2", "title": "K-semstamp: A Clustering-based Semantic Watermark For Detection Of Machine-generated Text", "abstract": "<p>Recent watermarked generation algorithms inject detectable signatures during\nlanguage generation to facilitate post-hoc detection. While token-level\nwatermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023)\napplies watermark on the semantic representation of sentences and demonstrates\npromising robustness. SemStamp employs locality-sensitive hashing (LSH) to\npartition the semantic space with arbitrary hyperplanes, which results in a\nsuboptimal tradeoff between robustness and speed. We propose k-SemStamp, a\nsimple yet effective enhancement of SemStamp, utilizing k-means clustering as\nan alternative of LSH to partition the embedding space with awareness of\ninherent semantic structure. Experimental results indicate that k-SemStamp\nsaliently improves its robustness and sampling efficiency while preserving the\ngeneration quality, advancing a more effective tool for machine-generated text\ndetection.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Robustness"], "tsne_embedding": [11.790932655334473, -21.78618049621582], "cluster": 7}, {"key": "hsieh2016fast", "year": "2016", "citations": "0", "title": "Fast Binary Embedding Via Circulant Downsampled Matrix -- A Data-independent Approach", "abstract": "<p>Binary embedding of high-dimensional data aims to produce low-dimensional\nbinary codes while preserving discriminative power. State-of-the-art methods\noften suffer from high computation and storage costs. We present a simple and\nfast embedding scheme by first downsampling N-dimensional data into\nM-dimensional data and then multiplying the data with an MxM circulant matrix.\nOur method requires O(N +M log M) computation and O(N) storage costs. We prove\nif data have sparsity, our scheme can achieve similarity-preserving well.\nExperiments further demonstrate that though our method is cost-effective and\nfast, it still achieves comparable performance in image applications.</p>\n", "tags": ["Compact-Codes", "Memory-Efficiency", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-9.582707405090332, 21.225149154663086], "cluster": 8}, {"key": "hsu2018unsupervised", "year": "2018", "citations": "26", "title": "Unsupervised Multimodal Representation Learning Across Medical Images And Reports", "abstract": "<p>Joint embeddings between medical imaging modalities and associated radiology\nreports have the potential to offer significant benefits to the clinical\ncommunity, ranging from cross-domain retrieval to conditional generation of\nreports to the broader goals of multimodal representation learning. In this\nwork, we establish baseline joint embedding results measured via both local and\nglobal retrieval methods on the soon to be released MIMIC-CXR dataset\nconsisting of both chest X-ray images and the associated radiology reports. We\nexamine both supervised and unsupervised methods on this task and show that for\ndocument retrieval tasks with the learned representations, only a limited\namount of supervision is needed to yield results comparable to those of\nfully-supervised methods.</p>\n", "tags": ["Supervised", "Unsupervised", "Text-Retrieval", "Datasets"], "tsne_embedding": [-54.61044692993164, 15.558707237243652], "cluster": 0}, {"key": "hu2017learning", "year": "2017", "citations": "206", "title": "Learning Discrete Representations Via Information Maximizing Self-augmented Training", "abstract": "<p>Learning discrete representations of data is a central machine learning task\nbecause of the compactness of the representations and ease of interpretation.\nThe task includes clustering and hash learning as special cases. Deep neural\nnetworks are promising to be used because they can model the non-linearity of\ndata and scale to large datasets. However, their model complexity is huge, and\ntherefore, we need to carefully regularize the networks in order to learn\nuseful representations that exhibit intended invariance for applications of\ninterest. To this end, we propose a method called Information Maximizing\nSelf-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose\nthe invariance on discrete representations. More specifically, we encourage the\npredicted representations of augmented data points to be close to those of the\noriginal data points in an end-to-end fashion. At the same time, we maximize\nthe information-theoretic dependency between data and their predicted discrete\nrepresentations. Extensive experiments on benchmark datasets show that IMSAT\nproduces state-of-the-art results for both clustering and unsupervised hash\nlearning.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Unsupervised", "Datasets"], "tsne_embedding": [31.79960823059082, -9.579512596130371], "cluster": 7}, {"key": "hu2017supervised", "year": "2017", "citations": "0", "title": "Supervised Hashing Based On Energy Minimization", "abstract": "<p>Recently, supervised hashing methods have attracted much attention since they\ncan optimize retrieval speed and storage cost while preserving semantic\ninformation. Because hashing codes learning is NP-hard, many methods resort to\nsome form of relaxation technique. But the performance of these methods can\neasily deteriorate due to the relaxation. Luckily, many supervised hashing\nformulations can be viewed as energy functions, hence solving hashing codes is\nequivalent to learning marginals in the corresponding conditional random field\n(CRF). By minimizing the KL divergence between a fully factorized distribution\nand the Gibbs distribution of this CRF, a set of consistency equations can be\nobtained, but updating them in parallel may not yield a local optimum since the\nvariational lower bound is not guaranteed to increase. In this paper, we use a\nlinear approximation of the sigmoid function to convert these consistency\nequations to linear systems, which have a closed-form solution. By applying\nthis novel technique to two classical hashing formulations KSH and SPLH, we\nobtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH.\nExperimental results on three datasets show the superiority of our methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [18.27781867980957, 5.885992050170898], "cluster": 6}, {"key": "hu2018deep", "year": "2018", "citations": "0", "title": "Deep LDA Hashing", "abstract": "<p>The conventional supervised hashing methods based on classification do not\nentirely meet the requirements of hashing technique, but Linear Discriminant\nAnalysis (LDA) does. In this paper, we propose to perform a revised LDA\nobjective over deep networks to learn efficient hashing codes in a truly\nend-to-end fashion. However, the complicated eigenvalue decomposition within\neach mini-batch in every epoch has to be faced with when simply optimizing the\ndeep network w.r.t. the LDA objective. In this work, the revised LDA objective\nis transformed into a simple least square problem, which naturally overcomes\nthe intractable problems and can be easily solved by the off-the-shelf\noptimizer. Such deep extension can also overcome the weakness of LDA Hashing in\nthe limited linear projection and feature learning. Amounts of experiments are\nconducted on three benchmark datasets. The proposed Deep LDA Hashing shows\nnearly 70 points improvement over the conventional one on the CIFAR-10 dataset.\nIt also beats several state-of-the-art methods on various metrics.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [21.715856552124023, 0.7172747850418091], "cluster": 6}, {"key": "hu2018hashing", "year": "2018", "citations": "42", "title": "From Hashing To Cnns: Training Binaryweight Networks Via Hashing", "abstract": "<p>Deep convolutional neural networks (CNNs) have shown appealing performance on\nvarious computer vision tasks in recent years. This motivates people to deploy\nCNNs to realworld applications. However, most of state-of-art CNNs require\nlarge memory and computational resources, which hinders the deployment on\nmobile devices. Recent studies show that low-bit weight representation can\nreduce much storage and memory demand, and also can achieve efficient network\ninference. To achieve this goal, we propose a novel approach named BWNH to\ntrain Binary Weight Networks via Hashing. In this paper, we first reveal the\nstrong connection between inner-product preserving hashing and binary weight\nnetworks, and show that training binary weight networks can be intrinsically\nregarded as a hashing problem. Based on this perspective, we propose an\nalternating optimization method to learn the hash codes instead of directly\nlearning binary weights. Extensive experiments on CIFAR10, CIFAR100 and\nImageNet demonstrate that our proposed BWNH outperforms current state-of-art by\na large margin.</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [-48.05379104614258, 5.070827960968018], "cluster": 0}, {"key": "hu2018web", "year": "2018", "citations": "49", "title": "Web-scale Responsive Visual Search At Bing", "abstract": "<p>In this paper, we introduce a web-scale general visual search system deployed\nin Microsoft Bing. The system accommodates tens of billions of images in the\nindex, with thousands of features for each image, and can respond in less than\n200 ms. In order to overcome the challenges in relevance, latency, and\nscalability in such large scale of data, we employ a cascaded learning-to-rank\nframework based on various latest deep learning visual features, and deploy in\na distributed heterogeneous computing platform. Quantitative and qualitative\nexperiments show that our system is able to support various applications on\nBing website and apps.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Tools-&-Libraries", "Large-Scale-Search", "KDD"], "tsne_embedding": [-16.10630226135254, -38.42390441894531], "cluster": 3}, {"key": "hu2019separated", "year": "2019", "citations": "30", "title": "Separated Variational Hashing Networks For Cross-modal Retrieval", "abstract": "<p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Similarity-Search", "Multimodal-Retrieval", "Memory-Efficiency", "Compact-Codes", "Evaluation"], "tsne_embedding": [5.353384494781494, 5.282506465911865], "cluster": 6}, {"key": "hu2020creating", "year": "2020", "citations": "117", "title": "Creating Something From Nothing: Unsupervised Knowledge Distillation For Cross-modal Hashing", "abstract": "<p>In recent years, cross-modal hashing (CMH) has attracted increasing attentions, mainly because its potential\nability of mapping contents from different modalities, especially in vision and language, into the same space, so that\nit becomes efficient in cross-modal data retrieval. There are\ntwo main frameworks for CMH, differing from each other in\nwhether semantic supervision is required. Compared to the\nunsupervised methods, the supervised methods often enjoy\nmore accurate results, but require much heavier labors in\ndata annotation. In this paper, we propose a novel approach\nthat enables guiding a supervised method using outputs produced by an unsupervised method. Specifically, we make\nuse of teacher-student optimization for propagating knowledge. Experiments are performed on two popular CMH\nbenchmarks, i.e., the MIRFlickr and NUS-WIDE datasets.\nOur approach outperforms all existing unsupervised methods by a large margin</p>\n", "tags": ["Hashing-Methods", "CVPR", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [27.8870792388916, -36.96365737915039], "cluster": 7}, {"key": "hu2020dasgil", "year": "2020", "citations": "48", "title": "DASGIL: Domain Adaptation For Semantic And Geometric-aware Image-based Localization", "abstract": "<p>Long-Term visual localization under changing environments is a challenging\nproblem in autonomous driving and mobile robotics due to season, illumination\nvariance, etc. Image retrieval for localization is an efficient and effective\nsolution to the problem. In this paper, we propose a novel multi-task\narchitecture to fuse the geometric and semantic information into the\nmulti-scale latent embedding representation for visual place recognition. To\nuse the high-quality ground truths without any human effort, the effective\nmulti-scale feature discriminator is proposed for adversarial training to\nachieve the domain adaptation from synthetic virtual KITTI dataset to\nreal-world KITTI dataset. The proposed approach is validated on the Extended\nCMU-Seasons dataset and Oxford RobotCar dataset through a series of crucial\ncomparison experiments, where our performance outperforms state-of-the-art\nbaselines for retrieval-based localization and large-scale place recognition\nunder the challenging environment.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-39.24625015258789, -7.62685489654541], "cluster": 0}, {"key": "hu2020efficient", "year": "2020", "citations": "0", "title": "Efficient Approximate Nearest Neighbor Search For Multiple Weighted \\(l_{p\\leq2}\\) Distance Functions", "abstract": "<p>Nearest neighbor search is fundamental to a wide range of applications. Since\nthe exact nearest neighbor search suffers from the \u201ccurse of dimensionality\u201d,\napproximate approaches, such as Locality-Sensitive Hashing (LSH), are widely\nused to trade a little query accuracy for a much higher query efficiency. In\nmany scenarios, it is necessary to perform nearest neighbor search under\nmultiple weighted distance functions in high-dimensional spaces. This paper\nconsiders the important problem of supporting efficient approximate nearest\nneighbor search for multiple weighted distance functions in high-dimensional\nspaces. To the best of our knowledge, prior work can only solve the problem for\nthe \\(l_2\\) distance. However, numerous studies have shown that the \\(l_p\\)\ndistance with \\(p\\in(0,2)\\) could be more effective than the \\(l_2\\) distance in\nhigh-dimensional spaces. We propose a novel method, WLSH, to address the\nproblem for the \\(l_p\\) distance for \\(p\\in(0,2]\\). WLSH takes the LSH approach and\ncan theoretically guarantee both the efficiency of processing queries and the\naccuracy of query results while minimizing the required total number of hash\ntables. We conduct extensive experiments on synthetic and real data sets, and\nthe results show that WLSH achieves high performance in terms of query\nefficiency, query accuracy and space consumption.</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [20.292865753173828, 42.14514923095703], "cluster": 4}, {"key": "hu2020pyretri", "year": "2020", "citations": "18", "title": "Pyretri: A Pytorch-based Library For Unsupervised Image Retrieval By Deep Convolutional Neural Networks", "abstract": "<p>Despite significant progress of applying deep learning methods to the field\nof content-based image retrieval, there has not been a software library that\ncovers these methods in a unified manner. In order to fill this gap, we\nintroduce PyRetri, an open source library for deep learning based unsupervised\nimage retrieval. The library encapsulates the retrieval process in several\nstages and provides functionality that covers various prominent methods for\neach stage. The idea underlying its design is to provide a unified platform for\ndeep learning based image retrieval research, with high usability and\nextensibility. To the best of our knowledge, this is the first open-source\nlibrary for unsupervised image retrieval by deep learning.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Unsupervised"], "tsne_embedding": [-5.700814247131348, -6.692312717437744], "cluster": 1}, {"key": "hu2022badhash", "year": "2022", "citations": "26", "title": "Badhash: Invisible Backdoor Attacks Against Deep Hashing With Clean Label", "abstract": "<p>Due to its powerful feature learning capability and high efficiency, deep\nhashing has achieved great success in large-scale image retrieval. Meanwhile,\nextensive works have demonstrated that deep neural networks (DNNs) are\nsusceptible to adversarial examples, and exploring adversarial attack against\ndeep hashing has attracted many research efforts. Nevertheless, backdoor\nattack, another famous threat to DNNs, has not been studied for deep hashing\nyet. Although various backdoor attacks have been proposed in the field of image\nclassification, existing approaches failed to realize a truly imperceptive\nbackdoor attack that enjoys invisible triggers and clean label setting\nsimultaneously, and they also cannot meet the intrinsic demand of image\nretrieval backdoor. In this paper, we propose BadHash, the first\ngenerative-based imperceptible backdoor attack against deep hashing, which can\neffectively generate invisible and input-specific poisoned images with clean\nlabel. Specifically, we first propose a new conditional generative adversarial\nnetwork (cGAN) pipeline to effectively generate poisoned samples. For any given\nbenign image, it seeks to generate a natural-looking poisoned counterpart with\na unique invisible trigger. In order to improve the attack effectiveness, we\nintroduce a label-based contrastive learning network LabCLN to exploit the\nsemantic characteristics of different labels, which are subsequently used for\nconfusing and misleading the target model to learn the embedded trigger. We\nfinally explore the mechanism of backdoor attacks on image retrieval in the\nhash space. Extensive experiments on multiple benchmark datasets verify that\nBadHash can generate imperceptible poisoned samples with strong attack ability\nand transferability over state-of-the-art deep hashing schemes.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Image-Retrieval", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-0.8393540978431702, 12.549225807189941], "cluster": 8}, {"key": "hu2022content", "year": "2022", "citations": "0", "title": "Content-based Landmark Retrieval Combining Global And Local Features Using Siamese Neural Networks", "abstract": "<p>In this work, we present a method for landmark retrieval that utilizes global\nand local features. A Siamese network is used for global feature extraction and\nmetric learning, which gives an initial ranking of the landmark search. We\nutilize the extracted feature maps from the Siamese architecture as local\ndescriptors, the search results are then further refined using a cosine\nsimilarity between local descriptors. We conduct a deeper analysis of the\nGoogle Landmark Dataset, which is used for evaluation, and augment the dataset\nto handle various intra-class variances. Furthermore, we conduct several\nexperiments to compare the effects of transfer learning and metric learning, as\nwell as experiments using other local descriptors. We show that a re-ranking\nusing local features can improve the search results. We believe that the\nproposed local feature extraction using cosine similarity is a simple approach\nthat can be extended to many other retrieval tasks.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [1.4538662433624268, -27.97908592224121], "cluster": 3}, {"key": "hu2022feature", "year": "2022", "citations": "12", "title": "Feature Representation Learning For Unsupervised Cross-domain Image Retrieval", "abstract": "<p>Current supervised cross-domain image retrieval methods can achieve excellent\nperformance. However, the cost of data collection and labeling imposes an\nintractable barrier to practical deployment in real applications. In this\npaper, we investigate the unsupervised cross-domain image retrieval task, where\nclass labels and pairing annotations are no longer a prerequisite for training.\nThis is an extremely challenging task because there is no supervision for both\nin-domain feature representation learning and cross-domain alignment. We\naddress both challenges by introducing: 1) a new cluster-wise contrastive\nlearning mechanism to help extract class semantic-aware features, and 2) a\nnovel distance-of-distance loss to effectively measure and minimize the domain\ndiscrepancy without any external supervision. Experiments on the Office-Home\nand DomainNet datasets consistently show the superior image retrieval\naccuracies of our framework over state-of-the-art approaches. Our source code\ncan be found at https://github.com/conghuihu/UCDIR.</p>\n", "tags": ["Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-31.977108001708984, -2.8664028644561768], "cluster": 0}, {"key": "hu2025creating", "year": "2020", "citations": "117", "title": "Creating Something From Nothing: Unsupervised Knowledge Distillation For Cross-modal Hashing", "abstract": "<p>In recent years, cross-modal hashing (CMH) has attracted increasing attentions, mainly because its potential\nability of mapping contents from different modalities, especially in vision and language, into the same space, so that\nit becomes efficient in cross-modal data retrieval. There are\ntwo main frameworks for CMH, differing from each other in\nwhether semantic supervision is required. Compared to the\nunsupervised methods, the supervised methods often enjoy\nmore accurate results, but require much heavier labors in\ndata annotation. In this paper, we propose a novel approach\nthat enables guiding a supervised method using outputs produced by an unsupervised method. Specifically, we make\nuse of teacher-student optimization for propagating knowledge. Experiments are performed on two popular CMH\nbenchmarks, i.e., the MIRFlickr and NUS-WIDE datasets.\nOur approach outperforms all existing unsupervised methods by a large margin</p>\n", "tags": ["Hashing-Methods", "CVPR", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [27.8870792388916, -36.963661193847656], "cluster": 7}, {"key": "hu2025separated", "year": "2019", "citations": "30", "title": "Separated Variational Hashing Networks For Cross-modal Retrieval", "abstract": "<p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Similarity-Search", "Multimodal-Retrieval", "Memory-Efficiency", "Compact-Codes", "Evaluation"], "tsne_embedding": [5.353365421295166, 5.282502174377441], "cluster": 6}, {"key": "huang2016local", "year": "2016", "citations": "124", "title": "Local Similarity-aware Deep Feature Embedding", "abstract": "<p>Existing deep embedding methods in vision tasks are capable of learning a\ncompact Euclidean space from images, where Euclidean distances correspond to a\nsimilarity metric. To make learning more effective and efficient, hard sample\nmining is usually employed, with samples identified through computing the\nEuclidean feature distance. However, the global Euclidean distance cannot\nfaithfully characterize the true feature similarity in a complex visual feature\nspace, where the intraclass distance in a high-density region may be larger\nthan the interclass distance in low-density regions. In this paper, we\nintroduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of\nlearning a similarity metric adaptive to local feature structure. The metric\ncan be used to select genuinely hard samples in a local neighborhood to guide\nthe deep embedding learning in an online and robust manner. The new layer is\nappealing in that it is pluggable to any convolutional networks and is trained\nend-to-end. Our local similarity-aware feature embedding not only demonstrates\nfaster convergence and boosted performance on two complex image retrieval\ndatasets, its large margin nature also leads to superior generalization results\nunder the large and open set scenarios of transfer learning and zero-shot\nlearning on ImageNet 2010 and ImageNet-10K datasets.</p>\n", "tags": ["Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-20.8002872467041, -13.422430038452148], "cluster": 1}, {"key": "huang2017cross", "year": "2017", "citations": "15", "title": "Cross-modal Deep Metric Learning With Multi-task Regularization", "abstract": "<p>DNN-based cross-modal retrieval has become a research hotspot, by which users\ncan search results across various modalities like image and text. However,\nexisting methods mainly focus on the pairwise correlation and reconstruction\nerror of labeled data. They ignore the semantically similar and dissimilar\nconstraints between different modalities, and cannot take advantage of\nunlabeled data. This paper proposes Cross-modal Deep Metric Learning with\nMulti-task Regularization (CDMLMR), which integrates quadruplet ranking loss\nand semi-supervised contrastive loss for modeling cross-modal semantic\nsimilarity in a unified multi-task learning architecture. The quadruplet\nranking loss can model the semantically similar and dissimilar constraints to\npreserve cross-modal relative similarity ranking information. The\nsemi-supervised contrastive loss is able to maximize the semantic similarity on\nboth labeled and unlabeled data. Compared to the existing methods, CDMLMR\nexploits not only the similarity ranking information but also unlabeled\ncross-modal data, and thus boosts cross-modal retrieval accuracy.</p>\n", "tags": ["Supervised", "Multimodal-Retrieval", "Distance-Metric-Learning"], "tsne_embedding": [-20.259714126586914, -5.881227016448975], "cluster": 1}, {"key": "huang2017online", "year": "2013", "citations": "51", "title": "Online Hashing", "abstract": "<p>Although hash function learning algorithms have achieved great success in\nrecent years, most existing hash models are off-line, which are not suitable\nfor processing sequential or online data. To address this problem, this work\nproposes an online hash model to accommodate data coming in stream for online\nlearning. Specifically, a new loss function is proposed to measure the\nsimilarity loss between a pair of data samples in hamming space. Then, a\nstructured hash model is derived and optimized in a passive-aggressive way.\nTheoretical analysis on the upper bound of the cumulative loss for the proposed\nonline hash model is provided. Furthermore, we extend our online hashing from a\nsingle-model to a multi-model online hashing that trains multiple models so as\nto retain diverse online hashing models in order to avoid biased update. The\ncompetitive efficiency and effectiveness of the proposed online hash models are\nverified through extensive experiments on several large-scale datasets as\ncompared to related hashing methods.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [29.681285858154297, -0.5823379158973694], "cluster": 6}, {"key": "huang2017unsupervised", "year": "2017", "citations": "54", "title": "Unsupervised Triplet Hashing For Fast Image Retrieval", "abstract": "<p>Hashing has played a pivotal role in large-scale image retrieval. With the\ndevelopment of Convolutional Neural Network (CNN), hashing learning has shown\ngreat promise. But existing methods are mostly tuned for classification, which\nare not optimized for retrieval tasks, especially for instance-level retrieval.\nIn this study, we propose a novel hashing method for large-scale image\nretrieval. Considering the difficulty in obtaining labeled datasets for image\nretrieval task in large scale, we propose a novel CNN-based unsupervised\nhashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised\nhashing network is designed under the following three principles: 1) more\ndiscriminative representations for image retrieval; 2) minimum quantization\nloss between the original real-valued feature descriptors and the learned hash\ncodes; 3) maximum information entropy for the learned hash codes. Extensive\nexperiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH\noutperforms several state-of-the-art unsupervised hashing methods in terms of\nretrieval accuracy.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-45.32376480102539, -0.8145344853401184], "cluster": 0}, {"key": "huang2019accelerate", "year": "2019", "citations": "23", "title": "Accelerate Learning Of Deep Hashing With Gradient Attention", "abstract": "<p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [29.264301300048828, 0.6018727421760559], "cluster": 6}, {"key": "huang2019learning", "year": "2019", "citations": "0", "title": "Learning Hash Function Through Codewords", "abstract": "<p>In this paper, we propose a novel hash learning approach that has the\nfollowing main distinguishing features, when compared to past frameworks.\nFirst, the codewords are utilized in the Hamming space as ancillary techniques\nto accomplish its hash learning task. These codewords, which are inferred from\nthe data, attempt to capture grouping aspects of the data\u2019s hash codes.\nFurthermore, the proposed framework is capable of addressing supervised,\nunsupervised and, even, semi-supervised hash learning scenarios. Additionally,\nthe framework adopts a regularization term over the codewords, which\nautomatically chooses the codewords for the problem. To efficiently solve the\nproblem, one Block Coordinate Descent algorithm is showcased in the paper. We\nalso show that one step of the algorithms can be casted into several Support\nVector Machine problems which enables our algorithms to utilize efficient\nsoftware package. For the regularization term, a closed form solution of the\nproximal operator is provided in the paper. A series of comparative experiments\nfocused on content-based image retrieval highlights its performance advantages.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [18.08964729309082, -3.9727180004119873], "cluster": 6}, {"key": "huang2019multi", "year": "2019", "citations": "17", "title": "Multi-head Attention With Diversity For Learning Grounded Multilingual Multimodal Representations", "abstract": "<p>With the aim of promoting and understanding the multilingual version of image\nsearch, we leverage visual object detection and propose a model with diverse\nmulti-head attention to learn grounded multilingual multimodal representations.\nSpecifically, our model attends to different types of textual semantics in two\nlanguages and visual objects for fine-grained alignments between sentences and\nimages. We introduce a new objective function which explicitly encourages\nattention diversity to learn an improved visual-semantic embedding space. We\nevaluate our model in the German-Image and English-Image matching tasks on the\nMulti30K dataset, and in the Semantic Textual Similarity task with the English\ndescriptions of visual content. Results show that our model yields a\nsignificant performance gain over other methods in all of the three tasks.</p>\n", "tags": ["Evaluation", "EMNLP", "Datasets"], "tsne_embedding": [-26.913761138916016, -33.74779510498047], "cluster": 5}, {"key": "huang2022mulan", "year": "2022", "citations": "26", "title": "Mulan: A Joint Embedding Of Music Audio And Natural Language", "abstract": "<p>Music tagging and content-based retrieval systems have traditionally been\nconstructed using pre-defined ontologies covering a rigid set of music\nattributes or text queries. This paper presents MuLan: a first attempt at a new\ngeneration of acoustic models that link music audio directly to unconstrained\nnatural language music descriptions. MuLan takes the form of a two-tower, joint\naudio-text embedding model trained using 44 million music recordings (370K\nhours) and weakly-associated, free-form text annotations. Through its\ncompatibility with a wide range of music genres and text styles (including\nconventional music tags), the resulting audio-text representation subsumes\nexisting ontologies while graduating to true zero-shot functionalities. We\ndemonstrate the versatility of the MuLan embeddings with a range of experiments\nincluding transfer learning, zero-shot music tagging, language understanding in\nthe music domain, and cross-modal retrieval applications.</p>\n", "tags": ["Multimodal-Retrieval", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [8.911294937133789, -46.50991439819336], "cluster": 3}, {"key": "huang2022sah", "year": "2023", "citations": "4", "title": "SAH: Shifting-aware Asymmetric Hashing For Reverse \\(k\\)-maximum Inner Product Search", "abstract": "<p>This paper investigates a new yet challenging problem called Reverse\n\\(k\\)-Maximum Inner Product Search (R\\(k\\)MIPS). Given a query (item) vector, a set\nof item vectors, and a set of user vectors, the problem of R\\(k\\)MIPS aims to\nfind a set of user vectors whose inner products with the query vector are one\nof the \\(k\\) largest among the query and item vectors. We propose the first\nsubquadratic-time algorithm, i.e., Shifting-aware Asymmetric Hashing (SAH), to\ntackle the R\\(k\\)MIPS problem. To speed up the Maximum Inner Product Search\n(MIPS) on item vectors, we design a shifting-invariant asymmetric\ntransformation and develop a novel sublinear-time Shifting-Aware Asymmetric\nLocality Sensitive Hashing (SA-ALSH) scheme. Furthermore, we devise a new\nblocking strategy based on the Cone-Tree to effectively prune user vectors (in\na batch). We prove that SAH achieves a theoretical guarantee for solving the\nRMIPS problem. Experimental results on five real-world datasets show that SAH\nruns 4\\(\\sim\\)8\\(\\times\\) faster than the state-of-the-art methods for R\\(k\\)MIPS\nwhile achieving F1-scores of over 90%. The code is available at\nhttps://github.com/HuangQiang/SAH.</p>\n", "tags": ["AAAI", "Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [10.886216163635254, 41.321537017822266], "cluster": 4}, {"key": "huang2023lightweight", "year": "2023", "citations": "4", "title": "Lightweight-yet-efficient: Revitalizing Ball-tree For Point-to-hyperplane Nearest Neighbor Search", "abstract": "<p>Finding the nearest neighbor to a hyperplane (or Point-to-Hyperplane Nearest\nNeighbor Search, simply P2HNNS) is a new and challenging problem with\napplications in many research domains. While existing state-of-the-art hashing\nschemes (e.g., NH and FH) are able to achieve sublinear time complexity without\nthe assumption of the data being in a unit hypersphere, they require an\nasymmetric transformation, which increases the data dimension from \\(d\\) to\n\\(\u03a9(d^2)\\). This leads to considerable overhead for indexing and incurs\nsignificant distortion errors.\n  In this paper, we investigate a tree-based approach for solving P2HNNS using\nthe classical Ball-Tree index. Compared to hashing-based methods, tree-based\nmethods usually require roughly linear costs for construction, and they provide\ndifferent kinds of approximations with excellent flexibility. A simple\nbranch-and-bound algorithm with a novel lower bound is first developed on\nBall-Tree for performing P2HNNS. Then, a new tree structure named BC-Tree,\nwhich maintains the Ball and Cone structures in the leaf nodes of Ball-Tree, is\ndescribed together with two effective strategies, i.e., point-level pruning and\ncollaborative inner product computing. BC-Tree inherits both the low\nconstruction cost and lightweight property of Ball-Tree while providing a\nsimilar or more efficient search. Experimental results over 16 real-world data\nsets show that Ball-Tree and BC-Tree are around 1.1\\(\\sim\\)10\\(\\times\\) faster than\nNH and FH, and they can reduce the index size and indexing time by about\n1\\(\\sim\\)3 orders of magnitudes on average. The code is available at\nhttps://github.com/HuangQiang/BC-Tree.</p>\n", "tags": ["Tree-Based-Ann", "Hashing-Methods"], "tsne_embedding": [33.603675842285156, 40.46635055541992], "cluster": 4}, {"key": "huang2024cross", "year": "2024", "citations": "9", "title": "Cross-modal And Uni-modal Soft-label Alignment For Image-text Retrieval", "abstract": "<p>Current image-text retrieval methods have demonstrated impressive performance\nin recent years. However, they still face two problems: the inter-modal\nmatching missing problem and the intra-modal semantic loss problem. These\nproblems can significantly affect the accuracy of image-text retrieval. To\naddress these challenges, we propose a novel method called Cross-modal and\nUni-modal Soft-label Alignment (CUSA). Our method leverages the power of\nuni-modal pre-trained models to provide soft-label supervision signals for the\nimage-text retrieval model. Additionally, we introduce two alignment\ntechniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label\nAlignment (USA), to overcome false negatives and enhance similarity recognition\nbetween uni-modal samples. Our method is designed to be plug-and-play, meaning\nit can be easily applied to existing image-text retrieval models without\nchanging their original architectures. Extensive experiments on various\nimage-text retrieval models and datasets, we demonstrate that our method can\nconsistently improve the performance of image-text retrieval and achieve new\nstate-of-the-art results. Furthermore, our method can also boost the uni-modal\nretrieval performance of image-text retrieval models, enabling it to achieve\nuniversal retrieval. The code and supplementary files can be found at\nhttps://github.com/lerogo/aaai24_itr_cusa.</p>\n", "tags": ["AAAI", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-31.496355056762695, -19.968822479248047], "cluster": 5}, {"key": "huang2024pairdistill", "year": "2024", "citations": "0", "title": "Pairdistill: Pairwise Relevance Distillation For Dense Retrieval", "abstract": "<p>Effective information retrieval (IR) from vast datasets relies on advanced\ntechniques to extract relevant information in response to queries. Recent\nadvancements in dense retrieval have showcased remarkable efficacy compared to\ntraditional sparse retrieval methods. To further enhance retrieval performance,\nknowledge distillation techniques, often leveraging robust cross-encoder\nrerankers, have been extensively explored. However, existing approaches\nprimarily distill knowledge from pointwise rerankers, which assign absolute\nrelevance scores to documents, thus facing challenges related to inconsistent\ncomparisons. This paper introduces Pairwise Relevance Distillation\n(PairDistill) to leverage pairwise reranking, offering fine-grained\ndistinctions between similarly relevant documents to enrich the training of\ndense retrieval models. Our experiments demonstrate that PairDistill\noutperforms existing methods, achieving new state-of-the-art results across\nmultiple benchmarks. This highlights the potential of PairDistill in advancing\ndense retrieval techniques effectively. Our source code and trained models are\nreleased at https://github.com/MiuLab/PairDistill</p>\n", "tags": ["Evaluation", "EMNLP", "Datasets"], "tsne_embedding": [7.520023345947266, -25.988239288330078], "cluster": 7}, {"key": "huang2024semantic", "year": "2025", "citations": "0", "title": "Semantic Residual For Multimodal Unified Discrete Representation", "abstract": "<p>Recent research in the domain of multimodal unified representations\npredominantly employs codebook as representation forms, utilizing Vector\nQuantization(VQ) for quantization, yet there has been insufficient exploration\nof other quantization representation forms. Our work explores more precise\nquantization methods and introduces a new framework, Semantic Residual\nCross-modal Information Disentanglement (SRCID), inspired by the numerical\nresidual concept inherent to Residual Vector Quantization (RVQ). SRCID employs\nsemantic residual-based information disentanglement for multimodal data to\nbetter handle the inherent discrepancies between different modalities. Our\nmethod enhances the capabilities of unified multimodal representations and\ndemonstrates exceptional performance in cross-modal generalization and\ncross-modal zero-shot retrieval. Its average results significantly surpass\nexisting state-of-the-art models, as well as previous attempts with RVQ and\nFinite Scalar Quantization (FSQ) based on these modals.</p>\n", "tags": ["Quantization", "Few-Shot-&-Zero-Shot", "ICASSP", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [2.9671471118927, -24.36358070373535], "cluster": 7}, {"key": "huang2025accelerate", "year": "2019", "citations": "23", "title": "Accelerate Learning Of Deep Hashing With Gradient Attention", "abstract": "<p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [29.264301300048828, 0.6018696427345276], "cluster": 6}, {"key": "hubenthal2023image", "year": "2023", "citations": "4", "title": "Image-text Pre-training For Logo Recognition", "abstract": "<p>Open-set logo recognition is commonly solved by first detecting possible logo\nregions and then matching the detected parts against an ever-evolving dataset\nof cropped logo images. The matching model, a metric learning problem, is\nespecially challenging for logo recognition due to the mixture of text and\nsymbols in logos. We propose two novel contributions to improve the matching\nmodel\u2019s performance: (a) using image-text paired samples for pre-training, and\n(b) an improved metric learning loss function. A standard paradigm of\nfine-tuning ImageNet pre-trained models fails to discover the text sensitivity\nnecessary to solve the matching problem effectively. This work demonstrates the\nimportance of pre-training on image-text pairs, which significantly improves\nthe performance of a visual embedder trained for the logo retrieval task,\nespecially for more text-dominant classes. We construct a composite public logo\ndataset combining LogoDet3K, OpenLogo, and FlickrLogos-47 deemed\nOpenLogoDet3K47. We show that the same vision backbone pre-trained on\nimage-text data, when fine-tuned on OpenLogoDet3K47, achieves \\(98.6%\\)\nrecall@1, significantly improving performance over pre-training on Imagenet1K\n(\\(97.6%\\)). We generalize the ProxyNCA++ loss function to propose ProxyNCAHN++\nwhich incorporates class-specific hard negative images. The proposed method\nsets new state-of-the-art on five public logo datasets considered, with a\n\\(3.5%\\) zero-shot recall@1 improvement on LogoDet3K test, \\(4%\\) on OpenLogo,\n\\(6.5%\\) on FlickrLogos-47, \\(6.2%\\) on Logos In The Wild, and \\(0.6%\\) on\nBelgaLogo.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-52.07411575317383, -13.821407318115234], "cluster": 5}, {"key": "hubert2023do", "year": "2024", "citations": "2", "title": "Do Similar Entities Have Similar Embeddings?", "abstract": "<p>Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for entities in a knowledge graph, known as embeddings.\nA common tacit assumption is the KGE entity similarity assumption, which states\nthat these KGEMs retain the graph\u2019s structure within their embedding space,\n\\textit{i.e.}, position similar entities within the graph close to one another.\nThis desirable property make KGEMs widely used in downstream tasks such as\nrecommender systems or drug repurposing. Yet, the relation of entity similarity\nand similarity in the embedding space has rarely been formally evaluated.\nTypically, KGEMs are assessed based on their sole link prediction capabilities,\nusing ranked-based metrics such as Hits@K or Mean Rank. This paper challenges\nthe prevailing assumption that entity similarity in the graph is inherently\nmirrored in the embedding space. Therefore, we conduct extensive experiments to\nmeasure the capability of KGEMs to cluster similar entities together, and\ninvestigate the nature of the underlying factors. Moreover, we study if\ndifferent KGEMs expose a different notion of similarity. Datasets, pre-trained\nembeddings and code are available at:\nhttps://github.com/nicolas-hbt/similar-embeddings/.</p>\n", "tags": ["Recommender-Systems", "Datasets"], "tsne_embedding": [61.47183609008789, 2.896249532699585], "cluster": 9}, {"key": "hubert2024do", "year": "2024", "citations": "2", "title": "Do Similar Entities Have Similar Embeddings?", "abstract": "<p>Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for entities in a knowledge graph, known as embeddings.\nA common tacit assumption is the KGE entity similarity assumption, which states\nthat these KGEMs retain the graph\u2019s structure within their embedding space,\n\\textit{i.e.}, position similar entities within the graph close to one another.\nThis desirable property make KGEMs widely used in downstream tasks such as\nrecommender systems or drug repurposing. Yet, the relation of entity similarity\nand similarity in the embedding space has rarely been formally evaluated.\nTypically, KGEMs are assessed based on their sole link prediction capabilities,\nusing ranked-based metrics such as Hits@K or Mean Rank. This paper challenges\nthe prevailing assumption that entity similarity in the graph is inherently\nmirrored in the embedding space. Therefore, we conduct extensive experiments to\nmeasure the capability of KGEMs to cluster similar entities together, and\ninvestigate the nature of the underlying factors. Moreover, we study if\ndifferent KGEMs expose a different notion of similarity. Datasets, pre-trained\nembeddings and code are available at:\nhttps://github.com/nicolas-hbt/similar-embeddings/.</p>\n", "tags": ["Recommender-Systems", "Datasets"], "tsne_embedding": [61.47183609008789, 2.896249532699585], "cluster": 9}, {"key": "hui2021efficient", "year": "2022", "citations": "15", "title": "Efficient 3D Point Cloud Feature Learning For Large-scale Place Recognition", "abstract": "<p>Point cloud based retrieval for place recognition is still a challenging\nproblem due to drastic appearance and illumination changes of scenes in\nchanging environments. Existing deep learning based global descriptors for the\nretrieval task usually consume a large amount of computation resources (e.g.,\nmemory), which may not be suitable for the cases of limited hardware resources.\nIn this paper, we develop an efficient point cloud learning network (EPC-Net)\nto form a global descriptor for visual place recognition, which can obtain good\nperformance and reduce computation memory and inference time. First, we propose\na lightweight but effective neural network module, called ProxyConv, to\naggregate the local geometric features of point clouds. We leverage the spatial\nadjacent matrix and proxy points to simplify the original edge convolution for\nlower memory consumption. Then, we design a lightweight grouped VLAD network\n(G-VLAD) to form global descriptors for retrieval. Compared with the original\nVLAD network, we propose a grouped fully connected (GFC) layer to decompose the\nhigh-dimensional vectors into a group of low-dimensional vectors, which can\nreduce the number of parameters of the network and maintain the discrimination\nof the feature vector. Finally, to further reduce the inference time, we\ndevelop a simple version of EPC-Net, called EPC-Net-L, which consists of two\nProxyConv modules and one max pooling layer to aggregate global descriptors. By\ndistilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative\nglobal descriptors for retrieval. Extensive experiments on the Oxford dataset\nand three in-house datasets demonstrate that our proposed method can achieve\nstate-of-the-art performance with lower parameters, FLOPs, and runtime per\nframe.</p>\n", "tags": ["Evaluation", "Scalability", "Datasets"], "tsne_embedding": [47.859554290771484, -9.198098182678223], "cluster": 9}, {"key": "hui2022efficient", "year": "2022", "citations": "15", "title": "Efficient 3D Point Cloud Feature Learning For Large-scale Place Recognition", "abstract": "<p>Point cloud based retrieval for place recognition is still a challenging\nproblem due to drastic appearance and illumination changes of scenes in\nchanging environments. Existing deep learning based global descriptors for the\nretrieval task usually consume a large amount of computation resources (e.g.,\nmemory), which may not be suitable for the cases of limited hardware resources.\nIn this paper, we develop an efficient point cloud learning network (EPC-Net)\nto form a global descriptor for visual place recognition, which can obtain good\nperformance and reduce computation memory and inference time. First, we propose\na lightweight but effective neural network module, called ProxyConv, to\naggregate the local geometric features of point clouds. We leverage the spatial\nadjacent matrix and proxy points to simplify the original edge convolution for\nlower memory consumption. Then, we design a lightweight grouped VLAD network\n(G-VLAD) to form global descriptors for retrieval. Compared with the original\nVLAD network, we propose a grouped fully connected (GFC) layer to decompose the\nhigh-dimensional vectors into a group of low-dimensional vectors, which can\nreduce the number of parameters of the network and maintain the discrimination\nof the feature vector. Finally, to further reduce the inference time, we\ndevelop a simple version of EPC-Net, called EPC-Net-L, which consists of two\nProxyConv modules and one max pooling layer to aggregate global descriptors. By\ndistilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative\nglobal descriptors for retrieval. Extensive experiments on the Oxford dataset\nand three in-house datasets demonstrate that our proposed method can achieve\nstate-of-the-art performance with lower parameters, FLOPs, and runtime per\nframe.</p>\n", "tags": ["Evaluation", "Scalability", "Datasets"], "tsne_embedding": [47.85953903198242, -9.198100090026855], "cluster": 9}, {"key": "huijben2024residual", "year": "2024", "citations": "0", "title": "Residual Quantization With Implicit Neural Codebooks", "abstract": "<p>Vector quantization is a fundamental operation for data compression and\nvector search. To obtain high accuracy, multi-codebook methods represent each\nvector using codewords across several codebooks. Residual quantization (RQ) is\none such method, which iteratively quantizes the error of the previous step.\nWhile the error distribution is dependent on previously-selected codewords,\nthis dependency is not accounted for in conventional RQ as it uses a fixed\ncodebook per quantization step. In this paper, we propose QINCo, a neural RQ\nvariant that constructs specialized codebooks per step that depend on the\napproximation of the vector from previous steps. Experiments show that QINCo\noutperforms state-of-the-art methods by a large margin on several datasets and\ncode sizes. For example, QINCo achieves better nearest-neighbor search accuracy\nusing 12-byte codes than the state-of-the-art UNQ using 16 bytes on the\nBigANN1M and Deep1M datasets.</p>\n", "tags": ["Quantization", "Datasets"], "tsne_embedding": [10.155472755432129, 34.41193389892578], "cluster": 4}, {"key": "husain2019remap", "year": "2019", "citations": "53", "title": "REMAP: Multi-layer Entropy-guided Pooling Of Dense CNN Features For Image Retrieval", "abstract": "<p>This paper addresses the problem of very large-scale image retrieval,\nfocusing on improving its accuracy and robustness. We target enhanced\nrobustness of search to factors such as variations in illumination, object\nappearance and scale, partial occlusions, and cluttered backgrounds -\nparticularly important when search is performed across very large datasets with\nsignificant variability. We propose a novel CNN-based global descriptor, called\nREMAP, which learns and aggregates a hierarchy of deep features from multiple\nCNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly\nlearns discriminative features which are mutually-supportive and complementary\nat various semantic levels of visual abstraction. These dense local features\nare max-pooled spatially at each layer, within multi-scale overlapping regions,\nbefore aggregation into a single image-level descriptor. To identify the\nsemantically useful regions and layers for retrieval, we propose to measure the\ninformation gain of each region and layer using KL-divergence. Our system\neffectively learns during training how useful various regions and layers are\nand weights them accordingly. We show that such relative entropy-guided\naggregation outperforms classical CNN-based aggregation controlled by SGD. The\nentire framework is trained in an end-to-end fashion, outperforming the latest\nstate-of-the-art results. On image retrieval datasets Holidays, Oxford and\nMPEG, the REMAP descriptor achieves mAP of 95.5%, 91.5%, and 80.1%\nrespectively, outperforming any results published to date. REMAP also formed\nthe core of the winning submission to the Google Landmark Retrieval Challenge\non Kaggle.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Scalability", "Robustness", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-43.5538330078125, -2.958022117614746], "cluster": 0}, {"key": "hussein2017unified", "year": "2017", "citations": "10", "title": "Unified Embedding And Metric Learning For Zero-exemplar Event Detection", "abstract": "<p>Event detection in unconstrained videos is conceived as a content-based video\nretrieval with two modalities: textual and visual. Given a text describing a\nnovel event, the goal is to rank related videos accordingly. This task is\nzero-exemplar, no video examples are given to the novel event.\n  Related works train a bank of concept detectors on external data sources.\nThese detectors predict confidence scores for test videos, which are ranked and\nretrieved accordingly. In contrast, we learn a joint space in which the visual\nand textual representations are embedded. The space casts a novel event as a\nprobability of pre-defined events. Also, it learns to measure the distance\nbetween an event and its related videos.\n  Our model is trained end-to-end on publicly available EventNet. When applied\nto TRECVID Multimedia Event Detection dataset, it outperforms the\nstate-of-the-art by a considerable margin.</p>\n", "tags": ["CVPR", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-11.480602264404297, -35.97235870361328], "cluster": 3}, {"key": "hyv\u00f6nen2019multilabel", "year": "2019", "citations": "1", "title": "A Multilabel Classification Framework For Approximate Nearest Neighbor Search", "abstract": "<p>Both supervised and unsupervised machine learning algorithms have been used\nto learn partition-based index structures for approximate nearest neighbor\n(ANN) search. Existing supervised algorithms formulate the learning task as\nfinding a partition in which the nearest neighbors of a training set point\nbelong to the same partition element as the point itself, so that the nearest\nneighbor candidates can be retrieved by naive lookup or backtracking search. We\nformulate candidate set selection in ANN search directly as a multilabel\nclassification problem where the labels correspond to the nearest neighbors of\nthe query point, and interpret the partitions as partitioning classifiers for\nsolving this task. Empirical results suggest that the natural classifier based\non this interpretation leads to strictly improved performance when combined\nwith any unsupervised or supervised partitioning strategy. We also prove a\nsufficient condition for consistency of a partitioning classifier for ANN\nsearch, and illustrate the result by verifying this condition for chronological\n\\(k\\)-d trees.</p>\n", "tags": ["Vector-Indexing", "Similarity-Search", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [36.050514221191406, -12.88520336151123], "cluster": 9}, {"key": "h\u00f6nig2023bi", "year": "2023", "citations": "0", "title": "Bi-encoder Cascades For Efficient Image Search", "abstract": "<p>Modern neural encoders offer unprecedented text-image retrieval (TIR)\naccuracy, but their high computational cost impedes an adoption to large-scale\nimage searches. To lower this cost, model cascades use an expensive encoder to\nrefine the ranking of a cheap encoder. However, existing cascading algorithms\nfocus on cross-encoders, which jointly process text-image pairs, but do not\nconsider cascades of bi-encoders, which separately process texts and images. We\nintroduce the small-world search scenario as a realistic setting where\nbi-encoder cascades can reduce costs. We then propose a cascading algorithm\nthat leverages the small-world search scenario to reduce lifetime image\nencoding costs of a TIR system. Our experiments show cost reductions by up to\n6x.</p>\n", "tags": ["Image-Retrieval", "Scalability"], "tsne_embedding": [7.447094440460205, 24.934932708740234], "cluster": 4}, {"key": "iijima2024multimodal", "year": "2024", "citations": "0", "title": "A Multimodal Approach For Cross-domain Image Retrieval", "abstract": "<p>Cross-Domain Image Retrieval (CDIR) is a challenging task in computer vision,\naiming to match images across different visual domains such as sketches,\npaintings, and photographs. Traditional approaches focus on visual image\nfeatures and rely heavily on supervised learning with labeled data and\ncross-domain correspondences, which leads to an often struggle with the\nsignificant domain gap. This paper introduces a novel unsupervised approach to\nCDIR that incorporates textual context by leveraging pre-trained\nvision-language models. Our method, dubbed as Caption-Matching (CM), uses\ngenerated image captions as a domain-agnostic intermediate representation,\nenabling effective cross-domain similarity computation without the need for\nlabeled data or fine-tuning. We evaluate our method on standard CDIR benchmark\ndatasets, demonstrating state-of-the-art performance in unsupervised settings\nwith improvements of 24.0% on Office-Home and 132.2% on DomainNet over previous\nmethods. We also demonstrate our method\u2019s effectiveness on a dataset of\nAI-generated images from Midjourney, showcasing its ability to handle complex,\nmulti-domain queries.</p>\n", "tags": ["Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-18.595870971679688, -19.682157516479492], "cluster": 5}, {"key": "ilievski2021user", "year": "2021", "citations": "3", "title": "User-friendly Comparison Of Similarity Algorithms On Wikidata", "abstract": "<p>While the similarity between two concept words has been evaluated and studied\nfor decades, much less attention has been devoted to algorithms that can\ncompute the similarity of nodes in very large knowledge graphs, like Wikidata.\nTo facilitate investigations and head-to-head comparisons of similarity\nalgorithms on Wikidata, we present a user-friendly interface that allows\nflexible computation of similarity between Qnodes in Wikidata. At present, the\nsimilarity interface supports four algorithms, based on: graph embeddings\n(TransE, ComplEx), text embeddings (BERT), and class-based similarity. We\ndemonstrate the behavior of the algorithms on representative examples about\nsemantically similar, related, and entirely unrelated entity pairs. To support\nanticipated applications that require efficient similarity computations, like\nentity linking and recommendation, we also provide a REST API that can compute\nmost similar neighbors for any Qnode in Wikidata.</p>\n", "tags": ["Recommender-Systems", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [60.237579345703125, 3.9227378368377686], "cluster": 9}, {"key": "imbriaco2019aggregated", "year": "2019", "citations": "73", "title": "Aggregated Deep Local Features For Remote Sensing Image Retrieval", "abstract": "<p>Remote Sensing Image Retrieval remains a challenging topic due to the special\nnature of Remote Sensing Imagery. Such images contain various different\nsemantic objects, which clearly complicates the retrieval task. In this paper,\nwe present an image retrieval pipeline that uses attentive, local convolutional\nfeatures and aggregates them using the Vector of Locally Aggregated Descriptors\n(VLAD) to produce a global descriptor. We study various system parameters such\nas the multiplicative and additive attention mechanisms and descriptor\ndimensionality. We propose a query expansion method that requires no external\ninputs. Experiments demonstrate that even without training, the local\nconvolutional features and global representation outperform other systems.\nAfter system tuning, we can achieve state-of-the-art or competitive results.\nFurthermore, we observe that our query expansion method increases overall\nsystem performance by about 3%, using only the top-three retrieved images.\nFinally, we show how dimensionality reduction produces compact descriptors with\nincreased retrieval performance and fast retrieval computation times, e.g. 50%\nfaster than the current systems.</p>\n", "tags": ["Efficiency", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-33.70970153808594, 15.76778507232666], "cluster": 0}, {"key": "indyk2016simultaneous", "year": "2016", "citations": "1", "title": "Simultaneous Nearest Neighbor Search", "abstract": "<p>Motivated by applications in computer vision and databases, we introduce and\nstudy the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of\ndata points, the goal of SNN is to design a data structure that, given a\ncollection of queries, finds a collection of close points that are compatible\nwith each other. Formally, we are given \\(k\\) query points \\(Q=q_1,\\cdots,q_k\\),\nand a compatibility graph \\(G\\) with vertices in \\(Q\\), and the goal is to return\ndata points \\(p_1,\\cdots,p_k\\) that minimize (i) the weighted sum of the\ndistances from \\(q_i\\) to \\(p_i\\) and (ii) the weighted sum, over all edges \\((i,j)\\)\nin the compatibility graph \\(G\\), of the distances between \\(p_i\\) and \\(p_j\\). The\nproblem has several applications, where one wants to return a set of consistent\nanswers to multiple related queries. This generalizes well-studied\ncomputational problems, including NN, Aggregate NN and the 0-extension problem.\n  In this paper we propose and analyze the following general two-step method\nfor designing efficient data structures for SNN. In the first step, for each\nquery point \\(q_i\\) we find its (approximate) nearest neighbor point \\(\\hat{p}_i\\);\nthis can be done efficiently using existing approximate nearest neighbor\nstructures. In the second step, we solve an off-line optimization problem over\nsets \\(q_1,\\cdots,q_k\\) and \\(\\hat{p}_1,\\cdots,\\hat{p}_k\\); this can be done\nefficiently given that \\(k\\) is much smaller than \\(n\\). Even though\n\\(\\hat{p}_1,\\cdots,\\hat{p}_k\\) might not constitute the optimal answers to\nqueries \\(q_1,\\cdots,q_k\\), we show that, for the unweighted case, the resulting\nalgorithm is \\(O(log k/log log k)\\)-approximation. Also, we show that the\napproximation factor can be in fact reduced to a constant for compatibility\ngraphs frequently occurring in practice.\n  Finally, we show that the \u201cempirical approximation factor\u201d provided by the\nabove approach is very close to 1.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [23.43094253540039, 50.03615951538086], "cluster": 4}, {"key": "indyk2018approximate", "year": "2018", "citations": "6", "title": "Approximate Nearest Neighbors In Limited Space", "abstract": "<p>We consider the \\((1+\\epsilon)\\)-approximate nearest neighbor search problem:\ngiven a set \\(X\\) of \\(n\\) points in a \\(d\\)-dimensional space, build a data\nstructure that, given any query point \\(y\\), finds a point \\(x \\in X\\) whose\ndistance to \\(y\\) is at most \\((1+\\epsilon) \\min_{x \\in X} |x-y|\\) for an\naccuracy parameter \\(\\epsilon \\in (0,1)\\). Our main result is a data structure\nthat occupies only \\(O(\\epsilon^{-2} n log(n) log(1/\\epsilon))\\) bits of space,\nassuming all point coordinates are integers in the range \\(\\{-n^{O(1)} \\ldots\nn^{O(1)}\\}\\), i.e., the coordinates have \\(O(log n)\\) bits of precision. This\nimproves over the best previously known space bound of \\(O(\\epsilon^{-2} n\nlog(n)^2)\\), obtained via the randomized dimensionality reduction method of\nJohnson and Lindenstrauss (1984). We also consider the more general problem of\nestimating all distances from a collection of query points to all data points\n\\(X\\), and provide almost tight upper and lower bounds for the space complexity\nof this problem.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [23.139442443847656, 48.07660675048828], "cluster": 4}, {"key": "indyk2023worst", "year": "2023", "citations": "2", "title": "Worst-case Performance Of Popular Approximate Nearest Neighbor Search Implementations: Guarantees And Limitations", "abstract": "<p>Graph-based approaches to nearest neighbor search are popular and powerful\ntools for handling large datasets in practice, but they have limited\ntheoretical guarantees. We study the worst-case performance of recent\ngraph-based approximate nearest neighbor search algorithms, such as HNSW, NSG\nand DiskANN. For DiskANN, we show that its \u201cslow preprocessing\u201d version\nprovably supports approximate nearest neighbor search query with constant\napproximation ratio and poly-logarithmic query time, on data sets with bounded\n\u201cintrinsic\u201d dimension. For the other data structure variants studied, including\nDiskANN with \u201cfast preprocessing\u201d, HNSW and NSG, we present a family of\ninstances on which the empirical query time required to achieve a \u201creasonable\u201d\naccuracy is linear in instance size. For example, for DiskANN, we show that the\nquery procedure can take at least \\(0.1 n\\) steps on instances of size \\(n\\) before\nit encounters any of the \\(5\\) nearest neighbors of the query.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Evaluation", "Datasets"], "tsne_embedding": [51.4442024230957, 10.745386123657227], "cluster": 9}, {"key": "irie2014locally", "year": "2014", "citations": "94", "title": "Locally Linear Hashing For Extracting Non-linear Manifolds", "abstract": "<p>Previous efforts in hashing intend to preserve data variance\nor pairwise affinity, but neither is adequate in capturing\nthe manifold structures hidden in most visual data. In\nthis paper, we tackle this problem by reconstructing the locally\nlinear structures of manifolds in the binary Hamming\nspace, which can be learned by locality-sensitive sparse\ncoding. We cast the problem as a joint minimization of\nreconstruction error and quantization loss, and show that,\ndespite its NP-hardness, a local optimum can be obtained\nefficiently via alternative optimization. Our method distinguishes\nitself from existing methods in its remarkable ability\nto extract the nearest neighbors of the query from the\nsame manifold, instead of from the ambient space. On extensive\nexperiments on various image benchmarks, our results\nimprove previous state-of-the-art by 28-74% typically,\nand 627% on the Yale face data.</p>\n", "tags": ["Quantization", "CVPR", "Hashing-Methods"], "tsne_embedding": [-24.29432487487793, 32.43151092529297], "cluster": 8}, {"key": "irie2025locally", "year": "2014", "citations": "94", "title": "Locally Linear Hashing For Extracting Non-linear Manifolds", "abstract": "<p>Previous efforts in hashing intend to preserve data variance\nor pairwise affinity, but neither is adequate in capturing\nthe manifold structures hidden in most visual data. In\nthis paper, we tackle this problem by reconstructing the locally\nlinear structures of manifolds in the binary Hamming\nspace, which can be learned by locality-sensitive sparse\ncoding. We cast the problem as a joint minimization of\nreconstruction error and quantization loss, and show that,\ndespite its NP-hardness, a local optimum can be obtained\nefficiently via alternative optimization. Our method distinguishes\nitself from existing methods in its remarkable ability\nto extract the nearest neighbors of the query from the\nsame manifold, instead of from the ambient space. On extensive\nexperiments on various image benchmarks, our results\nimprove previous state-of-the-art by 28-74% typically,\nand 627% on the Yale face data.</p>\n", "tags": ["Quantization", "CVPR", "Hashing-Methods"], "tsne_embedding": [-24.29432487487793, 32.43151092529297], "cluster": 8}, {"key": "iscen2016efficient", "year": "2017", "citations": "188", "title": "Efficient Diffusion On Region Manifolds: Recovering Small Objects With Compact CNN Representations", "abstract": "<p>Query expansion is a popular method to improve the quality of image retrieval\nwith both conventional and CNN representations. It has been so far limited to\nglobal image similarity. This work focuses on diffusion, a mechanism that\ncaptures the image manifold in the feature space. The diffusion is carried out\non descriptors of overlapping image regions rather than on a global image\ndescriptor like in previous approaches. An efficient off-line stage allows\noptional reduction in the number of stored regions. In the on-line stage, the\nproposed handling of unseen queries in the indexing stage removes additional\ncomputation to adjust the precomputed data. We perform diffusion through a\nsparse linear system solver, yielding practical query times well below one\nsecond. Experimentally, we observe a significant boost in performance of image\nretrieval with compact CNN descriptors on standard benchmarks, especially when\nthe query object covers only a small part of the image. Small objects have been\na common failure case of CNN-based retrieval.</p>\n", "tags": ["Efficiency", "CVPR", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-44.8934211730957, 1.7067842483520508], "cluster": 0}, {"key": "iscen2017fast", "year": "2018", "citations": "37", "title": "Fast Spectral Ranking For Similarity Search", "abstract": "<p>Despite the success of deep learning on representing images for particular\nobject retrieval, recent studies show that the learned representations still\nlie on manifolds in a high dimensional space. This makes the Euclidean nearest\nneighbor search biased for this task. Exploring the manifolds online remains\nexpensive even if a nearest neighbor graph has been computed offline. This work\nintroduces an explicit embedding reducing manifold search to Euclidean search\nfollowed by dot product similarity search. This is equivalent to linear graph\nfiltering of a sparse signal in the frequency domain. To speed up online\nsearch, we compute an approximate Fourier basis of the graph offline. We\nimprove the state of art on particular object retrieval datasets including the\nchallenging Instre dataset containing small objects. At a scale of 10^5 images,\nthe offline cost is only a few hours, while query time is comparable to\nstandard similarity search.</p>\n", "tags": ["Efficiency", "CVPR", "Similarity-Search", "Datasets"], "tsne_embedding": [52.22885513305664, 16.348880767822266], "cluster": 9}, {"key": "iscen2018local", "year": "2018", "citations": "1", "title": "Local Orthogonal-group Testing", "abstract": "<p>This work addresses approximate nearest neighbor search applied in the domain\nof large-scale image retrieval. Within the group testing framework we propose\nan efficient off-line construction of the search structures. The linear-time\ncomplexity orthogonal grouping increases the probability that at most one\nelement from each group is matching to a given query. Non-maxima suppression\nwith each group efficiently reduces the number of false positive results at no\nextra cost. Unlike in other well-performing approaches, all processing is\nlocal, fast, and suitable to process data in batches and in parallel. We\nexperimentally show that the proposed method achieves search accuracy of the\nexhaustive search with significant reduction in the search complexity. The\nmethod can be naturally combined with existing embedding methods.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Scalability"], "tsne_embedding": [-20.70697784423828, 14.320197105407715], "cluster": 8}, {"key": "ishaq2019clustered", "year": "2019", "citations": "5", "title": "Clustered Hierarchical Entropy-scaling Search Of Astronomical And Biological Data", "abstract": "<p>Both astronomy and biology are experiencing explosive growth of data,\nresulting in a \u201cbig data\u201d problem that stands in the way of a \u201cbig data\u201d\nopportunity for discovery. One common question asked of such data is that of\napproximate search (\\(\\rho-\\)nearest neighbors search). We present a hierarchical\nsearch algorithm for such data sets that takes advantage of particular\ngeometric properties apparent in both astronomical and biological data sets,\nnamely the metric entropy and fractal dimensionality of the data. We present\nCHESS (Clustered Hierarchical Entropy-Scaling Search), a search tool with\nvirtually no loss in specificity or sensitivity, demonstrating a \\(13.6\\times\\)\nspeedup over linear search on the Sloan Digital Sky Survey\u2019s APOGEE data set\nand a \\(68\\times\\) speedup on the GreenGenes 16S metagenomic data set, as well as\nasymptotically fewer distance comparisons on APOGEE when compared to the\nFALCONN locality-sensitive hashing library. CHESS demonstrates an asymptotic\ncomplexity not directly dependent on data set size, and is in practice at least\nan order of magnitude faster than linear search by performing fewer distance\ncomparisons. Unlike locality-sensitive hashing approaches, CHESS can work with\nany user-defined distance function. CHESS also allows for implicit data\ncompression, which we demonstrate on the APOGEE data set. We also discuss an\nextension allowing for efficient k-nearest neighbors search.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Hashing-Methods", "Survey-Paper"], "tsne_embedding": [-3.661223888397217, 39.62356185913086], "cluster": 4}, {"key": "islam2024spatially", "year": "2024", "citations": "0", "title": "Spatially Optimized Compact Deep Metric Learning Model For Similarity Search", "abstract": "<p>Spatial optimization is often overlooked in many computer vision tasks.\nFilters should be able to recognize the features of an object regardless of\nwhere it is in the image. Similarity search is a crucial task where spatial\nfeatures decide an important output. The capacity of convolution to capture\nvisual patterns across various locations is limited. In contrast to\nconvolution, the involution kernel is dynamically created at each pixel based\non the pixel value and parameters that have been learned. This study\ndemonstrates that utilizing a single layer of involution feature extractor\nalongside a compact convolution model significantly enhances the performance of\nsimilarity search. Additionally, we improve predictions by using the GELU\nactivation function rather than the ReLU. The negligible amount of weight\nparameters in involution with a compact model with better performance makes the\nmodel very useful in real-world implementations. Our proposed model is below 1\nmegabyte in size. We have experimented with our proposed methodology and other\nmodels on CIFAR-10, FashionMNIST, and MNIST datasets. Our proposed method\noutperforms across all three datasets.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Similarity-Search", "Datasets"], "tsne_embedding": [-36.17556381225586, 2.347655773162842], "cluster": 0}, {"key": "iwasaki2018optimization", "year": "2018", "citations": "39", "title": "Optimization Of Indexing Based On K-nearest Neighbor Graph For Proximity Search In High-dimensional Data", "abstract": "<p>Searching for high-dimensional vector data with high accuracy is an\ninevitable search technology for various types of data. Graph-based indexes are\nknown to reduce the query time for high-dimensional data. To further improve\nthe query time by using graphs, we focused on the indegrees and outdegrees of\ngraphs. While a sufficient number of incoming edges (indegrees) are\nindispensable for increasing search accuracy, an excessive number of outgoing\nedges (outdegrees) should be suppressed so as to not increase the query time.\nTherefore, we propose three degree-adjustment methods: static degree adjustment\nof not only outdegrees but also indegrees, dynamic degree adjustment with which\noutdegrees are determined by the search accuracy users require, and path\nadjustment to remove edges that have alternative search paths to reduce\noutdegrees. We also show how to obtain optimal degree-adjustment parameters and\nthat our methods outperformed previous methods for image and textual data.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann"], "tsne_embedding": [58.019691467285156, -0.5571433305740356], "cluster": 9}, {"key": "izadinia2018viser", "year": "2018", "citations": "4", "title": "VISER: Visual Self-regularization", "abstract": "<p>In this work, we propose the use of large set of unlabeled images as a source\nof regularization data for learning robust visual representation. Given a\nvisual model trained by a labeled dataset in a supervised fashion, we augment\nour training samples by incorporating large number of unlabeled data and train\na semi-supervised model. We demonstrate that our proposed learning approach\nleverages an abundance of unlabeled images and boosts the visual recognition\nperformance which alleviates the need to rely on large labeled datasets for\nlearning robust representation. To increment the number of image instances\nneeded to learn robust visual models in our approach, each labeled image\npropagates its label to its nearest unlabeled image instances. These retrieved\nunlabeled images serve as local perturbations of each labeled image to perform\nVisual Self-Regularization (VISER). To retrieve such visual self regularizers,\nwe compute the cosine similarity in a semantic space defined by the penultimate\nlayer in a fully convolutional neural network. We use the publicly available\nYahoo Flickr Creative Commons 100M dataset as the source of our unlabeled image\nset and propose a distributed approximate nearest neighbor algorithm to make\nretrieval practical at that scale. Using the labeled instances and their\nregularizer samples we show that we significantly improve object categorization\nand localization performance on the MS COCO and Visual Genome datasets where\nobjects appear in context.</p>\n", "tags": ["Supervised", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-22.096118927001953, -17.333145141601562], "cluster": 5}, {"key": "jacob2019efficient", "year": "2019", "citations": "2", "title": "Efficient Codebook And Factorization For Second Order Representation Learning", "abstract": "<p>Learning rich and compact representations is an open topic in many fields\nsuch as object recognition or image retrieval. Deep neural networks have made a\nmajor breakthrough during the last few years for these tasks but their\nrepresentations are not necessary as rich as needed nor as compact as expected.\nTo build richer representations, high order statistics have been exploited and\nhave shown excellent performances, but they produce higher dimensional\nfeatures. While this drawback has been partially addressed with factorization\nschemes, the original compactness of first order models has never been\nretrieved, or at the cost of a strong performance decrease. Our method, by\njointly integrating codebook strategy to factorization scheme, is able to\nproduce compact representations while keeping the second order performances\nwith few additional parameters. This formulation leads to state-of-the-art\nresults on three image retrieval datasets.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [30.16254425048828, -8.286181449890137], "cluster": 7}, {"key": "jacob2019metric", "year": "2019", "citations": "73", "title": "Metric Learning With HORDE: High-order Regularizer For Deep Embeddings", "abstract": "<p>Learning an effective similarity measure between image representations is key\nto the success of recent advances in visual search tasks (e.g. verification or\nzero-shot learning). Although the metric learning part is well addressed, this\nmetric is usually computed over the average of the extracted deep features.\nThis representation is then trained to be discriminative. However, these deep\nfeatures tend to be scattered across the feature space. Consequently, the\nrepresentations are not robust to outliers, object occlusions, background\nvariations, etc. In this paper, we tackle this scattering problem with a\ndistribution-aware regularization named HORDE. This regularizer enforces\nvisually-close images to have deep features with the same distribution which\nare well localized in the feature space. We provide a theoretical analysis\nsupporting this regularization effect. We also show the effectiveness of our\napproach by obtaining state-of-the-art results on 4 well-known datasets\n(Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes\nRetrieval).</p>\n", "tags": ["ICCV", "Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets"], "tsne_embedding": [-27.911468505859375, -7.029026508331299], "cluster": 1}, {"key": "jacob2020diablo", "year": "2020", "citations": "1", "title": "DIABLO: Dictionary-based Attention Block For Deep Metric Learning", "abstract": "<p>Recent breakthroughs in representation learning of unseen classes and\nexamples have been made in deep metric learning by training at the same time\nthe image representations and a corresponding metric with deep networks. Recent\ncontributions mostly address the training part (loss functions, sampling\nstrategies, etc.), while a few works focus on improving the discriminative\npower of the image representation. In this paper, we propose DIABLO, a\ndictionary-based attention method for image embedding. DIABLO produces richer\nrepresentations by aggregating only visually-related features together while\nbeing easier to train than other attention-based methods in deep metric\nlearning. This is experimentally confirmed on four deep metric learning\ndatasets (Cub-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes\nRetrieval) for which DIABLO shows state-of-the-art performances.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-16.21295166015625, -14.818602561950684], "cluster": 1}, {"key": "jacques2016time", "year": "2017", "citations": "32", "title": "Time For Dithering: Fast And Quantized Random Embeddings Via The Restricted Isometry Property", "abstract": "<p>Recently, many works have focused on the characterization of non-linear\ndimensionality reduction methods obtained by quantizing linear embeddings,\ne.g., to reach fast processing time, efficient data compression procedures,\nnovel geometry-preserving embeddings or to estimate the information/bits stored\nin this reduced data representation. In this work, we prove that many linear\nmaps known to respect the restricted isometry property (RIP) can induce a\nquantized random embedding with controllable multiplicative and additive\ndistortions with respect to the pairwise distances of the data points beings\nconsidered. In other words, linear matrices having fast matrix-vector\nmultiplication algorithms (e.g., based on partial Fourier ensembles or on the\nadjacency matrix of unbalanced expanders) can be readily used in the definition\nof fast quantized embeddings with small distortions. This implication is made\npossible by applying right after the linear map an additive and random \u201cdither\u201d\nthat stabilizes the impact of the uniform scalar quantization operator applied\nafterwards. For different categories of RIP matrices, i.e., for different\nlinear embeddings of a metric space \\((\\mathcal K \\subset \\mathbb R^n, \\ell_q)\\)\nin \\((\\mathbb R^m, \\ell_p)\\) with \\(p,q \\geq 1\\), we derive upper bounds on the\nadditive distortion induced by quantization, showing that it decays either when\nthe embedding dimension \\(m\\) increases or when the distance of a pair of\nembedded vectors in \\(\\mathcal K\\) decreases. Finally, we develop a novel\n\u201cbi-dithered\u201d quantization scheme, which allows for a reduced distortion that\ndecreases when the embedding dimension grows and independently of the\nconsidered pair of vectors.</p>\n", "tags": ["Quantization", "Evaluation"], "tsne_embedding": [12.989324569702148, 52.696044921875], "cluster": 4}, {"key": "jafari2019efficient", "year": "2020", "citations": "0", "title": "Efficient Bitmap-based Indexing And Retrieval Of Similarity Search Image Queries", "abstract": "<p>Finding similar images is a necessary operation in many multimedia\napplications. Images are often represented and stored as a set of\nhigh-dimensional features, which are extracted using localized feature\nextraction algorithms. Locality Sensitive Hashing is one of the most popular\napproximate processing techniques for finding similar points in\nhigh-dimensional spaces. Locality Sensitive Hashing (LSH) and its variants are\ndesigned to find similar points, but they are not designed to find objects\n(such as images, which are made up of a collection of points) efficiently. In\nthis paper, we propose an index structure, Bitmap-Image LSH (bImageLSH), for\nefficient processing of high-dimensional images. Using a real dataset, we\nexperimentally show the performance benefit of our novel design while keeping\nthe accuracy of the image results high.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Vector-Indexing", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [-16.70054817199707, 11.142290115356445], "cluster": 8}, {"key": "jafari2022experimental", "year": "2022", "citations": "1", "title": "Experimental Analysis Of Machine Learning Techniques For Finding Search Radius In Locality Sensitive Hashing", "abstract": "<p>Finding similar data in high-dimensional spaces is one of the important tasks\nin multimedia applications. Approaches introduced to find exact searching\ntechniques often use tree-based index structures which are known to suffer from\nthe curse of the dimensionality problem that limits their performance.\nApproximate searching techniques prefer performance over accuracy and they\nreturn good enough results while achieving a better performance. Locality\nSensitive Hashing (LSH) is one of the most popular approximate nearest neighbor\nsearch techniques for high-dimensional spaces. One of the most time-consuming\nprocesses in LSH is to find the neighboring points in the projected spaces. An\nimproved LSH-based index structure, called radius-optimized Locality Sensitive\nHashing (roLSH) has been proposed to utilize Machine Learning and efficiently\nfind these neighboring points; thus, further improve the overall performance of\nLSH. In this paper, we extend roLSH by experimentally studying the effect of\ndifferent types of famous Machine Learning techniques on overall performance.\nWe compare ten regression techniques on four real-world datasets and show that\nNeural Network-based techniques are the best fit to be used in roLSH as their\naccuracy and performance trade-off are the best compared to the other\ntechniques.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Vector-Indexing", "Tree-Based-Ann", "Datasets", "Evaluation"], "tsne_embedding": [43.28447723388672, 23.07438087463379], "cluster": 2}, {"key": "jain2009fast", "year": "2009", "citations": "265", "title": "Fast Similarity Search For Learned Metrics", "abstract": "<p>We propose a method to efficiently index into a large database of examples according to a learned metric.\nGiven a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric\nlearning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity\nconstraints. To enable sub-linear time similarity search under the learned metric, we show how\nto encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We\nfurther formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces\nwhose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.\nWe demonstrate the approach applied to systems and image datasets, and show that our learned metrics\nimprove accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-\ncient indexing with a learned distance and very large databases.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Similarity-Search", "Datasets"], "tsne_embedding": [-7.378785610198975, -13.422000885009766], "cluster": 1}, {"key": "jain2010hashing", "year": "2010", "citations": "69", "title": "Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning", "abstract": "<p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the \ndatabase. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method\u2019s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods\u2019 tradeoffs, and show that they make it practical to perform active selection with millions \nof unlabeled points.</p>\n", "tags": ["Hashing-Methods", "Scalability"], "tsne_embedding": [25.799072265625, 11.589046478271484], "cluster": 2}, {"key": "jain2016approximate", "year": "2016", "citations": "17", "title": "Approximate Search With Quantized Sparse Representations", "abstract": "<p>This paper tackles the task of storing a large collection of vectors, such as\nvisual descriptors, and of searching in it. To this end, we propose to\napproximate database vectors by constrained sparse coding, where possible atom\nweights are restricted to belong to a finite subset. This formulation\nencompasses, as particular cases, previous state-of-the-art methods such as\nproduct or residual quantization. As opposed to traditional sparse coding\nmethods, quantized sparse coding includes memory usage as a design constraint,\nthereby allowing us to index a large collection such as the BIGANN\nbillion-sized benchmark. Our experiments, carried out on standard benchmarks,\nshow that our formulation leads to competitive solutions when considering\ndifferent trade-offs between learning/coding time, index size and search\nquality.</p>\n", "tags": ["Quantization", "Memory-Efficiency", "Evaluation"], "tsne_embedding": [28.530879974365234, 12.797664642333984], "cluster": 2}, {"key": "jain2017compact", "year": "2017", "citations": "6", "title": "Compact Environment-invariant Codes For Robust Visual Place Recognition", "abstract": "<p>Robust visual place recognition (VPR) requires scene representations that are\ninvariant to various environmental challenges such as seasonal changes and\nvariations due to ambient lighting conditions during day and night. Moreover, a\npractical VPR system necessitates compact representations of environmental\nfeatures. To satisfy these requirements, in this paper we suggest a\nmodification to the existing pipeline of VPR systems to incorporate supervised\nhashing. The modified system learns (in a supervised setting) compact binary\ncodes from image feature descriptors. These binary codes imbibe robustness to\nthe visual variations exposed to it during the training phase, thereby, making\nthe system adaptive to severe environmental changes. Also, incorporating\nsupervised hashing makes VPR computationally more efficient and easy to\nimplement on simple hardware. This is because binary embeddings can be learned\nover simple-to-compute features and the distance computation is also in the\nlow-dimensional hamming space of binary codes. We have performed experiments on\nseveral challenging data sets covering seasonal, illumination and viewpoint\nvariations. We also compare two widely used supervised hashing methods of\nCCAITQ and MLH and show that this new pipeline out-performs or closely matches\nthe state-of-the-art deep learning VPR methods that are based on\nhigh-dimensional features extracted from pre-trained deep convolutional neural\nnetworks.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Robustness", "Supervised", "Neural-Hashing"], "tsne_embedding": [-38.01284408569336, -3.410447597503662], "cluster": 0}, {"key": "jain2017learning", "year": "2018", "citations": "14", "title": "Learning A Complete Image Indexing Pipeline", "abstract": "<p>To work at scale, a complete image indexing system comprises two components:\nAn inverted file index to restrict the actual search to only a subset that\nshould contain most of the items relevant to the query; An approximate distance\ncomputation mechanism to rapidly scan these lists. While supervised deep\nlearning has recently enabled improvements to the latter, the former continues\nto be based on unsupervised clustering in the literature. In this work, we\npropose a first system that learns both components within a unifying neural\nframework of structured binary encoding.</p>\n", "tags": ["Hashing-Methods", "Vector-Indexing", "CVPR", "Tools-&-Libraries", "Supervised", "Unsupervised"], "tsne_embedding": [-6.477030277252197, -8.53653335571289], "cluster": 1}, {"key": "jain2018learning", "year": "2018", "citations": "14", "title": "Learning A Complete Image Indexing Pipeline", "abstract": "<p>To work at scale, a complete image indexing system comprises two components:\nAn inverted file index to restrict the actual search to only a subset that\nshould contain most of the items relevant to the query; An approximate distance\ncomputation mechanism to rapidly scan these lists. While supervised deep\nlearning has recently enabled improvements to the latter, the former continues\nto be based on unsupervised clustering in the literature. In this work, we\npropose a first system that learns both components within a unifying neural\nframework of structured binary encoding.</p>\n", "tags": ["Hashing-Methods", "Vector-Indexing", "CVPR", "Tools-&-Libraries", "Supervised", "Unsupervised"], "tsne_embedding": [-6.477030277252197, -8.53653335571289], "cluster": 1}, {"key": "jain2021contrastive", "year": "2021", "citations": "1", "title": "Contrastive Learning Of Visual-semantic Embeddings", "abstract": "<p>Contrastive learning is a powerful technique to learn representations that\nare semantically distinctive and geometrically invariant. While most of the\nearlier approaches have demonstrated its effectiveness on single-modality\nlearning tasks such as image classification, recently there have been a few\nattempts towards extending this idea to multi-modal data. In this paper, we\npropose two loss functions based on normalized cross-entropy to perform the\ntask of learning joint visual-semantic embedding using batch contrastive\ntraining. In a batch, for a given anchor point from one modality, we consider\nits negatives only from another modality, and define our first contrastive loss\nbased on expected violations incurred by all the negatives. Next, we update\nthis loss and define the second contrastive loss based on the violation\nincurred only by the hardest negative. We compare our results with existing\nvisual-semantic embedding methods on cross-modal image-to-text and\ntext-to-image retrieval tasks using the MS-COCO and Flickr30K datasets, where\nwe outperform the state-of-the-art on the MS-COCO dataset and achieve\ncomparable results on the Flickr30K dataset.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Image-Retrieval", "Multimodal-Retrieval", "Datasets"], "tsne_embedding": [-25.88044548034668, -20.649864196777344], "cluster": 5}, {"key": "jain2023self", "year": "2023", "citations": "2", "title": "Self-supervised Multi-view Disentanglement For Expansion Of Visual Collections", "abstract": "<p>Image search engines enable the retrieval of images relevant to a query\nimage. In this work, we consider the setting where a query for similar images\nis derived from a collection of images. For visual search, the similarity\nmeasurements may be made along multiple axes, or views, such as style and\ncolor. We assume access to a set of feature extractors, each of which computes\nrepresentations for a specific view. Our objective is to design a retrieval\nalgorithm that effectively combines similarities computed over representations\nfrom multiple views. To this end, we propose a self-supervised learning method\nfor extracting disentangled view-specific representations for images such that\nthe inter-view overlap is minimized. We show how this allows us to compute the\nintent of a collection as a distribution over views. We show how effective\nretrieval can be performed by prioritizing candidate expansion images that\nmatch the intent of a query collection. Finally, we present a new querying\nmechanism for image search enabled by composing multiple collections and\nperform retrieval under this setting using the techniques presented in this\npaper.</p>\n", "tags": ["Supervised", "Self-Supervised", "Image-Retrieval"], "tsne_embedding": [-15.22790241241455, -8.570878982543945], "cluster": 1}, {"key": "jain2025fast", "year": "2009", "citations": "265", "title": "Fast Similarity Search For Learned Metrics", "abstract": "<p>We propose a method to efficiently index into a large database of examples according to a learned metric.\nGiven a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric\nlearning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity\nconstraints. To enable sub-linear time similarity search under the learned metric, we show how\nto encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We\nfurther formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces\nwhose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.\nWe demonstrate the approach applied to systems and image datasets, and show that our learned metrics\nimprove accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-\ncient indexing with a learned distance and very large databases.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Similarity-Search", "Datasets"], "tsne_embedding": [-7.378786087036133, -13.422000885009766], "cluster": 1}, {"key": "jain2025hashing", "year": "2010", "citations": "69", "title": "Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning", "abstract": "<p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the \ndatabase. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method\u2019s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods\u2019 tradeoffs, and show that they make it practical to perform active selection with millions \nof unlabeled points.</p>\n", "tags": ["Hashing-Methods", "Scalability"], "tsne_embedding": [25.7990665435791, 11.589665412902832], "cluster": 2}, {"key": "jaiswal2022ood", "year": "2022", "citations": "4", "title": "Ood-diskann: Efficient And Scalable Graph ANNS For Out-of-distribution Queries", "abstract": "<p>State-of-the-art algorithms for Approximate Nearest Neighbor Search (ANNS)\nsuch as DiskANN, FAISS-IVF, and HNSW build data dependent indices that offer\nsubstantially better accuracy and search efficiency over data-agnostic indices\nby overfitting to the index data distribution. When the query data is drawn\nfrom a different distribution - e.g., when index represents image embeddings\nand query represents textual embeddings - such algorithms lose much of this\nperformance advantage. On a variety of datasets, for a fixed recall target,\nlatency is worse by an order of magnitude or more for Out-Of-Distribution (OOD)\nqueries as compared to In-Distribution (ID) queries. The question we address in\nthis work is whether ANNS algorithms can be made efficient for OOD queries if\nthe index construction is given access to a small sample set of these queries.\nWe answer positively by presenting OOD-DiskANN, which uses a sparing sample (1%\nof index set size) of OOD queries, and provides up to 40% improvement in mean\nquery latency over SoTA algorithms of a similar memory footprint. OOD-DiskANN\nis scalable and has the efficiency of graph-based ANNS indices. Some of our\ncontributions can improve query efficiency for ID queries as well.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Vector-Indexing", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [21.34368896484375, 34.9739990234375], "cluster": 4}, {"key": "james2019deephashing", "year": "2019", "citations": "1", "title": "Deephashing Using Tripletloss", "abstract": "<p>Hashing is one of the most efficient techniques for approximate nearest\nneighbour search for large scale image retrieval. Most of the techniques are\nbased on hand-engineered features and do not give optimal results all the time.\nDeep Convolutional Neural Networks have proven to generate very effective\nrepresentation of images that are used for various computer vision tasks and\ninspired by this there have been several Deep Hashing models like Wang et al.\n(2016) have been proposed. These models train on the triplet loss function\nwhich can be used to train models with superior representation capabilities.\nTaking the latest advancements in training using the triplet loss I propose new\ntechniques that help the Deep Hash-ing models train more faster and\nefficiently. Experiment result1show that using the more efficient techniques\nfor training on the triplet loss, we have obtained a 5%percent improvement in\nour model compared to the original work of Wang et al.(2016). Using a larger\nmodel and more training data we can drastically improve the performance using\nthe techniques we propose</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Distance-Metric-Learning", "Image-Retrieval", "Evaluation"], "tsne_embedding": [-5.808608531951904, -3.505241870880127], "cluster": 1}, {"key": "jang2020generalized", "year": "2020", "citations": "41", "title": "Generalized Product Quantization Network For Semi-supervised Image Retrieval", "abstract": "<p>Image retrieval methods that employ hashing or vector quantization have\nachieved great success by taking advantage of deep learning. However, these\napproaches do not meet expectations unless expensive label information is\nsufficient. To resolve this issue, we propose the first quantization-based\nsemi-supervised image retrieval scheme: Generalized Product Quantization (GPQ)\nnetwork. We design a novel metric learning strategy that preserves semantic\nsimilarity between labeled data, and employ entropy regularization term to\nfully exploit inherent potentials of unlabeled data. Our solution increases the\ngeneralization capacity of the quantization network, which allows overcoming\nprevious limitations in the retrieval community. Extensive experimental results\ndemonstrate that GPQ yields state-of-the-art performance on large-scale real\nimage benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Quantization", "CVPR", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-9.577384948730469, -8.02954387664795], "cluster": 1}, {"key": "jang2021deep", "year": "2022", "citations": "23", "title": "Deep Hash Distillation For Image Retrieval", "abstract": "<p>In hash-based image retrieval systems, degraded or transformed inputs usually\ngenerate different codes from the original, deteriorating the retrieval\naccuracy. To mitigate this issue, data augmentation can be applied during\ntraining. However, even if augmented samples of an image are similar in real\nfeature space, the quantization can scatter them far away in Hamming space.\nThis results in representation discrepancies that can impede training and\ndegrade performance. In this work, we propose a novel self-distilled hashing\nscheme to minimize the discrepancy while exploiting the potential of augmented\ndata. By transferring the hash knowledge of the weakly-transformed samples to\nthe strong ones, we make the hash code insensitive to various transformations.\nWe also introduce hash proxy-based similarity learning and binary cross\nentropy-based quantization loss to provide fine quality hash codes. Ultimately,\nwe construct a deep hashing framework that not only improves the existing deep\nhashing approaches, but also achieves the state-of-the-art retrieval results.\nExtensive experiments are conducted and confirm the effectiveness of our work.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Tools-&-Libraries", "Evaluation", "Neural-Hashing"], "tsne_embedding": [-7.627143859863281, 11.053207397460938], "cluster": 8}, {"key": "jang2021self", "year": "2021", "citations": "52", "title": "Self-supervised Product Quantization For Deep Unsupervised Image Retrieval", "abstract": "<p>Supervised deep learning-based hash and vector quantization are enabling fast\nand large-scale image retrieval systems. By fully exploiting label annotations,\nthey are achieving outstanding retrieval performances compared to the\nconventional methods. However, it is painstaking to assign labels precisely for\na vast amount of training data, and also, the annotation process is\nerror-prone. To tackle these issues, we propose the first deep unsupervised\nimage retrieval method dubbed Self-supervised Product Quantization (SPQ)\nnetwork, which is label-free and trained in a self-supervised manner. We design\na Cross Quantized Contrastive learning strategy that jointly learns codewords\nand deep visual descriptors by comparing individually transformed images\n(views). Our method analyzes the image contents to extract descriptive\nfeatures, allowing us to understand image representations for accurate\nretrieval. By conducting extensive experiments on benchmarks, we demonstrate\nthat the proposed method yields state-of-the-art results even without\nsupervised pretraining.</p>\n", "tags": ["Self-Supervised", "ICCV", "Quantization", "Scalability", "Image-Retrieval", "Supervised", "Unsupervised"], "tsne_embedding": [-19.470632553100586, -21.450002670288086], "cluster": 5}, {"key": "jang2021similarity", "year": "2021", "citations": "3", "title": "Similarity Guided Deep Face Image Retrieval", "abstract": "<p>Face image retrieval, which searches for images of the same identity from the\nquery input face image, is drawing more attention as the size of the image\ndatabase increases rapidly. In order to conduct fast and accurate retrieval, a\ncompact hash code-based methods have been proposed, and recently, deep face\nimage hashing methods with supervised classification training have shown\noutstanding performance. However, classification-based scheme has a\ndisadvantage in that it cannot reveal complex similarities between face images\ninto the hash code learning. In this paper, we attempt to improve the face\nimage retrieval quality by proposing a Similarity Guided Hashing (SGH) method,\nwhich gently considers self and pairwise-similarity simultaneously. SGH employs\nvarious data augmentations designed to explore elaborate similarities between\nface images, solving both intra and inter identity-wise difficulties. Extensive\nexperimental results on the protocols with existing benchmarks and an\nadditionally proposed large scale higher resolution face image dataset\ndemonstrate that our SGH delivers state-of-the-art retrieval performance.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-12.862363815307617, 6.065816879272461], "cluster": 1}, {"key": "jang2021ultra", "year": "2021", "citations": "9", "title": "Ultra-high Dimensional Sparse Representations With Binarization For Efficient Text Retrieval", "abstract": "<p>The semantic matching capabilities of neural information retrieval can\nameliorate synonymy and polysemy problems of symbolic approaches. However,\nneural models\u2019 dense representations are more suitable for re-ranking, due to\ntheir inefficiency. Sparse representations, either in symbolic or latent form,\nare more efficient with an inverted index. Taking the merits of the sparse and\ndense representations, we propose an ultra-high dimensional (UHD)\nrepresentation scheme equipped with directly controllable sparsity. UHD\u2019s large\ncapacity and minimal noise and interference among the dimensions allow for\nbinarized representations, which are highly efficient for storage and search.\nAlso proposed is a bucketing method, where the embeddings from multiple layers\nof BERT are selected/merged to represent diverse linguistic aspects. We test\nour models with MS MARCO and TREC CAR, showing that our models outperforms\nother sparse models</p>\n", "tags": ["Re-Ranking", "Hybrid-Ann-Methods", "EMNLP", "Text-Retrieval"], "tsne_embedding": [5.826498031616211, -13.99974250793457], "cluster": 7}, {"key": "jang2024distilling", "year": "2024", "citations": "0", "title": "Distilling Vision-language Pretraining For Efficient Cross-modal Retrieval", "abstract": "<p>``Learning to hash\u2019\u2019 is a practical solution for efficient retrieval,\noffering fast search speed and low storage cost. It is widely applied in\nvarious applications, such as image-text cross-modal search. In this paper, we\nexplore the potential of enhancing the performance of learning to hash with the\nproliferation of powerful large pre-trained models, such as Vision-Language\nPre-training (VLP) models. We introduce a novel method named Distillation for\nCross-Modal Quantization (DCMQ), which leverages the rich semantic knowledge of\nVLP models to improve hash representation learning. Specifically, we use the\nVLP as a <code class=\"language-plaintext highlighter-rouge\">teacher' to distill knowledge into a </code>student\u2019 hashing model equipped\nwith codebooks. This process involves the replacement of supervised labels,\nwhich are composed of multi-hot vectors and lack semantics, with the rich\nsemantics of VLP. In the end, we apply a transformation termed Normalization\nwith Paired Consistency (NPC) to achieve a discriminative target for\ndistillation. Further, we introduce a new quantization method, Product\nQuantization with Gumbel (PQG) that promotes balanced codebook learning,\nthereby improving the retrieval performance. Extensive benchmark testing\ndemonstrates that DCMQ consistently outperforms existing supervised cross-modal\nhashing approaches, showcasing its significant potential.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Similarity-Search", "Memory-Efficiency", "Supervised", "Evaluation"], "tsne_embedding": [25.362897872924805, -38.87469482421875], "cluster": 7}, {"key": "janik2021zero", "year": "2021", "citations": "4", "title": "Zero In On Shape: A Generic 2D-3D Instance Similarity Metric Learned From Synthetic Data", "abstract": "<p>We present a network architecture which compares RGB images and untextured 3D\nmodels by the similarity of the represented shape. Our system is optimised for\nzero-shot retrieval, meaning it can recognise shapes never shown in training.\nWe use a view-based shape descriptor and a siamese network to learn object\ngeometry from pairs of 3D models and 2D images. Due to scarcity of datasets\nwith exact photograph-mesh correspondences, we train our network with only\nsynthetic data. Our experiments investigate the effect of different qualities\nand quantities of training data on retrieval accuracy and present insights from\nbridging the domain gap. We show that increasing the variety of synthetic data\nimproves retrieval accuracy and that our system\u2019s performance in zero-shot mode\ncan match that of the instance-aware mode, as far as narrowing down the search\nto the top 10% of objects.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-32.9792594909668, -8.687774658203125], "cluster": 5}, {"key": "jarrad2024tisis", "year": "2024", "citations": "0", "title": "TISIS : Trajectory Indexing For Similarity Search", "abstract": "<p>Social media platforms enable users to share diverse types of information,\nincluding geolocation data that captures their movement patterns. Such\ngeolocation data can be leveraged to reconstruct the trajectory of a user\u2019s\nvisited Points of Interest (POIs). A key requirement in numerous applications\nis the ability to measure the similarity between such trajectories, as this\nfacilitates the retrieval of trajectories that are similar to a given reference\ntrajectory. This is the main focus of our work. Existing methods predominantly\nrely on applying a similarity function to each candidate trajectory to identify\nthose that are sufficiently similar. However, this approach becomes\ncomputationally expensive when dealing with large-scale datasets. To mitigate\nthis challenge, we propose TISIS, an efficient method that uses trajectory\nindexing to quickly find similar trajectories that share common POIs in the\nsame order. Furthermore, to account for scenarios where POIs in trajectories\nmay not exactly match but are contextually similar, we introduce TISIS*, a\nvariant of TISIS that incorporates POI embeddings. This extension allows for\nmore comprehensive retrieval of similar trajectories by considering semantic\nsimilarities between POIs, beyond mere exact matches. Extensive experimental\nevaluations demonstrate that the proposed approach significantly outperforms a\nbaseline method based on the well-known Longest Common SubSequence (LCSS)\nalgorithm, yielding substantial performance improvements across various\nreal-world datasets.</p>\n", "tags": ["Evaluation", "Similarity-Search", "Scalability", "Datasets"], "tsne_embedding": [39.55396270751953, 10.25108528137207], "cluster": 9}, {"key": "jawade2025scot", "year": "2025", "citations": "0", "title": "SCOT: Self-supervised Contrastive Pretraining For Zero-shot Compositional Retrieval", "abstract": "<p>Compositional image retrieval (CIR) is a multimodal learning task where a\nmodel combines a query image with a user-provided text modification to retrieve\na target image. CIR finds applications in a variety of domains including\nproduct retrieval (e-commerce) and web search. Existing methods primarily focus\non fully-supervised learning, wherein models are trained on datasets of labeled\ntriplets such as FashionIQ and CIRR. This poses two significant challenges: (i)\ncurating such triplet datasets is labor intensive; and (ii) models lack\ngeneralization to unseen objects and domains. In this work, we propose SCOT\n(Self-supervised COmpositional Training), a novel zero-shot compositional\npretraining strategy that combines existing large image-text pair datasets with\nthe generative capabilities of large language models to contrastively train an\nembedding composition network. Specifically, we show that the text embedding\nfrom a large-scale contrastively-pretrained vision-language model can be\nutilized as proxy target supervision during compositional pretraining,\nreplacing the target image embedding. In zero-shot settings, this strategy\nsurpasses SOTA zero-shot compositional retrieval methods as well as many\nfully-supervised methods on standard benchmarks such as FashionIQ and CIRR.</p>\n", "tags": ["Self-Supervised", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Scalability", "Datasets", "Supervised"], "tsne_embedding": [-27.003360748291016, -16.8721866607666], "cluster": 5}, {"key": "jayaram2024data", "year": "2024", "citations": "0", "title": "Data-dependent LSH For The Earth Mover's Distance", "abstract": "<p>We give new data-dependent locality sensitive hashing schemes (LSH) for the\nEarth Mover\u2019s Distance (\\(\\mathsf{EMD}\\)), and as a result, improve the best\napproximation for nearest neighbor search under \\(\\mathsf{EMD}\\) by a quadratic\nfactor. Here, the metric \\(\\mathsf{EMD}_s(\\mathbb{R}^d,\\ell_p)\\) consists of sets\nof \\(s\\) vectors in \\(\\mathbb{R}^d\\), and for any two sets \\(x,y\\) of \\(s\\) vectors the\ndistance \\(\\mathsf{EMD}(x,y)\\) is the minimum cost of a perfect matching between\n\\(x,y\\), where the cost of matching two vectors is their \\(\\ell_p\\) distance.\nPreviously, Andoni, Indyk, and Krauthgamer gave a (data-independent)\nlocality-sensitive hashing scheme for \\(\\mathsf{EMD}_s(\\mathbb{R}^d,\\ell_p)\\)\nwhen \\(p \\in [1,2]\\) with approximation \\(O(log^2 s)\\). By being data-dependent,\nwe improve the approximation to \\(\\tilde{O}(log s)\\).\n  Our main technical contribution is to show that for any distribution \\(\\mu\\)\nsupported on the metric \\(\\mathsf{EMD}_s(\\mathbb{R}^d, \\ell_p)\\), there exists a\ndata-dependent LSH for dense regions of \\(\\mu\\) which achieves approximation\n\\(\\tilde{O}(log s)\\), and that the data-independent LSH actually achieves a\n\\(\\tilde{O}(log s)\\)-approximation outside of those dense regions. Finally, we\nshow how to \u201cglue\u201d together these two hashing schemes without any additional\nloss in the approximation.\n  Beyond nearest neighbor search, our data-dependent LSH also gives optimal\n(distributional) sketches for the Earth Mover\u2019s Distance. By known sketching\nlower bounds, this implies that our LSH is optimal (up to \\(\\mathrm{poly}(log\nlog s)\\) factors) among those that collide close points with constant\nprobability.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [21.59662437438965, 51.80207824707031], "cluster": 4}, {"key": "jedidi2024zero", "year": "2024", "citations": "0", "title": "Zero-shot Dense Retrieval With Embeddings From Relevance Feedback", "abstract": "<p>Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [9.364628791809082, -25.226186752319336], "cluster": 7}, {"key": "jegou2009searching", "year": "2009", "citations": "17", "title": "Searching With Quantization: Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators", "abstract": "<p>We propose an approximate nearest neighbor search method based\non quantization. It uses, in particular, product quantizer to produce short codes\nand corresponding distance estimators approximating the Euclidean distance\nbetween the orginal vectors. The method is advantageously used in an asymmetric\nmanner, by computing the distance between a vector and code, unlike\ncompeting techniques such as spectral hashing that only compare codes.\nOur approach approximates the Euclidean distance based on memory efficient codes and, thus, permits efficient nearest neighbor search. Experiments\nperformed on SIFT and GIST image descriptors show excellent search accuracy.\nThe method is shown to outperform two state-of-the-art approaches of the literature.\nTimings measured when searching a vector set of 2 billion vectors are\nshown to be excellent given the high accuracy of the method.</p>\n", "tags": ["Compact-Codes", "Quantization", "Hashing-Methods", "Distance-Metric-Learning"], "tsne_embedding": [6.776739120483398, 44.112457275390625], "cluster": 4}, {"key": "jegou2025searching", "year": "2009", "citations": "17", "title": "Searching With Quantization: Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators", "abstract": "<p>We propose an approximate nearest neighbor search method based\non quantization. It uses, in particular, product quantizer to produce short codes\nand corresponding distance estimators approximating the Euclidean distance\nbetween the orginal vectors. The method is advantageously used in an asymmetric\nmanner, by computing the distance between a vector and code, unlike\ncompeting techniques such as spectral hashing that only compare codes.\nOur approach approximates the Euclidean distance based on memory efficient codes and, thus, permits efficient nearest neighbor search. Experiments\nperformed on SIFT and GIST image descriptors show excellent search accuracy.\nThe method is shown to outperform two state-of-the-art approaches of the literature.\nTimings measured when searching a vector set of 2 billion vectors are\nshown to be excellent given the high accuracy of the method.</p>\n", "tags": ["Compact-Codes", "Quantization", "Hashing-Methods", "Distance-Metric-Learning"], "tsne_embedding": [6.776739120483398, 44.112457275390625], "cluster": 4}, {"key": "jeong2018efficient", "year": "2018", "citations": "9", "title": "Efficient End-to-end Learning For Quantizable Representations", "abstract": "<p>Embedding representation learning via neural networks is at the core\nfoundation of modern similarity based search. While much effort has been put in\ndeveloping algorithms for learning binary hamming code representations for\nsearch efficiency, this still requires a linear scan of the entire dataset per\neach query and trades off the search accuracy through binarization. To this\nend, we consider the problem of directly learning a quantizable embedding\nrepresentation and the sparse binary hash code end-to-end which can be used to\nconstruct an efficient hash table not only providing significant search\nreduction in the number of data but also achieving the state of the art search\naccuracy outperforming previous state of the art deep metric learning methods.\nWe also show that finding the optimal sparse binary hash code in a mini-batch\ncan be computed exactly in polynomial time by solving a minimum cost flow\nproblem. Our results on Cifar-100 and on ImageNet datasets show the state of\nthe art search accuracy in precision@k and NMI metrics while providing up to\n98X and 478X search speedup respectively over exhaustive linear search. The\nsource code is available at\nhttps://github.com/maestrojeong/Deep-Hash-Table-ICML18</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [23.28746223449707, 7.172863960266113], "cluster": 6}, {"key": "jeong2019end", "year": "2019", "citations": "0", "title": "End-to-end Efficient Representation Learning Via Cascading Combinatorial Optimization", "abstract": "<p>We develop hierarchically quantized efficient embedding representations for\nsimilarity-based search and show that this representation provides not only the\nstate of the art performance on the search accuracy but also provides several\norders of speed up during inference. The idea is to hierarchically quantize the\nrepresentation so that the quantization granularity is greatly increased while\nmaintaining the accuracy and keeping the computational complexity low. We also\nshow that the problem of finding the optimal sparse compound hash code\nrespecting the hierarchical structure can be optimized in polynomial time via\nminimum cost flow in an equivalent flow network. This allows us to train the\nmethod end-to-end in a mini-batch stochastic gradient descent setting. Our\nexperiments on Cifar100 and ImageNet datasets show the state of the art search\naccuracy while providing several orders of magnitude search speedup\nrespectively over exhaustive linear search over the dataset.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "CVPR", "Datasets", "Evaluation"], "tsne_embedding": [23.364208221435547, 17.925676345825195], "cluster": 2}, {"key": "jeong2022augmenting", "year": "2022", "citations": "7", "title": "Augmenting Document Representations For Dense Retrieval With Interpolation And Perturbation", "abstract": "<p>Dense retrieval models, which aim at retrieving the most relevant document\nfor an input query on a dense representation space, have gained considerable\nattention for their remarkable success. Yet, dense models require a vast amount\nof labeled training data for notable performance, whereas it is often\nchallenging to acquire query-document pairs annotated by humans. To tackle this\nproblem, we propose a simple but effective Document Augmentation for dense\nRetrieval (DAR) framework, which augments the representations of documents with\ntheir interpolation and perturbation. We validate the performance of DAR on\nretrieval tasks with two benchmark datasets, showing that the proposed DAR\nsignificantly outperforms relevant baselines on the dense retrieval of both the\nlabeled and unlabeled documents.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [6.954812526702881, -25.633235931396484], "cluster": 7}, {"key": "jeong20254bit", "year": "2025", "citations": "0", "title": "4bit-quantization In Vector-embedding For RAG", "abstract": "<p>Retrieval-augmented generation (RAG) is a promising technique that has shown\ngreat potential in addressing some of the limitations of large language models\n(LLMs). LLMs have two major limitations: they can contain outdated information\ndue to their training data, and they can generate factually inaccurate\nresponses, a phenomenon known as hallucinations. RAG aims to mitigate these\nissues by leveraging a database of relevant documents, which are stored as\nembedding vectors in a high-dimensional space. However, one of the challenges\nof using high-dimensional embeddings is that they require a significant amount\nof memory to store. This can be a major issue, especially when dealing with\nlarge databases of documents. To alleviate this problem, we propose the use of\n4-bit quantization to store the embedding vectors. This involves reducing the\nprecision of the vectors from 32-bit floating-point numbers to 4-bit integers,\nwhich can significantly reduce the memory requirements. Our approach has\nseveral benefits. Firstly, it significantly reduces the memory storage\nrequirements of the high-dimensional vector database, making it more feasible\nto deploy RAG systems in resource-constrained environments. Secondly, it speeds\nup the searching process, as the reduced precision of the vectors allows for\nfaster computation. Our code is available at\nhttps://github.com/taeheej/4bit-Quantization-in-Vector-Embedding-for-RAG</p>\n", "tags": ["Quantization", "Evaluation"], "tsne_embedding": [18.57962417602539, 20.12550926208496], "cluster": 2}, {"key": "jha2023mem", "year": "2023", "citations": "0", "title": "Mem-rec: Memory Efficient Recommendation System Using Alternative Representation", "abstract": "<p>Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI\nmodels to provide high-quality personalized recommendations. Training data used\nfor modern recommendation systems commonly includes categorical features taking\non tens-of-millions of possible distinct values. These categorical tokens are\ntypically assigned learned vector representations, that are stored in large\nembedding tables, on the order of 100s of GB. Storing and accessing these\ntables represent a substantial burden in commercial deployments. Our work\nproposes MEM-REC, a novel alternative representation approach for embedding\ntables. MEM-REC leverages bloom filters and hashing methods to encode\ncategorical features using two cache-friendly embedding tables. The first table\n(token embedding) contains raw embeddings (i.e. learned vector representation),\nand the second table (weight embedding), which is much smaller, contains\nweights to scale these raw embeddings to provide better discriminative\ncapability to each data point. We provide a detailed architecture, design and\nanalysis of MEM-REC addressing trade-offs in accuracy and computation\nrequirements, in comparison with state-of-the-art techniques. We show that\nMEM-REC can not only maintain the recommendation quality and significantly\nreduce the memory footprint for commercial scale recommendation models but can\nalso improve the embedding latency. In particular, based on our results,\nMEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by 2900x and\nperforms up to 3.4x faster embeddings while achieving the same AUC as that of\nthe full uncompressed model.</p>\n", "tags": ["Memory-Efficiency", "Evaluation", "Recommender-Systems", "Hashing-Methods"], "tsne_embedding": [23.288564682006836, -25.253873825073242], "cluster": 7}, {"key": "jha2024jina", "year": "2024", "citations": "1", "title": "Jina-colbert-v2: A General-purpose Multilingual Late Interaction Retriever", "abstract": "<p>Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT\u2019s late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis work we propose a number of incremental improvements to the ColBERT model\narchitecture and training pipeline, using methods shown to work in the more\nmature single-vector embedding model training paradigm, particularly those that\napply to heterogeneous multilingual data or boost efficiency with little\ntradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance\nacross a range of English and multilingual retrieval tasks.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [12.74345874786377, -26.182636260986328], "cluster": 7}, {"key": "jhuo2017set", "year": "2017", "citations": "0", "title": "Set-to-set Hashing With Applications In Visual Recognition", "abstract": "<p>Visual data, such as an image or a sequence of video frames, is often\nnaturally represented as a point set. In this paper, we consider the\nfundamental problem of finding a nearest set from a collection of sets, to a\nquery set. This problem has obvious applications in large-scale visual\nretrieval and recognition, and also in applied fields beyond computer vision.\nOne challenge stands out in solving the problem\u2014set representation and\nmeasure of similarity. Particularly, the query set and the sets in dataset\ncollection can have varying cardinalities. The training collection is large\nenough such that linear scan is impractical. We propose a simple representation\nscheme that encodes both statistical and structural information of the sets.\nThe derived representations are integrated in a kernel framework for flexible\nsimilarity measurement. For the query set process, we adopt a learning-to-hash\npipeline that turns the kernel representations into hash bits based on simple\nlearners, using multiple kernel learning. Experiments on two visual retrieval\ndatasets show unambiguously that our set-to-set hashing framework outperforms\nprior methods that do not take the set-to-set search setting.</p>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [15.935595512390137, -6.79144287109375], "cluster": 6}, {"key": "ji2017cross", "year": "2017", "citations": "79", "title": "Cross-domain Image Retrieval With Attention Modeling", "abstract": "<p>With the proliferation of e-commerce websites and the ubiquitousness of smart\nphones, cross-domain image retrieval using images taken by smart phones as\nqueries to search products on e-commerce websites is emerging as a popular\napplication. One challenge of this task is to locate the attention of both the\nquery and database images. In particular, database images, e.g. of fashion\nproducts, on e-commerce websites are typically displayed with other\naccessories, and the images taken by users contain noisy background and large\nvariations in orientation and lighting. Consequently, their attention is\ndifficult to locate. In this paper, we exploit the rich tag information\navailable on the e-commerce websites to locate the attention of database\nimages. For query images, we use each candidate image in the database as the\ncontext to locate the query attention. Novel deep convolutional neural network\narchitectures, namely TagYNet and CtxYNet, are proposed to learn the attention\nweights and then extract effective representations of the images. Experimental\nresults on public datasets confirm that our approaches have significant\nimprovement over the existing methods in terms of the retrieval accuracy and\nefficiency.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Datasets"], "tsne_embedding": [28.518083572387695, -27.22968101501465], "cluster": 7}, {"key": "ji2018attribute", "year": "2020", "citations": "83", "title": "Attribute-guided Network For Cross-modal Zero-shot Hashing", "abstract": "<p>Zero-Shot Hashing aims at learning a hashing model that is trained only by\ninstances from seen categories but can generate well to those of unseen\ncategories. Typically, it is achieved by utilizing a semantic embedding space\nto transfer knowledge from seen domain to unseen domain. Existing efforts\nmainly focus on single-modal retrieval task, especially Image-Based Image\nRetrieval (IBIR). However, as a highlighted research topic in the field of\nhashing, cross-modal retrieval is more common in real world applications. To\naddress the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task, we propose a\nnovel Attribute-Guided Network (AgNet), which can perform not only IBIR, but\nalso Text-Based Image Retrieval (TBIR). In particular, AgNet aligns different\nmodal data into a semantically rich attribute space, which bridges the gap\ncaused by modality heterogeneity and zero-shot setting. We also design an\neffective strategy that exploits the attribute to guide the generation of hash\ncodes for image and text within the same network. Extensive experimental\nresults on three benchmark datasets (AwA, SUN, and ImageNet) demonstrate the\nsuperiority of AgNet on both cross-modal and single-modal zero-shot image\nretrieval tasks.</p>\n", "tags": ["Hashing-Methods", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-9.979378700256348, 13.508806228637695], "cluster": 8}, {"key": "jia2019efficient", "year": "2019", "citations": "129", "title": "Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms", "abstract": "<p>Given a data set \\(\\mathcal{D}\\) containing millions of data points and a data\nconsumer who is willing to pay for $\\(X\\) to train a machine learning (ML) model\nover \\(\\mathcal{D}\\), how should we distribute this $\\(X\\) to each data point to\nreflect its \u201cvalue\u201d? In this paper, we define the \u201crelative value of data\u201d via\nthe Shapley value, as it uniquely possesses properties with appealing\nreal-world interpretations, such as fairness, rationality and\ndecentralizability. For general, bounded utility functions, the Shapley value\nis known to be challenging to compute: to get Shapley values for all \\(N\\) data\npoints, it requires \\(O(2^N)\\) model evaluations for exact computation and\n\\(O(Nlog N)\\) for \\((\\epsilon, \\delta)\\)-approximation. In this paper, we focus on\none popular family of ML models relying on \\(K\\)-nearest neighbors (\\(K\\)NN). The\nmost surprising result is that for unweighted \\(K\\)NN classifiers and regressors,\nthe Shapley value of all \\(N\\) data points can be computed, exactly, in \\(O(Nlog\nN)\\) time \u2013 an exponential improvement on computational complexity! Moreover,\nfor \\((\\epsilon, \\delta)\\)-approximation, we are able to develop an algorithm\nbased on Locality Sensitive Hashing (LSH) with only sublinear complexity\n\\(O(N^{h(\\epsilon,K)}log N)\\) when \\(\\epsilon\\) is not too small and \\(K\\) is not\ntoo large. We empirically evaluate our algorithms on up to \\(10\\) million data\npoints and even our exact algorithm is up to three orders of magnitude faster\nthan the baseline approximation algorithm. The LSH-based approximation\nalgorithm can accelerate the value calculation process even further. We then\nextend our algorithms to other scenarios such as (1) weighed \\(K\\)NN classifiers,\n(2) different data points are clustered by different data curators, and (3)\nthere are data analysts providing computation who also requires proper\nvaluation.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [19.427654266357422, 47.324981689453125], "cluster": 4}, {"key": "jia2022fast", "year": "2023", "citations": "4", "title": "Fast Online Hashing With Multi-label Projection", "abstract": "<p>Hashing has been widely researched to solve the large-scale approximate\nnearest neighbor search problem owing to its time and storage superiority. In\nrecent years, a number of online hashing methods have emerged, which can update\nthe hash functions to adapt to the new stream data and realize dynamic\nretrieval. However, existing online hashing methods are required to update the\nwhole database with the latest hash functions when a query arrives, which leads\nto low retrieval efficiency with the continuous increase of the stream data. On\nthe other hand, these methods ignore the supervision relationship among the\nexamples, especially in the multi-label case. In this paper, we propose a novel\nFast Online Hashing (FOH) method which only updates the binary codes of a small\npart of the database. To be specific, we first build a query pool in which the\nnearest neighbors of each central point are recorded. When a new query arrives,\nonly the binary codes of the corresponding potential neighbors are updated. In\naddition, we create a similarity matrix which takes the multi-label supervision\ninformation into account and bring in the multi-label projection loss to\nfurther preserve the similarity among the multi-label data. The experimental\nresults on two common benchmarks show that the proposed FOH can achieve\ndramatic superiority on query time up to 6.28 seconds less than\nstate-of-the-art baselines with competitive retrieval accuracy.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "AAAI", "Compact-Codes"], "tsne_embedding": [31.20171356201172, 6.597024440765381], "cluster": 2}, {"key": "jia2023fast", "year": "2023", "citations": "4", "title": "Fast Online Hashing With Multi-label Projection", "abstract": "<p>Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years, a number of online hashing methods have emerged, which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However, existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives, which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand, these methods ignore the supervision relationship among the examples, especially in the multi-label case. In this paper, we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific, we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives, only the binary codes of the corresponding potential neighbors are updated. In addition, we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "AAAI", "Compact-Codes"], "tsne_embedding": [31.20171356201172, 6.597024440765381], "cluster": 2}, {"key": "jia2025fast", "year": "2023", "citations": "4", "title": "Fast Online Hashing With Multi-label Projection", "abstract": "<p>Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years, a number of online hashing methods have emerged, which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However, existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives, which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand, these methods ignore the supervision relationship among the examples, especially in the multi-label case. In this paper, we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific, we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives, only the binary codes of the corresponding potential neighbors are updated. In addition, we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "AAAI", "Compact-Codes"], "tsne_embedding": [31.20212173461914, 6.596783638000488], "cluster": 2}, {"key": "jian2020fast", "year": "2020", "citations": "2", "title": "Fast Top-k Cosine Similarity Search Through Xor-friendly Binary Quantization On Gpus", "abstract": "<p>We explore the use of GPU for accelerating large scale nearest neighbor\nsearch and we propose a fast vector-quantization-based exhaustive nearest\nneighbor search algorithm that can achieve high accuracy without any indexing\nconstruction specifically designed for cosine similarity. This algorithm uses a\nnovel XOR-friendly binary quantization method to encode floating-point numbers\nsuch that high-complexity multiplications can be optimized as low-complexity\nbitwise operations. Experiments show that, our quantization method takes short\npreprocessing time, and helps make the search speed of our exhaustive search\nmethod much more faster than that of popular approximate nearest neighbor\nalgorithms when high accuracy is needed.</p>\n", "tags": ["Quantization", "Distance-Metric-Learning", "Similarity-Search"], "tsne_embedding": [46.661502838134766, 17.616209030151367], "cluster": 9}, {"key": "jian2023invgc", "year": "2023", "citations": "1", "title": "Invgc: Robust Cross-modal Retrieval By Inverse Graph Convolution", "abstract": "<p>Over recent decades, significant advancements in cross-modal retrieval are\nmainly driven by breakthroughs in visual and linguistic modeling. However, a\nrecent study shows that multi-modal data representations tend to cluster within\na limited convex cone (as representation degeneration problem), which hinders\nretrieval performance due to the inseparability of these representations. In\nour study, we first empirically validate the presence of the representation\ndegeneration problem across multiple cross-modal benchmarks and methods. Next,\nto address it, we introduce a novel method, called InvGC, a post-processing\ntechnique inspired by graph convolution and average pooling. Specifically,\nInvGC defines the graph topology within the datasets and then applies graph\nconvolution in a subtractive manner. This method effectively separates\nrepresentations by increasing the distances between data points. To improve the\nefficiency and effectiveness of InvGC, we propose an advanced graph topology,\nLocalAdj, which only aims to increase the distances between each data point and\nits nearest neighbors. To understand why InvGC works, we present a detailed\ntheoretical analysis, proving that the lower bound of recall will be improved\nafter deploying InvGC. Extensive empirical results show that InvGC and InvGC\nw/LocalAdj significantly mitigate the representation degeneration problem,\nthereby enhancing retrieval performance.\n  Our code is available at\nhttps://github.com/yimuwangcs/Better_Cross_Modal_Retrieval</p>\n", "tags": ["EMNLP", "Efficiency", "Multimodal-Retrieval", "Datasets", "Re-Ranking", "Evaluation"], "tsne_embedding": [54.8365364074707, -1.2357146739959717], "cluster": 9}, {"key": "jiang2015scalable", "year": "2015", "citations": "208", "title": "Scalable Graph Hashing With Feature Transformation", "abstract": "<p>Hashing has been widely used for approximate nearest\nneighbor (ANN) search in big data applications\nbecause of its low storage cost and fast retrieval\nspeed. The goal of hashing is to map the data\npoints from the original space into a binary-code\nspace where the similarity (neighborhood structure)\nin the original space is preserved. By directly\nexploiting the similarity to guide the hashing\ncode learning procedure, graph hashing has attracted\nmuch attention. However, most existing graph\nhashing methods cannot achieve satisfactory performance\nin real applications due to the high complexity\nfor graph modeling. In this paper, we propose\na novel method, called scalable graph hashing\nwith feature transformation (SGH), for large-scale\ngraph hashing. Through feature transformation, we\ncan effectively approximate the whole graph without\nexplicitly computing the similarity graph matrix,\nbased on which a sequential learning method\nis proposed to learn the hash functions in a bit-wise\nmanner. Experiments on two datasets with one million\ndata points show that our SGH method can\noutperform the state-of-the-art methods in terms of\nboth accuracy and scalability.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [55.15164566040039, 6.413501739501953], "cluster": 9}, {"key": "jiang2016deep", "year": "2017", "citations": "752", "title": "Deep Cross-modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, cross-modal hashing (CMH)\nhas been widely used for similarity search in multimedia retrieval\napplications. However, almost all existing CMH methods are based on\nhand-crafted features which might not be optimally compatible with the\nhash-code learning procedure. As a result, existing CMH methods with\nhandcrafted features may not achieve satisfactory performance. In this paper,\nwe propose a novel cross-modal hashing method, called deep crossmodal hashing\n(DCMH), by integrating feature learning and hash-code learning into the same\nframework. DCMH is an end-to-end learning framework with deep neural networks,\none for each modality, to perform feature learning from scratch. Experiments on\ntwo real datasets with text-image modalities show that DCMH can outperform\nother baselines to achieve the state-of-the-art performance in cross-modal\nretrieval applications.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Similarity-Search", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [0.8748953342437744, -8.673718452453613], "cluster": 1}, {"key": "jiang2017asymmetric", "year": "2018", "citations": "250", "title": "Asymmetric Deep Supervised Hashing", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "AAAI", "Supervised", "Evaluation"], "tsne_embedding": [18.119821548461914, 3.7283122539520264], "cluster": 6}, {"key": "jiang2017deep", "year": "2017", "citations": "752", "title": "Deep Cross-modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity\nsearch in multimedia retrieval applications. However, most\nexisting CMH methods are based on hand-crafted features\nwhich might not be optimally compatible with the hash-code\nlearning procedure. As a result, existing CMH methods\nwith hand-crafted features may not achieve satisfactory\nperformance. In this paper, we propose a novel CMH\nmethod, called deep cross-modal hashing (DCMH), by\nintegrating feature learning and hash-code learning into\nthe same framework. DCMH is an end-to-end learning\nframework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments\non three real datasets with image-text modalities show\nthat DCMH can outperform other baselines to achieve\nthe state-of-the-art performance in cross-modal retrieval\napplications.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Multimodal-Retrieval", "Tools-&-Libraries", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [0.915550947189331, -8.713764190673828], "cluster": 1}, {"key": "jiang2017discrete", "year": "2019", "citations": "143", "title": "Discrete Latent Factor Model For Cross-modal Hashing", "abstract": "<p>Due to its storage and retrieval efficiency, cross-modal hashing~(CMH) has\nbeen widely used for cross-modal similarity search in multimedia applications.\nAccording to the training strategy, existing CMH methods can be mainly divided\ninto two categories: relaxation-based continuous methods and discrete methods.\nIn general, the training of relaxation-based continuous methods is faster than\ndiscrete methods, but the accuracy of relaxation-based continuous methods is\nnot satisfactory. On the contrary, the accuracy of discrete methods is\ntypically better than relaxation-based continuous methods, but the training of\ndiscrete methods is time-consuming. In this paper, we propose a novel CMH\nmethod, called discrete latent factor model based cross-modal hashing~(DLFH),\nfor cross modal similarity search. DLFH is a discrete method which can directly\nlearn the binary hash codes for CMH. At the same time, the training of DLFH is\nefficient. Experiments on real datasets show that DLFH can achieve\nsignificantly better accuracy than existing methods, and the training time of\nDLFH is comparable to that of relaxation-based continuous methods which are\nmuch faster than existing discrete methods.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Similarity-Search", "Datasets"], "tsne_embedding": [18.7603759765625, 5.580205917358398], "cluster": 6}, {"key": "jiang2019evaluation", "year": "2019", "citations": "4", "title": "On The Evaluation Metric For Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, hashing has been widely\nused for large-scale approximate nearest neighbor (ANN) search. Bucket search,\nalso called hash lookup, can achieve fast query speed with a sub-linear time\ncost based on the inverted index table constructed from hash codes. Many\nmetrics have been adopted to evaluate hashing algorithms. However, all existing\nmetrics are improper to evaluate the hash codes for bucket search. On one hand,\nall existing metrics ignore the retrieval time cost which is an important\nfactor reflecting the performance of search. On the other hand, some of them,\nsuch as mean average precision (MAP), suffer from the uncertainty problem as\nthe ranked list is based on integer-valued Hamming distance, and are\ninsensitive to Hamming radius as these metrics only depend on relative Hamming\ndistance. Other metrics, such as precision at Hamming radius R, fail to\nevaluate global performance as these metrics only depend on one specific\nHamming radius. In this paper, we first point out the problems of existing\nmetrics which have been ignored by the hashing community, and then propose a\nnovel evaluation metric called radius aware mean average precision (RAMAP) to\nevaluate hash codes for bucket search. Furthermore, two coding strategies are\nalso proposed to qualitatively show the problems of existing metrics.\nExperiments demonstrate that our proposed RAMAP can provide more proper\nevaluation than existing metrics.</p>\n", "tags": ["Memory-Efficiency", "Evaluation", "Hashing-Methods", "Scalability"], "tsne_embedding": [20.26970863342285, 32.93305587768555], "cluster": 4}, {"key": "jiang2019graph", "year": "2020", "citations": "28", "title": "Graph-based Multi-view Binary Learning For Image Clustering", "abstract": "<p>Hashing techniques, also known as binary code learning, have recently gained\nincreasing attention in large-scale data analysis and storage. Generally, most\nexisting hash clustering methods are single-view ones, which lack complete\nstructure or complementary information from multiple views. For cluster tasks,\nabundant prior researches mainly focus on learning discrete hash code while few\nworks take original data structure into consideration. To address these\nproblems, we propose a novel binary code algorithm for clustering, which adopts\ngraph embedding to preserve the original data structure, called (Graph-based\nMulti-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding\nthe information of multiple views into a compact binary code, which explores\ncomplementary information from multiple views. In particular, in order to\nmaintain the graph-based structure of the original data, we adopt a Laplacian\nmatrix to preserve the local linear relationship of the data and map it to the\nHamming space. Considering different views have distinctive contributions to\nthe final clustering results, GMBL adopts a strategy of automatically assign\nweights for each view to better guide the clustering. Finally, An alternating\niterative optimization method is adopted to optimize discrete binary codes\ndirectly instead of relaxing the binary constraint in two steps. Experiments on\nfive public datasets demonstrate the superiority of our proposed method\ncompared with previous approaches in terms of clustering performance.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Scalability", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [44.43452453613281, 0.5186992287635803], "cluster": 9}, {"key": "jiang2022givens", "year": "2022", "citations": "1", "title": "Givens Coordinate Descent Methods For Rotation Matrix Learning In Trainable Embedding Indexes", "abstract": "<p>Product quantization (PQ) coupled with a space rotation, is widely used in\nmodern approximate nearest neighbor (ANN) search systems to significantly\ncompress the disk storage for embeddings and speed up the inner product\ncomputation. Existing rotation learning methods, however, minimize quantization\ndistortion for fixed embeddings, which are not applicable to an end-to-end\ntraining scenario where embeddings are updated constantly. In this paper, based\non geometric intuitions from Lie group theory, in particular the special\northogonal group \\(SO(n)\\), we propose a family of block Givens coordinate\ndescent algorithms to learn rotation matrix that are provably convergent on any\nconvex objectives. Compared to the state-of-the-art SVD method, the Givens\nalgorithms are much more parallelizable, reducing runtime by orders of\nmagnitude on modern GPUs, and converge more stably according to experimental\nstudies. They further improve upon vanilla product quantization significantly\nin an end-to-end training scenario.</p>\n", "tags": ["Quantization"], "tsne_embedding": [11.33708667755127, 39.23868942260742], "cluster": 4}, {"key": "jiang2023dual", "year": "2023", "citations": "1", "title": "Dual Relation Alignment For Composed Image Retrieval", "abstract": "<p>Composed image retrieval, a task involving the search for a target image\nusing a reference image and a complementary text as the query, has witnessed\nsignificant advancements owing to the progress made in cross-modal modeling.\nUnlike the general image-text retrieval problem with only one alignment\nrelation, i.e., image-text, we argue for the existence of two types of\nrelations in composed image retrieval. The explicit relation pertains to the\nreference image &amp; complementary text-target image, which is commonly exploited\nby existing methods. Besides this intuitive relation, the observations during\nour practice have uncovered another implicit yet crucial relation, i.e.,\nreference image &amp; target image-complementary text, since we found that the\ncomplementary text can be inferred by studying the relation between the target\nimage and the reference image. Regrettably, existing methods largely focus on\nleveraging the explicit relation to learn their networks, while overlooking the\nimplicit relation. In response to this weakness, We propose a new framework for\ncomposed image retrieval, termed dual relation alignment, which integrates both\nexplicit and implicit relations to fully exploit the correlations among the\ntriplets. Specifically, we design a vision compositor to fuse reference image\nand target image at first, then the resulted representation will serve two\nroles: (1) counterpart for semantic alignment with the complementary text and\n(2) compensation for the complementary text to boost the explicit relation\nmodeling, thereby implant the implicit relation into the alignment learning.\nOur method is evaluated on two popular datasets, CIRR and FashionIQ, through\nextensive experiments. The results confirm the effectiveness of our\ndual-relation learning in substantially enhancing composed image retrieval\nperformance.</p>\n", "tags": ["Text-Retrieval", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-29.821958541870117, -19.43283462524414], "cluster": 5}, {"key": "jiang2023efficient", "year": "2023", "citations": "4", "title": "Efficient Match Pair Retrieval For Large-scale UAV Images Via Graph Indexed Global Descriptor", "abstract": "<p>SfM (Structure from Motion) has been extensively used for UAV (Unmanned\nAerial Vehicle) image orientation. Its efficiency is directly influenced by\nfeature matching. Although image retrieval has been extensively used for match\npair selection, high computational costs are consumed due to a large number of\nlocal features and the large size of the used codebook. Thus, this paper\nproposes an efficient match pair retrieval method and implements an integrated\nworkflow for parallel SfM reconstruction. First, an individual codebook is\ntrained online by considering the redundancy of UAV images and local features,\nwhich avoids the ambiguity of training codebooks from other datasets. Second,\nlocal features of each image are aggregated into a single high-dimension global\ndescriptor through the VLAD (Vector of Locally Aggregated Descriptors)\naggregation by using the trained codebook, which remarkably reduces the number\nof features and the burden of nearest neighbor searching in image indexing.\nThird, the global descriptors are indexed via the HNSW (Hierarchical Navigable\nSmall World) based graph structure for the nearest neighbor searching. Match\npairs are then retrieved by using an adaptive threshold selection strategy and\nutilized to create a view graph for divide-and-conquer based parallel SfM\nreconstruction. Finally, the performance of the proposed solution has been\nverified using three large-scale UAV datasets. The test results demonstrate\nthat the proposed solution accelerates match pair retrieval with a speedup\nratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction\nwith competitive accuracy in both relative and absolute orientation.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-30.51548957824707, 5.4313507080078125], "cluster": 0}, {"key": "jiang2024arnet", "year": "2025", "citations": "0", "title": "Arnet: Self-supervised FG-SBIR With Unified Sample Feature Alignment And Multi-scale Token Recycling", "abstract": "<p>Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims to minimize the\ndistance between sketches and corresponding images in the embedding space.\nHowever, scalability is hindered by the growing complexity of solutions, mainly\ndue to the abstract nature of fine-grained sketches. In this paper, we propose\nan effective approach to narrow the gap between the two domains. It mainly\nfacilitates unified mutual information sharing both intra- and inter-samples,\nrather than treating them as a single feature alignment problem between\nmodalities. Specifically, our approach includes: (i) Employing dual\nweight-sharing networks to optimize alignment within the sketch and image\ndomain, which also effectively mitigates model learning saturation issues. (ii)\nIntroducing an objective optimization function based on contrastive loss to\nenhance the model\u2019s ability to align features in both intra- and inter-samples.\n(iii) Presenting a self-supervised Multi-Scale Token Recycling (MSTR) Module\nfeatured by recycling discarded patch tokens in multi-scale features, further\nenhancing representation capability and retrieval performance. Our framework\nachieves excellent results on CNN- and ViT-based backbones. Extensive\nexperiments demonstrate its superiority over existing methods. We also\nintroduce Cloths-V1, the first professional fashion sketch-image dataset,\nutilized to validate our method and will be beneficial for other applications.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Scalability", "Image-Retrieval", "AAAI", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-42.81353759765625, -13.312966346740723], "cluster": 5}, {"key": "jiang2025deep", "year": "2017", "citations": "752", "title": "Deep Cross-modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity\nsearch in multimedia retrieval applications. However, most\nexisting CMH methods are based on hand-crafted features\nwhich might not be optimally compatible with the hash-code\nlearning procedure. As a result, existing CMH methods\nwith hand-crafted features may not achieve satisfactory\nperformance. In this paper, we propose a novel CMH\nmethod, called deep cross-modal hashing (DCMH), by\nintegrating feature learning and hash-code learning into\nthe same framework. DCMH is an end-to-end learning\nframework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments\non three real datasets with image-text modalities show\nthat DCMH can outperform other baselines to achieve\nthe state-of-the-art performance in cross-modal retrieval\napplications.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Multimodal-Retrieval", "Tools-&-Libraries", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [0.915605902671814, -8.71377182006836], "cluster": 1}, {"key": "jiang2025scalable", "year": "2015", "citations": "208", "title": "Scalable Graph Hashing With Feature Transformation", "abstract": "<p>Hashing has been widely used for approximate nearest\nneighbor (ANN) search in big data applications\nbecause of its low storage cost and fast retrieval\nspeed. The goal of hashing is to map the data\npoints from the original space into a binary-code\nspace where the similarity (neighborhood structure)\nin the original space is preserved. By directly\nexploiting the similarity to guide the hashing\ncode learning procedure, graph hashing has attracted\nmuch attention. However, most existing graph\nhashing methods cannot achieve satisfactory performance\nin real applications due to the high complexity\nfor graph modeling. In this paper, we propose\na novel method, called scalable graph hashing\nwith feature transformation (SGH), for large-scale\ngraph hashing. Through feature transformation, we\ncan effectively approximate the whole graph without\nexplicitly computing the similarity graph matrix,\nbased on which a sequential learning method\nis proposed to learn the hash functions in a bit-wise\nmanner. Experiments on two datasets with one million\ndata points show that our SGH method can\noutperform the state-of-the-art methods in terms of\nboth accuracy and scalability.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [55.15180587768555, 6.413463592529297], "cluster": 9}, {"key": "jimenez2017class", "year": "2017", "citations": "71", "title": "Class-weighted Convolutional Features For Visual Instance Search", "abstract": "<p>Image retrieval in realistic scenarios targets large dynamic datasets of\nunlabeled images. In these cases, training or fine-tuning a model every time\nnew images are added to the database is neither efficient nor scalable.\nConvolutional neural networks trained for image classification over large\ndatasets have been proven effective feature extractors for image retrieval. The\nmost successful approaches are based on encoding the activations of\nconvolutional layers, as they convey the image spatial information. In this\npaper, we go beyond this spatial information and propose a local-aware encoding\nof convolutional features based on semantic information predicted in the target\nimage. To this end, we obtain the most discriminative regions of an image using\nClass Activation Maps (CAMs). CAMs are based on the knowledge contained in the\nnetwork and therefore, our approach, has the additional advantage of not\nrequiring external information. In addition, we use CAMs to generate object\nproposals during an unsupervised re-ranking stage after a first fast search.\nOur experiments on two public available datasets for instance retrieval,\nOxford5k and Paris6k, demonstrate the competitiveness of our approach\noutperforming the current state-of-the-art when using off-the-shelf models\ntrained on ImageNet. The source code and model used in this paper are publicly\navailable at http://imatge-upc.github.io/retrieval-2017-cam/.</p>\n", "tags": ["Image-Retrieval", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Unsupervised"], "tsne_embedding": [-30.116498947143555, -0.6621286273002625], "cluster": 0}, {"key": "jin2013complementary", "year": "2013", "citations": "59", "title": "Complementary Projection Hashing", "abstract": "<p>Recently, hashing techniques have been widely applied\nto solve the approximate nearest neighbors search problem\nin many vision applications. Generally, these hashing\napproaches generate 2^c buckets, where c is the length\nof the hash code. A good hashing method should satisfy\nthe following two requirements: 1) mapping the nearby\ndata points into the same bucket or nearby (measured by\nthe Hamming distance) buckets. 2) all the data points are\nevenly distributed among all the buckets. In this paper,\nwe propose a novel algorithm named Complementary Projection\nHashing (CPH) to find the optimal hashing functions\nwhich explicitly considers the above two requirements.\nSpecifically, CPH aims at sequentially finding a series of hyperplanes\n(hashing functions) which cross the sparse region\nof the data. At the same time, the data points are evenly distributed\nin the hypercubes generated by these hyperplanes.\nThe experiments comparing with the state-of-the-art hashing\nmethods demonstrate the effectiveness of the proposed\nmethod.</p>\n", "tags": ["ICCV", "Hashing-Methods"], "tsne_embedding": [21.110654830932617, 21.628267288208008], "cluster": 2}, {"key": "jin2017ranking", "year": "2017", "citations": "197", "title": "Ranking Based Locality Sensitive Hashing Enabled Cancelable Biometrics: Index-of-max Hashing", "abstract": "<p>In this paper, we propose a ranking based locality sensitive hashing inspired\ntwo-factor cancelable biometrics, dubbed \u201cIndex-of-Max\u201d (IoM) hashing for\nbiometric template protection. With externally generated random parameters, IoM\nhashing transforms a real-valued biometric feature vector into discrete index\n(max ranked) hashed code. We demonstrate two realizations from IoM hashing\nnotion, namely Gaussian Random Projection based and Uniformly Random\nPermutation based hashing schemes. The discrete indices representation nature\nof IoM hashed codes enjoy serveral merits. Firstly, IoM hashing empowers strong\nconcealment to the biometric information. This contributes to the solid ground\nof non-invertibility guarantee. Secondly, IoM hashing is insensitive to the\nfeatures magnitude, hence is more robust against biometric features variation.\nThirdly, the magnitude-independence trait of IoM hashing makes the hash codes\nbeing scale-invariant, which is critical for matching and feature alignment.\nThe experimental results demonstrate favorable accuracy performance on\nbenchmark FVC2002 and FVC2004 fingerprint databases. The analyses justify its\nresilience to the existing and newly introduced security and privacy attacks as\nwell as satisfy the revocability and unlinkability criteria of cancelable\nbiometrics.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-13.078666687011719, 29.23325538635254], "cluster": 8}, {"key": "jin2018deep", "year": "2018", "citations": "38", "title": "Deep Saliency Hashing", "abstract": "<p>In recent years, hashing methods have been proved to be effective and\nefficient for the large-scale Web media search. However, the existing general\nhashing methods have limited discriminative power for describing fine-grained\nobjects that share similar overall appearance but have subtle difference. To\nsolve this problem, we for the first time introduce the attention mechanism to\nthe learning of fine-grained hashing codes. Specifically, we propose a novel\ndeep hashing model, named deep saliency hashing (DSaH), which automatically\nmines salient regions and learns semantic-preserving hashing codes\nsimultaneously. DSaH is a two-step end-to-end model consisting of an attention\nnetwork and a hashing network. Our loss function contains three basic\ncomponents, including the semantic loss, the saliency loss, and the\nquantization loss. As the core of DSaH, the saliency loss guides the attention\nnetwork to mine discriminative regions from pairs of images. We conduct\nextensive experiments on both fine-grained and general retrieval datasets for\nperformance evaluation. Experimental results on fine-grained datasets,\nincluding Oxford Flowers-17, Stanford Dogs-120, and CUB Bird demonstrate that\nour DSaH performs the best for fine-grained retrieval task and beats the\nstrongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and\nCUB Bird. DSaH is also comparable to several state-of-the-art hashing methods\non general datasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-1.7615578174591064, 4.396027565002441], "cluster": 6}, {"key": "jin2018unsupervised", "year": "2019", "citations": "25", "title": "Unsupervised Semantic Deep Hashing", "abstract": "<p>In recent years, deep hashing methods have been proved to be efficient since\nit employs convolutional neural network to learn features and hashing codes\nsimultaneously. However, these methods are mostly supervised. In real-world\napplication, it is a time-consuming and overloaded task for annotating a large\nnumber of images. In this paper, we propose a novel unsupervised deep hashing\nmethod for large-scale image retrieval. Our method, namely unsupervised\nsemantic deep hashing (\\textbf{USDH}), uses semantic information preserved in\nthe CNN feature layer to guide the training of network. We enforce four\ncriteria on hashing codes learning based on VGG-19 model: 1) preserving\nrelevant information of feature space in hashing space; 2) minimizing\nquantization loss between binary-like codes and hashing codes; 3) improving the\nusage of each bit in hashing codes by using maximum information entropy, and 4)\ninvariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have\ndemonstrated that \\textbf{USDH} outperforms several state-of-the-art\nunsupervised hashing methods for image retrieval. We also conduct experiments\non Oxford 17 datasets for fine-grained classification to verify its efficiency\nfor other computer vision tasks.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-2.0345044136047363, -2.2219502925872803], "cluster": 1}, {"key": "jin2019deep", "year": "2020", "citations": "91", "title": "Deep Semantic Multimodal Hashing Network For Scalable Image-text And Video-text Retrievals", "abstract": "<p>Hashing has been widely applied to multimodal retrieval on large-scale\nmultimedia data due to its efficiency in computation and storage. In this\narticle, we propose a novel deep semantic multimodal hashing network (DSMHN)\nfor scalable image-text and video-text retrieval. The proposed deep hashing\nframework leverages 2-D convolutional neural networks (CNN) as the backbone\nnetwork to capture the spatial information for image-text retrieval, while the\n3-D CNN as the backbone network to capture the spatial and temporal information\nfor video-text retrieval. In the DSMHN, two sets of modality-specific hash\nfunctions are jointly learned by explicitly preserving both intermodality\nsimilarities and intramodality semantic labels. Specifically, with the\nassumption that the learned hash codes should be optimal for the classification\ntask, two stream networks are jointly trained to learn the hash functions by\nembedding the semantic labels on the resultant hash codes. Moreover, a unified\ndeep multimodal hashing framework is proposed to learn compact and high-quality\nhash codes by exploiting the feature representation learning, intermodality\nsimilarity-preserving learning, semantic label-preserving learning, and hash\nfunction learning with different types of loss functions simultaneously. The\nproposed DSMHN method is a generic and scalable deep hashing framework for both\nimage-text and video-text retrievals, which can be flexibly integrated with\ndifferent types of loss functions. We conduct extensive experiments for both\nsingle modal- and cross-modal-retrieval tasks on four widely used\nmultimodal-retrieval data sets. Experimental results on both image-text- and\nvideo-text-retrieval tasks demonstrate that the DSMHN significantly outperforms\nthe state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Efficiency", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Neural-Hashing"], "tsne_embedding": [-3.8459770679473877, 5.193300724029541], "cluster": 1}, {"key": "jin2019node2bits", "year": "2020", "citations": "28", "title": "Node2bits: Compact Time- And Attribute-aware Node Representations For User Stitching", "abstract": "<p>Identity stitching, the task of identifying and matching various online\nreferences (e.g., sessions over different devices and timespans) to the same\nuser in real-world web services, is crucial for personalization and\nrecommendations. However, traditional user stitching approaches, such as\ngrouping or blocking, require quadratic pairwise comparisons between a massive\nnumber of user activities, thus posing both computational and storage\nchallenges. Recent works, which are often application-specific, heuristically\nseek to reduce the amount of comparisons, but they suffer from low precision\nand recall. To solve the problem in an application-independent way, we take a\nheterogeneous network-based approach in which users (nodes) interact with\ncontent (e.g., sessions, websites), and may have attributes (e.g., location).\nWe propose node2bits, an efficient framework that represents multi-dimensional\nfeatures of node contexts with binary hashcodes. node2bits leverages\nfeature-based temporal walks to encapsulate short- and long-term interactions\nbetween nodes in heterogeneous web networks, and adopts SimHash to obtain\ncompact, binary representations and avoid the quadratic complexity for\nsimilarity search. Extensive experiments on large-scale real networks show that\nnode2bits outperforms traditional techniques and existing works that generate\nreal-valued embeddings by up to 5.16% in F1 score on user stitching, while\ntaking only up to 1.56% as much storage.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [42.11154556274414, -3.8336241245269775], "cluster": 9}, {"key": "jin2019ssah", "year": "2020", "citations": "28", "title": "SSAH: Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation", "abstract": "<p>Deep hashing methods have been proved to be effective and efficient for\nlarge-scale Web media search. The success of these data-driven methods largely\ndepends on collecting sufficient labeled data, which is usually a crucial\nlimitation in practical cases. The current solutions to this issue utilize\nGenerative Adversarial Network (GAN) to augment data in semi-supervised\nlearning. However, existing GAN-based methods treat image generations and\nhashing learning as two isolated processes, leading to generation\nineffectiveness. Besides, most works fail to exploit the semantic information\nin unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace\nAdversarial Hashing method, named SSAH to solve the above problems in a unified\nframework. The SSAH method consists of an adversarial network (A-Net) and a\nhashing network (H-Net). To improve the quality of generative images, first,\nthe A-Net learns hard samples with multi-scale occlusions and multi-angle\nrotated deformations which compete against the learning of accurate hashing\ncodes. Second, we design a novel self-paced hard generation policy to gradually\nincrease the hashing difficulty of generated samples. To make use of the\nsemantic information in unlabeled ones, we propose a semi-supervised consistent\nloss. The experimental results show that our method can significantly improve\nstate-of-the-art models on both the widely-used hashing datasets and\nfine-grained datasets.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Robustness", "AAAI", "Tools-&-Libraries", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-1.2502202987670898, 10.753785133361816], "cluster": 8}, {"key": "jin2020deep", "year": "2020", "citations": "59", "title": "Deep Saliency Hashing For Fine-grained Retrieval", "abstract": "<p>In recent years, hashing methods have been proved to be\neffective and efficient for the large-scale Web media search.\nHowever, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle\ndifference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained\nhashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which\nautomatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network\nand a hashing network. Our loss function contains three\nbasic components, including the semantic loss, the saliency\nloss, and the quantization loss. As the core of DSaH, the\nsaliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval\ndatasets for performance evaluation. Experimental results\non fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH\nperforms the best for fine-grained retrieval task and beats\nstrongest competitor (DTQ) by approximately 10% on both\nStanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general\ndatasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-1.7242234945297241, 4.373823165893555], "cluster": 6}, {"key": "jin2020ssah", "year": "2020", "citations": "28", "title": "SSAH: Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation", "abstract": "<p>Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Robustness", "AAAI", "Tools-&-Libraries", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-1.2505617141723633, 10.753822326660156], "cluster": 8}, {"key": "jin2021unsupervised", "year": "2021", "citations": "14", "title": "Unsupervised Discrete Hashing With Affinity Similarity", "abstract": "<p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>\n", "tags": ["Self-Supervised", "Graph-Based-Ann", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Image-Retrieval", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [10.523776054382324, 1.099948525428772], "cluster": 6}, {"key": "jin2024curator", "year": "2024", "citations": "0", "title": "Curator: Efficient Indexing For Multi-tenant Vector Databases", "abstract": "<p>Vector databases have emerged as key enablers for bridging intelligent\napplications with unstructured data, providing generic search and management\nsupport for embedding vectors extracted from the raw unstructured data. As\nmultiple data users can share the same database infrastructure, multi-tenancy\nsupport for vector databases is increasingly desirable. This hinges on an\nefficient filtered search operation, i.e., only querying the vectors accessible\nto a particular tenant. Multi-tenancy in vector databases is currently achieved\nby building either a single, shared index among all tenants, or a per-tenant\nindex. The former optimizes for memory efficiency at the expense of search\nperformance, while the latter does the opposite. Instead, this paper presents\nCurator, an in-memory vector index design tailored for multi-tenant queries\nthat simultaneously achieves the two conflicting goals, low memory overhead and\nhigh performance for queries, vector insertion, and deletion. Curator indexes\neach tenant\u2019s vectors with a tenant-specific clustering tree and encodes these\ntrees compactly as sub-trees of a shared clustering tree. Each tenant\u2019s\nclustering tree adapts dynamically to its unique vector distribution, while\nmaintaining a low per-tenant memory footprint. Our evaluation, based on two\nwidely used data sets, confirms that Curator delivers search performance on par\nwith per-tenant indexing, while maintaining memory consumption at the same\nlevel as metadata filtering on a single, shared index.</p>\n", "tags": ["Efficiency", "Memory-Efficiency", "Vector-Indexing", "Evaluation"], "tsne_embedding": [36.23591613769531, 17.177091598510742], "cluster": 2}, {"key": "jin2024edtformer", "year": "2025", "citations": "0", "title": "Edtformer: An Efficient Decoder Transformer For Visual Place Recognition", "abstract": "<p>Visual place recognition (VPR) aims to determine the general geographical location of a query image by retrieving visually similar images from a large geo-tagged database. To obtain a global representation for each place image, most approaches typically focus on the aggregation of deep features extracted from a backbone through using current prominent architectures (e.g., CNNs, MLPs, pooling layer, and transformer encoder), giving little attention to the transformer decoder. However, we argue that its strong capability to capture contextual dependencies and generate accurate features holds considerable potential for the VPR task. To this end, we propose an Efficient Decoder Transformer (EDTformer) for feature aggregation, which consists of several stacked simplified decoder blocks followed by two linear layers to directly produce robust and discriminative global representations. Specifically, we do this by formulating deep features as the keys and values, as well as a set of learnable parameters as the queries. Our EDTformer can fully utilize the contextual information within deep features, then gradually decode and aggregate the effective features into the learnable queries to output the global representations. Moreover, to provide more powerful deep features for EDTformer and further facilitate the robustness, we use the foundation model DINOv2 as the backbone and propose a Low-rank Parallel Adaptation (LoPA) method to enhance its performance in VPR, which can refine the intermediate features of the backbone progressively in a memory- and parameter-efficient way. As a result, our method not only outperforms single-stage VPR methods on multiple benchmark datasets, but also outperforms two-stage VPR methods which add a re-ranking with considerable cost. Code will be available at https://github.com/Tong-Jin01/EDTformer.</p>\n", "tags": ["Robustness", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-28.723392486572266, 5.967580318450928], "cluster": 0}, {"key": "jin2025complementary", "year": "2013", "citations": "59", "title": "Complementary Projection Hashing", "abstract": "<p>Recently, hashing techniques have been widely applied\nto solve the approximate nearest neighbors search problem\nin many vision applications. Generally, these hashing\napproaches generate 2^c buckets, where c is the length\nof the hash code. A good hashing method should satisfy\nthe following two requirements: 1) mapping the nearby\ndata points into the same bucket or nearby (measured by\nthe Hamming distance) buckets. 2) all the data points are\nevenly distributed among all the buckets. In this paper,\nwe propose a novel algorithm named Complementary Projection\nHashing (CPH) to find the optimal hashing functions\nwhich explicitly considers the above two requirements.\nSpecifically, CPH aims at sequentially finding a series of hyperplanes\n(hashing functions) which cross the sparse region\nof the data. At the same time, the data points are evenly distributed\nin the hypercubes generated by these hyperplanes.\nThe experiments comparing with the state-of-the-art hashing\nmethods demonstrate the effectiveness of the proposed\nmethod.</p>\n", "tags": ["ICCV", "Hashing-Methods"], "tsne_embedding": [21.11058807373047, 21.62827491760254], "cluster": 2}, {"key": "jin2025deep", "year": "2020", "citations": "59", "title": "Deep Saliency Hashing For Fine-grained Retrieval", "abstract": "<p>In recent years, hashing methods have been proved to be\neffective and efficient for the large-scale Web media search.\nHowever, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle\ndifference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained\nhashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which\nautomatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network\nand a hashing network. Our loss function contains three\nbasic components, including the semantic loss, the saliency\nloss, and the quantization loss. As the core of DSaH, the\nsaliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval\ndatasets for performance evaluation. Experimental results\non fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH\nperforms the best for fine-grained retrieval task and beats\nstrongest competitor (DTQ) by approximately 10% on both\nStanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general\ndatasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-1.7242624759674072, 4.373874187469482], "cluster": 6}, {"key": "jin2025ssah", "year": "2020", "citations": "28", "title": "SSAH: Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation", "abstract": "<p>Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Robustness", "AAAI", "Tools-&-Libraries", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-1.2496647834777832, 10.754225730895996], "cluster": 8}, {"key": "jin2025unsupervised", "year": "2021", "citations": "14", "title": "Unsupervised Discrete Hashing With Affinity Similarity", "abstract": "<p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>\n", "tags": ["Self-Supervised", "Graph-Based-Ann", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Image-Retrieval", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [10.523592948913574, 1.0999127626419067], "cluster": 6}, {"key": "jing2015visual", "year": "2015", "citations": "144", "title": "Visual Search At Pinterest", "abstract": "<p>We demonstrate that, with the availability of distributed computation\nplatforms such as Amazon Web Services and open-source tools, it is possible for\na small engineering team to build, launch and maintain a cost-effective,\nlarge-scale visual search system with widely available tools. We also\ndemonstrate, through a comprehensive set of live experiments at Pinterest, that\ncontent recommendation powered by visual search improve user engagement. By\nsharing our implementation details and the experiences learned from launching a\ncommercial visual search engines from scratch, we hope visual search are more\nwidely incorporated into today\u2019s commercial applications.</p>\n", "tags": ["Recommender-Systems", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "KDD"], "tsne_embedding": [-13.921869277954102, -41.1671142578125], "cluster": 3}, {"key": "johnson2017billion", "year": "2019", "citations": "2024", "title": "Billion-scale Similarity Search With Gpus", "abstract": "<p>Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.</p>\n", "tags": ["Quantization", "Similarity-Search", "Scalability", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [42.3579216003418, 28.162044525146484], "cluster": 2}, {"key": "joly2011random", "year": "2011", "citations": "151", "title": "Random Maximum Margin Hashing", "abstract": "<p>Following the success of hashing methods for multidimensional\nindexing, more and more works are interested\nin embedding visual feature space in compact hash codes.\nSuch approaches are not an alternative to using index structures\nbut a complementary way to reduce both the memory\nusage and the distance computation cost. Several data\ndependent hash functions have notably been proposed to\nclosely fit data distribution and provide better selectivity\nthan usual random projections such as LSH. However, improvements\noccur only for relatively small hash code sizes\nup to 64 or 128 bits. As discussed in the paper, this is mainly\ndue to the lack of independence between the produced hash\nfunctions. We introduce a new hash function family that\nattempts to solve this issue in any kernel space. Rather\nthan boosting the collision probability of close points, our\nmethod focus on data scattering. By training purely random\nsplits of the data, regardless the closeness of the training\nsamples, it is indeed possible to generate consistently\nmore independent hash functions. On the other side, the\nuse of large margin classifiers allows to maintain good generalization\nperformances. Experiments show that our new\nRandom Maximum Margin Hashing scheme (RMMH) outperforms\nfour state-of-the-art hashing methods, notably in\nkernel spaces.</p>\n", "tags": ["Vector-Indexing", "Hashing-Methods", "Locality-Sensitive-Hashing", "CVPR"], "tsne_embedding": [10.37290096282959, 26.495418548583984], "cluster": 4}, {"key": "joly2025random", "year": "2011", "citations": "151", "title": "Random Maximum Margin Hashing", "abstract": "<p>Following the success of hashing methods for multidimensional\nindexing, more and more works are interested\nin embedding visual feature space in compact hash codes.\nSuch approaches are not an alternative to using index structures\nbut a complementary way to reduce both the memory\nusage and the distance computation cost. Several data\ndependent hash functions have notably been proposed to\nclosely fit data distribution and provide better selectivity\nthan usual random projections such as LSH. However, improvements\noccur only for relatively small hash code sizes\nup to 64 or 128 bits. As discussed in the paper, this is mainly\ndue to the lack of independence between the produced hash\nfunctions. We introduce a new hash function family that\nattempts to solve this issue in any kernel space. Rather\nthan boosting the collision probability of close points, our\nmethod focus on data scattering. By training purely random\nsplits of the data, regardless the closeness of the training\nsamples, it is indeed possible to generate consistently\nmore independent hash functions. On the other side, the\nuse of large margin classifiers allows to maintain good generalization\nperformances. Experiments show that our new\nRandom Maximum Margin Hashing scheme (RMMH) outperforms\nfour state-of-the-art hashing methods, notably in\nkernel spaces.</p>\n", "tags": ["Vector-Indexing", "Hashing-Methods", "Locality-Sensitive-Hashing", "CVPR"], "tsne_embedding": [10.372980117797852, 26.495332717895508], "cluster": 4}, {"key": "jose2020optimized", "year": "2021", "citations": "7", "title": "Optimized Feature Space Learning For Generating Efficient Binary Codes For Image Retrieval", "abstract": "<p>In this paper we propose an approach for learning low dimensional optimized\nfeature space with minimum intra-class variance and maximum inter-class\nvariance. We address the problem of high-dimensionality of feature vectors\nextracted from neural networks by taking care of the global statistics of\nfeature space. Classical approach of Linear Discriminant Analysis (LDA) is\ngenerally used for generating an optimized low dimensional feature space for\nsingle-labeled images. Since, image retrieval involves both multi-labeled and\nsingle-labeled images, we utilize the equivalence between LDA and Canonical\nCorrelation Analysis (CCA) to generate an optimized feature space for\nsingle-labeled images and use CCA to generate an optimized feature space for\nmulti-labeled images. Our approach correlates the projections of feature\nvectors with label vectors in our CCA based network architecture. The neural\nnetwork minimize a loss function which maximizes the correlation coefficients.\nWe binarize our generated feature vectors with the popular Iterative\nQuantization (ITQ) approach and also propose an ensemble network to generate\nbinary codes of desired bit length for image retrieval. Our measurement of mean\naverage precision shows competitive results on other state-of-the-art\nsingle-labeled and multi-labeled image retrieval datasets.</p>\n", "tags": ["Quantization", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-25.69232177734375, 7.950301647186279], "cluster": 0}, {"key": "joulin2016fasttext", "year": "2016", "citations": "875", "title": "Fasttext.zip: Compressing Text Classification Models", "abstract": "<p>We consider the problem of producing compact architectures for text\nclassification, such that the full model fits in a limited amount of memory.\nAfter considering different solutions inspired by the hashing literature, we\npropose a method built upon product quantization to store word embeddings.\nWhile the original technique leads to a loss in accuracy, we adapt this method\nto circumvent quantization artefacts. Our experiments carried out on several\nbenchmarks show that our approach typically requires two orders of magnitude\nless memory than fastText while being only slightly inferior with respect to\naccuracy. As a result, it outperforms the state of the art by a good margin in\nterms of the compromise between memory usage and accuracy.</p>\n", "tags": ["Quantization", "Memory-Efficiency", "Hashing-Methods"], "tsne_embedding": [28.34703254699707, 21.830074310302734], "cluster": 2}, {"key": "joyce2023avscan2vec", "year": "2023", "citations": "2", "title": "Avscan2vec: Feature Learning On Antivirus Scan Data For Production-scale Malware Corpora", "abstract": "<p>When investigating a malicious file, searching for related files is a common\ntask that malware analysts must perform. Given that production malware corpora\nmay contain over a billion files and consume petabytes of storage, many feature\nextraction and similarity search approaches are computationally infeasible. Our\nwork explores the potential of antivirus (AV) scan data as a scalable source of\nfeatures for malware. This is possible because AV scan reports are widely\navailable through services such as VirusTotal and are ~100x smaller than the\naverage malware sample. The information within an AV scan report is abundant\nwith information and can indicate a malicious file\u2019s family, behavior, target\noperating system, and many other characteristics. We introduce AVScan2Vec, a\nlanguage model trained to comprehend the semantics of AV scan data. AVScan2Vec\ningests AV scan data for a malicious file and outputs a meaningful vector\nrepresentation. AVScan2Vec vectors are ~3 to 85x smaller than popular\nalternatives in use today, enabling faster vector comparisons and lower memory\nusage. By incorporating Dynamic Continuous Indexing, we show that\nnearest-neighbor queries on AVScan2Vec vectors can scale to even the largest\nmalware production datasets. We also demonstrate that AVScan2Vec vectors are\nsuperior to other leading malware feature vector representations across nearly\nall classification, clustering, and nearest-neighbor lookup algorithms that we\nevaluated.</p>\n", "tags": ["Similarity-Search", "Datasets"], "tsne_embedding": [27.11667823791504, -17.646100997924805], "cluster": 7}, {"key": "juan2019graph", "year": "2019", "citations": "33", "title": "Graph-rise: Graph-regularized Image Semantic Embedding", "abstract": "<p>Learning image representations to capture fine-grained semantics has been a\nchallenging and important task enabling many applications such as image search\nand clustering. In this paper, we present Graph-Regularized Image Semantic\nEmbedding (Graph-RISE), a large-scale neural graph learning framework that\nallows us to train embeddings to discriminate an unprecedented O(40M)\nultra-fine-grained semantic labels. Graph-RISE outperforms state-of-the-art\nimage embedding algorithms on several evaluation tasks, including image\nclassification and triplet ranking. We provide case studies to demonstrate\nthat, qualitatively, image retrieval based on Graph-RISE effectively captures\nsemantics and, compared to the state-of-the-art, differentiates nuances at\nlevels that are closer to human-perception.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval", "Scalability"], "tsne_embedding": [55.86225891113281, -7.540431022644043], "cluster": 9}, {"key": "jun2019combination", "year": "2019", "citations": "41", "title": "Combination Of Multiple Global Descriptors For Image Retrieval", "abstract": "<p>Recent studies in image retrieval task have shown that ensembling different\nmodels and combining multiple global descriptors lead to performance\nimprovement. However, training different models for the ensemble is not only\ndifficult but also inefficient with respect to time and memory. In this paper,\nwe propose a novel framework that exploits multiple global descriptors to get\nan ensemble effect while it can be trained in an end-to-end manner. The\nproposed framework is flexible and expandable by the global descriptor, CNN\nbackbone, loss, and dataset. Moreover, we investigate the effectiveness of\ncombining multiple global descriptors with quantitative and qualitative\nanalysis. Our extensive experiments show that the combined descriptor\noutperforms a single global descriptor, as it can utilize different types of\nfeature properties. In the benchmark evaluation, the proposed framework\nachieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop\nClothes, and Stanford Online Products on image retrieval tasks. Our model\nimplementations and pretrained models are publicly available.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-38.689815521240234, -7.187331199645996], "cluster": 0}, {"key": "jung2022few", "year": "2023", "citations": "6", "title": "Few-shot Metric Learning: Online Adaptation Of Embedding For Retrieval", "abstract": "<p>Metric learning aims to build a distance metric typically by learning an\neffective embedding function that maps similar objects into nearby points in\nits embedding space. Despite recent advances in deep metric learning, it\nremains challenging for the learned metric to generalize to unseen classes with\na substantial domain gap. To tackle the issue, we explore a new problem of\nfew-shot metric learning that aims to adapt the embedding function to the\ntarget domain with only a few annotated data. We introduce three few-shot\nmetric learning baselines and propose the Channel-Rectifier Meta-Learning\n(CRML), which effectively adapts the metric space online by adjusting channels\nof intermediate layers. Experimental analyses on miniImageNet, CUB-200-2011,\nMPII, as well as a new dataset, miniDeepFashion, demonstrate that our method\nconsistently improves the learned metric by adapting it to target classes and\nachieves a greater gain in image retrieval when the domain gap from the source\nclasses is larger.</p>\n", "tags": ["Image-Retrieval", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-15.458711624145508, -26.46571159362793], "cluster": 3}, {"key": "jush2023medical", "year": "2024", "citations": "8", "title": "Medical Image Retrieval Using Pretrained Embeddings", "abstract": "<p>A wide range of imaging techniques and data formats available for medical\nimages make accurate retrieval from image databases challenging.\n  Efficient retrieval systems are crucial in advancing medical research,\nenabling large-scale studies and innovative diagnostic tools. Thus, addressing\nthe challenges of medical image retrieval is essential for the continued\nenhancement of healthcare and research.\n  In this study, we evaluated the feasibility of employing four\nstate-of-the-art pretrained models for medical image retrieval at modality,\nbody region, and organ levels and compared the results of two similarity\nindexing approaches. Since the employed networks take 2D images, we analyzed\nthe impacts of weighting and sampling strategies to incorporate 3D information\nduring retrieval of 3D volumes. We showed that medical image retrieval is\nfeasible using pretrained networks without any additional training or\nfine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for\nvarious tasks at modality, body region, and organ level.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Similarity-Search", "Scalability"], "tsne_embedding": [-53.58236312866211, 13.375150680541992], "cluster": 0}, {"key": "juvekar2024cos", "year": "2024", "citations": "0", "title": "Cos-mix: Cosine Similarity And Distance Fusion For Improved Information Retrieval", "abstract": "<p>This study proposes a novel hybrid retrieval strategy for Retrieval-Augmented\nGeneration (RAG) that integrates cosine similarity and cosine distance measures\nto improve retrieval performance, particularly for sparse data. The traditional\ncosine similarity measure is widely used to capture the similarity between\nvectors in high-dimensional spaces. However, it has been shown that this\nmeasure can yield arbitrary results in certain scenarios. To address this\nlimitation, we incorporate cosine distance measures to provide a complementary\nperspective by quantifying the dissimilarity between vectors. Our approach is\nexperimented on proprietary data, unlike recent publications that have used\nopen-source datasets. The proposed method demonstrates enhanced retrieval\nperformance and provides a more comprehensive understanding of the semantic\nrelationships between documents or items. This hybrid strategy offers a\npromising solution for efficiently and accurately retrieving relevant\ninformation in knowledge-intensive applications, leveraging techniques such as\nBM25 (sparse) retrieval , vector (Dense) retrieval, and cosine distance based\nretrieval to facilitate efficient information retrieval.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [3.8430986404418945, -20.911577224731445], "cluster": 7}, {"key": "j\u00e4\u00e4saari2018efficient", "year": "2019", "citations": "12", "title": "Efficient Autotuning Of Hyperparameters In Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor algorithms are used to speed up nearest neighbor\nsearch in a wide array of applications. However, current indexing methods\nfeature several hyperparameters that need to be tuned to reach an acceptable\naccuracy\u2013speed trade-off. A grid search in the parameter space is often\nimpractically slow due to a time-consuming index-building procedure. Therefore,\nwe propose an algorithm for automatically tuning the hyperparameters of\nindexing methods based on randomized space-partitioning trees. In particular,\nwe present results using randomized k-d trees, random projection trees and\nrandomized PCA trees. The tuning algorithm adds minimal overhead to the\nindex-building process but is able to find the optimal hyperparameters\naccurately. We demonstrate that the algorithm is significantly faster than\nexisting approaches, and that the indexing methods used are competitive with\nthe state-of-the-art methods in query time while being faster to build.</p>\n", "tags": ["Efficiency", "Tree-Based-Ann", "Locality-Sensitive-Hashing"], "tsne_embedding": [50.3409538269043, 12.6058988571167], "cluster": 9}, {"key": "kadriu2022rapid", "year": "2022", "citations": "0", "title": "Rapid Solution For Searching Similar Audio Items", "abstract": "<p>A naive approach for finding similar audio items would be to compare each\nentry from the feature vector of the test example with each feature vector of\nthe candidates in a k-nearest neighbors fashion. There are already two problems\nwith this approach: audio signals are represented by high dimensional vectors\nand the number of candidates can be very large - think thousands. The search\nprocess would have a high complexity. Our paper will treat this problem through\nhashing methodologies more specifically the Locality Sensitive Hashing. This\nproject will be in the spirit of classification and clustering problems. The\ncomputer sound production principles will be used to determine which features\nthat describe an audio signal are the most useful. That will down-sample the\nsize of the feature vectors and speed up the process subsequently.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [3.9341025352478027, 33.50271224975586], "cluster": 4}, {"key": "kaga2019pdh", "year": "2019", "citations": "0", "title": "PDH : Probabilistic Deep Hashing Based On MAP Estimation Of Hamming Distance", "abstract": "<p>With the growth of image on the web, research on hashing which enables\nhigh-speed image retrieval has been actively studied. In recent years, various\nhashing methods based on deep neural networks have been proposed and achieved\nhigher precision than the other hashing methods. In these methods, multiple\nlosses for hash codes and the parameters of neural networks are defined. They\ngenerate hash codes that minimize the weighted sum of the losses. Therefore, an\nexpert has to tune the weights for the losses heuristically, and the\nprobabilistic optimality of the loss function cannot be explained. In order to\ngenerate explainable hash codes without weight tuning, we theoretically derive\na single loss function with no hyperparameters for the hash code from the\nprobability distribution of the images. By generating hash codes that minimize\nthis loss function, highly accurate image retrieval with probabilistic\noptimality is performed. We evaluate the performance of hashing using MNIST,\nCIFAR-10, SVHN and show that the proposed method outperforms the\nstate-of-the-art hashing methods.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Hashing-Methods", "Neural-Hashing"], "tsne_embedding": [0.6123294234275818, 18.85409927368164], "cluster": 8}, {"key": "kalantidis2015cross", "year": "2016", "citations": "418", "title": "Cross-dimensional Weighting For Aggregated Deep Convolutional Features", "abstract": "<p>We propose a simple and straightforward way of creating powerful image\nrepresentations via cross-dimensional weighting and aggregation of deep\nconvolutional neural network layer outputs. We first present a generalized\nframework that encompasses a broad family of approaches and includes\ncross-dimensional pooling and weighting steps. We then propose specific\nnon-parametric schemes for both spatial- and channel-wise weighting that boost\nthe effect of highly active spatial responses and at the same time regulate\nburstiness effects. We experiment on different public datasets for image search\nand show that our approach outperforms the current state-of-the-art for\napproaches based on pre-trained networks. We also provide an easy-to-use, open\nsource implementation that reproduces our results.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-16.69593620300293, 2.390711784362793], "cluster": 1}, {"key": "kalia2024jestr", "year": "2024", "citations": "0", "title": "JESTR: Joint Embedding Space Technique For Ranking Candidate Molecules For The Annotation Of Untargeted Metabolomics Data", "abstract": "<p>Motivation: A major challenge in metabolomics is annotation: assigning molecular structures to mass spectral fragmentation patterns. Despite recent advances in molecule-to-spectra and in spectra-to-molecular fingerprint prediction (FP), annotation rates remain low. Results: We introduce in this paper a novel paradigm (JESTR) for annotation. Unlike prior approaches that explicitly construct molecular fingerprints or spectra, JESTR leverages the insight that molecules and their corresponding spectra are views of the same data and effectively embeds their representations in a joint space. Candidate structures are ranked based on cosine similarity between the embeddings of query spectrum and each candidate. We evaluate JESTR against mol-to-spec and spec-to-FP annotation tools on three datasets. On average, for rank@[1-5], JESTR outperforms other tools by 23.6%-71.6%. We further demonstrate the strong value of regularization with candidate molecules during training, boosting rank@1 performance by 11.4% and enhancing the model\u2019s ability to discern between target and candidate molecules. When comparing JESTR\u2019s performance against that of publicly available pretrained models of SIRIUS and CFM-ID on appropriate subsets of MassSpecGym benchmark dataset, JESTR outperforms these tools by 31% and 238%, respectively. Through JESTR, we offer a novel promising avenue towards accurate annotation, therefore unlocking valuable insights into the metabolome.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-1.1043484210968018, 51.994876861572266], "cluster": 4}, {"key": "kallumadi2018line", "year": "2018", "citations": "0", "title": "A Line In The Sand: Recommendation Or Ad-hoc Retrieval?", "abstract": "<p>The popular approaches to recommendation and ad-hoc retrieval tasks are\nlargely distinct in the literature. In this work, we argue that many\nrecommendation problems can also be cast as ad-hoc retrieval tasks. To\ndemonstrate this, we build a solution for the RecSys 2018 Spotify challenge by\ncombining standard ad-hoc retrieval models and using popular retrieval tools\nsets. We draw a parallel between the playlist continuation task and the task of\nfinding good expansion terms for queries in ad-hoc retrieval, and show that\nstandard pseudo-relevance feedback can be effective as a collaborative\nfiltering approach. We also use ad-hoc retrieval for content-based\nrecommendation by treating the input playlist title as a query and associating\nall candidate tracks with meta-descriptions extracted from the background data.\nThe recommendations from these two approaches are further supplemented by a\nnearest neighbor search based on track embeddings learned by a popular neural\nmodel. Our final ranked list of recommendations is produced by a learning to\nrank model. Our proposed solution using ad-hoc retrieval models achieved a\ncompetitive performance on the music recommendation task at RecSys 2018\nchallenge\u2014finishing at rank 7 out of 112 participating teams and at rank 5\nout of 31 teams for the main and the creative tracks, respectively.</p>\n", "tags": ["Recommender-Systems", "Evaluation"], "tsne_embedding": [12.56706714630127, -42.052711486816406], "cluster": 7}, {"key": "kamal2022metric", "year": "2023", "citations": "2", "title": "Metric Learning As A Service With Covariance Embedding", "abstract": "<p>With the emergence of deep learning, metric learning has gained significant\npopularity in numerous machine learning tasks dealing with complex and\nlarge-scale datasets, such as information retrieval, object recognition and\nrecommendation systems. Metric learning aims to maximize and minimize inter-\nand intra-class similarities. However, existing models mainly rely on distance\nmeasures to obtain a separable embedding space and implicitly maximize the\nintra-class similarity while neglecting the inter-class relationship. We argue\nthat to enable metric learning as a service for high-performance deep learning\napplications, we should also wisely deal with inter-class relationships to\nobtain a more advanced and meaningful embedding space representation. In this\npaper, a novel metric learning is presented as a service methodology that\nincorporates covariance to signify the direction of the linear relationship\nbetween data points in an embedding space. Unlike conventional metric learning,\nour covariance-embedding-enhanced approach enables metric learning as a service\nto be more expressive for computing similar or dissimilar measures and can\ncapture positive, negative, or neutral relationships. Extensive experiments\nconducted using various benchmark datasets, including natural, biomedical, and\nfacial images, demonstrate that the proposed model as a service with\ncovariance-embedding optimizations can obtain higher-quality, more separable,\nand more expressive embedding representations than existing models.</p>\n", "tags": ["Distance-Metric-Learning", "Recommender-Systems", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-12.476308822631836, -14.574772834777832], "cluster": 1}, {"key": "kanda2019b", "year": "2019", "citations": "4", "title": "\\(b\\)-bit Sketch Trie: Scalable Similarity Search On Integer Sketches", "abstract": "<p>Recently, randomly mapping vectorial data to strings of discrete symbols\n(i.e., sketches) for fast and space-efficient similarity searches has become\npopular. Such random mapping is called similarity-preserving hashing and\napproximates a similarity metric by using the Hamming distance. Although many\nefficient similarity searches have been proposed, most of them are designed for\nbinary sketches. Similarity searches on integer sketches are in their infancy.\nIn this paper, we present a novel space-efficient trie named \\(b\\)-bit sketch\ntrie on integer sketches for scalable similarity searches by leveraging the\nidea behind succinct data structures (i.e., space-efficient data structures\nwhile supporting various data operations in the compressed format) and a\nfavorable property of integer sketches as fixed-length strings. Our\nexperimental results obtained using real-world datasets show that a trie-based\nindex is built from integer sketches and efficiently performs similarity\nsearches on the index by pruning useless portions of the search space, which\ngreatly improves the search time and space-efficiency of the similarity search.\nThe experimental results show that our similarity search is at most one order\nof magnitude faster than state-of-the-art similarity searches. Besides, our\nmethod needs only 10 GiB of memory on a billion-scale database, while\nstate-of-the-art similarity searches need 29 GiB of memory.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Scalability", "Similarity-Search", "Large-Scale-Search", "Datasets"], "tsne_embedding": [20.948575973510742, 26.54349136352539], "cluster": 2}, {"key": "kanda2020dynamic", "year": "2020", "citations": "2", "title": "Dynamic Similarity Search On Integer Sketches", "abstract": "<p>Similarity-preserving hashing is a core technique for fast similarity\nsearches, and it randomly maps data points in a metric space to strings of\ndiscrete symbols (i.e., sketches) in the Hamming space. While traditional\nhashing techniques produce binary sketches, recent ones produce integer\nsketches for preserving various similarity measures. However, most similarity\nsearch methods are designed for binary sketches and inefficient for integer\nsketches. Moreover, most methods are either inapplicable or inefficient for\ndynamic datasets, although modern real-world datasets are updated over time. We\npropose dynamic filter trie (DyFT), a dynamic similarity search method for both\nbinary and integer sketches. An extensive experimental analysis using large\nreal-world datasets shows that DyFT performs superiorly with respect to\nscalability, time performance, and memory efficiency. For example, on a huge\ndataset of 216 million data points, DyFT performs a similarity search 6,000\ntimes faster than a state-of-the-art method while reducing to one-thirteenth in\nmemory.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Similarity-Search", "Scalability", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [20.88353157043457, 26.390247344970703], "cluster": 2}, {"key": "kanda2020succinct", "year": "2020", "citations": "11", "title": "Succinct Trit-array Trie For Scalable Trajectory Similarity Search", "abstract": "<p>Massive datasets of spatial trajectories representing the mobility of a\ndiversity of moving objects are ubiquitous in research and industry. Similarity\nsearch of a large collection of trajectories is indispensable for turning these\ndatasets into knowledge. Locality sensitive hashing (LSH) is a powerful\ntechnique for fast similarity searches. Recent methods employ LSH and attempt\nto realize an efficient similarity search of trajectories; however, those\nmethods are inefficient in terms of search time and memory when applied to\nmassive datasets. To address this problem, we present the trajectory-indexing\nsuccinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for\ntrajectory similarity searches. tSTAT quickly performs the search on a tree\ndata structure called trie. We also present two novel techniques that enable to\ndramatically enhance the memory efficiency of tSTAT. One is a node reduction\ntechnique that substantially omits redundant trie nodes while maintaining the\ntime performance. The other is a space-efficient representation that leverages\nthe idea behind succinct data structures (i.e., a compressed data structure\nsupporting fast data operations). We experimentally test tSTAT on its ability\nto retrieve similar trajectories for a query from large collections of\ntrajectories and show that tSTAT performs superiorly in comparison to\nstate-of-the-art similarity search methods.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [39.68341827392578, 9.823711395263672], "cluster": 9}, {"key": "kang2014cross", "year": "2014", "citations": "5", "title": "Cross-modal Similarity Learning : A Low Rank Bilinear Formulation", "abstract": "<p>The cross-media retrieval problem has received much attention in recent years\ndue to the rapid increasing of multimedia data on the Internet. A new approach\nto the problem has been raised which intends to match features of different\nmodalities directly. In this research, there are two critical issues: how to\nget rid of the heterogeneity between different modalities and how to match the\ncross-modal features of different dimensions. Recently metric learning methods\nshow a good capability in learning a distance metric to explore the\nrelationship between data points. However, the traditional metric learning\nalgorithms only focus on single-modal features, which suffer difficulties in\naddressing the cross-modal features of different dimensions. In this paper, we\npropose a cross-modal similarity learning algorithm for the cross-modal feature\nmatching. The proposed method takes a bilinear formulation, and with the\nnuclear-norm penalization, it achieves low-rank representation. Accordingly,\nthe accelerated proximal gradient algorithm is successfully imported to find\nthe optimal solution with a fast convergence rate O(1/t^2). Experiments on\nthree well known image-text cross-media retrieval databases show that the\nproposed method achieves the best performance compared to the state-of-the-art\nalgorithms.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [1.1814675331115723, 0.4832901358604431], "cluster": 6}, {"key": "kang2016column", "year": "2016", "citations": "299", "title": "Column Sampling Based Discrete Supervised Hashing", "abstract": "<p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.\nHowever, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.\nCOSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "AAAI", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [12.630401611328125, 1.4439939260482788], "cluster": 6}, {"key": "kang2019candidate", "year": "2019", "citations": "53", "title": "Candidate Generation With Binary Codes For Large-scale Top-n Recommendation", "abstract": "<p>Generating the Top-N recommendations from a large corpus is computationally\nexpensive to perform at scale. Candidate generation and re-ranking based\napproaches are often adopted in industrial settings to alleviate efficiency\nproblems. However it remains to be fully studied how well such schemes\napproximate complete rankings (or how many candidates are required to achieve a\ngood approximation), or to develop systematic approaches to generate\nhigh-quality candidates efficiently. In this paper, we seek to investigate\nthese questions via proposing a candidate generation and re-ranking based\nframework (CIGAR), which first learns a preference-preserving binary embedding\nfor building a hash table to retrieve candidates, and then learns to re-rank\nthe candidates using real-valued ranking models with a candidate-oriented\nobjective. We perform a comprehensive study on several large-scale real-world\ndatasets consisting of millions of users/items and hundreds of millions of\ninteractions. Our results show that CIGAR significantly boosts the Top-N\naccuracy against state-of-the-art recommendation models, while reducing the\nquery time by orders of magnitude. We hope that this work could draw more\nattention to the candidate generation problem in recommender systems.</p>\n", "tags": ["Hashing-Methods", "CIKM", "Efficiency", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Hybrid-Ann-Methods", "Re-Ranking"], "tsne_embedding": [21.018856048583984, 15.748907089233398], "cluster": 2}, {"key": "kang2019maximum", "year": "2019", "citations": "37", "title": "Maximum-margin Hamming Hashing", "abstract": "<p>Deep hashing enables computation and memory efficient\nimage search through end-to-end learning of feature representations and binary codes. While linear scan over binary\nhash codes is more efficient than over the high-dimensional\nrepresentations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for\neach query, there is a Hamming ball centered at the query\nand the data points within the ball are returned as relevant.\nSince inside the Hamming ball implies retrievable while\noutside irretrievable, it is crucial to explicitly characterize\nthe Hamming ball. The main idea of this work is to directly\nembody the Hamming radius into the loss functions, leading\nto Maximum-Margin Hamming Hashing (MMHH), a new\nmodel specifically optimized for Hamming space retrieval.\nWe introduce a max-margin t-distribution loss, where the\nt-distribution concentrates more similar data points to be\nwithin the Hamming ball, and the margin characterizes the\nHamming radius such that less penalization is applied to\nsimilar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.\nThe model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.\nOur method yields state-of-the-art results on four datasets\nand shows superior performance on noisy data.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Robustness", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [0.3414837718009949, 25.974905014038086], "cluster": 8}, {"key": "kang2020learning", "year": "2020", "citations": "32", "title": "Learning To Embed Categorical Features Without Embedding Tables For Recommendation", "abstract": "<p>Embedding learning of categorical features (e.g. user/item IDs) is at the\ncore of various recommendation models including matrix factorization and neural\ncollaborative filtering. The standard approach creates an embedding table where\neach row represents a dedicated embedding vector for every unique feature\nvalue. However, this method fails to efficiently handle high-cardinality\nfeatures and unseen feature values (e.g. new video ID) that are prevalent in\nreal-world recommendation systems. In this paper, we propose an alternative\nembedding framework Deep Hash Embedding (DHE), replacing embedding tables by a\ndeep embedding network to compute embeddings on the fly. DHE first encodes the\nfeature value to a unique identifier vector with multiple hashing functions and\ntransformations, and then applies a DNN to convert the identifier vector to an\nembedding. The encoding module is deterministic, non-learnable, and free of\nstorage, while the embedding network is updated during the training time to\nlearn embedding generation. Empirical results show that DHE achieves comparable\nAUC against the standard one-hot full embedding, with smaller model sizes. Our\nwork sheds light on the design of DNN-based alternative embedding schemes for\ncategorical features without using embedding table lookup.</p>\n", "tags": ["Tools-&-Libraries", "Recommender-Systems", "Hashing-Methods", "Neural-Hashing"], "tsne_embedding": [-10.232608795166016, -27.199975967407227], "cluster": 3}, {"key": "kang2025column", "year": "2016", "citations": "299", "title": "Column Sampling Based Discrete Supervised Hashing", "abstract": "<p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.\nHowever, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.\nCOSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "AAAI", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [12.630433082580566, 1.4441349506378174], "cluster": 6}, {"key": "kang2025maximum", "year": "2019", "citations": "37", "title": "Maximum-margin Hamming Hashing", "abstract": "<p>Deep hashing enables computation and memory efficient\nimage search through end-to-end learning of feature representations and binary codes. While linear scan over binary\nhash codes is more efficient than over the high-dimensional\nrepresentations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for\neach query, there is a Hamming ball centered at the query\nand the data points within the ball are returned as relevant.\nSince inside the Hamming ball implies retrievable while\noutside irretrievable, it is crucial to explicitly characterize\nthe Hamming ball. The main idea of this work is to directly\nembody the Hamming radius into the loss functions, leading\nto Maximum-Margin Hamming Hashing (MMHH), a new\nmodel specifically optimized for Hamming space retrieval.\nWe introduce a max-margin t-distribution loss, where the\nt-distribution concentrates more similar data points to be\nwithin the Hamming ball, and the margin characterizes the\nHamming radius such that less penalization is applied to\nsimilar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.\nThe model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.\nOur method yields state-of-the-art results on four datasets\nand shows superior performance on noisy data.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Robustness", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [0.3414837718009949, 25.974905014038086], "cluster": 8}, {"key": "kannan2024placeformer", "year": "2024", "citations": "3", "title": "Placeformer: Transformer-based Visual Place Recognition Using Multi-scale Patch Selection And Fusion", "abstract": "<p>Visual place recognition is a challenging task in the field of computer\nvision, and autonomous robotics and vehicles, which aims to identify a location\nor a place from visual inputs. Contemporary methods in visual place recognition\nemploy convolutional neural networks and utilize every region within the image\nfor the place recognition task. However, the presence of dynamic and\ndistracting elements in the image may impact the effectiveness of the place\nrecognition process. Therefore, it is meaningful to focus on task-relevant\nregions of the image for improved recognition. In this paper, we present\nPlaceFormer, a novel transformer-based approach for visual place recognition.\nPlaceFormer employs patch tokens from the transformer to create global image\ndescriptors, which are then used for image retrieval. To re-rank the retrieved\nimages, PlaceFormer merges the patch tokens from the transformer to form\nmulti-scale patches. Utilizing the transformer\u2019s self-attention mechanism, it\nselects patches that correspond to task-relevant areas in an image. These\nselected patches undergo geometric verification, generating similarity scores\nacross different patch sizes. Subsequently, spatial scores from each patch size\nare fused to produce a final similarity score. This score is then used to\nre-rank the images initially retrieved using global image descriptors.\nExtensive experiments on benchmark datasets demonstrate that PlaceFormer\noutperforms several state-of-the-art methods in terms of accuracy and\ncomputational efficiency, requiring less time and memory.</p>\n", "tags": ["Efficiency", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-35.15834045410156, -6.861175060272217], "cluster": 0}, {"key": "kaplan2021locality", "year": "2021", "citations": "1", "title": "Locality Sensitive Hashing For Efficient Similar Polygon Retrieval", "abstract": "<p>Locality Sensitive Hashing (LSH) is an effective method of indexing a set of\nitems to support efficient nearest neighbors queries in high-dimensional\nspaces. The basic idea of LSH is that similar items should produce hash\ncollisions with higher probability than dissimilar items.\n  We study LSH for (not necessarily convex) polygons, and use it to give\nefficient data structures for similar shape retrieval. Arkin et al. represent\npolygons by their \u201cturning function\u201d - a function which follows the angle\nbetween the polygon\u2019s tangent and the \\( x \\)-axis while traversing the perimeter\nof the polygon. They define the distance between polygons to be variations of\nthe \\( L_p \\) (for \\(p=1,2\\)) distance between their turning functions. This metric\nis invariant under translation, rotation and scaling (and the selection of the\ninitial point on the perimeter) and therefore models well the intuitive notion\nof shape resemblance.\n  We develop and analyze LSH near neighbor data structures for several\nvariations of the \\( L_p \\) distance for functions (for \\(p=1,2\\)). By applying our\nschemes to the turning functions of a collection of polygons we obtain\nefficient near neighbor LSH-based structures for polygons. To tune our\nstructures to turning functions of polygons, we prove some new properties of\nthese turning functions that may be of independent interest.\n  As part of our analysis, we address the following problem which is of\nindependent interest. Find the vertical translation of a function \\( f \\) that is\nclosest in \\( L_1 \\) distance to a function \\( g \\). We prove tight bounds on the\napproximation guarantee obtained by the translation which is equal to the\ndifference between the averages of \\( g \\) and \\( f \\).</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [20.900562286376953, 52.542091369628906], "cluster": 4}, {"key": "kapralov2024adversarial", "year": "2025", "citations": "0", "title": "On The Adversarial Robustness Of Locality-sensitive Hashing In Hamming Space", "abstract": "<p>Locality-sensitive hashing~[Indyk,Motwani\u201998] is a classical data structure\nfor approximate nearest neighbor search. It allows, after a close to linear\ntime preprocessing of the input dataset, to find an approximately nearest\nneighbor of any fixed query in sublinear time in the dataset size. The\nresulting data structure is randomized and succeeds with high probability for\nevery fixed query.\n  In many modern applications of nearest neighbor search the queries are chosen\nadaptively. In this paper, we study the robustness of the locality-sensitive\nhashing to adaptive queries in Hamming space. We present a simple adversary\nthat can, under mild assumptions on the initial point set, provably find a\nquery to the approximate near neighbor search data structure that the data\nstructure fails on. Crucially, our adaptive algorithm finds the hard query\nexponentially faster than random sampling.</p>\n", "tags": ["Robustness", "Hashing-Methods", "Datasets"], "tsne_embedding": [23.884531021118164, 39.96657943725586], "cluster": 4}, {"key": "karaman2019unsupervised", "year": "2019", "citations": "14", "title": "Unsupervised Rank-preserving Hashing For Large-scale Image Retrieval", "abstract": "<p>We propose an unsupervised hashing method which aims to produce binary codes\nthat preserve the ranking induced by a real-valued representation. Such compact\nhash codes enable the complete elimination of real-valued feature storage and\nallow for significant reduction of the computation complexity and storage cost\nof large-scale image retrieval applications. Specifically, we learn a neural\nnetwork-based model, which transforms the input representation into a binary\nrepresentation. We formalize the training objective of the network in an\nintuitive and effective way, considering each training sample as a query and\naiming to obtain the same retrieval results using the produced hash codes as\nthose obtained with the original features. This training formulation directly\noptimizes the hashing model for the target usage of the hash codes it produces.\nWe further explore the addition of a decoder trained to obtain an approximated\nreconstruction of the original features. At test time, we retrieved the most\npromising database samples with an efficient graph-based search procedure using\nonly our hash codes and perform re-ranking using the reconstructed features,\nthus without needing to access the original features at all. Experiments\nconducted on multiple publicly available large-scale datasets show that our\nmethod consistently outperforms all compared state-of-the-art unsupervised\nhashing methods and that the reconstruction procedure can effectively boost the\nsearch accuracy with a minimal constant additional cost.</p>\n", "tags": ["Datasets", "Graph-Based-Ann", "Compact-Codes", "Neural-Hashing", "Image-Retrieval", "Memory-Efficiency", "Hybrid-Ann-Methods", "Hashing-Methods", "Supervised", "Unsupervised", "Multimodal-Retrieval", "Re-Ranking", "Scalability"], "tsne_embedding": [20.728946685791016, 6.525888442993164], "cluster": 6}, {"key": "karoui2023stop", "year": "2023", "citations": "1", "title": "Stop Pre-training: Adapt Visual-language Models To Unseen Languages", "abstract": "<p>Vision-Language Pre-training (VLP) has advanced the performance of many\nvision-language tasks, such as image-text retrieval, visual entailment, and\nvisual reasoning. The pre-training mostly utilizes lexical databases and image\nqueries in English. Previous work has demonstrated that the pre-training in\nEnglish does not transfer well to other languages in a zero-shot setting.\nHowever, multilingual pre-trained language models (MPLM) have excelled at a\nvariety of single-modal language tasks. In this paper, we propose a simple yet\nefficient approach to adapt VLP to unseen languages using MPLM. We utilize a\ncross-lingual contextualized token embeddings alignment approach to train text\nencoders for non-English languages. Our approach does not require image input\nand primarily uses machine translation, eliminating the need for target\nlanguage data. Our evaluation across three distinct tasks (image-text\nretrieval, visual entailment, and natural language visual reasoning)\ndemonstrates that this approach outperforms the state-of-the-art multilingual\nvision-language models without requiring large parallel corpora. Our code is\navailable at https://github.com/Yasminekaroui/CliCoTea.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Text-Retrieval"], "tsne_embedding": [-4.842216968536377, -39.881465911865234], "cluster": 3}, {"key": "karppa2021deann", "year": "2021", "citations": "2", "title": "DEANN: Speeding Up Kernel-density Estimation Using Approximate Nearest Neighbor Search", "abstract": "<p>Kernel Density Estimation (KDE) is a nonparametric method for estimating the\nshape of a density function, given a set of samples from the distribution.\nRecently, locality-sensitive hashing, originally proposed as a tool for nearest\nneighbor search, has been shown to enable fast KDE data structures. However,\nthese approaches do not take advantage of the many other advances that have\nbeen made in algorithms for nearest neighbor algorithms. We present an\nalgorithm called Density Estimation from Approximate Nearest Neighbors (DEANN)\nwhere we apply Approximate Nearest Neighbor (ANN) algorithms as a black box\nsubroutine to compute an unbiased KDE. The idea is to find points that have a\nlarge contribution to the KDE using ANN, compute their contribution exactly,\nand approximate the remainder with Random Sampling (RS). We present a\ntheoretical argument that supports the idea that an ANN subroutine can speed up\nthe evaluation. Furthermore, we provide a C++ implementation with a Python\ninterface that can make use of an arbitrary ANN implementation as a subroutine\nfor kernel density estimation. We show empirically that our implementation\noutperforms state of the art implementations in all high dimensional datasets\nwe considered, and matches the performance of RS in cases where the ANN yield\nno gains in performance.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [23.204059600830078, 38.4571647644043], "cluster": 4}, {"key": "karpusha2020calibrated", "year": "2020", "citations": "4", "title": "Calibrated Neighborhood Aware Confidence Measure For Deep Metric Learning", "abstract": "<p>Deep metric learning has gained promising improvement in recent years\nfollowing the success of deep learning. It has been successfully applied to\nproblems in few-shot learning, image retrieval, and open-set classifications.\nHowever, measuring the confidence of a deep metric learning model and\nidentifying unreliable predictions is still an open challenge. This paper\nfocuses on defining a calibrated and interpretable confidence metric that\nclosely reflects its classification accuracy. While performing similarity\ncomparison directly in the latent space using the learned distance metric, our\napproach approximates the distribution of data points for each class using a\nGaussian kernel smoothing function. The post-processing calibration algorithm\nwith proposed confidence metric on the held-out validation dataset improves\ngeneralization and robustness of state-of-the-art deep metric learning models\nwhile provides an interpretable estimation of the confidence. Extensive tests\non four popular benchmark datasets (Caltech-UCSD Birds, Stanford Online\nProduct, Stanford Car-196, and In-shop Clothes Retrieval) show consistent\nimprovements even at the presence of distribution shifts in test data related\nto additional noise or adversarial examples.</p>\n", "tags": ["Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Robustness", "Datasets", "Re-Ranking", "Evaluation"], "tsne_embedding": [-14.366398811340332, -15.060159683227539], "cluster": 1}, {"key": "karunanayake2020multi", "year": "2020", "citations": "9", "title": "A Multi-modal Neural Embeddings Approach For Detecting Mobile Counterfeit Apps: A Case Study On Google Play Store", "abstract": "<p>Counterfeit apps impersonate existing popular apps in attempts to misguide\nusers to install them for various reasons such as collecting personal\ninformation or spreading malware. Many counterfeits can be identified once\ninstalled, however even a tech-savvy user may struggle to detect them before\ninstallation. To this end, this paper proposes to leverage the recent advances\nin deep learning methods to create image and text embeddings so that\ncounterfeit apps can be efficiently identified when they are submitted for\npublication. We show that a novel approach of combining content embeddings and\nstyle embeddings outperforms the baseline methods for image similarity such as\nSIFT, SURF, and various image hashing methods. We first evaluate the\nperformance of the proposed method on two well-known datasets for evaluating\nimage similarity methods and show that content, style, and combined embeddings\nincrease precision@k and recall@k by 10%-15% and 12%-25%, respectively when\nretrieving five nearest neighbours. Second, specifically for the app\ncounterfeit detection problem, combined content and style embeddings achieve\n12% and 14% increase in precision@k and recall@k, respectively compared to the\nbaseline methods. Third, we present an analysis of approximately 1.2 million\napps from Google Play Store and identify a set of potential counterfeits for\ntop-10,000 popular apps. Under a conservative assumption, we were able to find\n2,040 potential counterfeits that contain malware in a set of 49,608 apps that\nshowed high similarity to one of the top-10,000 popular apps in Google Play\nStore. We also find 1,565 potential counterfeits asking for at least five\nadditional dangerous permissions than the original app and 1,407 potential\ncounterfeits having at least five extra third party advertisement libraries.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Image-Retrieval", "Datasets"], "tsne_embedding": [29.92278480529785, -24.70692253112793], "cluster": 7}, {"key": "kato2009solving", "year": "2010", "citations": "37", "title": "Solving \\(k\\)-nearest Neighbor Problem On Multiple Graphics Processors", "abstract": "<p>The recommendation system is a software system to predict customers\u2019 unknown\npreferences from known preferences. In the recommendation system, customers\u2019\npreferences are encoded into vectors, and finding the nearest vectors to each\nvector is an essential part. This vector-searching part of the problem is\ncalled a \\(k\\)-nearest neighbor problem. We give an effective algorithm to solve\nthis problem on multiple graphics processor units (GPUs).\n  Our algorithm consists of two parts: an \\(N\\)-body problem and a partial sort.\nFor a algorithm of the \\(N\\)-body problem, we applied the idea of a known\nalgorithm for the \\(N\\)-body problem in physics, although another trick is need\nto overcome the problem of small sized shared memory. For the partial sort, we\ngive a novel GPU algorithm which is effective for small \\(k\\). In our partial\nsort algorithm, a heap is accessed in parallel by threads with a low cost of\nsynchronization. Both of these two parts of our algorithm utilize maximal power\nof coalesced memory access, so that a full bandwidth is achieved.\n  By an experiment, we show that when the size of the problem is large, an\nimplementation of the algorithm on two GPUs runs more than 330 times faster\nthan a single core implementation on a latest CPU. We also show that our\nalgorithm scales well with respect to the number of GPUs.</p>\n", "tags": ["Recommender-Systems"], "tsne_embedding": [36.308414459228516, 24.64999771118164], "cluster": 2}, {"key": "kehl2016hashmod", "year": "2015", "citations": "46", "title": "Hashmod: A Hashing Method For Scalable 3D Object Detection", "abstract": "<p>We present a scalable method for detecting objects and estimating their 3D\nposes in RGB-D data. To this end, we rely on an efficient representation of\nobject views and employ hashing techniques to match these views against the\ninput frame in a scalable way. While a similar approach already exists for 2D\ndetection, we show how to extend it to estimate the 3D pose of the detected\nobjects. In particular, we explore different hashing strategies and identify\nthe one which is more suitable to our problem. We show empirically that the\ncomplexity of our method is sublinear with the number of objects and we enable\ndetection and pose estimation of many 3D objects with high accuracy while\noutperforming the state-of-the-art in terms of runtime.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [-39.88715362548828, -11.276366233825684], "cluster": 5}, {"key": "keisler2020visual", "year": "2019", "citations": "21", "title": "Visual Search Over Billions Of Aerial And Satellite Images", "abstract": "<p>We present a system for performing visual search over billions of aerial and\nsatellite images. The purpose of visual search is to find images that are\nvisually similar to a query image. We define visual similarity using 512\nabstract visual features generated by a convolutional neural network that has\nbeen trained on aerial and satellite imagery. The features are converted to\nbinary values to reduce data and compute requirements. We employ a hash-based\nsearch using Bigtable, a scalable database service from Google Cloud. Searching\nthe continental United States at 1-meter pixel resolution, corresponding to\napproximately 2 billion images, takes approximately 0.1 seconds. This system\nenables real-time visual search over the surface of the earth, and an\ninteractive demo is available at https://search.descarteslabs.com.</p>\n", "tags": ["Efficiency", "Image-Retrieval"], "tsne_embedding": [-35.290348052978516, 23.158201217651367], "cluster": 0}, {"key": "kennedy2016fast", "year": "2016", "citations": "9", "title": "Fast Cross-polytope Locality-sensitive Hashing", "abstract": "<p>We provide a variant of cross-polytope locality sensitive hashing with\nrespect to angular distance which is provably optimal in asymptotic sensitivity\nand enjoys \\(\\mathcal{O}(d \\ln d )\\) hash computation time. Building on a recent\nresult (by Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015), we show that\noptimal asymptotic sensitivity for cross-polytope LSH is retained even when the\ndense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform\nfollowed by discrete pseudo-rotation, reducing the hash computation time from\n\\(\\mathcal{O}(d^2)\\) to \\(\\mathcal{O}(d \\ln d )\\). Moreover, our scheme achieves\nthe optimal rate of convergence for sensitivity. By incorporating a\nlow-randomness Johnson-Lindenstrauss transform, our scheme can be modified to\nrequire only \\(\\mathcal{O}(\\ln^9(d))\\) random bits</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [15.929339408874512, 42.991703033447266], "cluster": 4}, {"key": "khadka2023content", "year": "2023", "citations": "0", "title": "Content-based Recommendation Engine For Video Streaming Platform", "abstract": "<p>Recommendation engine suggest content, product or services to the user by\nusing machine learning algorithm. This paper proposed a content-based\nrecommendation engine for providing video suggestion to the user based on their\nprevious interests and choices. We will use TF-IDF text vectorization method to\ndetermine the relevance of words in a document. Then we will find out the\nsimilarity between each content by calculating cosine similarity between them.\nFinally, engine will recommend videos to the users based on the obtained\nsimilarity score value. In addition, we will measure the engine\u2019s performance\nby computing precision, recall, and F1 core of the proposed system.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-43.865535736083984, -25.856792449951172], "cluster": 5}, {"key": "khaliq2022multires", "year": "2022", "citations": "37", "title": "Multires-netvlad: Augmenting Place Recognition Training With Low-resolution Imagery", "abstract": "<p>Visual Place Recognition (VPR) is a crucial component of 6-DoF localization,\nvisual SLAM and structure-from-motion pipelines, tasked to generate an initial\nlist of place match hypotheses by matching global place descriptors. However,\ncommonly-used CNN-based methods either process multiple image resolutions after\ntraining or use a single resolution and limit multi-scale feature extraction to\nthe last convolutional layer during training. In this paper, we augment NetVLAD\nrepresentation learning with low-resolution image pyramid encoding which leads\nto richer place representations. The resultant multi-resolution feature pyramid\ncan be conveniently aggregated through VLAD into a single compact\nrepresentation, avoiding the need for concatenation or summation of multiple\npatches in recent multi-scale approaches. Furthermore, we show that the\nunderlying learnt feature tensor can be combined with existing multi-scale\napproaches to improve their baseline performance. Evaluation on 15\nviewpoint-varying and viewpoint-consistent benchmarking datasets confirm that\nthe proposed MultiRes-NetVLAD leads to state-of-the-art Recall@N performance\nfor global descriptor based retrieval, compared against 11 existing techniques.\nSource code is publicly available at\nhttps://github.com/Ahmedest61/MultiRes-NetVLAD.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-41.73429489135742, 6.411300182342529], "cluster": 0}, {"key": "khandelwal2020nearest", "year": "2020", "citations": "130", "title": "Nearest Neighbor Machine Translation", "abstract": "<p>We introduce \\(k\\)-nearest-neighbor machine translation (\\(k\\)NN-MT), which\npredicts tokens with a nearest neighbor classifier over a large datastore of\ncached examples, using representations from a neural translation model for\nsimilarity search. This approach requires no additional training and scales to\ngive the decoder direct access to billions of examples at test time, resulting\nin a highly expressive model that consistently improves performance across many\nsettings. Simply adding nearest neighbor search improves a state-of-the-art\nGerman-English translation model by 1.5 BLEU. \\(k\\)NN-MT allows a single model to\nbe adapted to diverse domains by using a domain-specific datastore, improving\nresults by an average of 9.2 BLEU over zero-shot transfer, and achieving new\nstate-of-the-art results \u2013 without training on these domains. A massively\nmultilingual model can also be specialized for particular language pairs, with\nimprovements of 3 BLEU for translating from English into German and Chinese.\nQualitatively, \\(k\\)NN-MT is easily interpretable; it combines source and target\ncontext to retrieve highly relevant examples.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Similarity-Search"], "tsne_embedding": [9.566512107849121, -14.418450355529785], "cluster": 7}, {"key": "khasanova2016multi", "year": "2016", "citations": "7", "title": "Multi-modal Image Retrieval With Random Walk On Multi-layer Graphs", "abstract": "<p>The analysis of large collections of image data is still a challenging\nproblem due to the difficulty of capturing the true concepts in visual data.\nThe similarity between images could be computed using different and possibly\nmultimodal features such as color or edge information or even text labels. This\nmotivates the design of image analysis solutions that are able to effectively\nintegrate the multi-view information provided by different feature sets. We\ntherefore propose a new image retrieval solution that is able to sort images\nthrough a random walk on a multi-layer graph, where each layer corresponds to a\ndifferent type of information about the image data. We study in depth the\ndesign of the image graph and propose in particular an effective method to\nselect the edge weights for the multi-layer graph, such that the image ranking\nscores are optimised. We then provide extensive experiments in different\nreal-world photo collections, which confirm the high performance of our new\nimage retrieval algorithm that generally surpasses state-of-the-art solutions\ndue to a more meaningful image similarity computation.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [54.789188385009766, -9.228903770446777], "cluster": 9}, {"key": "khatry2023augmented", "year": "2023", "citations": "0", "title": "Augmented Embeddings For Custom Retrievals", "abstract": "<p>Information retrieval involves selecting artifacts from a corpus that are\nmost relevant to a given search query. The flavor of retrieval typically used\nin classical applications can be termed as homogeneous and relaxed, where\nqueries and corpus elements are both natural language (NL) utterances\n(homogeneous) and the goal is to pick most relevant elements from the corpus in\nthe Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed).\nRecently, retrieval is being used extensively in preparing prompts for large\nlanguage models (LLMs) to enable LLMs to perform targeted tasks. These new\napplications of retrieval are often heterogeneous and strict \u2013 the queries and\nthe corpus contain different kinds of entities, such as NL and code, and there\nis a need for improving retrieval at Top-K for small values of K, such as K=1\nor 3 or 5. Current dense retrieval techniques based on pretrained embeddings\nprovide a general-purpose and powerful approach for retrieval, but they are\noblivious to task-specific notions of similarity of heterogeneous artifacts. We\nintroduce Adapted Dense Retrieval, a mechanism to transform embeddings to\nenable improved task-specific, heterogeneous and strict retrieval. Adapted\nDense Retrieval works by learning a low-rank residual adaptation of the\npretrained black-box embedding. We empirically validate our approach by showing\nimprovements over the state-of-the-art general-purpose embeddings-based\nbaseline.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-1.153067946434021, -22.445865631103516], "cluster": 3}, {"key": "khojasteh2020deep", "year": "2020", "citations": "3", "title": "Deep Multimodal Image-text Embeddings For Automatic Cross-media Retrieval", "abstract": "<p>This paper considers the task of matching images and sentences by learning a\nvisual-textual embedding space for cross-modal retrieval. Finding such a space\nis a challenging task since the features and representations of text and image\nare not comparable. In this work, we introduce an end-to-end deep multimodal\nconvolutional-recurrent network for learning both vision and language\nrepresentations simultaneously to infer image-text similarity. The model learns\nwhich pairs are a match (positive) and which ones are a mismatch (negative)\nusing a hinge-based triplet ranking. To learn about the joint representations,\nwe leverage our newly extracted collection of tweets from Twitter. The main\ncharacteristic of our dataset is that the images and tweets are not\nstandardized the same as the benchmarks. Furthermore, there can be a higher\nsemantic correlation between the pictures and tweets contrary to benchmarks in\nwhich the descriptions are well-organized. Experimental results on MS-COCO\nbenchmark dataset show that our model outperforms certain methods presented\npreviously and has competitive performance compared to the state-of-the-art.\nThe code and dataset have been made available publicly.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [-8.45404052734375, -33.57618713378906], "cluster": 3}, {"key": "khoram2019interleaved", "year": "2019", "citations": "0", "title": "Interleaved Composite Quantization For High-dimensional Similarity Search", "abstract": "<p>Similarity search retrieves the nearest neighbors of a query vector from a\ndataset of high-dimensional vectors. As the size of the dataset grows, the cost\nof performing the distance computations needed to implement a query can become\nprohibitive. A method often used to reduce this computational cost is\nquantization of the vector space and location-based encoding of the dataset\nvectors. These encodings can be used during query processing to find\napproximate nearest neighbors of the query point quickly. Search speed can be\nimproved by using shorter codes, but shorter codes have higher quantization\nerror, leading to degraded precision. In this work, we propose the Interleaved\nComposite Quantization (ICQ) which achieves fast similarity search without\nusing shorter codes. In ICQ, a small subset of the code is used to approximate\nthe distances, with complete codes being used only when necessary. Our method\neffectively reduces both code length and quantization error. Furthermore, ICQ\nis compatible with several recently proposed techniques for reducing\nquantization error and can be used in conjunction with these other techniques\nto improve results. We confirm these claims and show strong empirical\nperformance of ICQ using several synthetic and real-word datasets.</p>\n", "tags": ["Quantization", "Evaluation", "Similarity-Search", "Datasets"], "tsne_embedding": [23.94801902770996, 32.590545654296875], "cluster": 4}, {"key": "khudabukhsh2020discovering", "year": "2020", "citations": "3", "title": "Discovering Bilingual Lexicons In Polyglot Word Embeddings", "abstract": "<p>Bilingual lexicons and phrase tables are critical resources for modern\nMachine Translation systems. Although recent results show that without any seed\nlexicon or parallel data, highly accurate bilingual lexicons can be learned\nusing unsupervised methods, such methods rely on the existence of large, clean\nmonolingual corpora. In this work, we utilize a single Skip-gram model trained\non a multilingual corpus yielding polyglot word embeddings, and present a novel\nfinding that a surprisingly simple constrained nearest-neighbor sampling\ntechnique in this embedding space can retrieve bilingual lexicons, even in\nharsh social media data sets predominantly written in English and Romanized\nHindi and often exhibiting code switching. Our method does not require\nmonolingual corpora, seed lexicons, or any other such resources. Additionally,\nacross three European language pairs, we observe that polyglot word embeddings\nindeed learn a rich semantic representation of words and substantial bilingual\nlexicons can be retrieved using our constrained nearest neighbor sampling. We\ninvestigate potential reasons and downstream applications in settings spanning\nboth clean texts and noisy social media data sets, and in both resource-rich\nand under-resourced language pairs.</p>\n", "tags": ["Unsupervised"], "tsne_embedding": [-5.437740802764893, -40.284481048583984], "cluster": 3}, {"key": "khurshid2020cross", "year": "2019", "citations": "5", "title": "Cross-view Image Retrieval -- Ground To Aerial Image Retrieval Through Deep Learning", "abstract": "<p>Cross-modal retrieval aims to measure the content similarity between\ndifferent types of data. The idea has been previously applied to visual, text,\nand speech data. In this paper, we present a novel cross-modal retrieval method\nspecifically for multi-view images, called Cross-view Image Retrieval CVIR. Our\napproach aims to find a feature space as well as an embedding space in which\nsamples from street-view images are compared directly to satellite-view images\n(and vice-versa). For this comparison, a novel deep metric learning based\nsolution \u201cDeepCVIR\u201d has been proposed. Previous cross-view image datasets are\ndeficient in that they (1) lack class information; (2) were originally\ncollected for cross-view image geolocalization task with coupled images; (3) do\nnot include any images from off-street locations. To train, compare, and\nevaluate the performance of cross-view image retrieval, we present a new 6\nclass cross-view image dataset termed as CrossViewRet which comprises of images\nincluding freeway, mountain, palace, river, ship, and stadium with 700\nhigh-resolution dual-view images for each class. Results show that the proposed\nDeepCVIR outperforms conventional matching approaches on the CVIR task for the\ngiven dataset and would also serve as the baseline for future research.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-17.808185577392578, -1.9114234447479248], "cluster": 1}, {"key": "kilias2018idel", "year": "2018", "citations": "8", "title": "IDEL: In-database Entity Linking With Neural Embeddings", "abstract": "<p>We present a novel architecture, In-Database Entity Linking (IDEL), in which\nwe integrate the analytics-optimized RDBMS MonetDB with neural text mining\nabilities. Our system design abstracts core tasks of most neural entity linking\nsystems for MonetDB. To the best of our knowledge, this is the first defacto\nimplemented system integrating entity-linking in a database. We leverage the\nability of MonetDB to support in-database-analytics with user defined functions\n(UDFs) implemented in Python. These functions call machine learning libraries\nfor neural text mining, such as TensorFlow. The system achieves zero cost for\ndata shipping and transformation by utilizing MonetDB\u2019s ability to embed Python\nprocesses in the database kernel and exchange data in NumPy arrays. IDEL\nrepresents text and relational data in a joint vector space with neural\nembeddings and can compensate errors with ambiguous entity representations. For\ndetecting matching entities, we propose a novel similarity function based on\njoint neural embeddings which are learned via minimizing pairwise contrastive\nranking loss. This function utilizes a high dimensional index structures for\nfast retrieval of matching entities. Our first implementation and experiments\nusing the WebNLG corpus show the effectiveness and the potentials of IDEL.</p>\n", "tags": ["Efficiency", "Vector-Indexing"], "tsne_embedding": [15.291741371154785, 14.873783111572266], "cluster": 6}, {"key": "kim2016visual", "year": "2016", "citations": "4", "title": "Visual Fashion-product Search At SK Planet", "abstract": "<p>We build a large-scale visual search system which finds similar product\nimages given a fashion item. Defining similarity among arbitrary\nfashion-products is still remains a challenging problem, even there is no exact\nground-truth. To resolve this problem, we define more than 90 fashion-related\nattributes, and combination of these attributes can represent thousands of\nunique fashion-styles. The fashion-attributes are one of the ingredients to\ndefine semantic similarity among fashion-product images. To build our system at\nscale, these fashion-attributes are again used to build an inverted indexing\nscheme. In addition to these fashion-attributes for semantic similarity, we\nextract colour and appearance features in a region-of-interest (ROI) of a\nfashion item for visual similarity. By sharing our approach, we expect active\ndiscussion on that how to apply current computer vision research into the\ne-commerce industry.</p>\n", "tags": ["Image-Retrieval", "Scalability"], "tsne_embedding": [-19.332735061645508, -50.615604400634766], "cluster": 3}, {"key": "kim2018attention", "year": "2018", "citations": "239", "title": "Attention-based Ensemble For Deep Metric Learning", "abstract": "<p>Deep metric learning aims to learn an embedding function, modeled as deep\nneural network. This embedding function usually puts semantically similar\nimages close while dissimilar images far from each other in the learned\nembedding space. Recently, ensemble has been applied to deep metric learning to\nyield state-of-the-art results. As one important aspect of ensemble, the\nlearners should be diverse in their feature embeddings. To this end, we propose\nan attention-based ensemble, which uses multiple attention masks, so that each\nlearner can attend to different parts of the object. We also propose a\ndivergence loss, which encourages diversity among the learners. The proposed\nmethod is applied to the standard benchmarks of deep metric learning and\nexperimental results show that it outperforms the state-of-the-art methods by a\nsignificant margin on image retrieval tasks.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-13.95964241027832, -19.553136825561523], "cluster": 1}, {"key": "kim2019deep", "year": "2019", "citations": "88", "title": "Deep Metric Learning Beyond Binary Supervision", "abstract": "<p>Metric Learning for visual similarity has mostly adopted binary supervision\nindicating whether a pair of images are of the same class or not. Such a binary\nindicator covers only a limited subset of image relations, and is not\nsufficient to represent semantic similarity between images described by\ncontinuous and/or structured labels such as object poses, image captions, and\nscene graphs. Motivated by this, we present a novel method for deep metric\nlearning using continuous labels. First, we propose a new triplet loss that\nallows distance ratios in the label space to be preserved in the learned metric\nspace. The proposed loss thus enables our model to learn the degree of\nsimilarity rather than just the order. Furthermore, we design a triplet mining\nstrategy adapted to metric learning with continuous labels. We address three\ndifferent image retrieval tasks with continuous labels in terms of human poses,\nroom layouts and image captions, and demonstrate the superior performance of\nour approach compared to previous methods.</p>\n", "tags": ["CVPR", "Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-16.577810287475586, -13.546184539794922], "cluster": 1}, {"key": "kim2019nearest", "year": "2019", "citations": "0", "title": "Nearest Neighbor Search-based Bitwise Source Separation Using Discriminant Winner-take-all Hashing", "abstract": "<p>We propose an iteration-free source separation algorithm based on\nWinner-Take-All (WTA) hash codes, which is a faster, yet accurate alternative\nto a complex machine learning model for single-channel source separation in a\nresource-constrained environment. We first generate random permutations with\nWTA hashing to encode the shape of the multidimensional audio spectrum to a\nreduced bitstring representation. A nearest neighbor search on the hash codes\nof an incoming noisy spectrum as the query string results in the closest\nmatches among the hashed mixture spectra. Using the indices of the matching\nframes, we obtain the corresponding ideal binary mask vectors for denoising.\nSince both the training data and the search operation are bitwise, the\nprocedure can be done efficiently in hardware implementations. Experimental\nresults show that the WTA hash codes are discriminant and provide an affordable\ndictionary search mechanism that leads to a competent performance compared to a\ncomprehensive model and oracle masking.</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [6.6022748947143555, 32.5054931640625], "cluster": 4}, {"key": "kim2020boosted", "year": "2020", "citations": "9", "title": "Boosted Locality Sensitive Hashing: Discriminative Binary Codes For Source Separation", "abstract": "<p>Speech enhancement tasks have seen significant improvements with the advance\nof deep learning technology, but with the cost of increased computational\ncomplexity. In this study, we propose an adaptive boosting approach to learning\nlocality sensitive hash codes, which represent audio spectra efficiently. We\nuse the learned hash codes for single-channel speech denoising tasks as an\nalternative to a complex machine learning model, particularly to address the\nresource-constrained environments. Our adaptive boosting algorithm learns\nsimple logistic regressors as the weak learners. Once trained, their binary\nclassification results transform each spectrum of test noisy speech into a bit\nstring. Simple bitwise operations calculate Hamming distance to find the\nK-nearest matching frames in the dictionary of training noisy speech spectra,\nwhose associated ideal binary masks are averaged to estimate the denoising mask\nfor that test mixture. Our proposed learning algorithm differs from AdaBoost in\nthe sense that the projections are trained to minimize the distances between\nthe self-similarity matrix of the hash codes and that of the original spectra,\nrather than the misclassification rate. We evaluate our discriminative hash\ncodes on the TIMIT corpus with various noise types, and show comparative\nperformance to deep learning methods in terms of denoising performance and\ncomplexity.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "ICASSP", "Compact-Codes", "Evaluation"], "tsne_embedding": [5.3290300369262695, 32.661258697509766], "cluster": 4}, {"key": "kim2021multi", "year": "2021", "citations": "11", "title": "Multi-level Distance Regularization For Deep Metric Learning", "abstract": "<p>We propose a novel distance-based regularization method for deep metric\nlearning called Multi-level Distance Regularization (MDR). MDR explicitly\ndisturbs a learning procedure by regularizing pairwise distances between\nembedding vectors into multiple levels that represents a degree of similarity\nbetween a pair. In the training stage, the model is trained with both MDR and\nan existing loss function of deep metric learning, simultaneously; the two\nlosses interfere with the objective of each other, and it makes the learning\nprocess difficult. Moreover, MDR prevents some examples from being ignored or\noverly influenced in the learning process. These allow the parameters of the\nembedding network to be settle on a local optima with better generalization.\nWithout bells and whistles, MDR with simple Triplet loss achieves\nthe-state-of-the-art performance in various benchmark datasets: CUB-200-2011,\nCars-196, Stanford Online Products, and In-Shop Clothes Retrieval. We\nextensively perform ablation studies on its behaviors to show the effectiveness\nof MDR. By easily adopting our MDR, the previous approaches can be improved in\nperformance and generalization ability.</p>\n", "tags": ["AAAI", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-36.9061164855957, -1.4177687168121338], "cluster": 0}, {"key": "kim2021swamp", "year": "2021", "citations": "0", "title": "Swamp: Swapped Assignment Of Multi-modal Pairs For Cross-modal Retrieval", "abstract": "<p>We tackle the cross-modal retrieval problem, where learning is only\nsupervised by relevant multi-modal pairs in the data. Although the contrastive\nlearning is the most popular approach for this task, it makes potentially wrong\nassumption that the instances in different pairs are automatically irrelevant.\nTo address the issue, we propose a novel loss function that is based on\nself-labeling of the unknown semantic classes. Specifically, we aim to predict\nclass labels of the data instances in each modality, and assign those labels to\nthe corresponding instances in the other modality (i.e., swapping the pseudo\nlabels). With these swapped labels, we learn the data embedding for each\nmodality using the supervised cross-entropy loss. This way, cross-modal\ninstances from different pairs that are semantically related can be aligned to\neach other by the class predictor. We tested our approach on several real-world\ncross-modal retrieval problems, including text-based video retrieval,\nsketch-based image retrieval, and image-text retrieval. For all these tasks our\nmethod achieves significant performance improvement over the contrastive\nlearning.</p>\n", "tags": ["Text-Retrieval", "Image-Retrieval", "Multimodal-Retrieval", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [9.419890403747559, -3.537510871887207], "cluster": 6}, {"key": "kim2022accelerating", "year": "2022", "citations": "20", "title": "Accelerating Large-scale Graph-based Nearest Neighbor Search On A Computational Storage Platform", "abstract": "<p>K-nearest neighbor search is one of the fundamental tasks in various\napplications and the hierarchical navigable small world (HNSW) has recently\ndrawn attention in large-scale cloud services, as it easily scales up the\ndatabase while offering fast search. On the other hand, a computational storage\ndevice (CSD) that combines programmable logic and storage modules on a single\nboard becomes popular to address the data bandwidth bottleneck of modern\ncomputing systems. In this paper, we propose a computational storage platform\nthat can accelerate a large-scale graph-based nearest neighbor search algorithm\nbased on SmartSSD CSD. To this end, we modify the algorithm more amenable on\nthe hardware and implement two types of accelerators using HLS- and RTL-based\nmethodology with various optimization methods. In addition, we scale up the\nproposed platform to have 4 SmartSSDs and apply graph parallelism to boost the\nsystem performance further. As a result, the proposed computational storage\nplatform achieves 75.59 query per second throughput for the SIFT1B dataset at\n258.66W power dissipation, which is 12.83x and 17.91x faster and 10.43x and\n24.33x more energy efficient than the conventional CPU-based and GPU-based\nserver platform, respectively. With multi-terabyte storage and custom\nacceleration capability, we believe that the proposed computational storage\nplatform is a promising solution for cost-sensitive cloud datacenters.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [48.018062591552734, 14.142094612121582], "cluster": 9}, {"key": "kim2022improving", "year": "2023", "citations": "28", "title": "Improving Cross-modal Retrieval With Set Of Diverse Embeddings", "abstract": "<p>Cross-modal retrieval across image and text modalities is a challenging task\ndue to its inherent ambiguity: An image often exhibits various situations, and\na caption can be coupled with diverse images. Set-based embedding has been\nstudied as a solution to this problem. It seeks to encode a sample into a set\nof different embedding vectors that capture different semantics of the sample.\nIn this paper, we present a novel set-based embedding method, which is distinct\nfrom previous work in two aspects. First, we present a new similarity function\ncalled smooth-Chamfer similarity, which is designed to alleviate the side\neffects of existing similarity functions for set-based embedding. Second, we\npropose a novel set prediction module to produce a set of embedding vectors\nthat effectively captures diverse semantics of input by the slot attention\nmechanism. Our method is evaluated on the COCO and Flickr30K datasets across\ndifferent visual backbones, where it outperforms existing methods including\nones that demand substantially larger computation at inference.</p>\n", "tags": ["Multimodal-Retrieval", "CVPR", "Datasets"], "tsne_embedding": [-23.38572883605957, -6.147492408752441], "cluster": 1}, {"key": "kim2023exposing", "year": "2023", "citations": "15", "title": "Exposing And Mitigating Spurious Correlations For Cross-modal Retrieval", "abstract": "<p>Cross-modal retrieval methods are the preferred tool to search databases for\nthe text that best matches a query image and vice versa. However, image-text\nretrieval models commonly learn to memorize spurious correlations in the\ntraining data, such as frequent object co-occurrence, instead of looking at the\nactual underlying reasons for the prediction in the image. For image-text\nretrieval, this manifests in retrieved sentences that mention objects that are\nnot present in the query image. In this work, we introduce ODmAP@k, an object\ndecorrelation metric that measures a model\u2019s robustness to spurious\ncorrelations in the training data. We use automatic image and text\nmanipulations to control the presence of such object correlations in designated\ntest data. Additionally, our data synthesis technique is used to tackle model\nbiases due to spurious correlations of semantically unrelated objects in the\ntraining data. We apply our proposed pipeline, which involves the finetuning of\nimage-text retrieval frameworks on carefully designed synthetic data, to three\nstate-of-the-art models for image-text retrieval. This results in significant\nimprovements for all three models, both in terms of the standard retrieval\nperformance and in terms of our object decorrelation metric. The code is\navailable at https://github.com/ExplainableML/Spurious_CM_Retrieval.</p>\n", "tags": ["Text-Retrieval", "CVPR", "Robustness", "Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-28.930652618408203, -21.535381317138672], "cluster": 5}, {"key": "kim2023semantic", "year": "2023", "citations": "2", "title": "Semantic-preserving Augmentation For Robust Image-text Retrieval", "abstract": "<p>Image text retrieval is a task to search for the proper textual descriptions\nof the visual world and vice versa. One challenge of this task is the\nvulnerability to input image and text corruptions. Such corruptions are often\nunobserved during the training, and degrade the retrieval model decision\nquality substantially. In this paper, we propose a novel image text retrieval\ntechnique, referred to as robust visual semantic embedding (RVSE), which\nconsists of novel image-based and text-based augmentation techniques called\nsemantic preserving augmentation for image (SPAugI) and text (SPAugT). Since\nSPAugI and SPAugT change the original data in a way that its semantic\ninformation is preserved, we enforce the feature extractors to generate\nsemantic aware embedding vectors regardless of the corruption, improving the\nmodel robustness significantly. From extensive experiments using benchmark\ndatasets, we show that RVSE outperforms conventional retrieval schemes in terms\nof image-text retrieval performance.</p>\n", "tags": ["Text-Retrieval", "Robustness", "ICASSP", "Datasets", "Evaluation"], "tsne_embedding": [-25.871837615966797, -27.335987091064453], "cluster": 5}, {"key": "kim2024computational", "year": "2024", "citations": "0", "title": "A Computational Analysis Of Lyric Similarity Perception", "abstract": "<p>In musical compositions that include vocals, lyrics significantly contribute\nto artistic expression. Consequently, previous studies have introduced the\nconcept of a recommendation system that suggests lyrics similar to a user\u2019s\nfavorites or personalized preferences, aiding in the discovery of lyrics among\nmillions of tracks. However, many of these systems do not fully consider human\nperceptions of lyric similarity, primarily due to limited research in this\narea. To bridge this gap, we conducted a comparative analysis of computational\nmethods for modeling lyric similarity with human perception. Results indicated\nthat computational models based on similarities between embeddings from\npre-trained BERT-based models, the audio from which the lyrics are derived, and\nphonetic components are indicative of perceptual lyric similarity. This finding\nunderscores the importance of semantic, stylistic, and phonetic similarities in\nhuman perception about lyric similarity. We anticipate that our findings will\nenhance the development of similarity-based lyric recommendation systems by\noffering pseudo-labels for neural network development and introducing objective\nevaluation metrics.</p>\n", "tags": ["Recommender-Systems", "Evaluation"], "tsne_embedding": [12.056892395019531, -45.50840759277344], "cluster": 3}, {"key": "kim2024fine", "year": "2024", "citations": "0", "title": "Fine-tuning CLIP Text Encoders With Two-step Paraphrasing", "abstract": "<p>Contrastive language-image pre-training (CLIP) models have demonstrated\nconsiderable success across various vision-language tasks, such as\ntext-to-image retrieval, where the model is required to effectively process\nnatural language input to produce an accurate visual output. However, current\nmodels still face limitations in dealing with linguistic variations in input\nqueries, such as paraphrases, making it challenging to handle a broad range of\nuser queries in real-world applications. In this study, we introduce a\nstraightforward fine-tuning approach to enhance the representations of CLIP\nmodels for paraphrases. Our approach involves a two-step paraphrase generation\nprocess, where we automatically create two categories of paraphrases from\nweb-scale image captions by leveraging large language models. Subsequently, we\nfine-tune the CLIP text encoder using these generated paraphrases while\nfreezing the image encoder. Our resulting model, which we call ParaCLIP,\nexhibits significant improvements over baseline CLIP models across various\ntasks, including paraphrased retrieval (with rank similarity scores improved by\nup to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven\nsemantic textual similarity tasks.</p>\n", "tags": ["Multimodal-Retrieval", "Image-Retrieval", "Large-Scale-Search", "Scalability"], "tsne_embedding": [-32.898223876953125, -31.065147399902344], "cluster": 5}, {"key": "kim2025kgmel", "year": "2025", "citations": "0", "title": "KGMEL: Knowledge Graph-enhanced Multimodal Entity Linking", "abstract": "<p>Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL.</p>\n", "tags": ["Self-Supervised", "SIGIR", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [0.5589749813079834, -33.896244049072266], "cluster": 3}, {"key": "kiran2024image", "year": "2024", "citations": "0", "title": "Image Retrieval Methods In The Dissimilarity Space", "abstract": "<p>Image retrieval methods rely on metric learning to train backbone feature\nextraction models that can extract discriminant queries and reference (gallery)\nfeature representations for similarity matching. Although state-of-the-art\naccuracy has improved considerably with the advent of deep learning (DL) models\ntrained on large datasets, image retrieval remains challenging in many\nreal-world video analytics and surveillance applications, e.g., person\nre-identification. Using the Euclidean space for matching limits the\nperformance in real-world applications due to the curse of dimensionality,\noverfitting, and sensitivity to noisy data.\n  We argue that the feature dissimilarity space is more suitable for similarity\nmatching, and propose a dichotomy transformation to project query and reference\nembeddings into a single embedding in the dissimilarity space.\n  We also advocate for end-to-end training of a backbone and binary\nclassification models for pair-wise matching. As opposed to comparing the\ndistance between queries and reference embeddings, we show the benefits of\nclassifying the single dissimilarity space embedding (as similar or\ndissimilar), especially when trained end-to-end. We propose a method to train\nthe max-margin classifier together with the backbone feature extractor by\napplying constraints to the L2 norm of the classifier weights along with the\nhinge loss.\n  Our extensive experiments on challenging image retrieval datasets and using\ndiverse feature extraction backbones highlight the benefits of similarity\nmatching in the dissimilarity space. In particular, when jointly training the\nfeature extraction backbone and regularised classifier for matching, the\ndissimilarity space provides a higher level of accuracy.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-25.479022979736328, -11.562572479248047], "cluster": 5}, {"key": "kirchoff2024utilizing", "year": "2024", "citations": "3", "title": "Utilizing Low-dimensional Molecular Embeddings For Rapid Chemical Similarity Search", "abstract": "<p>Nearest neighbor-based similarity searching is a common task in chemistry,\nwith notable use cases in drug discovery. Yet, some of the most commonly used\napproaches for this task still leverage a brute-force approach. In practice\nthis can be computationally costly and overly time-consuming, due in part to\nthe sheer size of modern chemical databases. Previous computational\nadvancements for this task have generally relied on improvements to hardware or\ndataset-specific tricks that lack generalizability. Approaches that leverage\nlower-complexity searching algorithms remain relatively underexplored. However,\nmany of these algorithms are approximate solutions and/or struggle with typical\nhigh-dimensional chemical embeddings. Here we evaluate whether a combination of\nlow-dimensional chemical embeddings and a k-d tree data structure can achieve\nfast nearest neighbor queries while maintaining performance on standard\nchemical similarity search benchmarks. We examine different dimensionality\nreductions of standard chemical embeddings as well as a learned,\nstructurally-aware embedding \u2013 SmallSA \u2013 for this task. With this framework,\nsearches on over one billion chemicals execute in less than a second on a\nsingle CPU core, five orders of magnitude faster than the brute-force approach.\nWe also demonstrate that SmallSA achieves competitive performance on chemical\nsimilarity benchmarks.</p>\n", "tags": ["Similarity-Search", "Tree-Based-Ann", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-0.710749089717865, 52.26974105834961], "cluster": 4}, {"key": "klein2017end", "year": "2019", "citations": "59", "title": "End-to-end Supervised Product Quantization For Image Search And Retrieval", "abstract": "<p>Product Quantization, a dictionary based hashing method, is one of the\nleading unsupervised hashing techniques. While it ignores the labels, it\nharnesses the features to construct look up tables that can approximate the\nfeature space. In recent years, several works have achieved state of the art\nresults on hashing benchmarks by learning binary representations in a\nsupervised manner. This work presents Deep Product Quantization (DPQ), a\ntechnique that leads to more accurate retrieval and classification than the\nlatest state of the art methods, while having similar computational complexity\nand memory footprint as the Product Quantization method. To our knowledge, this\nis the first work to introduce a dictionary-based representation that is\ninspired by Product Quantization and which is learned end-to-end, and thus\nbenefits from the supervised signal. DPQ explicitly learns soft and hard\nrepresentations to enable an efficient and accurate asymmetric search, by using\na straight-through estimator. Our method obtains state of the art results on an\nextensive array of retrieval and classification experiments.</p>\n", "tags": ["Hashing-Methods", "Quantization", "CVPR", "Image-Retrieval", "Memory-Efficiency", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [14.456170082092285, 1.844570517539978], "cluster": 6}, {"key": "klein2021learning", "year": "2021", "citations": "1", "title": "Learning Query Expansion Over The Nearest Neighbor Graph", "abstract": "<p>Query Expansion (QE) is a well established method for improving retrieval\nmetrics in image search applications. When using QE, the search is conducted on\na new query vector, constructed using an aggregation function over the query\nand images from the database. Recent works gave rise to QE techniques in which\nthe aggregation function is learned, whereas previous techniques were based on\nhand-crafted aggregation functions, e.g., taking the mean of the query\u2019s\nnearest neighbors. However, most QE methods have focused on aggregation\nfunctions that work directly over the query and its immediate nearest\nneighbors. In this work, a hierarchical model, Graph Query Expansion (GQE), is\npresented, which is learned in a supervised manner and performs aggregation\nover an extended neighborhood of the query, thus increasing the information\nused from the database when computing the query expansion, and using the\nstructure of the nearest neighbors graph. The technique achieves\nstate-of-the-art results over known benchmarks.</p>\n", "tags": ["Supervised", "Image-Retrieval"], "tsne_embedding": [57.88804626464844, -3.6127727031707764], "cluster": 9}, {"key": "kleyko2024design", "year": "2024", "citations": "0", "title": "On Design Choices In Similarity-preserving Sparse Randomized Embeddings", "abstract": "<p>Expand &amp; Sparsify is a principle that is observed in anatomically similar\nneural circuits found in the mushroom body (insects) and the cerebellum\n(mammals). Sensory data are projected randomly to much higher-dimensionality\n(expand part) where only few the most strongly excited neurons are activated\n(sparsify part). This principle has been leveraged to design a FlyHash\nalgorithm that forms similarity-preserving sparse embeddings, which have been\nfound useful for such tasks as novelty detection, pattern recognition, and\nsimilarity search. Despite its simplicity, FlyHash has a number of design\nchoices to be set such as preprocessing of the input data, choice of\nsparsifying activation function, and formation of the random projection matrix.\nIn this paper, we explore the effect of these choices on the performance of\nsimilarity search with FlyHash embeddings. We find that the right combination\nof design choices can lead to drastic difference in the search performance.</p>\n", "tags": ["Evaluation", "Locality-Sensitive-Hashing", "Similarity-Search"], "tsne_embedding": [-32.01346969604492, 19.25490951538086], "cluster": 0}, {"key": "kluser2021fast", "year": "2021", "citations": "0", "title": "Fast Single-core K-nearest Neighbor Graph Computation", "abstract": "<p>Fast and reliable K-Nearest Neighbor Graph algorithms are more important than\never due to their widespread use in many data processing techniques. This paper\npresents a runtime optimized C implementation of the heuristic \u201cNN-Descent\u201d\nalgorithm by Wei Dong et al. for the l2-distance metric. Various implementation\noptimizations are explained which improve performance for low-dimensional as\nwell as high dimensional datasets. Optimizations to speed up the selection of\nwhich datapoint pairs to evaluate the distance for are primarily impactful for\nlow-dimensional datasets. A heuristic which exploits the iterative nature of\nNN-Descent to reorder data in memory is presented which enables better use of\nlocality and thereby improves the runtime. The restriction to the l2-distance\nmetric allows for the use of blocked distance evaluations which significantly\nincrease performance for high dimensional datasets. In combination the\noptimizations yield an implementation which significantly outperforms a widely\nused implementation of NN-Descent on all considered datasets. For instance, the\nruntime on the popular MNIST handwritten digits dataset is halved.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Datasets"], "tsne_embedding": [42.25908279418945, 22.978378295898438], "cluster": 2}, {"key": "ko2019benchmark", "year": "2019", "citations": "1", "title": "A Benchmark On Tricks For Large-scale Image Retrieval", "abstract": "<p>Many studies have been performed on metric learning, which has become a key\ningredient in top-performing methods of instance-level image retrieval.\nMeanwhile, less attention has been paid to pre-processing and post-processing\ntricks that can significantly boost performance. Furthermore, we found that\nmost previous studies used small scale datasets to simplify processing. Because\nthe behavior of a feature representation in a deep learning model depends on\nboth domain and data, it is important to understand how model behave in\nlarge-scale environments when a proper combination of retrieval tricks is used.\nIn this paper, we extensively analyze the effect of well-known pre-processing,\npost-processing tricks, and their combination for large-scale image retrieval.\nWe found that proper use of these tricks can significantly improve model\nperformance without necessitating complex architecture or introducing loss, as\nconfirmed by achieving a competitive result on the Google Landmark Retrieval\nChallenge 2019.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Scalability", "Datasets", "Re-Ranking", "Evaluation"], "tsne_embedding": [-29.374677658081055, -9.233909606933594], "cluster": 5}, {"key": "ko2020embedding", "year": "2020", "citations": "47", "title": "Embedding Expansion: Augmentation In Embedding Space For Deep Metric Learning", "abstract": "<p>Learning the distance metric between pairs of samples has been studied for\nimage retrieval and clustering. With the remarkable success of pair-based\nmetric learning losses, recent works have proposed the use of generated\nsynthetic points on metric learning losses for augmentation and generalization.\nHowever, these methods require additional generative networks along with the\nmain network, which can lead to a larger model size, slower training speed, and\nharder optimization. Meanwhile, post-processing techniques, such as query\nexpansion and database augmentation, have proposed the combination of feature\npoints to obtain additional semantic information. In this paper, inspired by\nquery expansion and database augmentation, we propose an augmentation method in\nan embedding space for pair-based metric learning losses, called embedding\nexpansion. The proposed method generates synthetic points containing augmented\ninformation by a combination of feature points and performs hard negative pair\nmining to learn with the most informative feature representations. Because of\nits simplicity and flexibility, it can be used for existing metric learning\nlosses without affecting model size, training speed, or optimization\ndifficulty. Finally, the combination of embedding expansion and representative\nmetric learning losses outperforms the state-of-the-art losses and previous\nsample generation methods in both image retrieval and clustering tasks. The\nimplementation is publicly available.</p>\n", "tags": ["Re-Ranking", "CVPR", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-19.396713256835938, -13.27920150756836], "cluster": 1}, {"key": "ko2021low", "year": "2021", "citations": "0", "title": "Low-precision Quantization For Efficient Nearest Neighbor Search", "abstract": "<p>Fast k-Nearest Neighbor search over real-valued vector spaces (KNN) is an\nimportant algorithmic task for information retrieval and recommendation\nsystems. We present a method for using reduced precision to represent vectors\nthrough quantized integer values, enabling both a reduction in the memory\noverhead of indexing these vectors and faster distance computations at query\ntime. While most traditional quantization techniques focus on minimizing the\nreconstruction error between a point and its uncompressed counterpart, we focus\ninstead on preserving the behavior of the underlying distance metric.\nFurthermore, our quantization approach is applied at the implementation level\nand can be combined with existing KNN algorithms. Our experiments on both open\nsource and proprietary datasets across multiple popular KNN frameworks validate\nthat quantized distance metrics can reduce memory by 60% and improve query\nthroughput by 30%, while incurring only a 2% reduction in recall.</p>\n", "tags": ["Distance-Metric-Learning", "Quantization", "Recommender-Systems", "Datasets", "Evaluation"], "tsne_embedding": [34.130245208740234, 28.004596710205078], "cluster": 2}, {"key": "kobayashi2020decomposing", "year": "2021", "citations": "31", "title": "Decomposing Normal And Abnormal Features Of Medical Images For Content-based Image Retrieval", "abstract": "<p>Medical images can be decomposed into normal and abnormal features, which is\nconsidered as the compositionality. Based on this idea, we propose an\nencoder-decoder network to decompose a medical image into two discrete latent\ncodes: a normal anatomy code and an abnormal anatomy code. Using these latent\ncodes, we demonstrate a similarity retrieval by focusing on either normal or\nabnormal features of medical images.</p>\n", "tags": ["Similarity-Search", "Image-Retrieval"], "tsne_embedding": [-54.91946792602539, 14.150118827819824], "cluster": 0}, {"key": "kobayashi2023sketch", "year": "2023", "citations": "6", "title": "Sketch-based Medical Image Retrieval", "abstract": "<p>The amount of medical images stored in hospitals is increasing faster than\never; however, utilizing the accumulated medical images has been limited. This\nis because existing content-based medical image retrieval (CBMIR) systems\nusually require example images to construct query vectors; nevertheless,\nexample images cannot always be prepared. Besides, there can be images with\nrare characteristics that make it difficult to find similar example images,\nwhich we call isolated samples. Here, we introduce a novel sketch-based medical\nimage retrieval (SBMIR) system that enables users to find images of interest\nwithout example images. The key idea lies in feature decomposition of medical\nimages, whereby the entire feature of a medical image can be decomposed into\nand reconstructed from normal and abnormal features. By extending this idea,\nour SBMIR system provides an easy-to-use two-step graphical user interface:\nusers first select a template image to specify a normal feature and then draw a\nsemantic sketch of the disease on the template image to represent an abnormal\nfeature. Subsequently, it integrates the two kinds of input to construct a\nquery vector and retrieves reference images with the closest reference vectors.\nUsing two datasets, ten healthcare professionals with various clinical\nbackgrounds participated in the user test for evaluation. As a result, our\nSBMIR system enabled users to overcome previous challenges, including image\nretrieval based on fine-grained image characteristics, image retrieval without\nexample images, and image retrieval for isolated samples. Our SBMIR system\nachieves flexible medical image retrieval on demand, thereby expanding the\nutility of medical image databases.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-51.332855224609375, 15.78610610961914], "cluster": 0}, {"key": "kobs2022background", "year": "2023", "citations": "0", "title": "On Background Bias In Deep Metric Learning", "abstract": "<p>Deep Metric Learning trains a neural network to map input images to a\nlower-dimensional embedding space such that similar images are closer together\nthan dissimilar images. When used for item retrieval, a query image is embedded\nusing the trained model and the closest items from a database storing their\nrespective embeddings are returned as the most similar items for the query.\nEspecially in product retrieval, where a user searches for a certain product by\ntaking a photo of it, the image background is usually not important and thus\nshould not influence the embedding process. Ideally, the retrieval process\nalways returns fitting items for the photographed object, regardless of the\nenvironment the photo was taken in. In this paper, we analyze the influence of\nthe image background on Deep Metric Learning models by utilizing five common\nloss functions and three common datasets. We find that Deep Metric Learning\nnetworks are prone to so-called background bias, which can lead to a severe\ndecrease in retrieval performance when changing the image background during\ninference. We also show that replacing the background of images during training\nwith random background images alleviates this issue. Since we use an automatic\nbackground removal method to do this background replacement, no additional\nmanual labeling work and model changes are required while inference time stays\nthe same. Qualitative and quantitative analyses, for which we introduce a new\nevaluation metric, confirm that models trained with replaced backgrounds attend\nmore to the main object in the image, benefitting item retrieval systems.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-30.848066329956055, -7.623844623565674], "cluster": 5}, {"key": "kobs2022indirect", "year": "2023", "citations": "2", "title": "Indirect: Language-guided Zero-shot Deep Metric Learning For Images", "abstract": "<p>Common Deep Metric Learning (DML) datasets specify only one notion of\nsimilarity, e.g., two images in the Cars196 dataset are deemed similar if they\nshow the same car model. We argue that depending on the application, users of\nimage retrieval systems have different and changing similarity notions that\nshould be incorporated as easily as possible. Therefore, we present\nLanguage-Guided Zero-Shot Deep Metric Learning (LanZ-DML) as a new DML setting\nin which users control the properties that should be important for image\nrepresentations without training data by only using natural language. To this\nend, we propose InDiReCT (Image representations using Dimensionality Reduction\non CLIP embedded Texts), a model for LanZ-DML on images that exclusively uses a\nfew text prompts for training. InDiReCT utilizes CLIP as a fixed feature\nextractor for images and texts and transfers the variation in text prompt\nembeddings to the image embedding space. Extensive experiments on five datasets\nand overall thirteen similarity notions show that, despite not seeing any\nimages during training, InDiReCT performs better than strong baselines and\napproaches the performance of fully-supervised models. An analysis reveals that\nInDiReCT learns to focus on regions of the image that correlate with the\ndesired similarity notion, which makes it a fast to train and easy to use\nmethod to create custom embedding spaces only using natural language.</p>\n", "tags": ["Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-28.166643142700195, -17.4046630859375], "cluster": 5}, {"key": "komorowski2017evaluation", "year": "2017", "citations": "2", "title": "Evaluation Of Hashing Methods Performance On Binary Feature Descriptors", "abstract": "<p>In this paper we evaluate performance of data-dependent hashing methods on\nbinary data. The goal is to find a hashing method that can effectively produce\nlower dimensional binary representation of 512-bit FREAK descriptors. A\nrepresentative sample of recent unsupervised, semi-supervised and supervised\nhashing methods was experimentally evaluated on large datasets of labelled\nbinary FREAK feature descriptors.</p>\n", "tags": ["Hashing-Methods", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-15.623287200927734, 21.989988327026367], "cluster": 8}, {"key": "komorowski2017random", "year": "2019", "citations": "9", "title": "Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space", "abstract": "<p>Approximate nearest neighbour (ANN) search is one of the most important\nproblems in computer science fields such as data mining or computer vision. In\nthis paper, we focus on ANN for high-dimensional binary vectors and we propose\na simple yet powerful search method that uses Random Binary Search Trees\n(RBST). We apply our method to a dataset of 1.25M binary local feature\ndescriptors obtained from a real-life image-based localisation system provided\nby Google as a part of Project Tango. An extensive evaluation of our method\nagainst the state-of-the-art variations of Locality Sensitive Hashing (LSH),\nnamely Uniform LSH and Multi-probe LSH, shows the superiority of our method in\nterms of retrieval precision with performance boost of over 20%</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [13.270153045654297, 36.655704498291016], "cluster": 4}, {"key": "kong2012isotropic", "year": "2012", "citations": "260", "title": "Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [10.918699264526367, 29.31568145751953], "cluster": 4}, {"key": "kong2012manhattan", "year": "2012", "citations": "105", "title": "Manhattan Hashing For Large-scale Image Retrieval", "abstract": "<p>Hashing is used to learn binary-code representation for data with\nexpectation of preserving the neighborhood structure in the original\nfeature space. Due to its fast query speed and reduced storage\ncost, hashing has been widely used for efficient nearest neighbor\nsearch in a large variety of applications like text and image retrieval.\nMost existing hashing methods adopt Hamming distance to\nmeasure the similarity (neighborhood) between points in the hashcode\nspace. However, one problem with Hamming distance is that\nit may destroy the neighborhood structure in the original feature\nspace, which violates the essential goal of hashing. In this paper,\nManhattan hashing (MH), which is based on Manhattan distance, is\nproposed to solve the problem of Hamming distance based hashing.\nThe basic idea of MH is to encode each projected dimension with\nmultiple bits of natural binary code (NBC), based on which the\nManhattan distance between points in the hashcode space is calculated\nfor nearest neighbor search. MH can effectively preserve the\nneighborhood structure in the data to achieve the goal of hashing.\nTo the best of our knowledge, this is the first work to adopt Manhattan\ndistance with NBC for hashing. Experiments on several largescale\nimage data sets containing up to one million points show that\nour MH method can significantly outperform other state-of-the-art\nmethods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "SIGIR", "Compact-Codes"], "tsne_embedding": [-12.947286605834961, 38.812564849853516], "cluster": 8}, {"key": "kong2016coarse2fine", "year": "2016", "citations": "4", "title": "Coarse2fine: Two-layer Fusion For Image Retrieval", "abstract": "<p>This paper addresses the problem of large-scale image retrieval. We propose a\ntwo-layer fusion method which takes advantage of global and local cues and\nranks database images from coarse to fine (C2F). Departing from the previous\nmethods fusing multiple image descriptors simultaneously, C2F is featured by a\nlayered procedure composed by filtering and refining. In particular, C2F\nconsists of three components. 1) Distractor filtering. With holistic\nrepresentations, noise images are filtered out from the database, so the number\nof candidate images to be used for comparison with the query can be greatly\nreduced. 2) Adaptive weighting. For a certain query, the similarity of\ncandidate images can be estimated by holistic similarity scores in\ncomplementary to the local ones. 3) Candidate refining. Accurate retrieval is\nconducted via local features, combining the pre-computed adaptive weights.\nExperiments are presented on two benchmarks, <em>i.e.,</em> Holidays and Ukbench\ndatasets. We show that our method outperforms recent fusion methods in terms of\nstorage consumption and computation complexity, and that the accuracy is\ncompetitive to the state-of-the-arts.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-24.931564331054688, 11.939557075500488], "cluster": 0}, {"key": "kong2025isotropic", "year": "2012", "citations": "260", "title": "Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [10.91870403289795, 29.3156795501709], "cluster": 4}, {"key": "kong2025manhattan", "year": "2012", "citations": "105", "title": "Manhattan Hashing For Large-scale Image Retrieval", "abstract": "<p>Hashing is used to learn binary-code representation for data with\nexpectation of preserving the neighborhood structure in the original\nfeature space. Due to its fast query speed and reduced storage\ncost, hashing has been widely used for efficient nearest neighbor\nsearch in a large variety of applications like text and image retrieval.\nMost existing hashing methods adopt Hamming distance to\nmeasure the similarity (neighborhood) between points in the hashcode\nspace. However, one problem with Hamming distance is that\nit may destroy the neighborhood structure in the original feature\nspace, which violates the essential goal of hashing. In this paper,\nManhattan hashing (MH), which is based on Manhattan distance, is\nproposed to solve the problem of Hamming distance based hashing.\nThe basic idea of MH is to encode each projected dimension with\nmultiple bits of natural binary code (NBC), based on which the\nManhattan distance between points in the hashcode space is calculated\nfor nearest neighbor search. MH can effectively preserve the\nneighborhood structure in the data to achieve the goal of hashing.\nTo the best of our knowledge, this is the first work to adopt Manhattan\ndistance with NBC for hashing. Experiments on several largescale\nimage data sets containing up to one million points show that\nour MH method can significantly outperform other state-of-the-art\nmethods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "SIGIR", "Compact-Codes"], "tsne_embedding": [-12.947290420532227, 38.81256866455078], "cluster": 8}, {"key": "kordopatiszilos2019visil", "year": "2019", "citations": "67", "title": "Visil: Fine-grained Spatio-temporal Video Similarity Learning", "abstract": "<p>In this paper we introduce ViSiL, a Video Similarity Learning architecture\nthat considers fine-grained Spatio-Temporal relations between pairs of videos\n\u2013 such relations are typically lost in previous video retrieval approaches\nthat embed the whole frame or even the whole video into a vector descriptor\nbefore the similarity estimation. By contrast, our Convolutional Neural Network\n(CNN)-based approach is trained to calculate video-to-video similarity from\nrefined frame-to-frame similarity matrices, so as to consider both intra- and\ninter-frame relations. In the proposed method, pairwise frame similarity is\nestimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on\nregional CNN frame features - this avoids feature aggregation before the\nsimilarity calculation between frames. Subsequently, the similarity matrix\nbetween all video frames is fed to a four-layer CNN, and then summarized using\nChamfer Similarity (CS) into a video-to-video similarity score \u2013 this avoids\nfeature aggregation before the similarity calculation between videos and\ncaptures the temporal similarity patterns between matching frame sequences. We\ntrain the proposed network using a triplet loss scheme and evaluate it on five\npublic benchmark datasets on four different video retrieval problems where we\ndemonstrate large improvements in comparison to the state of the art. The\nimplementation of ViSiL is publicly available.</p>\n", "tags": ["ICCV", "Distance-Metric-Learning", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-50.26528549194336, -4.599327087402344], "cluster": 0}, {"key": "kordopatiszilos2021dns", "year": "2022", "citations": "27", "title": "Dns: Distill-and-select For Efficient And Accurate Video Indexing And Retrieval", "abstract": "<p>In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, called\nDistill-and-Select (DnS), that starting from a well-performing fine-grained\nTeacher Network learns: a) Student Networks at different retrieval performance\nand computational efficiency trade-offs and b) a Selector Network that at test\ntime rapidly directs samples to the appropriate student to maintain both high\nretrieval performance and high computational efficiency. We train several\nstudents with different architectures and arrive at different trade-offs of\nperformance and efficiency, i.e., speed and storage requirements, including\nfine-grained students that store/index videos using binary representations.\nImportantly, the proposed scheme allows Knowledge Distillation in large,\nunlabelled datasets \u2013 this leads to good students. We evaluate DnS on five\npublic datasets on three different video retrieval tasks and demonstrate a)\nthat our students achieve state-of-the-art performance in several cases and b)\nthat the DnS framework provides an excellent trade-off between retrieval\nperformance, computational speed, and storage space. In specific\nconfigurations, the proposed method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. The collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [27.759418487548828, -40.05989456176758], "cluster": 7}, {"key": "kordopatiszilos2021leveraging", "year": "2021", "citations": "15", "title": "Leveraging Efficientnet And Contrastive Learning For Accurate Global-scale Location Estimation", "abstract": "<p>In this paper, we address the problem of global-scale image geolocation,\nproposing a mixed classification-retrieval scheme. Unlike other methods that\nstrictly tackle the problem as a classification or retrieval task, we combine\nthe two practices in a unified solution leveraging the advantages of each\napproach with two different modules. The first leverages the EfficientNet\narchitecture to assign images to a specific geographic cell in a robust way.\nThe second introduces a new residual architecture that is trained with\ncontrastive learning to map input images to an embedding space that minimizes\nthe pairwise geodesic distance of same-location images. For the final location\nestimation, the two modules are combined with a search-within-cell scheme,\nwhere the locations of most similar images from the predicted geographic cell\nare aggregated based on a spatial clustering scheme. Our approach demonstrates\nvery competitive performance on four public datasets, achieving new\nstate-of-the-art performance in fine granularity scales, i.e., 15.0% at 1km\nrange on Im2GPS3k.</p>\n", "tags": ["Multimodal-Retrieval", "Self-Supervised", "Datasets", "Evaluation"], "tsne_embedding": [-30.68543815612793, 2.735264778137207], "cluster": 0}, {"key": "korfhage2023elastichash", "year": "2021", "citations": "3", "title": "Elastichash: Semantic Image Similarity Search By Deep Hashing With Elasticsearch", "abstract": "<p>We present ElasticHash, a novel approach for high-quality, efficient, and\nlarge-scale semantic image similarity search. It is based on a deep hashing\nmodel to learn hash codes for fine-grained image similarity search in natural\nimages and a two-stage method for efficiently searching binary hash codes using\nElasticsearch (ES). In the first stage, a coarse search based on short hash\ncodes is performed using multi-index hashing and ES terms lookup of neighboring\nhash codes. In the second stage, the list of results is re-ranked by computing\nthe Hamming distance on long hash codes. We evaluate the retrieval performance\nof \\textit{ElasticHash} for more than 120,000 query images on about 6.9 million\ndatabase images of the OpenImages data set. The results show that our approach\nachieves high-quality retrieval results and low search latencies.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Vector-Indexing", "Similarity-Search", "Scalability", "Evaluation"], "tsne_embedding": [1.3379874229431152, -13.348128318786621], "cluster": 1}, {"key": "korzeniowski2021artist", "year": "2021", "citations": "4", "title": "Artist Similarity With Graph Neural Networks", "abstract": "<p>Artist similarity plays an important role in organizing, understanding, and\nsubsequently, facilitating discovery in large collections of music. In this\npaper, we present a hybrid approach to computing similarity between artists\nusing graph neural networks trained with triplet loss. The novelty of using a\ngraph neural network architecture is to combine the topology of a graph of\nartist connections with content features to embed artists into a vector space\nthat encodes similarity. To evaluate the proposed method, we compile the new\nOLGA dataset, which contains artist similarities from AllMusic, together with\ncontent features from AcousticBrainz. With 17,673 artists, this is the largest\nacademic artist similarity dataset that includes content-based features to\ndate. Moreover, we also showcase the scalability of our approach by\nexperimenting with a much larger proprietary dataset. Results show the\nsuperiority of the proposed approach over current state-of-the-art methods for\nmusic similarity. Finally, we hope that the OLGA dataset will facilitate\nresearch on data-driven models for artist similarity.</p>\n", "tags": ["Distance-Metric-Learning", "Scalability", "Datasets"], "tsne_embedding": [51.984867095947266, -16.72760772705078], "cluster": 9}, {"key": "kosti\u01072021multi", "year": "2021", "citations": "12", "title": "Multi-modal Retrieval Of Tables And Texts Using Tri-encoder Models", "abstract": "<p>Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.</p>\n", "tags": ["Graph-Based-Ann", "Datasets", "Evaluation"], "tsne_embedding": [6.623163223266602, -30.351348876953125], "cluster": 7}, {"key": "kouros2022category", "year": "2022", "citations": "1", "title": "Category-level Pose Retrieval With Contrastive Features Learnt With Occlusion Augmentation", "abstract": "<p>Pose estimation is usually tackled as either a bin classification or a\nregression problem. In both cases, the idea is to directly predict the pose of\nan object. This is a non-trivial task due to appearance variations between\nsimilar poses and similarities between dissimilar poses. Instead, we follow the\nkey idea that comparing two poses is easier than directly predicting one.\nRender-and-compare approaches have been employed to that end, however, they\ntend to be unstable, computationally expensive, and slow for real-time\napplications. We propose doing category-level pose estimation by learning an\nalignment metric in an embedding space using a contrastive loss with a dynamic\nmargin and a continuous pose-label space. For efficient inference, we use a\nsimple real-time image retrieval scheme with a pre-rendered and pre-embedded\nreference set of renderings. To achieve robustness to real-world conditions, we\nemploy synthetic occlusions, bounding box perturbations, and appearance\naugmentations. Our approach achieves state-of-the-art performance on PASCAL3D\nand OccludedPASCAL3D and surpasses the competing methods on KITTI3D in a\ncross-dataset evaluation setting. The code is currently available at\nhttps://github.com/gkouros/contrastive-pose-retrieval.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Image-Retrieval", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-40.159034729003906, -10.760261535644531], "cluster": 5}, {"key": "koutaki2016fast", "year": "2016", "citations": "7", "title": "Fast Supervised Discrete Hashing And Its Analysis", "abstract": "<p>In this paper, we propose a learning-based supervised discrete hashing\nmethod. Binary hashing is widely used for large-scale image retrieval as well\nas video and document searches because the compact representation of binary\ncode is essential for data storage and reasonable for query searches using\nbit-operations. The recently proposed Supervised Discrete Hashing (SDH)\nefficiently solves mixed-integer programming problems by alternating\noptimization and the Discrete Cyclic Coordinate descent (DCC) method. We show\nthat the SDH model can be simplified without performance degradation based on\nsome preliminary experiments; we call the approximate model for this the \u201cFast\nSDH\u201d (FSDH) model. We analyze the FSDH model and provide a mathematically exact\nsolution for it. In contrast to SDH, our model does not require an alternating\noptimization algorithm and does not depend on initial values. FSDH is also\neasier to implement than Iterative Quantization (ITQ). Experimental results\ninvolving a large-scale database showed that FSDH outperforms conventional SDH\nin terms of precision, recall, and computation time.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Scalability", "Supervised", "Evaluation"], "tsne_embedding": [20.286861419677734, 2.1394400596618652], "cluster": 6}, {"key": "kraus2017fishing", "year": "2017", "citations": "2", "title": "Fishing In The Stream: Similarity Search Over Endless Data", "abstract": "<p>Similarity search is the task of retrieving data items that are similar to a\ngiven query. In this paper, we introduce the time-sensitive notion of\nsimilarity search over endless data-streams (SSDS), which takes into account\ndata quality and temporal characteristics in addition to similarity. SSDS is\nchallenging as it needs to process unbounded data, while computation resources\nare bounded. We propose Stream-LSH, a randomized SSDS algorithm that bounds the\nindex size by retaining items according to their freshness, quality, and\ndynamic popularity attributes. We analytically show that Stream-LSH increases\nthe probability to find similar items compared to alternative approaches using\nthe same space capacity. We further conduct an empirical study using real world\nstream datasets, which confirms our theoretical results.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Similarity-Search", "Datasets"], "tsne_embedding": [-21.807912826538086, 26.322542190551758], "cluster": 8}, {"key": "krishna2019video", "year": "2019", "citations": "0", "title": "Video Segment Copy Detection Using Memory Constrained Hierarchical Batch-normalized LSTM Autoencoder", "abstract": "<p>In this report, we introduce a video hashing method for scalable video\nsegment copy detection. The objective of video segment copy detection is to\nfind the video (s) present in a large database, one of whose segments (cropped\nin time) is a (transformed) copy of the given query video. This transformation\nmay be temporal (for example frame dropping, change in frame rate) or spatial\n(brightness and contrast change, addition of noise etc.) in nature although the\nprimary focus of this report is detecting temporal attacks. The video hashing\nmethod proposed by us uses a deep learning neural network to learn variable\nlength binary hash codes for the entire video considering both temporal and\nspatial features into account. This is in contrast to most existing video\nhashing methods, as they use conventional image hashing techniques to obtain\nhash codes for a video after extracting features for every frame or certain key\nframes, in which case the temporal information present in the video is not\nexploited. Our hashing method is specifically resilient to time cropping making\nit extremely useful in video segment copy detection. Experimental results\nobtained on the large augmented dataset consisting of around 25,000 videos with\nsegment copies demonstrate the efficacy of our proposed video hashing method.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Datasets"], "tsne_embedding": [-11.865628242492676, -0.8196297883987427], "cluster": 1}, {"key": "krishna2021evaluating", "year": "2021", "citations": "23", "title": "Evaluating Contrastive Models For Instance-based Image Retrieval", "abstract": "<p>In this work, we evaluate contrastive models for the task of image retrieval.\nWe hypothesise that models that are learned to encode semantic similarity among\ninstances via discriminative learning should perform well on the task of image\nretrieval, where relevancy is defined in terms of instances of the same object.\nThrough our extensive evaluation, we find that representations from models\ntrained using contrastive methods perform on-par with (and outperforms) a\npre-trained supervised baseline trained on the ImageNet labels in retrieval\ntasks under various configurations. This is remarkable given that the\ncontrastive models require no explicit supervision. Thus, we conclude that\nthese models can be used to bootstrap base models to build more robust image\nretrieval engines.</p>\n", "tags": ["Multimodal-Retrieval", "Supervised", "Image-Retrieval", "Evaluation"], "tsne_embedding": [-19.558616638183594, -21.857501983642578], "cluster": 5}, {"key": "krishna2023imaginator", "year": "2023", "citations": "0", "title": "IMAGINATOR: Pre-trained Image+text Joint Embeddings Using Word-level Grounding Of Images", "abstract": "<p>Word embeddings, i.e., semantically meaningful vector representation of\nwords, are largely influenced by the distributional hypothesis \u201cYou shall know\na word by the company it keeps\u201d (Harris, 1954), whereas modern prediction-based\nneural network embeddings rely on design choices and hyperparameter\noptimization. Word embeddings like Word2Vec, GloVe etc. well capture the\ncontextuality and real-world analogies but contemporary convolution-based image\nembeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge.\nThe popular king-queen analogy does not hold true for most commonly used vision\nembeddings.\n  In this paper, we introduce a pre-trained joint embedding (JE), named\nIMAGINATOR, trained on 21K distinct image objects level from 1M image+text\npairs. JE is a way to encode multimodal data into a vector space where the text\nmodality serves as the ground-ing key, which the complementary modality (in\nthis case, the image) is anchored with. IMAGINATOR encapsulates three\nindividual representations: (i) object-object co-location, (ii) word-object\nco-location, and (iii) word-object correlation. These three ways capture\ncomplementary aspects of the two modalities which are further combined to\nobtain the final JEs.\n  Generated JEs are intrinsically evaluated to assess how well they capture the\ncontextuality and real-world analogies. We also evaluate pre-trained IMAGINATOR\nJEs on three downstream tasks: (i) image captioning, (ii) Image2Tweet, and\n(iii) text-based image retrieval. IMAGINATOR establishes a new standard on the\naforementioned down-stream tasks by outperforming the current SoTA on all the\nselected tasks. IMAGINATOR will be made publicly available. The codes are\navailable at https://github.com/varunakk/IMAGINATOR</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-30.521831512451172, -12.623268127441406], "cluster": 5}, {"key": "krishnan2021projective", "year": "2021", "citations": "2", "title": "Projective Clustering Product Quantization", "abstract": "<p>This paper suggests the use of projective clustering based product\nquantization for improving nearest neighbor and max-inner-product vector search\n(MIPS) algorithms. We provide anisotropic and quantized variants of projective\nclustering which outperform previous clustering methods used for this problem\nsuch as ScaNN. We show that even with comparable running time complexity, in\nterms of lookup-multiply-adds, projective clustering produces more quantization\ncenters resulting in more accurate dot-product estimates. We provide thorough\nexperimentation to support our claims.</p>\n", "tags": ["Quantization", "Tools-&-Libraries"], "tsne_embedding": [-0.24021917581558228, -18.217266082763672], "cluster": 1}, {"key": "kuang2019fashion", "year": "2019", "citations": "87", "title": "Fashion Retrieval Via Graph Reasoning Networks On A Similarity Pyramid", "abstract": "<p>Matching clothing images from customers and online shopping stores has rich\napplications in E-commerce. Existing algorithms encoded an image as a global\nfeature vector and performed retrieval with the global representation. However,\ndiscriminative local information on clothes are submerged in this global\nrepresentation, resulting in sub-optimal performance. To address this issue, we\npropose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which\nlearns similarities between a query and a gallery cloth by using both global\nand local representations in multiple scales. The similarity pyramid is\nrepresented by a Graph of similarity, where nodes represent similarities\nbetween clothing components at different scales, and the final matching score\nis obtained by message passing along edges. In GRNet, graph reasoning is solved\nby training a graph convolutional network, enabling to align salient clothing\ncomponents to improve clothing retrieval. To facilitate future researches, we\nintroduce a new benchmark FindFashion, containing rich annotations of bounding\nboxes, views, occlusions, and cropping. Extensive experiments show that GRNet\nobtains new state-of-the-art results on two challenging benchmarks, e.g.,\npushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%,\nand 75% (i.e., 4%, 10%, and 10% absolute improvements), outperforming\ncompetitors with large margins. On FindFashion, GRNet achieves considerable\nimprovements on all empirical settings.</p>\n", "tags": ["ICCV", "Evaluation"], "tsne_embedding": [-17.86459732055664, -50.08426284790039], "cluster": 3}, {"key": "kubota2024impression", "year": "2024", "citations": "0", "title": "Impression-clip: Contrastive Shape-impression Embedding For Fonts", "abstract": "<p>Fonts convey different impressions to readers. These impressions often come\nfrom the font shapes. However, the correlation between fonts and their\nimpression is weak and unstable because impressions are subjective. To capture\nsuch weak and unstable cross-modal correlation between font shapes and their\nimpressions, we propose Impression-CLIP, which is a novel machine-learning\nmodel based on CLIP (Contrastive Language-Image Pre-training). By using the\nCLIP-based model, font image features and their impression features are pulled\ncloser, and font image features and unrelated impression features are pushed\napart. This procedure realizes co-embedding between font image and their\nimpressions. In our experiment, we perform cross-modal retrieval between fonts\nand impressions through co-embedding. The results indicate that Impression-CLIP\nachieves better retrieval accuracy than the state-of-the-art method.\nAdditionally, our model shows the robustness to noise and missing tags.</p>\n", "tags": ["Multimodal-Retrieval", "Robustness"], "tsne_embedding": [-27.278491973876953, -20.14267349243164], "cluster": 5}, {"key": "kulis2009kernelized", "year": "2009", "citations": "908", "title": "Kernelized Locality-sensitive Hashing For Scalable Image Search", "abstract": "<p>Fast retrieval methods are critical for large-scale and\ndata-driven vision applications. Recent work has explored\nways to embed high-dimensional features or complex distance\nfunctions into a low-dimensional Hamming space\nwhere items can be efficiently searched. However, existing\nmethods do not apply for high-dimensional kernelized\ndata when the underlying feature embedding for the kernel\nis unknown. We show how to generalize locality-sensitive\nhashing to accommodate arbitrary kernel functions, making\nit possible to preserve the algorithm\u2019s sub-linear time similarity\nsearch guarantees for a wide class of useful similarity\nfunctions. Since a number of successful image-based kernels\nhave unknown or incomputable embeddings, this is especially\nvaluable for image retrieval tasks. We validate our\ntechnique on several large-scale datasets, and show that it\nenables accurate and fast performance for example-based\nobject classification, feature matching, and content-based\nretrieval.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-18.17010498046875, 10.637450218200684], "cluster": 8}, {"key": "kulis2009learning", "year": "2009", "citations": "841", "title": "Learning To Hash With Binary Reconstructive Embeddings", "abstract": "<p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been\nseveral recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.\nIn this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the\nreconstruction error between the original distances and the Hamming distances of the corresponding binary\nembeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that\nis able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic\nhashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions\nabout the underlying distribution of the data. We present results over several domains to demonstrate that\nour method outperforms existing state-of-the-art techniques.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Scalability"], "tsne_embedding": [28.545162200927734, -1.9648849964141846], "cluster": 6}, {"key": "kulis2025kernelized", "year": "2009", "citations": "908", "title": "Kernelized Locality-sensitive Hashing For Scalable Image Search", "abstract": "<p>Fast retrieval methods are critical for large-scale and\ndata-driven vision applications. Recent work has explored\nways to embed high-dimensional features or complex distance\nfunctions into a low-dimensional Hamming space\nwhere items can be efficiently searched. However, existing\nmethods do not apply for high-dimensional kernelized\ndata when the underlying feature embedding for the kernel\nis unknown. We show how to generalize locality-sensitive\nhashing to accommodate arbitrary kernel functions, making\nit possible to preserve the algorithm\u2019s sub-linear time similarity\nsearch guarantees for a wide class of useful similarity\nfunctions. Since a number of successful image-based kernels\nhave unknown or incomputable embeddings, this is especially\nvaluable for image retrieval tasks. We validate our\ntechnique on several large-scale datasets, and show that it\nenables accurate and fast performance for example-based\nobject classification, feature matching, and content-based\nretrieval.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-18.17012596130371, 10.637449264526367], "cluster": 8}, {"key": "kulis2025learning", "year": "2009", "citations": "841", "title": "Learning To Hash With Binary Reconstructive Embeddings", "abstract": "<p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been\nseveral recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.\nIn this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the\nreconstruction error between the original distances and the Hamming distances of the corresponding binary\nembeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that\nis able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic\nhashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions\nabout the underlying distribution of the data. We present results over several domains to demonstrate that\nour method outperforms existing state-of-the-art techniques.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Scalability"], "tsne_embedding": [28.54448890686035, -1.964748740196228], "cluster": 6}, {"key": "kulkarni2016similarity", "year": "2016", "citations": "3", "title": "Similarity Preserving Compressions Of High Dimensional Sparse Data", "abstract": "<p>The rise of internet has resulted in an explosion of data consisting of\nmillions of articles, images, songs, and videos. Most of this data is high\ndimensional and sparse. The need to perform an efficient search for similar\nobjects in such high dimensional big datasets is becoming increasingly common.\nEven with the rapid growth in computing power, the brute-force search for such\na task is impractical and at times impossible. Therefore it is quite natural to\ninvestigate the techniques that compress the dimension of the data-set while\npreserving the similarity between data objects.\n  In this work, we propose an efficient compression scheme mapping binary\nvectors into binary vectors and simultaneously preserving Hamming distance and\nInner Product. The length of our compression depends only on the sparsity and\nis independent of the dimension of the data. Moreover our schemes provide\none-shot solution for Hamming distance and Inner Product, and work in the\nstreaming setting as well. In contrast with the \u201clocal projection\u201d strategies\nused by most of the previous schemes, our scheme combines (using sparsity) the\nfollowing two strategies: \\(1.\\) Partitioning the dimensions into several\nbuckets, \\(2.\\) Then obtaining \u201cglobal linear summaries\u201d in each of these\nbuckets. We generalize our scheme for real-valued data and obtain compressions\nfor Euclidean distance, Inner Product, and \\(k\\)-way Inner Product.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [9.468225479125977, 26.7658634185791], "cluster": 4}, {"key": "kulkarni2023lexically", "year": "2023", "citations": "19", "title": "Lexically-accelerated Dense Retrieval", "abstract": "<p>Retrieval approaches that score documents based on learned dense vectors\n(i.e., dense retrieval) rather than lexical signals (i.e., conventional\nretrieval) are increasingly popular. Their ability to identify related\ndocuments that do not necessarily contain the same terms as those appearing in\nthe user\u2019s query (thereby improving recall) is one of their key advantages.\nHowever, to actually achieve these gains, dense retrieval approaches typically\nrequire an exhaustive search over the document collection, making them\nconsiderably more expensive at query-time than conventional lexical approaches.\nSeveral techniques aim to reduce this computational overhead by approximating\nthe results of a full dense retriever. Although these approaches reasonably\napproximate the top results, they suffer in terms of recall \u2013 one of the key\nadvantages of dense retrieval. We introduce \u2018LADR\u2019 (Lexically-Accelerated Dense\nRetrieval), a simple-yet-effective approach that improves the efficiency of\nexisting dense retrieval models without compromising on retrieval\neffectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval\nexploration that uses a document proximity graph. We explore two variants of\nLADR: a proactive approach that expands the search space to the neighbors of\nall seed documents, and an adaptive approach that selectively searches the\ndocuments with the highest estimated relevance in an iterative fashion. Through\nextensive experiments across a variety of dense retrieval models, we find that\nLADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier\namong approximate k nearest neighbor techniques. Further, we find that when\ntuned to take around 8ms per query in retrieval latency on our hardware, LADR\nconsistently achieves both precision and recall that are on par with an\nexhaustive search on standard benchmarks.</p>\n", "tags": ["Efficiency", "SIGIR", "Graph-Based-Ann", "Evaluation"], "tsne_embedding": [26.15999412536621, 23.996807098388672], "cluster": 2}, {"key": "kulkarni2024lexboost", "year": "2024", "citations": "4", "title": "Lexboost: Improving Lexical Document Retrieval With Nearest Neighbors", "abstract": "<p>Sparse retrieval methods like BM25 are based on lexical overlap, focusing on\nthe surface form of the terms that appear in the query and the document. The\nuse of inverted indices in these methods leads to high retrieval efficiency. On\nthe other hand, dense retrieval methods are based on learned dense vectors and,\nconsequently, are effective but comparatively slow. Since sparse and dense\nmethods approach problems differently and use complementary relevance signals,\napproximation methods were proposed to balance effectiveness and efficiency.\nFor efficiency, approximation methods like HNSW are frequently used to\napproximate exhaustive dense retrieval. However, approximation techniques still\nexhibit considerably higher latency than sparse approaches. We propose LexBoost\nthat first builds a network of dense neighbors (a corpus graph) using a dense\nretrieval approach while indexing. Then, during retrieval, we consider both a\ndocument\u2019s lexical relevance scores and its neighbors\u2019 scores to rank the\ndocuments. In LexBoost this remarkably simple application of the Cluster\nHypothesis contributes to stronger ranking effectiveness while contributing\nlittle computational overhead (since the corpus graph is constructed offline).\nThe method is robust across the number of neighbors considered, various fusion\nparameters for determining the scores, and different dataset construction\nmethods. We also show that re-ranking on top of LexBoost outperforms\ntraditional dense re-ranking and leads to results comparable with\nhigher-latency exhaustive dense retrieval.</p>\n", "tags": ["Graph-Based-Ann", "Text-Retrieval", "Efficiency", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [26.084095001220703, 22.989227294921875], "cluster": 2}, {"key": "kumar2011learning", "year": "2011", "citations": "438", "title": "Learning Hash Functions For Cross-view Similarity Search", "abstract": "<p>Many applications in Multilingual and Multimodal\nInformation Access involve searching large\ndatabases of high dimensional data objects with\nmultiple (conditionally independent) views. In this\nwork we consider the problem of learning hash\nfunctions for similarity search across the views\nfor such applications. We propose a principled\nmethod for learning a hash function for each view\ngiven a set of multiview training data objects. The\nhash functions map similar objects to similar codes\nacross the views thus enabling cross-view similarity\nsearch. We present results from an extensive\nempirical study of the proposed approach\nwhich demonstrate its effectiveness on Japanese\nlanguage People Search and Multilingual People\nSearch problems.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [-5.840054035186768, 28.494169235229492], "cluster": 8}, {"key": "kumar2017neural", "year": "2017", "citations": "2", "title": "Neural Signatures For Licence Plate Re-identification", "abstract": "<p>The problem of vehicle licence plate re-identification is generally\nconsidered as a one-shot image retrieval problem. The objective of this task is\nto learn a feature representation (called a \u201csignature\u201d) for licence plates.\nIncoming licence plate images are converted to signatures and matched to a\npreviously collected template database through a distance measure. Then, the\ninput image is recognized as the template whose signature is \u201cnearest\u201d to the\ninput signature. The template database is restricted to contain only a single\nsignature per unique licence plate for our problem.\n  We measure the performance of deep convolutional net-based features adapted\nfrom face recognition on this task. In addition, we also test a hybrid approach\ncombining the Fisher vector with a neural network-based embedding called \u201cf2nn\u201d\ntrained with the Triplet loss function. We find that the hybrid approach\nperforms comparably while providing computational benefits. The signature\ngenerated by the hybrid approach also shows higher generalizability to datasets\nmore dissimilar to the training corpus.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-19.783899307250977, 5.998847007751465], "cluster": 1}, {"key": "kumar2023ehi", "year": "2023", "citations": "0", "title": "EHI: End-to-end Learning Of Hierarchical Index For Efficient Dense Retrieval", "abstract": "<p>Dense embedding-based retrieval is widely used for semantic search and\nranking. However, conventional two-stage approaches, involving contrastive\nembedding learning followed by approximate nearest neighbor search (ANNS), can\nsuffer from misalignment between these stages. This mismatch degrades retrieval\nperformance. We propose End-to-end Hierarchical Indexing (EHI), a novel method\nthat directly addresses this issue by jointly optimizing embedding generation\nand ANNS structure. EHI leverages a dual encoder for embedding queries and\ndocuments while simultaneously learning an inverted file index (IVF)-style tree\nstructure. To facilitate the effective learning of this discrete structure, EHI\nintroduces dense path embeddings that encodes the path traversed by queries and\ndocuments within the tree. Extensive evaluations on standard benchmarks,\nincluding MS MARCO (Dev set) and TREC DL19, demonstrate EHI\u2019s superiority over\ntraditional ANNS index. Under the same computational constraints, EHI\noutperforms existing state-of-the-art methods by +1.45% in MRR@10 on MS MARCO\n(Dev) and +8.2% in nDCG@10 on TREC DL19, highlighting the benefits of our\nend-to-end approach.</p>\n", "tags": ["Vector-Indexing", "Evaluation"], "tsne_embedding": [7.538625240325928, -22.662094116210938], "cluster": 7}, {"key": "kumar2025learning", "year": "2011", "citations": "438", "title": "Learning Hash Functions For Cross-view Similarity Search", "abstract": "<p>Many applications in Multilingual and Multimodal\nInformation Access involve searching large\ndatabases of high dimensional data objects with\nmultiple (conditionally independent) views. In this\nwork we consider the problem of learning hash\nfunctions for similarity search across the views\nfor such applications. We propose a principled\nmethod for learning a hash function for each view\ngiven a set of multiview training data objects. The\nhash functions map similar objects to similar codes\nacross the views thus enabling cross-view similarity\nsearch. We present results from an extensive\nempirical study of the proposed approach\nwhich demonstrate its effectiveness on Japanese\nlanguage People Search and Multilingual People\nSearch problems.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [-5.840043067932129, 28.494171142578125], "cluster": 8}, {"key": "kuo2016de", "year": "2016", "citations": "0", "title": "De-hashing: Server-side Context-aware Feature Reconstruction For Mobile Visual Search", "abstract": "<p>Due to the prevalence of mobile devices, mobile search becomes a more\nconvenient way than desktop search. Different from the traditional desktop\nsearch, mobile visual search needs more consideration for the limited resources\non mobile devices (e.g., bandwidth, computing power, and memory consumption).\nThe state-of-the-art approaches show that bag-of-words (BoW) model is robust\nfor image and video retrieval; however, the large vocabulary tree might not be\nable to be loaded on the mobile device. We observe that recent works mainly\nfocus on designing compact feature representations on mobile devices for\nbandwidth-limited network (e.g., 3G) and directly adopt feature matching on\nremote servers (cloud). However, the compact (binary) representation might fail\nto retrieve target objects (images, videos). Based on the hashed binary codes,\nwe propose a de-hashing process that reconstructs BoW by leveraging the\ncomputing power of remote servers. To mitigate the information loss from binary\ncodes, we further utilize contextual information (e.g., GPS) to reconstruct a\ncontext-aware BoW for better retrieval results. Experiment results show that\nthe proposed method can achieve competitive retrieval accuracy as BoW while\nonly transmitting few bits from mobile devices.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Video-Retrieval", "Image-Retrieval"], "tsne_embedding": [37.34227752685547, 5.894321918487549], "cluster": 9}, {"key": "kutuzov2019making", "year": "2019", "citations": "3", "title": "Making Fast Graph-based Algorithms With Graph Metric Embeddings", "abstract": "<p>The computation of distance measures between nodes in graphs is inefficient\nand does not scale to large graphs. We explore dense vector representations as\nan effective way to approximate the same information: we introduce a simple yet\nefficient and effective approach for learning graph embeddings. Instead of\ndirectly operating on the graph structure, our method takes structural measures\nof pairwise node similarities into account and learns dense node\nrepresentations reflecting user-defined graph distance measures, such as\ne.g.the shortest path distance or distance measures that take information\nbeyond the graph structure into account. We demonstrate a speed-up of several\norders of magnitude when predicting word similarity by vector operations on our\nembeddings as opposed to directly computing the respective path-based measures,\nwhile outperforming various other graph embeddings on semantic similarity and\nword sense disambiguation tasks and show evaluations on the WordNet graph and\ntwo knowledge base graphs.</p>\n", "tags": ["Graph-Based-Ann"], "tsne_embedding": [55.86774826049805, 4.248098373413086], "cluster": 9}, {"key": "kuwa2020embedding", "year": "2020", "citations": "1", "title": "Embedding Meta-textual Information For Improved Learning To Rank", "abstract": "<p>Neural approaches to learning term embeddings have led to improved\ncomputation of similarity and ranking in information retrieval (IR). So far\nneural representation learning has not been extended to meta-textual\ninformation that is readily available for many IR tasks, for example, patent\nclasses in prior-art retrieval, topical information in Wikipedia articles, or\nproduct categories in e-commerce data. We present a framework that learns\nembeddings for meta-textual categories, and optimizes a pairwise ranking\nobjective for improved matching based on combined embeddings of textual and\nmeta-textual information. We show considerable gains in an experimental\nevaluation on cross-lingual retrieval in the Wikipedia domain for three\nlanguage pairs, and in the Patent domain for one language pair. Our results\nemphasize that the mode of combining different types of information is crucial\nfor model improvement.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation"], "tsne_embedding": [0.32575249671936035, -31.142724990844727], "cluster": 3}, {"key": "kwok2020learning", "year": "2020", "citations": "10", "title": "Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval", "abstract": "<p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Scalability", "Image-Retrieval", "Compact-Codes", "Evaluation"], "tsne_embedding": [-1.6691198348999023, 22.59779167175293], "cluster": 8}, {"key": "kwok2025learning", "year": "2020", "citations": "10", "title": "Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval", "abstract": "<p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Scalability", "Image-Retrieval", "Compact-Codes", "Evaluation"], "tsne_embedding": [-1.6693706512451172, 22.5978946685791], "cluster": 8}, {"key": "k\u00fadela2018extracting", "year": "2017", "citations": "4", "title": "Extracting Parallel Paragraphs From Common Crawl", "abstract": "<p>Most of the current methods for mining parallel texts from the web assume\nthat web pages of web sites share same structure across languages. We believe\nthat there still exists a non-negligible amount of parallel data spread across\nsources not satisfying this assumption. We propose an approach based on a\ncombination of bivec (a bilingual extension of word2vec) and locality-sensitive\nhashing which allows us to efficiently identify pairs of parallel segments\nlocated anywhere on pages of a given web domain, regardless their structure. We\nvalidate our method on realigning segments from a large parallel corpus.\nAnother experiment with real-world data provided by Common Crawl Foundation\nconfirms that our solution scales to hundreds of terabytes large set of\nweb-crawled data.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [37.18633270263672, -23.202964782714844], "cluster": 7}, {"key": "laarhoven2017faster", "year": "2017", "citations": "2", "title": "Faster Tuple Lattice Sieving Using Spherical Locality-sensitive Filters", "abstract": "<p>To overcome the large memory requirement of classical lattice sieving\nalgorithms for solving hard lattice problems, Bai-Laarhoven-Stehl'{e} [ANTS\n2016] studied tuple lattice sieving, where tuples instead of pairs of lattice\nvectors are combined to form shorter vectors. Herold-Kirshanova [PKC 2017]\nrecently improved upon their results for arbitrary tuple sizes, for example\nshowing that a triple sieve can solve the shortest vector problem (SVP) in\ndimension \\(d\\) in time \\(2^{0.3717d + o(d)}\\), using a technique similar to\nlocality-sensitive hashing for finding nearest neighbors.\n  In this work, we generalize the spherical locality-sensitive filters of\nBecker-Ducas-Gama-Laarhoven [SODA 2016] to obtain space-time tradeoffs for near\nneighbor searching on dense data sets, and we apply these techniques to tuple\nlattice sieving to obtain even better time complexities. For instance, our\ntriple sieve heuristically solves SVP in time \\(2^{0.3588d + o(d)}\\). For\npractical sieves based on Micciancio-Voulgaris\u2019 GaussSieve [SODA 2010], this\nshows that a triple sieve uses less space and less time than the current best\nnear-linear space double sieve.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [17.277538299560547, 44.24072265625], "cluster": 4}, {"key": "laarhoven2017graph", "year": "2017", "citations": "5", "title": "Graph-based Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size \\(n = 2^{o(d)}\\) on\nthe \\(d\\)-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor \\(c &gt; 1\\) in query time \\(n^{\\rho_q + o(1)}\\) and space \\(n^{1 + \\rho_s +\no(1)}\\), for arbitrary \\(\\rho_q, \\rho_s \\geq 0\\) satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small \\(c\\) and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA\u201917]. We further\nstudy how the trade-offs scale when the data set is of size \\(n =\n2^{\\Theta(d)}\\), and analyze asymptotic complexities when applying these results\nto lattice sieving.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann"], "tsne_embedding": [24.714460372924805, 50.516944885253906], "cluster": 4}, {"key": "laarhoven2017hypercube", "year": "2017", "citations": "2", "title": "Hypercube LSH For Approximate Near Neighbors", "abstract": "<p>A celebrated technique for finding near neighbors for the angular distance\ninvolves using a set of \\textit{random} hyperplanes to partition the space into\nhash regions [Charikar, STOC 2002]. Experiments later showed that using a set\nof \\textit{orthogonal} hyperplanes, thereby partitioning the space into the\nVoronoi regions induced by a hypercube, leads to even better results [Terasawa\nand Tanaka, WADS 2007]. However, no theoretical explanation for this\nimprovement was ever given, and it remained unclear how the resulting hypercube\nhash method scales in high dimensions.\n  In this work, we provide explicit asymptotics for the collision probabilities\nwhen using hypercubes to partition the space. For instance, two near-orthogonal\nvectors are expected to collide with probability \\((\\frac{1}{\\pi})^{d + o(d)}\\)\nin dimension \\(d\\), compared to \\((\\frac{1}{2})^d\\) when using random hyperplanes.\nVectors at angle \\(\\frac{\\pi}{3}\\) collide with probability\n\\((\\frac{\\sqrt{3}}{\\pi})^{d + o(d)}\\), compared to \\((\\frac{2}{3})^d\\) for random\nhyperplanes, and near-parallel vectors collide with similar asymptotic\nprobabilities in both cases.\n  For \\(c\\)-approximate nearest neighbor searching, this translates to a decrease\nin the exponent \\(\\rho\\) of locality-sensitive hashing (LSH) methods of a factor\nup to \\(log_2(\\pi) \\approx 1.652\\) compared to hyperplane LSH. For \\(c = 2\\), we\nobtain \\(\\rho \\approx 0.302 + o(1)\\) for hypercube LSH, improving upon the \\(\\rho\n\\approx 0.377\\) for hyperplane LSH. We further describe how to use hypercube LSH\nin practice, and we consider an example application in the area of lattice\nalgorithms.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [23.16172981262207, 51.34019088745117], "cluster": 4}, {"key": "lahajal2024enhancing", "year": "2024", "citations": "0", "title": "Enhancing Image Retrieval : A Comprehensive Study On Photo Search Using The CLIP Mode", "abstract": "<p>Photo search, the task of retrieving images based on textual queries, has\nwitnessed significant advancements with the introduction of CLIP (Contrastive\nLanguage-Image Pretraining) model. CLIP leverages a vision-language pre\ntraining approach, wherein it learns a shared representation space for images\nand text, enabling cross-modal understanding. This model demonstrates the\ncapability to understand the semantic relationships between diverse image and\ntext pairs, allowing for efficient and accurate retrieval of images based on\nnatural language queries. By training on a large-scale dataset containing\nimages and their associated textual descriptions, CLIP achieves remarkable\ngeneralization, providing a powerful tool for tasks such as zero-shot learning\nand few-shot classification. This abstract summarizes the foundational\nprinciples of CLIP and highlights its potential impact on advancing the field\nof photo search, fostering a seamless integration of natural language\nunderstanding and computer vision for improved information retrieval in\nmultimedia applications</p>\n", "tags": ["Image-Retrieval", "Few-Shot-&-Zero-Shot", "Scalability", "Datasets"], "tsne_embedding": [-31.24212646484375, -31.560277938842773], "cluster": 5}, {"key": "lai2015simultaneous", "year": "2015", "citations": "916", "title": "Simultaneous Feature Learning And Hash Coding With Deep Neural Networks", "abstract": "<p>Similarity-preserving hashing is a widely-used method\nfor nearest neighbour search in large-scale image retrieval\ntasks. For most existing hashing methods, an image is\nfirst encoded as a vector of hand-engineering visual features,\nfollowed by another separate projection or quantization\nstep that generates binary codes. However, such visual\nfeature vectors may not be optimally compatible with the\ncoding process, thus producing sub-optimal hashing codes.\nIn this paper, we propose a deep architecture for supervised\nhashing, in which images are mapped into binary codes via\ncarefully designed deep neural networks. The pipeline of\nthe proposed deep architecture consists of three building\nblocks: 1) a sub-network with a stack of convolution layers\nto produce the effective intermediate image features; 2)\na divide-and-encode module to divide the intermediate image\nfeatures into multiple branches, each encoded into one\nhash bit; and 3) a triplet ranking loss designed to characterize\nthat one image is more similar to the second image than\nto the third one. Extensive evaluations on several benchmark\nimage datasets show that the proposed simultaneous\nfeature learning and hash coding pipeline brings substantial\nimprovements over other state-of-the-art supervised or\nunsupervised hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "CVPR", "Image-Retrieval", "Similarity-Search", "Scalability", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-8.886944770812988, 3.897609233856201], "cluster": 1}, {"key": "lai2016instance", "year": "2016", "citations": "78", "title": "Instance-aware Hashing For Multi-label Image Retrieval", "abstract": "<p>Similarity-preserving hashing is a commonly used method for nearest neighbour\nsearch in large-scale image retrieval. For image retrieval, deep-networks-based\nhashing methods are appealing since they can simultaneously learn effective\nimage representations and compact hash codes. This paper focuses on\ndeep-networks-based hashing for multi-label images, each of which may contain\nobjects of multiple categories. In most existing hashing methods, each image is\nrepresented by one piece of hash code, which is referred to as semantic\nhashing. This setting may be suboptimal for multi-label image retrieval. To\nsolve this problem, we propose a deep architecture that learns\n\\textbf{instance-aware} image representations for multi-label image data, which\nare organized in multiple groups, with each group containing the features for\none category. The instance-aware representations not only bring advantages to\nsemantic hashing, but also can be used in category-aware hashing, in which an\nimage is represented by multiple pieces of hash codes and each piece of code\ncorresponds to a category. Extensive evaluations conducted on several benchmark\ndatasets demonstrate that, for both semantic hashing and category-aware\nhashing, the proposed method shows substantial improvement over the\nstate-of-the-art supervised and unsupervised hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Text-Retrieval", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-8.770888328552246, 5.010998725891113], "cluster": 1}, {"key": "lai2017improved", "year": "2018", "citations": "12", "title": "Improved Search In Hamming Space Using Deep Multi-index Hashing", "abstract": "<p>Similarity-preserving hashing is a widely-used method for nearest neighbour\nsearch in large-scale image retrieval tasks. There has been considerable\nresearch on generating efficient image representation via the\ndeep-network-based hashing methods. However, the issue of efficient searching\nin the deep representation space remains largely unsolved. To this end, we\npropose a simple yet efficient deep-network-based multi-index hashing method\nfor simultaneously learning the powerful image representation and the efficient\nsearching. To achieve these two goals, we introduce the multi-index hashing\n(MIH) mechanism into the proposed deep architecture, which divides the binary\ncodes into multiple substrings. Due to the non-uniformly distributed codes will\nresult in inefficiency searching, we add the two balanced constraints at\nfeature-level and instance-level, respectively. Extensive evaluations on\nseveral benchmark image retrieval datasets show that the learned balanced\nbinary codes bring dramatic speedups and achieve comparable performance over\nthe existing baselines.</p>\n", "tags": ["Hashing-Methods", "Vector-Indexing", "Image-Retrieval", "Scalability", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-15.823418617248535, 7.006921291351318], "cluster": 1}, {"key": "lai2017transductive", "year": "2018", "citations": "9", "title": "Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining", "abstract": "<p>Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes\nwithout training data, which is an important and challenging problem. Most\nexisting ZSH approaches exploit transfer learning via an intermediate shared\nsemantic representations between the seen/source classes and novel/target\nclasses. However, due to having disjoint, the hash functions learned from the\nsource dataset are biased when applied directly to the target classes. In this\npaper, we study the transductive ZSH, i.e., we have unlabeled data for novel\nclasses. We put forward a simple yet efficient joint learning approach via\ncoarse-to-fine similarity mining which transfers knowledges from source data to\ntarget data. It mainly consists of two building blocks in the proposed deep\narchitecture: 1) a shared two-streams network, which the first stream operates\non the source data and the second stream operates on the unlabeled data, to\nlearn the effective common image representations, and 2) a coarse-to-fine\nmodule, which begins with finding the most representative images from target\nclasses and then further detect similarities among these images, to transfer\nthe similarities of the source data to the target data in a greedy fashion.\nExtensive evaluation results on several benchmark datasets demonstrate that the\nproposed hashing method achieves significant improvement over the\nstate-of-the-art methods.</p>\n", "tags": ["Datasets", "Evaluation", "Hashing-Methods", "Multimodal-Retrieval", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [1.4393985271453857, 9.870240211486816], "cluster": 6}, {"key": "lai2025simultaneous", "year": "2015", "citations": "916", "title": "Simultaneous Feature Learning And Hash Coding With Deep Neural Networks", "abstract": "<p>Similarity-preserving hashing is a widely-used method\nfor nearest neighbour search in large-scale image retrieval\ntasks. For most existing hashing methods, an image is\nfirst encoded as a vector of hand-engineering visual features,\nfollowed by another separate projection or quantization\nstep that generates binary codes. However, such visual\nfeature vectors may not be optimally compatible with the\ncoding process, thus producing sub-optimal hashing codes.\nIn this paper, we propose a deep architecture for supervised\nhashing, in which images are mapped into binary codes via\ncarefully designed deep neural networks. The pipeline of\nthe proposed deep architecture consists of three building\nblocks: 1) a sub-network with a stack of convolution layers\nto produce the effective intermediate image features; 2)\na divide-and-encode module to divide the intermediate image\nfeatures into multiple branches, each encoded into one\nhash bit; and 3) a triplet ranking loss designed to characterize\nthat one image is more similar to the second image than\nto the third one. Extensive evaluations on several benchmark\nimage datasets show that the proposed simultaneous\nfeature learning and hash coding pipeline brings substantial\nimprovements over other state-of-the-art supervised or\nunsupervised hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "CVPR", "Image-Retrieval", "Similarity-Search", "Scalability", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-8.887078285217285, 3.8976142406463623], "cluster": 1}, {"key": "lakshman2021embracing", "year": "2021", "citations": "1", "title": "Embracing Structure In Data For Billion-scale Semantic Product Search", "abstract": "<p>We present principled approaches to train and deploy dyadic neural embedding\nmodels at the billion scale, focusing our investigation on the application of\nsemantic product search. When training a dyadic model, one seeks to embed two\ndifferent types of entities (e.g., queries and documents or users and movies)\nin a common vector space such that pairs with high relevance are positioned\nnearby. During inference, given an embedding of one type (e.g., a query or a\nuser), one seeks to retrieve the entities of the other type (e.g., documents or\nmovies, respectively) that are highly relevant. In this work, we show that\nexploiting the natural structure of real-world datasets helps address both\nchallenges efficiently. Specifically, we model dyadic data as a bipartite graph\nwith edges between pairs with positive associations. We then propose to\npartition this network into semantically coherent clusters and thus reduce our\nsearch space by focusing on a small subset of these partitions for a given\ninput. During training, this technique enables us to efficiently mine hard\nnegative examples while, at inference, we can quickly find the nearest\nneighbors for a given embedding. We provide offline experimental results that\ndemonstrate the efficacy of our techniques for both training and inference on a\nbillion-scale Amazon.com product search dataset.</p>\n", "tags": ["Large-Scale-Search", "Scalability", "Datasets"], "tsne_embedding": [52.955814361572266, -4.1980767250061035], "cluster": 9}, {"key": "lam2018word2bits", "year": "2018", "citations": "27", "title": "Word2bits - Quantized Word Vectors", "abstract": "<p>Word vectors require significant amounts of memory and storage, posing issues\nto resource limited devices like mobile phones and GPUs. We show that high\nquality quantized word vectors using 1-2 bits per parameter can be learned by\nintroducing a quantization function into Word2Vec. We furthermore show that\ntraining with the quantization function acts as a regularizer. We train word\nvectors on English Wikipedia (2017) and evaluate them on standard word\nsimilarity and analogy tasks and on question answering (SQuAD). Our quantized\nword vectors not only take 8-16x less space than full precision (32 bit) word\nvectors but also outperform them on word similarity tasks and question\nanswering.</p>\n", "tags": ["Quantization", "Evaluation"], "tsne_embedding": [12.120330810546875, -18.23386573791504], "cluster": 7}, {"key": "lanzend\u00f6rfer2024audio", "year": "2024", "citations": "0", "title": "Audio Atlas: Visualizing And Exploring Audio Datasets", "abstract": "<p>We introduce Audio Atlas, an interactive web application for visualizing\naudio data using text-audio embeddings. Audio Atlas is designed to facilitate\nthe exploration and analysis of audio datasets using a contrastive embedding\nmodel and a vector database for efficient data management and semantic search.\nThe system maps audio embeddings into a two-dimensional space and leverages\nDeepScatter for dynamic visualization. Designed for extensibility, Audio Atlas\nallows easy integration of new datasets, enabling users to better understand\ntheir audio data and identify both patterns and outliers. We open-source the\ncodebase of Audio Atlas, and provide an initial implementation containing\nvarious audio and music datasets.</p>\n", "tags": ["Datasets"], "tsne_embedding": [9.224695205688477, -47.673065185546875], "cluster": 3}, {"key": "laskar2019geometric", "year": "2020", "citations": "12", "title": "Geometric Image Correspondence Verification By Dense Pixel Matching", "abstract": "<p>This paper addresses the problem of determining dense pixel correspondences\nbetween two images and its application to geometric correspondence verification\nin image retrieval. The main contribution is a geometric correspondence\nverification approach for re-ranking a shortlist of retrieved database images\nbased on their dense pair-wise matching with the query image at a pixel level.\nWe determine a set of cyclically consistent dense pixel matches between the\npair of images and evaluate local similarity of matched pixels using neural\nnetwork based image descriptors. Final re-ranking is based on a novel\nsimilarity function, which fuses the local similarity metric with a global\nsimilarity metric and a geometric consistency measure computed for the matched\npixels. For dense matching our approach utilizes a modified version of a\nrecently proposed dense geometric correspondence network (DGC-Net), which we\nalso improve by optimizing the architecture. The proposed model and similarity\nmetric compare favourably to the state-of-the-art image retrieval methods. In\naddition, we apply our method to the problem of long-term visual localization\ndemonstrating promising results and generalization across datasets.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [-13.164925575256348, -5.274209022521973], "cluster": 1}, {"key": "lassance2022composite", "year": "2021", "citations": "5", "title": "Composite Code Sparse Autoencoders For First Stage Retrieval", "abstract": "<p>We propose a Composite Code Sparse Autoencoder (CCSA) approach for\nApproximate Nearest Neighbor (ANN) search of document representations based on\nSiamese-BERT models. In Information Retrieval (IR), the ranking pipeline is\ngenerally decomposed in two stages: the first stage focus on retrieving a\ncandidate set from the whole collection. The second stage re-ranks the\ncandidate set by relying on more complex models. Recently, Siamese-BERT models\nhave been used as first stage ranker to replace or complement the traditional\nbag-of-word models. However, indexing and searching a large document collection\nrequire efficient similarity search on dense vectors and this is why ANN\ntechniques come into play. Since composite codes are naturally sparse, we first\nshow how CCSA can learn efficient parallel inverted index thanks to an\nuniformity regularizer. Second, CCSA can be used as a binary quantization\nmethod and we propose to combine it with the recent graph based ANN techniques.\nOur experiments on MSMARCO dataset reveal that CCSA outperforms IVF with\nproduct quantization. Furthermore, CCSA binary quantization is beneficial for\nthe index size, and memory usage for the graph-based HNSW method, while\nmaintaining a good level of recall and MRR. Third, we compare with recent\nsupervised quantization methods for image retrieval and find that CCSA is able\nto outperform them.</p>\n", "tags": ["Graph-Based-Ann", "Quantization", "Vector-Indexing", "Image-Retrieval", "Similarity-Search", "SIGIR", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [25.647716522216797, 18.233718872070312], "cluster": 2}, {"key": "lav2020proximity", "year": "2020", "citations": "0", "title": "Proximity Preserving Binary Code Using Signed Graph-cut", "abstract": "<p>We introduce a binary embedding framework, called Proximity Preserving Code\n(PPC), which learns similarity and dissimilarity between data points to create\na compact and affinity-preserving binary code. This code can be used to apply\nfast and memory-efficient approximation to nearest-neighbor searches. Our\nframework is flexible, enabling different proximity definitions between data\npoints. In contrast to previous methods that extract binary codes based on\nunsigned graph partitioning, our system models the attractive and repulsive\nforces in the data by incorporating positive and negative graph weights. The\nproposed framework is shown to boil down to finding the minimal cut of a signed\ngraph, a problem known to be NP-hard. We offer an efficient approximation and\nachieve superior results by constructing the code bit after bit. We show that\nthe proposed approximation is superior to the commonly used spectral methods\nwith respect to both accuracy and complexity. Thus, it is useful for many other\nproblems that can be translated into signed graph cut.</p>\n", "tags": ["Compact-Codes", "AAAI", "Tools-&-Libraries", "Hashing-Methods"], "tsne_embedding": [52.4786376953125, 4.8339667320251465], "cluster": 9}, {"key": "lawrie2024plaid", "year": "2024", "citations": "2", "title": "PLAID SHIRTTT For Large-scale Streaming Dense Retrieval", "abstract": "<p>PLAID, an efficient implementation of the ColBERT late interaction bi-encoder\nusing pretrained language models for ranking, consistently achieves\nstate-of-the-art performance in monolingual, cross-language, and multilingual\nretrieval. PLAID differs from ColBERT by assigning terms to clusters and\nrepresenting those terms as cluster centroids plus compressed residual vectors.\nWhile PLAID is effective in batch experiments, its performance degrades in\nstreaming settings where documents arrive over time because representations of\nnew tokens may be poorly modeled by the earlier tokens used to select cluster\ncentroids. PLAID Streaming Hierarchical Indexing that Runs on Terabytes of\nTemporal Text (PLAID SHIRTTT) addresses this concern using multi-phase\nincremental indexing based on hierarchical sharding. Experiments on ClueWeb09\nand the multilingual NeuCLIR collection demonstrate the effectiveness of this\napproach both for the largest collection indexed to date by the ColBERT\narchitecture and in the multilingual setting, respectively.</p>\n", "tags": ["SIGIR", "Evaluation", "Scalability"], "tsne_embedding": [13.769161224365234, -24.654687881469727], "cluster": 7}, {"key": "le2019btel", "year": "2019", "citations": "5", "title": "BTEL: A Binary Tree Encoding Approach For Visual Localization", "abstract": "<p>Visual localization algorithms have achieved significant improvements in\nperformance thanks to recent advances in camera technology and vision-based\ntechniques. However, there remains one critical caveat: all current approaches\nthat are based on image retrieval currently scale at best linearly with the\nsize of the environment with respect to both storage, and consequentially in\nmost approaches, query time. This limitation severely curtails the capability\nof autonomous systems in a wide range of compute, power, storage, size, weight\nor cost constrained applications such as drones. In this work, we present a\nnovel binary tree encoding approach for visual localization which can serve as\nan alternative for existing quantization and indexing techniques. The proposed\ntree structure allows us to derive a compressed training scheme that achieves\nsub-linearity in both required storage and inference time. The encoding memory\ncan be easily configured to satisfy different storage constraints. Moreover,\nour approach is amenable to an optional sequence filtering mechanism to further\nimprove the localization results, while maintaining the same amount of storage.\nOur system is entirely agnostic to the front-end descriptors, allowing it to be\nused on top of recent state-of-the-art image representations. Experimental\nresults show that the proposed method significantly outperforms\nstate-of-the-art approaches under limited storage constraints.</p>\n", "tags": ["Efficiency", "Quantization", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-26.05092430114746, -3.0577964782714844], "cluster": 1}, {"key": "le2020city", "year": "2020", "citations": "4", "title": "City-scale Visual Place Recognition With Deep Local Features Based On Multi-scale Ordered VLAD Pooling", "abstract": "<p>Visual place recognition is the task of recognizing a place depicted in an\nimage based on its pure visual appearance without metadata. In visual place\nrecognition, the challenges lie upon not only the changes in lighting\nconditions, camera viewpoint, and scale but also the characteristic of\nscene-level images and the distinct features of the area. To resolve these\nchallenges, one must consider both the local discriminativeness and the global\nsemantic context of images. On the other hand, the diversity of the datasets is\nalso particularly important to develop more general models and advance the\nprogress of the field. In this paper, we present a fully-automated system for\nplace recognition at a city-scale based on content-based image retrieval. Our\nmain contributions to the community lie in three aspects. Firstly, we take a\ncomprehensive analysis of visual place recognition and sketch out the unique\nchallenges of the task compared to general image retrieval tasks. Next, we\npropose yet a simple pooling approach on top of convolutional neural network\nactivations to embed the spatial information into the image representation\nvector. Finally, we introduce new datasets for place recognition, which are\nparticularly essential for application-based research. Furthermore, throughout\nextensive experiments, various issues in both image retrieval and place\nrecognition are analyzed and discussed to give some insights into improving the\nperformance of retrieval models in reality.\n  The dataset used in this paper can be found at\nhttps://github.com/canhld94/Daejeon520</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-8.278013229370117, -52.297096252441406], "cluster": 3}, {"key": "lebailly2023cribo", "year": "2023", "citations": "0", "title": "Cribo: Self-supervised Learning Via Cross-image Object-level Bootstrapping", "abstract": "<p>Leveraging nearest neighbor retrieval for self-supervised representation\nlearning has proven beneficial with object-centric images. However, this\napproach faces limitations when applied to scene-centric datasets, where\nmultiple objects within an image are only implicitly captured in the global\nrepresentation. Such global bootstrapping can lead to undesirable entanglement\nof object representations. Furthermore, even object-centric datasets stand to\nbenefit from a finer-grained bootstrapping approach. In response to these\nchallenges, we introduce a novel Cross-Image Object-Level Bootstrapping method\ntailored to enhance dense visual representation learning. By employing\nobject-level nearest neighbor bootstrapping throughout the training, CrIBo\nemerges as a notably strong and adequate candidate for in-context learning,\nleveraging nearest neighbor retrieval at test time. CrIBo shows\nstate-of-the-art performance on the latter task while being highly competitive\nin more standard downstream segmentation tasks. Our code and pretrained models\nare publicly available at https://github.com/tileb1/CrIBo.</p>\n", "tags": ["Supervised", "Self-Supervised", "Evaluation", "Datasets"], "tsne_embedding": [-22.366615295410156, -15.048959732055664], "cluster": 5}, {"key": "lee2018stacked", "year": "2018", "citations": "1151", "title": "Stacked Cross Attention For Image-text Matching", "abstract": "<p>In this paper, we study the problem of image-text matching. Inferring the\nlatent semantic alignment between objects or other salient stuff (e.g. snow,\nsky, lawn) and the corresponding words in sentences allows to capture\nfine-grained interplay between vision and language, and makes image-text\nmatching more interpretable. Prior work either simply aggregates the similarity\nof all possible pairs of regions and words without attending differentially to\nmore and less important words or regions, or uses a multi-step attentional\nprocess to capture limited number of semantic alignments which is less\ninterpretable. In this paper, we present Stacked Cross Attention to discover\nthe full latent alignments using both image regions and words in a sentence as\ncontext and infer image-text similarity. Our approach achieves the\nstate-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,\nour approach outperforms the current best methods by 22.1% relatively in text\nretrieval from image query, and 18.2% relatively in image retrieval with text\nquery (based on Recall@1). On MS-COCO, our approach improves sentence retrieval\nby 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1\nusing the 5K test set). Code has been made available at:\nhttps://github.com/kuanghuei/SCAN.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-33.087158203125, -23.37888526916504], "cluster": 5}, {"key": "lee2020metric", "year": "2020", "citations": "11", "title": "Metric Learning Vs Classification For Disentangled Music Representation Learning", "abstract": "<p>Deep representation learning offers a powerful paradigm for mapping input\ndata onto an organized embedding space and is useful for many music information\nretrieval tasks. Two central methods for representation learning include deep\nmetric learning and classification, both having the same goal of learning a\nrepresentation that can generalize well across tasks. Along with\ngeneralization, the emerging concept of disentangled representations is also of\ngreat interest, where multiple semantic concepts (e.g., genre, mood,\ninstrumentation) are learned jointly but remain separable in the learned\nrepresentation space. In this paper we present a single representation learning\nframework that elucidates the relationship between metric learning,\nclassification, and disentanglement in a holistic manner. For this, we (1)\noutline past work on the relationship between metric learning and\nclassification, (2) extend this relationship to multi-label data by exploring\nthree different learning approaches and their disentangled versions, and (3)\nevaluate all models on four tasks (training time, similarity retrieval,\nauto-tagging, and triplet prediction). We find that classification-based models\nare generally advantageous for training time, similarity retrieval, and\nauto-tagging, while deep metric learning exhibits better performance for\ntriplet-prediction. Finally, we show that our proposed approach yields\nstate-of-the-art results for music auto-tagging.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Distance-Metric-Learning", "Similarity-Search"], "tsne_embedding": [9.570880889892578, -44.79212951660156], "cluster": 3}, {"key": "lee2022correlation", "year": "2022", "citations": "59", "title": "Correlation Verification For Image Retrieval", "abstract": "<p>Geometric verification is considered a de facto solution for the re-ranking\ntask in image retrieval. In this study, we propose a novel image retrieval\nre-ranking network named Correlation Verification Networks (CVNet). Our\nproposed network, comprising deeply stacked 4D convolutional layers, gradually\ncompresses dense feature correlation into image similarity while learning\ndiverse geometric matching patterns from various image pairs. To enable\ncross-scale matching, it builds feature pyramids and constructs cross-scale\nfeature correlations within a single inference, replacing costly multi-scale\ninferences. In addition, we use curriculum learning with the hard negative\nmining and Hide-and-Seek strategy to handle hard samples without losing\ngenerality. Our proposed re-ranking network shows state-of-the-art performance\non several retrieval benchmarks with a significant margin (+12.6% in mAP on\nROxford-Hard+1M set) over state-of-the-art methods. The source code and models\nare available online: https://github.com/sungonce/CVNet.</p>\n", "tags": ["CVPR", "Image-Retrieval", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-18.381593704223633, -18.51511573791504], "cluster": 1}, {"key": "lee2022generative", "year": "2022", "citations": "6", "title": "Generative Multi-hop Retrieval", "abstract": "<p>A common practice for text retrieval is to use an encoder to map the\ndocuments and the query to a common vector space and perform a nearest neighbor\nsearch (NNS); multi-hop retrieval also often adopts the same paradigm, usually\nwith a modification of iteratively reformulating the query vector so that it\ncan retrieve different documents at each hop. However, such a bi-encoder\napproach has limitations in multi-hop settings; (1) the reformulated query gets\nlonger as the number of hops increases, which further tightens the embedding\nbottleneck of the query vector, and (2) it is prone to error propagation. In\nthis paper, we focus on alleviating these limitations in multi-hop settings by\nformulating the problem in a fully generative way. We propose an\nencoder-decoder model that performs multi-hop retrieval by simply generating\nthe entire text sequences of the retrieval targets, which means the query and\nthe documents interact in the language model\u2019s parametric space rather than L2\nor inner product space as in the bi-encoder approach. Our approach, Generative\nMulti-hop Retrieval(GMR), consistently achieves comparable or higher\nperformance than bi-encoder models in five datasets while demonstrating\nsuperior GPU memory and storage footprint.</p>\n", "tags": ["Evaluation", "EMNLP", "Text-Retrieval", "Datasets"], "tsne_embedding": [18.084199905395508, 19.097209930419922], "cluster": 2}, {"key": "lee2022set2box", "year": "2022", "citations": "1", "title": "Set2box: Similarity Preserving Representation Learning Of Sets", "abstract": "<p>Sets have been used for modeling various types of objects (e.g., a document\nas the set of keywords in it and a customer as the set of the items that she\nhas purchased). Measuring similarity (e.g., Jaccard Index) between sets has\nbeen a key building block of a wide range of applications, including,\nplagiarism detection, recommendation, and graph compression. However, as sets\nhave grown in numbers and sizes, the computational cost and storage required\nfor set similarity computation have become substantial, and this has led to the\ndevelopment of hashing and sketching based solutions. In this work, we propose\nSet2Box, a learning-based approach for compressed representations of sets from\nwhich various similarity measures can be estimated accurately in constant time.\nThe key idea is to represent sets as boxes to precisely capture overlaps of\nsets. Additionally, based on the proposed box quantization scheme, we design\nSet2Box+, which yields more concise but more accurate box representations of\nsets. Through extensive experiments on 8 real-world datasets, we show that,\ncompared to baseline approaches, Set2Box+ is (a) Accurate: achieving up to\n40.8X smaller estimation error while requiring 60% fewer bits to encode sets,\n(b) Concise: yielding up to 96.8X more concise representations with similar\nestimation error, and (c) Versatile: enabling the estimation of four\nset-similarity measures from a single representation of each set.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Recommender-Systems", "Datasets", "Compact-Codes"], "tsne_embedding": [-3.4713408946990967, -12.277363777160645], "cluster": 1}, {"key": "lee2023vmcml", "year": "2024", "citations": "0", "title": "VMCML: Video And Music Matching Via Cross-modality Lifting", "abstract": "<p>We propose a content-based system for matching video and background music.\nThe system aims to address the challenges in music recommendation for new users\nor new music give short-form videos. To this end, we propose a cross-modal\nframework VMCML that finds a shared embedding space between video and music\nrepresentations. To ensure the embedding space can be effectively shared by\nboth representations, we leverage CosFace loss based on margin-based cosine\nsimilarity loss. Furthermore, we establish a large-scale dataset called MSVD,\nin which we provide 390 individual music and the corresponding matched 150,000\nvideos. We conduct extensive experiments on Youtube-8M and our MSVD datasets.\nOur quantitative and qualitative results demonstrate the effectiveness of our\nproposed framework and achieve state-of-the-art video and music matching\nperformance.</p>\n", "tags": ["CVPR", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [2.629648208618164, -51.19767379760742], "cluster": 3}, {"key": "lee2024vmcml", "year": "2024", "citations": "0", "title": "VMCML: Video And Music Matching Via Cross-modality Lifting", "abstract": "<p>We propose a content-based system for matching video and background music.\nThe system aims to address the challenges in music recommendation for new users\nor new music give short-form videos. To this end, we propose a cross-modal\nframework VMCML that finds a shared embedding space between video and music\nrepresentations. To ensure the embedding space can be effectively shared by\nboth representations, we leverage CosFace loss based on margin-based cosine\nsimilarity loss. Furthermore, we establish a large-scale dataset called MSVD,\nin which we provide 390 individual music and the corresponding matched 150,000\nvideos. We conduct extensive experiments on Youtube-8M and our MSVD datasets.\nOur quantitative and qualitative results demonstrate the effectiveness of our\nproposed framework and achieve state-of-the-art video and music matching\nperformance.</p>\n", "tags": ["CVPR", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [2.629648208618164, -51.19767379760742], "cluster": 3}, {"key": "lei2019semi", "year": "2019", "citations": "49", "title": "Semi-heterogeneous Three-way Joint Embedding Network For Sketch-based Image Retrieval", "abstract": "<p>Sketch-based image retrieval (SBIR) is a challenging task due to the large\ncross-domain gap between sketches and natural images. How to align abstract\nsketches and natural images into a common high-level semantic space remains a\nkey problem in SBIR. In this paper, we propose a novel semi-heterogeneous\nthree-way joint embedding network (Semi3-Net), which integrates three branches\n(a sketch branch, a natural image branch, and an edgemap branch) to learn more\ndiscriminative cross-domain feature representations for the SBIR task. The key\ninsight lies with how we cultivate the mutual and subtle relationships amongst\nthe sketches, natural images, and edgemaps. A semi-heterogeneous feature\nmapping is designed to extract bottom features from each domain, where the\nsketch and edgemap branches are shared while the natural image branch is\nheterogeneous to the other branches. In addition, a joint semantic embedding is\nintroduced to embed the features from different domains into a common\nhigh-level semantic space, where all of the three branches are shared. To\nfurther capture informative features common to both natural images and the\ncorresponding edgemaps, a co-attention model is introduced to conduct common\nchannel-wise feature recalibration between different domains. A hybrid-loss\nmechanism is designed to align the three branches, where an alignment loss and\na sketch-edgemap contrastive loss are presented to encourage the network to\nlearn invariant cross-domain representations. Experimental results on two\nwidely used category-level datasets (Sketchy and TU-Berlin Extension)\ndemonstrate that the proposed method outperforms state-of-the-art methods.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-43.39677429199219, -14.855524063110352], "cluster": 5}, {"key": "lei2020locality", "year": "2020", "citations": "24", "title": "Locality-sensitive Hashing Scheme Based On Longest Circular Co-substring", "abstract": "<p>Locality-Sensitive Hashing (LSH) is one of the most popular methods for\n\\(c\\)-Approximate Nearest Neighbor Search (\\(c\\)-ANNS) in high-dimensional spaces.\nIn this paper, we propose a novel LSH scheme based on the Longest Circular\nCo-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.\nWe introduce a novel concept of LCCS and a new data structure named Circular\nShift Array (CSA) for \\(k\\)-LCCS search. The insight of LCCS search framework is\nthat close data objects will have a longer LCCS than the far-apart ones with\nhigh probability. LCCS-LSH is <em>LSH-family-independent</em>, and it supports\n\\(c\\)-ANNS with different kinds of distance metrics. We also introduce a\nmulti-probe version of LCCS-LSH and conduct extensive experiments over five\nreal-life datasets. The experimental results demonstrate that LCCS-LSH\noutperforms state-of-the-art LSH schemes.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [16.45343780517578, 38.88136291503906], "cluster": 4}, {"key": "lei2022grm", "year": "2023", "citations": "0", "title": "GRM: Gradient Rectification Module For Visual Place Retrieval", "abstract": "<p>Visual place retrieval aims to search images in the database that depict\nsimilar places as the query image. However, global descriptors encoded by the\nnetwork usually fall into a low dimensional principal space, which is harmful\nto the retrieval performance. We first analyze the cause of this phenomenon,\npointing out that it is due to degraded distribution of the gradients of\ndescriptors. Then, we propose Gradient Rectification Module(GRM) to alleviate\nthis issue. GRM is appended after the final pooling layer and can rectify\ngradients to the complementary space of the principal space. With GRM, the\nnetwork is encouraged to generate descriptors more uniformly in the whole\nspace. At last, we conduct experiments on multiple datasets and generalize our\nmethod to classification task under prototype learning framework.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [-28.07386016845703, -2.3279874324798584], "cluster": 1}, {"key": "lei2022loopitr", "year": "2022", "citations": "6", "title": "Loopitr: Combining Dual And Cross Encoder Architectures For Image-text Retrieval", "abstract": "<p>Dual encoders and cross encoders have been widely used for image-text\nretrieval. Between the two, the dual encoder encodes the image and text\nindependently followed by a dot product, while the cross encoder jointly feeds\nimage and text as the input and performs dense multi-modal fusion. These two\narchitectures are typically modeled separately without interaction. In this\nwork, we propose LoopITR, which combines them in the same network for joint\nlearning. Specifically, we let the dual encoder provide hard negatives to the\ncross encoder, and use the more discriminative cross encoder to distill its\npredictions back to the dual encoder. Both steps are efficiently performed\ntogether in the same model. Our work centers on empirical analyses of this\ncombined architecture, putting the main focus on the design of the distillation\nobjective. Our experimental results highlight the benefits of training the two\nencoders in the same network, and demonstrate that distillation can be quite\neffective with just a few hard negative examples. Experiments on two standard\ndatasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual\nencoder performance when compared with approaches using a similar amount of\ndata.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-21.01875114440918, -23.11212158203125], "cluster": 5}, {"key": "lei2025enhancing", "year": "2025", "citations": "0", "title": "Enhancing Lexicon-based Text Embeddings With Large Language Models", "abstract": "<p>Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).</p>\n", "tags": ["Evaluation"], "tsne_embedding": [11.516834259033203, -22.990793228149414], "cluster": 7}, {"key": "lejeune2019adaptive", "year": "2019", "citations": "8", "title": "Adaptive Estimation For Approximate K-nearest-neighbor Computations", "abstract": "<p>Algorithms often carry out equally many computations for \u201ceasy\u201d and \u201chard\u201d\nproblem instances. In particular, algorithms for finding nearest neighbors\ntypically have the same running time regardless of the particular problem\ninstance. In this paper, we consider the approximate k-nearest-neighbor\nproblem, which is the problem of finding a subset of O(k) points in a given set\nof points that contains the set of k nearest neighbors of a given query point.\nWe propose an algorithm based on adaptively estimating the distances, and show\nthat it is essentially optimal out of algorithms that are only allowed to\nadaptively estimate distances. We then demonstrate both theoretically and\nexperimentally that the algorithm can achieve significant speedups relative to\nthe naive method.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [34.61874771118164, 29.579505920410156], "cluster": 2}, {"key": "leng2015hashing", "year": "2015", "citations": "30", "title": "Hashing For Distributed Data", "abstract": "<p>Recently, hashing based approximate nearest\nneighbors search has attracted much attention.\nExtensive centralized hashing algorithms have\nbeen proposed and achieved promising performance. However, due to the large scale of many\napplications, the data is often stored or even collected in a distributed manner. Learning hash\nfunctions by aggregating all the data into a fusion\ncenter is infeasible because of the prohibitively\nexpensive communication and computation overhead.\nIn this paper, we develop a novel hashing\nmodel to learn hash functions in a distributed setting. We cast a centralized hashing model as a\nset of subproblems with consensus constraints.\nWe find these subproblems can be analytically\nsolved in parallel on the distributed compute nodes. Since no training data is transmitted across\nthe nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large\nscale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [33.93101119995117, -1.7163738012313843], "cluster": 9}, {"key": "leng2025hashing", "year": "2015", "citations": "30", "title": "Hashing For Distributed Data", "abstract": "<p>Recently, hashing based approximate nearest\nneighbors search has attracted much attention.\nExtensive centralized hashing algorithms have\nbeen proposed and achieved promising performance. However, due to the large scale of many\napplications, the data is often stored or even collected in a distributed manner. Learning hash\nfunctions by aggregating all the data into a fusion\ncenter is infeasible because of the prohibitively\nexpensive communication and computation overhead.\nIn this paper, we develop a novel hashing\nmodel to learn hash functions in a distributed setting. We cast a centralized hashing model as a\nset of subproblems with consensus constraints.\nWe find these subproblems can be analytically\nsolved in parallel on the distributed compute nodes. Since no training data is transmitted across\nthe nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large\nscale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [33.93101119995117, -1.7163738012313843], "cluster": 9}, {"key": "leonhardt2021efficient", "year": "2022", "citations": "12", "title": "Efficient Neural Ranking Using Forward Indexes", "abstract": "<p>Neural document ranking approaches, specifically transformer models, have\nachieved impressive gains in ranking performance. However, query processing\nusing such over-parameterized models is both resource and time intensive. In\nthis paper, we propose the Fast-Forward index \u2013 a simple vector forward index\nthat facilitates ranking documents using interpolation of lexical and semantic\nscores \u2013 as a replacement for contextual re-rankers and dense indexes based on\nnearest neighbor search. Fast-Forward indexes rely on efficient sparse models\nfor retrieval and merely look up pre-computed dense transformer-based vector\nrepresentations of documents and passages in constant time for fast CPU-based\nsemantic similarity computation during query processing. We propose index\npruning and theoretically grounded early stopping techniques to improve the\nquery processing throughput. We conduct extensive large-scale experiments on\nTREC-DL datasets and show improvements over hybrid indexes in performance and\nquery processing efficiency using only CPUs. Fast-Forward indexes can provide\nsuperior ranking performance using interpolation due to the complementary\nbenefits of lexical and semantic similarities.</p>\n", "tags": ["Efficiency", "Scalability", "Datasets", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [33.43181228637695, 12.512582778930664], "cluster": 2}, {"key": "leroy2022how", "year": "2022", "citations": "0", "title": "How Does The Degree Of Novelty Impacts Semi-supervised Representation Learning For Novel Class Retrieval?", "abstract": "<p>Supervised representation learning with deep networks tends to overfit the\ntraining classes and the generalization to novel classes is a challenging\nquestion. It is common to evaluate a learned embedding on held-out images of\nthe same training classes. In real applications however, data comes from new\nsources and novel classes are likely to arise. We hypothesize that\nincorporating unlabelled images of novel classes in the training set in a\nsemi-supervised fashion would be beneficial for the efficient retrieval of\nnovel-class images compared to a vanilla supervised representation. To verify\nthis hypothesis in a comprehensive way, we propose an original evaluation\nmethodology that varies the degree of novelty of novel classes by partitioning\nthe dataset category-wise either randomly, or semantically, i.e. by minimizing\nthe shared semantics between base and novel classes. This evaluation procedure\nallows to train a representation blindly to any novel-class labels and evaluate\nthe frozen representation on the retrieval of base or novel classes. We find\nthat a vanilla supervised representation falls short on the retrieval of novel\nclasses even more so when the semantics gap is higher. Semi-supervised\nalgorithms allow to partially bridge this performance gap but there is still\nmuch room for improvement.</p>\n", "tags": ["Supervised", "Evaluation", "Similarity-Search", "Datasets"], "tsne_embedding": [11.713420867919922, -0.7555136680603027], "cluster": 6}, {"key": "leung2020using", "year": "2020", "citations": "3", "title": "Using Affective Features From Media Content Metadata For Better Movie Recommendations", "abstract": "<p>This paper investigates the causality in the decision making of movie\nrecommendations through the users\u2019 affective profiles. We advocate a method of\nassigning emotional tags to a movie by the auto-detection of the affective\nfeatures in the movie\u2019s overview. We apply a text-based Emotion Detection and\nRecognition model, which trained by tweets short messages and transfers the\nlearned model to detect movie overviews\u2019 implicit affective features. We\nvectorized the affective movie tags to represent the mood embeddings of the\nmovie. We obtain the user\u2019s emotional features by taking the average of all the\nmovies\u2019 affective vectors the user has watched. We apply five-distance metrics\nto rank the Top-N movie recommendations against the user\u2019s emotion profile. We\nfound Cosine Similarity distance metrics performed better than other distance\nmetrics measures. We conclude that by replacing the top-N recommendations\ngenerated by the Recommender with the reranked recommendations list made by the\nCosine Similarity distance metrics, the user will effectively get affective\naware top-N recommendations while making the Recommender feels like an Emotion\nAware Recommender.</p>\n", "tags": ["Survey-Paper", "Recommender-Systems", "Distance-Metric-Learning"], "tsne_embedding": [-43.54888916015625, -29.282337188720703], "cluster": 5}, {"key": "leveni2025hashing", "year": "2023", "citations": "0", "title": "Hashing For Structure-based Anomaly Detection", "abstract": "<p>We focus on the problem of identifying samples in a set that do not conform to structured patterns represented by low-dimensional manifolds. An effective way to solve this problem is to embed data in a high dimensional space, called Preference Space, where anomalies can be identified as the most isolated points. In this work, we employ Locality Sensitive Hashing to avoid explicit computation of distances in high dimensions and thus improve Anomaly Detection efficiency. Specifically, we present an isolation-based anomaly detection technique designed to work in the Preference Space which achieves state-of-the-art performance at a lower computational cost. Code is publicly available at https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-27.35304832458496, 19.477645874023438], "cluster": 8}, {"key": "levi2020rethinking", "year": "2021", "citations": "15", "title": "Rethinking Preventing Class-collapsing In Metric Learning With Margin-based Losses", "abstract": "<p>Metric learning seeks perceptual embeddings where visually similar instances\nare close and dissimilar instances are apart, but learned representations can\nbe sub-optimal when the distribution of intra-class samples is diverse and\ndistinct sub-clusters are present. Although theoretically with optimal\nassumptions, margin-based losses such as the triplet loss and margin loss have\na diverse family of solutions. We theoretically prove and empirically show that\nunder reasonable noise assumptions, margin-based losses tend to project all\nsamples of a class with various modes onto a single point in the embedding\nspace, resulting in a class collapse that usually renders the space ill-sorted\nfor classification or retrieval. To address this problem, we propose a simple\nmodification to the embedding losses such that each sample selects its nearest\nsame-class counterpart in a batch as the positive element in the tuple. This\nallows for the presence of multiple sub-clusters within each class. The\nadaptation can be integrated into a wide range of metric learning losses. The\nproposed sampling method demonstrates clear benefits on various fine-grained\nimage retrieval datasets over a variety of existing losses; qualitative\nretrieval results show that samples with similar visual patterns are indeed\ncloser in the embedding space.</p>\n", "tags": ["ICCV", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-14.374551773071289, -17.971336364746094], "cluster": 1}, {"key": "levi2023object", "year": "2023", "citations": "0", "title": "Object-centric Open-vocabulary Image-retrieval With Aggregated Features", "abstract": "<p>The task of open-vocabulary object-centric image retrieval involves the\nretrieval of images containing a specified object of interest, delineated by an\nopen-set text query. As working on large image datasets becomes standard,\nsolving this task efficiently has gained significant practical importance.\nApplications include targeted performance analysis of retrieved images using\nad-hoc queries and hard example mining during training. Recent advancements in\ncontrastive-based open vocabulary systems have yielded remarkable\nbreakthroughs, facilitating large-scale open vocabulary image retrieval.\nHowever, these approaches use a single global embedding per image, thereby\nconstraining the system\u2019s ability to retrieve images containing relatively\nsmall object instances. Alternatively, incorporating local embeddings from\ndetection pipelines faces scalability challenges, making it unsuitable for\nretrieval from large databases.\n  In this work, we present a simple yet effective approach to object-centric\nopen-vocabulary image retrieval. Our approach aggregates dense embeddings\nextracted from CLIP into a compact representation, essentially combining the\nscalability of image retrieval pipelines with the object identification\ncapabilities of dense detection methods. We show the effectiveness of our\nscheme to the task by achieving significantly better results than global\nfeature approaches on three datasets, increasing accuracy by up to 15 mAP\npoints. We further integrate our scheme into a large scale retrieval framework\nand demonstrate our method\u2019s advantages in terms of scalability and\ninterpretability.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-24.990047454833984, 3.2616143226623535], "cluster": 1}, {"key": "leybovich2021efficient", "year": "2021", "citations": "2", "title": "Efficient Approximate Search For Sets Of Vectors", "abstract": "<p>We consider a similarity measure between two sets \\(A\\) and \\(B\\) of vectors,\nthat balances the average and maximum cosine distance between pairs of vectors,\none from set \\(A\\) and one from set \\(B\\). As a motivation for this measure, we\npresent lineage tracking in a database. To practically realize this measure, we\nneed an approximate search algorithm that given a set of vectors \\(A\\) and sets\nof vectors \\(B_1,\u2026,B_n\\), the algorithm quickly locates the set \\(B_i\\) that\nmaximizes the similarity measure. For the case where all sets are singleton\nsets, essentially each is a single vector, there are known efficient\napproximate search algorithms, e.g., approximated versions of tree search\nalgorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and\nproximity graph algorithms. In this work, we present approximate search\nalgorithms for the general case. The underlying idea in these algorithms is\nencoding a set of vectors via a \u201clong\u201d single vector. The proposed approximate\napproach achieves significant performance gains over an optimized, exact search\non vector sets.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization", "Evaluation"], "tsne_embedding": [23.585554122924805, 26.246742248535156], "cluster": 2}, {"key": "leyvavallina2021generalized", "year": "2021", "citations": "13", "title": "Generalized Contrastive Optimization Of Siamese Networks For Place Recognition", "abstract": "<p>Visual place recognition is a challenging task in computer vision and a key\ncomponent of camera-based localization and navigation systems. Recently,\nConvolutional Neural Networks (CNNs) achieved high results and good\ngeneralization capabilities. They are usually trained using pairs or triplets\nof images labeled as either similar or dissimilar, in a binary fashion. In\npractice, the similarity between two images is not binary, but continuous.\nFurthermore, training these CNNs is computationally complex and involves costly\npair and triplet mining strategies. We propose a Generalized Contrastive loss\n(GCL) function that relies on image similarity as a continuous measure, and use\nit to train a siamese CNN. Furthermore, we present three techniques for\nautomatic annotation of image pairs with labels indicating their degree of\nsimilarity, and deploy them to re-annotate the MSLS, TB-Places, and 7Scenes\ndatasets. We demonstrate that siamese CNNs trained using the GCL function and\nthe improved annotations consistently outperform their binary counterparts. Our\nmodels trained on MSLS outperform the state-of-the-art methods, including\nNetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the\nPittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons\ndatasets. Furthermore, training a siamese network using the GCL function does\nnot require complex pair mining. We release the source code at\nhttps://github.com/marialeyvallina/generalized_contrastive_loss.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-48.79222106933594, -4.902443885803223], "cluster": 0}, {"key": "leyvavallina2023data", "year": "2023", "citations": "24", "title": "Data-efficient Large Scale Place Recognition With Graded Similarity Supervision", "abstract": "<p>Visual place recognition (VPR) is a fundamental task of computer vision for\nvisual localization. Existing methods are trained using image pairs that either\ndepict the same place or not. Such a binary indication does not consider\ncontinuous relations of similarity between images of the same place taken from\ndifferent positions, determined by the continuous nature of camera pose. The\nbinary similarity induces a noisy supervision signal into the training of VPR\nmethods, which stall in local minima and require expensive hard mining\nalgorithms to guarantee convergence. Motivated by the fact that two images of\nthe same place only partially share visual cues due to camera pose differences,\nwe deploy an automatic re-annotation strategy to re-label VPR datasets. We\ncompute graded similarity labels for image pairs based on available\nlocalization metadata. Furthermore, we propose a new Generalized Contrastive\nLoss (GCL) that uses graded similarity labels for training contrastive\nnetworks. We demonstrate that the use of the new labels and GCL allow to\ndispense from hard-pair mining, and to train image descriptors that perform\nbetter in VPR by nearest neighbor search, obtaining superior or comparable\nresults than methods that require expensive hard-pair mining and re-ranking\ntechniques. Code and models available at:\nhttps://github.com/marialeyvallina/generalized_contrastive_loss</p>\n", "tags": ["Re-Ranking", "CVPR", "Hybrid-Ann-Methods", "Datasets"], "tsne_embedding": [-29.476062774658203, 8.749387741088867], "cluster": 0}, {"key": "leyvavallina2024regressing", "year": "2024", "citations": "0", "title": "Regressing Transformers For Data-efficient Visual Place Recognition", "abstract": "<p>Visual place recognition is a critical task in computer vision, especially\nfor localization and navigation systems. Existing methods often rely on\ncontrastive learning: image descriptors are trained to have small distance for\nsimilar images and larger distance for dissimilar ones in a latent space.\nHowever, this approach struggles to ensure accurate distance-based image\nsimilarity representation, particularly when training with binary pairwise\nlabels, and complex re-ranking strategies are required. This work introduces a\nfresh perspective by framing place recognition as a regression problem, using\ncamera field-of-view overlap as similarity ground truth for learning. By\noptimizing image descriptors to align directly with graded similarity labels,\nthis approach enhances ranking capabilities without expensive re-ranking,\noffering data-efficient training and strong generalization across several\nbenchmark datasets.</p>\n", "tags": ["Self-Supervised", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-18.391298294067383, -10.408092498779297], "cluster": 1}, {"key": "li2013learning", "year": "2013", "citations": "93", "title": "Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming\nan increasingly important tool in solving\nmany large-scale problems. Recently\na number of approaches to learning datadependent\nhash functions have been developed.\nIn this work, we propose a column\ngeneration based method for learning datadependent\nhash functions on the basis of\nproximity comparison information. Given a\nset of triplets that encode the pairwise proximity\ncomparison information, our method\nlearns hash functions that preserve the relative\ncomparison relationships in the data\nas well as possible within the large-margin\nlearning framework. The learning procedure\nis implemented using column generation and\nhence is named CGHash. At each iteration\nof the column generation procedure, the best\nhash function is selected. Unlike most other\nhashing methods, our method generalizes to\nnew data points naturally; and has a training\nobjective which is convex, thus ensuring\nthat the global optimum can be identi-\nfied. Experiments demonstrate that the proposed\nmethod learns compact binary codes\nand that its retrieval performance compares\nfavorably with state-of-the-art methods when\ntested on a few benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [25.14874267578125, -3.914707899093628], "cluster": 6}, {"key": "li2015feature", "year": "2015", "citations": "510", "title": "Feature Learning Based Deep Supervised Hashing With Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of\nhashing for large-scale image retrieval. However,\nmost existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep\nhashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown\nbetter performance than traditional hashing methods with hand-crafted features. Most of these deep\nhashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this\npaper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH),\nto perform simultaneous feature learning and hashcode learning for applications with pairwise labels.\nExperiments on real datasets show that our DPSH\nmethod can outperform other methods to achieve\nthe state-of-the-art performance in image retrieval\napplications.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-4.269711494445801, -5.191885948181152], "cluster": 1}, {"key": "li2017deep", "year": "2017", "citations": "171", "title": "Deep Unsupervised Image Hashing By Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy, we do not add a term to the loss function as this is difficult to optimize and tune. Instead, we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Image-Retrieval", "Datasets", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-14.961277961730957, 34.775508880615234], "cluster": 8}, {"key": "li2017fast", "year": "2017", "citations": "14", "title": "Fast K-nearest Neighbour Search Via Prioritized DCI", "abstract": "<p>Most exact methods for k-nearest neighbour search suffer from the curse of\ndimensionality; that is, their query times exhibit exponential dependence on\neither the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing\n(DCI) offers a promising way of circumventing the curse and successfully\nreduces the dependence of query time on intrinsic dimensionality from\nexponential to sublinear. In this paper, we propose a variant of DCI, which we\ncall Prioritized DCI, and show a remarkable improvement in the dependence of\nquery time on intrinsic dimensionality. In particular, a linear increase in\nintrinsic dimensionality, or equivalently, an exponential increase in the\nnumber of points near a query, can be mostly counteracted with just a linear\nincrease in space. We also demonstrate empirically that Prioritized DCI\nsignificantly outperforms prior methods. In particular, relative to\nLocality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of\ndistance evaluations by a factor of 14 to 116 and the memory consumption by a\nfactor of 21.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search"], "tsne_embedding": [13.684488296508789, 34.13287353515625], "cluster": 4}, {"key": "li2017image", "year": "2018", "citations": "16", "title": "Image Super-resolution Via Feature-augmented Random Forest", "abstract": "<p>Recent random-forest (RF)-based image super-resolution approaches inherit\nsome properties from dictionary-learning-based algorithms, but the\neffectiveness of the properties in RF is overlooked in the literature. In this\npaper, we present a novel feature-augmented random forest (FARF) for image\nsuper-resolution, where the conventional gradient-based features are augmented\nwith gradient magnitudes and different feature recipes are formulated on\ndifferent stages in an RF. The advantages of our method are that, firstly, the\ndictionary-learning-based features are enhanced by adding gradient magnitudes,\nbased on the observation that the non-linear gradient magnitude are with highly\ndiscriminative property. Secondly, generalized locality-sensitive hashing (LSH)\nis used to replace principal component analysis (PCA) for feature\ndimensionality reduction and original high-dimensional features are employed,\ninstead of the compressed ones, for the leaf-nodes\u2019 regressors, since\nregressors can benefit from higher dimensional features. This\noriginal-compressed coupled feature sets scheme unifies the unsupervised LSH\nevaluation on both image super-resolution and content-based image retrieval\n(CBIR). Finally, we present a generalized weighted ridge regression (GWRR)\nmodel for the leaf-nodes\u2019 regressors. Experiment results on several public\nbenchmark datasets show that our FARF method can achieve an average gain of\nabout 0.3 dB, compared to traditional RF-based methods. Furthermore, a\nfine-tuned FARF model can compare to or (in many cases) outperform some recent\nstateof-the-art deep-learning-based algorithms.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Image-Retrieval", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [15.56226634979248, 19.176652908325195], "cluster": 2}, {"key": "li2017ranking", "year": "2017", "citations": "2", "title": "Ranking With Adaptive Neighbors", "abstract": "<p>Retrieving the most similar objects in a large-scale database for a given\nquery is a fundamental building block in many application domains, ranging from\nweb searches, visual, cross media, and document retrievals. State-of-the-art\napproaches have mainly focused on capturing the underlying geometry of the data\nmanifolds. Graph-based approaches, in particular, define various diffusion\nprocesses on weighted data graphs. Despite success, these approaches rely on\nfixed-weight graphs, making ranking sensitive to the input affinity matrix. In\nthis study, we propose a new ranking algorithm that simultaneously learns the\ndata affinity matrix and the ranking scores. The proposed optimization\nformulation assigns adaptive neighbors to each point in the data based on the\nlocal connectivity, and the smoothness constraint assigns similar ranking\nscores to similar data points. We develop a novel and efficient algorithm to\nsolve the optimization problem. Evaluations using synthetic and real datasets\nsuggest that the proposed algorithm can outperform the existing methods.</p>\n", "tags": ["Graph-Based-Ann", "Text-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [49.94192886352539, 5.569582462310791], "cluster": 9}, {"key": "li2018dual", "year": "2019", "citations": "16", "title": "Dual Asymmetric Deep Hashing Learning", "abstract": "<p>Due to the impressive learning power, deep learning has achieved a remarkable\nperformance in supervised hash function learning. In this paper, we propose a\nnovel asymmetric supervised deep hashing method to preserve the semantic\nstructure among different categories and generate the binary codes\nsimultaneously. Specifically, two asymmetric deep networks are constructed to\nreveal the similarity between each pair of images according to their semantic\nlabels. The deep hash functions are then learned through two networks by\nminimizing the gap between the learned features and discrete codes.\nFurthermore, since the binary codes in the Hamming space also should keep the\nsemantic affinity existing in the original space, another asymmetric pairwise\nloss is introduced to capture the similarity between the binary codes and\nreal-value features. This asymmetric loss not only improves the retrieval\nperformance, but also contributes to a quick convergence at the training phase.\nBy taking advantage of the two-stream deep structures and two types of\nasymmetric pairwise functions, an alternating algorithm is designed to optimize\nthe deep features and high-quality binary codes efficiently. Experimental\nresults on three real-world datasets substantiate the effectiveness and\nsuperiority of our approach as compared with state-of-the-art.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [15.275267601013184, 3.139183282852173], "cluster": 6}, {"key": "li2018ranking", "year": "2017", "citations": "2", "title": "Ranking With Adaptive Neighbors", "abstract": "<p>Retrieving the most similar objects in a large-scale database for a given\nquery is a fundamental building block in many application domains, ranging from\nweb searches, visual, cross media, and document retrievals. State-of-the-art\napproaches have mainly focused on capturing the underlying geometry of the data\nmanifolds. Graph-based approaches, in particular, define various diffusion\nprocesses on weighted data graphs. Despite success, these approaches rely on\nfixed-weight graphs, making ranking sensitive to the input affinity matrix. In\nthis study, we propose a new ranking algorithm that simultaneously learns the\ndata affinity matrix and the ranking scores. The proposed optimization\nformulation assigns adaptive neighbors to each point in the data based on the\nlocal connectivity, and the smoothness constraint assigns similar ranking\nscores to similar data points. We develop a novel and efficient algorithm to\nsolve the optimization problem. Evaluations using synthetic and real datasets\nsuggest that the proposed algorithm can outperform the existing methods.</p>\n", "tags": ["Graph-Based-Ann", "Text-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [49.94195556640625, 5.569531440734863], "cluster": 9}, {"key": "li2018self", "year": "2018", "citations": "426", "title": "Self-supervised Adversarial Hashing Networks For Cross-modal Retrieval", "abstract": "<p>Thanks to the success of deep learning, cross-modal retrieval has made\nsignificant progress recently. However, there still remains a crucial\nbottleneck: how to bridge the modality gap to further enhance the retrieval\naccuracy. In this paper, we propose a self-supervised adversarial hashing\n(\\textbf{SSAH}) approach, which lies among the early attempts to incorporate\nadversarial learning into cross-modal hashing in a self-supervised fashion. The\nprimary contribution of this work is that two adversarial networks are\nleveraged to maximize the semantic correlation and consistency of the\nrepresentations between different modalities. In addition, we harness a\nself-supervised semantic network to discover high-level semantic information in\nthe form of multi-label annotations. Such information guides the feature\nlearning process and preserves the modality relationships in both the common\nsemantic space and the Hamming space. Extensive experiments carried out on\nthree benchmark datasets validate that the proposed SSAH surpasses the\nstate-of-the-art methods.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "CVPR", "Robustness", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-5.324751377105713, -22.515926361083984], "cluster": 3}, {"key": "li2019coupled", "year": "2019", "citations": "106", "title": "Coupled Cyclegan: Unsupervised Hashing Network For Cross-modal Retrieval", "abstract": "<p>In recent years, hashing has attracted more and more attention owing to its\nsuperior capacity of low storage cost and high query efficiency in large-scale\ncross-modal retrieval. Benefiting from deep leaning, continuously compelling\nresults in cross-modal retrieval community have been achieved. However,\nexisting deep cross-modal hashing methods either rely on amounts of labeled\ninformation or have no ability to learn an accuracy correlation between\ndifferent modalities. In this paper, we proposed Unsupervised coupled Cycle\ngenerative adversarial Hashing networks (UCH), for cross-modal retrieval, where\nouter-cycle network is used to learn powerful common representation, and\ninner-cycle network is explained to generate reliable hash codes. Specifically,\nour proposed UCH seamlessly couples these two networks with generative\nadversarial mechanism, which can be optimized simultaneously to learn\nrepresentation and hash codes. Extensive experiments on three popular benchmark\ndatasets show that the proposed UCH outperforms the state-of-the-art\nunsupervised cross-modal hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Robustness", "AAAI", "Multimodal-Retrieval", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [15.508102416992188, 6.110530376434326], "cluster": 6}, {"key": "li2019deep", "year": "2019", "citations": "2", "title": "Deep Multi-index Hashing For Person Re-identification", "abstract": "<p>Traditional person re-identification (ReID) methods typically represent\nperson images as real-valued features, which makes ReID inefficient when the\ngallery set is extremely large. Recently, some hashing methods have been\nproposed to make ReID more efficient. However, these hashing methods will\ndeteriorate the accuracy in general, and the efficiency of them is still not\nhigh enough. In this paper, we propose a novel hashing method, called deep\nmulti-index hashing (DMIH), to improve both efficiency and accuracy for ReID.\nDMIH seamlessly integrates multi-index hashing and multi-branch based networks\ninto the same framework. Furthermore, a novel block-wise multi-index hashing\ntable construction approach and a search-aware multi-index (SAMI) loss are\nproposed in DMIH to improve the search efficiency. Experiments on three widely\nused datasets show that DMIH can outperform other state-of-the-art baselines,\nincluding both hashing methods and real-valued methods, in terms of both\nefficiency and accuracy.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Vector-Indexing", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [6.4924421310424805, -2.970898389816284], "cluster": 6}, {"key": "li2019design", "year": "2018", "citations": "36", "title": "The Design And Implementation Of A Real Time Visual Search System On JD E-commerce Platform", "abstract": "<p>We present the design and implementation of a visual search system for real\ntime image retrieval on JD.com, the world\u2019s third largest and China\u2019s largest\ne-commerce site. We demonstrate that our system can support real time visual\nsearch with hundreds of billions of product images at sub-second timescales and\nhandle frequent image updates through distributed hierarchical architecture and\nefficient indexing methods. We hope that sharing our practice with our real\nproduction system will inspire the middleware community\u2019s interest and\nappreciation for building practical large scale systems for emerging\napplications, such as ecommerce visual search.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-14.858166694641113, -43.13447570800781], "cluster": 3}, {"key": "li2019designovel", "year": "2019", "citations": "3", "title": "Designovel's System Description For Fashion-iq Challenge 2019", "abstract": "<p>This paper describes Designovel\u2019s systems which are submitted to the Fashion\nIQ Challenge 2019. Goal of the challenge is building an image retrieval system\nwhere input query is a candidate image plus two text phrases describe user\u2019s\nfeedback about visual differences between the candidate image and the search\ntarget. We built the systems by combining methods from recent work on deep\nmetric learning, multi-modal retrieval and natual language processing. First,\nwe encode both candidate and target images with CNNs into high-level\nrepresentations, and encode text descriptions to a single text vector using\nTransformer-based encoder. Then we compose candidate image vector and text\nrepresentation into a single vector which is exptected to be biased toward\ntarget image vector. Finally, we compute cosine similarities between composed\nvector and encoded vectors of whole dataset, and rank them in desceding order\nto get ranked list. We experimented with Fashion IQ 2019 dataset in various\nsettings of hyperparameters, achieved 39.12% average recall by a single model\nand 43.67% average recall by an ensemble of 16 models on test dataset.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.558570861816406, -6.977651596069336], "cluster": 0}, {"key": "li2019memory", "year": "2019", "citations": "44", "title": "Memory-based Neighbourhood Embedding For Visual Recognition", "abstract": "<p>Learning discriminative image feature embeddings is of great importance to\nvisual recognition. To achieve better feature embeddings, most current methods\nfocus on designing different network structures or loss functions, and the\nestimated feature embeddings are usually only related to the input images. In\nthis paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a\ngeneral CNN feature by considering its neighbourhood. The method aims to solve\ntwo critical problems, i.e., how to acquire more relevant neighbours in the\nnetwork training and how to aggregate the neighbourhood information for a more\ndiscriminative embedding. We first augment an episodic memory module into the\nnetwork, which can provide more relevant neighbours for both training and\ntesting. Then the neighbours are organized in a tree graph with the target\ninstance as the root node. The neighbourhood information is gradually\naggregated to the root node in a bottom-up manner, and aggregation weights are\nsupervised by the class relationships between the nodes. We apply MNE on image\nsearch and few shot learning tasks. Extensive ablation studies demonstrate the\neffectiveness of each component, and our method significantly outperforms the\nstate-of-the-art approaches.</p>\n", "tags": ["Supervised", "ICCV"], "tsne_embedding": [-36.50290298461914, -5.346936225891113], "cluster": 0}, {"key": "li2019neighborhood", "year": "2019", "citations": "40", "title": "Neighborhood Preserving Hashing For Scalable Video Retrieval", "abstract": "<p>In this paper, we propose a Neighborhood Preserving\nHashing (NPH) method for scalable video retrieval in an\nunsupervised manner. Unlike most existing deep video\nhashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal\nneighborhood information into the encoding network such\nthat the neighborhood-relevant visual content of a video can\nbe preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses\non partial useful content of each input frame conditioned\non the neighborhood information. We then integrate the\nneighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the\nlearned hashing functions can map similar videos to similar\nbinary codes. Extensive experiments on three widely-used\nbenchmark datasets validate the effectiveness of our proposed approach.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Datasets", "Compact-Codes", "Evaluation", "Video-Retrieval", "Unsupervised"], "tsne_embedding": [-19.168027877807617, 30.951435089111328], "cluster": 8}, {"key": "li2019push", "year": "2019", "citations": "7", "title": "Push For Quantization: Deep Fisher Hashing", "abstract": "<p>Current massive datasets demand light-weight access for analysis. Discrete hashing methods are thus beneficial because they map high-dimensional data to compact binary codes that are efficient to store and process, while preserving semantic similarity. To optimize powerful deep learning methods for image hashing, gradient-based methods are required. Binary codes, however, are discrete and thus have no continuous derivatives. Relaxing the problem by solving it in a continuous space and then quantizing the solution is not guaranteed to yield separable binary codes. The quantization needs to be included in the optimization. In this paper we push for quantization: We optimize maximum class separability in the binary space. We introduce a margin on distances between dissimilar image pairs as measured in the binary space. In addition to pair-wise distances, we draw inspiration from Fisher\u2019s Linear Discriminant Analysis (Fisher LDA) to maximize the binary distances between classes and at the same time minimize the binary distance of images within the same class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate compact codes comparing favorably to the current state of the art.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-6.192027568817139, 34.91012191772461], "cluster": 8}, {"key": "li2019rethinking", "year": "2019", "citations": "5", "title": "Rethinking Loss Design For Large-scale 3D Shape Retrieval", "abstract": "<p>Learning discriminative shape representations is a crucial issue for\nlarge-scale 3D shape retrieval. In this paper, we propose the Collaborative\nInner Product Loss (CIP Loss) to obtain ideal shape embedding that\ndiscriminative among different categories and clustered within the same class.\nUtilizing simple inner product operation, CIP loss explicitly enforces the\nfeatures of the same class to be clustered in a linear subspace, while\ninter-class subspaces are constrained to be at least orthogonal. Compared to\nprevious metric loss functions, CIP loss could provide more clear geometric\ninterpretation for the embedding than Euclidean margin, and is easy to\nimplement without normalization operation referring to cosine margin. Moreover,\nour proposed loss term can combine with other commonly used loss functions and\ncan be easily plugged into existing off-the-shelf architectures. Extensive\nexperiments conducted on the two public 3D object retrieval datasets, ModelNet\nand ShapeNetCore 55, demonstrate the effectiveness of our proposal, and our\nmethod has achieved state-of-the-art results on both datasets.</p>\n", "tags": ["AAAI", "Scalability", "IJCAI", "Datasets"], "tsne_embedding": [-38.23271179199219, -13.57763957977295], "cluster": 5}, {"key": "li2020deep", "year": "2020", "citations": "8", "title": "Deep Unsupervised Image Hashing By Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video\ncollections without having expensive annotations available. Hashing aims to\nlearn short binary codes for compact storage and efficient semantic retrieval.\nWe propose an unsupervised deep hashing layer called Bi-half Net that maximizes\nentropy of the binary codes. Entropy is maximal when both possible values of\nthe bit are uniformly (half-half) distributed. To maximize bit entropy, we do\nnot add a term to the loss function as this is difficult to optimize and tune.\nInstead, we design a new parameter-free network layer to explicitly force\ncontinuous image features to approximate the optimal half-half bit\ndistribution. This layer is shown to minimize a penalized term of the\nWasserstein distance between the learned continuous image features and the\noptimal half-half bit distribution. Experimental results on the image datasets\nFlickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and\nHmdb-51 show that our approach leads to compact codes and compares favorably to\nthe current state-of-the-art.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Image-Retrieval", "Datasets", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-14.961538314819336, 34.775421142578125], "cluster": 8}, {"key": "li2020hamming", "year": "2020", "citations": "5", "title": "Hamming OCR: A Locality Sensitive Hashing Neural Network For Scene Text Recognition", "abstract": "<p>Recently, inspired by Transformer, self-attention-based scene text\nrecognition approaches have achieved outstanding performance. However, we find\nthat the size of model expands rapidly with the lexicon increasing.\nSpecifically, the number of parameters for softmax classification layer and\noutput embedding layer are proportional to the vocabulary size. It hinders the\ndevelopment of a lightweight text recognition model especially applied for\nChinese and multiple languages. Thus, we propose a lightweight scene text\nrecognition model named Hamming OCR. In this model, a novel Hamming classifier,\nwhich adopts locality sensitive hashing (LSH) algorithm to encode each\ncharacter, is proposed to replace the softmax regression and the generated LSH\ncode is directly employed to replace the output embedding. We also present a\nsimplified transformer decoder to reduce the number of parameters by removing\nthe feed-forward network and using cross-layer parameter sharing technique.\nCompared with traditional methods, the number of parameters in both\nclassification and embedding layers is independent on the size of vocabulary,\nwhich significantly reduces the storage requirement without loss of accuracy.\nExperimental results on several datasets, including four public benchmaks and a\nChinese text dataset synthesized by SynthText with more than 20,000 characters,\nshows that Hamming OCR achieves competitive results.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [16.48897933959961, 19.494585037231445], "cluster": 2}, {"key": "li2020multiple", "year": "2020", "citations": "0", "title": "Multiple Code Hashing For Efficient Image Retrieval", "abstract": "<p>Due to its low storage cost and fast query speed, hashing has been widely\nused in large-scale image retrieval tasks. Hash bucket search returns data\npoints within a given Hamming radius to each query, which can enable search at\na constant or sub-linear time cost. However, existing hashing methods cannot\nachieve satisfactory retrieval performance for hash bucket search in complex\nscenarios, since they learn only one hash code for each image. More\nspecifically, by using one hash code to represent one image, existing methods\nmight fail to put similar image pairs to the buckets with a small Hamming\ndistance to the query when the semantic information of images is complex. As a\nresult, a large number of hash buckets need to be visited for retrieving\nsimilar images, based on the learned codes. This will deteriorate the\nefficiency of hash bucket search. In this paper, we propose a novel hashing\nframework, called multiple code hashing (MCH), to improve the performance of\nhash bucket search. The main idea of MCH is to learn multiple hash codes for\neach image, with each code representing a different region of the image.\nFurthermore, we propose a deep reinforcement learning algorithm to learn the\nparameters in MCH. To the best of our knowledge, this is the first work that\nproposes to learn multiple hash codes for each image in image retrieval.\nExperiments demonstrate that MCH can achieve a significant improvement in hash\nbucket search, compared with existing methods that learn only one hash code for\neach image.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Scalability", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [-0.49825194478034973, 18.269792556762695], "cluster": 8}, {"key": "li2020sea", "year": "2020", "citations": "53", "title": "SEA: Sentence Encoder Assembly For Video Retrieval By Textual Queries", "abstract": "<p>Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search\n(AVS), is a core theme in multimedia data management and retrieval. The success\nof AVS counts on cross-modal representation learning that encodes both query\nsentences and videos into common spaces for semantic similarity computation.\nInspired by the initial success of previously few works in combining multiple\nsentence encoders, this paper takes a step forward by developing a new and\ngeneral method for effectively exploiting diverse sentence encoders. The\nnovelty of the proposed method, which we term Sentence Encoder Assembly (SEA),\nis two-fold. First, different from prior art that use only a single common\nspace, SEA supports text-video matching in multiple encoder-specific common\nspaces. Such a property prevents the matching from being dominated by a\nspecific encoder that produces an encoding vector much longer than other\nencoders. Second, in order to explore complementarities among the individual\ncommon spaces, we propose multi-space multi-loss learning. As extensive\nexperiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD)\nshow, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to\nimplement. All this makes SEA an appealing solution for AVS and promising for\ncontinuously advancing the task by harvesting new sentence encoders.</p>\n", "tags": ["Video-Retrieval"], "tsne_embedding": [-37.26035690307617, -26.828201293945312], "cluster": 5}, {"key": "li2020task", "year": "2021", "citations": "17", "title": "Task-adaptive Asymmetric Deep Cross-modal Hashing", "abstract": "<p>Supervised cross-modal hashing aims to embed the semantic correlations of\nheterogeneous modality data into the binary hash codes with discriminative\nsemantic labels. Because of its advantages on retrieval and storage efficiency,\nit is widely used for solving efficient cross-modal retrieval. However,\nexisting researches equally handle the different tasks of cross-modal\nretrieval, and simply learn the same couple of hash functions in a symmetric\nway for them. Under such circumstance, the uniqueness of different cross-modal\nretrieval tasks are ignored and sub-optimal performance may be brought.\nMotivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal\nHashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash\nfunctions for two sub-retrieval tasks via simultaneous modality representation\nand asymmetric hash learning. Unlike previous cross-modal hashing approaches,\nour learning framework jointly optimizes semantic preserving that transforms\ndeep features of multimedia data into binary hash codes, and the semantic\nregression which directly regresses query modality representation to explicit\nlabel. With our model, the binary codes can effectively preserve semantic\ncorrelations across different modalities, meanwhile, adaptively capture the\nquery semantics. The superiority of TA-ADCMH is proved on two standard datasets\nfrom many aspects.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [9.663026809692383, 4.562795639038086], "cluster": 6}, {"key": "li2021c", "year": "2021", "citations": "2", "title": "C-OPH: Improving The Accuracy Of One Permutation Hashing (OPH) With Circulant Permutations", "abstract": "<p>Minwise hashing (MinHash) is a classical method for efficiently estimating\nthe Jaccrad similarity in massive binary (0/1) data. To generate \\(K\\) hash\nvalues for each data vector, the standard theory of MinHash requires \\(K\\)\nindependent permutations. Interestingly, the recent work on \u201ccirculant MinHash\u201d\n(C-MinHash) has shown that merely two permutations are needed. The first\npermutation breaks the structure of the data and the second permutation is\nre-used \\(K\\) time in a circulant manner. Surprisingly, the estimation accuracy\nof C-MinHash is proved to be strictly smaller than that of the original\nMinHash. The more recent work further demonstrates that practically only one\npermutation is needed. Note that C-MinHash is different from the well-known\nwork on \u201cOne Permutation Hashing (OPH)\u201d published in NIPS\u201912. OPH and its\nvariants using different \u201cdensification\u201d schemes are popular alternatives to\nthe standard MinHash. The densification step is necessary in order to deal with\nempty bins which exist in One Permutation Hashing.\n  In this paper, we propose to incorporate the essential ideas of C-MinHash to\nimprove the accuracy of One Permutation Hashing. Basically, we develop a new\ndensification method for OPH, which achieves the smallest estimation variance\ncompared to all existing densification schemes for OPH. Our proposed method is\nnamed C-OPH (Circulant OPH). After the initial permutation (which breaks the\nexisting structure of the data), C-OPH only needs a \u201cshorter\u201d permutation of\nlength \\(D/K\\) (instead of \\(D\\)), where \\(D\\) is the original data dimension and \\(K\\)\nis the total number of bins in OPH. This short permutation is re-used in \\(K\\)\nbins in a circulant shifting manner. It can be shown that the estimation\nvariance of the Jaccard similarity is strictly smaller than that of the\nexisting (densified) OPH methods.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [16.440855026245117, 52.92826461791992], "cluster": 4}, {"key": "li2021discrete", "year": "2021", "citations": "4", "title": "Discrete Knowledge Graph Embedding Based On Discrete Optimization", "abstract": "<p>This paper proposes a discrete knowledge graph (KG) embedding (DKGE) method,\nwhich projects KG entities and relations into the Hamming space based on a\ncomputationally tractable discrete optimization algorithm, to solve the\nformidable storage and computation cost challenges in traditional continuous\ngraph embedding methods. The convergence of DKGE can be guaranteed\ntheoretically. Extensive experiments demonstrate that DKGE achieves superior\naccuracy than classical hashing functions that map the effective continuous\nembeddings into discrete codes. Besides, DKGE reaches comparable accuracy with\nmuch lower computational complexity and storage compared to many continuous\ngraph embedding methods.</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [23.895286560058594, 5.811424255371094], "cluster": 6}, {"key": "li2021extra", "year": "2021", "citations": "29", "title": "EXTRA: Explanation Ranking Datasets For Explainable Recommendation", "abstract": "<p>Recently, research on explainable recommender systems has drawn much\nattention from both academia and industry, resulting in a variety of\nexplainable models. As a consequence, their evaluation approaches vary from\nmodel to model, which makes it quite difficult to compare the explainability of\ndifferent models. To achieve a standard way of evaluating recommendation\nexplanations, we provide three benchmark datasets for EXplanaTion RAnking\n(denoted as EXTRA), on which explainability can be measured by ranking-oriented\nmetrics. Constructing such datasets, however, poses great challenges. First,\nuser-item-explanation triplet interactions are rare in existing recommender\nsystems, so how to find alternatives becomes a challenge. Our solution is to\nidentify nearly identical sentences from user reviews. This idea then leads to\nthe second challenge, i.e., how to efficiently categorize the sentences in a\ndataset into different groups, since it has quadratic runtime complexity to\nestimate the similarity between any two sentences. To mitigate this issue, we\nprovide a more efficient method based on Locality Sensitive Hashing (LSH) that\ncan detect near-duplicates in sub-linear time for a given query. Moreover, we\nmake our code publicly available to allow researchers in the community to\ncreate their own datasets.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "SIGIR", "Datasets", "Evaluation"], "tsne_embedding": [24.663755416870117, -20.90824317932129], "cluster": 7}, {"key": "li2021learning", "year": "2022", "citations": "47", "title": "Learning Semantic-aligned Feature Representation For Text-based Person Search", "abstract": "<p>Text-based person search aims to retrieve images of a certain pedestrian by a\ntextual description. The key challenge of this task is to eliminate the\ninter-modality gap and achieve the feature alignment across modalities. In this\npaper, we propose a semantic-aligned embedding method for text-based person\nsearch, in which the feature alignment across modalities is achieved by\nautomatically learning the semantic-aligned visual features and textual\nfeatures. First, we introduce two Transformer-based backbones to encode robust\nfeature representations of the images and texts. Second, we design a\nsemantic-aligned feature aggregation network to adaptively select and aggregate\nfeatures with the same semantics into part-aware features, which is achieved by\na multi-head attention module constrained by a cross-modality part alignment\nloss and a diversity loss. Experimental results on the CUHK-PEDES and Flickr30K\ndatasets show that our method achieves state-of-the-art performances.</p>\n", "tags": ["ICASSP", "Datasets"], "tsne_embedding": [-31.476987838745117, -43.04887008666992], "cluster": 5}, {"key": "li2021more", "year": "2021", "citations": "23", "title": "More Robust Dense Retrieval With Contrastive Dual Learning", "abstract": "<p>Dense retrieval conducts text retrieval in the embedding space and has shown\nmany advantages compared to sparse retrieval. Existing dense retrievers\noptimize representations of queries and documents with contrastive training and\nmap them to the embedding space. The embedding space is optimized by aligning\nthe matched query-document pairs and pushing the negative documents away from\nthe query. However, in such training paradigm, the queries are only optimized\nto align to the documents and are coarsely positioned, leading to an\nanisotropic query embedding space. In this paper, we analyze the embedding\nspace distributions and propose an effective training paradigm, Contrastive\nDual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained\nquery representations for dense retrieval. DANCE incorporates an additional\ndual training object of query retrieval, inspired by the classic information\nretrieval training axiom, query likelihood. With contrastive learning, the dual\ntraining object of DANCE learns more tailored representations for queries and\ndocuments to keep the embedding space smooth and uniform, thriving on the\nranking performance of DANCE on the MS MARCO document retrieval task. Different\nfrom ANCE that only optimized with the document retrieval task, DANCE\nconcentrates the query embeddings closer to document representations while\nmaking the document distribution more discriminative. Such concentrated query\nembedding distribution assigns more uniform negative sampling probabilities to\nqueries and helps to sufficiently optimize query representations in the query\nretrieval task. Our codes are released at https://github.com/thunlp/DANCE.</p>\n", "tags": ["Self-Supervised", "Evaluation", "Text-Retrieval", "SIGIR"], "tsne_embedding": [9.36310863494873, -20.87299346923828], "cluster": 7}, {"key": "li2021self", "year": "2021", "citations": "38", "title": "Self-supervised Video Hashing Via Bidirectional Transformers", "abstract": "<p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Compact-Codes", "CVPR", "Datasets", "Supervised", "Evaluation", "Video-Retrieval", "Unsupervised"], "tsne_embedding": [-9.322446823120117, -0.9040074944496155], "cluster": 1}, {"key": "li2022adaptive", "year": "2022", "citations": "23", "title": "Adaptive Structural Similarity Preserving For Unsupervised Cross Modal Hashing", "abstract": "<p>Cross-modal hashing is an important approach for multimodal data management\nand application. Existing unsupervised cross-modal hashing algorithms mainly\nrely on data features in pre-trained models to mine their similarity\nrelationships. However, their optimization objectives are based on the static\nmetric between the original uni-modal features, without further exploring data\ncorrelations during the training. In addition, most of them mainly focus on\nassociation mining and alignment among pairwise instances in continuous space\nbut ignore the latent structural correlations contained in the semantic hashing\nspace. In this paper, we propose an unsupervised hash learning framework,\nnamely Adaptive Structural Similarity Preservation Hashing (ASSPH), to solve\nthe above problems. Firstly, we propose an adaptive learning scheme, with\nlimited data and training batches, to enrich semantic correlations of unlabeled\ninstances during the training process and meanwhile to ensure a smooth\nconvergence of the training process. Secondly, we present an asymmetric\nstructural semantic representation learning scheme. We introduce structural\nsemantic metrics based on graph adjacency relations during the semantic\nreconstruction and correlation mining stage and meanwhile align the structure\nsemantics in the hash space with an asymmetric binary optimization process.\nFinally, we conduct extensive experiments to validate the enhancements of our\nwork in comparison with existing works.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Tools-&-Libraries", "Evaluation", "Unsupervised"], "tsne_embedding": [14.297442436218262, -0.7670259475708008], "cluster": 6}, {"key": "li2022asymmetric", "year": "2023", "citations": "0", "title": "Asymmetric Scalable Cross-modal Hashing", "abstract": "<p>Cross-modal hashing is a successful method to solve large-scale multimedia\nretrieval issue. A lot of matrix factorization-based hashing methods are\nproposed. However, the existing methods still struggle with a few problems,\nsuch as how to generate the binary codes efficiently rather than directly relax\nthem to continuity. In addition, most of the existing methods choose to use an\n\\(n\\times n\\) similarity matrix for optimization, which makes the memory and\ncomputation unaffordable. In this paper we propose a novel Asymmetric Scalable\nCross-Modal Hashing (ASCMH) to address these issues. It firstly introduces a\ncollective matrix factorization to learn a common latent space from the\nkernelized features of different modalities, and then transforms the similarity\nmatrix optimization to a distance-distance difference problem minimization with\nthe help of semantic labels and common latent space. Hence, the computational\ncomplexity of the \\(n\\times n\\) asymmetric optimization is relieved. In the\ngeneration of hash codes we also employ an orthogonal constraint of label\ninformation, which is indispensable for search accuracy. So the redundancy of\ncomputation can be much reduced. For efficient optimization and scalable to\nlarge-scale datasets, we adopt the two-step approach rather than optimizing\nsimultaneously. Extensive experiments on three benchmark datasets: Wiki,\nMIRFlickr-25K, and NUS-WIDE, demonstrate that our ASCMH outperforms the\nstate-of-the-art cross-modal hashing methods in terms of accuracy and\nefficiency.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [11.113251686096191, 14.718727111816406], "cluster": 6}, {"key": "li2022citadel", "year": "2023", "citations": "5", "title": "CITADEL: Conditional Token Interaction Via Dynamic Lexical Routing For Efficient And Effective Multi-vector Retrieval", "abstract": "<p>Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and\ndense (e.g. DPR) retrievers and have achieved state-of-the-art performance on\nvarious retrieval tasks. These methods, however, are orders of magnitude slower\nand need much more space to store their indices compared to their single-vector\ncounterparts. In this paper, we unify different multi-vector retrieval models\nfrom a token routing viewpoint and propose conditional token interaction via\ndynamic lexical routing, namely CITADEL, for efficient and effective\nmulti-vector retrieval. CITADEL learns to route different token vectors to the\npredicted lexical ``keys\u2019\u2019 such that a query token vector only interacts with\ndocument token vectors routed to the same key. This design significantly\nreduces the computation cost while maintaining high accuracy. Notably, CITADEL\nachieves the same or slightly better performance than the previous state of the\nart, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR)\nevaluations, while being nearly 40 times faster. Code and data are available at\nhttps://github.com/facebookresearch/dpr-scale.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [14.708467483520508, -22.59225845336914], "cluster": 7}, {"key": "li2022image", "year": "2022", "citations": "2", "title": "Image-text Retrieval With Binary And Continuous Label Supervision", "abstract": "<p>Most image-text retrieval work adopts binary labels indicating whether a pair\nof image and text matches or not. Such a binary indicator covers only a limited\nsubset of image-text semantic relations, which is insufficient to represent\nrelevance degrees between images and texts described by continuous labels such\nas image captions. The visual-semantic embedding space obtained by learning\nbinary labels is incoherent and cannot fully characterize the relevance\ndegrees. In addition to the use of binary labels, this paper further\nincorporates continuous pseudo labels (generally approximated by text\nsimilarity between captions) to indicate the relevance degrees. To learn a\ncoherent embedding space, we propose an image-text retrieval framework with\nBinary and Continuous Label Supervision (BCLS), where binary labels are used to\nguide the retrieval model to learn limited binary correlations, and continuous\nlabels are complementary to the learning of image-text semantic relations. For\nthe learning of binary labels, we improve the common Triplet ranking loss with\nSoft Negative mining (Triplet-SN) to improve convergence. For the learning of\ncontinuous labels, we design Kendall ranking loss inspired by Kendall rank\ncorrelation coefficient (Kendall), which improves the correlation between the\nsimilarity scores predicted by the retrieval model and the continuous labels.\nTo mitigate the noise introduced by the continuous pseudo labels, we further\ndesign Sliding Window sampling and Hard Sample mining strategy (SW-HS) to\nalleviate the impact of noise and reduce the complexity of our framework to the\nsame order of magnitude as the triplet ranking loss. Extensive experiments on\ntwo image-text retrieval benchmarks demonstrate that our method can improve the\nperformance of state-of-the-art image-text retrieval models.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Text-Retrieval"], "tsne_embedding": [-15.729305267333984, 17.61177635192871], "cluster": 8}, {"key": "li2022informative", "year": "2022", "citations": "0", "title": "Informative Sample-aware Proxy For Deep Metric Learning", "abstract": "<p>Among various supervised deep metric learning methods proxy-based approaches\nhave achieved high retrieval accuracies. Proxies, which are\nclass-representative points in an embedding space, receive updates based on\nproxy-sample similarities in a similar manner to sample representations. In\nexisting methods, a relatively small number of samples can produce large\ngradient magnitudes (ie, hard samples), and a relatively large number of\nsamples can produce small gradient magnitudes (ie, easy samples); these can\nplay a major part in updates. Assuming that acquiring too much sensitivity to\nsuch extreme sets of samples would deteriorate the generalizability of a\nmethod, we propose a novel proxy-based method called Informative Sample-Aware\nProxy (Proxy-ISA), which directly modifies a gradient weighting factor for each\nsample using a scheduled threshold function, so that the model is more\nsensitive to the informative samples. Extensive experiments on the\nCUB-200-2011, Cars-196, Stanford Online Products and In-shop Clothes Retrieval\ndatasets demonstrate the superiority of Proxy-ISA compared with the\nstate-of-the-art methods.</p>\n", "tags": ["Supervised", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-17.838125228881836, 14.796619415283203], "cluster": 8}, {"key": "li2022learning", "year": "2022", "citations": "3", "title": "Learning Diverse Document Representations With Deep Query Interactions For Dense Retrieval", "abstract": "<p>In this paper, we propose a new dense retrieval model which learns diverse\ndocument representations with deep query interactions. Our model encodes each\ndocument with a set of generated pseudo-queries to get query-informed,\nmulti-view document representations. It not only enjoys high inference\nefficiency like the vanilla dual-encoder models, but also enables deep\nquery-document interactions in document encoding and provides multi-faceted\nrepresentations to better match different queries. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed method, out-performing\nstrong dual encoder baselines.The code is available at\n\\url{https://github.com/jordane95/dual-cross-encoder</p>\n", "tags": ["Efficiency"], "tsne_embedding": [6.033962249755859, -27.71257209777832], "cluster": 7}, {"key": "li2023citadel", "year": "2023", "citations": "5", "title": "CITADEL: Conditional Token Interaction Via Dynamic Lexical Routing For Efficient And Effective Multi-vector Retrieval", "abstract": "<p>Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and\ndense (e.g. DPR) retrievers and have achieved state-of-the-art performance on\nvarious retrieval tasks. These methods, however, are orders of magnitude slower\nand need much more space to store their indices compared to their single-vector\ncounterparts. In this paper, we unify different multi-vector retrieval models\nfrom a token routing viewpoint and propose conditional token interaction via\ndynamic lexical routing, namely CITADEL, for efficient and effective\nmulti-vector retrieval. CITADEL learns to route different token vectors to the\npredicted lexical ``keys\u2019\u2019 such that a query token vector only interacts with\ndocument token vectors routed to the same key. This design significantly\nreduces the computation cost while maintaining high accuracy. Notably, CITADEL\nachieves the same or slightly better performance than the previous state of the\nart, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR)\nevaluations, while being nearly 40 times faster. Code and data are available at\nhttps://github.com/facebookresearch/dpr-scale.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [14.70848560333252, -22.592256546020508], "cluster": 7}, {"key": "li2023constructing", "year": "2023", "citations": "10", "title": "Constructing Tree-based Index For Efficient And Effective Dense Retrieval", "abstract": "<p>Recent studies have shown that Dense Retrieval (DR) techniques can\nsignificantly improve the performance of first-stage retrieval in IR systems.\nDespite its empirical effectiveness, the application of DR is still limited. In\ncontrast to statistic retrieval models that rely on highly efficient inverted\nindex solutions, DR models build dense embeddings that are difficult to be\npre-processed with most existing search indexing systems. To avoid the\nexpensive cost of brute-force search, the Approximate Nearest Neighbor (ANN)\nalgorithm and corresponding indexes are widely applied to speed up the\ninference process of DR models. Unfortunately, while ANN can improve the\nefficiency of DR models, it usually comes with a significant price on retrieval\nperformance.\n  To solve this issue, we propose JTR, which stands for Joint optimization of\nTRee-based index and query encoding. Specifically, we design a new unified\ncontrastive learning loss to train tree-based index and query encoder in an\nend-to-end manner. The tree-based negative sampling strategy is applied to make\nthe tree have the maximum heap property, which supports the effectiveness of\nbeam search well. Moreover, we treat the cluster assignment as an optimization\nproblem to update the tree-based index that allows overlapped clustering. We\nevaluate JTR on numerous popular retrieval benchmarks. Experimental results\nshow that JTR achieves better retrieval performance while retaining high system\nefficiency compared with widely-adopted baselines. It provides a potential\nsolution to balance efficiency and effectiveness in neural retrieval system\ndesigns.</p>\n", "tags": ["Self-Supervised", "Efficiency", "SIGIR", "Tree-Based-Ann", "Evaluation"], "tsne_embedding": [13.701102256774902, 19.41520118713379], "cluster": 2}, {"key": "li2023differentially", "year": "2023", "citations": "0", "title": "Differentially Private One Permutation Hashing And Bin-wise Consistent Weighted Sampling", "abstract": "<p>Minwise hashing (MinHash) is a standard algorithm widely used in the\nindustry, for large-scale search and learning applications with the binary\n(0/1) Jaccard similarity. One common use of MinHash is for processing massive\nn-gram text representations so that practitioners do not have to materialize\nthe original data (which would be prohibitive). Another popular use of MinHash\nis for building hash tables to enable sub-linear time approximate near neighbor\n(ANN) search. MinHash has also been used as a tool for building large-scale\nmachine learning systems. The standard implementation of MinHash requires\napplying \\(K\\) random permutations. In comparison, the method of one permutation\nhashing (OPH), is an efficient alternative of MinHash which splits the data\nvectors into \\(K\\) bins and generates hash values within each bin. OPH is\nsubstantially more efficient and also more convenient to use.\n  In this paper, we combine the differential privacy (DP) with OPH (as well as\nMinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix,\nDP-OPH-re and DP-OPH-rand, depending on which densification strategy is adopted\nto deal with empty bins in OPH. A detailed roadmap to the algorithm design is\npresented along with the privacy analysis. An analytical comparison of our\nproposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided to\njustify the advantage of DP-OPH. Experiments on similarity search confirm the\nmerits of DP-OPH, and guide the choice of the proper variant in different\npractical scenarios. Our technique is also extended to bin-wise consistent\nweighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS for\nnon-binary data. Experiments on classification tasks demonstrate that DP-BCWS\nis able to achieve excellent utility at around \\(\\epsilon = 5\\sim 10\\), where\n\\(\\epsilon\\) is the standard parameter in the language of \\((\\epsilon,\n\\delta)\\)-DP.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "Similarity-Search", "Privacy-&-Security", "Tools-&-Libraries", "Large-Scale-Search", "Evaluation"], "tsne_embedding": [11.940911293029785, 22.605846405029297], "cluster": 4}, {"key": "li2023dual", "year": "2022", "citations": "16", "title": "Dual-stream Knowledge-preserving Hashing For Unsupervised Video Retrieval", "abstract": "<p>Unsupervised video hashing usually optimizes binary codes by learning to\nreconstruct input videos. Such reconstruction constraint spends much effort on\nframe-level temporal context changes without focusing on video-level global\nsemantics that are more useful for retrieval. Hence, we address this problem by\ndecomposing video information into reconstruction-dependent and\nsemantic-dependent information, which disentangles the semantic extraction from\nreconstruction constraint. Specifically, we first design a simple dual-stream\nstructure, including a temporal layer and a hash layer. Then, with the help of\nsemantic similarity knowledge obtained from self-supervision, the hash layer\nlearns to capture information for semantic retrieval, while the temporal layer\nlearns to capture the information for reconstruction. In this way, the model\nnaturally preserves the disentangled semantics into binary codes. Validated by\ncomprehensive experiments, our method consistently outperforms the\nstate-of-the-arts on three video benchmarks.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Unsupervised"], "tsne_embedding": [-11.441272735595703, 0.18450689315795898], "cluster": 1}, {"key": "li2023locality", "year": "2023", "citations": "3", "title": "Locality Preserving Multiview Graph Hashing For Large Scale Remote Sensing Image Search", "abstract": "<p>Hashing is very popular for remote sensing image search. This article\nproposes a multiview hashing with learnable parameters to retrieve the queried\nimages for a large-scale remote sensing dataset. Existing methods always\nneglect that real-world remote sensing data lies on a low-dimensional manifold\nembedded in high-dimensional ambient space. Unlike previous methods, this\narticle proposes to learn the consensus compact codes in a view-specific\nlow-dimensional subspace. Furthermore, we have added a hyperparameter learnable\nmodule to avoid complex parameter tuning. In order to prove the effectiveness\nof our method, we carried out experiments on three widely used remote sensing\ndata sets and compared them with seven state-of-the-art methods. Extensive\nexperiments show that the proposed method can achieve competitive results\ncompared to the other method.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "ICASSP", "Datasets", "Compact-Codes"], "tsne_embedding": [-33.80419921875, 16.09636688232422], "cluster": 0}, {"key": "li2023pb", "year": "2024", "citations": "0", "title": "Pb-hash: Partitioned B-bit Hashing", "abstract": "<p>Many hashing algorithms including minwise hashing (MinHash), one permutation\nhashing (OPH), and consistent weighted sampling (CWS) generate integers of \\(B\\)\nbits. With \\(k\\) hashes for each data vector, the storage would be \\(B\\times k\\)\nbits; and when used for large-scale learning, the model size would be\n\\(2^B\\times k\\), which can be expensive. A standard strategy is to use only the\nlowest \\(b\\) bits out of the \\(B\\) bits and somewhat increase \\(k\\), the number of\nhashes. In this study, we propose to re-use the hashes by partitioning the \\(B\\)\nbits into \\(m\\) chunks, e.g., \\(b\\times m =B\\). Correspondingly, the model size\nbecomes \\(m\\times 2^b \\times k\\), which can be substantially smaller than the\noriginal \\(2^B\\times k\\).\n  Our theoretical analysis reveals that by partitioning the hash values into\n\\(m\\) chunks, the accuracy would drop. In other words, using \\(m\\) chunks of \\(B/m\\)\nbits would not be as accurate as directly using \\(B\\) bits. This is due to the\ncorrelation from re-using the same hash. On the other hand, our analysis also\nshows that the accuracy would not drop much for (e.g.,) \\(m=2\\sim 4\\). In some\nregions, Pb-Hash still works well even for \\(m\\) much larger than 4. We expect\nPb-Hash would be a good addition to the family of hashing methods/applications\nand benefit industrial practitioners.\n  We verify the effectiveness of Pb-Hash in machine learning tasks, for linear\nSVM models as well as deep learning models. Since the hashed data are\nessentially categorical (ID) features, we follow the standard practice of using\nembedding tables for each hash. With Pb-Hash, we need to design an effective\nstrategy to combine \\(m\\) embeddings. Our study provides an empirical evaluation\non four pooling schemes: concatenation, max pooling, mean pooling, and product\npooling. There is no definite answer which pooling would be always better and\nwe leave that for future study.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "SIGIR", "Evaluation"], "tsne_embedding": [11.331930160522461, 21.344966888427734], "cluster": 2}, {"key": "li2023slim", "year": "2023", "citations": "7", "title": "SLIM: Sparsified Late Interaction For Multi-vector Retrieval With Inverted Indexes", "abstract": "<p>This paper introduces Sparsified Late Interaction for Multi-vector (SLIM)\nretrieval with inverted indexes. Multi-vector retrieval methods have\ndemonstrated their effectiveness on various retrieval datasets, and among them,\nColBERT is the most established method based on the late interaction of\ncontextualized token embeddings of pre-trained language models. However,\nefficient ColBERT implementations require complex engineering and cannot take\nadvantage of off-the-shelf search libraries, impeding their practical use. To\naddress this issue, SLIM first maps each contextualized token vector to a\nsparse, high-dimensional lexical space before performing late interaction\nbetween these sparse token embeddings. We then introduce an efficient two-stage\nretrieval architecture that includes inverted index retrieval followed by a\nscore refinement module to approximate the sparsified late interaction, which\nis fully compatible with off-the-shelf lexical search libraries such as Lucene.\nSLIM achieves competitive accuracy on MS MARCO Passages and BEIR compared to\nColBERT while being much smaller and faster on CPUs. To our knowledge, we are\nthe first to explore using sparse token representations for multi-vector\nretrieval. Source code and data are integrated into the Pyserini IR toolkit.</p>\n", "tags": ["SIGIR", "Datasets"], "tsne_embedding": [14.748196601867676, -26.169097900390625], "cluster": 7}, {"key": "li2023style", "year": "2023", "citations": "5", "title": "The Style Transformer With Common Knowledge Optimization For Image-text Retrieval", "abstract": "<p>Image-text retrieval which associates different modalities has drawn broad\nattention due to its excellent research value and broad real-world application.\nHowever, most of the existing methods haven\u2019t taken the high-level semantic\nrelationships (\u201cstyle embedding\u201d) and common knowledge from multi-modalities\ninto full consideration. To this end, we introduce a novel style transformer\nnetwork with common knowledge optimization (CKSTN) for image-text retrieval.\nThe main module is the common knowledge adaptor (CKA) with both the style\nembedding extractor (SEE) and the common knowledge optimization (CKO) modules.\nSpecifically, the SEE uses the sequential update strategy to effectively\nconnect the features of different stages in SEE. The CKO module is introduced\nto dynamically capture the latent concepts of common knowledge from different\nmodalities. Besides, to get generalized temporal common knowledge, we propose a\nsequential update strategy to effectively integrate the features of different\nlayers in SEE with previous common feature units. CKSTN demonstrates the\nsuperiorities of the state-of-the-art methods in image-text retrieval on MSCOCO\nand Flickr30K datasets. Moreover, CKSTN is constructed based on the lightweight\ntransformer which is more convenient and practical for the application of real\nscenes, due to the better performance and lower parameters.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-30.786745071411133, -29.64404296875], "cluster": 5}, {"key": "li2024bert", "year": "2024", "citations": "0", "title": "BERT-LSH: Reducing Absolute Compute For Attention", "abstract": "<p>This study introduces a novel BERT-LSH model that incorporates Locality\nSensitive Hashing (LSH) to approximate the attention mechanism in the BERT\narchitecture. We examine the computational efficiency and performance of this\nmodel compared to a standard baseline BERT model. Our findings reveal that\nBERT-LSH significantly reduces computational demand for the self-attention\nlayer while unexpectedly outperforming the baseline model in pretraining and\nfine-tuning tasks. These results suggest that the LSH-based attention mechanism\nnot only offers computational advantages but also may enhance the model\u2019s\nability to generalize from its training data. For more information, visit our\nGitHub repository: https://github.com/leo4life2/algoml-final</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [14.016849517822266, -28.36037826538086], "cluster": 7}, {"key": "li2024comae", "year": "2024", "citations": "1", "title": "COMAE: Comprehensive Attribute Exploration For Zero-shot Hashing", "abstract": "<p>Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency and generalization in large-scale retrieval scenarios. While considerable success has been achieved, there still exist urgent limitations. Existing works ignore the locality relationships of representations and attributes, which have effective transferability between seeable classes and unseeable classes. Also, the continuous-value attributes are not fully harnessed. In response, we conduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which depicts the relationships from seen classes to unseen ones through three meticulously designed explorations, i.e., point-wise, pair-wise and class-wise consistency constraints. By regressing attributes from the proposed attribute prototype network, COMAE learns the local features that are relevant to the visual attributes. Then COMAE utilizes contrastive learning to comprehensively depict the context of attributes, rather than instance-independent optimization. Finally, the class-wise constraint is designed to cohesively learn the hash code, image representation, and visual attributes more effectively. Experimental results on the popular ZSH datasets demonstrate that COMAE outperforms state-of-the-art hashing techniques, especially in scenarios with a larger number of unseen label classes.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Efficiency", "Few-Shot-&-Zero-Shot", "Scalability", "Datasets"], "tsne_embedding": [-34.23430633544922, -0.6300277709960938], "cluster": 0}, {"key": "li2024location", "year": "2024", "citations": "0", "title": "Location Aware Modular Biencoder For Tourism Question Answering", "abstract": "<p>Answering real-world tourism questions that seek Point-of-Interest (POI)\nrecommendations is challenging, as it requires both spatial and non-spatial\nreasoning, over a large candidate pool. The traditional method of encoding each\npair of question and POI becomes inefficient when the number of candidates\nincreases, making it infeasible for real-world applications. To overcome this,\nwe propose treating the QA task as a dense vector retrieval problem, where we\nencode questions and POIs separately and retrieve the most relevant POIs for a\nquestion by utilizing embedding space similarity. We use pretrained language\nmodels (PLMs) to encode textual information, and train a location encoder to\ncapture spatial information of POIs. Experiments on a real-world tourism QA\ndataset demonstrate that our approach is effective, efficient, and outperforms\nprevious methods across all metrics. Enabled by the dense retrieval\narchitecture, we further build a global evaluation baseline, expanding the\nsearch space by 20 times compared to previous work. We also explore several\nfactors that impact on the model\u2019s performance through follow-up experiments.\nOur code and model are publicly available at https://github.com/haonan-li/LAMB.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-34.5015754699707, 12.603569984436035], "cluster": 0}, {"key": "li2024mixed", "year": "2024", "citations": "1", "title": "Mixed-precision Embeddings For Large-scale Recommendation Models", "abstract": "<p>Embedding techniques have become essential components of large databases in\nthe deep learning era. By encoding discrete entities, such as words, items, or\ngraph nodes, into continuous vector spaces, embeddings facilitate more\nefficient storage, retrieval, and processing in large databases. Especially in\nthe domain of recommender systems, millions of categorical features are encoded\nas unique embedding vectors, which facilitates the modeling of similarities and\ninteractions among features. However, numerous embedding vectors can result in\nsignificant storage overhead. In this paper, we aim to compress the embedding\ntable through quantization techniques. Given that features vary in importance\nlevels, we seek to identify an appropriate precision for each feature to\nbalance model accuracy and memory usage. To this end, we propose a novel\nembedding compression method, termed Mixed-Precision Embeddings (MPE).\nSpecifically, to reduce the size of the search space, we first group features\nby frequency and then search precision for each feature group. MPE further\nlearns the probability distribution over precision levels for each feature\ngroup, which can be used to identify the most suitable precision with a\nspecially designed sampling strategy. Extensive experiments on three public\ndatasets demonstrate that MPE significantly outperforms existing embedding\ncompression methods. Remarkably, MPE achieves about 200x compression on the\nCriteo dataset without comprising the prediction accuracy.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Scalability", "Memory-Efficiency", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [28.279699325561523, 15.853300094604492], "cluster": 2}, {"key": "li2024t2vindexer", "year": "2024", "citations": "0", "title": "T2vindexer: A Generative Video Indexer For Efficient Text-video Retrieval", "abstract": "<p>Current text-video retrieval methods mainly rely on cross-modal matching\nbetween queries and videos to calculate their similarity scores, which are then\nsorted to obtain retrieval results. This method considers the matching between\neach candidate video and the query, but it incurs a significant time cost and\nwill increase notably with the increase of candidates. Generative models are\ncommon in natural language processing and computer vision, and have been\nsuccessfully applied in document retrieval, but their application in multimodal\nretrieval remains unexplored. To enhance retrieval efficiency, in this paper,\nwe introduce a model-based video indexer named T2VIndexer, which is a\nsequence-to-sequence generative model directly generating video identifiers and\nretrieving candidate videos with constant time complexity. T2VIndexer aims to\nreduce retrieval time while maintaining high accuracy. To achieve this goal, we\npropose video identifier encoding and query-identifier augmentation approaches\nto represent videos as short sequences while preserving their semantic\ninformation. Our method consistently enhances the retrieval efficiency of\ncurrent state-of-the-art models on four standard datasets. It enables baselines\nwith only 30%-50% of the original retrieval time to achieve better retrieval\nperformance on MSR-VTT (+1.0%), MSVD (+1.8%), ActivityNet (+1.5%), and DiDeMo\n(+0.2%). The code is available at\nhttps://github.com/Lilidamowang/T2VIndexer-generativeSearch.</p>\n", "tags": ["Text-Retrieval", "Efficiency", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-39.042686462402344, -29.93328094482422], "cluster": 5}, {"key": "li2025deep", "year": "2017", "citations": "171", "title": "Deep Supervised Discrete Hashing", "abstract": "<p>With the rapid growth of image and video data on the web, hashing has been\nextensively studied for image or video search in recent years. Benefiting from\nrecent advances in deep learning, deep hashing methods have achieved promising\nresults for image retrieval. However, there are some limitations of previous deep\nhashing methods (e.g., the semantic information is not fully exploited). In this\npaper, we develop a deep supervised discrete hashing algorithm based on the\nassumption that the learned binary codes should be ideal for classification. Both the\npairwise label information and the classification information are used to learn the\nhash codes within one stream framework. We constrain the outputs of the last layer\nto be binary codes directly, which is rarely investigated in deep hashing algorithm.\nBecause of the discrete nature of hash codes, an alternating minimization method\nis used to optimize the objective function. Experimental results have shown that\nour method outperforms current state-of-the-art methods on benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-7.0845232009887695, -1.285170316696167], "cluster": 1}, {"key": "li2025feature", "year": "2015", "citations": "510", "title": "Feature Learning Based Deep Supervised Hashing With Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of\nhashing for large-scale image retrieval. However,\nmost existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep\nhashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown\nbetter performance than traditional hashing methods with hand-crafted features. Most of these deep\nhashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this\npaper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH),\nto perform simultaneous feature learning and hashcode learning for applications with pairwise labels.\nExperiments on real datasets show that our DPSH\nmethod can outperform other methods to achieve\nthe state-of-the-art performance in image retrieval\napplications.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-4.269804954528809, -5.191390037536621], "cluster": 1}, {"key": "li2025learning", "year": "2013", "citations": "93", "title": "Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming\nan increasingly important tool in solving\nmany large-scale problems. Recently\na number of approaches to learning datadependent\nhash functions have been developed.\nIn this work, we propose a column\ngeneration based method for learning datadependent\nhash functions on the basis of\nproximity comparison information. Given a\nset of triplets that encode the pairwise proximity\ncomparison information, our method\nlearns hash functions that preserve the relative\ncomparison relationships in the data\nas well as possible within the large-margin\nlearning framework. The learning procedure\nis implemented using column generation and\nhence is named CGHash. At each iteration\nof the column generation procedure, the best\nhash function is selected. Unlike most other\nhashing methods, our method generalizes to\nnew data points naturally; and has a training\nobjective which is convex, thus ensuring\nthat the global optimum can be identi-\nfied. Experiments demonstrate that the proposed\nmethod learns compact binary codes\nand that its retrieval performance compares\nfavorably with state-of-the-art methods when\ntested on a few benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [25.14874267578125, -3.914707899093628], "cluster": 6}, {"key": "li2025lightweight", "year": "2025", "citations": "1", "title": "Lightweight Contrastive Distilled Hashing For Online Cross-modal Retrieval", "abstract": "<p>Deep online cross-modal hashing has gained much attention from researchers\nrecently, as its promising applications with low storage requirement, fast\nretrieval efficiency and cross modality adaptive, etc. However, there still\nexists some technical hurdles that hinder its applications, e.g., 1) how to\nextract the coexistent semantic relevance of cross-modal data, 2) how to\nachieve competitive performance when handling the real time data streams, 3)\nhow to transfer the knowledge learned from offline to online training in a\nlightweight manner. To address these problems, this paper proposes a\nlightweight contrastive distilled hashing (LCDH) for cross-modal retrieval, by\ninnovatively bridging the offline and online cross-modal hashing by similarity\nmatrix approximation in a knowledge distillation framework. Specifically, in\nthe teacher network, LCDH first extracts the cross-modal features by the\ncontrastive language-image pre-training (CLIP), which are further fed into an\nattention module for representation enhancement after feature fusion. Then, the\noutput of the attention module is fed into a FC layer to obtain hash codes for\naligning the sizes of similarity matrices for online and offline training. In\nthe student network, LCDH extracts the visual and textual features by\nlightweight models, and then the features are fed into a FC layer to generate\nbinary codes. Finally, by approximating the similarity matrices, the\nperformance of online hashing in the lightweight student network can be\nenhanced by the supervision of coexistent semantic relevance that is distilled\nfrom the teacher network. Experimental results on three widely used datasets\ndemonstrate that LCDH outperforms some state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "AAAI", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [26.247636795043945, -38.59839630126953], "cluster": 7}, {"key": "li2025neighborhood", "year": "2019", "citations": "40", "title": "Neighborhood Preserving Hashing For Scalable Video Retrieval", "abstract": "<p>In this paper, we propose a Neighborhood Preserving\nHashing (NPH) method for scalable video retrieval in an\nunsupervised manner. Unlike most existing deep video\nhashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal\nneighborhood information into the encoding network such\nthat the neighborhood-relevant visual content of a video can\nbe preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses\non partial useful content of each input frame conditioned\non the neighborhood information. We then integrate the\nneighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the\nlearned hashing functions can map similar videos to similar\nbinary codes. Extensive experiments on three widely-used\nbenchmark datasets validate the effectiveness of our proposed approach.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Datasets", "Compact-Codes", "Evaluation", "Video-Retrieval", "Unsupervised"], "tsne_embedding": [-19.168027877807617, 30.951435089111328], "cluster": 8}, {"key": "li2025push", "year": "2019", "citations": "7", "title": "Push For Quantization: Deep Fisher Hashing", "abstract": "<p>Current massive datasets demand light-weight access for analysis. Discrete hashing methods are thus beneficial because they map high-dimensional data to compact binary codes that are efficient to store and process, while preserving semantic similarity. To optimize powerful deep learning methods for image hashing, gradient-based methods are required. Binary codes, however, are discrete and thus have no continuous derivatives. Relaxing the problem by solving it in a continuous space and then quantizing the solution is not guaranteed to yield separable binary codes. The quantization needs to be included in the optimization. In this paper we push for quantization: We optimize maximum class separability in the binary space. We introduce a margin on distances between dissimilar image pairs as measured in the binary space. In addition to pair-wise distances, we draw inspiration from Fisher\u2019s Linear Discriminant Analysis (Fisher LDA) to maximize the binary distances between classes and at the same time minimize the binary distance of images within the same class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate compact codes comparing favorably to the current state of the art.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-6.192944526672363, 34.90970993041992], "cluster": 8}, {"key": "li2025self", "year": "2021", "citations": "38", "title": "Self-supervised Video Hashing Via Bidirectional Transformers", "abstract": "<p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Compact-Codes", "CVPR", "Datasets", "Supervised", "Evaluation", "Video-Retrieval", "Unsupervised"], "tsne_embedding": [-9.322446823120117, -0.9040074944496155], "cluster": 1}, {"key": "li2025very", "year": "2006", "citations": "634", "title": "Very Sparse Random Projections", "abstract": "<p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2\u221aD, 1-1\u221aD, 1/2\u221aD for achieving a significant \u221aD-fold speedup, with little loss in accuracy.</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "KDD"], "tsne_embedding": [5.53079080581665, 42.18482208251953], "cluster": 4}, {"key": "lian2021quotient", "year": "2021", "citations": "1", "title": "Quotient Space-based Keyword Retrieval In Sponsored Search", "abstract": "<p>Synonymous keyword retrieval has become an important problem for sponsored\nsearch ever since major search engines relax the exact match product\u2019s matching\nrequirement to a synonymous level. Since the synonymous relations between\nqueries and keywords are quite scarce, the traditional information retrieval\nframework is inefficient in this scenario. In this paper, we propose a novel\nquotient space-based retrieval framework to address this problem. Considering\nthe synonymy among keywords as a mathematical equivalence relation, we can\ncompress the synonymous keywords into one representative, and the corresponding\nquotient space would greatly reduce the size of the keyword repository. Then an\nembedding-based retrieval is directly conducted between queries and the keyword\nrepresentatives. To mitigate the semantic gap of the quotient space-based\nretrieval, a single semantic siamese model is utilized to detect both the\nkeyword\u2013keyword and query-keyword synonymous relations. The experiments show\nthat with our quotient space-based retrieval method, the synonymous keyword\nretrieving performance can be greatly improved in terms of memory cost and\nrecall efficiency. This method has been successfully implemented in Baidu\u2019s\nonline sponsored search system and has yielded a significant improvement in\nrevenue.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [5.750290393829346, -0.019294552505016327], "cluster": 6}, {"key": "liang2016optimizing", "year": "2016", "citations": "52", "title": "Optimizing Top Precision Performance Measure Of Content-based Image Retrieval By Learning Similarity Function", "abstract": "<p>In this paper we study the problem of content-based image retrieval. In this\nproblem, the most popular performance measure is the top precision measure, and\nthe most important component of a retrieval system is the similarity function\nused to compare a query image against a database image. However, up to now,\nthere is no existing similarity learning method proposed to optimize the top\nprecision measure. To fill this gap, in this paper, we propose a novel\nsimilarity learning method to maximize the top precision measure. We model this\nproblem as a minimization problem with an objective function as the combination\nof the losses of the relevant images ranked behind the top-ranked irrelevant\nimage, and the squared Frobenius norm of the similarity function parameter.\nThis minimization problem is solved as a quadratic programming problem. The\nexperiments over two benchmark data sets show the advantages of the proposed\nmethod over other similarity learning methods when the top precision is used as\nthe performance measure.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-6.250809669494629, 22.677383422851562], "cluster": 8}, {"key": "liang2020dynamic", "year": "2021", "citations": "5", "title": "Dynamic Sampling For Deep Metric Learning", "abstract": "<p>Deep metric learning maps visually similar images onto nearby locations and\nvisually dissimilar images apart from each other in an embedding manifold. The\nlearning process is mainly based on the supplied image negative and positive\ntraining pairs. In this paper, a dynamic sampling strategy is proposed to\norganize the training pairs in an easy-to-hard order to feed into the network.\nIt allows the network to learn general boundaries between categories from the\neasy training pairs at its early stages and finalize the details of the model\nmainly relying on the hard training samples in the later. Compared to the\nexisting training sample mining approaches, the hard samples are mined with\nlittle harm to the learned general model. This dynamic sampling strategy is\nformularized as two simple terms that are compatible with various loss\nfunctions. Consistent performance boost is observed when it is integrated with\nseveral popular loss functions on fashion search, fine-grained classification,\nand person re-identification tasks.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-11.602940559387207, -23.231327056884766], "cluster": 3}, {"key": "liang2020embedding", "year": "2020", "citations": "23", "title": "Embedding-based Zero-shot Retrieval Through Query Generation", "abstract": "<p>Passage retrieval addresses the problem of locating relevant passages,\nusually from a large corpus, given a query. In practice, lexical term-matching\nalgorithms like BM25 are popular choices for retrieval owing to their\nefficiency. However, term-based matching algorithms often miss relevant\npassages that have no lexical overlap with the query and cannot be finetuned to\ndownstream datasets. In this work, we consider the embedding-based two-tower\narchitecture as our neural retrieval model. Since labeled data can be scarce\nand because neural retrieval models require vast amounts of data to train, we\npropose a novel method for generating synthetic training data for retrieval.\nOur system produces remarkable results, significantly outperforming BM25 on 5\nout of 6 datasets tested, by an average of 2.45 points for Recall@1. In some\ncases, our model trained on synthetic data can even outperform the same model\ntrained on real data</p>\n", "tags": ["Efficiency", "Evaluation", "Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [33.444602966308594, -15.242722511291504], "cluster": 7}, {"key": "liang2021dynamic", "year": "2021", "citations": "5", "title": "Dynamic Sampling For Deep Metric Learning", "abstract": "<p>Deep metric learning maps visually similar images onto nearby locations and\nvisually dissimilar images apart from each other in an embedding manifold. The\nlearning process is mainly based on the supplied image negative and positive\ntraining pairs. In this paper, a dynamic sampling strategy is proposed to\norganize the training pairs in an easy-to-hard order to feed into the network.\nIt allows the network to learn general boundaries between categories from the\neasy training pairs at its early stages and finalize the details of the model\nmainly relying on the hard training samples in the later. Compared to the\nexisting training sample mining approaches, the hard samples are mined with\nlittle harm to the learned general model. This dynamic sampling strategy is\nformularized as two simple terms that are compatible with various loss\nfunctions. Consistent performance boost is observed when it is integrated with\nseveral popular loss functions on fashion search, fine-grained classification,\nand person re-identification tasks.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-11.602774620056152, -23.231645584106445], "cluster": 3}, {"key": "liang2023learning", "year": "2023", "citations": "2", "title": "Learning Compact Compositional Embeddings Via Regularized Pruning For Recommendation", "abstract": "<p>Latent factor models are the dominant backbones of contemporary recommender\nsystems (RSs) given their performance advantages, where a unique vector\nembedding with a fixed dimensionality (e.g., 128) is required to represent each\nentity (commonly a user/item). Due to the large number of users and items on\ne-commerce sites, the embedding table is arguably the least memory-efficient\ncomponent of RSs. For any lightweight recommender that aims to efficiently\nscale with the growing size of users/items or to remain applicable in\nresource-constrained settings, existing solutions either reduce the number of\nembeddings needed via hashing, or sparsify the full embedding table to switch\noff selected embedding dimensions. However, as hash collision arises or\nembeddings become overly sparse, especially when adapting to a tighter memory\nbudget, those lightweight recommenders inevitably have to compromise their\naccuracy. To this end, we propose a novel compact embedding framework for RSs,\nnamely Compositional Embedding with Regularized Pruning (CERP). Specifically,\nCERP represents each entity by combining a pair of embeddings from two\nindependent, substantially smaller meta-embedding tables, which are then\njointly pruned via a learnable element-wise threshold. In addition, we\ninnovatively design a regularized pruning mechanism in CERP, such that the two\nsparsified meta-embedding tables are encouraged to encode information that is\nmutually complementary. Given the compatibility with agnostic latent factor\nmodels, we pair CERP with two popular recommendation models for extensive\nexperiments, where results on two real-world datasets under different memory\nbudgets demonstrate its superiority against state-of-the-art baselines. The\ncodebase of CERP is available in https://github.com/xurong-liang/CERP.</p>\n", "tags": ["Hashing-Methods", "Recommender-Systems", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [31.334993362426758, 16.67578125], "cluster": 2}, {"key": "liang2024unify", "year": "2024", "citations": "0", "title": "UNIFY: Unified Index For Range Filtered Approximate Nearest Neighbors Search", "abstract": "<p>This paper presents an efficient and scalable framework for Range Filtered Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors associated with attribute values. Given a query vector \\(q\\) and a range \\([l, h]\\), RF-ANNS aims to find the approximate \\(k\\) nearest neighbors of \\(q\\) among data whose attribute values fall within \\([l, h]\\). Existing methods including pre-, post-, and hybrid filtering strategies that perform attribute range filtering before, after, or during the ANNS process, all suffer from significant performance degradation when query ranges shift. Though building dedicated indexes for each strategy and selecting the best one based on the query range can address this problem, it leads to index consistency and maintenance issues.\n  Our framework, called UNIFY, constructs a unified Proximity Graph-based (PG-based) index that seamlessly supports all three strategies. In UNIFY, we introduce SIG, a novel Segmented Inclusive Graph, which segments the dataset by attribute values. It ensures the PG of objects from any segment combinations is a sub-graph of SIG, thereby enabling efficient hybrid filtering by reconstructing and searching a PG from relevant segments. Moreover, we present Hierarchical Segmented Inclusive Graph (HSIG), a variant of SIG which incorporates a hierarchical structure inspired by HNSW to achieve logarithmic hybrid filtering complexity. We also implement pre- and post-filtering for HSIG by fusing skip list connections and compressed HNSW edges into the hierarchical graph. Experimental results show that UNIFY delivers state-of-the-art RF-ANNS performance across small, mid, and large query ranges.</p>\n", "tags": ["Graph-Based-Ann", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [45.862762451171875, -4.485228061676025], "cluster": 9}, {"key": "liao2018triplet", "year": "2017", "citations": "32", "title": "Triplet-based Deep Similarity Learning For Person Re-identification", "abstract": "<p>In recent years, person re-identification (re-id) catches great attention in\nboth computer vision community and industry. In this paper, we propose a new\nframework for person re-identification with a triplet-based deep similarity\nlearning using convolutional neural networks (CNNs). The network is trained\nwith triplet input: two of them have the same class labels and the other one is\ndifferent. It aims to learn the deep feature representation, with which the\ndistance within the same class is decreased, while the distance between the\ndifferent classes is increased as much as possible. Moreover, we trained the\nmodel jointly on six different datasets, which differs from common practice -\none model is just trained on one dataset and tested also on the same one.\nHowever, the enormous number of possible triplet data among the large number of\ntraining samples makes the training impossible. To address this challenge, a\ndouble-sampling scheme is proposed to generate triplets of images as effective\nas possible. The proposed framework is evaluated on several benchmark datasets.\nThe experimental results show that, our method is effective for the task of\nperson re-identification and it is comparable or even outperforms the\nstate-of-the-art methods.</p>\n", "tags": ["ICCV", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [-24.89992332458496, -14.542977333068848], "cluster": 5}, {"key": "liao2020embedding", "year": "2020", "citations": "4", "title": "Embedding Compression With Isotropic Iterative Quantization", "abstract": "<p>Continuous representation of words is a standard component in deep\nlearning-based NLP models. However, representing a large vocabulary requires\nsignificant memory, which can cause problems, particularly on\nresource-constrained platforms. Therefore, in this paper we propose an\nisotropic iterative quantization (IIQ) approach for compressing embedding\nvectors into binary ones, leveraging the iterative quantization technique well\nestablished for image retrieval, while satisfying the desired isotropic\nproperty of PMI based models. Experiments with pre-trained embeddings (i.e.,\nGloVe and HDC) demonstrate a more than thirty-fold compression ratio with\ncomparable and sometimes even improved performance over the original\nreal-valued embedding vectors.</p>\n", "tags": ["AAAI", "Evaluation", "Image-Retrieval", "Quantization"], "tsne_embedding": [-0.010893099941313267, -21.299036026000977], "cluster": 7}, {"key": "liao2022supervised", "year": "2022", "citations": "1", "title": "Supervised Metric Learning To Rank For Retrieval Via Contextual Similarity Optimization", "abstract": "<p>There is extensive interest in metric learning methods for image retrieval.\nMany metric learning loss functions focus on learning a correct ranking of\ntraining samples, but strongly overfit semantically inconsistent labels and\nrequire a large amount of data. To address these shortcomings, we propose a new\nmetric learning method, called contextual loss, which optimizes contextual\nsimilarity in addition to cosine similarity. Our contextual loss implicitly\nenforces semantic consistency among neighbors while converging to the correct\nranking. We empirically show that the proposed loss is more robust to label\nnoise, and is less prone to overfitting even when a large portion of train data\nis withheld. Extensive experiments demonstrate that our method achieves a new\nstate-of-the-art across four image retrieval benchmarks and multiple different\nevaluation settings. Code is available at:\nhttps://github.com/Chris210634/metric-learning-using-contextual-similarity</p>\n", "tags": ["Supervised", "Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-15.940288543701172, -16.674230575561523], "cluster": 1}, {"key": "liao2023item", "year": "2023", "citations": "0", "title": "Item: Unsupervised Image-text Embedding Learning For Ecommerce", "abstract": "<p>Product embedding serves as a cornerstone for a wide range of applications in\neCommerce. The product embedding learned from multiple modalities shows\nsignificant improvement over that from a single modality, since different\nmodalities provide complementary information. However, some modalities are more\ninformatively dominant than others. How to teach a model to learn embedding\nfrom different modalities without neglecting information from the less dominant\nmodality is challenging. We present an image-text embedding model (ITEm), an\nunsupervised learning method that is designed to better attend to image and\ntext modalities. We extend BERT by (1) learning an embedding from text and\nimage without knowing the regions of interest; (2) training a global\nrepresentation to predict masked words and to construct masked image patches\nwithout their individual representations. We evaluate the pre-trained ITEm on\ntwo tasks: the search for extremely similar products and the prediction of\nproduct categories, showing substantial gains compared to strong baseline\nmodels.</p>\n", "tags": ["Unsupervised"], "tsne_embedding": [-34.71615219116211, -4.522017478942871], "cluster": 0}, {"key": "liberman2019search", "year": "2019", "citations": "0", "title": "Search-based Serving Architecture Of Embeddings-based Recommendations", "abstract": "<p>Over the past 10 years, many recommendation techniques have been based on\nembedding users and items in latent vector spaces, where the inner product of a\n(user,item) pair of vectors represents the predicted affinity of the user to\nthe item. A wealth of literature has focused on the various modeling approaches\nthat result in embeddings, and has compared their quality metrics, learning\ncomplexity, etc. However, much less attention has been devoted to the issues\nsurrounding productization of an embeddings-based high throughput, low latency\nrecommender system. In particular, how the system might keep up with the\nchanging embeddings as new models are learnt. This paper describes a reference\narchitecture of a high-throughput, large scale recommendation service which\nleverages a search engine as its runtime core. We describe how the search index\nand the query builder adapt to changes in the embeddings, which often happen at\na different cadence than index builds. We provide solutions for both id-based\nand feature-based embeddings, as well as for batch indexing and incremental\nindexing setups. The described system is at the core of a Web content discovery\nservice that serves tens of billions recommendations per day in response to\nbillions of user requests.</p>\n", "tags": ["Efficiency", "Recommender-Systems"], "tsne_embedding": [24.733482360839844, -27.348148345947266], "cluster": 7}, {"key": "lillis2017hierarchical", "year": "2018", "citations": "5", "title": "Hierarchical Bloom Filter Trees For Approximate Matching", "abstract": "<p>Bytewise approximate matching algorithms have in recent years shown\nsignificant promise in de- tecting files that are similar at the byte level.\nThis is very useful for digital forensic investigators, who are regularly faced\nwith the problem of searching through a seized device for pertinent data. A\ncommon scenario is where an investigator is in possession of a collection of\n\u201cknown-illegal\u201d files (e.g. a collection of child abuse material) and wishes to\nfind whether copies of these are stored on the seized device. Approximate\nmatching addresses shortcomings in traditional hashing, which can only find\nidentical files, by also being able to deal with cases of merged files,\nembedded files, partial files, or if a file has been changed in any way.\n  Most approximate matching algorithms work by comparing pairs of files, which\nis not a scalable approach when faced with large corpora. This paper\ndemonstrates the effectiveness of using a \u201cHierarchical Bloom Filter Tree\u201d\n(HBFT) data structure to reduce the running time of\ncollection-against-collection matching, with a specific focus on the MRSH-v2\nalgorithm. Three experiments are discussed, which explore the effects of\ndifferent configurations of HBFTs. The proposed approach dramatically reduces\nthe number of pairwise comparisons required, and demonstrates substantial speed\ngains, while maintaining effectiveness.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [7.421070098876953, 5.687829971313477], "cluster": 6}, {"key": "lin2013general", "year": "2013", "citations": "199", "title": "A General Two-step Approach To Learning-based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which\nis typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to\nrespond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose\na flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.\nThis framework allows a number of existing approaches to hashing to be placed in context, and simplifies the\ndevelopment of new problem-specific hashing methods. Our framework decomposes hashing learning problem\ninto two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically\nbe formulated as binary quadratic problems, and the second step can be accomplished by training standard binary\nclassifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate\nthat the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>\n", "tags": ["ICCV", "Tools-&-Libraries", "Hashing-Methods"], "tsne_embedding": [25.039894104003906, -1.049882411956787], "cluster": 6}, {"key": "lin2014fast", "year": "2014", "citations": "453", "title": "Fast Supervised Hashing With Decision Trees For High-dimensional Data", "abstract": "<p>Supervised hashing aims to map the original features to\ncompact binary codes that are able to preserve label based\nsimilarity in the Hamming space. Non-linear hash functions\nhave demonstrated their advantage over linear ones due to\ntheir powerful generalization capability. In the literature,\nkernel functions are typically used to achieve non-linearity\nin hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.\nHere we propose to use boosted decision trees for achieving\nnon-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional\ndata. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem\nand an efficient GraphCut based block search method for\nsolving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the binary\ncodes. Experiments demonstrate that our proposed method\nsignificantly outperforms most state-of-the-art methods in\nretrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster\nthan many methods in terms of training time.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Scalability", "Supervised", "Evaluation"], "tsne_embedding": [23.226083755493164, 2.2628626823425293], "cluster": 6}, {"key": "lin2014optimizing", "year": "2014", "citations": "23", "title": "Optimizing Ranking Measures For Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.\nThe resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [40.724082946777344, 14.226917266845703], "cluster": 2}, {"key": "lin2015deep", "year": "2015", "citations": "626", "title": "Deep Learning Of Binary Hash Codes For Fast Image Retrieval", "abstract": "<p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.\nhe utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "CVPR", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [-54.042625427246094, 5.1584343910217285], "cluster": 0}, {"key": "lin2015semantics", "year": "2015", "citations": "540", "title": "Semantics-preserving Hashing For Cross-view Retrieval", "abstract": "<p>With benefits of low storage costs and high query speeds,\nhashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.\nIn this paper, we study the problem of cross-view retrieval\nand propose an effective Semantics-Preserving Hashing\nmethod, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them\ninto a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the\nKullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt\nhash codes. And for any unseen instance, predicted hash\ncodes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,\nusing a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Scalability", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [18.23211097717285, -5.719451427459717], "cluster": 6}, {"key": "lin2016structured", "year": "2016", "citations": "0", "title": "Structured Learning Of Binary Codes With Column Generation", "abstract": "<p>Hashing methods aim to learn a set of hash functions which map the original\nfeatures to compact binary codes with similarity preserving in the Hamming\nspace. Hashing has proven a valuable tool for large-scale information\nretrieval. We propose a column generation based binary code learning framework\nfor data-dependent hash function learning. Given a set of triplets that encode\nthe pairwise similarity comparison information, our column generation based\nmethod learns hash functions that preserve the relative comparison relations\nwithin the large-margin learning framework. Our method iteratively learns the\nbest hash functions during the column generation procedure. Existing hashing\nmethods optimize over simple objectives such as the reconstruction error or\ngraph Laplacian related loss functions, instead of the performance evaluation\ncriteria of interest\u2014multivariate performance measures such as the AUC and\nNDCG. Our column generation based method can be further generalized from the\ntriplet loss to a general structured learning based framework that allows one\nto directly optimize multivariate performance measures. For optimizing general\nranking measures, the resulting optimization problem can involve exponentially\nor infinitely many variables and constraints, which is more challenging than\nstandard structured output learning. We use a combination of column generation\nand cutting-plane techniques to solve the optimization problem. To speed-up the\ntraining we further explore stage-wise training and propose to use a simplified\nNDCG loss for efficient inference. We demonstrate the generality of our method\nby applying it to ranking prediction and image retrieval, and show that it\noutperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [15.27088451385498, -2.413865327835083], "cluster": 6}, {"key": "lin2018learning", "year": "2018", "citations": "7", "title": "Learning A Disentangled Embedding For Monocular 3D Shape Retrieval And Pose Estimation", "abstract": "<p>We propose a novel approach to jointly perform 3D shape retrieval and pose\nestimation from monocular images.In order to make the method robust to\nreal-world image variations, e.g. complex textures and backgrounds, we learn an\nembedding space from 3D data that only includes the relevant information,\nnamely the shape and pose. Our approach explicitly disentangles a shape vector\nand a pose vector, which alleviates both pose bias for 3D shape retrieval and\ncategorical bias for pose estimation. We then train a CNN to map the images to\nthis embedding space, and then retrieve the closest 3D shape from the database\nand estimate the 6D pose of the object. Our method achieves 10.3 median error\nfor pose estimation and 0.592 top-1-accuracy for category agnostic 3D object\nretrieval on the Pascal3D+ dataset, outperforming the previous state-of-the-art\nmethods on both tasks.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-39.79595947265625, -11.654412269592285], "cluster": 5}, {"key": "lin2018supervised", "year": "2018", "citations": "64", "title": "Supervised Online Hashing Via Similarity Distribution Learning", "abstract": "<p>Online hashing has attracted extensive research attention when facing\nstreaming data. Most online hashing methods, learning binary codes based on\npairwise similarities of training instances, fail to capture the semantic\nrelationship, and suffer from a poor generalization in large-scale applications\ndue to large variations. In this paper, we propose to model the similarity\ndistributions between the input data and the hashing codes, upon which a novel\nsupervised online hashing method, dubbed as Similarity Distribution based\nOnline Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship\nin the produced Hamming space. Specifically, we first transform the discrete\nsimilarity matrix into a probability matrix via a Gaussian-based normalization\nto address the extremely imbalanced distribution issue. And then, we introduce\na scaling Student t-distribution to solve the challenging initialization\nproblem, and efficiently bridge the gap between the known and unknown\ndistributions. Lastly, we align the two distributions via minimizing the\nKullback-Leibler divergence (KL-diverence) with stochastic gradient descent\n(SGD), by which an intuitive similarity constraint is imposed to update hashing\nmodel on the new streaming data with a powerful generalizing ability to the\npast data. Extensive experiments on three widely-used benchmarks validate the\nsuperiority of the proposed SDOH over the state-of-the-art methods in the\nonline retrieval task.</p>\n", "tags": ["Compact-Codes", "Supervised", "Hashing-Methods", "Scalability"], "tsne_embedding": [13.552165985107422, -0.9113005995750427], "cluster": 6}, {"key": "lin2019fashion", "year": "2020", "citations": "81", "title": "Fashion Outfit Complementary Item Retrieval", "abstract": "<p>Complementary fashion item recommendation is critical for fashion outfit\ncompletion. Existing methods mainly focus on outfit compatibility prediction\nbut not in a retrieval setting. We propose a new framework for outfit\ncomplementary item retrieval. Specifically, a category-based subspace attention\nnetwork is presented, which is a scalable approach for learning the subspace\nattentions. In addition, we introduce an outfit ranking loss that better models\nthe item relationships of an entire outfit. We evaluate our method on the\noutfit compatibility, FITB and new retrieval tasks. Experimental results\ndemonstrate that our approach outperforms state-of-the-art methods in both\ncompatibility prediction and complementary item retrieval</p>\n", "tags": ["Tools-&-Libraries", "Recommender-Systems", "CVPR"], "tsne_embedding": [-20.837665557861328, -47.38508605957031], "cluster": 3}, {"key": "lin2019hadamard", "year": "2020", "citations": "41", "title": "Hadamard Matrix Guided Online Hashing", "abstract": "<p>Online image hashing has attracted increasing research attention recently,\nwhich receives large-scale data in a streaming manner to update the hash\nfunctions on-the-fly. Its key challenge lies in the difficulty of balancing the\nlearning timeliness and model accuracy. To this end, most works follow a\nsupervised setting, i.e., using class labels to boost the hashing performance,\nwhich defects in two aspects: First, strong constraints, e.g., orthogonal or\nsimilarity preserving, are used, which however are typically relaxed and lead\nto large accuracy drop. Second, large amounts of training batches are required\nto learn the up-to-date hash functions, which largely increase the learning\ncomplexity. To handle the above challenges, a novel supervised online hashing\nscheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this\npaper. Our key innovation lies in introducing Hadamard matrix, which is an\northogonal binary matrix built via Sylvester method. In particular, to release\nthe need of strong constraints, we regard each column of Hadamard matrix as the\ntarget code for each class label, which by nature satisfies several desired\nproperties of hashing codes. To accelerate the online training, LSH is first\nadopted to align the lengths of target code and to-be-learned binary code. We\nthen treat the learning of hash functions as a set of binary classification\nproblems to fit the assigned target code. Finally, extensive experiments\ndemonstrate the superior accuracy and efficiency of the proposed method over\nvarious state-of-the-art methods. Codes are available at\nhttps://github.com/lmbxmu/mycode.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [26.10819435119629, 2.049232244491577], "cluster": 6}, {"key": "lin2019situating", "year": "2019", "citations": "4", "title": "Situating Sentence Embedders With Nearest Neighbor Overlap", "abstract": "<p>As distributed approaches to natural language semantics have developed and\ndiversified, embedders for linguistic units larger than words have come to play\nan increasingly important role. To date, such embedders have been evaluated\nusing benchmark tasks (e.g., GLUE) and linguistic probes. We propose a\ncomparative approach, nearest neighbor overlap (N2O), that quantifies\nsimilarity between embedders in a task-agnostic manner. N2O requires only a\ncollection of examples and is simple to understand: two embedders are more\nsimilar if, for the same set of inputs, there is greater overlap between the\ninputs\u2019 nearest neighbors. Though applicable to embedders of texts of any size,\nwe focus on sentence embedders and use N2O to show the effects of different\ndesign choices and architectures.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-3.7716615200042725, -36.14128494262695], "cluster": 3}, {"key": "lin2019supervised", "year": "2018", "citations": "64", "title": "Supervised Online Hashing Via Hadamard Codebook Learning", "abstract": "<p>In recent years, binary code learning, a.k.a hashing, has received extensive\nattention in large-scale multimedia retrieval. It aims to encode\nhigh-dimensional data points to binary codes, hence the original\nhigh-dimensional metric space can be efficiently approximated via Hamming\nspace. However, most existing hashing methods adopted offline batch learning,\nwhich is not suitable to handle incremental datasets with streaming data or new\ninstances. In contrast, the robustness of the existing online hashing remains\nas an open problem, while the embedding of supervised/semantic information\nhardly boosts the performance of the online hashing, mainly due to the defect\nof unknown category numbers in supervised learning. In this paper, we proposed\nan online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH),\nwhich aims to solve the above problems towards robust and supervised online\nhashing. In particular, we first assign an appropriate high-dimensional binary\ncodes to each class label, which is generated randomly by Hadamard codes to\neach class label, which is generated randomly by Hadamard codes. Subsequently,\nLSH is adopted to reduce the length of such Hadamard codes in accordance with\nthe hash bits, which can adapt the predefined binary codes online, and\ntheoretically guarantee the semantic similarity. Finally, we consider the\nsetting of stochastic data acquisition, which facilitates our method to\nefficiently learn the corresponding hashing functions via stochastic gradient\ndescend (SGD) online. Notably, the proposed HCOH can be embedded with\nsupervised labels and it not limited to a predefined category number. Extensive\nexperiments on three widely-used benchmarks demonstrate the merits of the\nproposed scheme over the state-of-the-art methods. The code is available at\nhttps://github.com/lmbxmu/mycode/tree/master/2018ACMMM_HCOH.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Compact-Codes", "Scalability", "Robustness", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [18.179969787597656, 0.5860827565193176], "cluster": 6}, {"key": "lin2019towards", "year": "2019", "citations": "50", "title": "Towards Optimal Discrete Online Hashing With Balanced Similarity", "abstract": "<p>When facing large-scale image datasets, online hashing serves as a promising\nsolution for online retrieval and prediction tasks. It encodes the online\nstreaming data into compact binary codes, and simultaneously updates the hash\nfunctions to renew codes of the existing dataset. To this end, the existing\nmethods update hash functions solely based on the new data batch, without\ninvestigating the correlation between such new data and the existing dataset.\nIn addition, existing works update the hash functions using a relaxation\nprocess in its corresponding approximated continuous space. And it remains as\nan open problem to directly apply discrete optimizations in online hashing. In\nthis paper, we propose a novel supervised online hashing method, termed\nBalanced Similarity for Online Discrete Hashing (BSODH), to solve the above\nproblems in a unified framework. BSODH employs a well-designed hashing\nalgorithm to preserve the similarity between the streaming data and the\nexisting dataset via an asymmetric graph regularization. We further identify\nthe \u201cdata-imbalance\u201d problem brought by the constructed asymmetric graph, which\nrestricts the application of discrete optimization in our problem. Therefore, a\nnovel balanced similarity is further proposed, which uses two equilibrium\nfactors to balance the similar and dissimilar weights and eventually enables\nthe usage of discrete optimizations. Extensive experiments conducted on three\nwidely-used benchmarks demonstrate the advantages of the proposed method over\nthe state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Scalability", "AAAI", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [17.333330154418945, 9.19225025177002], "cluster": 6}, {"key": "lin2020fast", "year": "2020", "citations": "22", "title": "Fast Class-wise Updating For Online Hashing", "abstract": "<p>Online image hashing has received increasing research attention recently,\nwhich processes large-scale data in a streaming fashion to update the hash\nfunctions on-the-fly. To this end, most existing works exploit this problem\nunder a supervised setting, i.e., using class labels to boost the hashing\nperformance, which suffers from the defects in both adaptivity and efficiency:\nFirst, large amounts of training batches are required to learn up-to-date hash\nfunctions, which leads to poor online adaptivity. Second, the training is\ntime-consuming, which contradicts with the core need of online learning. In\nthis paper, a novel supervised online hashing scheme, termed Fast Class-wise\nUpdating for Online Hashing (FCOH), is proposed to address the above two\nchallenges by introducing a novel and efficient inner product operation. To\nachieve fast online adaptivity, a class-wise updating method is developed to\ndecompose the binary code learning and alternatively renew the hash functions\nin a class-wise fashion, which well addresses the burden on large amounts of\ntraining batches. Quantitatively, such a decomposition further leads to at\nleast 75% storage saving. To further achieve online efficiency, we propose a\nsemi-relaxation optimization, which accelerates the online training by treating\ndifferent binary constraints independently. Without additional constraints and\nvariables, the time complexity is significantly reduced. Such a scheme is also\nquantitatively shown to well preserve past information during updating hashing\nfunctions. We have quantitatively demonstrated that the collective effort of\nclass-wise updating and semi-relaxation optimization provides a superior\nperformance comparing to various state-of-the-art methods, which is verified\nthrough extensive experiments on three widely-used datasets.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [26.393068313598633, 2.3268022537231445], "cluster": 6}, {"key": "lin2021deep", "year": "2021", "citations": "8", "title": "Deep Self-adaptive Hashing For Image Retrieval", "abstract": "<p>Hashing technology has been widely used in image retrieval due to its\ncomputational and storage efficiency. Recently, deep unsupervised hashing\nmethods have attracted increasing attention due to the high cost of human\nannotations in the real world and the superiority of deep learning technology.\nHowever, most deep unsupervised hashing methods usually pre-compute a\nsimilarity matrix to model the pairwise relationship in the pre-trained feature\nspace. Then this similarity matrix would be used to guide hash learning, in\nwhich most of the data pairs are treated equivalently. The above process is\nconfronted with the following defects: 1) The pre-computed similarity matrix is\ninalterable and disconnected from the hash learning process, which cannot\nexplore the underlying semantic information. 2) The informative data pairs may\nbe buried by the large number of less-informative data pairs. To solve the\naforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model\nto adaptively capture the semantic information with two special designs:\nAdaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC).\nFirstly, we adopt the AND to initially construct a neighborhood-based\nsimilarity matrix, and then refine this initial similarity matrix with a novel\nupdate strategy to further investigate the semantic structure behind the\nlearned representation. Secondly, we measure the priorities of data pairs with\nPIC and assign adaptive weights to them, which is relies on the assumption that\nmore dissimilar data pairs contain more discriminative information for hash\nlearning. Extensive experiments on several datasets demonstrate that the above\ntwo technologies facilitate the deep hashing model to achieve superior\nperformance.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "CIKM", "Efficiency", "Image-Retrieval", "Hashing-Methods", "Supervised", "Unsupervised"], "tsne_embedding": [1.336166501045227, 12.955533981323242], "cluster": 8}, {"key": "lin2021large", "year": "2021", "citations": "14", "title": "Large-scale Network Embedding In Apache Spark", "abstract": "<p>Network embedding has been widely used in social recommendation and network\nanalysis, such as recommendation systems and anomaly detection with graphs.\nHowever, most of previous approaches cannot handle large graphs efficiently,\ndue to that (i) computation on graphs is often costly and (ii) the size of\ngraph or the intermediate results of vectors could be prohibitively large,\nrendering it difficult to be processed on a single machine. In this paper, we\npropose an efficient and effective distributed algorithm for network embedding\non large graphs using Apache Spark, which recursively partitions a graph into\nseveral small-sized subgraphs to capture the internal and external structural\ninformation of nodes, and then computes the network embedding for each subgraph\nin parallel. Finally, by aggregating the outputs on all subgraphs, we obtain\nthe embeddings of nodes in a linear cost. After that, we demonstrate in various\nexperiments that our proposed approach is able to handle graphs with billions\nof edges within a few hours and is at least 4 times faster than the\nstate-of-the-art approaches. Besides, it achieves up to \\(4.25%\\) and \\(4.27%\\)\nimprovements on link prediction and node classification tasks respectively. In\nthe end, we deploy the proposed algorithms in two online games of Tencent with\nthe applications of friend recommendation and item recommendation, which\nimprove the competitors by up to \\(91.11%\\) in running time and up to \\(12.80%\\)\nin the corresponding evaluation metrics.</p>\n", "tags": ["KDD", "Recommender-Systems", "Evaluation", "Scalability"], "tsne_embedding": [55.889530181884766, 1.1355350017547607], "cluster": 9}, {"key": "lin2022deep", "year": "2022", "citations": "17", "title": "Deep Unsupervised Hashing With Latent Semantic Components", "abstract": "<p>Deep unsupervised hashing has been appreciated in the regime of image\nretrieval. However, most prior arts failed to detect the semantic components\nand their relationships behind the images, which makes them lack discriminative\npower. To make up the defect, we propose a novel Deep Semantic Components\nHashing (DSCH), which involves a common sense that an image normally contains a\nbunch of semantic components with homology and co-occurrence relationships.\nBased on this prior, DSCH regards the semantic components as latent variables\nunder the Expectation-Maximization framework and designs a two-step iterative\nalgorithm with the objective of maximum likelihood of training data. Firstly,\nDSCH constructs a semantic component structure by uncovering the fine-grained\nsemantics components of images with a Gaussian Mixture Modal~(GMM), where an\nimage is represented as a mixture of multiple components, and the semantics\nco-occurrence are exploited. Besides, coarse-grained semantics components, are\ndiscovered by considering the homology relationships between fine-grained\ncomponents, and the hierarchy organization is then constructed. Secondly, DSCH\nmakes the images close to their semantic component centers at both fine-grained\nand coarse-grained levels, and also makes the images share similar semantic\ncomponents close to each other. Extensive experiments on three benchmark\ndatasets demonstrate that the proposed hierarchical semantic components indeed\nfacilitate the hashing model to achieve superior performance.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "AAAI", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-1.9281320571899414, 9.642359733581543], "cluster": 8}, {"key": "lin2022dense", "year": "2023", "citations": "8", "title": "A Dense Representation Framework For Lexical And Semantic Matching", "abstract": "<p>Lexical and semantic matching capture different successful approaches to text\nretrieval and the fusion of their results has proven to be more effective and\nrobust than either alone. Prior work performs hybrid retrieval by conducting\nlexical and semantic matching using different systems (e.g., Lucene and Faiss,\nrespectively) and then fusing their model outputs. In contrast, our work\nintegrates lexical representations with dense semantic representations by\ndensifying high-dimensional lexical representations into what we call\nlow-dimensional dense lexical representations (DLRs). Our experiments show that\nDLRs can effectively approximate the original lexical representations,\npreserving effectiveness while improving query latency. Furthermore, we can\ncombine dense lexical and semantic representations to generate dense hybrid\nrepresentations (DHRs) that are more flexible and yield faster retrieval\ncompared to existing hybrid techniques. In addition, we explore it jointly\ntraining lexical and semantic representations in a single model and empirically\nshow that the resulting DHRs are able to combine the advantages of the\nindividual components. Our best DHR model is competitive with state-of-the-art\nsingle-vector and multi-vector dense retrievers in both in-domain and zero-shot\nevaluation settings. Furthermore, our model is both faster and requires smaller\nindexes, making our dense representation framework an attractive approach to\ntext retrieval. Our code is available at https://github.com/castorini/dhr.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Few-Shot-&-Zero-Shot", "Text-Retrieval"], "tsne_embedding": [5.235421180725098, -21.414182662963867], "cluster": 7}, {"key": "lin2023rafic", "year": "2023", "citations": "0", "title": "RAFIC: Retrieval-augmented Few-shot Image Classification", "abstract": "<p>Few-shot image classification is the task of classifying unseen images to one\nof N mutually exclusive classes, using only a small number of training examples\nfor each class. The limited availability of these examples (denoted as K)\npresents a significant challenge to classification accuracy in some cases. To\naddress this, we have developed a method for augmenting the set of K with an\naddition set of A retrieved images. We call this system Retrieval-Augmented\nFew-shot Image Classification (RAFIC). Through a series of experiments, we\ndemonstrate that RAFIC markedly improves performance of few-shot image\nclassification across two challenging datasets. RAFIC consists of two main\ncomponents: (a) a retrieval component which uses CLIP, LAION-5B, and faiss, in\norder to efficiently retrieve images similar to the supplied images, and (b)\nretrieval meta-learning, which learns to judiciously utilize the retrieved\nimages. Code and data is available at github.com/amirziai/rafic.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [-14.723078727722168, -21.919836044311523], "cluster": 1}, {"key": "lin2023searching", "year": "2023", "citations": "0", "title": "Searching Dense Representations With Inverted Indexes", "abstract": "<p>Nearly all implementations of top-\\(k\\) retrieval with dense vector\nrepresentations today take advantage of hierarchical navigable small-world\nnetwork (HNSW) indexes. However, the generation of vector representations and\nefficiently searching large collections of vectors are distinct challenges that\ncan be decoupled. In this work, we explore the contrarian approach of\nperforming top-\\(k\\) retrieval on dense vector representations using inverted\nindexes. We present experiments on the MS MARCO passage ranking dataset,\nevaluating three dimensions of interest: output quality, speed, and index size.\nResults show that searching dense representations using inverted indexes is\npossible. Our approach exhibits reasonable effectiveness with compact indexes,\nbut is impractically slow. Thus, while workable, our solution does not provide\na compelling tradeoff and is perhaps best characterized today as a \u201ctechnical\ncuriosity\u201d.</p>\n", "tags": ["Graph-Based-Ann", "Datasets"], "tsne_embedding": [31.91349220275879, 23.05599594116211], "cluster": 2}, {"key": "lin2024irsc", "year": "2024", "citations": "0", "title": "IRSC: A Zero-shot Evaluation Benchmark For Information Retrieval Through Semantic Comprehension In Retrieval-augmented Generation Scenarios", "abstract": "<p>In Retrieval-Augmented Generation (RAG) tasks using Large Language Models\n(LLMs), the quality of retrieved information is critical to the final output.\nThis paper introduces the IRSC benchmark for evaluating the performance of\nembedding models in multilingual RAG tasks. The benchmark encompasses five\nretrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,\nkeyword retrieval, and summary retrieval. Our research addresses the current\nlack of comprehensive testing and effective comparison methods for embedding\nmodels in RAG scenarios. We introduced new metrics: the Similarity of Semantic\nComprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),\nand evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our\ncontributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and\n3) insights into the cross-lingual limitations of embedding models. The IRSC\nbenchmark aims to enhance the understanding and development of accurate\nretrieval systems in RAG tasks. All code and datasets are available at:\nhttps://github.com/Jasaxion/IRSC_Benchmark</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [1.1454133987426758, -25.548465728759766], "cluster": 3}, {"key": "lin2024mm", "year": "2024", "citations": "0", "title": "Mm-embed: Universal Multimodal Retrieval With Multimodal Llms", "abstract": "<p>State-of-the-art retrieval models typically address a straightforward search\nscenario, in which retrieval tasks are fixed (e.g., finding a passage to answer\na specific question) and only a single modality is supported for both queries\nand retrieved results. This paper introduces techniques for advancing\ninformation retrieval with multimodal large language models (MLLMs), enabling a\nbroader search scenario, termed universal multimodal retrieval, where multiple\nmodalities and diverse retrieval tasks are accommodated. To this end, we first\nstudy fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16\nretrieval tasks. Our empirical results show that the fine-tuned MLLM retriever\nis capable of understanding challenging queries, composed of both text and\nimage, but it underperforms compared to a smaller CLIP retriever in cross-modal\nretrieval tasks due to the modality bias exhibited by MLLMs. To address the\nissue, we propose modality-aware hard negative mining to mitigate the modality\nbias exhibited by MLLM retrievers. Second, we propose continuously fine-tuning\nthe universal multimodal retriever to enhance its text retrieval capability\nwhile preserving multimodal retrieval capability. As a result, our model,\nMM-Embed, achieves state-of-the-art performance on the multimodal retrieval\nbenchmark M-BEIR, which spans multiple domains and tasks, while also surpassing\nthe state-of-the-art text retrieval model, NV-Embed-v1, on the MTEB retrieval\nbenchmark. We also explore prompting the off-the-shelf MLLMs as zero-shot\nrerankers to refine the ranking of the candidates from the multimodal\nretriever. We find that, through prompt-and-reranking, MLLMs can further\nimprove multimodal retrieval when the user queries (e.g., text-image composed\nqueries) are more complex and challenging to understand. These findings also\npave the way for advancing universal multimodal retrieval in the future.</p>\n", "tags": ["Text-Retrieval", "Few-Shot-&-Zero-Shot", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-22.601058959960938, 0.7483013868331909], "cluster": 1}, {"key": "lin2025deep", "year": "2015", "citations": "626", "title": "Deep Learning Of Binary Hash Codes For Fast Image Retrieval", "abstract": "<p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.\nhe utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "CVPR", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [-54.04275131225586, 5.1583709716796875], "cluster": 0}, {"key": "lin2025fast", "year": "2014", "citations": "453", "title": "Fast Supervised Hashing With Decision Trees For High-dimensional Data", "abstract": "<p>Supervised hashing aims to map the original features to\ncompact binary codes that are able to preserve label based\nsimilarity in the Hamming space. Non-linear hash functions\nhave demonstrated their advantage over linear ones due to\ntheir powerful generalization capability. In the literature,\nkernel functions are typically used to achieve non-linearity\nin hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.\nHere we propose to use boosted decision trees for achieving\nnon-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional\ndata. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem\nand an efficient GraphCut based block search method for\nsolving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the binary\ncodes. Experiments demonstrate that our proposed method\nsignificantly outperforms most state-of-the-art methods in\nretrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster\nthan many methods in terms of training time.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Scalability", "Supervised", "Evaluation"], "tsne_embedding": [23.226221084594727, 2.263650894165039], "cluster": 6}, {"key": "lin2025general", "year": "2013", "citations": "199", "title": "A General Two-step Approach To Learning-based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which\nis typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to\nrespond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose\na flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.\nThis framework allows a number of existing approaches to hashing to be placed in context, and simplifies the\ndevelopment of new problem-specific hashing methods. Our framework decomposes hashing learning problem\ninto two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically\nbe formulated as binary quadratic problems, and the second step can be accomplished by training standard binary\nclassifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate\nthat the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>\n", "tags": ["ICCV", "Tools-&-Libraries", "Hashing-Methods"], "tsne_embedding": [25.039894104003906, -1.049882411956787], "cluster": 6}, {"key": "lin2025optimizing", "year": "2014", "citations": "23", "title": "Optimizing Ranking Measures For Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.\nThe resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [40.725223541259766, 14.227059364318848], "cluster": 2}, {"key": "lin2025semantics", "year": "2015", "citations": "540", "title": "Semantics-preserving Hashing For Cross-view Retrieval", "abstract": "<p>With benefits of low storage costs and high query speeds,\nhashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.\nIn this paper, we study the problem of cross-view retrieval\nand propose an effective Semantics-Preserving Hashing\nmethod, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them\ninto a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the\nKullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt\nhash codes. And for any unseen instance, predicted hash\ncodes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,\nusing a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Scalability", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [18.23211097717285, -5.719451427459717], "cluster": 6}, {"key": "lindgren2017leveraging", "year": "2016", "citations": "12", "title": "Leveraging Sparsity For Efficient Submodular Data Summarization", "abstract": "<p>The facility location problem is widely used for summarizing large datasets\nand has additional applications in sensor placement, image retrieval, and\nclustering. One difficulty of this problem is that submodular optimization\nalgorithms require the calculation of pairwise benefits for all items in the\ndataset. This is infeasible for large problems, so recent work proposed to only\ncalculate nearest neighbor benefits. One limitation is that several strong\nassumptions were invoked to obtain provable approximation guarantees. In this\npaper we establish that these extra assumptions are not necessary\u2014solving the\nsparsified problem will be almost optimal under the standard assumptions of the\nproblem. We then analyze a different method of sparsification that is a better\nmodel for methods such as Locality Sensitive Hashing to accelerate the nearest\nneighbor computations and extend the use of the problem to a broader family of\nsimilarities. We validate our approach by demonstrating that it rapidly\ngenerates interpretable summaries.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Image-Retrieval", "Datasets"], "tsne_embedding": [15.965691566467285, 31.77265739440918], "cluster": 4}, {"key": "liong2015deep", "year": "2015", "citations": "579", "title": "Deep Variational And Structural Hashing", "abstract": "<p>In this paper, we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features, we develop a probabilistic framework to infer latent feature representation inside the network. Then, we design a struct layer rather than a bottleneck hash layer, to obtain binary codes through a simple encoding procedure. By doing these, we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval, we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically, we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "CVPR", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-4.950389385223389, 3.611745834350586], "cluster": 1}, {"key": "liong2017cross", "year": "2017", "citations": "98", "title": "Cross-modal Deep Variational Hashing", "abstract": "<p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods\nwhich learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural\nnetwork to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent\nvariable as close as possible from the inferred binary codes,\nwhich is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-16.52135467529297, 24.571495056152344], "cluster": 8}, {"key": "liong2025cross", "year": "2017", "citations": "98", "title": "Cross-modal Deep Variational Hashing", "abstract": "<p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods\nwhich learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural\nnetwork to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent\nvariable as close as possible from the inferred binary codes,\nwhich is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-16.52115821838379, 24.57184600830078], "cluster": 8}, {"key": "liong2025deep", "year": "2015", "citations": "579", "title": "Deep Hashing For Compact Binary Codes Learning", "abstract": "<p>In this paper, we propose a new deep hashing (DH) approach\nto learn compact binary codes for large scale visual\nsearch. Unlike most existing binary codes learning methods\nwhich seek a single linear projection to map each sample\ninto a binary vector, we develop a deep neural network\nto seek multiple hierarchical non-linear transformations to\nlearn these binary codes, so that the nonlinear relationship\nof samples can be well exploited. Our model is learned under\nthree constraints at the top layer of the deep network:\n1) the loss between the original real-valued feature descriptor\nand the learned binary vector is minimized, 2) the binary\ncodes distribute evenly on each bit, and 3) different bits\nare as independent as possible. To further improve the discriminative\npower of the learned binary codes, we extend\nDH into supervised DH (SDH) by including one discriminative\nterm into the objective function of DH which simultaneously\nmaximizes the inter-class variations and minimizes\nthe intra-class variations of the learned binary codes. Experimental\nresults show the superiority of the proposed approach\nover the state-of-the-arts.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Supervised", "Evaluation"], "tsne_embedding": [-13.60728931427002, 17.64814567565918], "cluster": 8}, {"key": "liu2011hashing", "year": "2011", "citations": "861", "title": "Hashing With Graphs", "abstract": "<p>Hashing is becoming increasingly popular for\nefficient nearest neighbor search in massive\ndatabases. However, learning short codes\nthat yield good search performance is still\na challenge. Moreover, in many cases realworld\ndata lives on a low-dimensional manifold,\nwhich should be taken into account\nto capture meaningful nearest neighbors. In\nthis paper, we propose a novel graph-based\nhashing method which automatically discovers\nthe neighborhood structure inherent in\nthe data to learn appropriate compact codes.\nTo make such an approach computationally\nfeasible, we utilize Anchor Graphs to obtain\ntractable low-rank adjacency matrices. Our\nformulation allows constant time hashing of a\nnew data point by extrapolating graph Laplacian\neigenvectors to eigenfunctions. Finally,\nwe describe a hierarchical threshold learning\nprocedure in which each eigenfunction yields\nmultiple bits, leading to higher search accuracy.\nExperimental comparison with the\nother state-of-the-art methods on two large\ndatasets demonstrates the efficacy of the proposed\nmethod.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [50.525291442871094, 8.6829195022583], "cluster": 9}, {"key": "liu2012supervised", "year": "2012", "citations": "1447", "title": "Supervised Hashing With Kernels", "abstract": "<p>Recent years have witnessed the growing popularity of\nhashing in large-scale vision problems. It has been shown\nthat the hashing quality could be boosted by leveraging supervised\ninformation into hash function learning. However,\nthe existing supervised methods either lack adequate performance\nor often incur cumbersome model training. In this\npaper, we propose a novel kernel-based supervised hashing\nmodel which requires a limited amount of supervised information,\ni.e., similar and dissimilar data pairs, and a feasible\ntraining cost in achieving high quality hashing. The idea\nis to map the data to compact binary codes whose Hamming\ndistances are minimized on similar pairs and simultaneously\nmaximized on dissimilar pairs. Our approach is\ndistinct from prior works by utilizing the equivalence between\noptimizing the code inner products and the Hamming\ndistances. This enables us to sequentially and efficiently\ntrain the hash functions one bit at a time, yielding very\nshort yet discriminative codes. We carry out extensive experiments\non two image benchmarks with up to one million\nsamples, demonstrating that our approach significantly outperforms\nthe state-of-the-arts in searching both metric distance\nneighbors and semantically similar neighbors, with\naccuracy gains ranging from 13% to 46%.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Scalability", "Supervised", "Evaluation"], "tsne_embedding": [20.290456771850586, -0.052669283002614975], "cluster": 6}, {"key": "liu2013hash", "year": "2013", "citations": "91", "title": "Hash Bit Selection: A Unified Solution For Selection Problems In Hashing", "abstract": "<p>Hashing based methods recently have been shown promising for large-scale nearest neighbor search. However, good designs involve difficult decisions of many unknowns \u2013 data features, hashing algorithms, parameter settings, kernels, etc. In this paper, we provide a unified solution as hash bit selection, i.e., selecting the most informative hash bits from a pool of candidates that may have been generated under various conditions mentioned above. We represent the candidate bit pool as a vertex- and edge-weighted graph with the pooled bits as vertices. Then we formulate the bit selection problem as quadratic programming over the graph, and solve it efficiently by replicator dynamics. Extensive experiments show that our bit selection approach can achieve superior performance over both naive selection methods and state-of-the-art methods under each scenario, usually with significant accuracy gains from 10% to 50% relatively.</p>\n", "tags": ["CVPR", "Evaluation", "Hashing-Methods", "Scalability"], "tsne_embedding": [52.36333465576172, 12.879111289978027], "cluster": 9}, {"key": "liu2014collaborative", "year": "2014", "citations": "130", "title": "Collaborative Hashing", "abstract": "<p>Hashing technique has become a promising approach for\nfast similarity search. Most of existing hashing research\npursue the binary codes for the same type of entities by\npreserving their similarities. In practice, there are many\nscenarios involving nearest neighbor search on the data\ngiven in matrix form, where two different types of, yet\nnaturally associated entities respectively correspond to its\ntwo dimensions or views. To fully explore the duality\nbetween the two views, we propose a collaborative hashing\nscheme for the data in matrix form to enable fast search\nin various applications such as image search using bag of\nwords and recommendation using user-item ratings. By\nsimultaneously preserving both the entity similarities in\neach view and the interrelationship between views, our\ncollaborative hashing effectively learns the compact binary\ncodes and the explicit hash functions for out-of-sample\nextension in an alternating optimization way. Extensive\nevaluations are conducted on three well-known datasets\nfor search inside a single view and search across different\nviews, demonstrating that our proposed method outperforms\nstate-of-the-art baselines, with significant accuracy\ngains ranging from 7.67% to 45.87% relatively.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Recommender-Systems", "Similarity-Search", "Image-Retrieval", "Datasets", "Compact-Codes"], "tsne_embedding": [11.729166984558105, 9.080448150634766], "cluster": 6}, {"key": "liu2014discrete", "year": "2014", "citations": "496", "title": "Discrete Graph Hashing", "abstract": "<p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic\ndatabases. In particular, learning based hashing has received considerable\nattention due to its appealing storage and search efficiency. However, the performance\nof most unsupervised learning based hashing methods deteriorates rapidly\nas the hash code length increases. We argue that the degraded performance is due\nto inferior optimization procedures used to achieve discrete binary codes. This\npaper presents a graph-based unsupervised hashing model to preserve the neighborhood\nstructure of massive data in a discrete code space. We cast the graph\nhashing problem into a discrete optimization framework which directly learns the\nbinary codes. A tractable alternating maximization algorithm is then proposed to\nexplicitly deal with the discrete constraints, yielding high-quality codes to well\ncapture the local neighborhoods. Extensive experiments performed on four large\ndatasets with up to one million samples show that our discrete optimization based\ngraph hashing method obtains superior search accuracy over state-of-the-art unsupervised\nhashing methods, especially for longer codes.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [49.71467208862305, 8.644744873046875], "cluster": 9}, {"key": "liu2015multi", "year": "2015", "citations": "57", "title": "Multi-view Complementary Hash Tables For Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many\napplications (e.g., visual search, object detection, image\nmatching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.\nHowever, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build\nmultiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.\nIn\nthis paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,\nand learn discriminative hash functions in an efficient way.\nTo build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars\nshared by mis-separated neighbors. Extensive experiments\non three large-scale image datasets demonstrate that the\nproposed method significantly outperforms various naive\nsolutions and state-of-the-art multi-table methods.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [15.967610359191895, 10.366890907287598], "cluster": 6}, {"key": "liu2016accurate", "year": "2016", "citations": "2", "title": "Accurate Deep Representation Quantization With Gradient Snapping Layer For Similarity Search", "abstract": "<p>Recent advance of large scale similarity search involves using deeply learned\nrepresentations to improve the search accuracy and use vector quantization\nmethods to increase the search speed. However, how to learn deep\nrepresentations that strongly preserve similarities between data pairs and can\nbe accurately quantized via vector quantization remains a challenging task.\nExisting methods simply leverage quantization loss and similarity loss, which\nresult in unexpectedly biased back-propagating gradients and affect the search\nperformances. To this end, we propose a novel gradient snapping layer (GSL) to\ndirectly regularize the back-propagating gradient towards a neighboring\ncodeword, the generated gradients are un-biased for reducing similarity loss\nand also propel the learned representations to be accurately quantized. Joint\ndeep representation and vector quantization learning can be easily performed by\nalternatively optimize the quantization codebook and the deep neural network.\nThe proposed framework is compatible with various existing vector quantization\napproaches. Experimental results demonstrate that the proposed framework is\neffective, flexible and outperforms the state-of-the-art large scale similarity\nsearch methods.</p>\n", "tags": ["Quantization", "Tools-&-Libraries", "Similarity-Search"], "tsne_embedding": [1.9253429174423218, -18.575326919555664], "cluster": 7}, {"key": "liu2016dual", "year": "2016", "citations": "1", "title": "Dual Purpose Hashing", "abstract": "<p>Recent years have seen more and more demand for a unified framework to\naddress multiple realistic image retrieval tasks concerning both category and\nattributes. Considering the scale of modern datasets, hashing is favorable for\nits low complexity. However, most existing hashing methods are designed to\npreserve one single kind of similarity, thus improper for dealing with the\ndifferent tasks simultaneously. To overcome this limitation, we propose a new\nhashing method, named Dual Purpose Hashing (DPH), which jointly preserves the\ncategory and attribute similarities by exploiting the Convolutional Neural\nNetwork (CNN) models to hierarchically capture the correlations between\ncategory and attributes. Since images with both category and attribute labels\nare scarce, our method is designed to take the abundant partially labelled\nimages on the Internet as training inputs. With such a framework, the binary\ncodes of new-coming images can be readily obtained by quantizing the network\noutputs of a binary-like layer, and the attributes can be recovered from the\ncodes easily. Experiments on two large-scale datasets show that our dual\npurpose hash codes can achieve comparable or even better performance than those\nstate-of-the-art methods specifically designed for each individual retrieval\ntask, while being more compact than the compared methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-3.592405319213867, -0.1332041621208191], "cluster": 1}, {"key": "liu2016generalized", "year": "2016", "citations": "5", "title": "Generalized Residual Vector Quantization For Large Scale Data", "abstract": "<p>Vector quantization is an essential tool for tasks involving large scale\ndata, for example, large scale similarity search, which is crucial for\ncontent-based information retrieval and analysis. In this paper, we propose a\nnovel vector quantization framework that iteratively minimizes quantization\nerror. First, we provide a detailed review on a relevant vector quantization\nmethod named \\textit{residual vector quantization} (RVQ). Next, we propose\n\\textit{generalized residual vector quantization} (GRVQ) to further improve\nover RVQ. Many vector quantization methods can be viewed as the special cases\nof our proposed framework. We evaluate GRVQ on several large scale benchmark\ndatasets for large scale search, classification and object retrieval. We\ncompared GRVQ with existing methods in detail. Extensive experiments\ndemonstrate our GRVQ framework substantially outperforms existing methods in\nterm of quantization accuracy and computation efficiency.</p>\n", "tags": ["Survey-Paper", "Efficiency", "Quantization", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [2.9602184295654297, -24.269990921020508], "cluster": 7}, {"key": "liu2016generating", "year": "2016", "citations": "62", "title": "Generating Binary Tags For Fast Medical Image Retrieval Based On Convolutional Nets And Radon Transform", "abstract": "<p>Content-based image retrieval (CBIR) in large medical image archives is a\nchallenging and necessary task. Generally, different feature extraction methods\nare used to assign expressive and invariant features to each image such that\nthe search for similar images comes down to feature classification and/or\nmatching. The present work introduces a new image retrieval method for medical\napplications that employs a convolutional neural network (CNN) with recently\nintroduced Radon barcodes. We combine neural codes for global classification\nwith Radon barcodes for the final retrieval. We also examine image search based\non regions of interest (ROI) matching after image retrieval. The IRMA dataset\nwith more than 14,000 x-rays images is used to evaluate the performance of our\nmethod. Experimental results show that our approach is superior to many\npublished works.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.154972076416016, 13.810626983642578], "cluster": 0}, {"key": "liu2016ordinal", "year": "2017", "citations": "23", "title": "Ordinal Constrained Binary Code Learning For Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed extensive attention in binary code learning,\na.k.a. hashing, for nearest neighbor search problems. It has been seen that\nhigh-dimensional data points can be quantized into binary codes to give an\nefficient similarity approximation via Hamming distance. Among existing\nschemes, ranking-based hashing is recent promising that targets at preserving\nordinal relations of ranking in the Hamming space to minimize retrieval loss.\nHowever, the size of the ranking tuples, which shows the ordinal relations, is\nquadratic or cubic to the size of training samples. By given a large-scale\ntraining data set, it is very expensive to embed such ranking tuples in binary\ncode learning. Besides, it remains a dificulty to build ranking tuples\nefficiently for most ranking-preserving hashing, which are deployed over an\nordinal graph-based setting. To handle these problems, we propose a novel\nranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH),\nwhich efficiently learns the optimal hashing functions with a graph-based\napproximation to embed the ordinal relations. The core idea is to reduce the\nsize of ordinal graph with ordinal constraint projection, which preserves the\nordinal relations through a small data set (such as clusters or random\nsamples). In particular, to learn such hash functions effectively, we further\nrelax the discrete constraints and design a specific stochastic gradient decent\nalgorithm for optimization. Experimental results on three large-scale visual\nsearch benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the\nproposed OCH method can achieve superior performance over the state-of-the-arts\napproaches.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Scalability", "AAAI", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [42.386661529541016, 3.3354384899139404], "cluster": 9}, {"key": "liu2016perceptual", "year": "2016", "citations": "8", "title": "Perceptual Uniform Descriptor And Ranking On Manifold: A Bridge Between Image Representation And Ranking For Image Retrieval", "abstract": "<p>Incompatibility of image descriptor and ranking is always neglected in image\nretrieval. In this paper, manifold learning and Gestalt psychology theory are\ninvolved to solve the incompatibility problem. A new holistic descriptor called\nPerceptual Uniform Descriptor (PUD) based on Gestalt psychology is proposed,\nwhich combines color and gradient direction to imitate the human visual\nuniformity. PUD features in the same class images distributes on one manifold\nin most cases because PUD improves the visual uniformity of the traditional\ndescriptors. Thus, we use manifold ranking and PUD to realize image retrieval.\nExperiments were carried out on five benchmark data sets, and the proposed\nmethod can greatly improve the accuracy of image retrieval. Our experimental\nresults in the Ukbench and Corel-1K datasets demonstrated that N-S score\nreached to 3.58 (HSV 3.4) and mAP to 81.77% (ODBTC 77.9%) respectively by\nutilizing PUD which has only 280 dimension. The results are higher than other\nholistic image descriptors (even some local ones) and state-of-the-arts\nretrieval methods.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-42.1948127746582, 11.253925323486328], "cluster": 0}, {"key": "liu2016supervised", "year": "2016", "citations": "65", "title": "Supervised Matrix Factorization For Cross-modality Hashing", "abstract": "<p>Matrix factorization has been recently utilized for the task of multi-modal\nhashing for cross-modality visual search, where basis functions are learned to\nmap data from different modalities to the same Hamming embedding. In this\npaper, we propose a novel cross-modality hashing algorithm termed Supervised\nMatrix Factorization Hashing (SMFH) which tackles the multi-modal hashing\nproblem with a collective non-matrix factorization across the different\nmodalities. In particular, SMFH employs a well-designed binary code learning\nalgorithm to preserve the similarities among multi-modal original features\nthrough a graph regularization. At the same time, semantic labels, when\navailable, are incorporated into the learning procedure. We conjecture that all\nthese would facilitate to preserve the most relevant information during the\nbinary quantization process, and hence improve the retrieval accuracy. We\ndemonstrate the superior performance of SMFH on three cross-modality visual\nsearch benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with\nquantitative comparison to various state-of-the-art methods</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Quantization", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [50.914180755615234, -6.727812767028809], "cluster": 9}, {"key": "liu2016three", "year": "2016", "citations": "1", "title": "Three Tiers Neighborhood Graph And Multi-graph Fusion Ranking For Multi-feature Image Retrieval: A Manifold Aspect", "abstract": "<p>Single feature is inefficient to describe content of an image, which is a\nshortcoming in traditional image retrieval task. We know that one image can be\ndescribed by different features. Multi-feature fusion ranking can be utilized\nto improve the ranking list of query. In this paper, we first analyze graph\nstructure and multi-feature fusion re-ranking from manifold aspect. Then, Three\nTiers Neighborhood Graph (TTNG) is constructed to re-rank the original ranking\nlist by single feature and to enhance precision of single feature. Furthermore,\nwe propose Multi-graph Fusion Ranking (MFR) for multi-feature ranking, which\nconsiders the correlation of all images in multiple neighborhood graphs.\nEvaluations are conducted on UK-bench, Corel-1K, Corel-10K and Cifar-10\nbenchmark datasets. The experimental results show that our TTNG and MFR\noutperform than other state-of-the-art methods. For example, we achieve\ncompetitive results N-S score 3.91 and precision 65.00% on UK-bench and\nCorel-10K datasets respectively.</p>\n", "tags": ["Image-Retrieval", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [52.52281188964844, -12.584491729736328], "cluster": 9}, {"key": "liu2017deep", "year": "2017", "citations": "267", "title": "Deep Hashing With Category Mask For Fast Video Retrieval", "abstract": "<p>This paper proposes an end-to-end deep hashing framework with category mask\nfor fast video retrieval. We train our network in a supervised way by fully\nexploiting inter-class diversity and intra-class identity. Classification loss\nis optimized to maximize inter-class diversity, while intra-pair is introduced\nto learn representative intra-class identity. We investigate the binary bits\ndistribution related to categories and find out that the effectiveness of\nbinary bits is highly correlated with data categories, and some bits may\ndegrade classification performance of some categories. We then design hash code\ngeneration scheme with category mask to filter out bits with negative\ncontribution. Experimental results demonstrate the proposed method outperforms\nseveral state-of-the-arts under various evaluation metrics on public datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "CVPR", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-8.063158988952637, -1.5749924182891846], "cluster": 1}, {"key": "liu2017discretely", "year": "2017", "citations": "49", "title": "Discretely Coding Semantic Rank Orders For Supervised Image Hashing", "abstract": "<p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "CVPR", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [5.050721168518066, 15.991497039794922], "cluster": 6}, {"key": "liu2017end", "year": "2017", "citations": "7", "title": "End-to-end Binary Representation Learning Via Direct Binary Embedding", "abstract": "<p>Learning binary representation is essential to large-scale computer vision\ntasks. Most existing algorithms require a separate quantization constraint to\nlearn effective hashing functions. In this work, we present Direct Binary\nEmbedding (DBE), a simple yet very effective algorithm to learn binary\nrepresentation in an end-to-end fashion. By appending an ingeniously designed\nDBE layer to the deep convolutional neural network (DCNN), DBE learns binary\ncode directly from the continuous DBE layer activation without quantization\nerror. By employing the deep residual network (ResNet) as DCNN component, DBE\ncaptures rich semantics from images. Furthermore, in the effort of handling\nmultilabel images, we design a joint cross entropy loss that includes both\nsoftmax cross entropy and weighted binary cross entropy in consideration of the\ncorrelation and independence of labels, respectively. Extensive experiments\ndemonstrate the significant superiority of DBE over state-of-the-art methods on\ntasks of natural object recognition, image retrieval and image annotation.</p>\n", "tags": ["Quantization", "Image-Retrieval", "Hashing-Methods", "Scalability"], "tsne_embedding": [-14.20843505859375, 18.349031448364258], "cluster": 8}, {"key": "liu2017neural", "year": "2017", "citations": "0", "title": "Neural Method For Explicit Mapping Of Quasi-curvature Locally Linear Embedding In Image Retrieval", "abstract": "<p>This paper proposed a new explicit nonlinear dimensionality reduction using\nneural networks for image retrieval tasks. We first proposed a Quasi-curvature\nLocally Linear Embedding (QLLE) for training set. QLLE guarantees the linear\ncriterion in neighborhood of each sample. Then, a neural method (NM) is\nproposed for out-of-sample problem. Combining QLLE and NM, we provide a\nexplicit nonlinear dimensionality reduction approach for efficient image\nretrieval. The experimental results in three benchmark datasets illustrate that\nour method can get better performance than other state-of-the-art out-of-sample\nmethods.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-8.42840576171875, -16.837677001953125], "cluster": 1}, {"key": "liu2018adversarial", "year": "2019", "citations": "36", "title": "Adversarial Binary Coding For Efficient Person Re-identification", "abstract": "<p>Person re-identification (ReID) aims at matching persons across different\nviews/scenes. In addition to accuracy, the matching efficiency has received\nmore and more attention because of demanding applications using large-scale\ndata. Several binary coding based methods have been proposed for efficient\nReID, which either learn projections to map high-dimensional features to\ncompact binary codes, or directly adopt deep neural networks by simply\ninserting an additional fully-connected layer with tanh-like activations.\nHowever, the former approach requires time-consuming hand-crafted feature\nextraction and complicated (discrete) optimizations; the latter lacks the\nnecessary discriminative information greatly due to the straightforward\nactivation functions. In this paper, we propose a simple yet effective\nframework for efficient ReID inspired by the recent advances in adversarial\nlearning. Specifically, instead of learning explicit projections or adding\nfully-connected mapping layers, the proposed Adversarial Binary Coding (ABC)\nframework guides the extraction of binary codes implicitly and effectively. The\ndiscriminability of the extracted codes is further enhanced by equipping the\nABC with a deep triplet network for the ReID task. More importantly, the ABC\nand triplet network are simultaneously optimized in an end-to-end manner.\nExtensive experiments on three large-scale ReID benchmarks demonstrate the\nsuperiority of our approach over the state-of-the-art methods.</p>\n", "tags": ["Efficiency", "Scalability", "Robustness", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [3.9980616569519043, 23.617631912231445], "cluster": 8}, {"key": "liu2018discriminative", "year": "2018", "citations": "5", "title": "Discriminative Cross-view Binary Representation Learning", "abstract": "<p>Learning compact representation is vital and challenging for large scale\nmultimedia data. Cross-view/cross-modal hashing for effective binary\nrepresentation learning has received significant attention with exponentially\ngrowing availability of multimedia content. Most existing cross-view hashing\nalgorithms emphasize the similarities in individual views, which are then\nconnected via cross-view similarities. In this work, we focus on the\nexploitation of the discriminative information from different views, and\npropose an end-to-end method to learn semantic-preserving and discriminative\nbinary representation, dubbed Discriminative Cross-View Hashing (DCVH), in\nlight of learning multitasking binary representation for various tasks\nincluding cross-view retrieval, image-to-image retrieval, and image\nannotation/tagging. The proposed DCVH has the following key components. First,\nit uses convolutional neural network (CNN) based nonlinear hashing functions\nand multilabel classification for both images and texts simultaneously. Such\nhashing functions achieve effective continuous relaxation during training\nwithout explicit quantization loss by using Direct Binary Embedding (DBE)\nlayers. Second, we propose an effective view alignment via Hamming distance\nminimization, which is efficiently accomplished by bit-wise XOR operation.\nExtensive experiments on two image-text benchmark datasets demonstrate that\nDCVH outperforms state-of-the-art cross-view hashing algorithms as well as\nsingle-view image hashing algorithms. In addition, DCVH can provide competitive\nperformance for image annotation/tagging.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-4.5137810707092285, 3.984736204147339], "cluster": 1}, {"key": "liu2018fusion", "year": "2018", "citations": "0", "title": "Fusion Hashing: A General Framework For Self-improvement Of Hashing", "abstract": "<p>Hashing has been widely used for efficient similarity search based on its\nquery and storage efficiency. To obtain better precision, most studies focus on\ndesigning different objective functions with different constraints or penalty\nterms that consider neighborhood information. In this paper, in contrast to\nexisting hashing methods, we propose a novel generalized framework called\nfusion hashing (FH) to improve the precision of existing hashing methods\nwithout adding new constraints or penalty terms. In the proposed FH, given an\nexisting hashing method, we first execute it several times to get several\ndifferent hash codes for a set of training samples. We then propose two novel\nfusion strategies that combine these different hash codes into one set of final\nhash codes. Based on the final hash codes, we learn a simple linear hash\nfunction for the samples that can significantly improve model precision. In\ngeneral, the proposed FH can be adopted in existing hashing method and achieve\nmore precise and stable performance compared to the original hashing method\nwith little extra expenditure in terms of time and space. Extensive experiments\nwere performed based on three benchmark datasets and the results demonstrate\nthe superior performance of the proposed framework</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [23.577587127685547, -5.7326765060424805], "cluster": 6}, {"key": "liu2018mtfh", "year": "2019", "citations": "175", "title": "MTFH: A Matrix Tri-factorization Hashing Framework For Efficient Cross-modal Retrieval", "abstract": "<p>Hashing has recently sparked a great revolution in cross-modal retrieval\nbecause of its low storage cost and high query speed. Recent cross-modal\nhashing methods often learn unified or equal-length hash codes to represent the\nmulti-modal data and make them intuitively comparable. However, such unified or\nequal-length hash representations could inherently sacrifice their\nrepresentation scalability because the data from different modalities may not\nhave one-to-one correspondence and could be encoded more efficiently by\ndifferent hash codes of unequal lengths. To mitigate these problems, this paper\nexploits a related and relatively unexplored problem: encode the heterogeneous\ndata with varying hash lengths and generalize the cross-modal retrieval in\nvarious challenging scenarios. To this end, a generalized and flexible\ncross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH),\nis proposed to work seamlessly in various settings including paired or unpaired\nmulti-modal data, and equal or varying hash length encoding scenarios. More\nspecifically, MTFH exploits an efficient objective function to flexibly learn\nthe modality-specific hash codes with different length settings, while\nsynchronously learning two semantic correlation matrices to semantically\ncorrelate the different hash representations for heterogeneous data comparable.\nAs a result, the derived hash codes are more semantically meaningful for\nvarious challenging cross-modal retrieval tasks. Extensive experiments\nevaluated on public benchmark datasets highlight the superiority of MTFH under\nvarious retrieval scenarios and show its competitive performance with the\nstate-of-the-arts.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [9.786763191223145, 14.34033489227295], "cluster": 6}, {"key": "liu2018stochastic", "year": "2019", "citations": "96", "title": "Stochastic Attraction-repulsion Embedding For Large Scale Image Localization", "abstract": "<p>This paper tackles the problem of large-scale image-based localization (IBL)\nwhere the spatial location of a query image is determined by finding out the\nmost similar reference images in a large database. For solving this problem, a\ncritical task is to learn discriminative image representation that captures\ninformative information relevant for localization. We propose a novel\nrepresentation learning method having higher location-discriminating power. It\nprovides the following contributions: 1) we represent a place (location) as a\nset of exemplar images depicting the same landmarks and aim to maximize\nsimilarities among intra-place images while minimizing similarities among\ninter-place images; 2) we model a similarity measure as a probability\ndistribution on L_2-metric distances between intra-place and inter-place image\nrepresentations; 3) we propose a new Stochastic Attraction and Repulsion\nEmbedding (SARE) loss function minimizing the KL divergence between the learned\nand the actual probability distributions; 4) we give theoretical comparisons\nbetween SARE, triplet ranking and contrastive losses. It provides insights into\nwhy SARE is better by analyzing gradients. Our SARE loss is easy to implement\nand pluggable to any CNN. Experiments show that our proposed method improves\nthe localization performance on standard benchmarks by a large margin.\nDemonstrating the broad applicability of our method, we obtained the third\nplace out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our\ncode and model are available at https://github.com/Liumouliu/deepIBL.</p>\n", "tags": ["ICCV", "Evaluation", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [-27.52212142944336, 10.882712364196777], "cluster": 0}, {"key": "liu2019compositional", "year": "2019", "citations": "15", "title": "Compositional Coding For Collaborative Filtering", "abstract": "<p>Efficiency is crucial to the online recommender systems. Representing users\nand items as binary vectors for Collaborative Filtering (CF) can achieve fast\nuser-item affinity computation in the Hamming space, in recent years, we have\nwitnessed an emerging research effort in exploiting binary hashing techniques\nfor CF methods. However, CF with binary codes naturally suffers from low\naccuracy due to limited representation capability in each bit, which impedes it\nfrom modeling complex structure of the data.\n  In this work, we attempt to improve the efficiency without hurting the model\nperformance by utilizing both the accuracy of real-valued vectors and the\nefficiency of binary codes to represent users/items. In particular, we propose\nthe Compositional Coding for Collaborative Filtering (CCCF) framework, which\nnot only gains better recommendation efficiency than the state-of-the-art\nbinarized CF approaches but also achieves even higher accuracy than the\nreal-valued CF method. Specifically, CCCF innovatively represents each\nuser/item with a set of binary vectors, which are associated with a sparse\nreal-value weight vector. Each value of the weight vector encodes the\nimportance of the corresponding binary vector to the user/item. The continuous\nweight vectors greatly enhances the representation capability of binary codes,\nand its sparsity guarantees the processing speed. Furthermore, an integer\nweight approximation scheme is proposed to further accelerate the speed. Based\non the CCCF framework, we design an efficient discrete optimization algorithm\nto learn its parameters. Extensive experiments on three real-world datasets\nshow that our method outperforms the state-of-the-art binarized CF methods\n(even achieves better performance than the real-valued CF method) by a large\nmargin in terms of both recommendation accuracy and efficiency.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Recommender-Systems", "SIGIR", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [7.008323669433594, 20.922100067138672], "cluster": 8}, {"key": "liu2019cross", "year": "2019", "citations": "16", "title": "Cross-modal Zero-shot Hashing", "abstract": "<p>Hashing has been widely studied for big data retrieval due to its low storage\ncost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing\nmodel that is trained using only samples from seen categories, but can\ngeneralize well to samples of unseen categories. ZSH generally uses category\nattributes to seek a semantic embedding space to transfer knowledge from seen\ncategories to unseen ones. As a result, it may perform poorly when labeled data\nare insufficient. ZSH methods are mainly designed for single-modality data,\nwhich prevents their application to the widely spread multi-modal data. On the\nother hand, existing cross-modal hashing solutions assume that all the\nmodalities share the same category labels, while in practice the labels of\ndifferent data modalities may be different. To address these issues, we propose\na general Cross-modal Zero-shot Hashing (CZHash) solution to effectively\nleverage unlabeled and labeled multi-modality data with different label spaces.\nCZHash first quantifies the composite similarity between instances using label\nand feature information. It then defines an objective function to achieve deep\nfeature learning compatible with the composite similarity preserving, category\nattribute space learning, and hashing coding function learning. CZHash further\nintroduces an alternative optimization procedure to jointly optimize these\nlearning objectives. Experiments on benchmark multi-modal datasets show that\nCZHash significantly outperforms related representative hashing approaches both\non effectiveness and adaptability.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [1.7752479314804077, 11.056940078735352], "cluster": 6}, {"key": "liu2019deep", "year": "2018", "citations": "106", "title": "Deep Triplet Quantization", "abstract": "<p>Deep hashing establishes efficient and effective image retrieval by\nend-to-end learning of deep representations and hash codes from similarity\ndata. We present a compact coding solution, focusing on deep learning to\nquantization approach that has shown superior performance over hashing\nsolutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ),\na novel approach to learning deep quantization models from the similarity\ntriplets. To enable more effective triplet training, we design a new triplet\nselection approach, Group Hard, that randomly selects hard triplets in each\nimage group. To generate compact binary codes, we further apply a triplet\nquantization with weak orthogonality during triplet training. The quantization\nloss reduces the codebook redundancy and enhances the quantizability of deep\nrepresentations through back-propagation. Extensive experiments demonstrate\nthat DTQ can generate high-quality and compact binary codes, which yields\nstate-of-the-art image retrieval performance on three benchmark datasets,\nNUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Image-Retrieval", "Similarity-Search", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-8.875633239746094, 8.36998176574707], "cluster": 8}, {"key": "liu2019explore", "year": "2019", "citations": "3", "title": "Explore Entity Embedding Effectiveness In Entity Retrieval", "abstract": "<p>This paper explores entity embedding effectiveness in ad-hoc entity\nretrieval, which introduces distributed representation of entities into entity\nretrieval. The knowledge graph contains lots of knowledge and models entity\nsemantic relations with the well-formed structural representation. Entity\nembedding learns lots of semantic information from the knowledge graph and\nrepresents entities with a low-dimensional representation, which provides an\nopportunity to establish interactions between query related entities and\ncandidate entities for entity retrieval. Our experiments demonstrate the\neffectiveness of entity embedding based model, which achieves more than 5%\nimprovement than the previous state-of-the-art learning to rank based entity\nretrieval model. Our further analysis reveals that the entity semantic match\nfeature effective, especially for the scenario which needs more semantic\nunderstanding.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [58.95128631591797, -5.447987079620361], "cluster": 9}, {"key": "liu2019furnishing", "year": "2019", "citations": "6", "title": "Furnishing Your Room By What You See: An End-to-end Furniture Set Retrieval Framework With Rich Annotated Benchmark Dataset", "abstract": "<p>Understanding interior scenes has attracted enormous interest in computer\nvision community. However, few works focus on the understanding of furniture\nwithin the scenes and a large-scale dataset is also lacked to advance the\nfield. In this paper, we first fill the gap by presenting DeepFurniture, a\nrichly annotated large indoor scene dataset, including 24k indoor images, 170k\nfurniture instances and 20k unique furniture identities. On the dataset, we\nintroduce a new benchmark, named furniture set retrieval. Given an indoor photo\nas input, the task requires to detect all the furniture instances and search a\nmatched set of furniture identities. To address this challenging task, we\npropose a feature and context embedding based framework. It contains 3 major\ncontributions: (1) An improved Mask-RCNN model with an additional mask-based\nclassifier is introduced for better utilizing the mask information to relieve\nthe occlusion problems in furniture detection context. (2) A multi-task style\nSiamese network is proposed to train the feature embedding model for retrieval,\nwhich is composed of a classification subnet supervised by self-clustered\npseudo attributes and a verification subnet to estimate whether the input pair\nis matched. (3) In order to model the relationship of the furniture entities in\nan interior design, a context embedding model is employed to re-rank the\nretrieval results. Extensive experiments demonstrate the effectiveness of each\nmodule and the overall system.</p>\n", "tags": ["Scalability", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-9.200785636901855, -44.2629280090332], "cluster": 3}, {"key": "liu2019general", "year": "2019", "citations": "3", "title": "The General Pair-based Weighting Loss For Deep Metric Learning", "abstract": "<p>Deep metric learning aims at learning the distance metric between pair of\nsamples, through the deep neural networks to extract the semantic feature\nembeddings where similar samples are close to each other while dissimilar\nsamples are farther apart. A large amount of loss functions based on pair\ndistances have been presented in the literature for guiding the training of\ndeep metric learning. In this paper, we unify them in a general pair-based\nweighting loss function, where the minimizing objective loss is just the\ndistances weighting of informative pairs. The general pair-based weighting loss\nincludes two main aspects, (1) samples mining and (2) pairs weighting. Samples\nmining aims at selecting the informative positive and negative pair sets to\nexploit the structured relationship of samples in a mini-batch and also reduce\nthe number of non-trivial pairs. Pair weighting aims at assigning different\nweights for different pairs according to the pair distances for\ndiscriminatively training the network. We detailedly review those existing\npair-based losses inline with our general loss function, and explore some\npossible methods from the perspective of samples mining and pairs weighting.\nFinally, extensive experiments on three image retrieval datasets show that our\ngeneral pair-based weighting loss obtains new state-of-the-art performance,\ndemonstrating the effectiveness of the pair-based samples mining and pairs\nweighting for deep metric learning.</p>\n", "tags": ["Survey-Paper", "Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-11.127850532531738, -21.163278579711914], "cluster": 1}, {"key": "liu2019mutual", "year": "2019", "citations": "3", "title": "Mutual Linear Regression-based Discrete Hashing", "abstract": "<p>Label information is widely used in hashing methods because of its\neffectiveness of improving the precision. The existing hashing methods always\nuse two different projections to represent the mutual regression between hash\ncodes and class labels. In contrast to the existing methods, we propose a novel\nlearning-based hashing method termed stable supervised discrete hashing with\nmutual linear regression (S2DHMLR) in this study, where only one stable\nprojection is used to describe the linear correlation between hash codes and\ncorresponding labels. To the best of our knowledge, this strategy has not been\nused for hashing previously. In addition, we further use a boosting strategy to\nimprove the final performance of the proposed method without adding extra\nconstraints and with little extra expenditure in terms of time and space.\nExtensive experiments conducted on three image benchmarks demonstrate the\nsuperior performance of the proposed method.</p>\n", "tags": ["Supervised", "Evaluation", "Hashing-Methods"], "tsne_embedding": [19.870357513427734, 0.6662228107452393], "cluster": 6}, {"key": "liu2019optimal", "year": "2019", "citations": "31", "title": "Optimal Projection Guided Transfer Hashing For Image Retrieval", "abstract": "<p>Recently, learning to hash has been widely studied for image retrieval thanks\nto the computation and storage efficiency of binary codes. For most existing\nlearning to hash methods, sufficient training images are required and used to\nlearn precise hashing codes. However, in some real-world applications, there\nare not always sufficient training images in the domain of interest. In\naddition, some existing supervised approaches need a amount of labeled data,\nwhich is an expensive process in term of time, label and human expertise. To\nhandle such problems, inspired by transfer learning, we propose a simple yet\neffective unsupervised hashing method named Optimal Projection Guided Transfer\nHashing (GTH) where we borrow the images of other different but related domain\ni.e., source domain to help learn precise hashing codes for the domain of\ninterest i.e., target domain. Besides, we propose to seek for the maximum\nlikelihood estimation (MLE) solution of the hashing functions of target and\nsource domains due to the domain gap. Furthermore,an alternating optimization\nmethod is adopted to obtain the two projections of target and source domains\nsuch that the domain hashing disparity is reduced gradually. Extensive\nexperiments on various benchmark databases verify that our method outperforms\nmany state-of-the-art learning to hash methods. The implementation details are\navailable at https://github.com/liuji93/GTH.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Image-Retrieval", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [14.14195442199707, -2.9588329792022705], "cluster": 6}, {"key": "liu2019query", "year": "2016", "citations": "74", "title": "Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search", "abstract": "<p>Hash based nearest neighbor search has become attractive in many\napplications. However, the quantization in hashing usually degenerates the\ndiscriminative power when using Hamming distance ranking. Besides, for\nlarge-scale visual search, existing hashing methods cannot directly support the\nefficient search over the data with multiple sources, and while the literature\nhas shown that adaptively incorporating complementary information from diverse\nsources or views can significantly boost the search performance. To address the\nproblems, this paper proposes a novel and generic approach to building multiple\nhash tables with multiple views and generating fine-grained ranking results at\nbitwise and tablewise levels. For each hash table, a query-adaptive bitwise\nweighting is introduced to alleviate the quantization loss by simultaneously\nexploiting the quality of hash functions and their complement for nearest\nneighbor search. From the tablewise aspect, multiple hash tables are built for\ndifferent data views as a joint index, over which a query-specific rank fusion\nis proposed to rerank all results from the bitwise ranking by diffusing in a\ngraph. Comprehensive experiments on image search over three well-known\nbenchmarks show that the proposed method achieves up to 17.11% and 20.28%\nperformance gains on single and multiple table search over state-of-the-art\nmethods.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Scalability", "Re-Ranking", "Evaluation"], "tsne_embedding": [15.222865104675293, 10.232327461242676], "cluster": 6}, {"key": "liu2019ranking", "year": "2019", "citations": "59", "title": "Ranking-based Deep Cross-modal Hashing", "abstract": "<p>Cross-modal hashing has been receiving increasing interests for its low\nstorage cost and fast query speed in multi-modal data retrievals. However, most\nexisting hashing methods are based on hand-crafted or raw level features of\nobjects, which may not be optimally compatible with the coding process.\nBesides, these hashing methods are mainly designed to handle simple pairwise\nsimilarity. The complex multilevel ranking semantic structure of instances\nassociated with multiple labels has not been well explored yet. In this paper,\nwe propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH\nfirstly uses the feature and label information of data to derive a\nsemi-supervised semantic ranking list. Next, to expand the semantic\nrepresentation power of hand-crafted features, RDCMH integrates the semantic\nranking information into deep cross-modal hashing and jointly optimizes the\ncompatible parameters of deep feature representations and of hashing functions.\nExperiments on real multi-modal datasets show that RDCMH outperforms other\ncompetitive baselines and achieves the state-of-the-art performance in\ncross-modal retrieval applications.</p>\n", "tags": ["Hashing-Methods", "AAAI", "Multimodal-Retrieval", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [5.356119155883789, -2.018190383911133], "cluster": 6}, {"key": "liu2019strong", "year": "2019", "citations": "13", "title": "A Strong And Robust Baseline For Text-image Matching", "abstract": "<p>We review the current schemes of text-image matching models and propose\nimprovements for both training and inference. First, we empirically show\nlimitations of two popular loss (sum and max-margin loss) widely used in\ntraining text-image embeddings and propose a trade-off: a kNN-margin loss which\n1) utilizes information from hard negatives and 2) is robust to noise as all\n\\(K\\)-most hardest samples are taken into account, tolerating <em>pseudo</em>\nnegatives and outliers. Second, we advocate the use of Inverted Softmax\n(\\textsc{Is}) and Cross-modal Local Scaling (\\textsc{Csls}) during inference to\nmitigate the so-called hubness problem in high-dimensional embedding space,\nenhancing scores of all metrics by a large margin.</p>\n", "tags": ["Survey-Paper"], "tsne_embedding": [-20.08871078491211, -3.26875638961792], "cluster": 1}, {"key": "liu2019weakly", "year": "2019", "citations": "0", "title": "Weakly-paired Cross-modal Hashing", "abstract": "<p>Hashing has been widely adopted for large-scale data retrieval in many\ndomains, due to its low storage cost and high retrieval speed. Existing\ncross-modal hashing methods optimistically assume that the correspondence\nbetween training samples across modalities are readily available. This\nassumption is unrealistic in practical applications. In addition, these methods\ngenerally require the same number of samples across different modalities, which\nrestricts their flexibility. We propose a flexible cross-modal hashing approach\n(Flex-CMH) to learn effective hashing codes from weakly-paired data, whose\ncorrespondence across modalities are partially (or even totally) unknown.\nFlexCMH first introduces a clustering-based matching strategy to explore the\nlocal structure of each cluster, and thus to find the potential correspondence\nbetween clusters (and samples therein) across modalities. To reduce the impact\nof an incomplete correspondence, it jointly optimizes in a unified objective\nfunction the potential correspondence, the cross-modal hashing functions\nderived from the correspondence, and a hashing quantitative loss. An\nalternative optimization technique is also proposed to coordinate the\ncorrespondence and hash functions, and to reinforce the reciprocal effects of\nthe two objectives. Experiments on publicly multi-modal datasets show that\nFlexCMH achieves significantly better results than state-of-the-art methods,\nand it indeed offers a high degree of flexibility for practical cross-modal\nhashing tasks.</p>\n", "tags": ["Memory-Efficiency", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [40.72251510620117, 0.988023579120636], "cluster": 9}, {"key": "liu2020densernet", "year": "2021", "citations": "119", "title": "Densernet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation", "abstract": "<p>In this work, we introduce a Denser Feature Network (DenserNet) for visual\nlocalization. Our work provides three principal contributions. First, we\ndevelop a convolutional neural network (CNN) architecture which aggregates\nfeature maps at different semantic levels for image representations. Using\ndenser feature maps, our method can produce more keypoint features and increase\nimage retrieval accuracy. Second, our model is trained end-to-end without\npixel-level annotation other than positive and negative GPS-tagged image pairs.\nWe use a weakly supervised triplet ranking loss to learn discriminative\nfeatures and encourage keypoint feature repeatability for image representation.\nFinally, our method is computationally efficient as our architecture has shared\nfeatures and parameters during computation. Our method can perform accurate\nlarge-scale localization under challenging conditions while remaining the\ncomputational constraint. Extensive experiment results indicate that our method\nsets a new state-of-the-art on four challenging large-scale localization\nbenchmarks and three image retrieval benchmarks.</p>\n", "tags": ["Supervised", "AAAI", "Image-Retrieval", "Scalability"], "tsne_embedding": [-16.986562728881836, 1.6256977319717407], "cluster": 1}, {"key": "liu2020joint", "year": "2020", "citations": "148", "title": "Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval", "abstract": "<p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "SIGIR", "Multimodal-Retrieval", "Datasets", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [7.389468193054199, 13.746953964233398], "cluster": 6}, {"key": "liu2020model", "year": "2020", "citations": "23", "title": "Model Optimization Boosting Framework For Linear Model Hash Learning", "abstract": "<p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [22.95203399658203, -5.497006416320801], "cluster": 6}, {"key": "liu2020neuromorphic", "year": "2022", "citations": "15", "title": "Neuromorphic Computing For Content-based Image Retrieval", "abstract": "<p>Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in content-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and\n12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a\nlightweight convolutional neural network without batching while maintaining the\nsame level of matching accuracy. The study validates the potential of\nneuromorphic computing in low-power image retrieval, as a complementary\nparadigm to the existing von Neumann architectures.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Evaluation", "Image-Retrieval"], "tsne_embedding": [43.83430862426758, 18.869333267211914], "cluster": 2}, {"key": "liu2020reinforcing", "year": "2020", "citations": "28", "title": "Reinforcing Short-length Hashing", "abstract": "<p>Due to the compelling efficiency in retrieval and storage,\nsimilarity-preserving hashing has been widely applied to approximate nearest\nneighbor search in large-scale image retrieval. However, existing methods have\npoor performance in retrieval using an extremely short-length hash code due to\nweak ability of classification and poor distribution of hash bit. To address\nthis issue, in this study, we propose a novel reinforcing short-length hashing\n(RSLH). In this proposed RSLH, mutual reconstruction between the hash\nrepresentation and semantic labels is performed to preserve the semantic\ninformation. Furthermore, to enhance the accuracy of hash representation, a\npairwise similarity matrix is designed to make a balance between accuracy and\ntraining expenditure on memory. In addition, a parameter boosting strategy is\nintegrated to reinforce the precision with hash bits fusion. Extensive\nexperiments on three large-scale image benchmarks demonstrate the superior\nperformance of RSLH under various short-length hashing scenarios.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Scalability", "Evaluation"], "tsne_embedding": [-5.449552059173584, 10.426108360290527], "cluster": 8}, {"key": "liu2020shuffle", "year": "2020", "citations": "0", "title": "Shuffle And Learn: Minimizing Mutual Information For Unsupervised Hashing", "abstract": "<p>Unsupervised binary representation allows fast data retrieval without any\nannotations, enabling practical application like fast person re-identification\nand multimedia retrieval. It is argued that conflicts in binary space are one\nof the major barriers to high-performance unsupervised hashing as current\nmethods failed to capture the precise code conflicts in the full domain. A\nnovel relaxation method called Shuffle and Learn is proposed to tackle code\nconflicts in the unsupervised hash. Approximated derivatives for joint\nprobability and the gradients for the binary layer are introduced to bridge the\nupdate from the hash to the input. Proof on \\(\\epsilon\\)-Convergence of joint\nprobability with approximated derivatives is provided to guarantee the\npreciseness on update applied on the mutual information. The proposed algorithm\nis carried out with iterative global updates to minimize mutual information,\ndiverging the code before regular unsupervised optimization. Experiments\nsuggest that the proposed method can relax the code optimization from local\noptimum and help to generate binary representations that are more\ndiscriminative and informative without any annotations. Performance benchmarks\non image retrieval with the unsupervised binary code are conducted on three\nopen datasets, and the model achieves state-of-the-art accuracy on image\nretrieval task for all those datasets. Datasets and reproducible code are\nprovided.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-6.42451810836792, 19.540653228759766], "cluster": 8}, {"key": "liu2021fddh", "year": "2021", "citations": "50", "title": "FDDH: Fast Discriminative Discrete Hashing For Large-scale Cross-modal Retrieval", "abstract": "<p>Cross-modal hashing, favored for its effectiveness and efficiency, has\nreceived wide attention to facilitating efficient retrieval across different\nmodalities. Nevertheless, most existing methods do not sufficiently exploit the\ndiscriminative power of semantic information when learning the hash codes,\nwhile often involving time-consuming training procedure for handling the\nlarge-scale dataset. To tackle these issues, we formulate the learning of\nsimilarity-preserving hash codes in terms of orthogonally rotating the semantic\ndata so as to minimize the quantization loss of mapping such data to hamming\nspace, and propose an efficient Fast Discriminative Discrete Hashing (FDDH)\napproach for large-scale cross-modal retrieval. More specifically, FDDH\nintroduces an orthogonal basis to regress the targeted hash codes of training\nexamples to their corresponding semantic labels, and utilizes \u201c-dragging\ntechnique to provide provable large semantic margins. Accordingly, the\ndiscriminative power of semantic information can be explicitly captured and\nmaximized. Moreover, an orthogonal transformation scheme is further proposed to\nmap the nonlinear embedding data into the semantic subspace, which can well\nguarantee the semantic consistency between the data feature and its semantic\nrepresentation. Consequently, an efficient closed form solution is derived for\ndiscriminative hash code learning, which is very computationally efficient. In\naddition, an effective and stable online learning strategy is presented for\noptimizing modality-specific projection functions, featuring adaptivity to\ndifferent training sizes and streaming data. The proposed FDDH approach\ntheoretically approximates the bi-Lipschitz continuity, runs sufficiently fast,\nand also significantly improves the retrieval performance over the\nstate-of-the-art methods. The source code is released at:\nhttps://github.com/starxliu/FDDH.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Scalability", "Similarity-Search", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [4.688133239746094, 11.149823188781738], "cluster": 6}, {"key": "liu2021hit", "year": "2021", "citations": "134", "title": "Hit: Hierarchical Transformer With Momentum Contrast For Video-text Retrieval", "abstract": "<p>Video-Text Retrieval has been a hot research topic with the growth of\nmultimedia data on the internet. Transformer for video-text learning has\nattracted increasing attention due to its promising performance. However,\nexisting cross-modal transformer approaches typically suffer from two major\nlimitations: 1) Exploitation of the transformer architecture where different\nlayers have different feature characteristics is limited; 2) End-to-end\ntraining mechanism limits negative sample interactions in a mini-batch. In this\npaper, we propose a novel approach named Hierarchical Transformer (HiT) for\nvideo-text retrieval. HiT performs Hierarchical Cross-modal Contrastive\nMatching in both feature-level and semantic-level, achieving multi-view and\ncomprehensive retrieval results. Moreover, inspired by MoCo, we propose\nMomentum Cross-modal Contrast for cross-modal learning to enable large-scale\nnegative sample interactions on-the-fly, which contributes to the generation of\nmore precise and discriminative representations. Experimental results on the\nthree major Video-Text Retrieval benchmark datasets demonstrate the advantages\nof our method.</p>\n", "tags": ["ICCV", "Text-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-38.58213424682617, -28.6585750579834], "cluster": 5}, {"key": "liu2021image", "year": "2021", "citations": "108", "title": "Image Retrieval On Real-life Images With Pre-trained Vision-and-language Models", "abstract": "<p>We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&amp;L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.</p>\n", "tags": ["ICCV", "Image-Retrieval", "Datasets"], "tsne_embedding": [-21.99422264099121, -29.90735626220703], "cluster": 5}, {"key": "liu2021ovis", "year": "2022", "citations": "1", "title": "OVIS: Open-vocabulary Visual Instance Search Via Visual-semantic Aligned Representation Learning", "abstract": "<p>We introduce the task of open-vocabulary visual instance search (OVIS). Given\nan arbitrary textual search query, Open-vocabulary Visual Instance Search\n(OVIS) aims to return a ranked list of visual instances, i.e., image patches,\nthat satisfies the search intent from an image database. The term \u201copen\nvocabulary\u201d means that there are neither restrictions to the visual instance to\nbe searched nor restrictions to the word that can be used to compose the\ntextual search query. We propose to address such a search challenge via\nvisual-semantic aligned representation learning (ViSA). ViSA leverages massive\nimage-caption pairs as weak image-level (not instance-level) supervision to\nlearn a rich cross-modal semantic space where the representations of visual\ninstances (not images) and those of textual queries are aligned, thus allowing\nus to measure the similarities between any visual instance and an arbitrary\ntextual query. To evaluate the performance of ViSA, we build two datasets named\nOVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through\nextensive experiments on the two datasets, we demonstrate ViSA\u2019s ability to\nsearch for visual instances in images not available during training given a\nwide range of textual queries including those composed of uncommon words.\nExperimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under\nthe most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600\ndataset.</p>\n", "tags": ["AAAI", "Evaluation", "Datasets"], "tsne_embedding": [-30.335174560546875, 7.391700267791748], "cluster": 0}, {"key": "liu2021ternary", "year": "2021", "citations": "4", "title": "Ternary Hashing", "abstract": "<p>This paper proposes a novel ternary hash encoding for learning to hash\nmethods, which provides a principled more efficient coding scheme with\nperformances better than those of the state-of-the-art binary hashing\ncounterparts. Two kinds of axiomatic ternary logic, Kleene logic and\n{\\L}ukasiewicz logic are adopted to calculate the Ternary Hamming Distance\n(THD) for both the learning/encoding and testing/querying phases. Our work\ndemonstrates that, with an efficient implementation of ternary logic on\nstandard binary machines, the proposed ternary hashing is compared favorably to\nthe binary hashing methods with consistent improvements of retrieval mean\naverage precision (mAP) ranging from 1% to 5.9% as shown in CIFAR10, NUS-WIDE\nand ImageNet100 datasets.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [18.8900146484375, -1.558422565460205], "cluster": 6}, {"key": "liu2022das", "year": "2022", "citations": "13", "title": "DAS: Densely-anchored Sampling For Deep Metric Learning", "abstract": "<p>Deep Metric Learning (DML) serves to learn an embedding function to project\nsemantically similar data into nearby embedding space and plays a vital role in\nmany applications, such as image retrieval and face recognition. However, the\nperformance of DML methods often highly depends on sampling methods to choose\neffective data from the embedding space in the training. In practice, the\nembeddings in the embedding space are obtained by some deep models, where the\nembedding space is often with barren area due to the absence of training\npoints, resulting in so called \u201cmissing embedding\u201d issue. This issue may impair\nthe sample quality, which leads to degenerated DML performance. In this work,\nwe investigate how to alleviate the \u201cmissing embedding\u201d issue to improve the\nsampling quality and achieve effective DML. To this end, we propose a\nDensely-Anchored Sampling (DAS) scheme that considers the embedding with\ncorresponding data point as \u201canchor\u201d and exploits the anchor\u2019s nearby embedding\nspace to densely produce embeddings without data points. Specifically, we\npropose to exploit the embedding space around single anchor with Discriminative\nFeature Scaling (DFS) and multiple anchors with Memorized Transformation\nShifting (MTS). In this way, by combing the embeddings with and without data\npoints, we are able to provide more embeddings to facilitate the sampling\nprocess thus boosting the performance of DML. Our method is effortlessly\nintegrated into existing DML frameworks and improves them without bells and\nwhistles. Extensive experiments on three benchmark datasets demonstrate the\nsuperiority of our method.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-27.240406036376953, -10.713366508483887], "cluster": 5}, {"key": "liu2022descriptor", "year": "2024", "citations": "0", "title": "Descriptor Distillation: A Teacher-student-regularized Framework For Learning Local Descriptors", "abstract": "<p>Learning a fast and discriminative patch descriptor is a challenging topic in\ncomputer vision. Recently, many existing works focus on training various\ndescriptor learning networks by minimizing a triplet loss (or its variants),\nwhich is expected to decrease the distance between each positive pair and\nincrease the distance between each negative pair. However, such an expectation\nhas to be lowered due to the non-perfect convergence of network optimizer to a\nlocal solution. Addressing this problem and the open computational speed\nproblem, we propose a Descriptor Distillation framework for local descriptor\nlearning, called DesDis, where a student model gains knowledge from a\npre-trained teacher model, and it is further enhanced via a designed\nteacher-student regularizer. This teacher-student regularizer is to constrain\nthe difference between the positive (also negative) pair similarity from the\nteacher model and that from the student model, and we theoretically prove that\na more effective student model could be trained by minimizing a weighted\ncombination of the triplet loss and this regularizer, than its teacher which is\ntrained by minimizing the triplet loss singly. Under the proposed DesDis, many\nexisting descriptor networks could be embedded as the teacher model, and\naccordingly, both equal-weight and light-weight student models could be\nderived, which outperform their teacher in either accuracy or speed.\nExperimental results on 3 public datasets demonstrate that the equal-weight\nstudent models, derived from the proposed DesDis framework by utilizing three\ntypical descriptor learning networks as teacher models, could achieve\nsignificantly better performances than their teachers and several other\ncomparative methods. In addition, the derived light-weight models could achieve\n8 times or even faster speeds than the comparative methods under similar patch\nverification performances</p>\n", "tags": ["Tools-&-Libraries", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [27.929288864135742, -39.1826171875], "cluster": 7}, {"key": "liu2022dimension", "year": "2022", "citations": "2", "title": "Dimension Reduction For Efficient Dense Retrieval Via Conditional Autoencoder", "abstract": "<p>Dense retrievers encode queries and documents and map them in an embedding\nspace using pre-trained language models. These embeddings need to be\nhigh-dimensional to fit training signals and guarantee the retrieval\neffectiveness of dense retrievers. However, these high-dimensional embeddings\nlead to larger index storage and higher retrieval latency. To reduce the\nembedding dimensions of dense retrieval, this paper proposes a Conditional\nAutoencoder (ConAE) to compress the high-dimensional embeddings to maintain the\nsame embedding distribution and better recover the ranking features. Our\nexperiments show that ConAE is effective in compressing embeddings by achieving\ncomparable ranking performance with its teacher model and making the retrieval\nsystem more efficient. Our further analyses show that ConAE can alleviate the\nredundancy of the embeddings of dense retrieval with only one linear layer. All\ncodes of this work are available at https://github.com/NEUIR/ConAE.</p>\n", "tags": ["Evaluation", "EMNLP"], "tsne_embedding": [24.8445987701416, -39.021156311035156], "cluster": 7}, {"key": "liu2022prototype", "year": "2023", "citations": "2", "title": "Prototype-based Layered Federated Cross-modal Hashing", "abstract": "<p>Recently, deep cross-modal hashing has gained increasing attention. However,\nin many practical cases, data are distributed and cannot be collected due to\nprivacy concerns, which greatly reduces the cross-modal hashing performance on\neach client. And due to the problems of statistical heterogeneity, model\nheterogeneity, and forcing each client to accept the same parameters, applying\nfederated learning to cross-modal hash learning becomes very tricky. In this\npaper, we propose a novel method called prototype-based layered federated\ncross-modal hashing. Specifically, the prototype is introduced to learn the\nsimilarity between instances and classes on server, reducing the impact of\nstatistical heterogeneity (non-IID) on different clients. And we monitor the\ndistance between local and global prototypes to further improve the\nperformance. To realize personalized federated learning, a hypernetwork is\ndeployed on server to dynamically update different layers\u2019 weights of local\nmodel. Experimental results on benchmark datasets show that our method\noutperforms state-of-the-art methods.</p>\n", "tags": ["ICASSP", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [37.03040313720703, 2.003939628601074], "cluster": 9}, {"key": "liu2022towards", "year": "2022", "citations": "7", "title": "Towards Fast And Accurate Federated Learning With Non-iid Data For Cloud-based Iot Applications", "abstract": "<p>As a promising method of central model training on decentralized device data\nwhile securing user privacy, Federated Learning (FL)is becoming popular in\nInternet of Things (IoT) design. However, when the data collected by IoT\ndevices are highly skewed in a non-independent and identically distributed\n(non-IID) manner, the accuracy of vanilla FL method cannot be guaranteed.\nAlthough there exist various solutions that try to address the bottleneck of FL\nwith non-IID data, most of them suffer from extra intolerable communication\noverhead and low model accuracy. To enable fast and accurate FL, this paper\nproposes a novel data-based device grouping approach that can effectively\nreduce the disadvantages of weight divergence during the training of non-IID\ndata. However, since our grouping method is based on the similarity of\nextracted feature maps from IoT devices, it may incur additional risks of\nprivacy exposure. To solve this problem, we propose an improved version by\nexploiting similarity information using the Locality-Sensitive Hashing (LSH)\nalgorithm without exposing extracted feature maps. Comprehensive experimental\nresults on well-known benchmarks show that our approach can not only accelerate\nthe convergence rate, but also improve the prediction accuracy for FL with\nnon-IID data.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [37.75220489501953, 3.892537832260132], "cluster": 9}, {"key": "liu2022universal", "year": "2022", "citations": "4", "title": "Universal Vision-language Dense Retrieval: Learning A Unified Representation Space For Multi-modal Retrieval", "abstract": "<p>This paper presents Universal Vision-Language Dense Retrieval (UniVL-DR),\nwhich builds a unified model for multi-modal retrieval. UniVL-DR encodes\nqueries and multi-modality resources in an embedding space for searching\ncandidates from different modalities. To learn a unified embedding space for\nmulti-modal retrieval, UniVL-DR proposes two techniques: 1) Universal embedding\noptimization strategy, which contrastively optimizes the embedding space using\nthe modality-balanced hard negatives; 2) Image verbalization method, which\nbridges the modality gap between images and texts in the raw data space.\nUniVL-DR achieves the state-of-the-art on the multi-modal open-domain question\nanswering benchmark, WebQA, and outperforms all retrieval models on the two\nsubtasks, text-text retrieval and text-image retrieval. It demonstrates that\nuniversal multi-modal search is feasible to replace the divide-and-conquer\npipeline with a united model and also benefits single/cross modality tasks. All\nsource codes of this work are available at\nhttps://github.com/OpenMatch/UniVL-DR.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Text-Retrieval"], "tsne_embedding": [2.7143003940582275, -32.55385208129883], "cluster": 3}, {"key": "liu2023bi", "year": "2024", "citations": "13", "title": "Bi-directional Training For Composed Image Retrieval Via Text Prompt Learning", "abstract": "<p>Composed image retrieval searches for a target image based on a multi-modal\nuser query comprised of a reference image and modification text describing the\ndesired changes. Existing approaches to solving this challenging task learn a\nmapping from the (reference image, modification text)-pair to an image\nembedding that is then matched against a large image corpus. One area that has\nnot yet been explored is the reverse direction, which asks the question, what\nreference image when modified as described by the text would produce the given\ntarget image? In this work we propose a bi-directional training scheme that\nleverages such reversed queries and can be applied to existing composed image\nretrieval architectures with minimum changes, which improves the performance of\nthe model. To encode the bi-directional query we prepend a learnable token to\nthe modification text that designates the direction of the query and then\nfinetune the parameters of the text embedding module. We make no other changes\nto the network architecture. Experiments on two standard datasets show that our\nnovel approach achieves improved performance over a baseline BLIP-based model\nthat itself already achieves competitive performance. Our code is released at\nhttps://github.com/Cuberick-Orion/Bi-Blip4CIR.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-10.380127906799316, -27.22273063659668], "cluster": 3}, {"key": "liu2023can", "year": "2023", "citations": "1", "title": "Can LSH (locality-sensitive Hashing) Be Replaced By Neural Network?", "abstract": "<p>With the rapid development of GPU (Graphics Processing Unit) technologies and\nneural networks, we can explore more appropriate data structures and\nalgorithms. Recent progress shows that neural networks can partly replace\ntraditional data structures. In this paper, we proposed a novel DNN (Deep\nNeural Network)-based learned locality-sensitive hashing, called LLSH, to\nefficiently and flexibly map high-dimensional data to low-dimensional space.\nLLSH replaces the traditional LSH (Locality-sensitive Hashing) function\nfamilies with parallel multi-layer neural networks, which reduces the time and\nmemory consumption and guarantees query accuracy simultaneously. The proposed\nLLSH demonstrate the feasibility of replacing the hash index with\nlearning-based neural networks and open a new door for developers to design and\nconfigure data organization more accurately to improve information-searching\nperformance. Extensive experiments on different types of datasets show the\nsuperiority of the proposed method in query accuracy, time consumption, and\nmemory usage.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [45.55062484741211, 16.91621208190918], "cluster": 9}, {"key": "liu2023candidate", "year": "2023", "citations": "4", "title": "Candidate Set Re-ranking For Composed Image Retrieval With Dual Multi-modal Encoder", "abstract": "<p>Composed image retrieval aims to find an image that best matches a given\nmulti-modal user query consisting of a reference image and text pair. Existing\nmethods commonly pre-compute image embeddings over the entire corpus and\ncompare these to a reference image embedding modified by the query text at test\ntime. Such a pipeline is very efficient at test time since fast vector\ndistances can be used to evaluate candidates, but modifying the reference image\nembedding guided only by a short textual description can be difficult,\nespecially independent of potential candidates. An alternative approach is to\nallow interactions between the query and every possible candidate, i.e.,\nreference-text-candidate triplets, and pick the best from the entire set.\nThough this approach is more discriminative, for large-scale datasets the\ncomputational cost is prohibitive since pre-computation of candidate embeddings\nis no longer possible. We propose to combine the merits of both schemes using a\ntwo-stage model. Our first stage adopts the conventional vector distancing\nmetric and performs a fast pruning among candidates. Meanwhile, our second\nstage employs a dual-encoder architecture, which effectively attends to the\ninput triplet of reference-text-candidate and re-ranks the candidates. Both\nstages utilize a vision-and-language pre-trained network, which has proven\nbeneficial for various downstream tasks. Our method consistently outperforms\nstate-of-the-art approaches on standard benchmarks for the task. Our\nimplementation is available at\nhttps://github.com/Cuberick-Orion/Candidate-Reranking-CIR.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [-23.994062423706055, -0.22371017932891846], "cluster": 1}, {"key": "liu2023hs", "year": "2022", "citations": "34", "title": "HS-GCN: Hamming Spatial Graph Convolutional Networks For Recommendation", "abstract": "<p>An efficient solution to the large-scale recommender system is to represent\nusers and items as binary hash codes in the Hamming space. Towards this end,\nexisting methods tend to code users by modeling their Hamming similarities with\nthe items they historically interact with, which are termed as the first-order\nsimilarities in this work. Despite their efficiency, these methods suffer from\nthe suboptimal representative capacity, since they forgo the correlation\nestablished by connecting multiple first-order similarities, i.e., the relation\namong the indirect instances, which could be defined as the high-order\nsimilarity. To tackle this drawback, we propose to model both the first- and\nthe high-order similarities in the Hamming space through the user-item\nbipartite graph. Therefore, we develop a novel learning to hash framework,\nnamely Hamming Spatial Graph Convolutional Networks (HS-GCN), which explicitly\nmodels the Hamming similarity and embeds it into the codes of users and items.\nExtensive experiments on three public benchmark datasets demonstrate that our\nproposed model significantly outperforms several state-of-the-art hashing\nmodels, and obtains performance comparable with the real-valued recommendation\nmodels.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [20.554603576660156, -23.899110794067383], "cluster": 7}, {"key": "liu2023id", "year": "2023", "citations": "1", "title": "ID Embedding As Subtle Features Of Content And Structure For Multimodal Recommendation", "abstract": "<p>Multimodal recommendation aims to model user and item representations\ncomprehensively with the involvement of multimedia content for effective\nrecommendations. Existing research has shown that it is beneficial for\nrecommendation performance to combine (user- and item-) ID embeddings with\nmultimodal salient features, indicating the value of IDs. However, there is a\nlack of a thorough analysis of the ID embeddings in terms of feature semantics\nin the literature. In this paper, we revisit the value of ID embeddings for\nmultimodal recommendation and conduct a thorough study regarding its semantics,\nwhich we recognize as subtle features of <em>content</em> and <em>structure</em>.\nBased on our findings, we propose a novel recommendation model by incorporating\nID embeddings to enhance the salient features of both content and structure.\nSpecifically, we put forward a hierarchical attention mechanism to incorporate\nID embeddings in modality fusing, coupled with contrastive learning, to enhance\ncontent representations. Meanwhile, we propose a lightweight graph convolution\nnetwork for each modality to amalgamate neighborhood and ID embeddings for\nimproving structural representations. Finally, the content and structure\nrepresentations are combined to form the ultimate item embedding for\nrecommendation. Extensive experiments on three real-world datasets (Baby,\nSports, and Clothing) demonstrate the superiority of our method over\nstate-of-the-art multimodal recommendation methods and the effectiveness of\nfine-grained ID embeddings. Our code is available at\nhttps://anonymous.4open.science/r/IDSF-code/.</p>\n", "tags": ["Self-Supervised", "Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [20.520191192626953, -20.838550567626953], "cluster": 7}, {"key": "liu2023instance", "year": "2023", "citations": "0", "title": "Instance-variant Loss With Gaussian RBF Kernel For 3D Cross-modal Retriveal", "abstract": "<p>3D cross-modal retrieval is gaining attention in the multimedia community.\nCentral to this topic is learning a joint embedding space to represent data\nfrom different modalities, such as images, 3D point clouds, and polygon meshes,\nto extract modality-invariant and discriminative features. Hence, the\nperformance of cross-modal retrieval methods heavily depends on the\nrepresentational capacity of this embedding space. Existing methods treat all\ninstances equally, applying the same penalty strength to instances with varying\ndegrees of difficulty, ignoring the differences between instances. This can\nresult in ambiguous convergence or local optima, severely compromising the\nseparability of the feature space. To address this limitation, we propose an\nInstance-Variant loss to assign different penalty strengths to different\ninstances, improving the space separability. Specifically, we assign different\npenalty weights to instances positively related to their intra-class distance.\nSimultaneously, we reduce the cross-modal discrepancy between features by\nlearning a shared weight vector for the same class data from different\nmodalities. By leveraging the Gaussian RBF kernel to evaluate sample\nsimilarity, we further propose an Intra-Class loss function that minimizes the\nintra-class distance among same-class instances. Extensive experiments on three\n3D cross-modal datasets show that our proposed method surpasses recent\nstate-of-the-art approaches.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [-24.276660919189453, 8.726194381713867], "cluster": 0}, {"key": "liu2023learning", "year": "2024", "citations": "0", "title": "Learning Category Trees For Id-based Recommendation: Exploring The Power Of Differentiable Vector Quantization", "abstract": "<p>Category information plays a crucial role in enhancing the quality and\npersonalization of recommender systems. Nevertheless, the availability of item\ncategory information is not consistently present, particularly in the context\nof ID-based recommendations. In this work, we propose a novel approach to\nautomatically learn and generate entity (i.e., user or item) category trees for\nID-based recommendation. Specifically, we devise a differentiable vector\nquantization framework for automatic category tree generation, namely CAGE,\nwhich enables the simultaneous learning and refinement of categorical code\nrepresentations and entity embeddings in an end-to-end manner, starting from\nthe randomly initialized states. With its high adaptability, CAGE can be easily\nintegrated into both sequential and non-sequential recommender systems. We\nvalidate the effectiveness of CAGE on various recommendation tasks including\nlist completion, collaborative filtering, and click-through rate prediction,\nacross different recommendation models. We release the code and data for others\nto reproduce the reported results.</p>\n", "tags": ["Quantization", "Tools-&-Libraries", "Recommender-Systems"], "tsne_embedding": [-3.185295581817627, -25.219751358032227], "cluster": 3}, {"key": "liu2023sparse", "year": "2023", "citations": "0", "title": "Sparse-inductive Generative Adversarial Hashing For Nearest Neighbor Search", "abstract": "<p>Unsupervised hashing has received extensive research focus on the past\ndecade, which typically aims at preserving a predefined metric (i.e. Euclidean\nmetric) in the Hamming space. To this end, the encoding functions of the\nexisting hashing are typically quasi-isometric, which devote to reducing the\nquantization loss from the target metric space to the discrete Hamming space.\nHowever, it is indeed problematic to directly minimize such error, since such\nmentioned two metric spaces are heterogeneous, and the quasi-isometric mapping\nis non-linear. The former leads to inconsistent feature distributions, while\nthe latter leads to problematic optimization issues. In this paper, we propose\na novel unsupervised hashing method, termed Sparsity-Induced Generative\nAdversarial Hashing (SiGAH), to encode large-scale high-dimensional features\ninto binary codes, which well solves the two problems through a generative\nadversarial training framework. Instead of minimizing the quantization loss,\nour key innovation lies in enforcing the learned Hamming space to have similar\ndata distribution to the target metric space via a generative model. In\nparticular, we formulate a ReLU-based neural network as a generator to output\nbinary codes and an MSE-loss based auto-encoder network as a discriminator,\nupon which a generative adversarial learning is carried out to train hash\nfunctions. Furthermore, to generate the synthetic features from the hash codes,\na compressed sensing procedure is introduced into the generative model, which\nenforces the reconstruction boundary of binary codes to be consistent with that\nof original features. Finally, such generative adversarial framework can be\ntrained via the Adam optimizer. Experimental results on four benchmarks, i.e.,\nTiny100K, GIST1M, Deep1M, and MNIST, have shown that the proposed SiGAH has\nsuperior performance over the state-of-the-art approaches.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Robustness", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [8.093327522277832, 17.383508682250977], "cluster": 6}, {"key": "liu2023text", "year": "2024", "citations": "1", "title": "Text-guided Image Restoration And Semantic Enhancement For Text-to-image Person Retrieval", "abstract": "<p>The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model\u2019s discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.</p>\n", "tags": ["Datasets", "Evaluation", "Tools-&-Libraries", "Distance-Metric-Learning"], "tsne_embedding": [-30.580707550048828, -23.848575592041016], "cluster": 5}, {"key": "liu2023transformer", "year": "2023", "citations": "0", "title": "Transformer-empowered Multi-modal Item Embedding For Enhanced Image Search In E-commerce", "abstract": "<p>Over the past decade, significant advances have been made in the field of\nimage search for e-commerce applications. Traditional image-to-image retrieval\nmodels, which focus solely on image details such as texture, tend to overlook\nuseful semantic information contained within the images. As a result, the\nretrieved products might possess similar image details, but fail to fulfil the\nuser\u2019s search goals. Moreover, the use of image-to-image retrieval models for\nproducts containing multiple images results in significant online product\nfeature storage overhead and complex mapping implementations. In this paper, we\nreport the design and deployment of the proposed Multi-modal Item Embedding\nModel (MIEM) to address these limitations. It is capable of utilizing both\ntextual information and multiple images about a product to construct meaningful\nproduct features. By leveraging semantic information from images, MIEM\neffectively supplements the image search process, improving the overall\naccuracy of retrieval results. MIEM has become an integral part of the Shopee\nimage search platform. Since its deployment in March 2023, it has achieved a\nremarkable 9.90% increase in terms of clicks per user and a 4.23% boost in\nterms of orders per user for the image search feature on the Shopee e-commerce\nplatform.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-13.0736665725708, -45.972904205322266], "cluster": 3}, {"key": "liu2024codexembed", "year": "2024", "citations": "0", "title": "Codexembed: A Generalist Embedding Model Family For Multiligual And Multi-task Code Retrieval", "abstract": "<p>Despite the success of text retrieval in many NLP tasks, code retrieval\nremains a largely underexplored area. Most text retrieval systems are tailored\nfor natural language queries, often neglecting the specific challenges of\nretrieving code. This gap leaves existing models unable to effectively capture\nthe diversity of programming languages and tasks across different domains,\nhighlighting the need for more focused research in code retrieval. To address\nthis, we introduce CodeXEmbed, a family of large-scale code embedding models\nranging from 400M to 7B parameters. Our novel training pipeline unifies\nmultiple programming languages and transforms various code-related tasks into a\ncommon retrieval framework, enhancing model generalizability and retrieval\nperformance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,\noutperforming the previous leading model, Voyage-Code, by over 20% on CoIR\nbenchmark. In addition to excelling in code retrieval, our models demonstrate\ncompetitive performance on the widely adopted BeIR text retrieval benchmark,\noffering versatility across domains. Experimental results demonstrate that\nimproving retrieval performance significantly enhances end-to-end\nRetrieval-Augmented Generation (RAG) performance for code-related tasks.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Text-Retrieval", "Scalability"], "tsne_embedding": [17.111003875732422, -16.045150756835938], "cluster": 7}, {"key": "liu2024distribution", "year": "2025", "citations": "1", "title": "Distribution-consistency-guided Multi-modal Hashing", "abstract": "<p>Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model\u2019s performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.</p>\n", "tags": ["Hashing-Methods", "AAAI", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [6.79697847366333, 19.973020553588867], "cluster": 8}, {"key": "liu2024efficient", "year": "2024", "citations": "3", "title": "Efficient Token-guided Image-text Retrieval With Consistent Multimodal Contrastive Training", "abstract": "<p>Image-text retrieval is a central problem for understanding the semantic\nrelationship between vision and language, and serves as the basis for various\nvisual and language tasks. Most previous works either simply learn\ncoarse-grained representations of the overall image and text, or elaborately\nestablish the correspondence between image regions or pixels and text words.\nHowever, the close relations between coarse- and fine-grained representations\nfor each modality are important for image-text retrieval but almost neglected.\nAs a result, such previous works inevitably suffer from low retrieval accuracy\nor heavy computational cost. In this work, we address image-text retrieval from\na novel perspective by combining coarse- and fine-grained representation\nlearning into a unified framework. This framework is consistent with human\ncognition, as humans simultaneously pay attention to the entire sample and\nregional elements to understand the semantic content. To this end, a\nToken-Guided Dual Transformer (TGDT) architecture which consists of two\nhomogeneous branches for image and text modalities, respectively, is proposed\nfor image-text retrieval. The TGDT incorporates both coarse- and fine-grained\nretrievals into a unified framework and beneficially leverages the advantages\nof both retrieval approaches. A novel training objective called Consistent\nMultimodal Contrastive (CMC) loss is proposed accordingly to ensure the intra-\nand inter-modal semantic consistencies between images and texts in the common\nembedding space. Equipped with a two-stage inference method based on the mixed\nglobal and local cross-modal similarity, the proposed method achieves\nstate-of-the-art retrieval performances with extremely low inference time when\ncompared with representative recent approaches.</p>\n", "tags": ["Tools-&-Libraries", "Text-Retrieval"], "tsne_embedding": [-30.939006805419922, -15.575989723205566], "cluster": 5}, {"key": "liu2024hashevict", "year": "2024", "citations": "0", "title": "Hashevict: A Pre-attention KV Cache Eviction Strategy Using Locality-sensitive Hashing", "abstract": "<p>Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Text-Retrieval"], "tsne_embedding": [17.966548919677734, 14.328391075134277], "cluster": 2}, {"key": "liu2024unifying", "year": "2024", "citations": "0", "title": "Unifying Latent And Lexicon Representations For Effective Video-text Retrieval", "abstract": "<p>In video-text retrieval, most existing methods adopt the dual-encoder\narchitecture for fast retrieval, which employs two individual encoders to\nextract global latent representations for videos and texts. However, they face\nchallenges in capturing fine-grained semantic concepts. In this work, we\npropose the UNIFY framework, which learns lexicon representations to capture\nfine-grained semantics and combines the strengths of latent and lexicon\nrepresentations for video-text retrieval. Specifically, we map videos and texts\ninto a pre-defined lexicon space, where each dimension corresponds to a\nsemantic concept. A two-stage semantics grounding approach is proposed to\nactivate semantically relevant dimensions and suppress irrelevant dimensions.\nThe learned lexicon representations can thus reflect fine-grained semantics of\nvideos and texts. Furthermore, to leverage the complementarity between latent\nand lexicon representations, we propose a unified learning scheme to facilitate\nmutual learning via structure sharing and self-distillation. Experimental\nresults show our UNIFY framework largely outperforms previous video-text\nretrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and\nDiDeMo respectively.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Evaluation", "Text-Retrieval"], "tsne_embedding": [-35.15260314941406, -28.319561004638672], "cluster": 5}, {"key": "liu2025collaborative", "year": "2014", "citations": "130", "title": "Collaborative Hashing", "abstract": "<p>Hashing technique has become a promising approach for\nfast similarity search. Most of existing hashing research\npursue the binary codes for the same type of entities by\npreserving their similarities. In practice, there are many\nscenarios involving nearest neighbor search on the data\ngiven in matrix form, where two different types of, yet\nnaturally associated entities respectively correspond to its\ntwo dimensions or views. To fully explore the duality\nbetween the two views, we propose a collaborative hashing\nscheme for the data in matrix form to enable fast search\nin various applications such as image search using bag of\nwords and recommendation using user-item ratings. By\nsimultaneously preserving both the entity similarities in\neach view and the interrelationship between views, our\ncollaborative hashing effectively learns the compact binary\ncodes and the explicit hash functions for out-of-sample\nextension in an alternating optimization way. Extensive\nevaluations are conducted on three well-known datasets\nfor search inside a single view and search across different\nviews, demonstrating that our proposed method outperforms\nstate-of-the-art baselines, with significant accuracy\ngains ranging from 7.67% to 45.87% relatively.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Recommender-Systems", "Similarity-Search", "Image-Retrieval", "Datasets", "Compact-Codes"], "tsne_embedding": [11.729475021362305, 9.080418586730957], "cluster": 6}, {"key": "liu2025discrete", "year": "2014", "citations": "496", "title": "Discrete Graph Hashing", "abstract": "<p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic\ndatabases. In particular, learning based hashing has received considerable\nattention due to its appealing storage and search efficiency. However, the performance\nof most unsupervised learning based hashing methods deteriorates rapidly\nas the hash code length increases. We argue that the degraded performance is due\nto inferior optimization procedures used to achieve discrete binary codes. This\npaper presents a graph-based unsupervised hashing model to preserve the neighborhood\nstructure of massive data in a discrete code space. We cast the graph\nhashing problem into a discrete optimization framework which directly learns the\nbinary codes. A tractable alternating maximization algorithm is then proposed to\nexplicitly deal with the discrete constraints, yielding high-quality codes to well\ncapture the local neighborhoods. Extensive experiments performed on four large\ndatasets with up to one million samples show that our discrete optimization based\ngraph hashing method obtains superior search accuracy over state-of-the-art unsupervised\nhashing methods, especially for longer codes.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [49.71457290649414, 8.644731521606445], "cluster": 9}, {"key": "liu2025discretely", "year": "2017", "citations": "49", "title": "Discretely Coding Semantic Rank Orders For Supervised Image Hashing", "abstract": "<p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "CVPR", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [5.050838470458984, 15.9915132522583], "cluster": 6}, {"key": "liu2025hash", "year": "2013", "citations": "91", "title": "Hash Bit Selection: A Unified Solution For Selection Problems In Hashing", "abstract": "<p>Hashing based methods recently have been shown promising for large-scale nearest neighbor search. However, good designs involve difficult decisions of many unknowns \u2013 data features, hashing algorithms, parameter settings, kernels, etc. In this paper, we provide a unified solution as hash bit selection, i.e., selecting the most informative hash bits from a pool of candidates that may have been generated under various conditions mentioned above. We represent the candidate bit pool as a vertex- and edge-weighted graph with the pooled bits as vertices. Then we formulate the bit selection problem as quadratic programming over the graph, and solve it efficiently by replicator dynamics. Extensive experiments show that our bit selection approach can achieve superior performance over both naive selection methods and state-of-the-art methods under each scenario, usually with significant accuracy gains from 10% to 50% relatively.</p>\n", "tags": ["CVPR", "Evaluation", "Hashing-Methods", "Scalability"], "tsne_embedding": [52.362728118896484, 12.879500389099121], "cluster": 9}, {"key": "liu2025hashing", "year": "2011", "citations": "861", "title": "Hashing With Graphs", "abstract": "<p>Hashing is becoming increasingly popular for\nefficient nearest neighbor search in massive\ndatabases. However, learning short codes\nthat yield good search performance is still\na challenge. Moreover, in many cases realworld\ndata lives on a low-dimensional manifold,\nwhich should be taken into account\nto capture meaningful nearest neighbors. In\nthis paper, we propose a novel graph-based\nhashing method which automatically discovers\nthe neighborhood structure inherent in\nthe data to learn appropriate compact codes.\nTo make such an approach computationally\nfeasible, we utilize Anchor Graphs to obtain\ntractable low-rank adjacency matrices. Our\nformulation allows constant time hashing of a\nnew data point by extrapolating graph Laplacian\neigenvectors to eigenfunctions. Finally,\nwe describe a hierarchical threshold learning\nprocedure in which each eigenfunction yields\nmultiple bits, leading to higher search accuracy.\nExperimental comparison with the\nother state-of-the-art methods on two large\ndatasets demonstrates the efficacy of the proposed\nmethod.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [50.525386810302734, 8.683652877807617], "cluster": 9}, {"key": "liu2025joint", "year": "2020", "citations": "148", "title": "Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval", "abstract": "<p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "SIGIR", "Multimodal-Retrieval", "Datasets", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [7.389580249786377, 13.746898651123047], "cluster": 6}, {"key": "liu2025keenhash", "year": "2025", "citations": "0", "title": "Keenhash: Hashing Programs Into Function-aware Embeddings For Large-scale Binary Code Similarity Analysis", "abstract": "<p>Binary code similarity analysis (BCSA) is a crucial research area in many fields such as cybersecurity. Specifically, function-level diffing tools are the most widely used in BCSA: they perform function matching one by one for evaluating the similarity between binary programs. However, such methods need a high time complexity, making them unscalable in large-scale scenarios (e.g., 1/n-to-n search). Towards effective and efficient program-level BCSA, we propose KEENHash, a novel hashing approach that hashes binaries into program-level representations through large language model (LLM)-generated function embeddings. KEENHash condenses a binary into one compact and fixed-length program embedding using K-Means and Feature Hashing, allowing us to do effective and efficient large-scale program-level BCSA, surpassing the previous state-of-the-art methods. The experimental results show that KEENHash is at least 215 times faster than the state-of-the-art function matching tools while maintaining effectiveness. Furthermore, in a large-scale scenario with 5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while these tools will cost at least 56 days. We also evaluate KEENHash on the program clone search of large-scale BCSA across extensive datasets in 202,305 binaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of them by at least 23.16%, and displays remarkable superiority over them in the large-scale BCSA security scenario of malware detection.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [13.645954132080078, 13.507489204406738], "cluster": 6}, {"key": "liu2025model", "year": "2020", "citations": "23", "title": "Model Optimization Boosting Framework For Linear Model Hash Learning", "abstract": "<p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [22.951955795288086, -5.49700403213501], "cluster": 6}, {"key": "liu2025multi", "year": "2015", "citations": "57", "title": "Multi-view Complementary Hash Tables For Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many\napplications (e.g., visual search, object detection, image\nmatching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.\nHowever, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build\nmultiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.\nIn\nthis paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,\nand learn discriminative hash functions in an efficient way.\nTo build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars\nshared by mis-separated neighbors. Extensive experiments\non three large-scale image datasets demonstrate that the\nproposed method significantly outperforms various naive\nsolutions and state-of-the-art multi-table methods.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [15.967546463012695, 10.366382598876953], "cluster": 6}, {"key": "liu2025supervised", "year": "2012", "citations": "1447", "title": "Supervised Hashing With Kernels", "abstract": "<p>Recent years have witnessed the growing popularity of\nhashing in large-scale vision problems. It has been shown\nthat the hashing quality could be boosted by leveraging supervised\ninformation into hash function learning. However,\nthe existing supervised methods either lack adequate performance\nor often incur cumbersome model training. In this\npaper, we propose a novel kernel-based supervised hashing\nmodel which requires a limited amount of supervised information,\ni.e., similar and dissimilar data pairs, and a feasible\ntraining cost in achieving high quality hashing. The idea\nis to map the data to compact binary codes whose Hamming\ndistances are minimized on similar pairs and simultaneously\nmaximized on dissimilar pairs. Our approach is\ndistinct from prior works by utilizing the equivalence between\noptimizing the code inner products and the Hamming\ndistances. This enables us to sequentially and efficiently\ntrain the hash functions one bit at a time, yielding very\nshort yet discriminative codes. We carry out extensive experiments\non two image benchmarks with up to one million\nsamples, demonstrating that our approach significantly outperforms\nthe state-of-the-arts in searching both metric distance\nneighbors and semantically similar neighbors, with\naccuracy gains ranging from 13% to 46%.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Scalability", "Supervised", "Evaluation"], "tsne_embedding": [20.29054832458496, -0.05285992473363876], "cluster": 6}, {"key": "liu2025webanns", "year": "2025", "citations": "0", "title": "Webanns: Fast And Efficient Approximate Nearest Neighbor Search In Web Browsers", "abstract": "<p>Approximate nearest neighbor search (ANNS) has become vital to modern AI infrastructure, particularly in retrieval-augmented generation (RAG) applications. Numerous in-browser ANNS engines have emerged to seamlessly integrate with popular LLM-based web applications, while addressing privacy protection and challenges of heterogeneous device deployments. However, web browsers present unique challenges for ANNS, including computational limitations, external storage access issues, and memory utilization constraints, which state-of-the-art (SOTA) solutions fail to address comprehensively. We propose WebANNS, a novel ANNS engine specifically designed for web browsers. WebANNS leverages WebAssembly to overcome computational bottlenecks, designs a lazy loading strategy to optimize data retrieval from external storage, and applies a heuristic approach to reduce memory usage. Experiments show that WebANNS is fast and memory efficient, achieving up to \\(743.8\\times\\) improvement in 99th percentile query latency over the SOTA engine, while reducing memory usage by up to 39%. Note that WebANNS decreases query time from 10 seconds to the 10-millisecond range in browsers, making in-browser ANNS practical with user-acceptable latency.</p>\n", "tags": ["Efficiency", "SIGIR", "Memory-Efficiency", "Privacy-&-Security"], "tsne_embedding": [39.287574768066406, 17.670364379882812], "cluster": 2}, {"key": "llabr\u00e9s2024image", "year": "2024", "citations": "0", "title": "Image-text Matching For Large-scale Book Collections", "abstract": "<p>We address the problem of detecting and mapping all books in a collection of images to entries in a given book catalogue. Instead of performing independent retrieval for each book detected, we treat the image-text mapping problem as a many-to-many matching process, looking for the best overall match between the two sets. We combine a state-of-the-art segmentation method (SAM) to detect book spines and extract book information using a commercial OCR. We then propose a two-stage approach for text-image matching, where CLIP embeddings are used first for fast matching, followed by a second slower stage to refine the matching, employing either the Hungarian Algorithm or a BERT-based model trained to cope with noisy OCR input and partial text matches. To evaluate our approach, we publish a new dataset of annotated bookshelf images that covers the whole book collection of a public library in Spain. In addition, we provide two target lists of book metadata, a closed-set of 15k book titles that corresponds to the known library inventory, and an open-set of 2.3M book titles to simulate an open-world scenario. We report results on two settings, on one hand on a matching-only task, where the book segments and OCR is given and the objective is to perform many-to-many matching against the target lists, and a combined detection and matching task, where books must be first detected and recognised before they are matched to the target list entries. We show that both the Hungarian Matching and the proposed BERT-based model outperform a fuzzy string matching baseline, and we highlight inherent limitations of the matching algorithms as the target increases in size, and when either of the two sets (detected books or target book list) is incomplete. The dataset and code are available at https://github.com/llabres/library-dataset</p>\n", "tags": ["Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [26.07741355895996, 17.765962600708008], "cluster": 2}, {"key": "loncaric2018convolutional", "year": "2018", "citations": "1", "title": "Convolutional Hashing For Automated Scene Matching", "abstract": "<p>We present a powerful new loss function and training scheme for learning\nbinary hash functions. In particular, we demonstrate our method by creating for\nthe first time a neural network that outperforms state-of-the-art Haar wavelets\nand color layout descriptors at the task of automated scene matching. By\naccurately relating distance on the manifold of network outputs to distance in\nHamming space, we achieve a 100-fold reduction in nontrivial false positive\nrate and significantly higher true positive rate. We expect our insights to\nprovide large wins for hashing models applied to other information retrieval\nhashing tasks as well.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [24.271282196044922, 7.521005630493164], "cluster": 2}, {"key": "loncaric2018learning", "year": "2018", "citations": "4", "title": "Learning Hash Codes Via Hamming Distance Targets", "abstract": "<p>We present a powerful new loss function and training scheme for learning\nbinary hash codes with any differentiable model and similarity function. Our\nloss function improves over prior methods by using log likelihood loss on top\nof an accurate approximation for the probability that two inputs fall within a\nHamming distance target. Our novel training scheme obtains a good estimate of\nthe true gradient by better sampling inputs and evaluating loss terms between\nall pairs of inputs in each minibatch. To fully leverage the resulting hashes,\nwe use multi-indexing. We demonstrate that these techniques provide large\nimprovements to a similarity search tasks. We report the best results to date\non competitive information retrieval tasks for ImageNet and SIFT 1M, improving\nMAP from 73% to 84% and reducing query cost by a factor of 2-8, respectively.</p>\n", "tags": ["Vector-Indexing", "Evaluation", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [-20.10967445373535, 17.398252487182617], "cluster": 8}, {"key": "long2018deep", "year": "2018", "citations": "22", "title": "Deep Domain Adaptation Hashing With Adversarial Learning", "abstract": "<p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Robustness", "SIGIR", "Tools-&-Libraries"], "tsne_embedding": [1.409208059310913, 3.9388539791107178], "cluster": 6}, {"key": "long2018filter", "year": "2018", "citations": "1", "title": "A Filter Of Minhash For Image Similarity Measures", "abstract": "<p>Image similarity measures play an important role in nearest neighbor search\nand duplicate detection for large-scale image datasets. Recently, Minwise\nHashing (or Minhash) and its related hashing algorithms have achieved great\nperformances in large-scale image retrieval systems. However, there are a large\nnumber of comparisons for image pairs in these applications, which may spend a\nlot of computation time and affect the performance. In order to quickly obtain\nthe pairwise images that theirs similarities are higher than the specific\nthreshold T (e.g., 0.5), we propose a dynamic threshold filter of Minwise\nHashing for image similarity measures. It greatly reduces the calculation time\nby terminating the unnecessary comparisons in advance. We also find that the\nfilter can be extended to other hashing algorithms, on when the estimator\nsatisfies the binomial distribution, such as b-Bit Minwise Hashing, One\nPermutation Hashing, etc. In this pager, we use the Bag-of-Visual-Words (BoVW)\nmodel based on the Scale Invariant Feature Transform (SIFT) to represent the\nimage features. We have proved that the filter is correct and effective through\nthe experiment on real image datasets.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-22.54787826538086, 10.232600212097168], "cluster": 8}, {"key": "long2022adaptive", "year": "2022", "citations": "0", "title": "Adaptive Asymmetric Label-guided Hashing For Multimedia Search", "abstract": "<p>With the rapid growth of multimodal media data on the Web in recent years,\nhash learning methods as a way to achieve efficient and flexible cross-modal\nretrieval of massive multimedia data have received a lot of attention from the\ncurrent Web resource retrieval research community. Existing supervised hashing\nmethods simply transform label information into pairwise similarity information\nto guide hash learning, leading to a potential risk of semantic error in the\nface of multi-label data. In addition, most existing hash optimization methods\nsolve NP-hard optimization problems by employing approximate approximation\nstrategies based on relaxation strategies, leading to a large quantization\nerror. In order to address above obstacles, we present a simple yet efficient\nAdaptive Asymmetric Label-guided Hashing, named A2LH, for Multimedia Search.\nSpecifically, A2LH is a two-step hashing method. In the first step, we design\nan association representation model between the different modality\nrepresentations and semantic label representation separately, and use the\nsemantic label representation as an intermediate bridge to solve the semantic\ngap existing between different modalities. In addition, we present an efficient\ndiscrete optimization algorithm for solving the quantization error problem\ncaused by relaxation-based optimization algorithms. In the second step, we\nleverage the generated hash codes to learn the hash mapping functions. The\nexperimental results show that our proposed method achieves optimal performance\non all compared baseline methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Supervised", "Evaluation"], "tsne_embedding": [2.46125864982605, 1.2542848587036133], "cluster": 6}, {"key": "long2022retrieval", "year": "2022", "citations": "40", "title": "Retrieval Augmented Classification For Long-tail Visual Recognition", "abstract": "<p>We introduce Retrieval Augmented Classification (RAC), a generic approach to\naugmenting standard image classification pipelines with an explicit retrieval\nmodule. RAC consists of a standard base image encoder fused with a parallel\nretrieval branch that queries a non-parametric external memory of pre-encoded\nimages and associated text snippets. We apply RAC to the problem of long-tail\nclassification and demonstrate a significant improvement over previous\nstate-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7%\nrespectively), despite using only the training datasets themselves as the\nexternal information source. We demonstrate that RAC\u2019s retrieval module,\nwithout prompting, learns a high level of accuracy on tail classes. This, in\nturn, frees the base encoder to focus on common classes, and improve its\nperformance thereon. RAC represents an alternative approach to utilizing large,\npretrained models without requiring fine-tuning, as well as a first step\ntowards more effectively making use of external memory within common computer\nvision architectures.</p>\n", "tags": ["CVPR", "Evaluation", "Datasets"], "tsne_embedding": [-20.729158401489258, -20.886520385742188], "cluster": 5}, {"key": "long2024cfir", "year": "2024", "citations": "1", "title": "CFIR: Fast And Effective Long-text To Image Retrieval For Large Corpora", "abstract": "<p>Text-to-image retrieval aims to find the relevant images based on a text\nquery, which is important in various use-cases, such as digital libraries,\ne-commerce, and multimedia databases. Although Multimodal Large Language Models\n(MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in\nhandling large-scale, diverse, and ambiguous real-world needs of retrieval, due\nto the computation cost and the injective embeddings they produce. This paper\npresents a two-stage Coarse-to-Fine Index-shared Retrieval (CFIR) framework,\ndesigned for fast and effective large-scale long-text to image retrieval. The\nfirst stage, Entity-based Ranking (ER), adapts to long-text query ambiguity by\nemploying a multiple-queries-to-multiple-targets paradigm, facilitating\ncandidate filtering for the next stage. The second stage, Summary-based\nRe-ranking (SR), refines these rankings using summarized queries. We also\npropose a specialized Decoupling-BEiT-3 encoder, optimized for handling\nambiguous user needs and both stages, which also enhances computational\nefficiency through vector-based similarity inference. Evaluation on the AToMiC\ndataset reveals that CFIR surpasses existing MLLMs by up to 11.06% in\nRecall@1000, while reducing training and retrieval times by 68.75% and 99.79%,\nrespectively. We will release our code to facilitate future research at\nhttps://github.com/longkukuhi/CFIR.</p>\n", "tags": ["Efficiency", "Scalability", "Image-Retrieval", "SIGIR", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-23.748638153076172, 1.428164005279541], "cluster": 1}, {"key": "long2025deep", "year": "2018", "citations": "22", "title": "Deep Domain Adaptation Hashing With Adversarial Learning", "abstract": "<p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Robustness", "SIGIR", "Tools-&-Libraries"], "tsne_embedding": [1.409227967262268, 3.9388716220855713], "cluster": 6}, {"key": "loyman2020semi", "year": "2020", "citations": "3", "title": "Semi-supervised Lung Nodule Retrieval", "abstract": "<p>Content based image retrieval (CBIR) provides the clinician with visual\ninformation that can support, and hopefully improve, his or her decision making\nprocess. Given an input query image, a CBIR system provides as its output a set\nof images, ranked by similarity to the query image. Retrieved images may come\nwith relevant information, such as biopsy-based malignancy labeling, or\ncategorization. Ground truth on similarity between dataset elements (e.g.\nbetween nodules) is not readily available, thus greatly challenging machine\nlearning methods. Such annotations are particularly difficult to obtain, due to\nthe subjective nature of the task, with high inter-observer variability\nrequiring multiple expert annotators. Consequently, past approaches have\nfocused on manual feature extraction, while current approaches use auxiliary\ntasks, such as a binary classification task (e.g. malignancy), for which\nground-true is more readily accessible. However, in a previous study, we have\nshown that binary auxiliary tasks are inferior to the usage of a rough\nsimilarity estimate that are derived from data annotations. The current study\nsuggests a semi-supervised approach that involves two steps: 1) Automatic\nannotation of a given partially labeled dataset; 2) Learning a semantic\nsimilarity metric space based on the predicated annotations. The proposed\nsystem is demonstrated in lung nodule retrieval using the LIDC dataset, and\nshows that it is feasible to learn embedding from predicted ratings. The\nsemi-supervised approach has demonstrated a significantly higher discriminative\nability than the fully-unsupervised reference.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [-48.39488220214844, 17.337623596191406], "cluster": 0}, {"key": "lu2018domain", "year": "2021", "citations": "12", "title": "Domain-aware SE Network For Sketch-based Image Retrieval With Multiplicative Euclidean Margin Softmax", "abstract": "<p>This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),\nfor which the key is to bridge the gap between sketches and photos in terms of\nthe data representation. Inspired by channel-wise attention explored in recent\nyears, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which\nseamlessly incorporates the prior knowledge of sample sketch or photo into SE\nmodule and make the SE module capable of emphasizing appropriate channels\naccording to domain signal. Accordingly, the proposed network can switch its\nmode to achieve a better domain feature with lower intra-class discrepancy.\nMoreover, while previous works simply focus on minimizing intra-class distance\nand maximizing inter-class distance, we introduce a loss function, named\nMultiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative\nEuclidean margin into feature space and ensure that the maximum intra-class\ndistance is smaller than the minimum inter-class distance. This facilitates\nlearning a highly discriminative feature space and ensures a more accurate\nimage retrieval result. Extensive experiments are conducted on two widely used\nSBIR benchmark datasets. Our approach achieves better results on both datasets,\nsurpassing the state-of-the-art methods by a large margin.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-42.7216796875, -13.39194393157959], "cluster": 5}, {"key": "lu2018fmhash", "year": "2019", "citations": "9", "title": "Fmhash: Deep Hashing Of In-air-handwriting For User Identification", "abstract": "<p>Many mobile systems and wearable devices, such as Virtual Reality (VR) or\nAugmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID\nand password for signing into a virtual website. However, they are usually\nequipped with gesture capture interfaces to allow the user to interact with the\nsystem directly with hand gestures. Although gesture-based authentication has\nbeen well-studied, less attention is paid to the gesture-based user\nidentification problem, which is essentially an input method of account ID and\nan efficient searching and indexing method of a database of gesture signals. In\nthis paper, we propose FMHash (i.e., Finger Motion Hash), a user identification\nframework that can generate a compact binary hash code from a piece of\nin-air-handwriting of an ID string. This hash code enables indexing and fast\nsearch of a large account database using the in-air-handwriting by a hash\ntable. To demonstrate the effectiveness of the framework, we implemented a\nprototype and achieved &gt;99.5% precision and &gt;92.6% recall with exact hash code\nmatch on a dataset of 200 accounts collected by us. The ability of hashing\nin-air-handwriting pattern to binary code can be used to achieve convenient\nsign-in and sign-up with in-air-handwriting gesture ID on future mobile and\nwearable systems connected to the Internet.</p>\n", "tags": ["Hashing-Methods", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation", "Neural-Hashing"], "tsne_embedding": [17.02086067199707, 16.004594802856445], "cluster": 2}, {"key": "lu2019online", "year": "2019", "citations": "128", "title": "Online Multi-modal Hashing With Dynamic Query-adaption", "abstract": "<p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Quantization", "Scalability", "SIGIR", "Memory-Efficiency", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [9.237913131713867, 15.491659164428711], "cluster": 6}, {"key": "lu2021deep", "year": "2022", "citations": "3", "title": "Deep Asymmetric Hashing With Dual Semantic Regression And Class Structure Quantization", "abstract": "<p>Recently, deep hashing methods have been widely used in image retrieval task.\nMost existing deep hashing approaches adopt one-to-one quantization to reduce\ninformation loss. However, such class-unrelated quantization cannot give\ndiscriminative feedback for network training. In addition, these methods only\nutilize single label to integrate supervision information of data for hashing\nfunction learning, which may result in inferior network generalization\nperformance and relatively low-quality hash codes since the inter-class\ninformation of data is totally ignored. In this paper, we propose a dual\nsemantic asymmetric hashing (DSAH) method, which generates discriminative hash\ncodes under three-fold constraints. Firstly, DSAH utilizes class prior to\nconduct class structure quantization so as to transmit class information during\nthe quantization process. Secondly, a simple yet effective label mechanism is\ndesigned to characterize both the intra-class compactness and inter-class\nseparability of data, thereby achieving semantic-sensitive binary code\nlearning. Finally, a meaningful pairwise similarity preserving loss is devised\nto minimize the distances between class-related network outputs based on an\naffinity graph. With these three main components, high-quality hash codes can\nbe generated through network. Extensive experiments conducted on various\ndatasets demonstrate the superiority of DSAH in comparison with\nstate-of-the-art deep hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-0.3412335515022278, 9.105256080627441], "cluster": 6}, {"key": "lu2021label", "year": "2021", "citations": "1", "title": "Label Self-adaption Hashing For Image Retrieval", "abstract": "<p>Hashing has attracted widespread attention in image retrieval because of its fast retrieval speed and low storage cost. Compared with supervised methods, unsupervised hashing methods are more reasonable and suitable for large-scale image retrieval since it is always difficult and expensive to collect true labels of the massive data. Without label information, however, unsupervised hashing methods can not guarantee the quality of learned binary codes. To resolve this dilemma, this paper proposes a novel unsupervised hashing method called Label Self-Adaption Hashing (LSAH), which contains effective hashing function learning part and self-adaption label generation part. In the first part, we utilize anchor graph to keep the local structure of the data and introduce joint sparsity into the model to extract effective features for high-quality binary code learning. In the second part, a self-adaptive cluster label matrix is learned from the data under the assumption that the nearest neighbor points should have a large probability to be in the same cluster. Therefore, the proposed LSAH can make full use of the potential discriminative information of the data to guide the learning of binary code. It is worth noting that LSAH can learn effective binary codes, hashing function and cluster labels simultaneously in a unified optimization framework. To solve the resulting optimization problem, an Augmented Lagrange Multiplier based iterative algorithm is elaborately designed. Extensive experiments on three large-scale data sets indicate the promising performance of the proposed LSAH.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [43.06924057006836, 0.6654055118560791], "cluster": 9}, {"key": "lu2021learnable", "year": "2022", "citations": "23", "title": "Learnable Locality-sensitive Hashing For Video Anomaly Detection", "abstract": "<p>Video anomaly detection (VAD) mainly refers to identifying anomalous events\nthat have not occurred in the training set where only normal samples are\navailable. Existing works usually formulate VAD as a reconstruction or\nprediction problem. However, the adaptability and scalability of these methods\nare limited. In this paper, we propose a novel distance-based VAD method to\ntake advantage of all the available normal data efficiently and flexibly. In\nour method, the smaller the distance between a testing sample and normal\nsamples, the higher the probability that the testing sample is normal.\nSpecifically, we propose to use locality-sensitive hashing (LSH) to map samples\nwhose similarity exceeds a certain threshold into the same bucket in advance.\nIn this manner, the complexity of near neighbor search is cut down\nsignificantly. To make the samples that are semantically similar get closer and\nsamples not similar get further apart, we propose a novel learnable version of\nLSH that embeds LSH into a neural network and optimizes the hash functions with\ncontrastive learning strategy. The proposed method is robust to data imbalance\nand can handle the large intra-class variations in normal data flexibly.\nBesides, it has a good ability of scalability. Extensive experiments\ndemonstrate the superiority of our method, which achieves new state-of-the-art\nresults on VAD benchmarks.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "Evaluation"], "tsne_embedding": [22.47688102722168, 29.10137939453125], "cluster": 2}, {"key": "lu2021less", "year": "2021", "citations": "2", "title": "Less Is More: Pre-train A Strong Text Encoder For Dense Retrieval Using A Weak Decoder", "abstract": "<p>Dense retrieval requires high-quality text sequence embeddings to support\neffective search in the representation space. Autoencoder-based language models\nare appealing in dense retrieval as they train the encoder to output\nhigh-quality embedding that can reconstruct the input texts. However, in this\npaper, we provide theoretical analyses and show empirically that an autoencoder\nlanguage model with a low reconstruction loss may not provide good sequence\nrepresentations because the decoder may take shortcuts by exploiting language\npatterns. To address this, we propose a new self-learning method that\npre-trains the autoencoder using a \\textit{weak} decoder, with restricted\ncapacity and attention flexibility to push the encoder to provide better text\nrepresentations. Our experiments on web search, news recommendation, and open\ndomain question answering show that our pre-trained model significantly boosts\nthe effectiveness and few-shot ability of dense retrieval models. Our code is\navailable at https://github.com/microsoft/SEED-Encoder/.</p>\n", "tags": ["Recommender-Systems", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [13.487386703491211, -15.573357582092285], "cluster": 7}, {"key": "lu2021slosh", "year": "2024", "citations": "2", "title": "SLOSH: Set Locality Sensitive Hashing Via Sliced-wasserstein Embeddings", "abstract": "<p>Learning from set-structured data is an essential problem with many\napplications in machine learning and computer vision. This paper focuses on\nnon-parametric and data-independent learning from set-structured data using\napproximate nearest neighbor (ANN) solutions, particularly locality-sensitive\nhashing. We consider the problem of set retrieval from an input set query. Such\nretrieval problem requires: 1) an efficient mechanism to calculate the\ndistances/dissimilarities between sets, and 2) an appropriate data structure\nfor fast nearest neighbor search. To that end, we propose Sliced-Wasserstein\nset embedding as a computationally efficient \u201cset-2-vector\u201d mechanism that\nenables downstream ANN, with theoretical guarantees. The set elements are\ntreated as samples from an unknown underlying distribution, and the\nSliced-Wasserstein distance is used to compare sets. We demonstrate the\neffectiveness of our algorithm, denoted as Set-LOcality Sensitive Hashing\n(SLOSH), on various set retrieval datasets and compare our proposed embedding\nwith standard set embedding approaches, including Generalized Mean (GeM)\nembedding/pooling, Featurewise Sort Pooling (FSPool), and Covariance Pooling\nand show consistent improvement in retrieval results. The code for replicating\nour results is available here:\n\\href{https://github.com/mint-vu/SLOSH}{https://github.com/mint-vu/SLOSH}.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [17.48375701904297, -6.912607669830322], "cluster": 6}, {"key": "lu2021visualsparta", "year": "2021", "citations": "18", "title": "Visualsparta: An Embarrassingly Simple Approach To Large-scale Text-to-image Search With Weighted Bag-of-words", "abstract": "<p>Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Scalability", "Multimodal-Retrieval", "Datasets"], "tsne_embedding": [-23.57436752319336, 23.04426383972168], "cluster": 8}, {"key": "lu2022asymmetric", "year": "2023", "citations": "8", "title": "Asymmetric Transfer Hashing With Adaptive Bipartite Graph Learning", "abstract": "<p>Thanks to the efficient retrieval speed and low storage consumption, learning\nto hash has been widely used in visual retrieval tasks. However, existing\nhashing methods assume that the query and retrieval samples lie in homogeneous\nfeature space within the same domain. As a result, they cannot be directly\napplied to heterogeneous cross-domain retrieval. In this paper, we propose a\nGeneralized Image Transfer Retrieval (GITR) problem, which encounters two\ncrucial bottlenecks: 1) the query and retrieval samples may come from different\ndomains, leading to an inevitable {domain distribution gap}; 2) the features of\nthe two domains may be heterogeneous or misaligned, bringing up an additional\n{feature gap}. To address the GITR problem, we propose an Asymmetric Transfer\nHashing (ATH) framework with its unsupervised/semi-supervised/supervised\nrealizations. Specifically, ATH characterizes the domain distribution gap by\nthe discrepancy between two asymmetric hash functions, and minimizes the\nfeature gap with the help of a novel adaptive bipartite graph constructed on\ncross-domain data. By jointly optimizing asymmetric hash functions and the\nbipartite graph, not only can knowledge transfer be achieved but information\nloss caused by feature alignment can also be avoided. Meanwhile, to alleviate\nnegative transfer, the intrinsic geometrical structure of single-domain data is\npreserved by involving a domain affinity graph. Extensive experiments on both\nsingle-domain and cross-domain benchmarks under different GITR subtasks\nindicate the superiority of our ATH method in comparison with the\nstate-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Similarity-Search", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [2.298297166824341, 13.400465965270996], "cluster": 8}, {"key": "lu2023attributes", "year": "2023", "citations": "10", "title": "Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval", "abstract": "<p>In recent years, hashing methods have been popular in the large-scale media\nsearch for low storage and strong representation capabilities. To describe\nobjects with similar overall appearance but subtle differences, more and more\nstudies focus on hashing-based fine-grained image retrieval. Existing hashing\nnetworks usually generate both local and global features through attention\nguidance on the same deep activation tensor, which limits the diversity of\nfeature representations. To handle this limitation, we substitute convolutional\ndescriptors for attention-guided features and propose an Attributes Grouping\nand Mining Hashing (AGMH), which groups and embeds the category-specific visual\nattributes in multiple descriptors to generate a comprehensive feature\nrepresentation for efficient fine-grained image retrieval. Specifically, an\nAttention Dispersion Loss (ADL) is designed to force the descriptors to attend\nto various local regions and capture diverse subtle details. Moreover, we\npropose a Stepwise Interactive External Attention (SIEA) to mine critical\nattributes in each descriptor and construct correlations between fine-grained\nattributes and objects. The attention mechanism is dedicated to learning\ndiscrete attributes, which will not cost additional computations in hash codes\ngeneration. Finally, the compact binary codes are learned by preserving\npairwise similarities. Experimental results demonstrate that AGMH consistently\nyields the best performance against state-of-the-art methods on fine-grained\nbenchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-26.058828353881836, 4.040355682373047], "cluster": 0}, {"key": "lu2024adapting", "year": "2024", "citations": "0", "title": "Adapting Pre-trained Vision Models For Novel Instance Detection And Segmentation", "abstract": "<p>Novel Instance Detection and Segmentation (NIDS) aims at detecting and\nsegmenting novel object instances given a few examples of each instance. We\npropose a unified, simple, yet effective framework (NIDS-Net) comprising object\nproposal generation, embedding creation for both instance templates and\nproposal regions, and embedding matching for instance label assignment.\nLeveraging recent advancements in large vision methods, we utilize Grounding\nDINO and Segment Anything Model (SAM) to obtain object proposals with accurate\nbounding boxes and masks. Central to our approach is the generation of\nhigh-quality instance embeddings. We utilized foreground feature averages of\npatch embeddings from the DINOv2 ViT backbone, followed by refinement through a\nweight adapter mechanism that we introduce.\n  We show experimentally that our weight adapter can adjust the embeddings\nlocally within their feature space and effectively limit overfitting in the\nfew-shot setting. Furthermore, the weight adapter optimizes weights to enhance\nthe distinctiveness of instance embeddings during similarity computation. This\nmethodology enables a straightforward matching strategy that results in\nsignificant performance gains. Our framework surpasses current state-of-the-art\nmethods, demonstrating notable improvements in four detection datasets. In the\nsegmentation tasks on seven core datasets of the BOP challenge, our method\noutperforms the leading published RGB methods and remains competitive with the\nbest RGB-D method. We have also verified our method using real-world images\nfrom a Fetch robot and a RealSense camera. Project Page:\nhttps://irvlutd.github.io/NIDSNet/</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [-42.128021240234375, 7.240268707275391], "cluster": 0}, {"key": "lu2025dogr", "year": "2025", "citations": "0", "title": "DOGR: Leveraging Document-oriented Contrastive Learning In Generative Retrieval", "abstract": "<p>Generative retrieval constitutes an innovative approach in information\nretrieval, leveraging generative language models (LM) to generate a ranked list\nof document identifiers (docid) for a given query. It simplifies the retrieval\npipeline by replacing the large external index with model parameters. However,\nexisting works merely learned the relationship between queries and document\nidentifiers, which is unable to directly represent the relevance between\nqueries and documents. To address the above problem, we propose a novel and\ngeneral generative retrieval framework, namely Leveraging Document-Oriented\nContrastive Learning in Generative Retrieval (DOGR), which leverages\ncontrastive learning to improve generative retrieval tasks. It adopts a\ntwo-stage learning strategy that captures the relationship between queries and\ndocuments comprehensively through direct interactions. Furthermore, negative\nsampling methods and corresponding contrastive learning objectives are\nimplemented to enhance the learning of semantic representations, thereby\npromoting a thorough comprehension of the relationship between queries and\ndocuments. Experimental results demonstrate that DOGR achieves state-of-the-art\nperformance compared to existing generative retrieval methods on two public\nbenchmark datasets. Further experiments have shown that our framework is\ngenerally effective for common identifier construction techniques.</p>\n", "tags": ["Self-Supervised", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [-1.278212547302246, -25.26025390625], "cluster": 3}, {"key": "lu2025label", "year": "2021", "citations": "1", "title": "Label Self-adaption Hashing For Image Retrieval", "abstract": "<p>Hashing has attracted widespread attention in image retrieval because of its fast retrieval speed and low storage cost. Compared with supervised methods, unsupervised hashing methods are more reasonable and suitable for large-scale image retrieval since it is always difficult and expensive to collect true labels of the massive data. Without label information, however, unsupervised hashing methods can not guarantee the quality of learned binary codes. To resolve this dilemma, this paper proposes a novel unsupervised hashing method called Label Self-Adaption Hashing (LSAH), which contains effective hashing function learning part and self-adaption label generation part. In the first part, we utilize anchor graph to keep the local structure of the data and introduce joint sparsity into the model to extract effective features for high-quality binary code learning. In the second part, a self-adaptive cluster label matrix is learned from the data under the assumption that the nearest neighbor points should have a large probability to be in the same cluster. Therefore, the proposed LSAH can make full use of the potential discriminative information of the data to guide the learning of binary code. It is worth noting that LSAH can learn effective binary codes, hashing function and cluster labels simultaneously in a unified optimization framework. To solve the resulting optimization problem, an Augmented Lagrange Multiplier based iterative algorithm is elaborately designed. Extensive experiments on three large-scale data sets indicate the promising performance of the proposed LSAH.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [43.06969451904297, 0.6654689311981201], "cluster": 9}, {"key": "lu2025online", "year": "2019", "citations": "128", "title": "Online Multi-modal Hashing With Dynamic Query-adaption", "abstract": "<p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Quantization", "Scalability", "SIGIR", "Memory-Efficiency", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [9.2379150390625, 15.491661071777344], "cluster": 6}, {"key": "luo2017arrays", "year": "2017", "citations": "5", "title": "Arrays Of (locality-sensitive) Count Estimators (ACE): High-speed Anomaly Detection Via Cache Lookups", "abstract": "<p>Anomaly detection is one of the frequent and important subroutines deployed\nin large-scale data processing systems. Even being a well-studied topic,\nexisting techniques for unsupervised anomaly detection require storing\nsignificant amounts of data, which is prohibitive from memory and latency\nperspective. In the big-data world existing methods fail to address the new set\nof memory and latency constraints. In this paper, we propose ACE (Arrays of\n(locality-sensitive) Count Estimators) algorithm that can be 60x faster than\nthe ELKI package~\\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastest\nimplementation of the unsupervised anomaly detection algorithms. ACE algorithm\nrequires less than \\(4MB\\) memory, to dynamically compress the full data\ninformation into a set of count arrays. These tiny \\(4MB\\) arrays of counts are\nsufficient for unsupervised anomaly detection. At the core of the ACE\nalgorithm, there is a novel statistical estimator which is derived from the\nsampling view of Locality Sensitive Hashing(LSH). This view is significantly\ndifferent and efficient than the widely popular view of LSH for near-neighbor\nsearch. We show the superiority of ACE algorithm over 11 popular baselines on 3\nbenchmark datasets, including the KDD-Cup99 data which is the largest available\nbenchmark comprising of more than half a million entries with ground truth\nanomaly labels.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [31.046554565429688, 28.283540725708008], "cluster": 2}, {"key": "luo2018collaborative", "year": "2020", "citations": "14", "title": "Collaborative Learning For Extremely Low Bit Asymmetric Hashing", "abstract": "<p>Hashing techniques are in great demand for a wide range of real-world\napplications such as image retrieval and network compression. Nevertheless,\nexisting approaches could hardly guarantee a satisfactory performance with the\nextremely low-bit (e.g., 4-bit) hash codes due to the severe information loss\nand the shrink of the discrete solution space. In this paper, we propose a\nnovel \\textit{Collaborative Learning} strategy that is tailored for generating\nhigh-quality low-bit hash codes. The core idea is to jointly distill\nbit-specific and informative representations for a group of pre-defined code\nlengths. The learning of short hash codes among the group can benefit from the\nmanifold shared with other long codes, where multiple views from different hash\ncodes provide the supplementary guidance and regularization, making the\nconvergence faster and more stable. To achieve that, an asymmetric hashing\nframework with two variants of multi-head embedding structures is derived,\ntermed as Multi-head Asymmetric Hashing (MAH), leading to great efficiency of\ntraining and querying. Extensive experiments on three benchmark datasets have\nbeen conducted to verify the superiority of the proposed MAH, and have shown\nthat the 8-bit hash codes generated by MAH achieve \\(94.3%\\) of the MAP (Mean\nAverage Precision (MAP)) score on the CIFAR-10 dataset, which significantly\nsurpasses the performance of the 48-bit codes by the state-of-the-arts in image\nretrieval tasks.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-0.6972976326942444, 17.34808921813965], "cluster": 8}, {"key": "luo2018fast", "year": "2018", "citations": "89", "title": "Fast Scalable Supervised Hashing", "abstract": "<p>Despite significant progress in supervised hashing, there are three\ncommon limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning\nprocedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply\nsampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods\ntry to replace the large matrix with a smaller one, but the size is\nstill large. Third, among the methods that leverage the pairwise\nsimilarity matrix, most of them only encode the semantic label\ninformation in learning the hash codes, failing to fully capture\nthe characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing\n(FSSH), which circumvents the use of the large similarity matrix by\nintroducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn\nthe hash codes with not only the semantic information but also\nthe features of data. Extensive experiments on three widely used\ndatasets demonstrate its superiority over several state-of-the-art\nmethods in both accuracy and scalability. Our experiment codes\nare available at: https://lcbwlx.wixsite.com/fssh.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "SIGIR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [23.052478790283203, 0.32615014910697937], "cluster": 6}, {"key": "luo2019snap", "year": "2019", "citations": "4", "title": "Snap And Find: Deep Discrete Cross-domain Garment Image Retrieval", "abstract": "<p>With the increasing number of online stores, there is a pressing need for\nintelligent search systems to understand the item photos snapped by customers\nand search against large-scale product databases to find their desired items.\nHowever, it is challenging for conventional retrieval systems to match up the\nitem photos captured by customers and the ones officially released by stores,\nespecially for garment images. To bridge the customer- and store- provided\ngarment photos, existing studies have been widely exploiting the clothing\nattributes (\\textit{e.g.,} black) and landmarks (\\textit{e.g.,} collar) to\nlearn a common embedding space for garment representations. Unfortunately they\nomit the sequential correlation of attributes and consume large quantity of\nhuman labors to label the landmarks. In this paper, we propose a deep\nmulti-task cross-domain hashing termed \\textit{DMCH}, in which cross-domain\nembedding and sequential attribute learning are modeled simultaneously.\nSequential attribute learning not only provides the semantic guidance for\nembedding, but also generates rich attention on discriminative local details\n(\\textit{e.g.,} black buttons) of clothing items without requiring extra\nlandmark labels. This leads to promising performance and 306\\(\\times\\) boost on\nefficiency when compared with the state-of-the-art models, which is\ndemonstrated through rigorous experiments on two public fashion datasets.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-17.501502990722656, -49.17854690551758], "cluster": 3}, {"key": "luo2020cimon", "year": "2021", "citations": "26", "title": "CIMON: Towards High-quality Hash Codes", "abstract": "<p>Recently, hashing is widely used in approximate nearest neighbor search for\nits storage and computational efficiency. Most of the unsupervised hashing\nmethods learn to map images into semantic similarity-preserving hash codes by\nconstructing local semantic similarity structure from the pre-trained model as\nthe guiding information, i.e., treating each point pair similar if their\ndistance is small in feature space. However, due to the inefficient\nrepresentation ability of the pre-trained model, many false positives and\nnegatives in local semantic similarity will be introduced and lead to error\npropagation during the hash code learning. Moreover, few of the methods\nconsider the robustness of models, which will cause instability of hash codes\nto disturbance. In this paper, we propose a new method named\n{\\textbf{C}}omprehensive s{\\textbf{I}}milarity {\\textbf{M}}ining and\nc{\\textbf{O}}nsistency lear{\\textbf{N}}ing (CIMON). First, we use global\nrefinement and similarity statistical distribution to obtain reliable and\nsmooth guidance. Second, both semantic and contrastive consistency learning are\nintroduced to derive both disturb-invariant and discriminative hash codes.\nExtensive experiments on several benchmark datasets show that the proposed\nmethod outperforms a wide range of state-of-the-art methods in both retrieval\nperformance and robustness.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "AAAI", "Efficiency", "Hashing-Methods", "Supervised", "Unsupervised", "IJCAI", "Robustness"], "tsne_embedding": [-0.49347952008247375, 14.690204620361328], "cluster": 8}, {"key": "luo2020survey", "year": "2022", "citations": "123", "title": "A Survey On Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims to obtain the samples in the database with the\nsmallest distances from them to the queries, which is a basic task in a range\nof fields, including computer vision and data mining. Hashing is one of the\nmost widely used methods for its computational and storage efficiency. With the\ndevelopment of deep learning, deep hashing methods show more advantages than\ntraditional methods. In this survey, we detailedly investigate current deep\nhashing algorithms including deep supervised hashing and deep unsupervised\nhashing. Specifically, we categorize deep supervised hashing methods into\npairwise methods, ranking-based methods, pointwise methods as well as\nquantization according to how measuring the similarities of the learned hash\ncodes. Moreover, deep unsupervised hashing is categorized into similarity\nreconstruction-based methods, pseudo-label-based methods and prediction-free\nself-supervised learning-based methods based on their semantic learning\nmanners. We also introduce three related important topics including\nsemi-supervised deep hashing, domain adaption deep hashing and multi-modal deep\nhashing. Meanwhile, we present some commonly used public datasets and the\nscheme to measure the performance of deep hashing algorithms. Finally, we\ndiscuss some potential research directions in conclusion.</p>\n", "tags": ["Survey-Paper", "Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Quantization", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [11.962996482849121, 5.215452671051025], "cluster": 6}, {"key": "luo2021deep", "year": "2021", "citations": "2", "title": "Deep Unsupervised Hashing By Distilled Smooth Guidance", "abstract": "<p>Hashing has been widely used in approximate nearest neighbor search for its\nstorage and computational efficiency. Deep supervised hashing methods are not\nwidely used because of the lack of labeled data, especially when the domain is\ntransferred. Meanwhile, unsupervised deep hashing models can hardly achieve\nsatisfactory performance due to the lack of reliable similarity signals. To\ntackle this problem, we propose a novel deep unsupervised hashing method,\nnamely Distilled Smooth Guidance (DSG), which can learn a distilled dataset\nconsisting of similarity signals as well as smooth confidence signals. To be\nspecific, we obtain the similarity confidence weights based on the initial\nnoisy similarity signals learned from local structures and construct a priority\nloss function for smooth similarity-preserving learning. Besides, global\ninformation based on clustering is utilized to distill the image pairs by\nremoving contradictory similarity signals. Extensive experiments on three\nwidely used benchmark datasets show that the proposed DSG consistently\noutperforms the state-of-the-art search methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [4.008639335632324, -0.5697170495986938], "cluster": 6}, {"key": "luo2022survey", "year": "2022", "citations": "123", "title": "A Survey On Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>\n", "tags": ["Survey-Paper", "Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Quantization", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [11.980558395385742, 5.249454975128174], "cluster": 6}, {"key": "luo2023end", "year": "2023", "citations": "5", "title": "End-to-end Knowledge Retrieval With Multi-modal Queries", "abstract": "<p>We investigate knowledge retrieval with multi-modal queries, i.e. queries\ncontaining information split across image and text inputs, a challenging task\nthat differs from previous work on cross-modal retrieval. We curate a new\ndataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a\nsystem to retrieve knowledge from a large corpus by integrating contents from\nboth text and image queries. We introduce a retriever model ``ReViz\u2019\u2019 that can\ndirectly process input text and images to retrieve relevant knowledge in an\nend-to-end fashion without being dependent on intermediate modules such as\nobject detectors or caption generators. We introduce a new pretraining task\nthat is effective for learning knowledge retrieval with multimodal queries and\nalso improves performance on downstream tasks. We demonstrate superior\nperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot\nsettings as well as further improvements when finetuned on these datasets.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-26.05878257751465, -28.30377960205078], "cluster": 5}, {"key": "luo2023lexlip", "year": "2023", "citations": "1", "title": "Lexlip: Lexicon-bottlenecked Language-image Pre-training For Large-scale Image-text Retrieval", "abstract": "<p>Image-text retrieval (ITR) is a task to retrieve the relevant images/texts,\ngiven the query from another modality. The conventional dense retrieval\nparadigm relies on encoding images and texts into dense representations using\ndual-stream encoders, however, it faces challenges with low retrieval speed in\nlarge-scale retrieval scenarios. In this work, we propose the lexicon-weighting\nparadigm, where sparse representations in vocabulary space are learned for\nimages and texts to take advantage of the bag-of-words models and efficient\ninverted indexes, resulting in significantly reduced retrieval latency. A\ncrucial gap arises from the continuous nature of image data, and the\nrequirement for a sparse vocabulary space representation. To bridge this gap,\nwe introduce a novel pre-training framework, Lexicon-Bottlenecked\nLanguage-Image Pre-Training (LexLIP), that learns importance-aware lexicon\nrepresentations. This framework features lexicon-bottlenecked modules between\nthe dual-stream encoders and weakened text decoders, allowing for constructing\ncontinuous bag-of-words bottlenecks to learn lexicon-importance distributions.\nUpon pre-training with same-scale data, our LexLIP achieves state-of-the-art\nperformance on two benchmark ITR datasets, MSCOCO and Flickr30k. Furthermore,\nin large-scale retrieval scenarios, LexLIP outperforms CLIP with a 5.5 ~ 221.3X\nfaster retrieval speed and 13.2 ~ 48.8X less index storage memory.</p>\n", "tags": ["Text-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [6.711330890655518, -16.219642639160156], "cluster": 7}, {"key": "luo2024fine", "year": "2024", "citations": "0", "title": "Fine-grained Embedding Dimension Optimization During Training For Recommender Systems", "abstract": "<p>Huge embedding tables in modern deep learning recommender models (DLRM)\nrequire prohibitively large memory during training and inference. This paper\nproposes FIITED, a system to automatically reduce the memory footprint via\nFIne-grained In-Training Embedding Dimension pruning. By leveraging the key\ninsight that embedding vectors are not equally important, FIITED adaptively\nadjusts the dimension of each individual embedding vector during model\ntraining, assigning larger dimensions to more important embeddings while\nadapting to dynamic changes in data. We prioritize embedding dimensions with\nhigher frequencies and gradients as more important. To enable efficient pruning\nof embeddings and their dimensions during model training, we propose an\nembedding storage system based on virtually-hashed physically-indexed hash\ntables. Experiments on two industry models and months of realistic datasets\nshow that FIITED can reduce DLRM embedding size by more than 65% while\npreserving model quality, outperforming state-of-the-art in-training embedding\npruning methods. On public datasets, FIITED can reduce the size of embedding\ntables by 2.1x to 800x with negligible accuracy drop, while improving model\nthroughput.</p>\n", "tags": ["Memory-Efficiency", "Recommender-Systems", "Datasets"], "tsne_embedding": [31.2755126953125, 13.758735656738281], "cluster": 2}, {"key": "luo2024learning", "year": "2024", "citations": "0", "title": "Learning To Hash For Recommendation: A Survey", "abstract": "<p>With the explosive growth of users and items, Recommender Systems (RS) are\nfacing unprecedented challenges on both retrieval efficiency and storage cost.\nFortunately, Learning to Hash (L2H) techniques have been shown as a promising\nsolution to address the two dilemmas, whose core idea is encoding\nhigh-dimensional data into compact hash codes. To this end, L2H for RS (HashRec\nfor short) has recently received widespread attention to support large-scale\nrecommendations. In this survey, we present a comprehensive review of current\nHashRec algorithms. Specifically, we first introduce the commonly used\ntwo-tower models in the recall stage and identify two search strategies\nfrequently employed in L2H. Then, we categorize prior works into two-tier\ntaxonomy based on: (i) the type of loss function and (ii) the optimization\nstrategy. We also introduce some commonly used evaluation metrics to measure\nthe performance of HashRec algorithms. Finally, we shed light on the\nlimitations of the current research and outline the future research directions.\nFurthermore, the summary of HashRec methods reviewed in this survey can be\nfound at\n\\href{https://github.com/Luo-Fangyuan/HashRec}{https://github.com/Luo-Fangyuan/HashRec}.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Efficiency", "Recommender-Systems", "Scalability", "Memory-Efficiency", "Evaluation"], "tsne_embedding": [23.00037956237793, 13.635154724121094], "cluster": 2}, {"key": "luo2025cross", "year": "2025", "citations": "0", "title": "Cross-domain Diffusion With Progressive Alignment For Efficient Adaptive Retrieval", "abstract": "<p>Unsupervised efficient domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, while maintaining low storage cost and high retrieval efficiency. However, existing methods typically fail to address potential noise in the target domain, and directly align high-level features across domains, thus resulting in suboptimal retrieval performance. To address these challenges, we propose a novel Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This approach revisits unsupervised efficient domain adaptive retrieval from a graph diffusion perspective, simulating cross-domain adaptation dynamics to achieve a stable target domain adaptation process. First, we construct a cross-domain relationship graph and leverage noise-robust graph flow diffusion to simulate the transfer dynamics from the source domain to the target domain, identifying lower noise clusters. We then leverage the graph diffusion results for discriminative hash code learning, effectively learning from the target domain while reducing the negative impact of noise. Furthermore, we employ a hierarchical Mixup operation for progressive domain alignment, which is performed along the cross-domain random walk paths. Utilizing target domain discriminative hash learning and progressive domain alignment, COUPLE enables effective domain adaptive hash learning. Extensive experiments demonstrate COUPLE\u2019s effectiveness on competitive benchmarks.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Memory-Efficiency", "Evaluation", "Unsupervised"], "tsne_embedding": [48.33898162841797, -0.36983489990234375], "cluster": 9}, {"key": "luo2025fast", "year": "2018", "citations": "89", "title": "Fast Scalable Supervised Hashing", "abstract": "<p>Despite significant progress in supervised hashing, there are three\ncommon limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning\nprocedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply\nsampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods\ntry to replace the large matrix with a smaller one, but the size is\nstill large. Third, among the methods that leverage the pairwise\nsimilarity matrix, most of them only encode the semantic label\ninformation in learning the hash codes, failing to fully capture\nthe characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing\n(FSSH), which circumvents the use of the large similarity matrix by\nintroducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn\nthe hash codes with not only the semantic information but also\nthe features of data. Extensive experiments on three widely used\ndatasets demonstrate its superiority over several state-of-the-art\nmethods in both accuracy and scalability. Our experiment codes\nare available at: https://lcbwlx.wixsite.com/fssh.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "SIGIR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [23.052478790283203, 0.32615089416503906], "cluster": 6}, {"key": "luo2025survey", "year": "2022", "citations": "123", "title": "A Survey On Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>\n", "tags": ["Survey-Paper", "Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Quantization", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [11.980631828308105, 5.249438285827637], "cluster": 6}, {"key": "lv2007multi", "year": "2007", "citations": "620", "title": "Multi-probe LSH: Efficient Indexing For High-dimensional Similarity Search", "abstract": "<p>Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio, images, videos, and other sensor\ndata. Recently, locality sensitive hashing (LSH) and its\nvariations have been proposed as indexing techniques for\napproximate similarity search. A significant drawback of\nthese approaches is the requirement for a large number of\nhash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH\nthat overcomes this drawback. Multi-probe LSH is built on\nthe well-known LSH technique, but it intelligently probes\nmultiple buckets that are likely to contain query results in\na hash table. Our method is inspired by and improves upon\nrecent theoretical work on entropy-based LSH designed to\nreduce the space requirement of the basic LSH method. We\nhave implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional\ndatasets. Our evaluation shows that the multi-probe LSH\nmethod substantially improves upon previously proposed\nmethods in both space and time efficiency. To achieve the\nsame search quality, multi-probe LSH has a similar timeefficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison\nwith the entropy-based LSH method, to achieve the same\nsearch quality, multi-probe LSH uses less query time and 5\nto 8 times fewer number of hash tables.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [20.10274314880371, 29.699556350708008], "cluster": 4}, {"key": "lv2025multi", "year": "2007", "citations": "620", "title": "Multi-probe LSH: Efficient Indexing For High-dimensional Similarity Search", "abstract": "<p>Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio, images, videos, and other sensor\ndata. Recently, locality sensitive hashing (LSH) and its\nvariations have been proposed as indexing techniques for\napproximate similarity search. A significant drawback of\nthese approaches is the requirement for a large number of\nhash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH\nthat overcomes this drawback. Multi-probe LSH is built on\nthe well-known LSH technique, but it intelligently probes\nmultiple buckets that are likely to contain query results in\na hash table. Our method is inspired by and improves upon\nrecent theoretical work on entropy-based LSH designed to\nreduce the space requirement of the basic LSH method. We\nhave implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional\ndatasets. Our evaluation shows that the multi-probe LSH\nmethod substantially improves upon previously proposed\nmethods in both space and time efficiency. To achieve the\nsame search quality, multi-probe LSH has a similar timeefficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison\nwith the entropy-based LSH method, to achieve the same\nsearch quality, multi-probe LSH uses less query time and 5\nto 8 times fewer number of hash tables.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [20.102977752685547, 29.699810028076172], "cluster": 4}, {"key": "l\u00fclf2024clip", "year": "2024", "citations": "2", "title": "Clip-branches: Interactive Fine-tuning For Text-image Retrieval", "abstract": "<p>The advent of text-image models, most notably CLIP, has significantly\ntransformed the landscape of information retrieval. These models enable the\nfusion of various modalities, such as text and images. One significant outcome\nof CLIP is its capability to allow users to search for images using text as a\nquery, as well as vice versa. This is achieved via a joint embedding of images\nand text data that can, for instance, be used to search for similar items.\nDespite efficient query processing techniques such as approximate nearest\nneighbor search, the results may lack precision and completeness. We introduce\nCLIP-Branches, a novel text-image search engine built upon the CLIP\narchitecture. Our approach enhances traditional text-image search engines by\nincorporating an interactive fine-tuning phase, which allows the user to\nfurther concretize the search query by iteratively defining positive and\nnegative examples. Our framework involves training a classification model given\nthe additional user feedback and essentially outputs all positively classified\ninstances of the entire data catalog. By building upon recent techniques, this\ninference phase, however, is not implemented by scanning the entire data\ncatalog, but by employing efficient index structures pre-built for the data.\nOur results show that the fine-tuned results can improve the initial search\noutputs in terms of relevance and accuracy while maintaining swift response\ntimes</p>\n", "tags": ["Vector-Indexing", "Image-Retrieval", "SIGIR", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [-32.0167236328125, -31.246490478515625], "cluster": 5}, {"key": "ma2018progressive", "year": "2018", "citations": "18", "title": "Progressive Generative Hashing For Image Retrieval", "abstract": "<p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image\nretrieval. Owing to the great learning capacity,\ndeep hashing has become one of the most promising solutions, and achieved attractive performance\nin practice. However, without semantic label information, the unsupervised deep hashing still remains\nan open question. In this paper, we propose a novel\nprogressive generative hashing (PGH) framework\nto help learn a discriminative hashing network in an\nunsupervised way. Different from existing studies,\nit first treats the hash codes as a kind of semantic\ncondition for the similar image generation, and simultaneously feeds the original image and its codes\ninto the generative adversarial networks (GANs).\nThe real images together with the synthetic ones\ncan further help train a discriminative hashing network based on a triplet loss. By iteratively inputting\nthe learnt codes into the hash conditioned GANs, we can progressively enable the hashing network\nto discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Tools-&-Libraries", "AAAI", "Image-Retrieval", "Hashing-Methods", "Robustness", "Supervised", "IJCAI", "Unsupervised", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [-1.2061671018600464, 2.34610652923584], "cluster": 6}, {"key": "ma2019hierarchy", "year": "2019", "citations": "2", "title": "Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval", "abstract": "<p>Recently, deep supervised hashing methods have become popular for large-scale\nimage retrieval task. To preserve the semantic similarity notion between\nexamples, they typically utilize the pairwise supervision or the triplet\nsupervised information for hash learning. However, these methods usually ignore\nthe semantic class information which can help the improvement of the semantic\ndiscriminative ability of hash codes. In this paper, we propose a novel\nhierarchy neighborhood discriminative hashing method. Specifically, we\nconstruct a bipartite graph to build coarse semantic neighbourhood relationship\nbetween the sub-class feature centers and the embeddings features. Moreover, we\nutilize the pairwise supervised information to construct the fined semantic\nneighbourhood relationship between embeddings features. Finally, we propose a\nhierarchy neighborhood discriminative hashing loss to unify the single-label\nand multilabel image retrieval problem with a one-stream deep neural network\narchitecture. Experimental results on two largescale datasets demonstrate that\nthe proposed method can outperform the state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [0.2692335546016693, -1.366269588470459], "cluster": 6}, {"key": "ma2021hierarchical", "year": "2021", "citations": "3", "title": "Hierarchical Similarity Learning For Language-based Product Image Retrieval", "abstract": "<p>This paper aims for the language-based product image retrieval task. The\nmajority of previous works have made significant progress by designing network\nstructure, similarity measurement, and loss function. However, they typically\nperform vision-text matching at certain granularity regardless of the intrinsic\nmultiple granularities of images. In this paper, we focus on the cross-modal\nsimilarity measurement, and propose a novel Hierarchical Similarity Learning\n(HSL) network. HSL first learns multi-level representations of input data by\nstacked encoders, and object-granularity similarity and image-granularity\nsimilarity are computed at each level. All the similarities are combined as the\nfinal hierarchical cross-modal similarity. Experiments on a large-scale product\nretrieval dataset demonstrate the effectiveness of our proposed method. Code\nand data are available at https://github.com/liufh1/hsl.</p>\n", "tags": ["ICASSP", "Image-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [-18.789344787597656, -32.85701370239258], "cluster": 3}, {"key": "ma2021rank", "year": "2020", "citations": "3", "title": "Rank-consistency Deep Hashing For Scalable Multi-label Image Search", "abstract": "<p>As hashing becomes an increasingly appealing technique for large-scale image\nretrieval, multi-label hashing is also attracting more attention for the\nability to exploit multi-level semantic contents. In this paper, we propose a\nnovel deep hashing method for scalable multi-label image search. Unlike\nexisting approaches with conventional objectives such as contrast and triplet\nlosses, we employ a rank list, rather than pairs or triplets, to provide\nsufficient global supervision information for all the samples. Specifically, a\nnew rank-consistency objective is applied to align the similarity orders from\ntwo spaces, the original space and the hamming space. A powerful loss function\nis designed to penalize the samples whose semantic similarity and hamming\ndistance are mismatched in two spaces. Besides, a multi-label softmax\ncross-entropy loss is presented to enhance the discriminative power with a\nconcise formulation of the derivative function. In order to manipulate the\nneighborhood structure of the samples with different labels, we design a\nmulti-label clustering loss to cluster the hashing vectors of the samples with\nthe same labels by reducing the distances between the samples and their\nmultiple corresponding class centers. The state-of-the-art experimental results\nachieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and\nNUS-WIDE, demonstrate the effectiveness of the proposed method.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Neural-Hashing"], "tsne_embedding": [-23.509864807128906, 10.159516334533691], "cluster": 8}, {"key": "ma2022pre", "year": "2022", "citations": "30", "title": "Pre-train A Discriminative Text Encoder For Dense Retrieval Via Contrastive Span Prediction", "abstract": "<p>Dense retrieval has shown promising results in many information retrieval\n(IR) related tasks, whose foundation is high-quality text representation\nlearning for effective search. Some recent studies have shown that\nautoencoder-based language models are able to boost the dense retrieval\nperformance using a weak decoder. However, we argue that 1) it is not\ndiscriminative to decode all the input texts and, 2) even a weak decoder has\nthe bypass effect on the encoder. Therefore, in this work, we introduce a novel\ncontrastive span prediction task to pre-train the encoder alone, but still\nretain the bottleneck ability of the autoencoder. % Therefore, in this work, we\npropose to drop out the decoder and introduce a novel contrastive span\nprediction task to pre-train the encoder alone. The key idea is to force the\nencoder to generate the text representation close to its own random spans while\nfar away from others using a group-wise contrastive loss. In this way, we can\n1) learn discriminative text representations efficiently with the group-wise\ncontrastive learning over spans and, 2) avoid the bypass effect of the decoder\nthoroughly. Comprehensive experiments over publicly available retrieval\nbenchmark datasets show that our approach can outperform existing pre-training\nmethods for dense retrieval significantly.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "SIGIR", "Datasets", "Evaluation"], "tsne_embedding": [12.670961380004883, -14.901658058166504], "cluster": 7}, {"key": "ma2022x", "year": "2022", "citations": "157", "title": "X-CLIP: End-to-end Multi-grained Contrastive Learning For Video-text Retrieval", "abstract": "<p>Video-text retrieval has been a crucial and fundamental task in multi-modal\nresearch. The development of video-text retrieval has been considerably\npromoted by large-scale multi-modal contrastive pre-training, which primarily\nfocuses on coarse-grained or fine-grained contrast. However, cross-grained\ncontrast, which is the contrast between coarse-grained representations and\nfine-grained representations, has rarely been explored in prior research.\nCompared with fine-grained or coarse-grained contrasts, cross-grained contrast\ncalculate the correlation between coarse-grained features and each fine-grained\nfeature, and is able to filter out the unnecessary fine-grained features guided\nby the coarse-grained feature during similarity calculation, thus improving the\naccuracy of retrieval. To this end, this paper presents a novel multi-grained\ncontrastive model, namely X-CLIP, for video-text retrieval. However, another\nchallenge lies in the similarity aggregation problem, which aims to aggregate\nfine-grained and cross-grained similarity matrices to instance-level\nsimilarity. To address this challenge, we propose the Attention Over Similarity\nMatrix (AOSM) module to make the model focus on the contrast between essential\nframes and words, thus lowering the impact of unnecessary frames and words on\nretrieval results. With multi-grained contrast and the proposed AOSM module,\nX-CLIP achieves outstanding performance on five widely-used video-text\nretrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1\nR@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous\nstate-of-theart by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on\nthese benchmarks, demonstrating the superiority of multi-grained contrast and\nAOSM.</p>\n", "tags": ["Self-Supervised", "Text-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-35.16408157348633, -25.017908096313477], "cluster": 5}, {"key": "ma2023anserini", "year": "2023", "citations": "5", "title": "Anserini Gets Dense Retrieval: Integration Of Lucene's HNSW Indexes", "abstract": "<p>Anserini is a Lucene-based toolkit for reproducible information retrieval\nresearch in Java that has been gaining traction in the community. It provides\nretrieval capabilities for both \u201ctraditional\u201d bag-of-words retrieval models\nsuch as BM25 as well as retrieval using learned sparse representations such as\nSPLADE. With Pyserini, which provides a Python interface to Anserini, users\ngain access to both sparse and dense retrieval models, as Pyserini implements\nbindings to the Faiss vector search library alongside Lucene inverted indexes\nin a uniform, consistent interface. Nevertheless, hybrid fusion techniques that\nintegrate sparse and dense retrieval models need to stitch together results\nfrom two completely different \u201csoftware stacks\u201d, which creates unnecessary\ncomplexities and inefficiencies. However, the introduction of HNSW indexes for\ndense vector search in Lucene promises the integration of both dense and sparse\nretrieval within a single software framework. We explore exactly this\nintegration in the context of Anserini. Experiments on the MS MARCO passage and\nBEIR datasets show that our Anserini HNSW integration supports (reasonably)\neffective and (reasonably) efficient approximate nearest neighbor search for\ndense retrieval models, using only Lucene.</p>\n", "tags": ["Graph-Based-Ann", "Tools-&-Libraries", "CIKM", "Datasets"], "tsne_embedding": [35.337615966796875, -18.9725341796875], "cluster": 7}, {"key": "ma2023cot", "year": "2023", "citations": "1", "title": "Cot-mote: Exploring Contextual Masked Auto-encoder Pre-training With Mixture-of-textual-experts For Passage Retrieval", "abstract": "<p>Passage retrieval aims to retrieve relevant passages from large collections\nof the open-domain corpus. Contextual Masked Auto-Encoding has been proven\neffective in representation bottleneck pre-training of a monolithic\ndual-encoder for passage retrieval. Siamese or fully separated dual-encoders\nare often adopted as basic retrieval architecture in the pre-training and\nfine-tuning stages for encoding queries and passages into their latent\nembedding spaces. However, simply sharing or separating the parameters of the\ndual-encoder results in an imbalanced discrimination of the embedding spaces.\nIn this work, we propose to pre-train Contextual Masked Auto-Encoder with\nMixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate\ntextual-specific experts for individually encoding the distinct properties of\nqueries and passages. Meanwhile, a shared self-attention layer is still kept\nfor unified attention modeling. Results on large-scale passage retrieval\nbenchmarks show steady improvement in retrieval performances. The quantitive\nanalysis also shows a more balanced discrimination of the latent embedding\nspaces.</p>\n", "tags": ["Scalability"], "tsne_embedding": [-1.9699740409851074, -22.411846160888672], "cluster": 3}, {"key": "ma2023direction", "year": "2024", "citations": "11", "title": "Direction-oriented Visual-semantic Embedding Model For Remote Sensing Image-text Retrieval", "abstract": "<p>Image-text retrieval has developed rapidly in recent years. However, it is\nstill a challenge in remote sensing due to visual-semantic imbalance, which\nleads to incorrect matching of non-semantic visual and textual features. To\nsolve this problem, we propose a novel Direction-Oriented Visual-semantic\nEmbedding Model (DOVE) to mine the relationship between vision and language.\nOur highlight is to conduct visual and textual representations in latent space,\ndirecting them as close as possible to a redundancy-free regional visual\nrepresentation. Concretely, a Regional-Oriented Attention Module (ROAM)\nadaptively adjusts the distance between the final visual and textual embeddings\nin the latent semantic space, oriented by regional visual features. Meanwhile,\na lightweight Digging Text Genome Assistant (DTGA) is designed to expand the\nrange of tractable textual representation and enhance global word-level\nsemantic connections using less attention operations. Ultimately, we exploit a\nglobal visual-semantic constraint to reduce single visual dependency and serve\nas an external constraint for the final visual and textual representations. The\neffectiveness and superiority of our method are verified by extensive\nexperiments including parameter evaluation, quantitative comparison, ablation\nstudies and visual analysis, on two benchmark datasets, RSICD and RSITMD.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-25.935819625854492, 1.9325718879699707], "cluster": 1}, {"key": "ma2023harr", "year": "2023", "citations": "5", "title": "HARR: Learning Discriminative And High-quality Hash Codes For Image Retrieval", "abstract": "<p>This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Image-Retrieval", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [0.9103215932846069, 13.712115287780762], "cluster": 8}, {"key": "ma2023let", "year": "2024", "citations": "2", "title": "Let All Be Whitened: Multi-teacher Distillation For Efficient Visual Retrieval", "abstract": "<p>Visual retrieval aims to search for the most relevant visual items, e.g.,\nimages and videos, from a candidate gallery with a given query item. Accuracy\nand efficiency are two competing objectives in retrieval tasks. Instead of\ncrafting a new method pursuing further improvement on accuracy, in this paper\nwe propose a multi-teacher distillation framework Whiten-MTD, which is able to\ntransfer knowledge from off-the-shelf pre-trained retrieval models to a\nlightweight student model for efficient visual retrieval. Furthermore, we\ndiscover that the similarities obtained by different retrieval models are\ndiversified and incommensurable, which makes it challenging to jointly distill\nknowledge from multiple models. Therefore, we propose to whiten the output of\nteacher models before fusion, which enables effective multi-teacher\ndistillation for retrieval models. Whiten-MTD is conceptually simple and\npractically effective. Extensive experiments on two landmark image retrieval\ndatasets and one video retrieval dataset demonstrate the effectiveness of our\nproposed method, and its good balance of retrieval performance and efficiency.\nOur source code is released at https://github.com/Maryeon/whiten_mtd.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "AAAI", "Tools-&-Libraries", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [26.792095184326172, -40.264198303222656], "cluster": 7}, {"key": "ma2024beat", "year": "2023", "citations": "11", "title": "Beat: Bi-directional One-to-many Embedding Alignment For Text-based Person Retrieval", "abstract": "<p>Text-based person retrieval (TPR) is a challenging task that involves\nretrieving a specific individual based on a textual description. Despite\nconsiderable efforts to bridge the gap between vision and language, the\nsignificant differences between these modalities continue to pose a challenge.\nPrevious methods have attempted to align text and image samples in a\nmodal-shared space, but they face uncertainties in optimization directions due\nto the movable features of both modalities and the failure to account for\none-to-many relationships of image-text pairs in TPR datasets. To address this\nissue, we propose an effective bi-directional one-to-many embedding paradigm\nthat offers a clear optimization direction for each sample, thus mitigating the\noptimization problem. Additionally, this embedding scheme generates multiple\nfeatures for each sample without introducing trainable parameters, making it\neasier to align with several positive samples. Based on this paradigm, we\npropose a novel Bi-directional one-to-many Embedding Alignment (Beat) model to\naddress the TPR task. Our experimental results demonstrate that the proposed\nBeat model achieves state-of-the-art performance on three popular TPR datasets,\nincluding CUHK-PEDES (65.61 R@1), ICFG-PEDES (58.25 R@1), and RSTPReID (48.10\nR@1). Furthermore, additional experiments on MS-COCO, CUB, and Flowers datasets\nfurther demonstrate the potential of Beat to be applied to other image-text\nretrieval tasks.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-22.095487594604492, 0.006182524841278791], "cluster": 1}, {"key": "ma2025harr", "year": "2023", "citations": "5", "title": "HARR: Learning Discriminative And High-quality Hash Codes For Image Retrieval", "abstract": "<p>This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Image-Retrieval", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [0.9104092121124268, 13.711432456970215], "cluster": 8}, {"key": "ma2025multi", "year": "2025", "citations": "1", "title": "Multi-modal Reference Learning For Fine-grained Text-to-image Retrieval", "abstract": "<p>Fine-grained text-to-image retrieval aims to retrieve a fine-grained target\nimage with a given text query. Existing methods typically assume that each\ntraining image is accurately depicted by its textual descriptions. However,\ntextual descriptions can be ambiguous and fail to depict discriminative visual\ndetails in images, leading to inaccurate representation learning. To alleviate\nthe effects of text ambiguity, we propose a Multi-Modal Reference learning\nframework to learn robust representations. We first propose a multi-modal\nreference construction module to aggregate all visual and textual details of\nthe same object into a comprehensive multi-modal reference. The multi-modal\nreference hence facilitates the subsequent representation learning and\nretrieval similarity computation. Specifically, a reference-guided\nrepresentation learning module is proposed to use multi-modal references to\nlearn more accurate visual and textual representations. Additionally, we\nintroduce a reference-based refinement method that employs the object\nreferences to compute a reference-based similarity that refines the initial\nretrieval results. Extensive experiments are conducted on five fine-grained\ntext-to-image retrieval datasets for different text-to-image retrieval tasks.\nThe proposed method has achieved superior performance over state-of-the-art\nmethods. For instance, on the text-to-person image retrieval dataset RSTPReid,\nour method achieves the Rank1 accuracy of 56.2%, surpassing the recent CFine\nby 5.6%.</p>\n", "tags": ["Image-Retrieval", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-29.31979751586914, -22.819303512573242], "cluster": 5}, {"key": "ma2025progressive", "year": "2018", "citations": "18", "title": "Progressive Generative Hashing For Image Retrieval", "abstract": "<p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image\nretrieval. Owing to the great learning capacity,\ndeep hashing has become one of the most promising solutions, and achieved attractive performance\nin practice. However, without semantic label information, the unsupervised deep hashing still remains\nan open question. In this paper, we propose a novel\nprogressive generative hashing (PGH) framework\nto help learn a discriminative hashing network in an\nunsupervised way. Different from existing studies,\nit first treats the hash codes as a kind of semantic\ncondition for the similar image generation, and simultaneously feeds the original image and its codes\ninto the generative adversarial networks (GANs).\nThe real images together with the synthetic ones\ncan further help train a discriminative hashing network based on a triplet loss. By iteratively inputting\nthe learnt codes into the hash conditioned GANs, we can progressively enable the hashing network\nto discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Tools-&-Libraries", "AAAI", "Image-Retrieval", "Hashing-Methods", "Robustness", "Supervised", "IJCAI", "Unsupervised", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [-1.2061470746994019, 2.346118450164795], "cluster": 6}, {"key": "macavaney2025efficient", "year": "2025", "citations": "1", "title": "Efficient Constant-space Multi-vector Retrieval", "abstract": "<p>Multi-vector retrieval methods, exemplified by the ColBERT architecture, have\nshown substantial promise for retrieval by providing strong trade-offs in terms\nof retrieval latency and effectiveness. However, they come at a high cost in\nterms of storage since a (potentially compressed) vector needs to be stored for\nevery token in the input collection. To overcome this issue, we propose\nencoding documents to a fixed number of vectors, which are no longer\nnecessarily tied to the input tokens. Beyond reducing the storage costs, our\napproach has the advantage that document representations become of a fixed size\non disk, allowing for better OS paging management. Through experiments using\nthe MSMARCO passage corpus and BEIR with the ColBERT-v2 architecture, a\nrepresentative multi-vector ranking model architecture, we find that passages\ncan be effectively encoded into a fixed number of vectors while retaining most\nof the original effectiveness.</p>\n", "tags": ["Memory-Efficiency"], "tsne_embedding": [15.327154159545898, -22.744047164916992], "cluster": 7}, {"key": "macdonald2021approximate", "year": "2021", "citations": "14", "title": "On Approximate Nearest Neighbour Selection For Multi-stage Dense Retrieval", "abstract": "<p>Dense retrieval, which describes the use of contextualised language models\nsuch as BERT to identify documents from a collection by leveraging approximate\nnearest neighbour (ANN) techniques, has been increasing in popularity. Two\nfamilies of approaches have emerged, depending on whether documents and queries\nare represented by single or multiple embeddings. ColBERT, the exemplar of the\nlatter, uses an ANN index and approximate scores to identify a set of candidate\ndocuments for each query embedding, which are then re-ranked using accurate\ndocument representations. In this manner, a large number of documents can be\nretrieved for each query, hindering the efficiency of the approach. In this\nwork, we investigate the use of ANN scores for ranking the candidate documents,\nin order to decrease the number of candidate documents being fully scored.\nExperiments conducted on the MSMARCO passage ranking corpus demonstrate that,\nby cutting of the candidate set by using the approximate scores to only 200\ndocuments, we can still obtain an effective ranking without statistically\nsignificant differences in effectiveness, and resulting in a 2x speedup in\nefficiency.</p>\n", "tags": ["Similarity-Search", "CIKM", "Efficiency", "Vector-Indexing"], "tsne_embedding": [17.827869415283203, 22.57526969909668], "cluster": 2}, {"key": "madasu2022mumur", "year": "2023", "citations": "1", "title": "Mumur : Multilingual Multimodal Universal Retrieval", "abstract": "<p>Multi-modal retrieval has seen tremendous progress with the development of\nvision-language models. However, further improving these models require\nadditional labelled data which is a huge manual effort. In this paper, we\npropose a framework MuMUR, that utilizes knowledge transfer from a multilingual\nmodel to boost the performance of multi-modal (image and video) retrieval. We\nfirst use state-of-the-art machine translation models to construct pseudo\nground-truth multilingual visual-text pairs. We then use this data to learn a\njoint vision-text representation where English and non-English text queries are\nrepresented in a common embedding space based on pretrained multilingual\nmodels. We evaluate our proposed approach on a diverse set of retrieval\ndatasets: five video retrieval datasets such as MSRVTT, MSVD, DiDeMo, Charades\nand MSRVTT multilingual, two image retrieval datasets such as Flickr30k and\nMulti30k . Experimental results demonstrate that our approach achieves\nstate-of-the-art results on all video retrieval datasets outperforming previous\nmodels. Additionally, our framework MuMUR significantly beats other\nmultilingual video retrieval dataset. We also observe that MuMUR exhibits\nstrong performance on image retrieval. This demonstrates the universal ability\nof MuMUR to perform retrieval across all visual inputs (image and video) and\ntext inputs (monolingual and multilingual).</p>\n", "tags": ["Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-35.214012145996094, -29.99386978149414], "cluster": 5}, {"key": "mafla2020fine", "year": "2020", "citations": "30", "title": "Fine-grained Image Classification And Retrieval By Combining Visual And Locally Pooled Textual Features", "abstract": "<p>Text contained in an image carries high-level semantics that can be exploited\nto achieve richer image understanding. In particular, the mere presence of text\nprovides strong guiding content that should be employed to tackle a diversity\nof computer vision tasks such as image retrieval, fine-grained classification,\nand visual question answering. In this paper, we address the problem of\nfine-grained classification and image retrieval by leveraging textual\ninformation along with visual cues to comprehend the existing intrinsic\nrelation between the two modalities. The novelty of the proposed model consists\nof the usage of a PHOC descriptor to construct a bag of textual words along\nwith a Fisher Vector Encoding that captures the morphology of text. This\napproach provides a stronger multimodal representation for this task and as our\nexperiments demonstrate, it achieves state-of-the-art results on two different\ntasks, fine-grained classification and image retrieval.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-26.26015853881836, -31.917682647705078], "cluster": 5}, {"key": "mafla2020stacmr", "year": "2021", "citations": "25", "title": "Stacmr: Scene-text Aware Cross-modal Retrieval", "abstract": "<p>Recent models for cross-modal retrieval have benefited from an increasingly\nrich understanding of visual scenes, afforded by scene graphs and object\ninteractions to mention a few. This has resulted in an improved matching\nbetween the visual representation of an image and the textual representation of\nits caption. Yet, current visual representations overlook a key aspect: the\ntext appearing in images, which may contain crucial information for retrieval.\nIn this paper, we first propose a new dataset that allows exploration of\ncross-modal retrieval where images contain scene-text instances. Then, armed\nwith this dataset, we describe several approaches which leverage scene text,\nincluding a better scene-text aware cross-modal retrieval method which uses\nspecialized representations for text from the captions and text from the visual\nscene, and reconcile them in a common embedding space. Extensive experiments\nconfirm that cross-modal retrieval approaches benefit from scene text and\nhighlight interesting research questions worth exploring further. Dataset and\ncode are available at http://europe.naverlabs.com/stacmr</p>\n", "tags": ["Multimodal-Retrieval", "Datasets"], "tsne_embedding": [-29.845903396606445, -35.746131896972656], "cluster": 5}, {"key": "magliani2018accurate", "year": "2018", "citations": "17", "title": "An Accurate Retrieval Through R-MAC+ Descriptors For Landmark Recognition", "abstract": "<p>The landmark recognition problem is far from being solved, but with the use\nof features extracted from intermediate layers of Convolutional Neural Networks\n(CNNs), excellent results have been obtained. In this work, we propose some\nimprovements on the creation of R-MAC descriptors in order to make the\nnewly-proposed R-MAC+ descriptors more representative than the previous ones.\nHowever, the main contribution of this paper is a novel retrieval technique,\nthat exploits the fine representativeness of the MAC descriptors of the\ndatabase images. Using this descriptors called \u201cdb regions\u201d during the\nretrieval stage, the performance is greatly improved. The proposed method is\ntested on different public datasets: Oxford5k, Paris6k and Holidays. It\noutperforms the state-of-the- art results on Holidays and reached excellent\nresults on Oxford5k and Paris6k, overcame only by approaches based on\nfine-tuning strategies.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [16.71915626525879, 25.811214447021484], "cluster": 2}, {"key": "magliani2018dense", "year": "2018", "citations": "4", "title": "A Dense-depth Representation For VLAD Descriptors In Content-based Image Retrieval", "abstract": "<p>The recent advances brought by deep learning allowed to improve the\nperformance in image retrieval tasks. Through the many convolutional layers,\navailable in a Convolutional Neural Network (CNN), it is possible to obtain a\nhierarchy of features from the evaluated image. At every step, the patches\nextracted are smaller than the previous levels and more representative.\nFollowing this idea, this paper introduces a new detector applied on the\nfeature maps extracted from pre-trained CNN. Specifically, this approach lets\nto increase the number of features in order to increase the performance of the\naggregation algorithms like the most famous and used VLAD embedding. The\nproposed approach is tested on different public datasets: Holidays, Oxford5k,\nParis6k and UKB.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-53.26940155029297, 1.4578726291656494], "cluster": 0}, {"key": "magliani2018efficient", "year": "2018", "citations": "8", "title": "Efficient Nearest Neighbors Search For Large-scale Landmark Recognition", "abstract": "<p>The problem of landmark recognition has achieved excellent results in\nsmall-scale datasets. When dealing with large-scale retrieval, issues that were\nirrelevant with small amount of data, quickly become fundamental for an\nefficient retrieval phase. In particular, computational time needs to be kept\nas low as possible, whilst the retrieval accuracy has to be preserved as much\nas possible. In this paper we propose a novel multi-index hashing method called\nBag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows\nto drastically reduce the query time and outperforms the accuracy results\ncompared to the state-of-the-art methods for large-scale landmark recognition.\nIt has been demonstrated that this family of algorithms can be applied on\ndifferent embedding techniques like VLAD and R-MAC obtaining excellent results\nin very short times on different public datasets: Holidays+Flickr1M, Oxford105k\nand Paris106k.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Vector-Indexing", "Similarity-Search", "Scalability", "Datasets"], "tsne_embedding": [16.8603572845459, 25.737289428710938], "cluster": 2}, {"key": "magliani2019efficient", "year": "2019", "citations": "8", "title": "An Efficient Approximate Knn Graph Method For Diffusion On Image Retrieval", "abstract": "<p>The application of the diffusion in many computer vision and artificial\nintelligence projects has been shown to give excellent improvements in\nperformance. One of the main bottlenecks of this technique is the quadratic\ngrowth of the kNN graph size due to the high-quantity of new connections\nbetween nodes in the graph, resulting in long computation times. Several\nstrategies have been proposed to address this, but none are effective and\nefficient. Our novel technique, based on LSH projections, obtains the same\nperformance as the exact kNN graph after diffusion, but in less time\n(approximately 18 times faster on a dataset of a hundred thousand images). The\nproposed method was validated and compared with other state-of-the-art on\nseveral public image datasets, including Oxford5k, Paris6k, and Oxford105k.</p>\n", "tags": ["Evaluation", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [49.91586685180664, 14.957463264465332], "cluster": 9}, {"key": "mahajan2019joint", "year": "2019", "citations": "8", "title": "Joint Wasserstein Autoencoders For Aligning Multimodal Embeddings", "abstract": "<p>One of the key challenges in learning joint embeddings of multiple\nmodalities, e.g. of images and text, is to ensure coherent cross-modal\nsemantics that generalize across datasets. We propose to address this through\njoint Gaussian regularization of the latent representations. Building on\nWasserstein autoencoders (WAEs) to encode the input in each domain, we enforce\nthe latent embeddings to be similar to a Gaussian prior that is shared across\nthe two domains, ensuring compatible continuity of the encoded semantic\nrepresentations of images and texts. Semantic alignment is achieved through\nsupervision from matching image-text pairs. To show the benefits of our\nsemi-supervised representation, we apply it to cross-modal retrieval and phrase\nlocalization. We not only achieve state-of-the-art accuracy, but significantly\nbetter generalization across datasets, owing to the semantic continuity of the\nlatent space.</p>\n", "tags": ["Supervised", "ICCV", "Multimodal-Retrieval", "Datasets"], "tsne_embedding": [-29.57071304321289, -28.469615936279297], "cluster": 5}, {"key": "maheshwari2020learning", "year": "2020", "citations": "5", "title": "Learning Colour Representations Of Search Queries", "abstract": "<p>Image search engines rely on appropriately designed ranking features that\ncapture various aspects of the content semantics as well as the historic\npopularity. In this work, we consider the role of colour in this relevance\nmatching process. Our work is motivated by the observation that a significant\nfraction of user queries have an inherent colour associated with them. While\nsome queries contain explicit colour mentions (such as \u2018black car\u2019 and \u2018yellow\ndaisies\u2019), other queries have implicit notions of colour (such as \u2018sky\u2019 and\n\u2018grass\u2019). Furthermore, grounding queries in colour is not a mapping to a single\ncolour, but a distribution in colour space. For instance, a search for \u2018trees\u2019\ntends to have a bimodal distribution around the colours green and brown. We\nleverage historical clickthrough data to produce a colour representation for\nsearch queries and propose a recurrent neural network architecture to encode\nunseen queries into colour space. We also show how this embedding can be learnt\nalongside a cross-modal relevance ranker from impression logs where a subset of\nthe result images were clicked. We demonstrate that the use of a query-image\ncolour distance feature leads to an improvement in the ranker performance as\nmeasured by users\u2019 preferences of clicked versus skipped images.</p>\n", "tags": ["SIGIR", "Evaluation", "Image-Retrieval"], "tsne_embedding": [59.70561981201172, -9.843988418579102], "cluster": 9}, {"key": "maheshwari2021scene", "year": "2021", "citations": "10", "title": "Scene Graph Embeddings Using Relative Similarity Supervision", "abstract": "<p>Scene graphs are a powerful structured representation of the underlying\ncontent of images, and embeddings derived from them have been shown to be\nuseful in multiple downstream tasks. In this work, we employ a graph\nconvolutional network to exploit structure in scene graphs and produce image\nembeddings useful for semantic image retrieval. Different from\nclassification-centric supervision traditionally available for learning image\nrepresentations, we address the task of learning from relative similarity\nlabels in a ranking context. Rooted within the contrastive learning paradigm,\nwe propose a novel loss function that operates on pairs of similar and\ndissimilar images and imposes relative ordering between them in embedding\nspace. We demonstrate that this Ranking loss, coupled with an intuitive triple\nsampling strategy, leads to robust representations that outperform well-known\ncontrastive losses on the retrieval task. In addition, we provide qualitative\nevidence of how retrieved results that utilize structured scene information\ncapture the global context of the scene, different from visual similarity\nsearch.</p>\n", "tags": ["Self-Supervised", "AAAI", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [55.936119079589844, -8.697733879089355], "cluster": 9}, {"key": "mahowald2024integrating", "year": "2024", "citations": "0", "title": "Integrating Visual And Textual Inputs For Searching Large-scale Map Collections With CLIP", "abstract": "<p>Despite the prevalence and historical importance of maps in digital\ncollections, current methods of navigating and exploring map collections are\nlargely restricted to catalog records and structured metadata. In this paper,\nwe explore the potential for interactively searching large-scale map\ncollections using natural language inputs (\u201cmaps with sea monsters\u201d), visual\ninputs (i.e., reverse image search), and multimodal inputs (an example map +\n\u201cmore grayscale\u201d). As a case study, we adopt 562,842 images of maps publicly\naccessible via the Library of Congress\u2019s API. To accomplish this, we use the\nmulitmodal Contrastive Language-Image Pre-training (CLIP) machine learning\nmodel to generate embeddings for these maps, and we develop code to implement\nexploratory search capabilities with these input strategies. We present results\nfor example searches created in consultation with staff in the Library of\nCongress\u2019s Geography and Map Division and describe the strengths, weaknesses,\nand possibilities for these search queries. Moreover, we introduce a\nfine-tuning dataset of 10,504 map-caption pairs, along with an architecture for\nfine-tuning a CLIP model on this dataset. To facilitate re-use, we provide all\nof our code in documented, interactive Jupyter notebooks and place all code\ninto the public domain. Lastly, we discuss the opportunities and challenges for\napplying these approaches across both digitized and born-digital collections\nheld by galleries, libraries, archives, and museums.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-19.07564926147461, -26.1259708404541], "cluster": 5}, {"key": "maji2020cbir", "year": "2021", "citations": "10", "title": "CBIR Using Features Derived By Deep Learning", "abstract": "<p>In a Content Based Image Retrieval (CBIR) System, the task is to retrieve\nsimilar images from a large database given a query image. The usual procedure\nis to extract some useful features from the query image, and retrieve images\nwhich have similar set of features. For this purpose, a suitable similarity\nmeasure is chosen, and images with high similarity scores are retrieved.\nNaturally the choice of these features play a very important role in the\nsuccess of this system, and high level features are required to reduce the\nsemantic gap.\n  In this paper, we propose to use features derived from pre-trained network\nmodels from a deep-learning convolution network trained for a large image\nclassification problem. This approach appears to produce vastly superior\nresults for a variety of databases, and it outperforms many contemporary CBIR\nsystems. We analyse the retrieval time of the method, and also propose a\npre-clustering of the database based on the above-mentioned features which\nyields comparable results in a much shorter time in most of the cases.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-15.366908073425293, -6.038760662078857], "cluster": 1}, {"key": "malali2022learning", "year": "2021", "citations": "8", "title": "Learning To Embed Semantic Similarity For Joint Image-text Retrieval", "abstract": "<p>We present a deep learning approach for learning the joint semantic\nembeddings of images and captions in a Euclidean space, such that the semantic\nsimilarity is approximated by the L2 distances in the embedding space. For\nthat, we introduce a metric learning scheme that utilizes multitask learning to\nlearn the embedding of identical semantic concepts using a center loss. By\nintroducing a differentiable quantization scheme into the end-to-end trainable\nnetwork, we derive a semantic embedding of semantically similar concepts in\nEuclidean space. We also propose a novel metric learning formulation using an\nadaptive margin hinge loss, that is refined during the training phase. The\nproposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets,\nand was shown to compare favorably with contemporary state-of-the-art\napproaches.</p>\n", "tags": ["Quantization", "Distance-Metric-Learning", "Text-Retrieval", "Datasets"], "tsne_embedding": [-16.39653968811035, -26.90872573852539], "cluster": 3}, {"key": "malkov2016efficient", "year": "2018", "citations": "1091", "title": "Efficient And Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs", "abstract": "<p>We present a new approach for the approximate K-nearest neighbor search based\non navigable small world graphs with controllable hierarchy (Hierarchical NSW,\nHNSW). The proposed solution is fully graph-based, without any need for\nadditional search structures, which are typically used at the coarse search\nstage of the most proximity graph techniques. Hierarchical NSW incrementally\nbuilds a multi-layer structure consisting from hierarchical set of proximity\ngraphs (layers) for nested subsets of the stored elements. The maximum layer in\nwhich an element is present is selected randomly with an exponentially decaying\nprobability distribution. This allows producing graphs similar to the\npreviously studied Navigable Small World (NSW) structures while additionally\nhaving the links separated by their characteristic distance scales. Starting\nsearch from the upper layer together with utilizing the scale separation boosts\nthe performance compared to NSW and allows a logarithmic complexity scaling.\nAdditional employment of a heuristic for selecting proximity graph neighbors\nsignificantly increases performance at high recall and in case of highly\nclustered data. Performance evaluation has demonstrated that the proposed\ngeneral metric space search index is able to strongly outperform previous\nopensource state-of-the-art vector-only approaches. Similarity of the algorithm\nto the skip list structure allows straightforward balanced distributed\nimplementation.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation"], "tsne_embedding": [47.322509765625, 7.397125244140625], "cluster": 9}, {"key": "man2025lusifer", "year": "2025", "citations": "0", "title": "LUSIFER: Language Universal Space Integration For Enhanced Multilingual Embeddings With Large Language Models", "abstract": "<p>Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER\u2019s architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder\u2019s language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [-4.525210857391357, -39.86648941040039], "cluster": 3}, {"key": "manandhar2019semantic", "year": "2020", "citations": "10", "title": "Semantic Granularity Metric Learning For Visual Search", "abstract": "<p>Deep metric learning applied to various applications has shown promising\nresults in identification, retrieval and recognition. Existing methods often do\nnot consider different granularity in visual similarity. However, in many\ndomain applications, images exhibit similarity at multiple granularities with\nvisual semantic concepts, e.g. fashion demonstrates similarity ranging from\nclothing of the exact same instance to similar looks/design or a common\ncategory. Therefore, training image triplets/pairs used for metric learning\ninherently possess different degree of information. However, the existing\nmethods often treats them with equal importance during training. This hinders\ncapturing the underlying granularities in feature similarity required for\neffective visual search.\n  In view of this, we propose a new deep semantic granularity metric learning\n(SGML) that develops a novel idea of leveraging attribute semantic space to\ncapture different granularity of similarity, and then integrate this\ninformation into deep metric learning. The proposed method simultaneously\nlearns image attributes and embeddings using multitask CNNs. The two tasks are\nnot only jointly optimized but are further linked by the semantic granularity\nsimilarity mappings to leverage the correlations between the tasks. To this\nend, we propose a new soft-binomial deviance loss that effectively integrates\nthe degree of information in training samples, which helps to capture visual\nsimilarity at multiple granularities. Compared to recent ensemble-based\nmethods, our framework is conceptually elegant, computationally simple and\nprovides better performance. We perform extensive experiments on benchmark\nmetric learning datasets and demonstrate that our method outperforms recent\nstate-of-the-art methods, e.g., 1-4.5% improvement in Recall@1 over the\nprevious state-of-the-arts [1],[2] on DeepFashion In-Shop dataset.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-26.20039939880371, -12.25573444366455], "cluster": 5}, {"key": "mandal2020novel", "year": "2020", "citations": "3", "title": "A Novel Incremental Cross-modal Hashing Approach", "abstract": "<p>Cross-modal retrieval deals with retrieving relevant items from one modality,\nwhen provided with a search query from another modality. Hashing techniques,\nwhere the data is represented as binary bits have specifically gained\nimportance due to the ease of storage, fast computations and high accuracy. In\nreal world, the number of data categories is continuously increasing, which\nrequires algorithms capable of handling this dynamic scenario. In this work, we\npropose a novel incremental cross-modal hashing algorithm termed \u201ciCMH\u201d, which\ncan adapt itself to handle incoming data of new categories. The proposed\napproach consists of two sequential stages, namely, learning the hash codes and\ntraining the hash functions. At every stage, a small amount of old category\ndata termed \u201cexemplars\u201d is is used so as not to forget the old data while\ntrying to learn for the new incoming data, i.e. to avoid catastrophic\nforgetting. In the first stage, the hash codes for the exemplars is used, and\nsimultaneously, hash codes for the new data is computed such that it maintains\nthe semantic relations with the existing data. For the second stage, we propose\nboth a non-deep and deep architectures to learn the hash functions effectively.\nExtensive experiments across a variety of cross-modal datasets and comparisons\nwith state-of-the-art cross-modal algorithms shows the usefulness of our\napproach.</p>\n", "tags": ["Multimodal-Retrieval", "Hashing-Methods", "Datasets"], "tsne_embedding": [26.304922103881836, 0.5092400908470154], "cluster": 6}, {"key": "mandarapu2023arkade", "year": "2024", "citations": "5", "title": "Arkade: K-nearest Neighbor Search With Non-euclidean Distances Using GPU Ray Tracing", "abstract": "<p>High-performance implementations of \\(k\\)-Nearest Neighbor Search (\\(k\\)NN) in\nlow dimensions use tree-based data structures. Tree algorithms are hard to\nparallelize on GPUs due to their irregularity. However, newer Nvidia GPUs offer\nhardware support for tree operations through ray-tracing cores. Recent works\nhave proposed using RT cores to implement \\(k\\)NN search, but they all have a\nhardware-imposed constraint on the distance metric used in the search \u2013 the\nEuclidean distance. We propose and implement two reductions to support \\(k\\)NN\nfor a broad range of distances other than the Euclidean distance: Arkade\nFilter-Refine and Arkade Monotone Transformation, each of which allows\nnon-Euclidean distance-based nearest neighbor queries to be performed in terms\nof the Euclidean distance. With our reductions, we observe that \\(k\\)NN search\ntime speedups range between \\(1.6\\)x-\\(200\\)x and \\(1.3\\)x-\\(33.1\\)x over various\nstate-of-the-art GPU shader core and RT core baselines, respectively. In\nevaluation, we provide several insights on RT architectures\u2019 ability to\nefficiently build and traverse the tree by analyzing the \\(k\\)NN search time\ntrends.</p>\n", "tags": ["Tree-Based-Ann", "Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [33.193206787109375, 40.566307067871094], "cluster": 4}, {"key": "manek2017pruning", "year": "2017", "citations": "3", "title": "Pruning Convolutional Neural Networks For Image Instance Retrieval", "abstract": "<p>In this work, we focus on the problem of image instance retrieval with deep\ndescriptors extracted from pruned Convolutional Neural Networks (CNN). The\nobjective is to heavily prune convolutional edges while maintaining retrieval\nperformance. To this end, we introduce both data-independent and data-dependent\nheuristics to prune convolutional edges, and evaluate their performance across\nvarious compression rates with different deep descriptors over several\nbenchmark datasets. Further, we present an end-to-end framework to fine-tune\nthe pruned network, with a triplet loss function specially designed for the\nretrieval task. We show that the combination of heuristic pruning and\nfine-tuning offers 5x compression rate without considerable loss in retrieval\nperformance.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-53.5589599609375, -2.72337007522583], "cluster": 0}, {"key": "manocha2017content", "year": "2018", "citations": "38", "title": "Content-based Representations Of Audio Using Siamese Neural Networks", "abstract": "<p>In this paper, we focus on the problem of content-based retrieval for audio,\nwhich aims to retrieve all semantically similar audio recordings for a given\naudio clip query. This problem is similar to the problem of query by example of\naudio, which aims to retrieve media samples from a database, which are similar\nto the user-provided example. We propose a novel approach which encodes the\naudio into a vector representation using Siamese Neural Networks. The goal is\nto obtain an encoding similar for files belonging to the same audio class, thus\nallowing retrieval of semantically similar audio. Using simple similarity\nmeasures such as those based on simple euclidean distance and cosine similarity\nwe show that these representations can be very effectively used for retrieving\nrecordings similar in audio content.</p>\n", "tags": ["ICASSP", "Distance-Metric-Learning"], "tsne_embedding": [8.62083625793457, -49.110496520996094], "cluster": 3}, {"key": "manohar2023parlayann", "year": "2024", "citations": "6", "title": "Parlayann: Scalable And Deterministic Parallel Graph-based Approximate Nearest Neighbor Search Algorithms", "abstract": "<p>Approximate nearest-neighbor search (ANNS) algorithms are a key part of the\nmodern deep learning stack due to enabling efficient similarity search over\nhigh-dimensional vector space representations (i.e., embeddings) of data. Among\nvarious ANNS algorithms, graph-based algorithms are known to achieve the best\nthroughput-recall tradeoffs. Despite the large scale of modern ANNS datasets,\nexisting parallel graph based implementations suffer from significant\nchallenges to scale to large datasets due to heavy use of locks and other\nsequential bottlenecks, which 1) prevents them from efficiently scaling to a\nlarge number of processors, and 2) results in nondeterminism that is\nundesirable in certain applications.\n  In this paper, we introduce ParlayANN, a library of deterministic and\nparallel graph-based approximate nearest neighbor search algorithms, along with\na set of useful tools for developing such algorithms. In this library, we\ndevelop novel parallel implementations for four state-of-the-art graph-based\nANNS algorithms that scale to billion-scale datasets. Our algorithms are\ndeterministic and achieve high scalability across a diverse set of challenging\ndatasets. In addition to the new algorithmic ideas, we also conduct a detailed\nexperimental study of our new algorithms as well as two existing non-graph\napproaches. Our experimental results both validate the effectiveness of our new\ntechniques, and lead to a comprehensive comparison among ANNS algorithms on\nlarge scale datasets with a list of interesting findings.</p>\n", "tags": ["Graph-Based-Ann", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [54.2662353515625, 9.502273559570312], "cluster": 9}, {"key": "manohar2025range", "year": "2025", "citations": "0", "title": "Range Retrieval With Graph-based Indices", "abstract": "<p>Retrieving points based on proximity in a high-dimensional vector space is a\ncrucial step in information retrieval applications. The approximate nearest\nneighbor search (ANNS) problem, which identifies the \\(k\\) nearest neighbors for\na query (approximately, since exactly is hard), has been extensively studied in\nrecent years. However, comparatively little attention has been paid to the\nrelated problem of finding all points within a given distance of a query, the\nrange retrieval problem, despite its applications in areas such as duplicate\ndetection, plagiarism checking, and facial recognition. In this paper, we\npresent a set of algorithms for range retrieval on graph-based vector indices,\nwhich are known to achieve excellent performance on ANNS queries. Since a range\nquery may have anywhere from no matching results to thousands of matching\nresults in the database, we introduce a set of range retrieval algorithms based\non modifications of the standard graph search that adapt to terminate quickly\non queries in the former group, and to put more resources into finding results\nfor the latter group. Due to the lack of existing benchmarks for range\nretrieval, we also undertake a comprehensive study of range characteristics of\nexisting embedding datasets, and select a suitable range retrieval radius for\neight existing datasets with up to 100 million points in addition to the one\nexisting benchmark. We test our algorithms on these datasets, and find up to\n100x improvement in query throughput over a naive baseline approach, with 5-10x\nimprovement on average, and strong performance up to 100 million data points.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Datasets"], "tsne_embedding": [54.29524230957031, 11.249178886413574], "cluster": 9}, {"key": "marin2018recipe1m", "year": "2019", "citations": "256", "title": "Recipe1m+: A Dataset For Learning Cross-modal Embeddings For Cooking Recipes And Food Images", "abstract": "<p>In this paper, we introduce Recipe1M+, a new large-scale, structured corpus\nof over one million cooking recipes and 13 million food images. As the largest\npublicly available collection of recipe data, Recipe1M+ affords the ability to\ntrain high-capacity modelson aligned, multimodal data. Using these data, we\ntrain a neural network to learn a joint embedding of recipes and images that\nyields impressive results on an image-recipe retrieval task. Moreover, we\ndemonstrate that regularization via the addition of a high-level classification\nobjective both improves retrieval performance to rival that of humans and\nenables semantic vector arithmetic. We postulate that these embeddings will\nprovide a basis for further exploration of the Recipe1M+ dataset and food and\ncooking in general. Code, data and models are publicly available.</p>\n", "tags": ["Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-43.55474090576172, 32.44899368286133], "cluster": 0}, {"key": "markchit2019effective", "year": "2019", "citations": "6", "title": "Effective And Efficient Indexing In Cross-modal Hashing-based Datasets", "abstract": "<p>To overcome the barrier of storage and computation, the hashing technique has\nbeen widely used for nearest neighbor search in multimedia retrieval\napplications recently. Particularly, cross-modal retrieval that searches across\ndifferent modalities becomes an active but challenging problem. Although dozens\nof cross-modal hashing algorithms are proposed to yield compact binary codes,\nthe exhaustive search is impractical for the real-time purpose, and Hamming\ndistance computation suffers inaccurate results. In this paper, we propose a\nnovel search method that utilizes a probability-based index scheme over binary\nhash codes in cross-modal retrieval. The proposed hash code indexing scheme\nexploits a few binary bits of the hash code as the index code. We construct an\ninverted index table based on index codes and train a neural network to improve\nthe indexing accuracy and efficiency. Experiments are performed on two\nbenchmark datasets for retrieval across image and text modalities, where hash\ncodes are generated by three cross-modal hashing methods. Results show the\nproposed method effectively boost the performance on these hash methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Multimodal-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [4.045861721038818, 3.9003005027770996], "cluster": 6}, {"key": "mashnoor2024locality", "year": "2023", "citations": "1", "title": "Locality Sensitive Hashing For Network Traffic Fingerprinting", "abstract": "<p>The advent of the Internet of Things (IoT) has brought forth additional\nintricacies and difficulties to computer networks. These gadgets are\nparticularly susceptible to cyber-attacks because of their simplistic design.\nTherefore, it is crucial to recognise these devices inside a network for the\npurpose of network administration and to identify any harmful actions. Network\ntraffic fingerprinting is a crucial technique for identifying devices and\ndetecting anomalies. Currently, the predominant methods for this depend heavily\non machine learning (ML). Nevertheless, machine learning (ML) methods need the\nselection of features, adjustment of hyperparameters, and retraining of models\nto attain optimal outcomes and provide resilience to concept drifts detected in\na network. In this research, we suggest using locality-sensitive hashing (LSH)\nfor network traffic fingerprinting as a solution to these difficulties. Our\nstudy focuses on examining several design options for the Nilsimsa LSH\nfunction. We then use this function to create unique fingerprints for network\ndata, which may be used to identify devices. We also compared it with ML-based\ntraffic fingerprinting and observed that our method increases the accuracy of\nstate-of-the-art by 12% achieving around 94% accuracy in identifying devices in\na network.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [29.391613006591797, -14.664600372314453], "cluster": 7}, {"key": "massarelli2018safe", "year": "2019", "citations": "158", "title": "SAFE: Self-attentive Function Embeddings For Binary Similarity", "abstract": "<p>The binary similarity problem consists in determining if two functions are\nsimilar by only considering their compiled form. Advanced techniques for binary\nsimilarity recently gained momentum as they can be applied in several fields,\nsuch as copyright disputes, malware analysis, vulnerability detection, etc.,\nand thus have an immediate practical impact. Current solutions compare\nfunctions by first transforming their binary code in multi-dimensional vector\nrepresentations (embeddings), and then comparing vectors through simple and\nefficient geometric operations. However, embeddings are usually derived from\nbinary code using manual feature extraction, that may fail in considering\nimportant function characteristics, or may consider features that are not\nimportant for the binary similarity problem. In this paper we propose SAFE, a\nnovel architecture for the embedding of functions based on a self-attentive\nneural network. SAFE works directly on disassembled binary functions, does not\nrequire manual feature extraction, is computationally more efficient than\nexisting solutions (i.e., it does not incur in the computational overhead of\nbuilding or manipulating control flow graphs), and is more general as it works\non stripped binaries and on multiple architectures. We report the results from\na quantitative and qualitative analysis that show how SAFE provides a\nnoticeable performance improvement with respect to previous solutions.\nFurthermore, we show how clusters of our embedding vectors are closely related\nto the semantic of the implemented algorithms, paving the way for further\ninteresting applications (e.g. semantic-based binary function search).</p>\n", "tags": ["Compact-Codes", "Evaluation"], "tsne_embedding": [20.298545837402344, 10.131624221801758], "cluster": 6}, {"key": "matatov2022dataset", "year": "2022", "citations": "1", "title": "Dataset And Case Studies For Visual Near-duplicates Detection In The Context Of Social Media", "abstract": "<p>The massive spread of visual content through the web and social media poses\nboth challenges and opportunities. Tracking visually-similar content is an\nimportant task for studying and analyzing social phenomena related to the\nspread of such content. In this paper, we address this need by building a\ndataset of social media images and evaluating visual near-duplicates retrieval\nmethods based on image retrieval and several advanced visual feature extraction\nmethods. We evaluate the methods using a large-scale dataset of images we crawl\nfrom social media and their manipulated versions we generated, presenting\npromising results in terms of recall. We demonstrate the potential of this\nmethod in two case studies: one that shows the value of creating systems\nsupporting manual content review, and another that demonstrates the usefulness\nof automatic large-scale data analysis.</p>\n", "tags": ["Survey-Paper", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-21.397647857666016, -39.3040771484375], "cluster": 3}, {"key": "matsubara2019target", "year": "2020", "citations": "8", "title": "Target-oriented Deformation Of Visual-semantic Embedding Space", "abstract": "<p>Multimodal embedding is a crucial research topic for cross-modal\nunderstanding, data mining, and translation. Many studies have attempted to\nextract representations from given entities and align them in a shared\nembedding space. However, because entities in different modalities exhibit\ndifferent abstraction levels and modality-specific information, it is\ninsufficient to embed related entities close to each other. In this study, we\npropose the Target-Oriented Deformation Network (TOD-Net), a novel module that\ncontinuously deforms the embedding space into a new space under a given\ncondition, thereby adjusting similarities between entities. Unlike methods\nbased on cross-modal attention, TOD-Net is a post-process applied to the\nembedding space learned by existing embedding systems and improves their\nperformances of retrieval. In particular, when combined with cutting-edge\nmodels, TOD-Net gains the state-of-the-art cross-modal retrieval model\nassociated with the MSCOCO dataset. Qualitative analysis reveals that TOD-Net\nsuccessfully emphasizes entity-specific concepts and retrieves diverse targets\nvia handling higher levels of diversity than existing models.</p>\n", "tags": ["Multimodal-Retrieval", "Datasets"], "tsne_embedding": [-17.35454559326172, -32.314056396484375], "cluster": 3}, {"key": "matsui2017pqk", "year": "2017", "citations": "3", "title": "Pqk-means: Billion-scale Clustering For Product-quantized Codes", "abstract": "<p>Data clustering is a fundamental operation in data analysis. For handling\nlarge-scale data, the standard k-means clustering method is not only slow, but\nalso memory-inefficient. We propose an efficient clustering method for\nbillion-scale feature vectors, called PQk-means. By first compressing input\nvectors into short product-quantized (PQ) codes, PQk-means achieves fast and\nmemory-efficient clustering, even for high-dimensional vectors. Similar to\nk-means, PQk-means repeats the assignment and update steps, both of which can\nbe performed in the PQ-code domain. Experimental results show that even\nshort-length (32 bit) PQ-codes can produce competitive results compared with\nk-means. This result is of practical importance for clustering in\nmemory-restricted environments. Using the proposed PQk-means scheme, the\nclustering of one billion 128D SIFT features with K = 10^5 is achieved within\n14 hours, using just 32 GB of memory consumption on a single computer.</p>\n", "tags": ["Quantization", "Large-Scale-Search", "Scalability"], "tsne_embedding": [32.94063949584961, 20.16264533996582], "cluster": 2}, {"key": "matsui2017pqtable", "year": "2017", "citations": "0", "title": "Pqtable: Non-exhaustive Fast Search For Product-quantized Codes Using Hash Tables", "abstract": "<p>In this paper, we propose a product quantization table (PQTable); a fast\nsearch method for product-quantized codes via hash-tables. An identifier of\neach database vector is associated with the slot of a hash table by using its\nPQ-code as a key. For querying, an input vector is PQ-encoded and hashed, and\nthe items associated with that code are then retrieved. The proposed PQTable\nproduces the same results as a linear PQ scan, and is 10^2 to 10^5 times\nfaster. Although state-of-the-art performance can be achieved by previous\ninverted-indexing-based approaches, such methods require manually-designed\nparameter setting and significant training; our PQTable is free of these\nlimitations, and therefore offers a practical and effective solution for\nreal-world problems. Specifically, when the vectors are highly compressed, our\nPQTable achieves one of the fastest search performances on a single CPU to date\nwith significantly efficient memory usage (0.059 ms per query over 10^9 data\npoints with just 5.5 GB memory consumption). Finally, we show that our proposed\nPQTable can naturally handle the codes of an optimized product quantization\n(OPQTable).</p>\n", "tags": ["Quantization", "Memory-Efficiency", "Evaluation"], "tsne_embedding": [25.921688079833984, 6.423581123352051], "cluster": 2}, {"key": "mazumder2021few", "year": "2021", "citations": "22", "title": "Few-shot Keyword Spotting In Any Language", "abstract": "<p>We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe\npromising initial results on keyword search.</p>\n", "tags": ["Few-Shot-&-Zero-Shot"], "tsne_embedding": [-4.500997066497803, -32.83781814575195], "cluster": 3}, {"key": "mccallum2024effect", "year": "2024", "citations": "2", "title": "On The Effect Of Data-augmentation On Local Embedding Properties In The Contrastive Learning Of Music Audio Representations", "abstract": "<p>Audio embeddings are crucial tools in understanding large catalogs of music.\nTypically embeddings are evaluated on the basis of the performance they provide\nin a wide range of downstream tasks, however few studies have investigated the\nlocal properties of the embedding spaces themselves which are important in\nnearest neighbor algorithms, commonly used in music search and recommendation.\nIn this work we show that when learning audio representations on music datasets\nvia contrastive learning, musical properties that are typically homogeneous\nwithin a track (e.g., key and tempo) are reflected in the locality of\nneighborhoods in the resulting embedding space. By applying appropriate data\naugmentation strategies, localisation of such properties can not only be\nreduced but the localisation of other attributes is increased. For example,\nlocality of features such as pitch and tempo that are less relevant to\nnon-expert listeners, may be mitigated while improving the locality of more\nsalient features such as genre and mood, achieving state-of-the-art performance\nin nearest neighbor retrieval accuracy. Similarly, we show that the optimal\nselection of data augmentation strategies for contrastive learning of music\naudio embeddings is dependent on the downstream task, highlighting this as an\nimportant embedding design decision.</p>\n", "tags": ["Self-Supervised", "Recommender-Systems", "ICASSP", "Datasets", "Evaluation"], "tsne_embedding": [6.929017066955566, -43.44184494018555], "cluster": 3}, {"key": "mccauley2018adaptive", "year": "2018", "citations": "6", "title": "Adaptive Mapreduce Similarity Joins", "abstract": "<p>Similarity joins are a fundamental database operation. Given data sets S and\nR, the goal of a similarity join is to find all points x in S and y in R with\ndistance at most r. Recent research has investigated how locality-sensitive\nhashing (LSH) can be used for similarity join, and in particular two recent\nlines of work have made exciting progress on LSH-based join performance. Hu,\nTao, and Yi (PODS 17) investigated joins in a massively parallel setting,\nshowing strong results that adapt to the size of the output. Meanwhile, Ahle,\nAum\"uller, and Pagh (SODA 17) showed a sequential algorithm that adapts to the\nstructure of the data, matching classic bounds in the worst case but improving\nthem significantly on more structured data. We show that this adaptive strategy\ncan be adapted to the parallel setting, combining the advantages of these\napproaches. In particular, we show that a simple modification to Hu et al.\u2019s\nalgorithm achieves bounds that depend on the density of points in the dataset\nas well as the total outsize of the output. Our algorithm uses no extra\nparameters over other LSH approaches (in particular, its execution does not\ndepend on the structure of the dataset), and is likely to be efficient in\npractice.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [17.040935516357422, 10.748462677001953], "cluster": 6}, {"key": "mccauley2019approximate", "year": "2019", "citations": "2", "title": "Approximate Similarity Search Under Edit Distance Using Locality-sensitive Hashing", "abstract": "<p>Edit distance similarity search, also called approximate pattern matching, is\na fundamental problem with widespread database applications. The goal of the\nproblem is to preprocess \\(n\\) strings of length \\(d\\), to quickly answer queries\n\\(q\\) of the form: if there is a database string within edit distance \\(r\\) of \\(q\\),\nreturn a database string within edit distance \\(cr\\) of \\(q\\). Previous approaches\nto this problem either rely on very large (superconstant) approximation ratios\n\\(c\\), or very small search radii \\(r\\). Outside of a narrow parameter range, these\nsolutions are not competitive with trivially searching through all \\(n\\) strings.\n  In this work give a simple and easy-to-implement hash function that can\nquickly answer queries for a wide range of parameters. Specifically, our\nstrategy can answer queries in time \\(\\tilde{O}(d3^rn^{1/c})\\). The best known\npractical results require \\(c \\gg r\\) to achieve any correctness guarantee;\nmeanwhile, the best known theoretical results are very involved and difficult\nto implement, and require query time at least \\(24^r\\). Our results significantly\nbroaden the range of parameters for which we can achieve nontrivial bounds,\nwhile retaining the practicality of a locality-sensitive hash function.\n  We also show how to apply our ideas to the closely-related Approximate\nNearest Neighbor problem for edit distance, obtaining similar time bounds.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [21.22272300720215, 43.95427322387695], "cluster": 4}, {"key": "mccauley2024improved", "year": "2024", "citations": "0", "title": "Improved Space-efficient Approximate Nearest Neighbor Search Using Function Inversion", "abstract": "<p>Approximate nearest neighbor search (ANN) data structures have widespread\napplications in machine learning, computational biology, and text processing.\nThe goal of ANN is to preprocess a set S so that, given a query q, we can find\na point y whose distance from q approximates the smallest distance from q to\nany point in S. For most distance functions, the best-known ANN bounds for\nhigh-dimensional point sets are obtained using techniques based on\nlocality-sensitive hashing (LSH).\n  Unfortunately, space efficiency is a major challenge for LSH-based data\nstructures. Classic LSH techniques require a very large amount of space,\noftentimes polynomial in |S|. A long line of work has developed intricate\ntechniques to reduce this space usage, but these techniques suffer from\ndownsides: they must be hand tailored to each specific LSH, are often\ncomplicated, and their space reduction comes at the cost of significantly\nincreased query times.\n  In this paper we explore a new way to improve the space efficiency of LSH\nusing function inversion techniques, originally developed in (Fiat and Naor\n2000).\n  We begin by describing how function inversion can be used to improve LSH data\nstructures. This gives a fairly simple, black box method to reduce LSH space\nusage.\n  Then, we give a data structure that leverages function inversion to improve\nthe query time of the best known near-linear space data structure for\napproximate nearest neighbor search under Euclidean distance: the ALRW data\nstructure of (Andoni, Laarhoven, Razenshteyn, and Waingarten 2017). ALRW was\npreviously shown to be optimal among \u201clist-of-points\u201d data structures for both\nEuclidean and Manhattan ANN; thus, in addition to giving improved bounds, our\nresults imply that list-of-points data structures are not optimal for Euclidean\nor Manhattan ANN.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning"], "tsne_embedding": [19.767742156982422, 42.61058807373047], "cluster": 4}, {"key": "mcfee2010learning", "year": "2011", "citations": "122", "title": "Learning Multi-modal Similarity", "abstract": "<p>In many applications involving multi-media data, the definition of similarity\nbetween items is integral to several key tasks, e.g., nearest-neighbor\nretrieval, classification, and recommendation. Data in such regimes typically\nexhibits multiple modalities, such as acoustic and visual content of video.\nIntegrating such heterogeneous data to form a holistic similarity space is\ntherefore a key challenge to be overcome in many real-world applications.\n  We present a novel multiple kernel learning technique for integrating\nheterogeneous data into a single, unified similarity space. Our algorithm\nlearns an optimal ensemble of kernel transfor- mations which conform to\nmeasurements of human perceptual similarity, as expressed by relative\ncomparisons. To cope with the ubiquitous problems of subjectivity and\ninconsistency in multi- media similarity, we develop graph-based techniques to\nfilter similarity measurements, resulting in a simplified and robust training\nprocedure.</p>\n", "tags": ["Graph-Based-Ann", "Recommender-Systems"], "tsne_embedding": [-16.99457359313965, -8.719477653503418], "cluster": 1}, {"key": "mckeown2022hamming", "year": "2023", "citations": "9", "title": "Hamming Distributions Of Popular Perceptual Hashing Techniques", "abstract": "<p>Content-based file matching has been widely deployed for decades, largely for\nthe detection of sources of copyright infringement, extremist materials, and\nabusive sexual media. Perceptual hashes, such as Microsoft\u2019s PhotoDNA, are one\nautomated mechanism for facilitating detection, allowing for machines to\napproximately match visual features of an image or video in a robust manner.\nHowever, there does not appear to be much public evaluation of such approaches,\nparticularly when it comes to how effective they are against content-preserving\nmodifications to media files. In this paper, we present a million-image scale\nevaluation of several perceptual hashing archetypes for popular algorithms\n(including Facebook\u2019s PDQ, Apple\u2019s Neuralhash, and the popular pHash library)\nagainst seven image variants. The focal point is the distribution of Hamming\ndistance scores between both unrelated images and image variants to better\nunderstand the problems faced by each approach.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-3.8219590187072754, -7.441591262817383], "cluster": 1}, {"key": "medini2020solar", "year": "2021", "citations": "6", "title": "SOLAR: Sparse Orthogonal Learned And Random Embeddings", "abstract": "<p>Dense embedding models are commonly deployed in commercial search engines,\nwherein all the document vectors are pre-computed, and near-neighbor search\n(NNS) is performed with the query vector to find relevant documents. However,\nthe bottleneck of indexing a large number of dense vectors and performing an\nNNS hurts the query time and accuracy of these models. In this paper, we argue\nthat high-dimensional and ultra-sparse embedding is a significantly superior\nalternative to dense low-dimensional embedding for both query efficiency and\naccuracy. Extreme sparsity eliminates the need for NNS by replacing them with\nsimple lookups, while its high dimensionality ensures that the embeddings are\ninformative even when sparse. However, learning extremely high dimensional\nembeddings leads to blow up in the model size. To make the training feasible,\nwe propose a partitioning algorithm that learns such high dimensional\nembeddings across multiple GPUs without any communication. This is facilitated\nby our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random\n(SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal\nby design, while the query vectors are learned and sparse. We theoretically\nprove that our way of one-sided learning is equivalent to learning both query\nand label embeddings. With these unique properties, we can successfully train\n500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books\nand multi-label classification on the three largest public datasets. We achieve\nsuperior precision and recall compared to the respective state-of-the-art\nbaselines for each of the tasks with up to 10 times faster speed.</p>\n", "tags": ["Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [29.265050888061523, 26.060550689697266], "cluster": 2}, {"key": "medini2021solar", "year": "2021", "citations": "6", "title": "SOLAR: Sparse Orthogonal Learned And Random Embeddings", "abstract": "<p>Dense embedding models are commonly deployed in commercial search engines,\nwherein all the document vectors are pre-computed, and near-neighbor search\n(NNS) is performed with the query vector to find relevant documents. However,\nthe bottleneck of indexing a large number of dense vectors and performing an\nNNS hurts the query time and accuracy of these models. In this paper, we argue\nthat high-dimensional and ultra-sparse embedding is a significantly superior\nalternative to dense low-dimensional embedding for both query efficiency and\naccuracy. Extreme sparsity eliminates the need for NNS by replacing them with\nsimple lookups, while its high dimensionality ensures that the embeddings are\ninformative even when sparse. However, learning extremely high dimensional\nembeddings leads to blow up in the model size. To make the training feasible,\nwe propose a partitioning algorithm that learns such high dimensional\nembeddings across multiple GPUs without any communication. This is facilitated\nby our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random\n(SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal\nby design, while the query vectors are learned and sparse. We theoretically\nprove that our way of one-sided learning is equivalent to learning both query\nand label embeddings. With these unique properties, we can successfully train\n500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books\nand multi-label classification on the three largest public datasets. We achieve\nsuperior precision and recall compared to the respective state-of-the-art\nbaselines for each of the tasks with up to 10 times faster speed.</p>\n", "tags": ["Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [29.265050888061523, 26.060550689697266], "cluster": 2}, {"key": "mehrotra2024building", "year": "2024", "citations": "0", "title": "Building Retrieval Systems For The Clueweb22-b Corpus", "abstract": "<p>The ClueWeb22 dataset containing nearly 10 billion documents was released in\n2022 to support academic and industry research. The goal of this project was to\nbuild retrieval baselines for the English section of the \u201csuper head\u201d part\n(category B) of this dataset. These baselines can then be used by the research\ncommunity to compare their systems and also to generate data to train/evaluate\nnew retrieval and ranking algorithms. The report covers sparse and dense first\nstage retrievals as well as neural rerankers that were implemented for this\ndataset. These systems are available as a service on a Carnegie Mellon\nUniversity cluster.</p>\n", "tags": ["Datasets"], "tsne_embedding": [5.006999492645264, -33.89296340942383], "cluster": 3}, {"key": "mei20203rd", "year": "2020", "citations": "2", "title": "3rd Place Solution To \"google Landmark Retrieval 2020\"", "abstract": "<p>Image retrieval is a fundamental problem in computer vision. This paper\npresents our 3rd place detailed solution to the Google Landmark Retrieval 2020\nchallenge. We focus on the exploration of data cleaning and models with metric\nlearning. We use a data cleaning strategy based on embedding clustering.\nBesides, we employ a data augmentation method called Corner-Cutmix, which\nimproves the model\u2019s ability to recognize multi-scale and occluded landmark\nimages. We show in detail the ablation experiments and results of our method.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-17.66836166381836, -39.43925857543945], "cluster": 3}, {"key": "meiner2023data", "year": "2025", "citations": "0", "title": "Data-free Dynamic Compression Of Cnns For Tractable Efficiency", "abstract": "<p>To reduce the computational cost of convolutional neural networks (CNNs) on\nresource-constrained devices, structured pruning approaches have shown promise\nin lowering floating-point operations (FLOPs) without substantial drops in\naccuracy. However, most methods require fine-tuning or specific training\nprocedures to achieve a reasonable trade-off between retained accuracy and\nreduction in FLOPs, adding computational overhead and requiring training data\nto be available. To this end, we propose HASTE (Hashing for Tractable\nEfficiency), a data-free, plug-and-play convolution module that instantly\nreduces a network\u2019s test-time inference cost without training or fine-tuning.\nOur approach utilizes locality-sensitive hashing (LSH) to detect redundancies\nin the channel dimension of latent feature maps, compressing similar channels\nto reduce input and filter depth simultaneously, resulting in cheaper\nconvolutions. We demonstrate our approach on the popular vision benchmarks\nCIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a\n1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on\nCIFAR-10 for our HASTE module.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [8.201102256774902, 24.193986892700195], "cluster": 4}, {"key": "meisburger2020distributed", "year": "2020", "citations": "2", "title": "Distributed Tera-scale Similarity Search With MPI: Provably Efficient Similarity Search Over Billions Without A Single Distance Computation", "abstract": "<p>We present SLASH (Sketched LocAlity Sensitive Hashing), an MPI (Message\nPassing Interface) based distributed system for approximate similarity search\nover terabyte scale datasets. SLASH provides a multi-node implementation of the\npopular LSH (locality sensitive hashing) algorithm, which is generally\nimplemented on a single machine. We show how we can append the LSH algorithm\nwith heavy hitters sketches to provably solve the (high) similarity search\nproblem without a single distance computation. Overall, we mathematically show\nthat, under realistic data assumptions, we can identify the near-neighbor of a\ngiven query \\(q\\) in sub-linear (\\( \\ll O(n)\\)) number of simple sketch aggregation\noperations only. To make such a system practical, we offer a novel design and\nsketching solution to reduce the inter-machine communication overheads\nexponentially. In a direct comparison on comparable hardware, SLASH is more\nthan 10000x faster than the popular LSH package in PySpark. PySpark is a\nwidely-adopted distributed implementation of the LSH algorithm for large\ndatasets and is deployed in commercial platforms. In the end, we show how our\nsystem scale to Tera-scale Criteo dataset with more than 4 billion samples.\nSLASH can index this 2.3 terabyte data over 20 nodes in under an hour, with\nquery times in a fraction of milliseconds. To the best of our knowledge, there\nis no open-source system that can index and perform a similarity search on\nCriteo with a commodity cluster.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [24.72369956970215, 30.693971633911133], "cluster": 2}, {"key": "melekhov2021digging", "year": "2021", "citations": "7", "title": "Digging Into Self-supervised Learning Of Feature Descriptors", "abstract": "<p>Fully-supervised CNN-based approaches for learning local image descriptors\nhave shown remarkable results in a wide range of geometric tasks. However, most\nof them require per-pixel ground-truth keypoint correspondence data which is\ndifficult to acquire at scale. To address this challenge, recent weakly- and\nself-supervised methods can learn feature descriptors from relative camera\nposes or using only synthetic rigid transformations such as homographies. In\nthis work, we focus on understanding the limitations of existing\nself-supervised approaches and propose a set of improvements that combined lead\nto powerful feature descriptors. We show that increasing the search space from\nin-pair to in-batch for hard negative mining brings consistent improvement. To\nenhance the discriminativeness of feature descriptors, we propose a\ncoarse-to-fine method for mining local hard negatives from a wider search space\nby using global visual image descriptors. We demonstrate that a combination of\nsynthetic homography transformation, color augmentation, and photorealistic\nimage stylization produces useful representations that are viewpoint and\nillumination invariant. The feature descriptors learned by the proposed\napproach perform competitively and surpass their fully- and weakly-supervised\ncounterparts on various geometric benchmarks such as image-based localization,\nsparse feature matching, and image retrieval.</p>\n", "tags": ["Supervised", "Self-Supervised", "Image-Retrieval"], "tsne_embedding": [-49.07590103149414, 2.0338497161865234], "cluster": 0}, {"key": "meloacosta2022instance", "year": "2022", "citations": "0", "title": "An Instance Selection Algorithm For Big Data In High Imbalanced Datasets Based On LSH", "abstract": "<p>Training of Machine Learning (ML) models in real contexts often deals with\nbig data sets and high-class imbalance samples where the class of interest is\nunrepresented (minority class). Practical solutions using classical ML models\naddress the problem of large data sets using parallel/distributed\nimplementations of training algorithms, approximate model-based solutions, or\napplying instance selection (IS) algorithms to eliminate redundant information.\nHowever, the combined problem of big and high imbalanced datasets has been less\naddressed. This work proposes three new methods for IS to be able to deal with\nlarge and imbalanced data sets. The proposed methods use Locality Sensitive\nHashing (LSH) as a base clustering technique, and then three different sampling\nmethods are applied on top of the clusters (or buckets) generated by LSH. The\nalgorithms were developed in the Apache Spark framework, guaranteeing their\nscalability. The experiments carried out in three different datasets suggest\nthat the proposed IS methods can improve the performance of a base ML model\nbetween 5% and 19% in terms of the geometric mean.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [30.22903823852539, -10.58356761932373], "cluster": 7}, {"key": "memmesheimer2020skeleton", "year": "2022", "citations": "29", "title": "Skeleton-dml: Deep Metric Learning For Skeleton-based One-shot Action Recognition", "abstract": "<p>One-shot action recognition allows the recognition of human-performed actions\nwith only a single training example. This can influence human-robot-interaction\npositively by enabling the robot to react to previously unseen behaviour. We\nformulate the one-shot action recognition problem as a deep metric learning\nproblem and propose a novel image-based skeleton representation that performs\nwell in a metric learning setting. Therefore, we train a model that projects\nthe image representations into an embedding space. In embedding space the\nsimilar actions have a low euclidean distance while dissimilar actions have a\nhigher distance. The one-shot action recognition problem becomes a\nnearest-neighbor search in a set of activity reference samples. We evaluate the\nperformance of our proposed representation against a variety of other\nskeleton-based image representations. In addition, we present an ablation study\nthat shows the influence of different embedding vector sizes, losses and\naugmentation. Our approach lifts the state-of-the-art by 3.3% for the one-shot\naction recognition protocol on the NTU RGB+D 120 dataset under a comparable\ntraining setup. With additional augmentation our result improved over 7.7%.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-35.053409576416016, -9.644993782043457], "cluster": 5}, {"key": "meng2024efficient", "year": "2024", "citations": "0", "title": "Efficient Approximation Of Earth Mover's Distance Based On Nearest Neighbor Search", "abstract": "<p>Earth Mover\u2019s Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algorithms on image classification and retrieval tasks. We also apply NNS-EMD to calculate transport mapping and realize color transfer between images. NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods.</p>\n", "tags": ["Efficiency", "Memory-Efficiency", "Scalability", "Datasets"], "tsne_embedding": [40.04647445678711, 20.566801071166992], "cluster": 2}, {"key": "merrick2024embedding", "year": "2024", "citations": "0", "title": "Embedding And Clustering Your Data Can Improve Contrastive Pretraining", "abstract": "<p>Recent studies of large-scale contrastive pretraining in the text embedding\ndomain show that using single-source minibatches, rather than mixed-source\nminibatches, can substantially improve overall model accuracy. In this work, we\nexplore extending training data stratification beyond source granularity by\nleveraging a pretrained text embedding model and the classic k-means clustering\nalgorithm to further split training data apart by the semantic clusters within\neach source. Experimentally, we observe a notable increase in NDCG@10 when\npretraining a BERT-based text embedding model on query-passage pairs from the\nMSMARCO passage retrieval dataset. Additionally, we conceptually connect our\nclustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B\nmethodology and the nearest-neighbor-based hard-negative mining aspect of the\nANCE methodology and discuss how this unified view motivates future lines of\nresearch on the organization of contrastive pretraining data.</p>\n", "tags": ["Scalability", "Datasets"], "tsne_embedding": [2.1036946773529053, -28.670673370361328], "cluster": 3}, {"key": "messina2020fine", "year": "2021", "citations": "116", "title": "Fine-grained Visual Textual Alignment For Cross-modal Retrieval Using Transformer Encoders", "abstract": "<p>Despite the evolution of deep-learning-based visual-textual processing\nsystems, precise multi-modal matching remains a challenging task. In this work,\nwe tackle the task of cross-modal retrieval through image-sentence matching\nbased on word-region alignments, using supervision only at the global\nimage-sentence level. Specifically, we present a novel approach called\nTransformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a\nfine-grained match between the underlying components of images and sentences,\ni.e., image regions and words, respectively, in order to preserve the\ninformative richness of both modalities. TERAN obtains state-of-the-art results\non the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover,\non MS-COCO, it also outperforms current approaches on the sentence retrieval\ntask.\n  Focusing on scalable cross-modal information retrieval, TERAN is designed to\nkeep the visual and textual data pipelines well separated. Cross-attention\nlinks invalidate any chance to separately extract visual and textual features\nneeded for the online search and the offline indexing steps in large-scale\nretrieval systems. In this respect, TERAN merges the information from the two\ndomains only during the final alignment phase, immediately before the loss\ncomputation. We argue that the fine-grained alignments produced by TERAN pave\nthe way towards the research for effective and efficient methods for\nlarge-scale cross-modal information retrieval. We compare the effectiveness of\nour approach against relevant state-of-the-art methods. On the MS-COCO 1K test\nset, we obtain an improvement of 5.7% and 3.5% respectively on the image and\nthe sentence retrieval tasks on the Recall@1 metric. The code used for the\nexperiments is publicly available on GitHub at\nhttps://github.com/mesnico/TERAN.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-28.995691299438477, -13.381911277770996], "cluster": 5}, {"key": "messina2020transformer", "year": "2021", "citations": "56", "title": "Transformer Reasoning Network For Image-text Matching And Retrieval", "abstract": "<p>Image-text matching is an interesting and fascinating task in modern AI\nresearch. Despite the evolution of deep-learning-based image and text\nprocessing systems, multi-modal matching remains a challenging problem. In this\nwork, we consider the problem of accurate image-text matching for the task of\nmulti-modal large-scale information retrieval. State-of-the-art results in\nimage-text matching are achieved by inter-playing image and text features from\nthe two different processing pipelines, usually using mutual attention\nmechanisms. However, this invalidates any chance to extract separate visual and\ntextual features needed for later indexing steps in large-scale retrieval\nsystems. In this regard, we introduce the Transformer Encoder Reasoning Network\n(TERN), an architecture built upon one of the modern relationship-aware\nself-attentive architectures, the Transformer Encoder (TE). This architecture\nis able to separately reason on the two different modalities and to enforce a\nfinal common abstract concept space by sharing the weights of the deeper\ntransformer layers. Thanks to this design, the implemented network is able to\nproduce compact and very rich visual and textual features available for the\nsuccessive indexing step. Experiments are conducted on the MS-COCO dataset, and\nwe evaluate the results using a discounted cumulative gain metric with\nrelevance computed exploiting caption similarities, in order to assess possibly\nnon-exact but relevant search results. We demonstrate that on this metric we\nare able to achieve state-of-the-art results in the image retrieval task. Our\ncode is freely available at https://github.com/mesnico/TERN.</p>\n", "tags": ["Image-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [-28.980981826782227, -13.54343318939209], "cluster": 5}, {"key": "messina2021towards", "year": "2021", "citations": "7", "title": "Towards Efficient Cross-modal Visual Textual Retrieval Using Transformer-encoder Deep Features", "abstract": "<p>Cross-modal retrieval is an important functionality in modern search engines,\nas it increases the user experience by allowing queries and retrieved objects\nto pertain to different modalities. In this paper, we focus on the\nimage-sentence retrieval task, where the objective is to efficiently find\nrelevant images for a given sentence (image-retrieval) or the relevant\nsentences for a given image (sentence-retrieval). Computer vision literature\nreports the best results on the image-sentence matching task using deep neural\nnetworks equipped with attention and self-attention mechanisms. They evaluate\nthe matching performance on the retrieval task by performing sequential scans\nof the whole dataset. This method does not scale well with an increasing amount\nof images or captions. In this work, we explore different preprocessing\ntechniques to produce sparsified deep multi-modal features extracting them from\nstate-of-the-art deep-learning architectures for image-text matching. Our main\nobjective is to lay down the paths for efficient indexing of complex\nmulti-modal descriptions. We use the recently introduced TERN architecture as\nan image-sentence features extractor. It is designed for producing fixed-size\n1024-d vectors describing whole images and sentences, as well as\nvariable-length sets of 1024-d vectors describing the various building\ncomponents of the two modalities (image regions and sentence words\nrespectively). All these vectors are enforced by the TERN design to lie into\nthe same common space. Our experiments show interesting preliminary results on\nthe explored methods and suggest further experimentation in this important\nresearch direction.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [-28.759681701660156, -15.14874267578125], "cluster": 5}, {"key": "messina2022aladin", "year": "2022", "citations": "20", "title": "ALADIN: Distilling Fine-grained Alignment Scores For Efficient Image-text Matching And Retrieval", "abstract": "<p>Image-text matching is gaining a leading role among tasks involving the joint\nunderstanding of vision and language. In literature, this task is often used as\na pre-training objective to forge architectures able to jointly deal with\nimages and texts. Nonetheless, it has a direct downstream application:\ncross-modal retrieval, which consists in finding images related to a given\nquery text or vice-versa. Solving this task is of critical importance in\ncross-modal search engines. Many recent methods proposed effective solutions to\nthe image-text matching problem, mostly using recent large vision-language (VL)\nTransformer networks. However, these models are often computationally\nexpensive, especially at inference time. This prevents their adoption in\nlarge-scale cross-modal retrieval scenarios, where results should be provided\nto the user almost instantaneously. In this paper, we propose to fill in the\ngap between effectiveness and efficiency by proposing an ALign And DIstill\nNetwork (ALADIN). ALADIN first produces high-effective scores by aligning at\nfine-grained level images and texts. Then, it learns a shared embedding space -\nwhere an efficient kNN search can be performed - by distilling the relevance\nscores obtained from the fine-grained alignments. We obtained remarkable\nresults on MS-COCO, showing that our method can compete with state-of-the-art\nVL Transformers while being almost 90 times faster. The code for reproducing\nour results is available at https://github.com/mesnico/ALADIN.</p>\n", "tags": ["Efficiency", "Multimodal-Retrieval", "Similarity-Search", "Scalability"], "tsne_embedding": [-23.723400115966797, -0.6267921328544617], "cluster": 1}, {"key": "messina2024towards", "year": "2025", "citations": "0", "title": "Towards Identity-aware Cross-modal Retrieval: A Dataset And A Baseline", "abstract": "<p>Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-23.19193458557129, -19.773590087890625], "cluster": 5}, {"key": "meyer2017deep", "year": "2018", "citations": "29", "title": "Deep Metric Learning And Image Classification With Nearest Neighbour Gaussian Kernels", "abstract": "<p>We present a Gaussian kernel loss function and training algorithm for\nconvolutional neural networks that can be directly applied to both distance\nmetric learning and image classification problems. Our method treats all\ntraining features from a deep neural network as Gaussian kernel centres and\ncomputes loss by summing the influence of a feature\u2019s nearby centres in the\nfeature embedding space. Our approach is made scalable by treating it as an\napproximate nearest neighbour search problem. We show how to make end-to-end\nlearning feasible, resulting in a well formed embedding space, in which\nsemantically related instances are likely to be located near one another,\nregardless of whether or not the network was trained on those classes. Our\napproach outperforms state-of-the-art deep metric learning approaches on\nembedding learning challenges, as well as conventional softmax classification\non several datasets.</p>\n", "tags": ["Distance-Metric-Learning", "Similarity-Search", "Datasets"], "tsne_embedding": [-9.730106353759766, -18.37718963623047], "cluster": 1}, {"key": "miao2024locality", "year": "2024", "citations": "1", "title": "Locality-sensitive Hashing-based Efficient Point Transformer With Applications In High-energy Physics", "abstract": "<p>This study introduces a novel transformer model optimized for large-scale\npoint cloud processing in scientific domains such as high-energy physics (HEP)\nand astrophysics. Addressing the limitations of graph neural networks and\nstandard transformers, our model integrates local inductive bias and achieves\nnear-linear complexity with hardware-friendly regular operations. One\ncontribution of this work is the quantitative analysis of the error-complexity\ntradeoff of various sparsification techniques for building efficient\ntransformers. Our findings highlight the superiority of using\nlocality-sensitive hashing (LSH), especially OR &amp; AND-construction LSH, in\nkernel approximation for large-scale point cloud data with local inductive\nbias. Based on this finding, we propose LSH-based Efficient Point Transformer\n(HEPT), which combines E\\(^2\\)LSH with OR &amp; AND constructions and is built upon\nregular computations. HEPT demonstrates remarkable performance on two critical\nyet time-consuming HEP tasks, significantly outperforming existing GNNs and\ntransformers in accuracy and computational speed, marking a significant\nadvancement in geometric deep learning and large-scale scientific data\nprocessing. Our code is available at https://github.com/Graph-COM/HEPT.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Scalability"], "tsne_embedding": [37.82243347167969, 36.83819580078125], "cluster": 2}, {"key": "miao2024scenegraphloc", "year": "2024", "citations": "0", "title": "Scenegraphloc: Cross-modal Coarse Visual Localization On 3D Scene Graphs", "abstract": "<p>We introduce a novel problem, i.e., the localization of an input image within\na multi-modal reference map represented by a database of 3D scene graphs. These\ngraphs comprise multiple modalities, including object-level point clouds,\nimages, attributes, and relationships between objects, offering a lightweight\nand efficient alternative to conventional methods that rely on extensive image\ndatabases. Given the available modalities, the proposed method SceneGraphLoc\nlearns a fixed-sized embedding for each node (i.e., representing an object\ninstance) in the scene graph, enabling effective matching with the objects\nvisible in the input query image. This strategy significantly outperforms other\ncross-modal methods, even without incorporating images into the map embeddings.\nWhen images are leveraged, SceneGraphLoc achieves performance close to that of\nstate-of-the-art techniques depending on large image databases, while requiring\nthree orders-of-magnitude less storage and operating orders-of-magnitude\nfaster. The code will be made public.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [53.79188537597656, -10.1995267868042], "cluster": 9}, {"key": "miech2021thinking", "year": "2021", "citations": "111", "title": "Thinking Fast And Slow: Efficient Text-to-visual Retrieval With Transformers", "abstract": "<p>Our objective is language-based search of large-scale image and video\ndatasets. For this task, the approach that consists of independently mapping\ntext and vision to a joint embedding space, a.k.a. dual encoders, is attractive\nas retrieval scales and is efficient for billions of images using approximate\nnearest neighbour search. An alternative approach of using vision-text\ntransformers with cross-attention gives considerable improvements in accuracy\nover the joint embeddings, but is often inapplicable in practice for\nlarge-scale retrieval given the cost of the cross-attention mechanisms required\nfor each sample at test time. This work combines the best of both worlds. We\nmake the following three contributions. First, we equip transformer-based\nmodels with a new fine-grained cross-attention architecture, providing\nsignificant improvements in retrieval accuracy whilst preserving scalability.\nSecond, we introduce a generic approach for combining a Fast dual encoder model\nwith our Slow but accurate transformer-based model via distillation and\nre-ranking. Finally, we validate our approach on the Flickr30K image dataset\nwhere we show an increase in inference speed by several orders of magnitude\nwhile having results competitive to the state of the art. We also extend our\nmethod to the video domain, improving the state of the art on the VATEX\ndataset.</p>\n", "tags": ["CVPR", "Similarity-Search", "Scalability", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [-37.05522155761719, -28.75495147705078], "cluster": 5}, {"key": "mikriukov2022deep", "year": "2022", "citations": "19", "title": "Deep Unsupervised Contrastive Hashing For Large-scale Cross-modal Text-image Retrieval In Remote Sensing", "abstract": "<p>Due to the availability of large-scale multi-modal data (e.g., satellite\nimages acquired by different sensors, text sentences, etc) archives, the\ndevelopment of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in RS. In this paper, we focus our\nattention on cross-modal text-image retrieval, where queries from one modality\n(e.g., text) can be matched to archive entries from another (e.g., image). Most\nof the existing cross-modal text-image retrieval systems require a high number\nof labeled training samples and also do not allow fast and memory-efficient\nretrieval due to their intrinsic characteristics. These issues limit the\napplicability of the existing cross-modal retrieval systems for large-scale\napplications in RS. To address this problem, in this paper we introduce a novel\ndeep unsupervised cross-modal contrastive hashing (DUCH) method for RS\ntext-image retrieval. The proposed DUCH is made up of two main modules: 1)\nfeature extraction module (which extracts deep representations of the\ntext-image modalities); and 2) hashing module (which learns to generate\ncross-modal binary hash codes from the extracted representations). Within the\nhashing module, we introduce a novel multi-objective loss function including:\ni) contrastive objectives that enable similarity preservation in both intra-\nand inter-modal similarities; ii) an adversarial objective that is enforced\nacross two modalities for cross-modal representation consistency; iii)\nbinarization objectives for generating representative hash codes. Experimental\nresults show that the proposed DUCH outperforms state-of-the-art unsupervised\ncross-modal hashing methods on two multi-modal (image and text) benchmark\narchives in RS. Our code is publicly available at\nhttps://git.tu-berlin.de/rsim/duch.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "Image-Retrieval", "Robustness", "Multimodal-Retrieval", "Evaluation", "Unsupervised"], "tsne_embedding": [-3.1091222763061523, 13.771963119506836], "cluster": 8}, {"key": "mikriukov2022unsupervised", "year": "2022", "citations": "5", "title": "Unsupervised Contrastive Hashing For Cross-modal Retrieval In Remote Sensing", "abstract": "<p>The development of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in remote sensing (RS). In this paper,\nwe focus our attention on cross-modal text-image retrieval, where queries from\none modality (e.g., text) can be matched to archive entries from another (e.g.,\nimage). Most of the existing cross-modal text-image retrieval systems in RS\nrequire a high number of labeled training samples and also do not allow fast\nand memory-efficient retrieval. These issues limit the applicability of the\nexisting cross-modal retrieval systems for large-scale applications in RS. To\naddress this problem, in this paper we introduce a novel unsupervised\ncross-modal contrastive hashing (DUCH) method for text-image retrieval in RS.\nTo this end, the proposed DUCH is made up of two main modules: 1) feature\nextraction module, which extracts deep representations of two modalities; 2)\nhashing module that learns to generate cross-modal binary hash codes from the\nextracted representations. We introduce a novel multi-objective loss function\nincluding: i) contrastive objectives that enable similarity preservation in\nintra- and inter-modal similarities; ii) an adversarial objective that is\nenforced across two modalities for cross-modal representation consistency; and\niii) binarization objectives for generating hash codes. Experimental results\nshow that the proposed DUCH outperforms state-of-the-art methods. Our code is\npublicly available at https://git.tu-berlin.de/rsim/duch.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Similarity-Search", "Image-Retrieval", "Robustness", "Multimodal-Retrieval", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-3.0742275714874268, 13.751011848449707], "cluster": 8}, {"key": "mishchuk2017working", "year": "2017", "citations": "264", "title": "Working Hard To Know Your Neighbor's Margins: Local Descriptor Learning Loss", "abstract": "<p>We introduce a novel loss for learning local feature descriptors which is\ninspired by the Lowe\u2019s matching criterion for SIFT. We show that the proposed\nloss that maximizes the distance between the closest positive and closest\nnegative patch in the batch is better than complex regularization methods; it\nworks well for both shallow and deep convolution network architectures.\nApplying the novel loss to the L2Net CNN architecture results in a compact\ndescriptor \u2013 it has the same dimensionality as SIFT (128) that shows\nstate-of-art performance in wide baseline stereo, patch verification and\ninstance retrieval benchmarks. It is fast, computing a descriptor takes about 1\nmillisecond on a low-end GPU.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-43.790428161621094, 2.1018762588500977], "cluster": 0}, {"key": "misra2018bernoulli", "year": "2018", "citations": "9", "title": "Bernoulli Embeddings For Graphs", "abstract": "<p>Just as semantic hashing can accelerate information retrieval, binary valued\nembeddings can significantly reduce latency in the retrieval of graphical data.\nWe introduce a simple but effective model for learning such binary vectors for\nnodes in a graph. By imagining the embeddings as independent coin flips of\nvarying bias, continuous optimization techniques can be applied to the\napproximate expected loss. Embeddings optimized in this fashion consistently\noutperform the quantization of both spectral graph embeddings and various\nlearned real-valued embeddings, on both ranking and pre-ranking tasks for a\nvariety of datasets.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Quantization", "AAAI", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [51.84925842285156, 3.5225279331207275], "cluster": 9}, {"key": "misraa2020multi", "year": "2020", "citations": "5", "title": "Multi-modal Retrieval Using Graph Neural Networks", "abstract": "<p>Most real world applications of image retrieval such as Adobe Stock, which is\na marketplace for stock photography and illustrations, need a way for users to\nfind images which are both visually (i.e. aesthetically) and conceptually (i.e.\ncontaining the same salient objects) as a query image. Learning visual-semantic\nrepresentations from images is a well studied problem for image retrieval.\nFiltering based on image concepts or attributes is traditionally achieved with\nindex-based filtering (e.g. on textual tags) or by re-ranking after an initial\nvisual embedding based retrieval. In this paper, we learn a joint vision and\nconcept embedding in the same high-dimensional space. This joint model gives\nthe user fine-grained control over the semantics of the result set, allowing\nthem to explore the catalog of images more rapidly. We model the visual and\nconcept relationships as a graph structure, which captures the rich information\nthrough node neighborhood. This graph structure helps us learn multi-modal node\nembeddings using Graph Neural Networks. We also introduce a novel inference\ntime control, based on selective neighborhood connectivity allowing the user\ncontrol over the retrieval algorithm. We evaluate these multi-modal embeddings\nquantitatively on the downstream relevance task of image retrieval on MS-COCO\ndataset and qualitatively on MS-COCO and an Adobe Stock dataset.</p>\n", "tags": ["Re-Ranking", "Hybrid-Ann-Methods", "Image-Retrieval", "Datasets"], "tsne_embedding": [56.867210388183594, -8.81978702545166], "cluster": 9}, {"key": "mithun2018webly", "year": "2018", "citations": "73", "title": "Webly Supervised Joint Embedding For Cross-modal Image-text Retrieval", "abstract": "<p>Cross-modal retrieval between visual data and natural language description\nremains a long-standing challenge in multimedia. While recent image-text\nretrieval methods offer great promise by learning deep representations aligned\nacross modalities, most of these methods are plagued by the issue of training\nwith small-scale datasets covering a limited number of images with ground-truth\nsentences. Moreover, it is extremely expensive to create a larger dataset by\nannotating millions of images with sentences and may lead to a biased model.\nInspired by the recent success of webly supervised learning in deep neural\nnetworks, we capitalize on readily-available web images with noisy annotations\nto learn robust image-text joint representation. Specifically, our main idea is\nto leverage web images and corresponding tags, along with fully annotated\ndatasets, in training for learning the visual-semantic joint embedding. We\npropose a two-stage approach for the task that can augment a typical supervised\npair-wise ranking loss based formulation with weakly-annotated web images to\nlearn a more robust visual-semantic embedding. Experiments on two standard\nbenchmark datasets demonstrate that our method achieves a significant\nperformance gain in image-text retrieval compared to state-of-the-art\napproaches.</p>\n", "tags": ["Text-Retrieval", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-22.65339469909668, -15.609441757202148], "cluster": 5}, {"key": "mitra2016dual", "year": "2016", "citations": "110", "title": "A Dual Embedding Space Model For Document Ranking", "abstract": "<p>A fundamental goal of search engines is to identify, given a query, documents\nthat have relevant text. This is intrinsically difficult because the query and\nthe document may use different vocabulary, or the document may contain query\nwords without being relevant. We investigate neural word embeddings as a source\nof evidence in document ranking. We train a word2vec embedding model on a large\nunlabelled query corpus, but in contrast to how the model is commonly used, we\nretain both the input and the output projections, allowing us to leverage both\nthe embedding spaces to derive richer distributional relationships. During\nranking we map the query words into the input space and the document words into\nthe output space, and compute a query-document relevance score by aggregating\nthe cosine similarities across all the query-document word pairs.\n  We postulate that the proposed Dual Embedding Space Model (DESM) captures\nevidence on whether a document is about a query term in addition to what is\nmodelled by traditional term-frequency based approaches. Our experiments show\nthat the DESM can re-rank top documents returned by a commercial Web search\nengine, like Bing, better than a term-matching based signal like TF-IDF.\nHowever, when ranking a larger set of candidate documents, we find the\nembeddings-based approach is prone to false positives, retrieving documents\nthat are only loosely related to the query. We demonstrate that this problem\ncan be solved effectively by ranking based on a linear mixture of the DESM and\nthe word counting features.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [8.437684059143066, -21.865299224853516], "cluster": 7}, {"key": "mitzenmacher2018robust", "year": "2019", "citations": "1", "title": "Robust Set Reconciliation Via Locality Sensitive Hashing", "abstract": "<p>We consider variations of set reconciliation problems where two parties,\nAlice and Bob, each hold a set of points in a metric space, and the goal is for\nBob to conclude with a set of points that is close to Alice\u2019s set of points in\na well-defined way. This setting has been referred to as robust set\nreconciliation. More specifically, in one variation we examine the goal is for\nBob to end with a set of points that is close to Alice\u2019s in earth mover\u2019s\ndistance, and in another the goal is for Bob to have a point that is close to\neach of Alice\u2019s. The first problem has been studied before; our results scale\nbetter with the dimension of the space. The second problem appears new.\n  Our primary novelty is utilizing Invertible Bloom Lookup Tables in\ncombination with locality sensitive hashing. This combination allows us to cope\nwith the geometric setting in a communication-efficient manner.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [-10.217222213745117, 24.739551544189453], "cluster": 8}, {"key": "miyaguchi2024tile", "year": "2024", "citations": "0", "title": "Tile Compression And Embeddings For Multi-label Classification In Geolifeclef 2024", "abstract": "<p>We explore methods to solve the multi-label classification task posed by the\nGeoLifeCLEF 2024 competition with the DS@GT team, which aims to predict the\npresence and absence of plant species at specific locations using spatial and\ntemporal remote sensing data. Our approach uses frequency-domain coefficients\nvia the Discrete Cosine Transform (DCT) to compress and pre-compute the raw\ninput data for convolutional neural networks. We also investigate nearest\nneighborhood models via locality-sensitive hashing (LSH) for prediction and to\naid in the self-supervised contrastive learning of embeddings through tile2vec.\nOur best competition model utilized geolocation features with a leaderboard\nscore of 0.152 and a best post-competition score of 0.161. Source code and\nmodels are available at https://github.com/dsgt-kaggle-clef/geolifeclef-2024.</p>\n", "tags": ["Supervised", "Self-Supervised", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [-12.420899391174316, -56.95161437988281], "cluster": 3}, {"key": "mohammad2021compact", "year": "2021", "citations": "0", "title": "Compact Binary Fingerprint For Image Copy Re-ranking", "abstract": "<p>Image copy detection is challenging and appealing topic in computer vision\nand signal processing. Recent advancements in multimedia have made distribution\nof image across the global easy and fast: that leads to many other issues such\nas forgery and image copy retrieval.\n  Local keypoint descriptors such as SIFT are used to represent the images, and\nbased on those descriptors matching, images are matched and retrieved. Features\nare quantized so that searching/matching may be made feasible for large\ndatabases at the cost of accuracy loss. In this paper, we propose binary\nfeature that is obtained by quantizing the SIFT into binary, and rank list is\nre-examined to remove the false positives. Experiments on challenging dataset\nshows the gain in accuracy and time.</p>\n", "tags": ["Re-Ranking", "Hybrid-Ann-Methods", "Datasets"], "tsne_embedding": [-16.155902862548828, 11.700058937072754], "cluster": 8}, {"key": "mohammadi2018multi", "year": "2018", "citations": "1", "title": "Multi-reference Cosine: A New Approach To Text Similarity Measurement In Large Collections", "abstract": "<p>The importance of an efficient and scalable document similarity detection\nsystem is undeniable nowadays. Search engines need batch text similarity\nmeasures to detect duplicated and near-duplicated web pages in their indexes in\norder to prevent indexing a web page multiple times. Furthermore, in the\nscoring phase, search engines need similarity measures to detect duplicated\ncontents on web pages so as to increase the quality of their results. In this\npaper, a new approach to batch text similarity detection is proposed by\ncombining some ideas from dimensionality reduction techniques and information\ngain theory. The new approach is focused on search engines need to detect\nduplicated and near-duplicated web pages. The new approach is evaluated on the\nNEWS20 dataset and the results show that the new approach is faster than the\ncosine text similarity algorithm in terms of speed and performance. On top of\nthat, It is faster and more accurate than the other rival method, Simhash\nsimilarity algorithm.</p>\n", "tags": ["Evaluation", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [20.42267417907715, 25.525745391845703], "cluster": 2}, {"key": "mohammadshahi2019aligning", "year": "2019", "citations": "8", "title": "Aligning Multilingual Word Embeddings For Cross-modal Retrieval Task", "abstract": "<p>In this paper, we propose a new approach to learn multimodal multilingual\nembeddings for matching images and their relevant captions in two languages. We\ncombine two existing objective functions to make images and captions close in a\njoint embedding space while adapting the alignment of word embeddings between\nexisting languages in our model. We show that our approach enables better\ngeneralization, achieving state-of-the-art performance in text-to-image and\nimage-to-text retrieval task, and caption-caption similarity task. Two\nmultimodal multilingual datasets are used for evaluation: Multi30k with German\nand English captions and Microsoft-COCO with English and Japanese captions.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-26.92156219482422, -34.44541549682617], "cluster": 5}, {"key": "mohan2023deep", "year": "2023", "citations": "4", "title": "Deep Metric Learning For Computer Vision: A Brief Overview", "abstract": "<p>Objective functions that optimize deep neural networks play a vital role in\ncreating an enhanced feature representation of the input data. Although\ncross-entropy-based loss formulations have been extensively used in a variety\nof supervised deep-learning applications, these methods tend to be less\nadequate when there is large intra-class variance and low inter-class variance\nin input data distribution. Deep Metric Learning seeks to develop methods that\naim to measure the similarity between data samples by learning a representation\nfunction that maps these data samples into a representative embedding space. It\nleverages carefully designed sampling strategies and loss functions that aid in\noptimizing the generation of a discriminative embedding space even for\ndistributions having low inter-class and high intra-class variances. In this\nchapter, we will provide an overview of recent progress in this area and\ndiscuss state-of-the-art Deep Metric Learning approaches.</p>\n", "tags": ["Supervised", "Distance-Metric-Learning", "Survey-Paper"], "tsne_embedding": [-9.927628517150879, -18.07846450805664], "cluster": 1}, {"key": "mohedano2016bags", "year": "2016", "citations": "154", "title": "Bags Of Local Convolutional Features For Scalable Instance Search", "abstract": "<p>This work proposes a simple instance retrieval pipeline based on encoding the\nconvolutional features of CNN using the bag of words aggregation scheme (BoW).\nAssigning each local array of activations in a convolutional layer to a visual\nword produces an \\textit{assignment map}, a compact representation that relates\nregions of an image with a visual word. We use the assignment map for fast\nspatial reranking, obtaining object localizations that are used for query\nexpansion. We demonstrate the suitability of the BoW representation based on\nlocal CNN features for instance retrieval, achieving competitive performance on\nthe Oxford and Paris buildings benchmarks. We show that our proposed system for\nCNN feature aggregation with BoW outperforms state-of-the-art techniques using\nsum pooling at a subset of the challenging TRECVid INS benchmark.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-50.77306365966797, -1.3106695413589478], "cluster": 0}, {"key": "mohedano2017saliency", "year": "2018", "citations": "37", "title": "Saliency Weighted Convolutional Features For Instance Search", "abstract": "<p>This work explores attention models to weight the contribution of local\nconvolutional representations for the instance search task. We present a\nretrieval framework based on bags of local convolutional features (BLCF) that\nbenefits from saliency weighting to build an efficient image representation.\nThe use of human visual attention models (saliency) allows significant\nimprovements in retrieval performance without the need to conduct region\nanalysis or spatial verification, and without requiring any feature fine\ntuning. We investigate the impact of different saliency models, finding that\nhigher performance on saliency benchmarks does not necessarily equate to\nimproved performance when used in instance search tasks. The proposed approach\noutperforms the state-of-the-art on the challenging INSTRE benchmark by a large\nmargin, and provides similar performance on the Oxford and Paris benchmarks\ncompared to more complex methods that use off-the-shelf representations. The\nsource code used in this project is available at\nhttps://imatge-upc.github.io/salbow/</p>\n", "tags": ["Tools-&-Libraries", "Evaluation"], "tsne_embedding": [-34.400901794433594, 1.5388176441192627], "cluster": 0}, {"key": "moiseev2023samtone", "year": "2023", "citations": "1", "title": "Samtone: Improving Contrastive Loss For Dual Encoder Retrieval Models With Same Tower Negatives", "abstract": "<p>Dual encoders have been used for retrieval tasks and representation learning\nwith good results. A standard way to train dual encoders is using a contrastive\nloss with in-batch negatives. In this work, we propose an improved contrastive\nlearning objective by adding queries or documents from the same encoder towers\nto the negatives, for which we name it as \u201ccontrastive loss with SAMe TOwer\nNEgatives\u201d (SamToNe). By evaluating on question answering retrieval benchmarks\nfrom MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval\nbenchmarks (BEIR), we demonstrate that SamToNe can effectively improve the\nretrieval quality for both symmetric and asymmetric dual encoders. By directly\nprobing the embedding spaces of the two encoding towers via the t-SNE algorithm\n(van der Maaten and Hinton, 2008), we observe that SamToNe ensures the\nalignment between the embedding spaces from the two encoder towers. Based on\nthe analysis of the embedding distance distributions of the top-\\(1\\) retrieved\nresults, we further explain the efficacy of the method from the perspective of\nregularisation.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Distance-Metric-Learning"], "tsne_embedding": [-11.403410911560059, 10.382123947143555], "cluster": 8}, {"key": "monir2024vectorsearch", "year": "2024", "citations": "0", "title": "Vectorsearch: Enhancing Document Retrieval With Semantic Embeddings And Optimized Search", "abstract": "<p>Traditional retrieval methods have been essential for assessing document\nsimilarity but struggle with capturing semantic nuances. Despite advancements\nin latent semantic analysis (LSA) and deep learning, achieving comprehensive\nsemantic understanding and accurate retrieval remains challenging due to high\ndimensionality and semantic gaps. The above challenges call for new techniques\nto effectively reduce the dimensions and close the semantic gaps. To this end,\nwe propose VectorSearch, which leverages advanced algorithms, embeddings, and\nindexing techniques for refined retrieval. By utilizing innovative multi-vector\nsearch operations and encoding searches with advanced language models, our\napproach significantly improves retrieval accuracy. Experiments on real-world\ndatasets show that VectorSearch outperforms baseline metrics, demonstrating its\nefficacy for large-scale retrieval tasks.</p>\n", "tags": ["Text-Retrieval", "Scalability", "Datasets"], "tsne_embedding": [3.420654773712158, -22.546531677246094], "cluster": 7}, {"key": "mopuri2017deep", "year": "2017", "citations": "0", "title": "Deep Image Representations Using Caption Generators", "abstract": "<p>Deep learning exploits large volumes of labeled data to learn powerful\nmodels. When the target dataset is small, it is a common practice to perform\ntransfer learning using pre-trained models to learn new task specific\nrepresentations. However, pre-trained CNNs for image recognition are provided\nwith limited information about the image during training, which is label alone.\nTasks such as scene retrieval suffer from features learned from this weak\nsupervision and require stronger supervision to better understand the contents\nof the image. In this paper, we exploit the features learned from caption\ngenerating models to learn novel task specific image representations. In\nparticular, we consider the state-of-the art captioning system Show and\nTell~\\cite{SnT-pami-2016} and the dense region description model\nDenseCap~\\cite{densecap-cvpr-2016}. We demonstrate that, owing to richer\nsupervision provided during the process of training, the features learned by\nthe captioning system perform better than those of CNNs. Further, we train a\nsiamese network with a modified pair-wise loss to fuse the features learned\nby~\\cite{SnT-pami-2016} and~\\cite{densecap-cvpr-2016} and learn image\nrepresentations suitable for retrieval. Experiments show that the proposed\nfusion exploits the complementary nature of the individual features and yields\nstate-of-the art retrieval results on benchmark datasets.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-23.34227752685547, -15.908228874206543], "cluster": 5}, {"key": "moran2013neighbourhood", "year": "2013", "citations": "22", "title": "Neighbourhood Preserving Quantisation For LSH", "abstract": "<p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>\n", "tags": ["SIGIR", "Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization"], "tsne_embedding": [2.4980545043945312, 45.96505355834961], "cluster": 4}, {"key": "moran2013variable", "year": "2013", "citations": "21", "title": "Variable Bit Quantisation For LSH", "abstract": "<p>We introduce a scheme for optimally allocating\na variable number of bits per\nLSH hyperplane. Previous approaches assign\na constant number of bits per hyperplane.\nThis neglects the fact that a subset\nof hyperplanes may be more informative\nthan others. Our method, dubbed Variable\nBit Quantisation (VBQ), provides a datadriven\nnon-uniform bit allocation across\nhyperplanes. Despite only using a fraction\nof the available hyperplanes, VBQ outperforms\nuniform quantisation by up to 168%\nfor retrieval across standard text and image\ndatasets.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [-0.3673449456691742, 43.347076416015625], "cluster": 4}, {"key": "moran2015graph", "year": "2015", "citations": "2", "title": "Graph Regularised Hashing", "abstract": "<p>In this paper we propose a two-step iterative scheme, Graph Regularised Hashing (GRH), for incrementally adjusting the positioning of the hashing hypersurfaces to better conform to the supervisory signal: in the first step the binary bits are regularised using a data similarity graph so that similar data points receive similar bits. In the second step the regularised hashcodes form targets for a set of binary classifiers which shift the position of each hypersurface so as to separate opposite bits with maximum margin. GRH exhibits superior retrieval accuracy to competing hashing methods.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [45.098812103271484, -15.639093399047852], "cluster": 9}, {"key": "moran2015regularised", "year": "2015", "citations": "18", "title": "Regularised Cross-modal Hashing", "abstract": "<p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>\n", "tags": ["SIGIR", "Hashing-Methods"], "tsne_embedding": [45.02787399291992, -14.075854301452637], "cluster": 9}, {"key": "moran2016enhancing", "year": "2016", "citations": "29", "title": "Enhancing First Story Detection Using Word Embeddings", "abstract": "<p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>\n", "tags": ["SIGIR", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [35.47897720336914, -36.28533935546875], "cluster": 7}, {"key": "moran2016learning", "year": "2016", "citations": "1", "title": "Learning To Project And Binarise For Hashing-based Approximate Nearest Neighbour Search", "abstract": "<p>In this paper we focus on improving the effectiveness of hashing-based approximate nearest neighbour search. Generating similarity preserving hashcodes for images has been shown to be an effective and efficient method for searching through large datasets. Hashcode generation generally involves two steps: bucketing the input feature space with a set of hyperplanes, followed by quantising the projection of the data-points onto the normal vectors to those hyperplanes. This procedure results in the makeup of the hashcodes depending on the positions of the data-points with respect to the hyperplanes in the feature space, allowing a degree of locality to be encoded into the hashcodes. In this paper we study the effect of learning both the hyperplanes and the thresholds as part of the same model. Most previous research either learn the hyperplanes assuming a fixed set of thresholds, or vice-versa. In our experiments over two standard image datasets we find statistically significant increases in retrieval effectiveness versus a host of state-of-the-art data-dependent and independent hashing models.</p>\n", "tags": ["SIGIR", "Hashing-Methods", "Similarity-Search", "Datasets"], "tsne_embedding": [-20.74472427368164, 16.756452560424805], "cluster": 8}, {"key": "moran2025enhancing", "year": "2016", "citations": "29", "title": "Enhancing First Story Detection Using Word Embeddings", "abstract": "<p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>\n", "tags": ["SIGIR", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [35.47897720336914, -36.28533935546875], "cluster": 7}, {"key": "moran2025graph", "year": "2015", "citations": "2", "title": "Graph Regularised Hashing", "abstract": "<p>In this paper we propose a two-step iterative scheme, Graph Regularised Hashing (GRH), for incrementally adjusting the positioning of the hashing hypersurfaces to better conform to the supervisory signal: in the first step the binary bits are regularised using a data similarity graph so that similar data points receive similar bits. In the second step the regularised hashcodes form targets for a set of binary classifiers which shift the position of each hypersurface so as to separate opposite bits with maximum margin. GRH exhibits superior retrieval accuracy to competing hashing methods.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [45.098812103271484, -15.639093399047852], "cluster": 9}, {"key": "moran2025learning", "year": "2016", "citations": "1", "title": "Learning To Project And Binarise For Hashing-based Approximate Nearest Neighbour Search", "abstract": "<p>In this paper we focus on improving the effectiveness of hashing-based approximate nearest neighbour search. Generating similarity preserving hashcodes for images has been shown to be an effective and efficient method for searching through large datasets. Hashcode generation generally involves two steps: bucketing the input feature space with a set of hyperplanes, followed by quantising the projection of the data-points onto the normal vectors to those hyperplanes. This procedure results in the makeup of the hashcodes depending on the positions of the data-points with respect to the hyperplanes in the feature space, allowing a degree of locality to be encoded into the hashcodes. In this paper we study the effect of learning both the hyperplanes and the thresholds as part of the same model. Most previous research either learn the hyperplanes assuming a fixed set of thresholds, or vice-versa. In our experiments over two standard image datasets we find statistically significant increases in retrieval effectiveness versus a host of state-of-the-art data-dependent and independent hashing models.</p>\n", "tags": ["SIGIR", "Hashing-Methods", "Similarity-Search", "Datasets"], "tsne_embedding": [-20.747085571289062, 16.757450103759766], "cluster": 8}, {"key": "moran2025neighbourhood", "year": "2013", "citations": "22", "title": "Neighbourhood Preserving Quantisation For LSH", "abstract": "<p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>\n", "tags": ["SIGIR", "Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization"], "tsne_embedding": [2.4983370304107666, 45.96467208862305], "cluster": 4}, {"key": "moran2025regularised", "year": "2015", "citations": "18", "title": "Regularised Cross-modal Hashing", "abstract": "<p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>\n", "tags": ["SIGIR", "Hashing-Methods"], "tsne_embedding": [45.02803039550781, -14.07568073272705], "cluster": 9}, {"key": "moran2025variable", "year": "2013", "citations": "21", "title": "Variable Bit Quantisation For LSH", "abstract": "<p>We introduce a scheme for optimally allocating\na variable number of bits per\nLSH hyperplane. Previous approaches assign\na constant number of bits per hyperplane.\nThis neglects the fact that a subset\nof hyperplanes may be more informative\nthan others. Our method, dubbed Variable\nBit Quantisation (VBQ), provides a datadriven\nnon-uniform bit allocation across\nhyperplanes. Despite only using a fraction\nof the available hyperplanes, VBQ outperforms\nuniform quantisation by up to 168%\nfor retrieval across standard text and image\ndatasets.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [-0.3673449456691742, 43.347076416015625], "cluster": 4}, {"key": "mordido2021evaluating", "year": "2021", "citations": "0", "title": "Evaluating Post-training Compression In Gans Using Locality-sensitive Hashing", "abstract": "<p>The analysis of the compression effects in generative adversarial networks\n(GANs) after training, i.e. without any fine-tuning, remains an unstudied,\nalbeit important, topic with the increasing trend of their computation and\nmemory requirements. While existing works discuss the difficulty of compressing\nGANs during training, requiring novel methods designed with the instability of\nGANs training in mind, we show that existing compression methods (namely\nclipping and quantization) may be directly applied to compress GANs\npost-training, without any additional changes. High compression levels may\ndistort the generated set, likely leading to an increase of outliers that may\nnegatively affect the overall assessment of existing k-nearest neighbor (KNN)\nbased metrics. We propose two new precision and recall metrics based on\nlocality-sensitive hashing (LSH), which, on top of increasing the outlier\nrobustness, decrease the complexity of assessing an evaluation sample against\n\\(n\\) reference samples from \\(O(n)\\) to \\(O(log(n))\\), if using LSH and KNN, and to\n\\(O(1)\\), if only applying LSH. We show that low-bit compression of several\npre-trained GANs on multiple datasets induces a trade-off between precision and\nrecall, retaining sample quality while sacrificing sample diversity.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization", "Robustness", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-4.527345180511475, 15.465771675109863], "cluster": 8}, {"key": "morgado2020deep", "year": "2020", "citations": "6", "title": "Deep Hashing With Hash-consistent Large Margin Proxy Embeddings", "abstract": "<p>Image hash codes are produced by binarizing\nthe embeddings of convolutional neural networks (CNN)\ntrained for either classification or retrieval. While proxy\nembeddings achieve good performance on both tasks,\nthey are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use\nof a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and\na procedure to design proxy sets that are nearly optimal\nfor both classification and hashing is introduced. The\nresulting hash-consistent large margin (HCLM) proxies\nare shown to encourage saturation of hashing units, thus\nguaranteeing a small binarization error, while producing\nhighly discriminative hash-codes. A semantic extension\n(sHCLM), aimed to improve hashing performance in\na transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant\nimprovements over state-of-the-art hashing procedures\non several small and large datasets, both within and\nbeyond the set of training classes.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Neural-Hashing", "Datasets"], "tsne_embedding": [-46.27500915527344, 3.1905949115753174], "cluster": 0}, {"key": "morgado2025deep", "year": "2020", "citations": "6", "title": "Deep Hashing With Hash-consistent Large Margin Proxy Embeddings", "abstract": "<p>Image hash codes are produced by binarizing\nthe embeddings of convolutional neural networks (CNN)\ntrained for either classification or retrieval. While proxy\nembeddings achieve good performance on both tasks,\nthey are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use\nof a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and\na procedure to design proxy sets that are nearly optimal\nfor both classification and hashing is introduced. The\nresulting hash-consistent large margin (HCLM) proxies\nare shown to encourage saturation of hashing units, thus\nguaranteeing a small binarization error, while producing\nhighly discriminative hash-codes. A semantic extension\n(sHCLM), aimed to improve hashing performance in\na transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant\nimprovements over state-of-the-art hashing procedures\non several small and large datasets, both within and\nbeyond the set of training classes.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Neural-Hashing", "Datasets"], "tsne_embedding": [-46.27500915527344, 3.1905949115753174], "cluster": 0}, {"key": "morozov2019unsupervised", "year": "2019", "citations": "21", "title": "Unsupervised Neural Quantization For Compressed-domain Similarity Search", "abstract": "<p>We tackle the problem of unsupervised visual descriptors compression, which\nis a key ingredient of large-scale image retrieval systems. While the deep\nlearning machinery has benefited literally all computer vision pipelines, the\nexisting state-of-the-art compression methods employ shallow architectures, and\nwe aim to close this gap by our paper. In more detail, we introduce a DNN\narchitecture for the unsupervised compressed-domain retrieval, based on\nmulti-codebook quantization. The proposed architecture is designed to\nincorporate both fast data encoding and efficient distances computation via\nlookup tables. We demonstrate the exceptional advantage of our scheme over\nexisting quantization approaches on several datasets of visual descriptors via\noutperforming the previous state-of-the-art by a large margin.</p>\n", "tags": ["ICCV", "Quantization", "Scalability", "Similarity-Search", "Image-Retrieval", "Datasets", "Unsupervised"], "tsne_embedding": [-9.429821014404297, -9.597040176391602], "cluster": 1}, {"key": "morvan2017streaming", "year": "2017", "citations": "0", "title": "Streaming Binary Sketching Based On Subspace Tracking And Diagonal Uniformization", "abstract": "<p>In this paper, we address the problem of learning compact\nsimilarity-preserving embeddings for massive high-dimensional streams of data\nin order to perform efficient similarity search. We present a new online method\nfor computing binary compressed representations -sketches- of high-dimensional\nreal feature vectors. Given an expected code length \\(c\\) and high-dimensional\ninput data points, our algorithm provides a \\(c\\)-bits binary code for preserving\nthe distance between the points from the original high-dimensional space. Our\nalgorithm does not require neither the storage of the whole dataset nor a\nchunk, thus it is fully adaptable to the streaming setting. It also provides\nlow time complexity and convergence guarantees. We demonstrate the quality of\nour binary sketches through experiments on real data for the nearest neighbors\nsearch task in the online setting.</p>\n", "tags": ["Compact-Codes", "Similarity-Search", "Datasets"], "tsne_embedding": [21.49750518798828, 19.084508895874023], "cluster": 2}, {"key": "morvan2018needs", "year": "2018", "citations": "2", "title": "On The Needs For Rotations In Hypercubic Quantization Hashing", "abstract": "<p>The aim of this paper is to endow the well-known family of hypercubic\nquantization hashing methods with theoretical guarantees. In hypercubic\nquantization, applying a suitable (random or learned) rotation after\ndimensionality reduction has been experimentally shown to improve the results\naccuracy in the nearest neighbors search problem. We prove in this paper that\nthe use of these rotations is optimal under some mild assumptions: getting\noptimal binary sketches is equivalent to applying a rotation uniformizing the\ndiagonal of the covariance matrix between data points. Moreover, for two closed\npoints, the probability to have dissimilar binary sketches is upper bounded by\na factor of the initial distance between the data points. Relaxing these\nassumptions, we obtain a general concentration result for random matrices. We\nalso provide some experiments illustrating these theoretical points and compare\na set of algorithms in both the batch and online settings.</p>\n", "tags": ["Quantization", "Hashing-Methods"], "tsne_embedding": [16.52018165588379, 28.933809280395508], "cluster": 4}, {"key": "mor\u00e8re2016group", "year": "2016", "citations": "6", "title": "Group Invariant Deep Representations For Image Instance Retrieval", "abstract": "<p>Most image instance retrieval pipelines are based on comparison of vectors\nknown as global image descriptors between a query image and the database\nimages. Due to their success in large scale image classification,\nrepresentations extracted from Convolutional Neural Networks (CNN) are quickly\ngaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors\nfor image instance retrieval. While CNN-based descriptors are generally\nremarked for good retrieval performance at lower bitrates, they nevertheless\npresent a number of drawbacks including the lack of robustness to common object\ntransformations such as rotations compared with their interest point based FV\ncounterparts.\n  In this paper, we propose a method for computing invariant global descriptors\nfrom CNNs. Our method implements a recently proposed mathematical theory for\ninvariance in a sensory cortex modeled as a feedforward neural network. The\nresulting global descriptors can be made invariant to multiple arbitrary\ntransformation groups while retaining good discriminativeness.\n  Based on a thorough empirical evaluation using several publicly available\ndatasets, we show that our method is able to significantly and consistently\nimprove retrieval results every time a new type of invariance is incorporated.\nWe also show that our method which has few parameters is not prone to\noverfitting: improvements generalize well across datasets with different\nproperties with regard to invariances. Finally, we show that our descriptors\nare able to compare favourably to other state-of-the-art compact descriptors in\nsimilar bitranges, exceeding the highest retrieval results reported in the\nliterature on some datasets. A dedicated dimensionality reduction step\n\u2013quantization or hashing\u2013 may be able to further improve the competitiveness\nof the descriptors.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-45.36039733886719, 4.860988616943359], "cluster": 0}, {"key": "mor\u00e8re2016nested", "year": "2017", "citations": "18", "title": "Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval", "abstract": "<p>The goal of this work is the computation of very compact binary hashes for\nimage instance retrieval. Our approach has two novel contributions. The first\none is Nested Invariance Pooling (NIP), a method inspired from i-theory, a\nmathematical theory for computing group invariant transformations with\nfeed-forward neural networks. NIP is able to produce compact and\nwell-performing descriptors with visual representations extracted from\nconvolutional neural networks. We specifically incorporate scale, translation\nand rotation invariances but the scheme can be extended to any arbitrary sets\nof transformations. We also show that using moments of increasing order\nthroughout nesting is important. The NIP descriptors are then hashed to the\ntarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novel\nbatch-level regularization scheme specifically designed for the purpose of\nhashing (RBMH). A thorough empirical evaluation with state-of-the-art shows\nthat the results obtained both with the NIP descriptors and the NIP+RBMH hashes\nare consistently outstanding across a wide range of datasets.</p>\n", "tags": ["Hashing-Methods", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-18.599267959594727, 20.77208709716797], "cluster": 8}, {"key": "moskvyak2020keypoint", "year": "2021", "citations": "22", "title": "Keypoint-aligned Embeddings For Image Retrieval And Re-identification", "abstract": "<p>Learning embeddings that are invariant to the pose of the object is crucial\nin visual image retrieval and re-identification. The existing approaches for\nperson, vehicle, or animal re-identification tasks suffer from high intra-class\nvariance due to deformable shapes and different camera viewpoints. To overcome\nthis limitation, we propose to align the image embedding with a predefined\norder of the keypoints. The proposed keypoint aligned embeddings model\n(KAE-Net) learns part-level features via multi-task learning which is guided by\nkeypoint locations. More specifically, KAE-Net extracts channels from a feature\nmap activated by a specific keypoint through learning the auxiliary task of\nheatmap reconstruction for this keypoint. The KAE-Net is compact, generic and\nconceptually simple. It achieves state of the art performance on the benchmark\ndatasets of CUB-200-2011, Cars196 and VeRi-776 for retrieval and\nre-identification tasks.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-14.946412086486816, -24.424942016601562], "cluster": 3}, {"key": "mou2016refined", "year": "2017", "citations": "1", "title": "A Refined Analysis Of LSH For Well-dispersed Data Points", "abstract": "<p>Near neighbor problems are fundamental in algorithms for high-dimensional\nEuclidean spaces. While classical approaches suffer from the curse of\ndimensionality, locality sensitive hashing (LSH) can effectively solve\na-approximate r-near neighbor problem, and has been proven to be optimal in the\nworst case. However, for real-world data sets, LSH can naturally benefit from\nwell-dispersed data and low doubling dimension, leading to significantly\nimproved performance. In this paper, we address this issue and propose a\nrefined analyses for running time of approximating near neighbors queries via\nLSH. We characterize dispersion of data using N_b, the number of b*r-near pairs\namong the data points. Combined with optimal data-oblivious LSH scheme, we get\na new query time bound depending on N_b and doubling dimension. For many\nnatural scenarios where points are well-dispersed or lying in a\nlow-doubling-dimension space, our result leads to sharper performance than\nexisting worst-case analysis. This paper not only present first rigorous proof\non how LSHs make use of the structure of data points, but also provide\nimportant insights into parameter setting in the practice of LSH beyond worst\ncase. Besides, the techniques in our analysis involve a generalized version of\nsphere packing problem, which might be of some independent interest.</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [18.095699310302734, 39.98577117919922], "cluster": 4}, {"key": "moulton2018maximally", "year": "2018", "citations": "15", "title": "Maximally Consistent Sampling And The Jaccard Index Of Probability Distributions", "abstract": "<p>We introduce simple, efficient algorithms for computing a MinHash of a\nprobability distribution, suitable for both sparse and dense data, with\nequivalent running times to the state of the art for both cases. The collision\nprobability of these algorithms is a new measure of the similarity of positive\nvectors which we investigate in detail. We describe the sense in which this\ncollision probability is optimal for any Locality Sensitive Hash based on\nsampling. We argue that this similarity measure is more useful for probability\ndistributions than the similarity pursued by other algorithms for weighted\nMinHash, and is the natural generalization of the Jaccard index.</p>\n", "tags": ["Locality-Sensitive-Hashing"], "tsne_embedding": [5.704324722290039, 37.5392951965332], "cluster": 4}, {"key": "mu2016deep", "year": "2017", "citations": "9", "title": "Deep Hashing: A Joint Approach For Image Signature Learning", "abstract": "<p>Similarity-based image hashing represents crucial technique for visual data\nstorage reduction and expedited image search. Conventional hashing schemes\ntypically feed hand-crafted features into hash functions, which separates the\nprocedures of feature extraction and hash function learning. In this paper, we\npropose a novel algorithm that concurrently performs feature engineering and\nnon-linear supervised hashing function learning. Our technical contributions in\nthis paper are two-folds: 1) deep network optimization is often achieved by\ngradient propagation, which critically requires a smooth objective function.\nThe discrete nature of hash codes makes them not amenable for gradient-based\noptimization. To address this issue, we propose an exponentiated hashing loss\nfunction and its bilinear smooth approximation. Effective gradient calculation\nand propagation are thereby enabled; 2) pre-training is an important trick in\nsupervised deep learning. The impact of pre-training on the hash code quality\nhas never been discussed in current deep hashing literature. We propose a\npre-training scheme inspired by recent advance in deep network based image\nclassification, and experimentally demonstrate its effectiveness. Comprehensive\nquantitative evaluations are conducted on several widely-used image benchmarks.\nOn all benchmarks, our proposed deep hashing algorithm outperforms all\nstate-of-the-art competitors by significant margins. In particular, our\nalgorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy\nwith only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In\ncomparison, the best accuracies obtained on CIFAR10 by existing hashing\nalgorithms without or with deep networks are known to be 0.36 and 0.58\nrespectively.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "AAAI", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [0.3311263918876648, 11.085578918457031], "cluster": 8}, {"key": "mu2018towards", "year": "2018", "citations": "11", "title": "Towards Practical Visual Search Engine Within Elasticsearch", "abstract": "<p>In this paper, we describe our end-to-end content-based image retrieval\nsystem built upon Elasticsearch, a well-known and popular textual search\nengine. As far as we know, this is the first time such a system has been\nimplemented in eCommerce, and our efforts have turned out to be highly\nworthwhile. We end up with a novel and exciting visual search solution that is\nextremely easy to be deployed, distributed, scaled and monitored in a\ncost-friendly manner. Moreover, our platform is intrinsically flexible in\nsupporting multimodal searches, where visual and textual information can be\njointly leveraged in retrieval.\n  The core idea is to encode image feature vectors into a collection of string\ntokens in a way such that closer vectors will share more string tokens in\ncommon. By doing that, we can utilize Elasticsearch to efficiently retrieve\nsimilar images based on similarities within encoded sting tokens. As part of\nthe development, we propose a novel vector to string encoding method, which is\nshown to substantially outperform the previous ones in terms of both precision\nand latency.\n  First-hand experiences in implementing this Elasticsearch-based platform are\nextensively addressed, which should be valuable to practitioners also\ninterested in building visual search engine on top of Elasticsearch.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-14.692602157592773, -0.4394131898880005], "cluster": 1}, {"key": "mu2019empirical", "year": "2019", "citations": "3", "title": "An Empirical Comparison Of FAISS And FENSHSES For Nearest Neighbor Search In Hamming Space", "abstract": "<p>In this paper, we compare the performances of FAISS and FENSHSES on nearest\nneighbor search in Hamming space\u2013a fundamental task with ubiquitous\napplications in nowadays eCommerce. Comprehensive evaluations are made in terms\nof indexing speed, search latency and RAM consumption. This comparison is\nconducted towards a better understanding on trade-offs between nearest neighbor\nsearch systems implemented in main memory and the ones implemented in secondary\nmemory, which is largely unaddressed in literature.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation"], "tsne_embedding": [37.6054573059082, 14.935016632080078], "cluster": 2}, {"key": "mu2019fast", "year": "2019", "citations": "4", "title": "Fast And Exact Nearest Neighbor Search In Hamming Space On Full-text Search Engines", "abstract": "<p>A growing interest has been witnessed recently from both academia and\nindustry in building nearest neighbor search (NNS) solutions on top of\nfull-text search engines. Compared with other NNS systems, such solutions are\ncapable of effectively reducing main memory consumption, coherently supporting\nmulti-model search and being immediately ready for production deployment. In\nthis paper, we continue the journey to explore specifically how to empower\nfull-text search engines with fast and exact NNS in Hamming space (i.e., the\nset of binary codes). By revisiting three techniques (bit operation, subs-code\nfiltering and data preprocessing with permutation) in information retrieval\nliterature, we develop a novel engineering solution for full-text search\nengines to efficiently accomplish this special but important NNS task. In the\nexperiment, we show that our proposed approach enables full-text search engines\nto achieve significant speed-ups over its state-of-the-art term match approach\nfor NNS within binary codes.</p>\n", "tags": ["Compact-Codes", "Text-Retrieval"], "tsne_embedding": [10.117172241210938, -31.472095489501953], "cluster": 7}, {"key": "mukherjee2015nmf", "year": "2015", "citations": "14", "title": "An NMF Perspective On Binary Hashing", "abstract": "<p>The pervasiveness of massive data repositories has led\nto much interest in efficient methods for indexing, search,\nand retrieval. For image data, a rapidly developing body of\nwork for these applications shows impressive performance\nwith methods that broadly fall under the umbrella term of\nBinary Hashing. Given a distance matrix, a binary hashing\nalgorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the\noriginal distances. The formulation is non-convex \u2014 so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective\nthat is numerically more tractable. In this paper, we first\nderive an Augmented Lagrangian approach to optimize the\nstandard binary Hashing objective (i.e., maintain fidelity\nwith a given distance matrix). With appropriate step sizes,\nwe find that this scheme already yields results that match or\nsubstantially outperform state of the art methods on most\nbenchmarks used in the literature. Then, to allow the model\nto scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm \u2014 whose parallelization properties\nare exploited to obtain a fast GPU based implementation.\nWe give a probabilistic analysis of our initialization scheme\nand present a range of experiments to show that the method\nis simple to implement and competes favorably with available methods (both for optimization and generalization).</p>\n", "tags": ["ICCV", "Hashing-Methods", "Quantization", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-3.367530584335327, 35.542057037353516], "cluster": 8}, {"key": "mukherjee2025nmf", "year": "2015", "citations": "14", "title": "An NMF Perspective On Binary Hashing", "abstract": "<p>The pervasiveness of massive data repositories has led\nto much interest in efficient methods for indexing, search,\nand retrieval. For image data, a rapidly developing body of\nwork for these applications shows impressive performance\nwith methods that broadly fall under the umbrella term of\nBinary Hashing. Given a distance matrix, a binary hashing\nalgorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the\noriginal distances. The formulation is non-convex \u2014 so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective\nthat is numerically more tractable. In this paper, we first\nderive an Augmented Lagrangian approach to optimize the\nstandard binary Hashing objective (i.e., maintain fidelity\nwith a given distance matrix). With appropriate step sizes,\nwe find that this scheme already yields results that match or\nsubstantially outperform state of the art methods on most\nbenchmarks used in the literature. Then, to allow the model\nto scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm \u2014 whose parallelization properties\nare exploited to obtain a fast GPU based implementation.\nWe give a probabilistic analysis of our initialization scheme\nand present a range of experiments to show that the method\nis simple to implement and competes favorably with available methods (both for optimization and generalization).</p>\n", "tags": ["ICCV", "Hashing-Methods", "Quantization", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-3.367530584335327, 35.542057037353516], "cluster": 8}, {"key": "murray2016interferences", "year": "2016", "citations": "30", "title": "Interferences In Match Kernels", "abstract": "<p>We consider the design of an image representation that embeds and aggregates\na set of local descriptors into a single vector. Popular representations of\nthis kind include the bag-of-visual-words, the Fisher vector and the VLAD. When\ntwo such image representations are compared with the dot-product, the\nimage-to-image similarity can be interpreted as a match kernel. In match\nkernels, one has to deal with interference, i.e. with the fact that even if two\ndescriptors are unrelated, their matching score may contribute to the overall\nsimilarity.\n  We formalise this problem and propose two related solutions, both aimed at\nequalising the individual contributions of the local descriptors in the final\nrepresentation. These methods modify the aggregation stage by including a set\nof per-descriptor weights. They differ by the objective function that is\noptimised to compute those weights. The first is a \u201cdemocratisation\u201d strategy\nthat aims at equalising the relative importance of each descriptor in the set\ncomparison metric. The second one involves equalising the match of a single\ndescriptor to the aggregated vector.\n  These concurrent methods give a substantial performance boost over the state\nof the art in image search with short or mid-size vectors, as demonstrated by\nour experiments on standard public image retrieval benchmarks.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-24.316513061523438, 11.349161148071289], "cluster": 0}, {"key": "nan2025revisiting", "year": "2025", "citations": "0", "title": "Revisiting Medical Image Retrieval Via Knowledge Consolidation", "abstract": "<p>As artificial intelligence and digital medicine increasingly permeate healthcare systems, robust governance frameworks are essential to ensure ethical, secure, and effective implementation. In this context, medical image retrieval becomes a critical component of clinical data management, playing a vital role in decision-making and safeguarding patient information. Existing methods usually learn hash functions using bottleneck features, which fail to produce representative hash codes from blended embeddings. Although contrastive hashing has shown superior performance, current approaches often treat image retrieval as a classification task, using category labels to create positive/negative pairs. Moreover, many methods fail to address the out-of-distribution (OOD) issue when models encounter external OOD queries or adversarial attacks. In this work, we propose a novel method to consolidate knowledge of hierarchical features and optimisation functions. We formulate the knowledge consolidation by introducing Depth-aware Representation Fusion (DaRF) and Structure-aware Contrastive Hashing (SCH). DaRF adaptively integrates shallow and deep representations into blended features, and SCH incorporates image fingerprints to enhance the adaptability of positive/negative pairings. These blended features further facilitate OOD detection and content-based recommendation, contributing to a secure AI-driven healthcare environment. Moreover, we present a content-guided ranking to improve the robustness and reproducibility of retrieval results. Our comprehensive assessments demonstrate that the proposed method could effectively recognise OOD samples and significantly outperform existing approaches in medical image retrieval (p&lt;0.05). In particular, our method achieves a 5.6-38.9% improvement in mean Average Precision on the anatomical radiology dataset.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Recommender-Systems", "Image-Retrieval", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-50.481388092041016, 17.751440048217773], "cluster": 0}, {"key": "nara2024revisiting", "year": "2025", "citations": "0", "title": "Revisiting Relevance Feedback For Clip-based Interactive Image Retrieval", "abstract": "<p>Many image retrieval studies use metric learning to train an image encoder.\nHowever, metric learning cannot handle differences in users\u2019 preferences, and\nrequires data to train an image encoder. To overcome these limitations, we\nrevisit relevance feedback, a classic technique for interactive retrieval\nsystems, and propose an interactive CLIP-based image retrieval system with\nrelevance feedback. Our retrieval system first executes the retrieval, collects\neach user\u2019s unique preferences through binary feedback, and returns images the\nuser prefers. Even when users have various preferences, our retrieval system\nlearns each user\u2019s preference through the feedback and adapts to the\npreference. Moreover, our retrieval system leverages CLIP\u2019s zero-shot\ntransferability and achieves high accuracy without training. We empirically\nshow that our retrieval system competes well with state-of-the-art metric\nlearning in category-based image retrieval, despite not training image encoders\nspecifically for each dataset. Furthermore, we set up two additional\nexperimental settings where users have various preferences: one-label-based\nimage retrieval and conditioned image retrieval. In both cases, our retrieval\nsystem effectively adapts to each user\u2019s preferences, resulting in improved\naccuracy compared to image retrieval without feedback. Overall, our work\nhighlights the potential benefits of integrating CLIP with classic relevance\nfeedback techniques to enhance image retrieval.</p>\n", "tags": ["Image-Retrieval", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-30.04204559326172, -7.3023271560668945], "cluster": 5}, {"key": "narayana2019huse", "year": "2019", "citations": "7", "title": "HUSE: Hierarchical Universal Semantic Embeddings", "abstract": "<p>There is a recent surge of interest in cross-modal representation learning\ncorresponding to images and text. The main challenge lies in mapping images and\ntext to a shared latent space where the embeddings corresponding to a similar\nsemantic concept lie closer to each other than the embeddings corresponding to\ndifferent semantic concepts, irrespective of the modality. Ranking losses are\ncommonly used to create such shared latent space \u2013 however, they do not impose\nany constraints on inter-class relationships resulting in neighboring clusters\nto be completely unrelated. The works in the domain of visual semantic\nembeddings address this problem by first constructing a semantic embedding\nspace based on some external knowledge and projecting image embeddings onto\nthis fixed semantic embedding space. These works are confined only to image\ndomain and constraining the embeddings to a fixed space adds additional burden\non learning. This paper proposes a novel method, HUSE, to learn cross-modal\nrepresentation with semantic information. HUSE learns a shared latent space\nwhere the distance between any two universal embeddings is similar to the\ndistance between their corresponding class embeddings in the semantic embedding\nspace. HUSE also uses a classification objective with a shared classification\nlayer to make sure that the image and text embeddings are in the same shared\nlatent space. Experiments on UPMC Food-101 show our method outperforms previous\nstate-of-the-art on retrieval, hierarchical precision and classification\nresults.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-33.829593658447266, -5.423446178436279], "cluster": 0}, {"key": "nardini2024efficient", "year": "2024", "citations": "0", "title": "Efficient Multi-vector Dense Retrieval Using Bit Vectors", "abstract": "<p>Dense retrieval techniques employ pre-trained large language models to build\na high-dimensional representation of queries and passages. These\nrepresentations compute the relevance of a passage w.r.t. to a query using\nefficient similarity measures. In this line, multi-vector representations show\nimproved effectiveness at the expense of a one-order-of-magnitude increase in\nmemory footprint and query latency by encoding queries and documents on a\nper-token level. Recently, PLAID has tackled these problems by introducing a\ncentroid-based term representation to reduce the memory impact of multi-vector\nsystems. By exploiting a centroid interaction mechanism, PLAID filters out\nnon-relevant documents, thus reducing the cost of the successive ranking\nstages. This paper proposes ``Efficient Multi-Vector dense retrieval with Bit\nvectors\u2019\u2019 (EMVB), a novel framework for efficient query processing in\nmulti-vector dense retrieval. First, EMVB employs a highly efficient\npre-filtering step of passages using optimized bit vectors. Second, the\ncomputation of the centroid interaction happens column-wise, exploiting SIMD\ninstructions, thus reducing its latency. Third, EMVB leverages Product\nQuantization (PQ) to reduce the memory footprint of storing vector\nrepresentations while jointly allowing for fast late interaction. Fourth, we\nintroduce a per-document term filtering method that further improves the\nefficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVB\nis up to 2.8x faster while reducing the memory footprint by 1.8x with no loss\nin retrieval accuracy compared to PLAID.</p>\n", "tags": ["Efficiency", "Quantization", "Memory-Efficiency", "Tools-&-Libraries"], "tsne_embedding": [17.09797477722168, 19.716590881347656], "cluster": 2}, {"key": "nath2022identical", "year": "2022", "citations": "0", "title": "Identical Image Retrieval Using Deep Learning", "abstract": "<p>In recent years, we know that the interaction with images has increased.\nImage similarity involves fetching similar-looking images abiding by a given\nreference image. The target is to find out whether the image searched as a\nquery can result in similar pictures. We are using the BigTransfer Model, which\nis a state-of-art model itself. BigTransfer(BiT) is essentially a ResNet but\npre-trained on a larger dataset like ImageNet and ImageNet-21k with additional\nmodifications. Using the fine-tuned pre-trained Convolution Neural Network\nModel, we extract the key features and train on the K-Nearest Neighbor model to\nobtain the nearest neighbor. The application of our model is to find similar\nimages, which are hard to achieve through text queries within a low inference\ntime. We analyse the benchmark of our model based on this application.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-14.258743286132812, -7.188994407653809], "cluster": 1}, {"key": "nawaz2018revisiting", "year": "2018", "citations": "5", "title": "Revisiting Cross Modal Retrieval", "abstract": "<p>This paper proposes a cross-modal retrieval system that leverages on image\nand text encoding. Most multimodal architectures employ separate networks for\neach modality to capture the semantic relationship between them. However, in\nour work image-text encoding can achieve comparable results in terms of\ncross-modal retrieval without having to use a separate network for each\nmodality. We show that text encodings can capture semantic relationships\nbetween multiple modalities. In our knowledge, this work is the first of its\nkind in terms of employing a single network and fused image-text embedding for\ncross-modal retrieval. We evaluate our approach on two famous multimodal\ndatasets: MS-COCO and Flickr30K.</p>\n", "tags": ["Multimodal-Retrieval", "Datasets"], "tsne_embedding": [-9.586668968200684, -11.818466186523438], "cluster": 1}, {"key": "nawaz2019do", "year": "2019", "citations": "9", "title": "Do Cross Modal Systems Leverage Semantic Relationships?", "abstract": "<p>Current cross-modal retrieval systems are evaluated using R@K measure which\ndoes not leverage semantic relationships rather strictly follows the manually\nmarked image text query pairs. Therefore, current systems do not generalize\nwell for the unseen data in the wild. To handle this, we propose a new measure,\nSemanticMap, to evaluate the performance of cross-modal systems. Our proposed\nmeasure evaluates the semantic similarity between the image and text\nrepresentations in the latent embedding space. We also propose a novel\ncross-modal retrieval system using a single stream network for bidirectional\nretrieval. The proposed system is based on a deep neural network trained using\nextended center loss, minimizing the distance of image and text descriptions in\nthe latent space from the class centers. In our system, the text descriptions\nare also encoded as images which enabled us to use a single stream network for\nboth text and images. To the best of our knowledge, our work is the first of\nits kind in terms of employing a single stream network for cross-modal\nretrieval systems. The proposed system is evaluated on two publicly available\ndatasets including MSCOCO and Flickr30K and has shown comparable results to the\ncurrent state-of-the-art methods.</p>\n", "tags": ["ICCV", "Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [-27.962318420410156, 0.21925197541713715], "cluster": 0}, {"key": "ndungu2023deep", "year": "2023", "citations": "1", "title": "Deep Supervised Hashing For Fast Retrieval Of Radio Image Cubes", "abstract": "<p>The shear number of sources that will be detected by next-generation radio\nsurveys will be astronomical, which will result in serendipitous discoveries.\nData-dependent deep hashing algorithms have been shown to be efficient at image\nretrieval tasks in the fields of computer vision and multimedia. However, there\nare limited applications of these methodologies in the field of astronomy. In\nthis work, we utilize deep hashing to rapidly search for similar images in a\nlarge database. The experiment uses a balanced dataset of 2708 samples\nconsisting of four classes: Compact, FRI, FRII, and Bent. The performance of\nthe method was evaluated using the mean average precision (mAP) metric where a\nprecision of 88.5% was achieved. The experimental results demonstrate the\ncapability to search and retrieve similar radio images efficiently and at\nscale. The retrieval is based on the Hamming distance between the binary hash\nof the query image and those of the reference images in the database.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-2.685704469680786, 39.90811538696289], "cluster": 4}, {"key": "neculai2022probabilistic", "year": "2022", "citations": "21", "title": "Probabilistic Compositional Embeddings For Multimodal Image Retrieval", "abstract": "<p>Existing works in image retrieval often consider retrieving images with one\nor two query inputs, which do not generalize to multiple queries. In this work,\nwe investigate a more challenging scenario for composing multiple multimodal\nqueries in image retrieval. Given an arbitrary number of query images and (or)\ntexts, our goal is to retrieve target images containing the semantic concepts\nspecified in multiple multimodal queries. To learn an informative embedding\nthat can flexibly encode the semantics of various queries, we propose a novel\nmultimodal probabilistic composer (MPC). Specifically, we model input images\nand texts as probabilistic embeddings, which can be further composed by a\nprobabilistic composition rule to facilitate image retrieval with multiple\nmultimodal queries. We propose a new benchmark based on the MS-COCO dataset and\nevaluate our model on various setups that compose multiple images and (or) text\nqueries for multimodal image retrieval. Without bells and whistles, we show\nthat our probabilistic model formulation significantly outperforms existing\nrelated methods on multimodal image retrieval while generalizing well to query\nwith different amounts of inputs given in arbitrary visual and (or) textual\nmodalities. Code is available here: https://github.com/andreineculai/MPC.</p>\n", "tags": ["CVPR", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-23.926488876342773, -29.212759017944336], "cluster": 5}, {"key": "nedelec2017specializing", "year": "2017", "citations": "17", "title": "Specializing Joint Representations For The Task Of Product Recommendation", "abstract": "<p>We propose a unified product embedded representation that is optimized for\nthe task of retrieval-based product recommendation. To this end, we introduce a\nnew way to fuse modality-specific product embeddings into a joint product\nembedding, in order to leverage both product content information, such as\ntextual descriptions and images, and product collaborative filtering signal. By\nintroducing the fusion step at the very end of our architecture, we are able to\ntrain each modality separately, allowing us to keep a modular architecture that\nis preferable in real-world recommendation deployments. We analyze our\nperformance on normal and hard recommendation setups such as cold-start and\ncross-category recommendations and achieve good performance on a large product\nshopping dataset.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [-15.049184799194336, -45.786155700683594], "cluster": 3}, {"key": "neelakantan2022text", "year": "2022", "citations": "110", "title": "Text And Code Embeddings By Contrastive Pre-training", "abstract": "<p>Text embeddings are useful features in many applications such as semantic\nsearch and computing text similarity. Previous work typically trains models\ncustomized for different use cases, varying in dataset choice, training\nobjective and model architecture. In this work, we show that contrastive\npre-training on unsupervised data at scale leads to high quality vector\nrepresentations of text and code. The same unsupervised text embeddings that\nachieve new state-of-the-art results in linear-probe classification also\ndisplay impressive semantic search capabilities and sometimes even perform\ncompetitively with fine-tuned models. On linear-probe classification accuracy\naveraging over 7 tasks, our best unsupervised model achieves a relative\nimprovement of 4% and 1.8% over previous best unsupervised and supervised text\nembedding models respectively. The same text embeddings when evaluated on\nlarge-scale semantic search attains a relative improvement of 23.4%, 14.7%, and\n10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and\nTriviaQA benchmarks, respectively. Similarly to text embeddings, we train code\nembedding models on (text, code) pairs, obtaining a 20.8% relative improvement\nover prior best work on code search.</p>\n", "tags": ["Supervised", "Unsupervised", "Scalability", "Datasets"], "tsne_embedding": [-24.6629581451416, -17.963300704956055], "cluster": 5}, {"key": "neto2024predictive", "year": "2024", "citations": "0", "title": "Predictive Query-based Pipeline For Graph Data", "abstract": "<p>Graphs face challenges when dealing with massive datasets. They are essential\ntools for modeling interconnected data and often become computationally\nexpensive. Graph embedding techniques, on the other hand, provide an efficient\napproach. By projecting complex graphs into a lower-dimensional space, these\ntechniques simplify the analysis and processing of large-scale graphs. By\ntransforming graphs into vectors, it simplifies the analysis and processing of\nlarge-scale datasets. Several approaches, such as GraphSAGE, Node2Vec, and\nFastRP, offer efficient methods for generating graph embeddings. By storing\nembeddings as node properties, it is possible to compare different embedding\ntechniques and evaluate their effectiveness for specific tasks. This\nflexibilityallows for dynamic updates to embeddings and facilitates\nexperimentation with different approaches. By analyzing these embeddings, one\ncan extract valuable insights into the relationships between nodes and their\nsimilarities within the embedding space</p>\n", "tags": ["Scalability", "Datasets"], "tsne_embedding": [56.587589263916016, -0.09558771550655365], "cluster": 9}, {"key": "neyshabur2013power", "year": "2013", "citations": "57", "title": "The Power Of Asymmetry In Binary Hashing", "abstract": "<p>When approximating binary similarity using the hamming distance between short\nbinary hashes, we show that even if the similarity is symmetric, we can have\nshorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\n0\nas the hamming distance between f(x)\nand g(x0), for two distinct binary codes f, g, rather than as the hamming distance\nbetween f(x) and f(x0).</p>\n", "tags": ["Compact-Codes", "Hashing-Methods"], "tsne_embedding": [-9.123091697692871, 41.69248580932617], "cluster": 8}, {"key": "neyshabur2025power", "year": "2013", "citations": "57", "title": "The Power Of Asymmetry In Binary Hashing", "abstract": "<p>When approximating binary similarity using the hamming distance between short\nbinary hashes, we show that even if the similarity is symmetric, we can have\nshorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\n0\nas the hamming distance between f(x)\nand g(x0), for two distinct binary codes f, g, rather than as the hamming distance\nbetween f(x) and f(x0).</p>\n", "tags": ["Compact-Codes", "Hashing-Methods"], "tsne_embedding": [-9.123089790344238, 41.69248962402344], "cluster": 8}, {"key": "ng2020solar", "year": "2020", "citations": "98", "title": "SOLAR: Second-order Loss And Attention For Image Retrieval", "abstract": "<p>Recent works in deep-learning have shown that second-order information is\nbeneficial in many computer-vision tasks. Second-order information can be\nenforced both in the spatial context and the abstract feature dimensions. In\nthis work, we explore two second-order components. One is focused on\nsecond-order spatial information to increase the performance of image\ndescriptors, both local and global. It is used to re-weight feature maps, and\nthus emphasise salient image locations that are subsequently used for\ndescription. The second component is concerned with a second-order similarity\n(SOS) loss, that we extend to global descriptors for image retrieval, and is\nused to enhance the triplet loss with hard-negative mining. We validate our\napproach on two different tasks and datasets for image retrieval and image\nmatching. The results show that our two second-order components complement each\nother, bringing significant performance improvements in both tasks and lead to\nstate-of-the-art results across the public benchmarks. Code available at:\nhttp://github.com/tonyngjichun/SOLAR</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-36.16535186767578, 7.222583293914795], "cluster": 0}, {"key": "ng2023unsupervised", "year": "2023", "citations": "0", "title": "Unsupervised Hashing With Similarity Distribution Calibration", "abstract": "<p>Unsupervised hashing methods typically aim to preserve the similarity between data points in a feature space by mapping them to binary hash codes. However, these methods often overlook the fact that the similarity between data points in the continuous feature space may not be preserved in the discrete hash code space, due to the limited similarity range of hash codes. The similarity range is bounded by the code length and can lead to a problem known as similarity collapse. That is, the positive and negative pairs of data points become less distinguishable from each other in the hash space. To alleviate this problem, in this paper a novel Simialrity Distribution Calibration (SDC) method is introduced. SDC aligns the hash code similarity distribution towards a calibration distribution (e.g., beta distribution) with sufficient spread across the entire similarity range, thus alleviating the similarity collapse problem. Extensive experiments show that our SDC outperforms significantly the state-of-the-art alternatives on coarse category-level and instance-level image retrieval.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-11.551097869873047, 34.39958953857422], "cluster": 8}, {"key": "ng2024concepthash", "year": "2024", "citations": "0", "title": "Concepthash: Interpretable Fine-grained Hashing Via Concept Discovery", "abstract": "<p>Existing fine-grained hashing methods typically lack code interpretability as\nthey compute hash code bits holistically using both global and local features.\nTo address this limitation, we propose ConceptHash, a novel method that\nachieves sub-code level interpretability. In ConceptHash, each sub-code\ncorresponds to a human-understandable concept, such as an object part, and\nthese concepts are automatically discovered without human annotations.\nSpecifically, we leverage a Vision Transformer architecture and introduce\nconcept tokens as visual prompts, along with image patch tokens as model\ninputs. Each concept is then mapped to a specific sub-code at the model output,\nproviding natural sub-code interpretability. To capture subtle visual\ndifferences among highly similar sub-categories (e.g., bird species), we\nincorporate language guidance to ensure that the learned hash codes are\ndistinguishable within fine-grained object classes while maintaining semantic\nalignment. This approach allows us to develop hash codes that exhibit\nsimilarity within families of species while remaining distinct from species in\nother families. Extensive experiments on four fine-grained image retrieval\nbenchmarks demonstrate that ConceptHash outperforms previous methods by a\nsignificant margin, offering unique sub-code interpretability as an additional\nbenefit. Code at: https://github.com/kamwoh/concepthash.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [2.4393718242645264, -6.520247459411621], "cluster": 6}, {"key": "ng2025unsupervised", "year": "2023", "citations": "0", "title": "Unsupervised Hashing With Similarity Distribution Calibration", "abstract": "<p>Unsupervised hashing methods typically aim to preserve the similarity between data points in a feature space by mapping them to binary hash codes. However, these methods often overlook the fact that the similarity between data points in the continuous feature space may not be preserved in the discrete hash code space, due to the limited similarity range of hash codes. The similarity range is bounded by the code length and can lead to a problem known as similarity collapse. That is, the positive and negative pairs of data points become less distinguishable from each other in the hash space. To alleviate this problem, in this paper a novel Simialrity Distribution Calibration (SDC) method is introduced. SDC aligns the hash code similarity distribution towards a calibration distribution (e.g., beta distribution) with sufficient spread across the entire similarity range, thus alleviating the similarity collapse problem. Extensive experiments show that our SDC outperforms significantly the state-of-the-art alternatives on coarse category-level and instance-level image retrieval.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-11.551097869873047, 34.39958953857422], "cluster": 8}, {"key": "nguyen2020deep", "year": "2020", "citations": "9", "title": "Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised Approach For Feature Embedding", "abstract": "<p>Unsupervised Deep Distance Metric Learning (UDML) aims to learn sample\nsimilarities in the embedding space from an unlabeled dataset. Traditional UDML\nmethods usually use the triplet loss or pairwise loss which requires the mining\nof positive and negative samples w.r.t. anchor data points. This is, however,\nchallenging in an unsupervised setting as the label information is not\navailable. In this paper, we propose a new UDML method that overcomes that\nchallenge. In particular, we propose to use a deep clustering loss to learn\ncentroids, i.e., pseudo labels, that represent semantic classes. During\nlearning, these centroids are also used to reconstruct the input samples. It\nhence ensures the representativeness of centroids - each centroid represents\nvisually similar samples. Therefore, the centroids give information about\npositive (visually similar) and negative (visually dissimilar) samples. Based\non pseudo labels, we propose a novel unsupervised metric loss which enforces\nthe positive concentration and negative separation of samples in the embedding\nspace. Experimental results on benchmarking datasets show that the proposed\napproach outperforms other UDML methods.</p>\n", "tags": ["Distance-Metric-Learning", "Unsupervised", "Datasets"], "tsne_embedding": [9.181170463562012, -3.9687976837158203], "cluster": 6}, {"key": "nguyen2020multiple", "year": "2021", "citations": "1", "title": "Multiple Visual-semantic Embedding For Video Retrieval From Query Sentence", "abstract": "<p>Visual-semantic embedding aims to learn a joint embedding space where related\nvideo and sentence instances are located close to each other. Most existing\nmethods put instances in a single embedding space. However, they struggle to\nembed instances due to the difficulty of matching visual dynamics in videos to\ntextual features in sentences. A single space is not enough to accommodate\nvarious videos and sentences. In this paper, we propose a novel framework that\nmaps instances into multiple individual embedding spaces so that we can capture\nmultiple relationships between instances, leading to compelling video\nretrieval. We propose to produce a final similarity between instances by fusing\nsimilarities measured in each embedding space using a weighted sum strategy. We\ndetermine the weights according to a sentence. Therefore, we can flexibly\nemphasize an embedding space. We conducted sentence-to-video retrieval\nexperiments on a benchmark dataset. The proposed method achieved superior\nperformance, and the results are competitive to state-of-the-art methods. These\nexperimental results demonstrated the effectiveness of the proposed multiple\nembedding approach compared to existing methods.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Video-Retrieval", "Datasets"], "tsne_embedding": [-39.60652542114258, -25.954519271850586], "cluster": 5}, {"key": "nguyen2021deep", "year": "2021", "citations": "24", "title": "A Deep Local And Global Scene-graph Matching For Image-text Retrieval", "abstract": "<p>Conventional approaches to image-text retrieval mainly focus on indexing\nvisual objects appearing in pictures but ignore the interactions between these\nobjects. Such objects occurrences and interactions are equivalently useful and\nimportant in this field as they are usually mentioned in the text. Scene graph\npresentation is a suitable method for the image-text matching challenge and\nobtained good results due to its ability to capture the inter-relationship\ninformation. Both images and text are represented in scene graph levels and\nformulate the retrieval challenge as a scene graph matching challenge. In this\npaper, we introduce the Local and Global Scene Graph Matching (LGSGM) model\nthat enhances the state-of-the-art method by integrating an extra graph\nconvolution network to capture the general information of a graph.\nSpecifically, for a pair of scene graphs of an image and its caption, two\nseparate models are used to learn the features of each graph\u2019s nodes and edges.\nThen a Siamese-structure graph convolution model is employed to embed graphs\ninto vector forms. We finally combine the graph-level and the vector-level to\ncalculate the similarity of this image-text pair. The empirical experiments\nshow that our enhancement with the combination of levels can improve the\nperformance of the baseline method by increasing the recall by more than 10% on\nthe Flickr30k dataset.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [55.44860076904297, -8.391026496887207], "cluster": 9}, {"key": "nguyen2021oscar", "year": "2021", "citations": "14", "title": "Oscar-net: Object-centric Scene Graph Attention For Image Attribution", "abstract": "<p>Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject\u2019s visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.</p>\n", "tags": ["Self-Supervised", "ICCV", "Hashing-Methods", "Image-Retrieval", "Datasets"], "tsne_embedding": [-26.957050323486328, 4.714121341705322], "cluster": 0}, {"key": "nguyen2023generative", "year": "2023", "citations": "3", "title": "Generative Retrieval As Dense Retrieval", "abstract": "<p>Generative retrieval is a promising new neural retrieval paradigm that aims\nto optimize the retrieval pipeline by performing both indexing and retrieval\nwith a single transformer model. However, this new paradigm faces challenges\nwith updating the index and scaling to large collections. In this paper, we\nanalyze two prominent variants of generative retrieval and show that they can\nbe conceptually viewed as bi-encoders for dense retrieval. Specifically, we\nanalytically demonstrate that the generative retrieval process can be\ndecomposed into dot products between query and document vectors, similar to\ndense retrieval. This analysis leads us to propose a new variant of generative\nretrieval, called Tied-Atomic, which addresses the updating and scaling issues\nby incorporating techniques from dense retrieval. In experiments on two\ndatasets, NQ320k and the full MSMARCO, we confirm that this approach does not\nreduce retrieval effectiveness while enabling the model to scale to large\ncollections.</p>\n", "tags": ["SIGIR", "Datasets"], "tsne_embedding": [31.313640594482422, -20.85407257080078], "cluster": 7}, {"key": "nguyen2024dyvo", "year": "2024", "citations": "0", "title": "Dyvo: Dynamic Vocabularies For Learned Sparse Retrieval With Entities", "abstract": "<p>Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained\ntransformers, which often split entities into nonsensical fragments. Splitting\nentities can reduce retrieval accuracy and limits the model\u2019s ability to\nincorporate up-to-date world knowledge not included in the training data. In\nthis work, we enhance the LSR vocabulary with Wikipedia concepts and entities,\nenabling the model to resolve ambiguities more effectively and stay current\nwith evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo)\nhead, which leverages existing entity embeddings and an entity retrieval\ncomponent that identifies entities relevant to a query or document. We use the\nDyVo head to generate entity weights, which are then merged with word piece\nweights to create joint representations for efficient indexing and retrieval\nusing an inverted index. In experiments across three entity-rich document\nranking datasets, the resulting DyVo model substantially outperforms\nstate-of-the-art baselines.</p>\n", "tags": ["EMNLP", "Datasets"], "tsne_embedding": [4.650125503540039, -30.619657516479492], "cluster": 3}, {"key": "nguyen2024multimodal", "year": "2024", "citations": "0", "title": "Multimodal Learned Sparse Retrieval With Probabilistic Expansion Control", "abstract": "<p>Learned sparse retrieval (LSR) is a family of neural methods that encode\nqueries and documents into sparse lexical vectors that can be indexed and\nretrieved efficiently with an inverted index. We explore the application of LSR\nto the multi-modal domain, with a focus on text-image retrieval. While LSR has\nseen success in text retrieval, its application in multimodal retrieval remains\nunderexplored. Current approaches like LexLIP and STAIR require complex\nmulti-step training on massive datasets. Our proposed approach efficiently\ntransforms dense vectors from a frozen dense model into sparse lexical vectors.\nWe address issues of high dimension co-activation and semantic deviation\nthrough a new training algorithm, using Bernoulli random variables to control\nquery expansion. Experiments with two dense models (BLIP, ALBEF) and two\ndatasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively\nreduces co-activation and semantic deviation. Our best-performing sparsified\nmodel outperforms state-of-the-art text-image LSR models with a shorter\ntraining time and lower GPU memory requirements. Our approach offers an\neffective solution for training LSR retrieval models in multimodal settings.\nOur code and model checkpoints are available at\ngithub.com/thongnt99/lsr-multimodal</p>\n", "tags": ["Multimodal-Retrieval", "Image-Retrieval", "Text-Retrieval", "Datasets"], "tsne_embedding": [32.64140701293945, 12.08350944519043], "cluster": 2}, {"key": "ni2020layered", "year": "2020", "citations": "27", "title": "Layered Graph Embedding For Entity Recommendation Using Wikipedia In The Yahoo! Knowledge Graph", "abstract": "<p>In this paper, we describe an embedding-based entity recommendation framework\nfor Wikipedia that organizes Wikipedia into a collection of graphs layered on\ntop of each other, learns complementary entity representations from their\ntopology and content, and combines them with a lightweight learning-to-rank\napproach to recommend related entities on Wikipedia. Through offline and online\nevaluations, we show that the resulting embeddings and recommendations perform\nwell in terms of quality and user engagement. Balancing simplicity and quality,\nthis framework provides default entity recommendations for English and other\nlanguages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.</p>\n", "tags": ["Tools-&-Libraries", "Recommender-Systems"], "tsne_embedding": [55.48691940307617, -5.034360408782959], "cluster": 9}, {"key": "nikhal2023hashreid", "year": "2024", "citations": "1", "title": "Hashreid: Dynamic Network With Binary Codes For Efficient Person Re-identification", "abstract": "<p>Biometric applications, such as person re-identification (ReID), are often\ndeployed on energy constrained devices. While recent ReID methods prioritize\nhigh retrieval performance, they often come with large computational costs and\nhigh search time, rendering them less practical in real-world settings. In this\nwork, we propose an input-adaptive network with multiple exit blocks, that can\nterminate computation early if the retrieval is straightforward or noisy,\nsaving a lot of computation. To assess the complexity of the input, we\nintroduce a temporal-based classifier driven by a new training strategy.\nFurthermore, we adopt a binary hash code generation approach instead of relying\non continuous-valued features, which significantly improves the search process\nby a factor of 20. To ensure similarity preservation, we utilize a new ranking\nregularizer that bridges the gap between continuous and binary features.\nExtensive analysis of our proposed method is conducted on three datasets:\nMarket1501, MSMT17 (Multi-Scene Multi-Time), and the BGC1 (BRIAR Government\nCollection). Using our approach, more than 70% of the samples with compact hash\ncodes exit early on the Market1501 dataset, saving 80% of the networks\ncomputational cost and improving over other hash-based methods by 60%. These\nresults demonstrate a significant improvement over dynamic networks and\nshowcase comparable accuracy performance to conventional ReID methods. Code\nwill be made available.</p>\n", "tags": ["Compact-Codes", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [33.8211784362793, 8.606717109680176], "cluster": 2}, {"key": "ning2016scalable", "year": "2016", "citations": "40", "title": "Scalable Image Retrieval By Sparse Product Quantization", "abstract": "<p>Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional\nfeature indexing and retrieval is the crux of large-scale image retrieval. A\nrecent promising technique is Product Quantization, which attempts to index\nhigh-dimensional image features by decomposing the feature space into a\nCartesian product of low dimensional subspaces and quantizing each of them\nseparately. Despite the promising results reported, their quantization approach\nfollows the typical hard assignment of traditional quantization methods, which\nmay result in large quantization errors and thus inferior search performance.\nUnlike the existing approaches, in this paper, we propose a novel approach\ncalled Sparse Product Quantization (SPQ) to encoding the high-dimensional\nfeature vectors into sparse representation. We optimize the sparse\nrepresentations of the feature vectors by minimizing their quantization errors,\nmaking the resulting representation is essentially close to the original data\nin practice. Experiments show that the proposed SPQ technique is not only able\nto compress data, but also an effective encoding technique. We obtain\nstate-of-the-art results for ANN search on four public image datasets and the\npromising results of content-based image retrieval further validate the\nefficacy of our proposed method.</p>\n", "tags": ["Quantization", "Image-Retrieval", "Similarity-Search", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-4.062700271606445, 24.885581970214844], "cluster": 8}, {"key": "ning2023multi", "year": "2023", "citations": "11", "title": "Multi-domain Recommendation With Embedding Disentangling And Domain Alignment", "abstract": "<p>Multi-domain recommendation (MDR) aims to provide recommendations for\ndifferent domains (e.g., types of products) with overlapping users/items and is\ncommon for platforms such as Amazon, Facebook, and LinkedIn that host multiple\nservices. Existing MDR models face two challenges: First, it is difficult to\ndisentangle knowledge that generalizes across domains (e.g., a user likes cheap\nitems) and knowledge specific to a single domain (e.g., a user likes blue\nclothing but not blue cars). Second, they have limited ability to transfer\nknowledge across domains with small overlaps. We propose a new MDR method named\nEDDA with two key components, i.e., embedding disentangling recommender and\ndomain alignment, to tackle the two challenges respectively. In particular, the\nembedding disentangling recommender separates both the model and embedding for\nthe inter-domain part and the intra-domain part, while most existing MDR\nmethods only focus on model-level disentangling. The domain alignment leverages\nrandom walks from graph processing to identify similar user/item pairs from\ndifferent domains and encourages similar user/item pairs to have similar\nembeddings, enhancing knowledge transfer. We compare EDDA with 12\nstate-of-the-art baselines on 3 real datasets. The results show that EDDA\nconsistently outperforms the baselines on all datasets and domains. All\ndatasets and codes are available at https://github.com/Stevenn9981/EDDA.</p>\n", "tags": ["Recommender-Systems", "CIKM", "Datasets"], "tsne_embedding": [44.9154052734375, -4.365448474884033], "cluster": 9}, {"key": "noh2016large", "year": "2017", "citations": "812", "title": "Large-scale Image Retrieval With Attentive Deep Local Features", "abstract": "<p>We propose an attentive local feature descriptor suitable for large-scale\nimage retrieval, referred to as DELF (DEep Local Feature). The new feature is\nbased on convolutional neural networks, which are trained only with image-level\nannotations on a landmark image dataset. To identify semantically useful local\nfeatures for image retrieval, we also propose an attention mechanism for\nkeypoint selection, which shares most network layers with the descriptor. This\nframework can be used for image retrieval as a drop-in replacement for other\nkeypoint detectors and descriptors, enabling more accurate feature matching and\ngeometric verification. Our system produces reliable confidence scores to\nreject false positives\u2014in particular, it is robust against queries that have\nno correct match in the database. To evaluate the proposed descriptor, we\nintroduce a new large-scale dataset, referred to as Google-Landmarks dataset,\nwhich involves challenges in both database and query such as background\nclutter, partial occlusion, multiple landmarks, objects in variable scales,\netc. We show that DELF outperforms the state-of-the-art global and local\ndescriptors in the large-scale setting by significant margins. Code and dataset\ncan be found at the project webpage:\nhttps://github.com/tensorflow/models/tree/master/research/delf .</p>\n", "tags": ["ICCV", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-29.00845718383789, 1.8971294164657593], "cluster": 0}, {"key": "norouzi2012hamming", "year": "2012", "citations": "540", "title": "Hamming Distance Metric Learning", "abstract": "<p>Motivated by large-scale multimedia applications we propose to learn mappings\nfrom high-dimensional data to binary codes that preserve semantic similarity.\nBinary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable\nto broad families of mappings, and uses a flexible form of triplet ranking loss.\nWe overcome discontinuous optimization of the discrete mappings by minimizing\na piecewise-smooth upper bound on empirical loss, inspired by latent structural\nSVMs. We develop a new loss-augmented inference algorithm that is quadratic in\nthe code length. We show strong retrieval performance on CIFAR-10 and MNIST,\nwith promising classification results using no more than kNN on the binary codes.</p>\n", "tags": ["Distance-Metric-Learning", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [1.5547170639038086, -14.557175636291504], "cluster": 1}, {"key": "norouzi2022snapmode", "year": "2022", "citations": "2", "title": "Snapmode: An Intelligent And Distributed Large-scale Fashion Image Retrieval Platform Based On Big Data And Deep Generative Adversarial Network Technologies", "abstract": "<p>Fashion is now among the largest industries worldwide, for it represents\nhuman history and helps tell the worlds story. As a result of the Fourth\nIndustrial Revolution, the Internet has become an increasingly important source\nof fashion information. However, with a growing number of web pages and social\ndata, it is nearly impossible for humans to manually catch up with the ongoing\nevolution and the continuously variable content in this domain. The proper\nmanagement and exploitation of big data can pave the way for the substantial\ngrowth of the global economy as well as citizen satisfaction. Therefore,\ncomputer scientists have found it challenging to handle e-commerce fashion\nwebsites by using big data and machine learning technologies. This paper first\nproposes a scalable focused Web Crawler engine based on the distributed\ncomputing platforms to extract and process fashion data on e-commerce websites.\nThe role of the proposed platform is then described in developing a\ndisentangled feature extraction method by employing deep convolutional\ngenerative adversarial networks (DCGANs) for content-based image indexing and\nretrieval. Finally, the state-of-the-art solutions are compared, and the\nresults of the proposed approach are analyzed on a standard dataset. For the\nreal-life implementation of the proposed solution, a Web-based application is\ndeveloped on Apache Storm, Kafka, Solr, and Milvus platforms to create a\nfashion search engine called SnapMode.</p>\n", "tags": ["Robustness", "Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [18.641555786132812, -42.537593841552734], "cluster": 7}, {"key": "norouzi2025hamming", "year": "2012", "citations": "540", "title": "Hamming Distance Metric Learning", "abstract": "<p>Motivated by large-scale multimedia applications we propose to learn mappings\nfrom high-dimensional data to binary codes that preserve semantic similarity.\nBinary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable\nto broad families of mappings, and uses a flexible form of triplet ranking loss.\nWe overcome discontinuous optimization of the discrete mappings by minimizing\na piecewise-smooth upper bound on empirical loss, inspired by latent structural\nSVMs. We develop a new loss-augmented inference algorithm that is quadratic in\nthe code length. We show strong retrieval performance on CIFAR-10 and MNIST,\nwith promising classification results using no more than kNN on the binary codes.</p>\n", "tags": ["Distance-Metric-Learning", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [1.5547189712524414, -14.556808471679688], "cluster": 1}, {"key": "nouredanesh2016gabor", "year": "2016", "citations": "9", "title": "Gabor Barcodes For Medical Image Retrieval", "abstract": "<p>In recent years, advances in medical imaging have led to the emergence of\nmassive databases, containing images from a diverse range of modalities. This\nhas significantly heightened the need for automated annotation of the images on\none side, and fast and memory-efficient content-based image retrieval systems\non the other side. Binary descriptors have recently gained more attention as a\npotential vehicle to achieve these goals. One of the recently introduced binary\ndescriptors for tagging of medical images are Radon barcodes (RBCs) that are\ndriven from Radon transform via local thresholding. Gabor transform is also a\npowerful transform to extract texture-based information. Gabor features have\nexhibited robustness against rotation, scale, and also photometric\ndisturbances, such as illumination changes and image noise in many\napplications. This paper introduces Gabor Barcodes (GBCs), as a novel framework\nfor the image annotation. To find the most discriminative GBC for a given query\nimage, the effects of employing Gabor filters with different parameters, i.e.,\ndifferent sets of scales and orientations, are investigated, resulting in\ndifferent barcode lengths and retrieval performances. The proposed method has\nbeen evaluated on the IRMA dataset with 193 classes comprising of 12,677 x-ray\nimages for indexing, and 1,733 x-rays images for testing. A total error score\nas low as \\(351\\) (\\(\\approx 80%\\) accuracy for the first hit) was achieved.</p>\n", "tags": ["Robustness", "Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-50.63108825683594, 13.935760498046875], "cluster": 0}, {"key": "novotn\u00fd2018implementation", "year": "2018", "citations": "27", "title": "Implementation Notes For The Soft Cosine Measure", "abstract": "<p>The standard bag-of-words vector space model (VSM) is efficient, and\nubiquitous in information retrieval, but it underestimates the similarity of\ndocuments with the same meaning, but different terminology. To overcome this\nlimitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that\nincorporates term similarity relations. Charlet and Damnati showed that the SCM\nis highly effective in question answering (QA) systems. However, the\northonormalization algorithm proposed by Sidorov et al. has an impractical time\ncomplexity of \\(\\mathcal O(n^4)\\), where n is the size of the vocabulary.\n  In this paper, we prove a tighter lower worst-case time complexity bound of\n\\(\\mathcal O(n^3)\\). We also present an algorithm for computing the similarity\nbetween documents and we show that its worst-case time complexity is \\(\\mathcal\nO(1)\\) given realistic conditions. Lastly, we describe implementation in\ngeneral-purpose vector databases such as Annoy, and Faiss and in the inverted\nindices of text search engines such as Apache Lucene, and ElasticSearch. Our\nresults enable the deployment of the SCM in real-world information retrieval\nsystems.</p>\n", "tags": ["Tools-&-Libraries", "Text-Retrieval", "CIKM"], "tsne_embedding": [25.510480880737305, 25.405588150024414], "cluster": 2}, {"key": "nunes2023dothash", "year": "2023", "citations": "4", "title": "Dothash: Estimating Set Similarity Metrics For Link Prediction And Document Deduplication", "abstract": "<p>Metrics for set similarity are a core aspect of several data mining tasks. To\nremove duplicate results in a Web search, for example, a common approach looks\nat the Jaccard index between all pairs of pages. In social network analysis, a\nmuch-celebrated metric is the Adamic-Adar index, widely used to compare node\nneighborhood sets in the important problem of predicting links. However, with\nthe increasing amount of data to be processed, calculating the exact similarity\nbetween all pairs can be intractable. The challenge of working at this scale\nhas motivated research into efficient estimators for set similarity metrics.\nThe two most popular estimators, MinHash and SimHash, are indeed used in\napplications such as document deduplication and recommender systems where large\nvolumes of data need to be processed. Given the importance of these tasks, the\ndemand for advancing estimators is evident. We propose DotHash, an unbiased\nestimator for the intersection size of two sets. DotHash can be used to\nestimate the Jaccard index and, to the best of our knowledge, is the first\nmethod that can also estimate the Adamic-Adar index and a family of related\nmetrics. We formally define this family of metrics, provide theoretical bounds\non the probability of estimate errors, and analyze its empirical performance.\nOur experimental results indicate that DotHash is more accurate than the other\nestimators in link prediction and detecting duplicate documents with the same\ncomplexity and similar comparison time.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Recommender-Systems", "Evaluation", "KDD"], "tsne_embedding": [38.810302734375, 9.544668197631836], "cluster": 9}, {"key": "oguni2019character", "year": "2019", "citations": "0", "title": "Character 3-gram Mover's Distance: An Effective Method For Detecting Near-duplicate Japanese-language Recipes", "abstract": "<p>In user-generated recipe websites, users post their-original recipes. Some\nrecipes, however, are very similar in major components such as the cooking\ninstructions to other recipes. We refer to such recipes as \u201cnear-duplicate\nrecipes\u201d. In this study, we propose a method that extends the \u201cWord Mover\u2019s\nDistance\u201d, which calculates distances between texts based on word embedding, to\ncharacter 3-gram embedding. Using a corpus of over 1.21 million recipes, we\nlearned the word embedding and the character 3-gram embedding by using a\nSkip-Gram model with negative sampling and fastText to extract candidate pairs\nof near-duplicate recipes. We then annotated these candidates and evaluated the\nproposed method against a comparison method. Our results demonstrated that\nnear-duplicate recipes that were not detected by the comparison method were\nsuccessfully detected by the proposed method.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-42.72806167602539, 33.721778869628906], "cluster": 0}, {"key": "oguri2023general", "year": "2023", "citations": "3", "title": "General And Practical Tuning Method For Off-the-shelf Graph-based Index: SISAP Indexing Challenge Report By Team Utokyo", "abstract": "<p>Despite the efficacy of graph-based algorithms for Approximate Nearest\nNeighbor (ANN) searches, the optimal tuning of such systems remains unclear.\nThis study introduces a method to tune the performance of off-the-shelf\ngraph-based indexes, focusing on the dimension of vectors, database size, and\nentry points of graph traversal. We utilize a black-box optimization algorithm\nto perform integrated tuning to meet the required levels of recall and Queries\nPer Second (QPS). We applied our approach to Task A of the SISAP 2023 Indexing\nChallenge and got second place in the 10M and 30M tracks. It improves\nperformance substantially compared to brute force methods. This research offers\na universally applicable tuning method for graph-based indexes, extending\nbeyond the specific conditions of the competition to broader uses.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation"], "tsne_embedding": [53.39503479003906, 10.958900451660156], "cluster": 9}, {"key": "oguri2024theoretical", "year": "2024", "citations": "0", "title": "Theoretical And Empirical Analysis Of Adaptive Entry Point Selection For Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>We present a theoretical and empirical analysis of the adaptive entry point\nselection for graph-based approximate nearest neighbor search (ANNS). We\nintroduce novel concepts: \\(b\\textit{-monotonic path}\\) and \\(B\\textit{-MSNET}\\),\nwhich better capture an actual graph in practical algorithms than existing\nconcepts like MSNET. We prove that adaptive entry point selection offers better\nperformance upper bound than the fixed central entry point under more general\nconditions than previous work. Empirically, we validate the method\u2019s\neffectiveness in accuracy, speed, and memory usage across various datasets,\nespecially in challenging scenarios with out-of-distribution data and hard\ninstances. Our comprehensive study provides deeper insights into optimizing\nentry points for graph-based ANNS for real-world high-dimensional data\napplications.</p>\n", "tags": ["Graph-Based-Ann", "Memory-Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [53.125038146972656, 10.297645568847656], "cluster": 9}, {"key": "omama2024exploiting", "year": "2024", "citations": "0", "title": "Exploiting Distribution Constraints For Scalable And Efficient Image Retrieval", "abstract": "<p>Image retrieval is crucial in robotics and computer vision, with downstream\napplications in robot place recognition and vision-based product\nrecommendations. Modern retrieval systems face two key challenges: scalability\nand efficiency. State-of-the-art image retrieval systems train specific neural\nnetworks for each dataset, an approach that lacks scalability. Furthermore,\nsince retrieval speed is directly proportional to embedding size, existing\nsystems that use large embeddings lack efficiency. To tackle scalability,\nrecent works propose using off-the-shelf foundation models. However, these\nmodels, though applicable across datasets, fall short in achieving performance\ncomparable to that of dataset-specific models. Our key observation is that,\nwhile foundation models capture necessary subtleties for effective retrieval,\nthe underlying distribution of their embedding space can negatively impact\ncosine similarity searches. We introduce Autoencoders with Strong Variance\nConstraints (AE-SVC), which, when used for projection, significantly improves\nthe performance of foundation models. We provide an in-depth theoretical\nanalysis of AE-SVC. Addressing efficiency, we introduce Single-shot Similarity\nSpace Distillation ((SS)\\(_2\\)D), a novel approach to learn embeddings with\nadaptive sizes that offers a better trade-off between size and performance. We\nconducted extensive experiments on four retrieval datasets, including Stanford\nOnline Products (SoP) and Pittsburgh30k, using four different off-the-shelf\nfoundation models, including DinoV2 and CLIP. AE-SVC demonstrates up to a\n\\(16%\\) improvement in retrieval performance, while (SS)\\(_2\\)D shows a further\n\\(10%\\) improvement for smaller embedding sizes.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Similarity-Search", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-25.203718185424805, -1.1229838132858276], "cluster": 1}, {"key": "ong2017siamese", "year": "2017", "citations": "37", "title": "Siamese Network Of Deep Fisher-vector Descriptors For Image Retrieval", "abstract": "<p>This paper addresses the problem of large scale image retrieval, with the aim\nof accurately ranking the similarity of a large number of images to a given\nquery image. To achieve this, we propose a novel Siamese network. This network\nconsists of two computational strands, each comprising of a CNN component\nfollowed by a Fisher vector component. The CNN component produces dense, deep\nconvolutional descriptors that are then aggregated by the Fisher Vector method.\nCrucially, we propose to simultaneously learn both the CNN filter weights and\nFisher Vector model parameters. This allows us to account for the evolving\ndistribution of deep descriptors over the course of the learning process. We\nshow that the proposed approach gives significant improvements over the\nstate-of-the-art methods on the Oxford and Paris image retrieval datasets.\nAdditionally, we provide a baseline performance measure for both these datasets\nwith the inclusion of 1 million distractors.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.44786834716797, 0.7428889870643616], "cluster": 0}, {"key": "ono2023relative", "year": "2023", "citations": "3", "title": "Relative Nn-descent: A Fast Index Construction For Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is the task of finding the\ndatabase vector that is closest to a given query vector. Graph-based ANNS is\nthe family of methods with the best balance of accuracy and speed for\nmillion-scale datasets. However, graph-based methods have the disadvantage of\nlong index construction time. Recently, many researchers have improved the\ntradeoff between accuracy and speed during a search. However, there is little\nresearch on accelerating index construction. We propose a fast graph\nconstruction algorithm, Relative NN-Descent (RNN-Descent). RNN-Descent combines\nNN-Descent, an algorithm for constructing approximate K-nearest neighbor graphs\n(K-NN graphs), and RNG Strategy, an algorithm for selecting edges effective for\nsearch. This algorithm allows the direct construction of graph-based indexes\nwithout ANNS. Experimental results demonstrated that the proposed method had\nthe fastest index construction speed, while its search performance is\ncomparable to existing state-of-the-art methods such as NSG. For example, in\nexperiments on the GIST1M dataset, the construction of the proposed method is\n2x faster than NSG. Additionally, it was even faster than the construction\nspeed of NN-Descent.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Datasets"], "tsne_embedding": [55.29507827758789, 8.0836820602417], "cluster": 9}, {"key": "ootomo2023cagra", "year": "2024", "citations": "7", "title": "CAGRA: Highly Parallel Graph Construction And Approximate Nearest Neighbor Search For Gpus", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) plays a critical role in various\ndisciplines spanning data mining and artificial intelligence, from information\nretrieval and computer vision to natural language processing and recommender\nsystems. Data volumes have soared in recent years and the computational cost of\nan exhaustive exact nearest neighbor search is often prohibitive, necessitating\nthe adoption of approximate techniques. The balanced performance and recall of\ngraph-based approaches have more recently garnered significant attention in\nANNS algorithms, however, only a few studies have explored harnessing the power\nof GPUs and multi-core processors despite the widespread use of massively\nparallel and general-purpose computing. To bridge this gap, we introduce a\nnovel parallel computing hardware-based proximity graph and search algorithm.\nBy leveraging the high-performance capabilities of modern hardware, our\napproach achieves remarkable efficiency gains. In particular, our method\nsurpasses existing CPU and GPU-based methods in constructing the proximity\ngraph, demonstrating higher throughput in both large- and small-batch searches\nwhile maintaining compatible accuracy. In graph construction time, our method,\nCAGRA, is 2.2~27x faster than HNSW, which is one of the CPU SOTA\nimplementations. In large-batch query throughput in the 90% to 95% recall\nrange, our method is 33~77x faster than HNSW, and is 3.8~8.8x faster than the\nSOTA implementations for GPU. For a single query, our method is 3.4~53x faster\nthan HNSW at 95% recall.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Recommender-Systems", "Evaluation"], "tsne_embedding": [48.372947692871094, 15.76438045501709], "cluster": 9}, {"key": "opitz2018deep", "year": "2018", "citations": "164", "title": "Deep Metric Learning With BIER: Boosting Independent Embeddings Robustly", "abstract": "<p>Learning similarity functions between image pairs with deep neural networks\nyields highly correlated activations of embeddings. In this work, we show how\nto improve the robustness of such embeddings by exploiting the independence\nwithin ensembles. To this end, we divide the last embedding layer of a deep\nnetwork into an embedding ensemble and formulate training this ensemble as an\nonline gradient boosting problem. Each learner receives a reweighted training\nsample from the previous learners. Further, we propose two loss functions which\nincrease the diversity in our ensemble. These loss functions can be applied\neither for weight initialization or during training. Together, our\ncontributions leverage large embedding sizes more effectively by significantly\nreducing correlation of the embedding and consequently increase retrieval\naccuracy of the embedding. Our method works with any differentiable loss\nfunction and does not introduce any additional parameters during test time. We\nevaluate our metric learning method on image retrieval tasks and show that it\nimproves over state-of-the-art methods on the CUB 200-2011, Cars-196, Stanford\nOnline Products, In-Shop Clothes Retrieval and VehicleID datasets.</p>\n", "tags": ["Robustness", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-18.107805252075195, -16.175447463989258], "cluster": 1}, {"key": "ortega2022unconventional", "year": "2022", "citations": "5", "title": "Unconventional Application Of K-means For Distributed Approximate Similarity Search", "abstract": "<p>Similarity search based on a distance function in metric spaces is a\nfundamental problem for many applications. Queries for similar objects lead to\nthe well-known machine learning task of nearest-neighbours identification. Many\ndata indexing strategies, collectively known as Metric Access Methods (MAM),\nhave been proposed to speed up queries for similar elements in this context.\nMoreover, since exact approaches to solve similarity queries can be complex and\ntime-consuming, alternative options have appeared to reduce query execution\ntime, such as returning approximate results or resorting to distributed\ncomputing platforms. In this paper, we introduce MASK (Multilevel Approximate\nSimilarity search with \\(k\\)-means), an unconventional application of the\n\\(k\\)-means algorithm as the foundation of a multilevel index structure for\napproximate similarity search, suitable for metric spaces. We show that\ninherent properties of \\(k\\)-means, like representing high-density data areas\nwith fewer prototypes, can be leveraged for this purpose. An implementation of\nthis new indexing method is evaluated, using a synthetic dataset and a\nreal-world dataset in a high-dimensional and high-sparsity space. Results are\npromising and underpin the applicability of this novel indexing method in\nmultiple domains.</p>\n", "tags": ["Vector-Indexing", "Similarity-Search", "Datasets"], "tsne_embedding": [26.078630447387695, 33.59529113769531], "cluster": 4}, {"key": "otani2016learning", "year": "2016", "citations": "92", "title": "Learning Joint Representations Of Videos And Sentences With Web Image Search", "abstract": "<p>Our objective is video retrieval based on natural language queries. In\naddition, we consider the analogous problem of retrieving sentences or\ngenerating descriptions given an input video. Recent work has addressed the\nproblem by embedding visual and textual inputs into a common space where\nsemantic similarities correlate to distances. We also adopt the embedding\napproach, and make the following contributions: First, we utilize web image\nsearch in sentence embedding process to disambiguate fine-grained visual\nconcepts. Second, we propose embedding models for sentence, image, and video\ninputs whose parameters are learned simultaneously. Finally, we show how the\nproposed model can be applied to description generation. Overall, we observe a\nclear improvement over the state-of-the-art methods in the video and sentence\nretrieval tasks. In description generation, the performance level is comparable\nto the current state-of-the-art, although our embeddings were trained for the\nretrieval tasks.</p>\n", "tags": ["Evaluation", "Video-Retrieval"], "tsne_embedding": [-26.835664749145508, -27.945377349853516], "cluster": 5}, {"key": "ou2021integrating", "year": "2021", "citations": "3", "title": "Integrating Semantics And Neighborhood Information With Graph-driven Generative Models For Document Retrieval", "abstract": "<p>With the need of fast retrieval speed and small memory footprint, document\nhashing has been playing a crucial role in large-scale information retrieval.\nTo generate high-quality hashing code, both semantics and neighborhood\ninformation are crucial. However, most existing methods leverage only one of\nthem or simply combine them via some intuitive criteria, lacking a theoretical\nprinciple to guide the integration process. In this paper, we encode the\nneighborhood information with a graph-induced Gaussian distribution, and\npropose to integrate the two types of information with a graph-driven\ngenerative model. To deal with the complicated correlations among documents, we\nfurther propose a tree-structured approximation method for learning. Under the\napproximation, we prove that the training objective can be decomposed into\nterms involving only singleton or pairwise documents, enabling the model to be\ntrained as efficiently as uncorrelated ones. Extensive experimental results on\nthree benchmark datasets show that our method achieves superior performance\nover state-of-the-art methods, demonstrating the effectiveness of the proposed\nmodel for simultaneously preserving semantic and neighborhood information.\\</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Efficiency", "Scalability", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [46.54147720336914, 3.8074791431427], "cluster": 9}, {"key": "ou2021refining", "year": "2021", "citations": "3", "title": "Refining BERT Embeddings For Document Hashing Via Mutual Information Maximization", "abstract": "<p>Existing unsupervised document hashing methods are mostly established on\ngenerative models. Due to the difficulties of capturing long dependency\nstructures, these methods rarely model the raw documents directly, but instead\nto model the features extracted from them (e.g. bag-of-words (BOW), TFIDF). In\nthis paper, we propose to learn hash codes from BERT embeddings after observing\ntheir tremendous successes on downstream tasks. As a first try, we modify\nexisting generative hashing models to accommodate the BERT embeddings. However,\nlittle improvement is observed over the codes learned from the old BOW or TFIDF\nfeatures. We attribute this to the reconstruction requirement in the generative\nhashing, which will enforce irrelevant information that is abundant in the BERT\nembeddings also compressed into the codes. To remedy this issue, a new\nunsupervised hashing paradigm is further proposed based on the mutual\ninformation (MI) maximization principle. Specifically, the method first\nconstructs appropriate global and local codes from the documents and then seeks\nto maximize their mutual information. Experimental results on three benchmark\ndatasets demonstrate that the proposed method is able to generate hash codes\nthat outperform existing ones learned from BOW features by a substantial\nmargin.</p>\n", "tags": ["Hashing-Methods", "EMNLP", "Neural-Hashing", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [11.669527053833008, -12.606877326965332], "cluster": 7}, {"key": "ouyang2021contextual", "year": "2021", "citations": "4", "title": "Contextual Similarity Aggregation With Self-attention For Visual Re-ranking", "abstract": "<p>In content-based image retrieval, the first-round retrieval result by simple\nvisual feature comparison may be unsatisfactory, which can be refined by visual\nre-ranking techniques. In image retrieval, it is observed that the contextual\nsimilarity among the top-ranked images is an important clue to distinguish the\nsemantic relevance. Inspired by this observation, in this paper, we propose a\nvisual re-ranking method by contextual similarity aggregation with\nself-attention. In our approach, for each image in the top-K ranking list, we\nrepresent it into an affinity feature vector by comparing it with a set of\nanchor images. Then, the affinity features of the top-K images are refined by\naggregating the contextual information with a transformer encoder. Finally, the\naffinity features are used to recalculate the similarity scores between the\nquery and the top-K images for re-ranking of the latter. To further improve the\nrobustness of our re-ranking model and enhance the performance of our method, a\nnew data augmentation scheme is designed. Since our re-ranking model is not\ndirectly involved with the visual feature used in the initial retrieval, it is\nready to be applied to retrieval result lists obtained from various retrieval\nalgorithms. We conduct comprehensive experiments on four benchmark datasets to\ndemonstrate the generality and effectiveness of our proposed visual re-ranking\nmethod.</p>\n", "tags": ["Image-Retrieval", "Robustness", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-29.675413131713867, 11.571993827819824], "cluster": 0}, {"key": "oymak2016near", "year": "2017", "citations": "13", "title": "Near-optimal Sample Complexity Bounds For Circulant Binary Embedding", "abstract": "<p>Binary embedding is the problem of mapping points from a high-dimensional\nspace to a Hamming cube in lower dimension while preserving pairwise distances.\nAn efficient way to accomplish this is to make use of fast embedding techniques\ninvolving Fourier transform e.g.~circulant matrices. While binary embedding has\nbeen studied extensively, theoretical results on fast binary embedding are\nrather limited. In this work, we build upon the recent literature to obtain\nsignificantly better dependencies on the problem parameters. A set of \\(N\\)\npoints in \\(\\mathbb{R}^n\\) can be properly embedded into the Hamming cube \\(\\{\\pm\n1\\}^k\\) with \\(\\delta\\) distortion, by using \\(k\\sim\\delta^{-3}log N\\) samples\nwhich is optimal in the number of points \\(N\\) and compares well with the optimal\ndistortion dependency \\(\\delta^{-2}\\). Our optimal embedding result applies in\nthe regime \\(log N\\lesssim n^{1/3}\\). Furthermore, if the looser condition \\(log\nN\\lesssim \\sqrt{n}\\) holds, we show that all but an arbitrarily small fraction\nof the points can be optimally embedded. We believe our techniques can be\nuseful to obtain improved guarantees for other nonlinear embedding problems.</p>\n", "tags": ["ICASSP", "Hashing-Methods"], "tsne_embedding": [11.898588180541992, 51.63432312011719], "cluster": 4}, {"key": "oymak2017near", "year": "2017", "citations": "13", "title": "Near-optimal Sample Complexity Bounds For Circulant Binary Embedding", "abstract": "<p>Binary embedding is the problem of mapping points from a high-dimensional\nspace to a Hamming cube in lower dimension while preserving pairwise distances.\nAn efficient way to accomplish this is to make use of fast embedding techniques\ninvolving Fourier transform e.g.~circulant matrices. While binary embedding has\nbeen studied extensively, theoretical results on fast binary embedding are\nrather limited. In this work, we build upon the recent literature to obtain\nsignificantly better dependencies on the problem parameters. A set of \\(N\\)\npoints in \\(\\mathbb{R}^n\\) can be properly embedded into the Hamming cube \\(\\{\\pm\n1\\}^k\\) with \\(\\delta\\) distortion, by using \\(k\\sim\\delta^{-3}log N\\) samples\nwhich is optimal in the number of points \\(N\\) and compares well with the optimal\ndistortion dependency \\(\\delta^{-2}\\). Our optimal embedding result applies in\nthe regime \\(log N\\lesssim n^{1/3}\\). Furthermore, if the looser condition \\(log\nN\\lesssim \\sqrt{n}\\) holds, we show that all but an arbitrarily small fraction\nof the points can be optimally embedded. We believe our techniques can be\nuseful to obtain improved guarantees for other nonlinear embedding problems.</p>\n", "tags": ["ICASSP", "Hashing-Methods"], "tsne_embedding": [11.898615837097168, 51.634307861328125], "cluster": 4}, {"key": "ozdemir2016scalable", "year": "2016", "citations": "0", "title": "Scalable Gaussian Processes For Supervised Hashing", "abstract": "<p>We propose a flexible procedure for large-scale image search by hash\nfunctions with kernels. Our method treats binary codes and pairwise semantic\nsimilarity as latent and observed variables, respectively, in a probabilistic\nmodel based on Gaussian processes for binary classification. We present an\nefficient inference algorithm with the sparse pseudo-input Gaussian process\n(SPGP) model and parallelization. Experiments on three large-scale image\ndataset demonstrate the effectiveness of the proposed hashing method, Gaussian\nProcess Hashing (GPH), for short binary codes and the datasets without\npredefined classes in comparison to the state-of-the-art supervised hashing\nmethods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-14.48493766784668, 21.83160972595215], "cluster": 8}, {"key": "ozdemir2016supervised", "year": "2016", "citations": "2", "title": "Supervised Incremental Hashing", "abstract": "<p>We propose an incremental strategy for learning hash functions with kernels\nfor large-scale image search. Our method is based on a two-stage classification\nframework that treats binary codes as intermediate variables between the\nfeature space and the semantic space. In the first stage of classification,\nbinary codes are considered as class labels by a set of binary SVMs; each\ncorresponds to one bit. In the second stage, binary codes become the input\nspace of a multi-class SVM. Hash functions are learned by an efficient\nalgorithm where the NP-hard problem of finding optimal binary codes is solved\nvia cyclic coordinate descent and SVMs are trained in a parallelized\nincremental manner. For modifications like adding images from a previously\nunseen class, we describe an incremental procedure for effective and efficient\nupdates to the previous hash functions. Experiments on three large-scale image\ndatasets demonstrate the effectiveness of the proposed hashing method,\nSupervised Incremental Hashing (SIH), over the state-of-the-art supervised\nhashing methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [17.74714469909668, -3.9097886085510254], "cluster": 6}, {"key": "ozkan2018exploiting", "year": "2021", "citations": "1", "title": "Exploiting Local Indexing And Deep Feature Confidence Scores For Fast Image-to-video Search", "abstract": "<p>The cost-effective visual representation and fast query-by-example search are\ntwo challenging goals that should be maintained for web-scale visual retrieval\ntasks on moderate hardware. This paper introduces a fast and robust method that\nensures both of these goals by obtaining state-of-the-art performance for an\nimage-to-video search scenario. Hence, we present critical enhancements to\nwell-known indexing and visual representation techniques by promoting faster,\nbetter and moderate retrieval performance. We also boost the superiority of our\nmethod for some visual challenges by exploiting individual decisions of local\nand global descriptors at query time. For instance, local content descriptors\nrepresent copied/duplicated scenes with large geometric deformations such as\nscale, orientation and affine transformation. In contrast, the use of global\ncontent descriptors is more practical for near-duplicate and semantic searches.\nExperiments are conducted on a large-scale Stanford I2V dataset. The\nexperimental results show that our method is useful in terms of complexity and\nquery processing time for large-scale visual retrieval scenarios, even if local\nand global representations are used together. The proposed method is superior\nand achieves state-of-the-art performance based on the mean average precision\n(MAP) score of this dataset. Lastly, we report additional MAP scores after\nupdating the ground annotations unveiled by retrieval results of the proposed\nmethod, and it shows that the actual performance.</p>\n", "tags": ["Efficiency", "Scalability", "Large-Scale-Search", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-38.274925231933594, 6.7676801681518555], "cluster": 0}, {"key": "ozkan2021exploiting", "year": "2021", "citations": "1", "title": "Exploiting Local Indexing And Deep Feature Confidence Scores For Fast Image-to-video Search", "abstract": "<p>The cost-effective visual representation and fast query-by-example search are\ntwo challenging goals that should be maintained for web-scale visual retrieval\ntasks on moderate hardware. This paper introduces a fast and robust method that\nensures both of these goals by obtaining state-of-the-art performance for an\nimage-to-video search scenario. Hence, we present critical enhancements to\nwell-known indexing and visual representation techniques by promoting faster,\nbetter and moderate retrieval performance. We also boost the superiority of our\nmethod for some visual challenges by exploiting individual decisions of local\nand global descriptors at query time. For instance, local content descriptors\nrepresent copied/duplicated scenes with large geometric deformations such as\nscale, orientation and affine transformation. In contrast, the use of global\ncontent descriptors is more practical for near-duplicate and semantic searches.\nExperiments are conducted on a large-scale Stanford I2V dataset. The\nexperimental results show that our method is useful in terms of complexity and\nquery processing time for large-scale visual retrieval scenarios, even if local\nand global representations are used together. The proposed method is superior\nand achieves state-of-the-art performance based on the mean average precision\n(MAP) score of this dataset. Lastly, we report additional MAP scores after\nupdating the ground annotations unveiled by retrieval results of the proposed\nmethod, and it shows that the actual performance.</p>\n", "tags": ["Efficiency", "Scalability", "Large-Scale-Search", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-38.274986267089844, 6.767661094665527], "cluster": 0}, {"key": "ozsoy2020utilizing", "year": "2020", "citations": "0", "title": "Utilizing Fasttext For Venue Recommendation", "abstract": "<p>Venue recommendation systems model the past interactions (i.e., check-ins) of\nthe users and recommend venues. Traditional recommendation systems employ\ncollaborative filtering, content-based filtering or matrix factorization.\nRecently, vector space embedding and deep learning algorithms are also used for\nrecommendation. In this work, I propose a method for recommending top-k venues\nby utilizing the sequentiality feature of check-ins and a recent vector space\nembedding method, namely the FastText. Our proposed method; forms groups of\ncheck-ins, learns the vector space representations of the venues and utilizes\nthe learned embeddings to make venue recommendations. I measure the performance\nof the proposed method using a Foursquare check-in dataset.The results show\nthat the proposed method performs better than the state-of-the-art methods.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [41.263877868652344, -8.742517471313477], "cluster": 9}, {"key": "pachori2017hashing", "year": "2017", "citations": "24", "title": "Hashing In The Zero Shot Framework With Domain Adaptation", "abstract": "<p>Techniques to learn hash codes which can store and retrieve large dimensional\nmultimedia data efficiently have attracted broad research interests in the\nrecent years. With rapid explosion of newly emerged concepts and online data,\nexisting supervised hashing algorithms suffer from the problem of scarcity of\nground truth annotations due to the high cost of obtaining manual annotations.\nTherefore, we propose an algorithm to learn a hash function from training\nimages belonging to <code class=\"language-plaintext highlighter-rouge\">seen' classes which can efficiently encode images of\n</code>unseen\u2019 classes to binary codes. Specifically, we project the image features\nfrom visual space and semantic features from semantic space into a common\nHamming subspace. Earlier works to generate hash codes have tried to relax the\ndiscrete constraints on hash codes and solve the continuous optimization\nproblem. However, it often leads to quantization errors. In this work, we use\nthe max-margin classifier to learn an efficient hash function. To address the\nconcern of domain-shift which may arise due to the introduction of new classes,\nwe also introduce an unsupervised domain adaptation model in the proposed\nhashing framework. Results on the three datasets show the advantage of using\ndomain adaptation in learning a high-quality hash function and superiority of\nour method for the task of image retrieval performance as compared to several\nstate-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [14.005694389343262, -3.136535882949829], "cluster": 6}, {"key": "pacuk2016locality", "year": "2016", "citations": "7", "title": "Locality-sensitive Hashing Without False Negatives For L_p", "abstract": "<p>In this paper, we show a construction of locality-sensitive hash functions\nwithout false negatives, i.e., which ensure collision for every pair of points\nwithin a given radius \\(R\\) in \\(d\\) dimensional space equipped with \\(l_p\\) norm\nwhen \\(p \\in [1,\\infty]\\). Furthermore, we show how to use these hash functions\nto solve the \\(c\\)-approximate nearest neighbor search problem without false\nnegatives. Namely, if there is a point at distance \\(R\\), we will certainly\nreport it and points at distance greater than \\(cR\\) will not be reported for\n\\(c=\u03a9(\\sqrt{d},d^{1-\\frac{1}{p}})\\). The constructed algorithms work: - with\npreprocessing time \\(\\mathcal{O}(n log(n))\\) and sublinear expected query time,</p>\n<ul>\n  <li>with preprocessing time \\(\\mathcal{O}(\\mathrm{poly}(n))\\) and expected query\ntime \\(\\mathcal{O}(log(n))\\). Our paper reports progress on answering the open\nproblem presented by Pagh [8] who considered the nearest neighbor search\nwithout false negatives for the Hamming distance.</li>\n</ul>\n", "tags": ["Efficiency", "Hashing-Methods"], "tsne_embedding": [24.698347091674805, 48.290870666503906], "cluster": 4}, {"key": "pagh2016approximate", "year": "2016", "citations": "10", "title": "Approximate Furthest Neighbor With Application To Annulus Query", "abstract": "<p>Much recent work has been devoted to approximate nearest neighbor queries.\nMotivated by applications in recommender systems, we consider approximate\nfurthest neighbor (AFN) queries and present a simple, fast, and highly\npractical data structure for answering AFN queries in high- dimensional\nEuclidean space. The method builds on the technique of In- dyk (SODA 2003),\nstoring random projections to provide sublinear query time for AFN. However, we\nintroduce a different query algorithm, improving on Indyk\u2019s approximation\nfactor and reducing the running time by a logarithmic factor. We also present a\nvariation based on a query- independent ordering of the database points; while\nthis does not have the provable approximation factor of the query-dependent\ndata structure, it offers significant improvement in time and space complexity.\nWe give a theoretical analysis, and experimental results. As an application,\nthe query-dependent approach is used for deriving a data structure for the\napproximate annulus query problem, which is defined as follows: given an input\nset S and two parameters r &gt; 0 and w &gt;= 1, construct a data structure that\nreturns for each query point q a point p in S such that the distance between p\nand q is at least r/w and at most wr.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Locality-Sensitive-Hashing"], "tsne_embedding": [21.252960205078125, 41.289955139160156], "cluster": 4}, {"key": "pande2019swag", "year": "2019", "citations": "12", "title": "SWAG: Item Recommendations Using Convolutions On Weighted Graphs", "abstract": "<p>Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. In this\nwork, we present a Graph Convolutional Network (GCN) algorithm SWAG (Sample\nWeight and AGgregate), which combines efficient random walks and graph\nconvolutions on weighted graphs to generate embeddings for nodes (items) that\nincorporate both graph structure as well as node feature information such as\nitem-descriptions and item-images. The three important SWAG operations that\nenable us to efficiently generate node embeddings based on graph structures are\n(a) Sampling of graph to homogeneous structure, (b) Weighting the sampling,\nwalks and convolution operations, and (c) using AGgregation functions for\ngenerating convolutions. The work is an adaptation of graphSAGE over weighted\ngraphs. We deploy SWAG at Target and train it on a graph of more than 500K\nproducts sold online with over 50M edges. Offline and online evaluations reveal\nthe benefit of using a graph-based approach and the benefits of weighing to\nproduce high quality embeddings and product recommendations.</p>\n", "tags": ["Graph-Based-Ann", "Recommender-Systems", "Evaluation"], "tsne_embedding": [56.83165740966797, -1.8207697868347168], "cluster": 9}, {"key": "pang2018deep", "year": "2018", "citations": "40", "title": "Deep Feature Aggregation And Image Re-ranking With Heat Diffusion For Image Retrieval", "abstract": "<p>Image retrieval based on deep convolutional features has demonstrated\nstate-of-the-art performance in popular benchmarks. In this paper, we present a\nunified solution to address deep convolutional feature aggregation and image\nre-ranking by simulating the dynamics of heat diffusion. A distinctive problem\nin image retrieval is that repetitive or <em>bursty</em> features tend to\ndominate final image representations, resulting in representations less\ndistinguishable. We show that by considering each deep feature as a heat\nsource, our unsupervised aggregation method is able to avoid\nover-representation of <em>bursty</em> features. We additionally provide a\npractical solution for the proposed aggregation method and further show the\nefficiency of our method in experimental evaluation. Inspired by the\naforementioned deep feature aggregation method, we also propose a method to\nre-rank a number of top ranked images for a given query image by considering\nthe query as the heat source. Finally, we extensively evaluate the proposed\napproach with pre-trained and fine-tuned deep networks on common public\nbenchmarks and show superior performance compared to previous work.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation", "Unsupervised"], "tsne_embedding": [-5.8403730392456055, 25.238384246826172], "cluster": 8}, {"key": "pansare2022learning", "year": "2022", "citations": "3", "title": "Learning Compressed Embeddings For On-device Inference", "abstract": "<p>In deep learning, embeddings are widely used to represent categorical\nentities such as words, apps, and movies. An embedding layer maps each entity\nto a unique vector, causing the layer\u2019s memory requirement to be proportional\nto the number of entities. In the recommendation domain, a given category can\nhave hundreds of thousands of entities, and its embedding layer can take\ngigabytes of memory. The scale of these networks makes them difficult to deploy\nin resource constrained environments. In this paper, we propose a novel\napproach for reducing the size of an embedding table while still mapping each\nentity to its own unique embedding. Rather than maintaining the full embedding\ntable, we construct each entity\u2019s embedding \u201con the fly\u201d using two separate\nembedding tables. The first table employs hashing to force multiple entities to\nshare an embedding. The second table contains one trainable weight per entity,\nallowing the model to distinguish between entities sharing the same embedding.\nSince these two tables are trained jointly, the network is able to learn a\nunique embedding per entity, helping it maintain a discriminative capability\nsimilar to a model with an uncompressed embedding table. We call this approach\nMEmCom (Multi-Embedding Compression). We compare with state-of-the-art model\ncompression techniques for multiple problem classes including classification\nand ranking. On four popular recommender system datasets, MEmCom had a 4%\nrelative loss in nDCG while compressing the input embedding sizes of our\nrecommendation models by 16x, 4x, 12x, and 40x. MEmCom outperforms the\nstate-of-the-art techniques, which achieved 16%, 6%, 10%, and 8% relative loss\nin nDCG at the respective compression ratios. Additionally, MEmCom is able to\ncompress the RankNet ranking model by 32x on a dataset with millions of users\u2019\ninteractions with games while incurring only a 1% relative loss in nDCG.</p>\n", "tags": ["Recommender-Systems", "Hashing-Methods", "Datasets"], "tsne_embedding": [30.285940170288086, 16.225679397583008], "cluster": 2}, {"key": "paria2020minimizing", "year": "2020", "citations": "14", "title": "Minimizing Flops To Learn Efficient Sparse Representations", "abstract": "<p>Deep representation learning has become one of the most widely adopted\napproaches for visual search, recommendation, and identification. Retrieval of\nsuch representations from a large database is however computationally\nchallenging. Approximate methods based on learning compact representations,\nhave been widely explored for this problem, such as locality sensitive hashing,\nproduct quantization, and PCA. In this work, in contrast to learning compact\nrepresentations, we propose to learn high dimensional and sparse\nrepresentations that have similar representational capacity as dense embeddings\nwhile being more efficient due to sparse matrix multiplication operations which\ncan be much faster than dense multiplication. Following the key insight that\nthe number of operations decreases quadratically with the sparsity of\nembeddings provided the non-zero entries are distributed uniformly across\ndimensions, we propose a novel approach to learn such distributed sparse\nembeddings via the use of a carefully constructed regularization function that\ndirectly minimizes a continuous relaxation of the number of floating-point\noperations (FLOPs) incurred during retrieval. Our experiments show that our\napproach is competitive to the other baselines and yields a similar or better\nspeed-vs-accuracy tradeoff on practical datasets.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization", "Recommender-Systems", "Image-Retrieval", "Datasets"], "tsne_embedding": [31.256671905517578, 20.280790328979492], "cluster": 2}, {"key": "park2020mhsan", "year": "2020", "citations": "20", "title": "MHSAN: Multi-head Self-attention Network For Visual Semantic Embedding", "abstract": "<p>Visual-semantic embedding enables various tasks such as image-text retrieval,\nimage captioning, and visual question answering. The key to successful\nvisual-semantic embedding is to express visual and textual data properly by\naccounting for their intricate relationship. While previous studies have\nachieved much advance by encoding the visual and textual data into a joint\nspace where similar concepts are closely located, they often represent data by\na single vector ignoring the presence of multiple important components in an\nimage or text. Thus, in addition to the joint embedding space, we propose a\nnovel multi-head self-attention network to capture various components of visual\nand textual data by attending to important parts in data. Our approach achieves\nthe new state-of-the-art results in image-text retrieval tasks on MS-COCO and\nFlicker30K datasets. Through the visualization of the attention maps that\ncapture distinct semantic components at multiple positions in the image and the\ntext, we demonstrate that our method achieves an effective and interpretable\nvisual-semantic joint space.</p>\n", "tags": ["Text-Retrieval", "Datasets"], "tsne_embedding": [-26.16475486755371, -30.922504425048828], "cluster": 5}, {"key": "park2021homography", "year": "2021", "citations": "0", "title": "Homography Augumented Momentum Constrastive Learning For SAR Image Retrieval", "abstract": "<p>Deep learning-based image retrieval has been emphasized in computer vision.\nRepresentation embedding extracted by deep neural networks (DNNs) not only aims\nat containing semantic information of the image, but also can manage\nlarge-scale image retrieval tasks. In this work, we propose a deep\nlearning-based image retrieval approach using homography transformation\naugmented contrastive learning to perform large-scale synthetic aperture radar\n(SAR) image search tasks. Moreover, we propose a training method for the DNNs\ninduced by contrastive learning that does not require any labeling procedure.\nThis may enable tractability of large-scale datasets with relative ease.\nFinally, we verify the performance of the proposed method by conducting\nexperiments on the polarimetric SAR image datasets.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-11.819708824157715, -30.0369815826416], "cluster": 3}, {"key": "park2021unsupervised", "year": "2021", "citations": "29", "title": "Unsupervised Representation Learning Via Neural Activation Coding", "abstract": "<p>We present neural activation coding (NAC) as a novel approach for learning\ndeep representations from unlabeled data for downstream applications. We argue\nthat the deep encoder should maximize its nonlinear expressivity on the data\nfor downstream predictors to take full advantage of its representation power.\nTo this end, NAC maximizes the mutual information between activation patterns\nof the encoder and the data over a noisy communication channel. We show that\nlearning for a noise-robust activation code increases the number of distinct\nlinear regions of ReLU encoders, hence the maximum nonlinear expressivity. More\ninterestingly, NAC learns both continuous and discrete representations of data,\nwhich we respectively evaluate on two downstream tasks: (i) linear\nclassification on CIFAR-10 and ImageNet-1K and (ii) nearest neighbor retrieval\non CIFAR-10 and FLICKR-25K. Empirical results show that NAC attains better or\ncomparable performance on both tasks over recent baselines including SimCLR and\nDistillHash. In addition, NAC pretraining provides significant benefits to the\ntraining of deep generative models. Our code is available at\nhttps://github.com/yookoon/nac.</p>\n", "tags": ["Self-Supervised", "CVPR", "Evaluation", "Unsupervised"], "tsne_embedding": [32.397727966308594, -11.014216423034668], "cluster": 7}, {"key": "park2022deepsketch", "year": "2022", "citations": "0", "title": "Deepsketch: A New Machine Learning-based Reference Search Technique For Post-deduplication Delta Compression", "abstract": "<p>Data reduction in storage systems is becoming increasingly important as an\neffective solution to minimize the management cost of a data center. To\nmaximize data-reduction efficiency, existing post-deduplication\ndelta-compression techniques perform delta compression along with traditional\ndata deduplication and lossless compression. Unfortunately, we observe that\nexisting techniques achieve significantly lower data-reduction ratios than the\noptimal due to their limited accuracy in identifying similar data blocks.\n  In this paper, we propose DeepSketch, a new reference search technique for\npost-deduplication delta compression that leverages the learning-to-hash method\nto achieve higher accuracy in reference search for delta compression, thereby\nimproving data-reduction efficiency. DeepSketch uses a deep neural network to\nextract a data block\u2019s sketch, i.e., to create an approximate data signature of\nthe block that can preserve similarity with other blocks. Our evaluation using\neleven real-world workloads shows that DeepSketch improves the data-reduction\nratio by up to 33% (21% on average) over a state-of-the-art post-deduplication\ndelta-compression technique.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [-0.3145655691623688, -11.579143524169922], "cluster": 1}, {"key": "parkerholder2018compressing", "year": "2018", "citations": "0", "title": "Compressing Deep Neural Networks: A New Hashing Pipeline Using Kac's Random Walk Matrices", "abstract": "<p>The popularity of deep learning is increasing by the day. However, despite\nthe recent advancements in hardware, deep neural networks remain\ncomputationally intensive. Recent work has shown that by preserving the angular\ndistance between vectors, random feature maps are able to reduce dimensionality\nwithout introducing bias to the estimator. We test a variety of established\nhashing pipelines as well as a new approach using Kac\u2019s random walk matrices.\nWe demonstrate that this method achieves similar accuracy to existing\npipelines.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [32.017513275146484, -6.955938816070557], "cluster": 9}, {"key": "parola2021web", "year": "2021", "citations": "3", "title": "Web Image Search Engine Based On LSH Index And CNN Resnet50", "abstract": "<p>To implement a good Content Based Image Retrieval (CBIR) system, it is\nessential to adopt efficient search methods. One way to achieve this results is\nby exploiting approximate search techniques. In fact, when we deal with very\nlarge collections of data, using an exact search method makes the system very\nslow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to\nimplement a CBIR system that allows us to perform fast similarity search on\ndeep features. Specifically, we exploit transfer learning techniques to extract\ndeep features from images; this phase is done using two famous Convolutional\nNeural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both\npre-trained on ImageNet. Then we try out several fully connected deep neural\nnetworks, built on top of both of the previously mentioned CNNs in order to\nfine-tuned them on our dataset. In both of previous cases, we index the\nfeatures within our LSH index implementation and within a sequential scan, to\nbetter understand how much the introduction of the index affects the results.\nFinally, we carry out a performance analysis: we evaluate the relevance of the\nresult set, computing the mAP (mean Average Precision) value obtained during\nthe different experiments with respect to the number of done comparison and\nvarying the hyper-parameter values of the LSH index.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-51.66111755371094, -3.504654884338379], "cluster": 0}, {"key": "partalidou2021improving", "year": "2022", "citations": "5", "title": "Improving Zero-Shot Entity Retrieval through Effective Dense Representations", "abstract": "<p>Entity Linking (EL) seeks to align entity mentions in text to entries in a\nknowledge-base and is usually comprised of two phases: candidate generation and\ncandidate ranking. While most methods focus on the latter, it is the candidate\ngeneration phase that sets an upper bound to both time and accuracy performance\nof the overall EL system. This work\u2019s contribution is a significant improvement\nin candidate generation which thus raises the performance threshold for EL, by\ngenerating candidates that include the gold entity in the least candidate set\n(top-K). We propose a simple approach that efficiently embeds mention-entity\npairs in dense space through a BERT-based bi-encoder. Specifically, we extend\n(Wu et al., 2020) by introducing a new pooling function and incorporating\nentity type side-information. We achieve a new state-of-the-art 84.28% accuracy\non top-50 candidates on the Zeshel dataset, compared to the previous 82.06% on\nthe top-64 of (Wu et al., 2020). We report the results from extensive\nexperimentation using our proposed model on both seen and unseen entity\ndatasets. Our results suggest that our method could be a useful complement to\nexisting EL approaches.</p>\n", "tags": ["AAAI", "Evaluation", "Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [-0.05998317897319794, -34.906028747558594], "cluster": 3}, {"key": "partalidou2022improving", "year": "2022", "citations": "5", "title": "Improving Zero-shot Entity Retrieval Through Effective Dense Representations", "abstract": "<p>Entity Linking (EL) seeks to align entity mentions in text to entries in a\nknowledge-base and is usually comprised of two phases: candidate generation and\ncandidate ranking. While most methods focus on the latter, it is the candidate\ngeneration phase that sets an upper bound to both time and accuracy performance\nof the overall EL system. This work\u2019s contribution is a significant improvement\nin candidate generation which thus raises the performance threshold for EL, by\ngenerating candidates that include the gold entity in the least candidate set\n(top-K). We propose a simple approach that efficiently embeds mention-entity\npairs in dense space through a BERT-based bi-encoder. Specifically, we extend\n(Wu et al., 2020) by introducing a new pooling function and incorporating\nentity type side-information. We achieve a new state-of-the-art 84.28% accuracy\non top-50 candidates on the Zeshel dataset, compared to the previous 82.06% on\nthe top-64 of (Wu et al., 2020). We report the results from extensive\nexperimentation using our proposed model on both seen and unseen entity\ndatasets. Our results suggest that our method could be a useful complement to\nexisting EL approaches.</p>\n", "tags": ["AAAI", "Datasets", "Few-Shot-&-Zero-Shot", "Evaluation"], "tsne_embedding": [-0.05999346077442169, -34.90616226196289], "cluster": 3}, {"key": "passalis2019deep", "year": "2019", "citations": "1", "title": "Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval", "abstract": "<p>Several deep supervised hashing techniques have been proposed to allow for\nefficiently querying large image databases. However, deep supervised image\nhashing techniques are developed, to a great extent, heuristically often\nleading to suboptimal results. Contrary to this, we propose an efficient deep\nsupervised hashing algorithm that optimizes the learned codes using an\ninformation-theoretic measure, the Quadratic Mutual Information (QMI). The\nproposed method is adapted to the needs of large-scale hashing and information\nretrieval leading to a novel information-theoretic measure, the Quadratic\nSpherical Mutual Information (QSMI). Apart from demonstrating the effectiveness\nof the proposed method under different scenarios and outperforming existing\nstate-of-the-art image hashing techniques, this paper provides a structured way\nto model the process of information retrieval and develop novel methods adapted\nto the needs of each application.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Supervised", "Neural-Hashing"], "tsne_embedding": [-7.046482563018799, -5.05056619644165], "cluster": 1}, {"key": "patel2019self", "year": "2019", "citations": "7", "title": "Self-supervised Visual Representations For Cross-modal Retrieval", "abstract": "<p>Cross-modal retrieval methods have been significantly improved in last years\nwith the use of deep neural networks and large-scale annotated datasets such as\nImageNet and Places. However, collecting and annotating such datasets requires\na tremendous amount of human effort and, besides, their annotations are usually\nlimited to discrete sets of popular visual classes that may not be\nrepresentative of the richer semantics found on large-scale cross-modal\nretrieval datasets. In this paper, we present a self-supervised cross-modal\nretrieval framework that leverages as training data the correlations between\nimages and text on the entire set of Wikipedia articles. Our method consists in\ntraining a CNN to predict: (1) the semantic context of the article in which an\nimage is more probable to appear as an illustration (global context), and (2)\nthe semantic context of its caption (local context). Our experiments\ndemonstrate that the proposed method is not only capable of learning\ndiscriminative visual representations for solving vision tasks like image\nclassification and object detection, but that the learned representations are\nbetter for cross-modal retrieval when compared to supervised pre-training of\nthe network on the ImageNet dataset.</p>\n", "tags": ["Self-Supervised", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [-49.64262771606445, -0.4634241461753845], "cluster": 0}, {"key": "patel2019tinysearch", "year": "2019", "citations": "7", "title": "Tinysearch -- Semantics Based Search Engine Using Bert Embeddings", "abstract": "<p>Existing search engines use keyword matching or tf-idf based matching to map\nthe query to the web-documents and rank them. They also consider other factors\nsuch as page rank, hubs-and-authority scores, knowledge graphs to make the\nresults more meaningful. However, the existing search engines fail to capture\nthe meaning of query when it becomes large and complex. BERT, introduced by\nGoogle in 2018, provides embeddings for words as well as sentences. In this\npaper, I have developed a semantics-oriented search engine using neural\nnetworks and BERT embeddings that can search for query and rank the documents\nin the order of the most meaningful to least meaningful. The results shows\nimprovement over one existing search engine for complex queries for given set\nof documents.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [8.323100090026855, -27.947874069213867], "cluster": 7}, {"key": "patel2021recall", "year": "2022", "citations": "29", "title": "Recall@k Surrogate Loss With Large Batches And Similarity Mixup", "abstract": "<p>This work focuses on learning deep visual representation models for retrieval\nby exploring the interplay between a new loss function, the batch size, and a\nnew regularization approach. Direct optimization, by gradient descent, of an\nevaluation metric, is not possible when it is non-differentiable, which is the\ncase for recall in retrieval. A differentiable surrogate loss for the recall is\nproposed in this work. Using an implementation that sidesteps the hardware\nconstraints of the GPU memory, the method trains with a very large batch size,\nwhich is essential for metrics computed on the entire retrieval database. It is\nassisted by an efficient mixup regularization approach that operates on\npairwise scalar similarities and virtually increases the batch size further.\nThe suggested method achieves state-of-the-art performance in several image\nretrieval benchmarks when used for deep metric learning. For instance-level\nrecognition, the method outperforms similar approaches that train using an\napproximation of average precision.</p>\n", "tags": ["CVPR", "Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-11.927597999572754, -9.74929428100586], "cluster": 1}, {"key": "patel2024acorn", "year": "2024", "citations": "13", "title": "ACORN: Performant And Predicate-agnostic Search Over Vector Embeddings And Structured Data", "abstract": "<p>Applications increasingly leverage mixed-modality data, and must jointly\nsearch over vector data, such as embedded images, text and video, as well as\nstructured data, such as attributes and keywords. Proposed methods for this\nhybrid search setting either suffer from poor performance or support a severely\nrestricted set of search predicates (e.g., only small sets of equality\npredicates), making them impractical for many applications. To address this, we\npresent ACORN, an approach for performant and predicate-agnostic hybrid search.\nACORN builds on Hierarchical Navigable Small Worlds (HNSW), a state-of-the-art\ngraph-based approximate nearest neighbor index, and can be implemented\nefficiently by extending existing HNSW libraries. ACORN introduces the idea of\npredicate subgraph traversal to emulate a theoretically ideal, but impractical,\nhybrid search strategy. ACORN\u2019s predicate-agnostic construction algorithm is\ndesigned to enable this effective search strategy, while supporting a wide\narray of predicate sets and query semantics. We systematically evaluate ACORN\non both prior benchmark datasets, with simple, low-cardinality predicate sets,\nand complex multi-modal datasets not supported by prior methods. We show that\nACORN achieves state-of-the-art performance on all datasets, outperforming\nprior methods with 2-1,000x higher throughput at a fixed recall.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Datasets"], "tsne_embedding": [48.86330032348633, 6.718629837036133], "cluster": 9}, {"key": "patel2024three", "year": "2024", "citations": "0", "title": "Three Things To Know About Deep Metric Learning", "abstract": "<p>This paper addresses supervised deep metric learning for open-set image\nretrieval, focusing on three key aspects: the loss function, mixup\nregularization, and model initialization. In deep metric learning, optimizing\nthe retrieval evaluation metric, recall@k, via gradient descent is desirable\nbut challenging due to its non-differentiable nature. To overcome this, we\npropose a differentiable surrogate loss that is computed on large batches,\nnearly equivalent to the entire training set. This computationally intensive\nprocess is made feasible through an implementation that bypasses the GPU memory\nlimitations. Additionally, we introduce an efficient mixup regularization\ntechnique that operates on pairwise scalar similarities, effectively increasing\nthe batch size even further. The training process is further enhanced by\ninitializing the vision encoder using foundational models, which are\npre-trained on large-scale datasets. Through a systematic study of these\ncomponents, we demonstrate that their synergy enables large models to nearly\nsolve popular benchmarks.</p>\n", "tags": ["Distance-Metric-Learning", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-11.909890174865723, -9.47692584991455], "cluster": 1}, {"key": "patra2017boostjet", "year": "2017", "citations": "0", "title": "Boostjet: Towards Combining Statistical Aggregates With Neural Embeddings For Recommendations", "abstract": "<p>Recommenders have become widely popular in recent years because of their\nbroader applicability in many e-commerce applications. These applications rely\non recommenders for generating advertisements for various offers or providing\ncontent recommendations. However, the quality of the generated recommendations\ndepends on user features (like demography, temporality), offer features (like\npopularity, price), and user-offer features (like implicit or explicit\nfeedback). Current state-of-the-art recommenders do not explore such diverse\nfeatures concurrently while generating the recommendations.\n  In this paper, we first introduce the notion of Trackers which enables us to\ncapture the above-mentioned features and thus incorporate users\u2019 online\nbehaviour through statistical aggregates of different features (demography,\ntemporality, popularity, price). We also show how to capture offer-to-offer\nrelations, based on their consumption sequence, leveraging neural embeddings\nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel\nrecommender which integrates the Trackers along with the neural embeddings\nusing MatrixNet, an efficient distributed implementation of gradient boosted\ndecision tree, to improve the recommendation quality significantly. We provide\nan in-depth evaluation of BoostJet on Yandex\u2019s dataset, collecting online\nbehaviour from tens of millions of online users, to demonstrate the\npracticality of BoostJet in terms of recommendation quality as well as\nscalability.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [26.548086166381836, -27.724239349365234], "cluster": 7}, {"key": "paudel2024pixelmod", "year": "2024", "citations": "0", "title": "PIXELMOD: Improving Soft Moderation Of Visual Misleading Information On Twitter", "abstract": "<p>Images are a powerful and immediate vehicle to carry misleading or outright\nfalse messages, yet identifying image-based misinformation at scale poses\nunique challenges. In this paper, we present PIXELMOD, a system that leverages\nperceptual hashes, vector databases, and optical character recognition (OCR) to\nefficiently identify images that are candidates to receive soft moderation\nlabels on Twitter. We show that PIXELMOD outperforms existing image similarity\napproaches when applied to soft moderation, with negligible performance\noverhead. We then test PIXELMOD on a dataset of tweets surrounding the 2020 US\nPresidential Election, and find that it is able to identify visually misleading\nimages that are candidates for soft moderation with 0.99% false detection and\n2.06% false negatives.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-9.010699272155762, -34.40388488769531], "cluster": 3}, {"key": "pauleve2010locality", "year": "2010", "citations": "299", "title": "Locality Sensitive Hashing: A Comparison Of Hash Function Types And Querying Mechanisms", "abstract": "<p>It is well known that high-dimensional nearest-neighbor retrieval is very expensive. Dramatic performance gains are obtained using\napproximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to\naddress the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector\nspace. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting\nits performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when\nsearching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical\nk-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space.\nWe then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their\nrespective merits and limitations.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [18.886825561523438, 32.56930160522461], "cluster": 4}, {"key": "pauleve2025locality", "year": "2010", "citations": "299", "title": "Locality Sensitive Hashing: A Comparison Of Hash Function Types And Querying Mechanisms", "abstract": "<p>It is well known that high-dimensional nearest-neighbor retrieval is very expensive. Dramatic performance gains are obtained using\napproximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to\naddress the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector\nspace. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting\nits performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when\nsearching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical\nk-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space.\nWe then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their\nrespective merits and limitations.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [18.886606216430664, 32.56965637207031], "cluster": 4}, {"key": "paulin2016convolutional", "year": "2016", "citations": "62", "title": "Convolutional Patch Representations For Image Retrieval: An Unsupervised Approach", "abstract": "<p>Convolutional neural networks (CNNs) have recently received a lot of\nattention due to their ability to model local stationary structures in natural\nimages in a multi-scale fashion, when learning all model parameters with\nsupervision. While excellent performance was achieved for image classification\nwhen large amounts of labeled visual data are available, their success for\nun-supervised tasks such as image retrieval has been moderate so far. Our paper\nfocuses on this latter setting and explores several methods for learning patch\ndescriptors without supervision with application to matching and instance-level\nretrieval. To that effect, we propose a new family of convolutional descriptors\nfor patch representation , based on the recently introduced convolutional\nkernel networks. We show that our descriptor, named Patch-CKN, performs better\nthan SIFT as well as other convolutional networks learned by artificially\nintroducing supervision and is significantly faster to train. To demonstrate\nits effectiveness, we perform an extensive evaluation on standard benchmarks\nfor patch and image retrieval where we obtain state-of-the-art results. We also\nintroduce a new dataset called RomePatches, which allows to simultaneously\nstudy descriptor performance for patch and image retrieval.</p>\n", "tags": ["Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-49.01262664794922, 2.963635206222534], "cluster": 0}, {"key": "peer2023towards", "year": "2023", "citations": "7", "title": "Towards Writer Retrieval For Historical Datasets", "abstract": "<p>This paper presents an unsupervised approach for writer retrieval based on\nclustering SIFT descriptors detected at keypoint locations resulting in\npseudo-cluster labels. With those cluster labels, a residual network followed\nby our proposed NetRVLAD, an encoding layer with reduced complexity compared to\nNetVLAD, is trained on 32x32 patches at keypoint locations. Additionally, we\nsuggest a graph-based reranking algorithm called SGR to exploit similarities of\nthe page embeddings to boost the retrieval performance. Our approach is\nevaluated on two historical datasets (Historical-WI and HisIR19). We include an\nevaluation of different backbones and NetRVLAD. It competes with related work\non historical datasets without using explicit encodings. We set a new\nState-of-the-art on both datasets by applying our reranking scheme and show\nthat our approach achieves comparable performance on a modern dataset as well.</p>\n", "tags": ["Graph-Based-Ann", "Evaluation", "Unsupervised", "Datasets"], "tsne_embedding": [37.78732681274414, -19.83106231689453], "cluster": 7}, {"key": "pei2021vision", "year": "2023", "citations": "4", "title": "Vision Transformer Based Video Hashing Retrieval For Tracing The Source Of Fake Videos", "abstract": "<p>In recent years, the spread of fake videos has brought great influence on\nindividuals and even countries. It is important to provide robust and reliable\nresults for fake videos. The results of conventional detection methods are not\nreliable and not robust for unseen videos. Another alternative and more\neffective way is to find the original video of the fake video. For example,\nfake videos from the Russia-Ukraine war and the Hong Kong law revision storm\nare refuted by finding the original video. We use an improved retrieval method\nto find the original video, named ViTHash. Specifically, tracing the source of\nfake videos requires finding the unique one, which is difficult when there are\nonly small differences in the original videos. To solve the above problems, we\ndesigned a novel loss Hash Triplet Loss. In addition, we designed a tool called\nLocalizator to compare the difference between the original traced video and the\nfake video. We have done extensive experiments on FaceForensics++, Celeb-DF and\nDeepFakeDetection, and we also have done additional experiments on our built\nthree datasets: DAVIS2016-TL (video inpainting), VSTL (video splicing) and DFTL\n(similar videos). Experiments have shown that our performance is better than\nstate-of-the-art methods, especially in cross-dataset mode. Experiments also\ndemonstrated that ViTHash is effective in various forgery detection: video\ninpainting, video splicing and deepfakes. Our code and datasets have been\nreleased on GitHub: https://github.com/lajlksdf/vtl.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-1.1356617212295532, 13.325101852416992], "cluster": 8}, {"key": "peng2017modality", "year": "2018", "citations": "141", "title": "Modality-specific Cross-modal Similarity Measurement With Recurrent Attention Network", "abstract": "<p>Nowadays, cross-modal retrieval plays an indispensable role to flexibly find\ninformation across different modalities of data. Effectively measuring the\nsimilarity between different modalities of data is the key of cross-modal\nretrieval. Different modalities such as image and text have imbalanced and\ncomplementary relationships, which contain unequal amount of information when\ndescribing the same semantics. For example, images often contain more details\nthat cannot be demonstrated by textual descriptions and vice versa. Existing\nworks based on Deep Neural Network (DNN) mostly construct one common space for\ndifferent modalities to find the latent alignments between them, which lose\ntheir exclusive modality-specific characteristics. Different from the existing\nworks, we propose modality-specific cross-modal similarity measurement (MCSM)\napproach by constructing independent semantic space for each modality, which\nadopts end-to-end framework to directly generate modality-specific cross-modal\nsimilarity without explicit common representation. For each semantic space,\nmodality-specific characteristics within one modality are fully exploited by\nrecurrent attention network, while the data of another modality is projected\ninto this space with attention based joint embedding to utilize the learned\nattention weights for guiding the fine-grained cross-modal correlation\nlearning, which can capture the imbalanced and complementary relationships\nbetween different modalities. Finally, the complementarity between the semantic\nspaces for different modalities is explored by adaptive fusion of the\nmodality-specific cross-modal similarities to perform cross-modal retrieval.\nExperiments on the widely-used Wikipedia and Pascal Sentence datasets as well\nas our constructed large-scale XMediaNet dataset verify the effectiveness of\nour proposed approach, outperforming 9 state-of-the-art methods.</p>\n", "tags": ["Multimodal-Retrieval", "Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [-20.515296936035156, -5.894977569580078], "cluster": 1}, {"key": "peng2018deep", "year": "2019", "citations": "44", "title": "Deep Reinforcement Learning For Image Hashing", "abstract": "<p>Deep hashing methods have received much attention recently, which achieve\npromising results by taking advantage of the strong representation power of\ndeep networks. However, most existing deep hashing methods learn a whole set of\nhashing functions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accuracy greatly.\nInspired by the sequential decision ability of deep reinforcement learning, we\npropose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).\nOur proposed DRLIH approach models the hashing learning problem as a sequential\ndecision process, which learns each hashing function by correcting the errors\nimposed by previous ones and promotes retrieval accuracy. To the best of our\nknowledge, this is the first work to address hashing problem from deep\nreinforcement learning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose a deep\nreinforcement learning hashing network. In the proposed network, we utilize\nrecurrent neural network (RNN) as agents to model the hashing functions, which\ntake actions of projecting images into binary codes sequentially, so that the\ncurrent hashing function learning can take previous hashing functions\u2019 error\ninto account. (2) We propose a sequential learning strategy based on proposed\nDRLIH. We define the state as a tuple of internal features of RNN\u2019s hidden\nlayers and image features, which can reflect history decisions made by the\nagents. We also propose an action group method to enhance the correlation of\nhash functions in the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Datasets", "Compact-Codes", "Neural-Hashing"], "tsne_embedding": [-0.1714531034231186, 2.7129456996917725], "cluster": 6}, {"key": "peng2023embedding", "year": "2023", "citations": "14", "title": "Embedding-based Retrieval With LLM For Effective Agriculture Information Extracting From Unstructured Data", "abstract": "<p>Pest identification is a crucial aspect of pest control in agriculture.\nHowever, most farmers are not capable of accurately identifying pests in the\nfield, and there is a limited number of structured data sources available for\nrapid querying. In this work, we explored using domain-agnostic general\npre-trained large language model(LLM) to extract structured data from\nagricultural documents with minimal or no human intervention. We propose a\nmethodology that involves text retrieval and filtering using embedding-based\nretrieval, followed by LLM question-answering to automatically extract entities\nand attributes from the documents, and transform them into structured data. In\ncomparison to existing methods, our approach achieves consistently better\naccuracy in the benchmark while maintaining efficiency.</p>\n", "tags": ["Efficiency", "Evaluation", "Text-Retrieval"], "tsne_embedding": [-56.29182815551758, 21.58856964111328], "cluster": 0}, {"key": "peng2025range", "year": "2025", "citations": "0", "title": "Range And Bird's Eye View Fused Cross-modal Visual Place Recognition", "abstract": "<p>Image-to-point cloud cross-modal Visual Place Recognition (VPR) is a\nchallenging task where the query is an RGB image, and the database samples are\nLiDAR point clouds. Compared to single-modal VPR, this approach benefits from\nthe widespread availability of RGB cameras and the robustness of point clouds\nin providing accurate spatial geometry and distance information. However,\ncurrent methods rely on intermediate modalities that capture either the\nvertical or horizontal field of view, limiting their ability to fully exploit\nthe complementary information from both sensors. In this work, we propose an\ninnovative initial retrieval + re-rank method that effectively combines\ninformation from range (or RGB) images and Bird\u2019s Eye View (BEV) images. Our\napproach relies solely on a computationally efficient global descriptor\nsimilarity search process to achieve re-ranking. Additionally, we introduce a\nnovel similarity label supervision technique to maximize the utility of limited\ntraining data. Specifically, we employ points average distance to approximate\nappearance similarity and incorporate an adaptive margin, based on similarity\ndifferences, into the vanilla triplet loss. Experimental results on the KITTI\ndataset demonstrate that our method significantly outperforms state-of-the-art\napproaches.</p>\n", "tags": ["Distance-Metric-Learning", "Similarity-Search", "Robustness", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [-29.000160217285156, 9.360982894897461], "cluster": 0}, {"key": "pereira2017genetic", "year": "2017", "citations": "0", "title": "A Genetic Algorithm Approach For Imagerepresentation Learning Through Color Quantization", "abstract": "<p>Over the last decades, hand-crafted feature extractors have been used to\nencode image visual properties into feature vectors. Recently, data-driven\nfeature learning approaches have been successfully explored as alternatives for\nproducing more representative visual features. In this work, we combine both\nresearch venues, focusing on the color quantization problem. We propose two\ndata-driven approaches to learn image representations through the search for\noptimized quantization schemes, which lead to more effective feature extraction\nalgorithms and compact representations. Our strategy employs Genetic Algorithm,\na soft-computing apparatus successfully utilized in\nInformation-retrieval-related optimization problems. We hypothesize that\nchanging the quantization affects the quality of image description approaches,\nleading to effective and efficient representations. We evaluate our approaches\nin content-based image retrieval tasks, considering eight well-known datasets\nwith different visual properties. Results indicate that the approach focused on\nrepresentation effectiveness outperformed baselines in all tested scenarios.\nThe other approach, which also considers the size of created representations,\nproduced competitive results keeping or even reducing the dimensionality of\nfeature vectors up to 25%.</p>\n", "tags": ["Quantization", "Image-Retrieval", "Datasets"], "tsne_embedding": [-15.673436164855957, -10.429488182067871], "cluster": 1}, {"key": "petrovic2010streaming", "year": "2010", "citations": "565", "title": "Streaming First Story Detection With Application To Twitter", "abstract": "<p>With the recent rise in popularity and size of\nsocial media, there is a growing need for systems\nthat can extract useful information from\nthis amount of data. We address the problem\nof detecting new events from a stream of\nTwitter posts. To make event detection feasible\non web-scale corpora, we present an algorithm\nbased on locality-sensitive hashing which\nis able overcome the limitations of traditional\napproaches, while maintaining competitive results.\nIn particular, a comparison with a stateof-the-art\nsystem on the first story detection\ntask shows that we achieve over an order of\nmagnitude speedup in processing time, while\nretaining comparable performance. Event detection\nexperiments on a collection of 160 million\nTwitter posts show that celebrity deaths\nare the fastest spreading news on Twitter.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Large-Scale-Search", "Evaluation"], "tsne_embedding": [37.25666809082031, -32.922271728515625], "cluster": 7}, {"key": "petrovic2012using", "year": "2012", "citations": "91", "title": "Using Paraphrases For Improving First Story Detection In News And Twitter", "abstract": "<p>First story detection (FSD) involves identifying\nfirst stories about events from a continuous\nstream of documents. A major problem in this\ntask is the high degree of lexical variation in\ndocuments which makes it very difficult to detect\nstories that talk about the same event but\nexpressed using different words. We suggest\nusing paraphrases to alleviate this problem,\nmaking this the first work to use paraphrases\nfor FSD. We show a novel way of integrating\nparaphrases with locality sensitive hashing\n(LSH) in order to obtain an efficient FSD system\nthat can scale to very large datasets. Our\nsystem achieves state-of-the-art results on the\nfirst story detection task, beating both the best\nsupervised and unsupervised systems. To test\nour approach on large data, we construct a corpus\nof events for Twitter, consisting of 50 million\ndocuments, and show that paraphrasing is\nalso beneficial in this domain.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [36.59508514404297, -34.22844696044922], "cluster": 7}, {"key": "petrovic2025streaming", "year": "2010", "citations": "565", "title": "Streaming First Story Detection With Application To Twitter", "abstract": "<p>With the recent rise in popularity and size of\nsocial media, there is a growing need for systems\nthat can extract useful information from\nthis amount of data. We address the problem\nof detecting new events from a stream of\nTwitter posts. To make event detection feasible\non web-scale corpora, we present an algorithm\nbased on locality-sensitive hashing which\nis able overcome the limitations of traditional\napproaches, while maintaining competitive results.\nIn particular, a comparison with a stateof-the-art\nsystem on the first story detection\ntask shows that we achieve over an order of\nmagnitude speedup in processing time, while\nretaining comparable performance. Event detection\nexperiments on a collection of 160 million\nTwitter posts show that celebrity deaths\nare the fastest spreading news on Twitter.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Large-Scale-Search", "Evaluation"], "tsne_embedding": [37.25666809082031, -32.922271728515625], "cluster": 7}, {"key": "petrovic2025using", "year": "2012", "citations": "91", "title": "Using Paraphrases For Improving First Story Detection In News And Twitter", "abstract": "<p>First story detection (FSD) involves identifying\nfirst stories about events from a continuous\nstream of documents. A major problem in this\ntask is the high degree of lexical variation in\ndocuments which makes it very difficult to detect\nstories that talk about the same event but\nexpressed using different words. We suggest\nusing paraphrases to alleviate this problem,\nmaking this the first work to use paraphrases\nfor FSD. We show a novel way of integrating\nparaphrases with locality sensitive hashing\n(LSH) in order to obtain an efficient FSD system\nthat can scale to very large datasets. Our\nsystem achieves state-of-the-art results on the\nfirst story detection task, beating both the best\nsupervised and unsupervised systems. To test\nour approach on large data, we construct a corpus\nof events for Twitter, consisting of 50 million\ndocuments, and show that paraphrasing is\nalso beneficial in this domain.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [36.59508514404297, -34.22844696044922], "cluster": 7}, {"key": "pham2016scalability", "year": "2016", "citations": "11", "title": "Scalability And Total Recall With Fast Coveringlsh", "abstract": "<p>Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic\ntechnique for similarity search with strong performance guarantees in\nhigh-dimensional spaces. A drawback of traditional LSH schemes is that they may\nhave <em>false negatives</em>, i.e., the recall is less than 100%. This limits\nthe applicability of LSH in settings requiring precise performance guarantees.\nBuilding on the recent theoretical \u201cCoveringLSH\u201d construction that eliminates\nfalse negatives, we propose a fast and practical covering LSH scheme for\nHamming space called <em>Fast CoveringLSH (fcLSH)</em>. Inheriting the design\nbenefits of CoveringLSH our method avoids false negatives and always reports\nall near neighbors. Compared to CoveringLSH we achieve an asymptotic\nimprovement to the hash function computation time from \\(\\mathcal{O}(dL)\\) to\n\\(\\mathcal{O}(d + Llog{L})\\), where \\(d\\) is the dimensionality of data and \\(L\\) is\nthe number of hash tables. Our experiments on synthetic and real-world data\nsets demonstrate that <em>fcLSH</em> is comparable (and often superior) to\ntraditional hashing-based approaches for search radius up to 20 in\nhigh-dimensional Hamming space.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "CIKM", "Similarity-Search", "Scalability", "Evaluation"], "tsne_embedding": [17.513521194458008, 38.22568893432617], "cluster": 4}, {"key": "pham2021chef", "year": "2021", "citations": "13", "title": "CHEF: Cross-modal Hierarchical Embeddings For Food Domain Retrieval", "abstract": "<p>Despite the abundance of multi-modal data, such as image-text pairs, there\nhas been little effort in understanding the individual entities and their\ndifferent roles in the construction of these data instances. In this work, we\nendeavour to discover the entities and their corresponding importance in\ncooking recipes automaticall} as a visual-linguistic association problem. More\nspecifically, we introduce a novel cross-modal learning framework to jointly\nmodel the latent representations of images and text in the food image-recipe\nassociation and retrieval tasks. This model allows one to discover complex\nfunctional and hierarchical relationships between images and text, and among\ntextual parts of a recipe including title, ingredients and cooking\ninstructions. Our experiments show that by making use of efficient\ntree-structured Long Short-Term Memory as the text encoder in our computational\ncross-modal retrieval framework, we are not only able to identify the main\ningredients and cooking actions in the recipe descriptions without explicit\nsupervision, but we can also learn more meaningful feature representations of\nfood recipes, appropriate for challenging cross-modal retrieval and recipe\nadaption tasks.</p>\n", "tags": ["AAAI", "Multimodal-Retrieval", "Tools-&-Libraries"], "tsne_embedding": [-42.270408630371094, 32.13358688354492], "cluster": 0}, {"key": "pham2022falconn", "year": "2022", "citations": "1", "title": "Falconn++: A Locality-sensitive Filtering Approach For Approximate Nearest Neighbor Search", "abstract": "<p>We present Falconn++, a novel locality-sensitive filtering approach for\napproximate nearest neighbor search on angular distance. Falconn++ can filter\nout potential far away points in any hash bucket \\textit{before} querying,\nwhich results in higher quality candidates compared to other hashing-based\nsolutions. Theoretically, Falconn++ asymptotically achieves lower query time\ncomplexity than Falconn, an optimal locality-sensitive hashing scheme on\nangular distance. Empirically, Falconn++ achieves higher recall-speed tradeoffs\nthan Falconn on many real-world data sets. Falconn++ is also competitive with\nHNSW, an efficient representative of graph-based solutions on high search\nrecall regimes.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Evaluation", "Hashing-Methods"], "tsne_embedding": [45.82224655151367, 22.25062370300293], "cluster": 2}, {"key": "pishdad2022uncertainty", "year": "2022", "citations": "0", "title": "Uncertainty-based Cross-modal Retrieval With Probabilistic Representations", "abstract": "<p>Probabilistic embeddings have proven useful for capturing polysemous word\nmeanings, as well as ambiguity in image matching. In this paper, we study the\nadvantages of probabilistic embeddings in a cross-modal setting (i.e., text and\nimages), and propose a simple approach that replaces the standard vector point\nembeddings in extant image-text matching models with probabilistic\ndistributions that are parametrically learned. Our guiding hypothesis is that\nthe uncertainty encoded in the probabilistic embeddings captures the\ncross-modal ambiguity in the input instances, and that it is through capturing\nthis uncertainty that the probabilistic models can perform better at downstream\ntasks, such as image-to-text or text-to-image retrieval. Through extensive\nexperiments on standard and new benchmarks, we show a consistent advantage for\nprobabilistic representations in cross-modal retrieval, and validate the\nability of our embeddings to capture uncertainty.</p>\n", "tags": ["Multimodal-Retrieval", "Image-Retrieval"], "tsne_embedding": [-23.655344009399414, -8.31640625], "cluster": 1}, {"key": "pisov2018brain", "year": "2018", "citations": "5", "title": "Brain Tumor Image Retrieval Via Multitask Learning", "abstract": "<p>Classification-based image retrieval systems are built by training\nconvolutional neural networks (CNNs) on a relevant classification problem and\nusing the distance in the resulting feature space as a similarity metric.\nHowever, in practical applications, it is often desirable to have\nrepresentations which take into account several aspects of the data (e.g.,\nbrain tumor type and its localization). In our work, we extend the\nclassification-based approach with multitask learning: we train a CNN on brain\nMRI scans with heterogeneous labels and implement a corresponding tumor image\nretrieval system. We validate our approach on brain tumor data which contains\ninformation about tumor types, shapes and localization. We show that our method\nallows us to build representations that contain more relevant information about\ntumors than single-task classification-based approaches.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-52.57135009765625, 11.1723051071167], "cluster": 0}, {"key": "plummer2018give", "year": "2019", "citations": "15", "title": "Give Me A Hint! Navigating Image Databases Using Human-in-the-loop Feedback", "abstract": "<p>In this paper, we introduce an attribute-based interactive image search which\ncan leverage human-in-the-loop feedback to iteratively refine image search\nresults. We study active image search where human feedback is solicited\nexclusively in visual form, without using relative attribute annotations used\nby prior work which are not typically found in many datasets. In order to\noptimize the image selection strategy, a deep reinforcement model is trained to\nlearn what images are informative rather than rely on hand-crafted measures\ntypically leveraged in prior work. Additionally, we extend the recently\nintroduced Conditional Similarity Network to incorporate global similarity in\ntraining visual embeddings, which results in more natural transitions as the\nuser explores the learned similarity embeddings. Our experiments demonstrate\nthe effectiveness of our approach, producing compelling results on both active\nimage search and image attribute representation tasks.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [-22.688005447387695, -28.544836044311523], "cluster": 5}, {"key": "podlesnaya2016deep", "year": "2017", "citations": "20", "title": "Deep Learning Based Semantic Video Indexing And Retrieval", "abstract": "<p>We share the implementation details and testing results for video retrieval\nsystem based exclusively on features extracted by convolutional neural\nnetworks. We show that deep learned features might serve as universal signature\nfor semantic content of video useful in many search and retrieval tasks. We\nfurther show that graph-based storage structure for video index allows to\nefficiently retrieving the content with complicated spatial and temporal search\nqueries.</p>\n", "tags": ["Graph-Based-Ann", "Video-Retrieval"], "tsne_embedding": [-26.129318237304688, -39.91995620727539], "cluster": 5}, {"key": "portaz2019image", "year": "2019", "citations": "10", "title": "Image Search Using Multilingual Texts: A Cross-modal Learning Approach Between Image And Text", "abstract": "<p>Multilingual (or cross-lingual) embeddings represent several languages in a\nunique vector space. Using a common embedding space enables for a shared\nsemantic between words from different languages. In this paper, we propose to\nembed images and texts into a unique distributional vector space, enabling to\nsearch images by using text queries expressing information needs related to the\n(visual) content of images, as well as using image similarity. Our framework\nforces the representation of an image to be similar to the representation of\nthe text that describes it. Moreover, by using multilingual embeddings we\nensure that words from two different languages have close descriptors and thus\nare attached to similar images. We provide experimental evidence of the\nefficiency of our approach by experimenting it on two datasets: Common Objects\nin COntext (COCO) [19] and Multi30K [7].</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-28.083913803100586, -31.193960189819336], "cluster": 5}, {"key": "portilloquintero2021straightforward", "year": "2021", "citations": "79", "title": "A Straightforward Framework For Video Retrieval Using CLIP", "abstract": "<p>Video Retrieval is a challenging task where a text query is matched to a\nvideo or vice versa. Most of the existing approaches for addressing such a\nproblem rely on annotations made by the users. Although simple, this approach\nis not always feasible in practice. In this work, we explore the application of\nthe language-image model, CLIP, to obtain video representations without the\nneed for said annotations. This model was explicitly trained to learn a common\nspace where images and text can be compared. Using various techniques described\nin this document, we extended its application to videos, obtaining\nstate-of-the-art results on the MSR-VTT and MSVD benchmarks.</p>\n", "tags": ["Tools-&-Libraries", "Video-Retrieval"], "tsne_embedding": [-39.23477554321289, -31.827396392822266], "cluster": 5}, {"key": "porwal2019learning", "year": "2019", "citations": "0", "title": "Learning Image Information For Ecommerce Queries", "abstract": "<p>Computing similarity between a query and a document is fundamental in any\ninformation retrieval system. In search engines, computing query-document\nsimilarity is an essential step in both retrieval and ranking stages. In eBay\nsearch, document is an item and the query-item similarity can be computed by\ncomparing different facets of the query-item pair. Query text can be compared\nwith the text of the item title. Likewise, a category constraint applied on the\nquery can be compared with the listing category of the item. However, images\nare one signal that are usually present in the items but are not present in the\nquery. Images are one of the most intuitive signals used by users to determine\nthe relevance of the item given a query. Including this signal in estimating\nsimilarity between the query-item pair is likely to improve the relevance of\nthe search engine. We propose a novel way of deriving image information for\nqueries. We attempt to learn image information for queries from item images\ninstead of generating explicit image features or an image for queries. We use\ncanonical correlation analysis (CCA) to learn a new subspace where projecting\nthe original data will give us a new query and item representation. We\nhypothesize that this new query representation will also have image information\nabout the query. We estimate the query-item similarity using a vector space\nmodel and report the performance of the proposed method on eBay\u2019s search data.\nWe show 11.89% relevance improvement over the baseline using area under the\nreceiver operating characteristic curve (AUROC) as the evaluation metric. We\nalso show 3.1% relevance improvement over the baseline with area under the\nprecision recall curve (AUPRC) .</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-30.439863204956055, 11.762188911437988], "cluster": 0}, {"key": "pound2025micronn", "year": "2025", "citations": "0", "title": "Micronn: An On-device Disk-resident Updatable Vector Database", "abstract": "<p>Nearest neighbour search over dense vector collections has important\napplications in information retrieval, retrieval augmented generation (RAG),\nand content ranking. Performing efficient search over large vector collections\nis a well studied problem with many existing approaches and open source\nimplementations. However, most state-of-the-art systems are generally targeted\ntowards scenarios using large servers with an abundance of memory, static\nvector collections that are not updatable, and nearest neighbour search in\nisolation of other search criteria. We present Micro Nearest Neighbour\n(MicroNN), an embedded nearest-neighbour vector search engine designed for\nscalable similarity search in low-resource environments. MicroNN addresses the\nproblem of on-device vector search for real-world workloads containing updates\nand hybrid search queries that combine nearest neighbour search with structured\nattribute filters. In this scenario, memory is highly constrained and\ndisk-efficient index structures and algorithms are required, as well as support\nfor continuous inserts and deletes. MicroNN is an embeddable library that can\nscale to large vector collections with minimal resources. MicroNN is used in\nproduction and powers a wide range of vector search use-cases on-device.\nMicroNN takes less than 7 ms to retrieve the top-100 nearest neighbours with\n90% recall on publicly available million-scale vector benchmark while using ~10\nMB of memory.</p>\n", "tags": ["Vector-Indexing", "Tools-&-Libraries", "Evaluation", "Similarity-Search"], "tsne_embedding": [35.374027252197266, 18.208677291870117], "cluster": 2}, {"key": "pratap2023minwise", "year": "2023", "citations": "2", "title": "Minwise-independent Permutations With Insertion And Deletion Of Features", "abstract": "<p>In their seminal work, Broder \\textit{et. al.}~\\citep{BroderCFM98} introduces\nthe \\(\\mathrm{minHash}\\) algorithm that computes a low-dimensional sketch of\nhigh-dimensional binary data that closely approximates pairwise Jaccard\nsimilarity. Since its invention, \\(\\mathrm{minHash}\\) has been commonly used by\npractitioners in various big data applications. Further, the data is dynamic in\nmany real-life scenarios, and their feature sets evolve over time. We consider\nthe case when features are dynamically inserted and deleted in the dataset. We\nnote that a naive solution to this problem is to repeatedly recompute\n\\(\\mathrm{minHash}\\) with respect to the updated dimension. However, this is an\nexpensive task as it requires generating fresh random permutations. To the best\nof our knowledge, no systematic study of \\(\\mathrm{minHash}\\) is recorded in the\ncontext of dynamic insertion and deletion of features. In this work, we\ninitiate this study and suggest algorithms that make the \\(\\mathrm{minHash}\\)\nsketches adaptable to the dynamic insertion and deletion of features. We show a\nrigorous theoretical analysis of our algorithms and complement it with\nextensive experiments on several real-world datasets. Empirically we observe a\nsignificant speed-up in the running time while simultaneously offering\ncomparable performance with respect to running \\(\\mathrm{minHash}\\) from scratch.\nOur proposal is efficient, accurate, and easy to implement in practice.</p>\n", "tags": ["Evaluation", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [28.11051368713379, 5.9147868156433105], "cluster": 2}, {"key": "pronobis2016sharing", "year": "2018", "citations": "1", "title": "Sharing Hash Codes For Multiple Purposes", "abstract": "<p>Locality sensitive hashing (LSH) is a powerful tool for sublinear-time\napproximate nearest neighbor search, and a variety of hashing schemes have been\nproposed for different dissimilarity measures. However, hash codes\nsignificantly depend on the dissimilarity, which prohibits users from adjusting\nthe dissimilarity at query time. In this paper, we propose {multiple purpose\nLSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH\nsupports L2, cosine, and inner product dissimilarities, and their corresponding\nweighted sums, where the weights can be adjusted at query time. It also allows\nus to modify the importance of pre-defined groups of features. Thus, mp-LSH\nenables us, for example, to retrieve similar items to a query with the user\npreference taken into account, to find a similar material to a query with some\nproperties (stability, utility, etc.) optimized, and to turn on or off a part\nof multi-modal information (brightness, color, audio, text, etc.) in\nimage/video retrieval. We theoretically and empirically analyze the performance\nof three variants of mp-LSH, and demonstrate their usefulness on real-world\ndata sets.</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [-13.435121536254883, 25.53534698486328], "cluster": 8}, {"key": "pu2025robust", "year": "2025", "citations": "0", "title": "Robust Self-paced Hashing For Cross-modal Retrieval With Noisy Labels", "abstract": "<p>Cross-modal hashing (CMH) has appeared as a popular technique for cross-modal\nretrieval due to its low storage cost and high computational efficiency in\nlarge-scale data. Most existing methods implicitly assume that multi-modal data\nis correctly labeled, which is expensive and even unattainable due to the\ninevitable imperfect annotations (i.e., noisy labels) in real-world scenarios.\nInspired by human cognitive learning, a few methods introduce self-paced\nlearning (SPL) to gradually train the model from easy to hard samples, which is\noften used to mitigate the effects of feature noise or outliers. It is a\nless-touched problem that how to utilize SPL to alleviate the misleading of\nnoisy labels on the hash model. To tackle this problem, we propose a new\ncognitive cross-modal retrieval method called Robust Self-paced Hashing with\nNoisy Labels (RSHNL), which can mimic the human cognitive process to identify\nthe noise while embracing robustness against noisy labels. Specifically, we\nfirst propose a contrastive hashing learning (CHL) scheme to improve\nmulti-modal consistency, thereby reducing the inherent semantic gap. Afterward,\nwe propose center aggregation learning (CAL) to mitigate the intra-class\nvariations. Finally, we propose Noise-tolerance Self-paced Hashing (NSH) that\ndynamically estimates the learning difficulty for each instance and\ndistinguishes noisy labels through the difficulty level. For all estimated\nclean pairs, we further adopt a self-paced regularizer to gradually learn hash\ncodes from easy to hard. Extensive experiments demonstrate that the proposed\nRSHNL performs remarkably well over the state-of-the-art CMH methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Robustness", "Multimodal-Retrieval", "Memory-Efficiency", "Neural-Hashing"], "tsne_embedding": [6.43256950378418, 20.368423461914062], "cluster": 8}, {"key": "qasemizadeh2017sketching", "year": "2017", "citations": "4", "title": "Sketching Word Vectors Through Hashing", "abstract": "<p>We propose a new fast word embedding technique using hash functions. The\nmethod is a derandomization of a new type of random projections: By\ndisregarding the classic constraint used in designing random projections (i.e.,\npreserving pairwise distances in a particular normed space), our solution\nexploits extremely sparse non-negative random projections. Our experiments show\nthat the proposed method can achieve competitive results, comparable to neural\nembedding learning techniques, however, with only a fraction of the\ncomputational complexity of these methods. While the proposed derandomization\nenhances the computational and space complexity of our method, the possibility\nof applying weighting methods such as positive pointwise mutual information\n(PPMI) to our models after their construction (and at a reduced dimensionality)\nimparts a high discriminatory power to the resulting embeddings. Obviously,\nthis method comes with other known benefits of random projection-based\ntechniques such as ease of update.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [15.087760925292969, 29.86886978149414], "cluster": 4}, {"key": "qi2017efficient", "year": "2017", "citations": "8", "title": "An Efficient Deep Learning Hashing Neural Network For Mobile Visual Search", "abstract": "<p>Mobile visual search applications are emerging that enable users to sense\ntheir surroundings with smart phones. However, because of the particular\nchallenges of mobile visual search, achieving a high recognition bitrate has\nbecomes a consistent target of previous related works. In this paper, we\npropose a few-parameter, low-latency, and high-accuracy deep hashing approach\nfor constructing binary hash codes for mobile visual search. First, we exploit\nthe architecture of the MobileNet model, which significantly decreases the\nlatency of deep feature extraction by reducing the number of model parameters\nwhile maintaining accuracy. Second, we add a hash-like layer into MobileNet to\ntrain the model on labeled mobile visual data. Evaluations show that the\nproposed system can exceed state-of-the-art accuracy performance in terms of\nthe MAP. More importantly, the memory consumption is much less than that of\nother deep learning models. The proposed method requires only \\(13\\) MB of memory\nfor the neural network and achieves a MAP of \\(97.80%\\) on the mobile location\nrecognition dataset used for testing.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [37.342933654785156, 6.109306335449219], "cluster": 9}, {"key": "qi2018accurate", "year": "2018", "citations": "1", "title": "Accurate And Efficient Similarity Search For Large Scale Face Recognition", "abstract": "<p>Face verification is a relatively easy task with the help of discriminative\nfeatures from deep neural networks. However, it is still a challenge to\nrecognize faces on millions of identities while keeping high performance and\nefficiency. The challenge 2 of MS-Celeb-1M is a classification task. However,\nthe number of identities is too large and it is not that elegant to treat the\ntask as an image classification task. We treat the classification task as\nsimilarity search and do experiments on different similarity search strategies.\nSimilarity search strategy accelerates the speed of searching and boosts the\naccuracy of final results. The model used for extracting features is a single\ndeep neural network pretrained on CASIA-Webface, which is not trained on the\nbase set or novel set offered by official. Finally, we rank \\textbf{3rd}, while\nthe speed of searching is 1ms/image.</p>\n", "tags": ["Efficiency", "Evaluation", "Similarity-Search"], "tsne_embedding": [-7.8466477394104, -6.727349281311035], "cluster": 1}, {"key": "qi2023lesion", "year": "2023", "citations": "0", "title": "Lesion Search With Self-supervised Learning", "abstract": "<p>Content-based image retrieval (CBIR) with self-supervised learning (SSL)\naccelerates clinicians\u2019 interpretation of similar images without manual\nannotations. We develop a CBIR from the contrastive learning SimCLR and\nincorporate a generalized-mean (GeM) pooling followed by L2 normalization to\nclassify lesion types and retrieve similar images before clinicians\u2019 analysis.\nResults have shown improved performance. We additionally build an open-source\napplication for image analysis and retrieval. The application is easy to\nintegrate, relieving manual efforts and suggesting the potential to support\nclinicians\u2019 everyday activities.</p>\n", "tags": ["Supervised", "Self-Supervised", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-41.1113395690918, -1.0701625347137451], "cluster": 0}, {"key": "qian2017differential", "year": "2017", "citations": "2", "title": "Differential Geometric Retrieval Of Deep Features", "abstract": "<p>Comparing images to recommend items from an image-inventory is a subject of\ncontinued interest. Added with the scalability of deep-learning architectures\nthe once `manual\u2019 job of hand-crafting features have been largely alleviated,\nand images can be compared according to features generated from a deep\nconvolutional neural network. In this paper, we compare distance metrics (and\ndivergences) to rank features generated from a neural network, for\ncontent-based image retrieval. Specifically, after modelling individual images\nusing approximations of mixture models or sparse covariance estimators, we\nresort to their information-theoretic and Riemann geometric comparisons. We\nshow that using approximations of mixture models enable us to compute a\ndistance measure based on the Wasserstein metric that requires less effort than\nother computationally intensive optimal transport plans; finally, an affine\ninvariant metric is used to compare the optimal transport metric to its Riemann\ngeometric counterpart \u2013 we conclude that although expensive, retrieval metric\nbased on Wasserstein geometry is more suitable than information theoretic\ncomparison of images. In short, we combine GPU scalability in learning deep\nfeature vectors with statistically efficient metrics that we foresee being\nutilised in a commercial setting.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [-14.344161033630371, -10.362288475036621], "cluster": 1}, {"key": "qiao2019deep", "year": "2019", "citations": "21", "title": "Deep Heterogeneous Hashing For Face Video Retrieval", "abstract": "<p>Retrieving videos of a particular person with face image as a query via\nhashing technique has many important applications. While face images are\ntypically represented as vectors in Euclidean space, characterizing face videos\nwith some robust set modeling techniques (e.g. covariance matrices as exploited\nin this study, which reside on Riemannian manifold), has recently shown\nappealing advantages. This hence results in a thorny heterogeneous spaces\nmatching problem. Moreover, hashing with handcrafted features as done in many\nexisting works is clearly inadequate to achieve desirable performance for this\ntask. To address such problems, we present an end-to-end Deep Heterogeneous\nHashing (DHH) method that integrates three stages including image feature\nlearning, video modeling, and heterogeneous hashing in a single framework, to\nlearn unified binary codes for both face images and videos. To tackle the key\nchallenge of hashing on the manifold, a well-studied Riemannian kernel mapping\nis employed to project data (i.e. covariance matrices) into Euclidean space and\nthus enables to embed the two heterogeneous representations into a common\nHamming space, where both intra-space discriminability and inter-space\ncompatibility are considered. To perform network optimization, the gradient of\nthe kernel mapping is innovatively derived via structured matrix\nbackpropagation in a theoretically principled way. Experiments on three\nchallenging datasets show that our method achieves quite competitive\nperformance compared with existing hashing methods.</p>\n", "tags": ["Hashing-Methods", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-2.9520111083984375, 10.729138374328613], "cluster": 8}, {"key": "qiu2017deep", "year": "2017", "citations": "97", "title": "Deep Semantic Hashing With Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest\nneighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can\nlead to high quality hashing. However, the cost of annotating\ndata is often an obstacle when applying supervised hashing\nto a new domain. Moreover, the results can suffer from the\nrobustness problem as the data at training and test stage\nmay come from different distributions. This paper studies\nthe exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which\nleverages largely unlabeled and limited labeled training data\nto produce highly compelling data with intrinsic invariance\nand global coherence, for better understanding statistical\nstructures of natural data. We demonstrate that the above\ntwo limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic\nhashing with GANs (DSH-GANs) is presented, which mainly\nconsists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary\nstream to distinguish synthetic images from real ones, a hash\nstream for encoding image representations to hash codes and\na classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss\nto correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the\ninput real-synthetic triplets and classification loss to classify\neach sample accurately. Extensive experiments conducted on\nboth CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our\nframework also achieves superior results when compared to\nstate-of-the-art deep hash models.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Image-Retrieval", "Robustness", "SIGIR", "Tools-&-Libraries", "Supervised", "Neural-Hashing"], "tsne_embedding": [0.9976481199264526, 7.1447224617004395], "cluster": 6}, {"key": "qiu2017foresthash", "year": "2018", "citations": "8", "title": "Foresthash: Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks", "abstract": "<p>Hash codes are efficient data representations for coping with the ever\ngrowing amounts of data. In this paper, we introduce a random forest semantic\nhashing scheme that embeds tiny convolutional neural networks (CNN) into\nshallow random forests, with near-optimal information-theoretic code\naggregation among trees. We start with a simple hashing scheme, where random\ntrees in a forest act as hashing functions by setting <code class=\"language-plaintext highlighter-rouge\">1' for the visited tree\nleaf, and </code>0\u2019 for the rest. We show that traditional random forests fail to\ngenerate hashes that preserve the underlying similarity between the trees,\nrendering the random forests approach to hashing challenging. To address this,\nwe propose to first randomly group arriving classes at each tree split node\ninto two groups, obtaining a significantly simplified two-class classification\nproblem, which can be handled using a light-weight CNN weak learner. Such\nrandom class grouping scheme enables code uniqueness by enforcing each class to\nshare its code with different classes in different trees. A non-conventional\nlow-rank loss is further adopted for the CNN weak learners to encourage code\nconsistency by minimizing intra-class variations and maximizing inter-class\ndistance for the two random class groups. Finally, we introduce an\ninformation-theoretic approach for aggregating codes of individual trees into a\nsingle hash code, producing a near-optimal unique hash for each class. The\nproposed approach significantly outperforms state-of-the-art hashing methods\nfor image retrieval tasks on large-scale public datasets, while performing at\nthe level of other state-of-the-art image classification techniques while\nutilizing a more compact and efficient scalable representation. This work\nproposes a principled and robust procedure to train and deploy in parallel an\nensemble of light-weight CNNs, instead of simply going deeper.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Image-Retrieval", "Datasets"], "tsne_embedding": [46.20314407348633, 0.049959808588027954], "cluster": 9}, {"key": "qiu2018deep", "year": "2017", "citations": "97", "title": "Deep Semantic Hashing With Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Image-Retrieval", "Robustness", "SIGIR", "Tools-&-Libraries", "Supervised", "Neural-Hashing"], "tsne_embedding": [1.0369136333465576, 7.137806415557861], "cluster": 6}, {"key": "qiu2021unsupervised", "year": "2021", "citations": "51", "title": "Unsupervised Hashing With Contrastive Information Bottleneck", "abstract": "<p>Many unsupervised hashing methods are implicitly established on the idea of\nreconstructing the input data, which basically encourages the hashing codes to\nretain as much information of original data as possible. However, this\nrequirement may force the models spending lots of their effort on\nreconstructing the unuseful background information, while ignoring to preserve\nthe discriminative semantic information that is more important for the hashing\ntask. To tackle this problem, inspired by the recent success of contrastive\nlearning in learning continuous representations, we propose to adapt this\nframework to learn binary hashing codes. Specifically, we first propose to\nmodify the objective function to meet the specific requirement of hashing and\nthen introduce a probabilistic binary representation layer into the model to\nfacilitate end-to-end training of the entire model. We further prove the strong\nconnection between the proposed contrastive-learning-based hashing method and\nthe mutual information, and show that the proposed model can be considered\nunder the broader framework of the information bottleneck (IB). Under this\nperspective, a more general hashing model is naturally obtained. Extensive\nexperimental results on three benchmark image datasets demonstrate that the\nproposed hashing method significantly outperforms existing baselines.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Tools-&-Libraries", "AAAI", "Hashing-Methods", "Supervised", "Unsupervised", "IJCAI"], "tsne_embedding": [15.920479774475098, -1.1210756301879883], "cluster": 6}, {"key": "qiu2022efficient", "year": "2022", "citations": "4", "title": "Efficient Document Retrieval By End-to-end Refining And Quantizing BERT Embedding With Contrastive Product Quantization", "abstract": "<p>Efficient document retrieval heavily relies on the technique of semantic\nhashing, which learns a binary code for every document and employs Hamming\ndistance to evaluate document distances. However, existing semantic hashing\nmethods are mostly established on outdated TFIDF features, which obviously do\nnot contain lots of important semantic information about documents.\nFurthermore, the Hamming distance can only be equal to one of several integer\nvalues, significantly limiting its representational ability for document\ndistances. To address these issues, in this paper, we propose to leverage BERT\nembeddings to perform efficient retrieval based on the product quantization\ntechnique, which will assign for every document a real-valued codeword from the\ncodebook, instead of a binary code as in semantic hashing. Specifically, we\nfirst transform the original BERT embeddings via a learnable mapping and feed\nthe transformed embedding into a probabilistic product quantization module to\noutput the assigned codeword. The refining and quantizing modules can be\noptimized in an end-to-end manner by minimizing the probabilistic contrastive\nloss. A mutual information maximization based method is further proposed to\nimprove the representativeness of codewords, so that documents can be quantized\nmore accurately. Extensive experiments conducted on three benchmarks\ndemonstrate that our proposed method significantly outperforms current\nstate-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods", "EMNLP", "Text-Retrieval", "Quantization", "Similarity-Search", "Compact-Codes"], "tsne_embedding": [11.975357055664062, -11.64004135131836], "cluster": 7}, {"key": "qiu2022pre", "year": "2022", "citations": "13", "title": "Pre-training Tasks For User Intent Detection And Embedding Retrieval In E-commerce Search", "abstract": "<p>BERT-style models pre-trained on the general corpus (e.g., Wikipedia) and\nfine-tuned on specific task corpus, have recently emerged as breakthrough\ntechniques in many NLP tasks: question answering, text classification, sequence\nlabeling and so on. However, this technique may not always work, especially for\ntwo scenarios: a corpus that contains very different text from the general\ncorpus Wikipedia, or a task that learns embedding spacial distribution for a\nspecific purpose (e.g., approximate nearest neighbor search). In this paper, to\ntackle the above two scenarios that we have encountered in an industrial\ne-commerce search system, we propose customized and novel pre-training tasks\nfor two critical modules: user intent detection and semantic embedding\nretrieval. The customized pre-trained models after fine-tuning, being less than\n10% of BERT-base\u2019s size in order to be feasible for cost-efficient CPU serving,\nsignificantly improve the other baseline models: 1) no pre-training model and\n2) fine-tuned model from the official pre-trained BERT using general corpus, on\nboth offline datasets and online system. We have open sourced our datasets for\nthe sake of reproducibility and future works.</p>\n", "tags": ["Datasets", "CIKM"], "tsne_embedding": [19.92432403564453, -32.014278411865234], "cluster": 7}, {"key": "qiu2024hihpq", "year": "2024", "citations": "5", "title": "Hihpq: Hierarchical Hyperbolic Product Quantization For Unsupervised Image Retrieval", "abstract": "<p>Existing unsupervised deep product quantization methods primarily aim for the\nincreased similarity between different views of the identical image, whereas\nthe delicate multi-level semantic similarities preserved between images are\noverlooked. Moreover, these methods predominantly focus on the Euclidean space\nfor computational convenience, compromising their ability to map the\nmulti-level semantic relationships between images effectively. To mitigate\nthese shortcomings, we propose a novel unsupervised product quantization method\ndubbed \\textbf{Hi}erarchical \\textbf{H}yperbolic \\textbf{P}roduct\n\\textbf{Q}uantization (HiHPQ), which learns quantized representations by\nincorporating hierarchical semantic similarity within hyperbolic geometry.\nSpecifically, we propose a hyperbolic product quantizer, where the hyperbolic\ncodebook attention mechanism and the quantized contrastive learning on the\nhyperbolic product manifold are introduced to expedite quantization.\nFurthermore, we propose a hierarchical semantics learning module, designed to\nenhance the distinction between similar and non-matching images for a query by\nutilizing the extracted hierarchical semantics as an additional training\nsupervision. Experiments on benchmarks show that our proposed method\noutperforms state-of-the-art baselines.</p>\n", "tags": ["Self-Supervised", "Quantization", "Image-Retrieval", "AAAI", "Evaluation", "Unsupervised"], "tsne_embedding": [-25.164752960205078, -9.15550422668457], "cluster": 1}, {"key": "qiu2025deep", "year": "2017", "citations": "97", "title": "Deep Semantic Hashing With Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest\nneighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can\nlead to high quality hashing. However, the cost of annotating\ndata is often an obstacle when applying supervised hashing\nto a new domain. Moreover, the results can suffer from the\nrobustness problem as the data at training and test stage\nmay come from different distributions. This paper studies\nthe exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which\nleverages largely unlabeled and limited labeled training data\nto produce highly compelling data with intrinsic invariance\nand global coherence, for better understanding statistical\nstructures of natural data. We demonstrate that the above\ntwo limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic\nhashing with GANs (DSH-GANs) is presented, which mainly\nconsists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary\nstream to distinguish synthetic images from real ones, a hash\nstream for encoding image representations to hash codes and\na classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss\nto correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the\ninput real-synthetic triplets and classification loss to classify\neach sample accurately. Extensive experiments conducted on\nboth CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our\nframework also achieves superior results when compared to\nstate-of-the-art deep hash models.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Scalability", "Image-Retrieval", "Robustness", "SIGIR", "Tools-&-Libraries", "Supervised", "Neural-Hashing"], "tsne_embedding": [0.9979009032249451, 7.144744873046875], "cluster": 6}, {"key": "qu2023learnable", "year": "2023", "citations": "12", "title": "Learnable Pillar-based Re-ranking For Image-text Retrieval", "abstract": "<p>Image-text retrieval aims to bridge the modality gap and retrieve cross-modal\ncontent based on semantic similarities. Prior work usually focuses on the\npairwise relations (i.e., whether a data sample matches another) but ignores\nthe higher-order neighbor relations (i.e., a matching structure among multiple\ndata samples). Re-ranking, a popular post-processing practice, has revealed the\nsuperiority of capturing neighbor relations in single-modality retrieval tasks.\nHowever, it is ineffective to directly extend existing re-ranking algorithms to\nimage-text retrieval. In this paper, we analyze the reason from four\nperspectives, i.e., generalization, flexibility, sparsity, and asymmetry, and\npropose a novel learnable pillar-based re-ranking paradigm. Concretely, we\nfirst select top-ranked intra- and inter-modal neighbors as pillars, and then\nreconstruct data samples with the neighbor relations between them and the\npillars. In this way, each sample can be mapped into a multimodal pillar space\nonly using similarities, ensuring generalization. After that, we design a\nneighbor-aware graph reasoning module to flexibly exploit the relations and\nexcavate the sparse positive items within a neighborhood. We also present a\nstructure alignment constraint to promote cross-modal collaboration and align\nthe asymmetric modalities. On top of various base backbones, we carry out\nextensive experiments on two benchmark datasets, i.e., Flickr30K and MS-COCO,\ndemonstrating the effectiveness, superiority, generalization, and\ntransferability of our proposed re-ranking paradigm.</p>\n", "tags": ["Text-Retrieval", "SIGIR", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [46.06153106689453, -2.4557087421417236], "cluster": 9}, {"key": "qu2024automated", "year": "2024", "citations": "0", "title": "Automated Similarity Metric Generation For Recommendation", "abstract": "<p>The embedding-based architecture has become the dominant approach in modern\nrecommender systems, mapping users and items into a compact vector space. It\nthen employs predefined similarity metrics, such as the inner product, to\ncalculate similarity scores between user and item embeddings, thereby guiding\nthe recommendation of items that align closely with a user\u2019s preferences. Given\nthe critical role of similarity metrics in recommender systems, existing\nmethods mainly employ handcrafted similarity metrics to capture the complex\ncharacteristics of user-item interactions. Yet, handcrafted metrics may not\nfully capture the diverse range of similarity patterns that can significantly\nvary across different domains.\n  To address this issue, we propose an Automated Similarity Metric Generation\nmethod for recommendations, named AutoSMG, which can generate tailored\nsimilarity metrics for various domains and datasets. Specifically, we first\nconstruct a similarity metric space by sampling from a set of basic embedding\noperators, which are then integrated into computational graphs to represent\nmetrics. We employ an evolutionary algorithm to search for the optimal metrics\nwithin this metric space iteratively. To improve search efficiency, we utilize\nan early stopping strategy and a surrogate model to approximate the performance\nof candidate metrics instead of fully training models. Notably, our proposed\nmethod is model-agnostic, which can seamlessly plugin into different\nrecommendation model architectures. The proposed method is validated on three\npublic recommendation datasets across various domains in the Top-K\nrecommendation task, and experimental results demonstrate that AutoSMG\noutperforms both commonly used handcrafted metrics and those generated by other\nsearch strategies.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Recommender-Systems", "Datasets", "Evaluation"], "tsne_embedding": [19.64470100402832, -28.820537567138672], "cluster": 7}, {"key": "qu2024tokenrec", "year": "2024", "citations": "0", "title": "Tokenrec: Learning To Tokenize ID For Llm-based Generative Recommendation", "abstract": "<p>There is a growing interest in utilizing large-scale language models (LLMs)\nto advance next-generation Recommender Systems (RecSys), driven by their\noutstanding language understanding and in-context learning capabilities. In\nthis scenario, tokenizing (i.e., indexing) users and items becomes essential\nfor ensuring a seamless alignment of LLMs with recommendations. While several\nstudies have made progress in representing users and items through textual\ncontents or latent representations, challenges remain in efficiently capturing\nhigh-order collaborative knowledge into discrete tokens that are compatible\nwith LLMs. Additionally, the majority of existing tokenization approaches often\nface difficulties in generalizing effectively to new/unseen users or items that\nwere not in the training corpus. To address these challenges, we propose a\nnovel framework called TokenRec, which introduces not only an effective ID\ntokenization strategy but also an efficient retrieval paradigm for LLM-based\nrecommendations. Specifically, our tokenization strategy, Masked\nVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item\nrepresentations learned from collaborative filtering into discrete tokens, thus\nachieving a smooth incorporation of high-order collaborative knowledge and a\ngeneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,\nour generative retrieval paradigm is designed to efficiently recommend top-\\(K\\)\nitems for users to eliminate the need for the time-consuming auto-regressive\ndecoding and beam search processes used by LLMs, thus significantly reducing\ninference time. Comprehensive experiments validate the effectiveness of the\nproposed methods, demonstrating that TokenRec outperforms competitive\nbenchmarks, including both traditional recommender systems and emerging\nLLM-based recommender systems.</p>\n", "tags": ["Tools-&-Libraries", "Recommender-Systems", "Similarity-Search", "Scalability"], "tsne_embedding": [21.672609329223633, -21.018884658813477], "cluster": 7}, {"key": "quedenfeld2017variant", "year": "2017", "citations": "4", "title": "Variant Tolerant Read Mapping Using Min-hashing", "abstract": "<p>DNA read mapping is a ubiquitous task in bioinformatics, and many tools have\nbeen developed to solve the read mapping problem. However, there are two trends\nthat are changing the landscape of readmapping: First, new sequencing\ntechnologies provide very long reads with high error rates (up to 15%). Second,\nmany genetic variants in the population are known, so the reference genome is\nnot considered as a single string over ACGT, but as a complex object containing\nthese variants. Most existing read mappers do not handle these new\ncircumstances appropriately.\n  We introduce a new read mapper prototype called VATRAM that considers\nvariants. It is based on Min-Hashing of q-gram sets of reference genome\nwindows. Min-Hashing is one form of locality sensitive hashing. The variants\nare directly inserted into VATRAMs index which leads to a fast mapping process.\nOur results show that VATRAM achieves better precision and recall than\nstate-of-the-art read mappers like BWA under certain cirumstances. VATRAM is\nopen source and can be accessed at\nhttps://bitbucket.org/Quedenfeld/vatram-src/.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [3.7040345668792725, 54.26174545288086], "cluster": 4}, {"key": "quinn2017semantic", "year": "2018", "citations": "12", "title": "Semantic Image Retrieval Via Active Grounding Of Visual Situations", "abstract": "<p>We describe a novel architecture for semantic image retrieval\u2014in\nparticular, retrieval of instances of visual situations. Visual situations are\nconcepts such as \u201ca boxing match,\u201d \u201cwalking the dog,\u201d \u201ca crowd waiting for a\nbus,\u201d or \u201ca game of ping-pong,\u201d whose instantiations in images are linked more\nby their common spatial and semantic structure than by low-level visual\nsimilarity. Given a query situation description, our architecture\u2014called\nSituate\u2014learns models capturing the visual features of expected objects as\nwell the expected spatial configuration of relationships among objects. Given a\nnew image, Situate uses these models in an attempt to ground (i.e., to create a\nbounding box locating) each expected component of the situation in the image\nvia an active search procedure. Situate uses the resulting grounding to compute\na score indicating the degree to which the new image is judged to contain an\ninstance of the situation. Such scores can be used to rank images in a\ncollection as part of a retrieval system. In the preliminary study described\nhere, we demonstrate the promise of this system by comparing Situate\u2019s\nperformance with that of two baseline methods, as well as with a related\nsemantic image-retrieval system based on \u201cscene graphs.\u201d</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-16.32612419128418, -23.283920288085938], "cluster": 5}, {"key": "rabbani2023large", "year": "2023", "citations": "0", "title": "Large-scale Distributed Learning Via Private On-device Locality-sensitive Hashing", "abstract": "<p>Locality-sensitive hashing (LSH) based frameworks have been used efficiently\nto select weight vectors in a dense hidden layer with high cosine similarity to\nan input, enabling dynamic pruning. While this type of scheme has been shown to\nimprove computational training efficiency, existing algorithms require repeated\nrandomized projection of the full layer weight, which is impractical for\ncomputational- and memory-constrained devices. In a distributed setting,\ndeferring LSH analysis to a centralized host is (i) slow if the device cluster\nis large and (ii) requires access to input data which is forbidden in a\nfederated context. Using a new family of hash functions, we develop one of the\nfirst private, personalized, and memory-efficient on-device LSH frameworks. Our\nframework enables privacy and personalization by allowing each device to\ngenerate hash tables, without the help of a central host, using device-specific\nhashing hyper-parameters (e.g. number of hash tables or hash length). Hash\ntables are generated with a compressed set of the full weights, and can be\nserially generated and discarded if the process is memory-intensive. This\nallows devices to avoid maintaining (i) the fully-sized model and (ii) large\namounts of hash tables in local memory for LSH analysis. We prove several\nstatistical and sensitivity properties of our hash functions, and\nexperimentally demonstrate that our framework is competitive in training\nlarge-scale recommender networks compared to other LSH frameworks which assume\nunrestricted on-device capacity.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Efficiency", "Recommender-Systems", "Scalability", "Tools-&-Libraries"], "tsne_embedding": [36.628353118896484, 3.686603307723999], "cluster": 9}, {"key": "radenovi\u01072016cnn", "year": "2016", "citations": "523", "title": "CNN Image Retrieval Learns From Bow: Unsupervised Fine-tuning With Hard Examples", "abstract": "<p>Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in\nmany computer vision tasks. However, this achievement is preceded by extreme\nmanual annotation in order to perform either training from scratch or\nfine-tuning for the target task. In this work, we propose to fine-tune CNN for\nimage retrieval from a large collection of unordered images in a fully\nautomated manner. We employ state-of-the-art retrieval and\nStructure-from-Motion (SfM) methods to obtain 3D models, which are used to\nguide the selection of the training data for CNN fine-tuning. We show that both\nhard positive and hard negative examples enhance the final performance in\nparticular object retrieval with compact codes.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [-49.07823181152344, 0.6564382910728455], "cluster": 0}, {"key": "radenovi\u01072017fine", "year": "2018", "citations": "1184", "title": "Fine-tuning CNN Image Retrieval With No Human Annotation", "abstract": "<p>Image descriptors based on activations of Convolutional Neural Networks\n(CNNs) have become dominant in image retrieval due to their discriminative\npower, compactness of representation, and search efficiency. Training of CNNs,\neither from scratch or fine-tuning, requires a large amount of annotated data,\nwhere a high quality of annotation is often crucial. In this work, we propose\nto fine-tune CNNs for image retrieval on a large collection of unordered images\nin a fully automated manner. Reconstructed 3D models obtained by the\nstate-of-the-art retrieval and structure-from-motion methods guide the\nselection of the training data. We show that both hard-positive and\nhard-negative examples, selected by exploiting the geometry and the camera\npositions available from the 3D models, enhance the performance of\nparticular-object retrieval. CNN descriptor whitening discriminatively learned\nfrom the same training data outperforms commonly used PCA whitening. We propose\na novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and\naverage pooling and show that it boosts retrieval performance. Applying the\nproposed method to the VGG network achieves state-of-the-art performance on the\nstandard benchmarks: Oxford Buildings, Paris, and Holidays datasets.</p>\n", "tags": ["Efficiency", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-49.27739715576172, 0.7601736187934875], "cluster": 0}, {"key": "radhakrishnan2021deep", "year": "2021", "citations": "3", "title": "Deep Metric Learning For Ground Images", "abstract": "<p>Ground texture based localization methods are potential prospects for\nlow-cost, high-accuracy self-localization solutions for robots. These methods\nestimate the pose of a given query image, i.e. the current observation of the\nground from a downward-facing camera, in respect to a set of reference images\nwhose poses are known in the application area. In this work, we deal with the\ninitial localization task, in which we have no prior knowledge about the\ncurrent robot positioning. In this situation, the localization method would\nhave to consider all available reference images. However, in order to reduce\ncomputational effort and the risk of receiving a wrong result, we would like to\nconsider only those reference images that are actually overlapping with the\nquery image. For this purpose, we propose a deep metric learning approach that\nretrieves the most similar reference images to the query image. In contrast to\nexisting approaches to image retrieval for ground images, our approach achieves\nsignificantly better recall performance and improves the localization\nperformance of a state-of-the-art ground texture based localization method.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-36.560115814208984, -9.529520034790039], "cluster": 5}, {"key": "raff2018toward", "year": "2018", "citations": "3", "title": "Toward Metric Indexes For Incremental Insertion And Querying", "abstract": "<p>In this work we explore the use of metric index structures, which accelerate\nnearest neighbor queries, in the scenario where we need to interleave\ninsertions and queries during deployment. This use-case is inspired by a\nreal-life need in malware analysis triage, and is surprisingly understudied.\nExisting literature tends to either focus on only final query efficiency, often\ndoes not support incremental insertion, or does not support arbitrary distance\nmetrics. We modify and improve three algorithms to support our scenario of\nincremental insertion and querying with arbitrary metrics, and evaluate them on\nmultiple datasets and distance metrics while varying the value of \\(k\\) for the\ndesired number of nearest neighbors. In doing so we determine that our improved\nVantage-Point tree of Minimum-Variance performs best for this scenario.</p>\n", "tags": ["Efficiency", "Vector-Indexing", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [24.99282455444336, 38.656982421875], "cluster": 4}, {"key": "rafiei2023class", "year": "2023", "citations": "1", "title": "Class-specific Variational Auto-encoder For Content-based Image Retrieval", "abstract": "<p>Using a discriminative representation obtained by supervised deep learning\nmethods showed promising results on diverse Content-Based Image Retrieval\n(CBIR) problems. However, existing methods exploiting labels during training\ntry to discriminate all available classes, which is not ideal in cases where\nthe retrieval problem focuses on a class of interest. In this paper, we propose\na regularized loss for Variational Auto-Encoders (VAEs) forcing the model to\nfocus on a given class of interest. As a result, the model learns to\ndiscriminate the data belonging to the class of interest from any other\npossibility, making the learnt latent space of the VAE suitable for\nclass-specific retrieval tasks. The proposed Class-Specific Variational\nAuto-Encoder (CS-VAE) is evaluated on three public and one custom datasets, and\nits performance is compared with that of three related VAE-based methods.\nExperimental results show that the proposed method outperforms its competition\nin both in-domain and out-of-domain retrieval problems.</p>\n", "tags": ["Supervised", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-14.454339027404785, -20.493711471557617], "cluster": 1}, {"key": "raginsky2009locality", "year": "2009", "citations": "633", "title": "Locality-sensitive Binary Codes From Shift-invariant Kernels", "abstract": "<p>This paper addresses the problem of designing binary codes for high-dimensional\ndata such that vectors that are similar in the original space map to similar binary\nstrings. We introduce a simple distribution-free encoding scheme based on\nrandom projections, such that the expected Hamming distance between the binary\ncodes of two vectors is related to the value of a shift-invariant kernel (e.g., a\nGaussian kernel) between the vectors. We present a full theoretical analysis of the\nconvergence properties of the proposed scheme, and report favorable experimental\nperformance as compared to a recent state-of-the-art method, spectral hashing.</p>\n", "tags": ["Compact-Codes", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [3.968275547027588, 39.83330154418945], "cluster": 4}, {"key": "raginsky2025locality", "year": "2009", "citations": "633", "title": "Locality-sensitive Binary Codes From Shift-invariant Kernels", "abstract": "<p>This paper addresses the problem of designing binary codes for high-dimensional\ndata such that vectors that are similar in the original space map to similar binary\nstrings. We introduce a simple distribution-free encoding scheme based on\nrandom projections, such that the expected Hamming distance between the binary\ncodes of two vectors is related to the value of a shift-invariant kernel (e.g., a\nGaussian kernel) between the vectors. We present a full theoretical analysis of the\nconvergence properties of the proposed scheme, and report favorable experimental\nperformance as compared to a recent state-of-the-art method, spectral hashing.</p>\n", "tags": ["Compact-Codes", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [3.968275547027588, 39.83330154418945], "cluster": 4}, {"key": "rahman2024optimizing", "year": "2024", "citations": "0", "title": "Optimizing Domain-specific Image Retrieval: A Benchmark Of FAISS And Annoy With Fine-tuned Features", "abstract": "<p>Approximate Nearest Neighbor search is one of the keys to high-scale data\nretrieval performance in many applications. The work is a bridge between\nfeature extraction and ANN indexing through fine-tuning a ResNet50 model with\nvarious ANN methods: FAISS and Annoy. We evaluate the systems with respect to\nindexing time, memory usage, query time, precision, recall, F1-score, and\nRecall@5 on a custom image dataset. FAISS\u2019s Product Quantization can achieve a\nprecision of 98.40% with low memory usage at 0.24 MB index size, and Annoy is\nthe fastest, with average query times of 0.00015 seconds, at a slight cost to\naccuracy. These results reveal trade-offs among speed, accuracy, and memory\nefficiency and offer actionable insights into the optimization of feature-based\nimage retrieval systems. This study will serve as a blueprint for constructing\nactual retrieval pipelines and be built on fine-tuned deep learning networks\nand associated ANN methods.</p>\n", "tags": ["Efficiency", "Quantization", "Vector-Indexing", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-13.750898361206055, -11.934969902038574], "cluster": 1}, {"key": "rahmani2023improving", "year": "2023", "citations": "0", "title": "Improving Code Example Recommendations On Informal Documentation Using BERT And Query-aware LSH: A Comparative Study", "abstract": "<p>Our research investigates the recommendation of code examples to aid software\ndevelopers, a practice that saves developers significant time by providing\nready-to-use code snippets. The focus of our study is Stack Overflow, a\ncommonly used resource for coding discussions and solutions, particularly in\nthe context of the Java programming language. We applied BERT, a powerful Large\nLanguage Model (LLM) that enables us to transform code examples into numerical\nvectors by extracting their semantic information. Once these numerical\nrepresentations are prepared, we identify Approximate Nearest Neighbors (ANN)\nusing Locality-Sensitive Hashing (LSH). Our research employed two variants of\nLSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared\nthese two approaches across four parameters: HitRate, Mean Reciprocal Rank\n(MRR), Average Execution Time, and Relevance. Our study revealed that the\nQuery-Aware (QA) approach showed superior performance over the Random\nHyperplane-based (RH) method. Specifically, it exhibited a notable improvement\nof 20% to 35% in HitRate for query pairs compared to the RH approach.\nFurthermore, the QA approach proved significantly more time-efficient, with its\nspeed in creating hashing tables and assigning data samples to buckets being at\nleast four times faster. It can return code examples within milliseconds,\nwhereas the RH approach typically requires several seconds to recommend code\nexamples. Due to the superior performance of the QA approach, we tested it\nagainst PostFinder and FaCoY, the state-of-the-art baselines. Our QA method\nshowed comparable efficiency proving its potential for effective code\nrecommendation.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Recommender-Systems", "Evaluation"], "tsne_embedding": [14.285676956176758, 12.3720064163208], "cluster": 6}, {"key": "rajput2023recommender", "year": "2023", "citations": "17", "title": "Recommender Systems With Generative Retrieval", "abstract": "<p>Modern recommender systems perform large-scale retrieval by first embedding\nqueries and item candidates in the same unified space, followed by approximate\nnearest neighbor search to select top candidates given a query embedding. In\nthis paper, we propose a novel generative retrieval approach, where the\nretrieval model autoregressively decodes the identifiers of the target\ncandidates. To that end, we create semantically meaningful tuple of codewords\nto serve as a Semantic ID for each item. Given Semantic IDs for items in a user\nsession, a Transformer-based sequence-to-sequence model is trained to predict\nthe Semantic ID of the next item that the user will interact with. To the best\nof our knowledge, this is the first Semantic ID-based generative model for\nrecommendation tasks. We show that recommender systems trained with the\nproposed paradigm significantly outperform the current SOTA models on various\ndatasets. In addition, we show that incorporating Semantic IDs into the\nsequence-to-sequence model enhances its ability to generalize, as evidenced by\nthe improved retrieval performance observed for items with no prior interaction\nhistory.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [21.291044235229492, -25.962520599365234], "cluster": 7}, {"key": "ram2021federated", "year": "2021", "citations": "1", "title": "Federated Nearest Neighbor Classification With A Colony Of Fruit-flies: With Supplement", "abstract": "<p>The mathematical formalization of a neurological mechanism in the olfactory\ncircuit of a fruit-fly as a locality sensitive hash (Flyhash) and bloom filter\n(FBF) has been recently proposed and \u201creprogrammed\u201d for various machine\nlearning tasks such as similarity search, outlier detection and text\nembeddings. We propose a novel reprogramming of this hash and bloom filter to\nemulate the canonical nearest neighbor classifier (NNC) in the challenging\nFederated Learning (FL) setup where training and test data are spread across\nparties and no data can leave their respective parties. Specifically, we\nutilize Flyhash and FBF to create the FlyNN classifier, and theoretically\nestablish conditions where FlyNN matches NNC. We show how FlyNN is trained\nexactly in a FL setup with low communication overhead to produce FlyNNFL, and\nhow it can be differentially private. Empirically, we demonstrate that (i)\nFlyNN matches NNC accuracy across 70 OpenML datasets, (ii) FlyNNFL training is\nhighly scalable with low communication overhead, providing up to \\(8\\times\\)\nspeedup with \\(16\\) parties.</p>\n", "tags": ["Efficiency", "Similarity-Search", "Datasets"], "tsne_embedding": [19.764814376831055, -7.635740756988525], "cluster": 6}, {"key": "raman2022structure", "year": "2022", "citations": "4", "title": "Structure And Semantics Preserving Document Representations", "abstract": "<p>Retrieving relevant documents from a corpus is typically based on the\nsemantic similarity between the document content and query text. The inclusion\nof structural relationship between documents can benefit the retrieval\nmechanism by addressing semantic gaps. However, incorporating these\nrelationships requires tractable mechanisms that balance structure with\nsemantics and take advantage of the prevalent pre-train/fine-tune paradigm. We\npropose here a holistic approach to learning document representations by\nintegrating intra-document content with inter-document relations. Our deep\nmetric learning solution analyzes the complex neighborhood structure in the\nrelationship network to efficiently sample similar/dissimilar document pairs\nand defines a novel quintuplet loss function that simultaneously encourages\ndocument pairs that are semantically relevant to be closer and structurally\nunrelated to be far apart in the representation space. Furthermore, the\nseparation margins between the documents are varied flexibly to encode the\nheterogeneity in relationship strengths. The model is fully fine-tunable and\nnatively supports query projection during inference. We demonstrate that it\noutperforms competing methods on multiple datasets for document retrieval\ntasks.</p>\n", "tags": ["SIGIR", "Distance-Metric-Learning", "Text-Retrieval", "Datasets"], "tsne_embedding": [6.168389320373535, -22.777673721313477], "cluster": 7}, {"key": "ranjan2020parallel", "year": "2020", "citations": "0", "title": "A Parallel Approach For Real-time Face Recognition From A Large Database", "abstract": "<p>We present a new facial recognition system, capable of identifying a person,\nprovided their likeness has been previously stored in the system, in real time.\nThe system is based on storing and comparing facial embeddings of the subject,\nand identifying them later within a live video feed. This system is highly\naccurate, and is able to tag people with their ID in real time. It is able to\ndo so, even when using a database containing thousands of facial embeddings, by\nusing a parallelized searching technique. This makes the system quite fast and\nallows it to be highly scalable.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [-13.777107238769531, 6.459587097167969], "cluster": 1}, {"key": "rashno2019content", "year": "2019", "citations": "14", "title": "Content-based Image Retrieval System With Most Relevant Features Among Wavelet And Color Features", "abstract": "<p>Content-based image retrieval (CBIR) has become one of the most important\nresearch directions in the domain of digital data management. In this paper, a\nnew feature extraction schema including the norm of low frequency components in\nwavelet transformation and color features in RGB and HSV domains are proposed\nas representative feature vector for images in database followed by appropriate\nsimilarity measure for each feature type. In CBIR systems, retrieving results\nare so sensitive to image features. We address this problem with selection of\nmost relevant features among complete feature set by ant colony optimization\n(ACO)-based feature selection which minimize the number of features as well as\nmaximize F-measure in CBIR system. To evaluate the performance of our proposed\nCBIR system, it has been compared with three older proposed systems. Results\nshow that the precision and recall of our proposed system are higher than older\nones for the majority of image categories in Corel database.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-28.358060836791992, 15.204350471496582], "cluster": 0}, {"key": "rashtchian2020lsf", "year": "2020", "citations": "5", "title": "Lsf-join: Locality Sensitive Filtering For Distributed All-pairs Set Similarity Under Skew", "abstract": "<p>All-pairs set similarity is a widely used data mining task, even for large\nand high-dimensional datasets. Traditionally, similarity search has focused on\ndiscovering very similar pairs, for which a variety of efficient algorithms are\nknown. However, recent work highlights the importance of finding pairs of sets\nwith relatively small intersection sizes. For example, in a recommender system,\ntwo users may be alike even though their interests only overlap on a small\npercentage of items. In such systems, some dimensions are often highly skewed\nbecause they are very popular. Together these two properties render previous\napproaches infeasible for large input sizes. To address this problem, we\npresent a new distributed algorithm, LSF-Join, for approximate all-pairs set\nsimilarity. The core of our algorithm is a randomized selection procedure based\non Locality Sensitive Filtering. Our method deviates from prior approximate\nalgorithms, which are based on Locality Sensitive Hashing. Theoretically, we\nshow that LSF-Join efficiently finds most close pairs, even for small\nsimilarity thresholds and for skewed input sets. We prove guarantees on the\ncommunication, work, and maximum load of LSF-Join, and we also experimentally\ndemonstrate its accuracy on multiple graphs.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "Similarity-Search", "Datasets"], "tsne_embedding": [23.0426025390625, 30.255956649780273], "cluster": 2}, {"key": "ravi2020buy", "year": "2021", "citations": "6", "title": "Buy Me That Look: An Approach For Recommending Similar Fashion Products", "abstract": "<p>Have you ever looked at an Instagram model, or a model in a fashion\ne-commerce web-page, and thought \\textit{\u201cWish I could get a list of fashion\nitems similar to the ones worn by the model!\u201d}. This is what we address in this\npaper, where we propose a novel computer vision based technique called\n\\textbf{ShopLook} to address the challenging problem of recommending similar\nfashion products. The proposed method has been evaluated at Myntra\n(www.myntra.com), a leading online fashion e-commerce platform. In particular,\ngiven a user query and the corresponding Product Display Page (PDP) against the\nquery, the goal of our method is to recommend similar fashion products\ncorresponding to the entire set of fashion articles worn by a model in the PDP\nfull-shot image (the one showing the entire model from head to toe). The\nnovelty and strength of our method lies in its capability to recommend similar\narticles for all the fashion items worn by the model, in addition to the\nprimary article corresponding to the query. This is not only important to\npromote cross-sells for boosting revenue, but also for improving customer\nexperience and engagement. In addition, our approach is also capable of\nrecommending similar products for User Generated Content (UGC), eg., fashion\narticle images uploaded by users. Formally, our proposed method consists of the\nfollowing components (in the same order): i) Human keypoint detection, ii) Pose\nclassification, iii) Article localisation and object detection, along with\nactive learning feedback, and iv) Triplet network based image embedding model.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-19.66244125366211, -51.03470993041992], "cluster": 3}, {"key": "razeghi2017privacy", "year": "2017", "citations": "25", "title": "Privacy Preserving Identification Using Sparse Approximation With Ambiguization", "abstract": "<p>In this paper, we consider a privacy preserving encoding framework for\nidentification applications covering biometrics, physical object security and\nthe Internet of Things (IoT). The proposed framework is based on a sparsifying\ntransform, which consists of a trained linear map, an element-wise\nnonlinearity, and privacy amplification. The sparsifying transform and privacy\namplification are not symmetric for the data owner and data user. We\ndemonstrate that the proposed approach is closely related to sparse ternary\ncodes (STC), a recent information-theoretic concept proposed for fast\napproximate nearest neighbor (ANN) search in high dimensional feature spaces\nthat being machine learning in nature also offers significant benefits in\ncomparison to sparse approximation and binary embedding approaches. We\ndemonstrate that the privacy of the database outsourced to a server as well as\nthe privacy of the data user are preserved at a low computational cost, storage\nand communication burdens.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Hashing-Methods"], "tsne_embedding": [13.900485038757324, 16.761245727539062], "cluster": 6}, {"key": "razeghi2021privacy", "year": "2021", "citations": "2", "title": "Privacy-preserving Near Neighbor Search Via Sparse Coding With Ambiguation", "abstract": "<p>In this paper, we propose a framework for privacy-preserving approximate near\nneighbor search via stochastic sparsifying encoding. The core of the framework\nrelies on sparse coding with ambiguation (SCA) mechanism that introduces the\nnotion of inherent shared secrecy based on the support intersection of sparse\ncodes. This approach is `fairness-aware\u2019, in the sense that any point in the\nneighborhood has an equiprobable chance to be chosen. Our approach can be\napplied to raw data, latent representation of autoencoders, and aggregated\nlocal descriptors. The proposed method is tested on both synthetic i.i.d data\nand real large-scale image databases.</p>\n", "tags": ["Tools-&-Libraries", "Scalability", "ICASSP"], "tsne_embedding": [-17.494991302490234, 29.130640029907227], "cluster": 8}, {"key": "reddy2022context", "year": "2023", "citations": "1", "title": "Context Unaware Knowledge Distillation For Image Retrieval", "abstract": "<p>Existing data-dependent hashing methods use large backbone networks with\nmillions of parameters and are computationally complex. Existing knowledge\ndistillation methods use logits and other features of the deep (teacher) model\nand as knowledge for the compact (student) model, which requires the teacher\u2019s\nnetwork to be fine-tuned on the context in parallel with the student model on\nthe context. Training teacher on the target context requires more time and\ncomputational resources. In this paper, we propose context unaware knowledge\ndistillation that uses the knowledge of the teacher model without fine-tuning\nit on the target context. We also propose a new efficient student model\narchitecture for knowledge distillation. The proposed approach follows a\ntwo-step process. The first step involves pre-training the student model with\nthe help of context unaware knowledge distillation from the teacher model. The\nsecond step involves fine-tuning the student model on the context of image\nretrieval. In order to show the efficacy of the proposed approach, we compare\nthe retrieval results, no. of parameters and no. of operations of the student\nmodels with the teacher models under different retrieval frameworks, including\ndeep cauchy hashing (DCH) and central similarity quantization (CSQ). The\nexperimental results confirm that the proposed approach provides a promising\ntrade-off between the retrieval results and efficiency. The code used in this\npaper is released publicly at https://github.com/satoru2001/CUKDFIR.</p>\n", "tags": ["Efficiency", "Quantization", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [27.272621154785156, -39.46909713745117], "cluster": 7}, {"key": "remil2023deeplsh", "year": "2023", "citations": "1", "title": "Deeplsh: Deep Locality-sensitive Hash Learning For Fast And Efficient Near-duplicate Crash Report Detection", "abstract": "<p>Automatic crash bucketing is a crucial phase in the software development\nprocess for efficiently triaging bug reports. It generally consists in grouping\nsimilar reports through clustering techniques. However, with real-time\nstreaming bug collection, systems are needed to quickly answer the question:\nWhat are the most similar bugs to a new one?, that is, efficiently find\nnear-duplicates. It is thus natural to consider nearest neighbors search to\ntackle this problem and especially the well-known locality-sensitive hashing\n(LSH) to deal with large datasets due to its sublinear performance and\ntheoretical guarantees on the similarity search accuracy. Surprisingly, LSH has\nnot been considered in the crash bucketing literature. It is indeed not trivial\nto derive hash functions that satisfy the so-called locality-sensitive property\nfor the most advanced crash bucketing metrics. Consequently, we study in this\npaper how to leverage LSH for this task. To be able to consider the most\nrelevant metrics used in the literature, we introduce DeepLSH, a Siamese DNN\narchitecture with an original loss function, that perfectly approximates the\nlocality-sensitivity property even for Jaccard and Cosine metrics for which\nexact LSH solutions exist. We support this claim with a series of experiments\non an original dataset, which we make available.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [15.880406379699707, 11.76167106628418], "cluster": 6}, {"key": "ren2020beyond", "year": "2020", "citations": "1", "title": "Beyond The Deep Metric Learning: Enhance The Cross-modal Matching With Adversarial Discriminative Domain Regularization", "abstract": "<p>Matching information across image and text modalities is a fundamental\nchallenge for many applications that involve both vision and natural language\nprocessing. The objective is to find efficient similarity metrics to compare\nthe similarity between visual and textual information. Existing approaches\nmainly match the local visual objects and the sentence words in a shared space\nwith attention mechanisms. The matching performance is still limited because\nthe similarity computation is based on simple comparisons of the matching\nfeatures, ignoring the characteristics of their distribution in the data. In\nthis paper, we address this limitation with an efficient learning objective\nthat considers the discriminative feature distributions between the visual\nobjects and sentence words. Specifically, we propose a novel Adversarial\nDiscriminative Domain Regularization (ADDR) learning framework, beyond the\nparadigm metric learning objective, to construct a set of discriminative data\ndomains within each image-text pairs. Our approach can generally improve the\nlearning efficiency and the performance of existing metrics learning frameworks\nby regulating the distribution of the hidden space between the matching pairs.\nThe experimental results show that this new approach significantly improves the\noverall performance of several popular cross-modal matching techniques (SCAN,\nVSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Robustness", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [-24.26863670349121, -9.68239974975586], "cluster": 1}, {"key": "ren2021pair", "year": "2021", "citations": "22", "title": "PAIR: Leveraging Passage-centric Similarity Relation For Improving Dense Passage Retrieval", "abstract": "<p>Recently, dense passage retrieval has become a mainstream approach to finding\nrelevant information in various natural language processing tasks. A number of\nstudies have been devoted to improving the widely adopted dual-encoder\narchitecture. However, most of the previous studies only consider query-centric\nsimilarity relation when learning the dual-encoder retriever. In order to\ncapture more comprehensive similarity relations, we propose a novel approach\nthat leverages both query-centric and PAssage-centric sImilarity Relations\n(called PAIR) for dense passage retrieval. To implement our approach, we make\nthree major technical contributions by introducing formal formulations of the\ntwo kinds of similarity relations, generating high-quality pseudo labeled data\nvia knowledge distillation, and designing an effective two-stage training\nprocedure that incorporates passage-centric similarity relation constraint.\nExtensive experiments show that our approach significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions datasets.</p>\n", "tags": ["Datasets"], "tsne_embedding": [1.4713252782821655, -30.53679847717285], "cluster": 3}, {"key": "ren2022leaner", "year": "2022", "citations": "4", "title": "Leaner And Faster: Two-stage Model Compression For Lightweight Text-image Retrieval", "abstract": "<p>Current text-image approaches (e.g., CLIP) typically adopt dual-encoder\narchitecture using pre-trained vision-language representation. However, these\nmodels still pose non-trivial memory requirements and substantial incremental\nindexing time, which makes them less practical on mobile devices. In this\npaper, we present an effective two-stage framework to compress large\npre-trained dual-encoder for lightweight text-image retrieval. The resulting\nmodel is smaller (39% of the original), faster (1.6x/2.9x for processing\nimage/text respectively), yet performs on par with or better than the original\nfull model on Flickr30K and MSCOCO benchmarks. We also open-source an\naccompanying realistic mobile image search application.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval"], "tsne_embedding": [-8.393850326538086, -10.211689949035645], "cluster": 1}, {"key": "revaud2019learning", "year": "2019", "citations": "348", "title": "Learning With Average Precision: Training Image Retrieval With A Listwise Loss", "abstract": "<p>Image retrieval can be formulated as a ranking problem where the goal is to\norder database images by decreasing similarity to the query. Recent deep models\nfor image retrieval have outperformed traditional methods by leveraging\nranking-tailored loss functions, but important theoretical and practical\nproblems remain. First, rather than directly optimizing the global ranking,\nthey minimize an upper-bound on the essential loss, which does not necessarily\nresult in an optimal mean average precision (mAP). Second, these methods\nrequire significant engineering efforts to work well, e.g. special pre-training\nand hard-negative mining. In this paper we propose instead to directly optimize\nthe global mAP by leveraging recent advances in listwise loss formulations.\nUsing a histogram binning approximation, the AP can be differentiated and thus\nemployed to end-to-end learning. Compared to existing losses, the proposed\nmethod considers thousands of images simultaneously at each iteration and\neliminates the need for ad hoc tricks. It also establishes a new state of the\nart on many standard retrieval benchmarks. Models and evaluation scripts have\nbeen made available at https://europe.naverlabs.com/Deep-Image-Retrieval/</p>\n", "tags": ["ICCV", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-32.533145904541016, 5.698794841766357], "cluster": 0}, {"key": "riazi2016sub", "year": "2016", "citations": "11", "title": "Sub-linear Privacy-preserving Near-neighbor Search", "abstract": "<p>In Near-Neighbor Search (NNS), a new client queries a database (held by a\nserver) for the most similar data (near-neighbors) given a certain similarity\nmetric. The Privacy-Preserving variant (PP-NNS) requires that neither server\nnor the client shall learn information about the other party\u2019s data except what\ncan be inferred from the outcome of NNS. The overwhelming growth in the size of\ncurrent datasets and the lack of a truly secure server in the online world\nrender the existing solutions impractical; either due to their high\ncomputational requirements or non-realistic assumptions which potentially\ncompromise privacy. PP-NNS having query time {\\it sub-linear} in the size of\nthe database has been suggested as an open research direction by Li et al.\n(CCSW\u201915). In this paper, we provide the first such algorithm, called Secure\nLocality Sensitive Indexing (SLSI) which has a sub-linear query time and the\nability to handle honest-but-curious parties. At the heart of our proposal lies\na secure binary embedding scheme generated from a novel probabilistic\ntransformation over locality sensitive hashing family. We provide information\ntheoretic bound for the privacy guarantees and support our theoretical claims\nusing substantial empirical evidence on real-world datasets.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [38.529151916503906, 2.6383135318756104], "cluster": 9}, {"key": "riba2020learning", "year": "2021", "citations": "26", "title": "Learning Graph Edit Distance By Graph Neural Networks", "abstract": "<p>The emergence of geometric deep learning as a novel framework to deal with\ngraph-based representations has faded away traditional approaches in favor of\ncompletely new methodologies. In this paper, we propose a new framework able to\ncombine the advances on deep metric learning with traditional approximations of\nthe graph edit distance. Hence, we propose an efficient graph distance based on\nthe novel field of geometric deep learning. Our method employs a message\npassing neural network to capture the graph structure, and thus, leveraging\nthis information for its use on a distance computation. The performance of the\nproposed graph distance is validated on two different scenarios. On the one\nhand, in a graph retrieval of handwritten words~\\ie~keyword spotting, showing\nits superior performance when compared with (approximate) graph edit distance\nbenchmarks. On the other hand, demonstrating competitive results for graph\nsimilarity learning when compared with the current state-of-the-art on a recent\nbenchmark dataset.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Graph-Based-Ann", "Tools-&-Libraries", "Distance-Metric-Learning"], "tsne_embedding": [54.702945709228516, 4.22628927230835], "cluster": 9}, {"key": "ribeiro2020sketchformer", "year": "2020", "citations": "83", "title": "Sketchformer: Transformer-based Representation For Sketched Structure", "abstract": "<p>Sketchformer is a novel transformer-based representation for encoding\nfree-hand sketches input in a vector form, i.e. as a sequence of strokes.\nSketchformer effectively addresses multiple tasks: sketch classification,\nsketch based image retrieval (SBIR), and the reconstruction and interpolation\nof sketches. We report several variants exploring continuous and tokenized\ninput representations, and contrast their performance. Our learned embedding,\ndriven by a dictionary learning tokenization scheme, yields state of the art\nperformance in classification and image retrieval tasks, when compared against\nbaseline representations driven by LSTM sequence to sequence architectures:\nSketchRNN and derivatives. We show that sketch reconstruction and interpolation\nare improved significantly by the Sketchformer embedding for complex sketches\nwith longer stroke sequences.</p>\n", "tags": ["CVPR", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-46.27144241333008, -21.473661422729492], "cluster": 5}, {"key": "ribeiro2021scene", "year": "2021", "citations": "6", "title": "Scene Designer: A Unified Model For Scene Search And Synthesis From Sketch", "abstract": "<p>Scene Designer is a novel method for searching and generating images using\nfree-hand sketches of scene compositions; i.e. drawings that describe both the\nappearance and relative positions of objects. Our core contribution is a single\nunified model to learn both a cross-modal search embedding for matching\nsketched compositions to images, and an object embedding for layout synthesis.\nWe show that a graph neural network (GNN) followed by Transformer under our\nnovel contrastive learning setting is required to allow learning correlations\nbetween object type, appearance and arrangement, driving a mask generation\nmodule that synthesises coherent scene layouts, whilst also delivering state of\nthe art sketch based visual search of scenes.</p>\n", "tags": ["Self-Supervised", "ICCV", "Image-Retrieval"], "tsne_embedding": [-43.991729736328125, -21.12373161315918], "cluster": 5}, {"key": "ribeiro2023embedding", "year": "2023", "citations": "0", "title": "Embedding Aggregation For Forensic Facial Comparison", "abstract": "<p>In forensic facial comparison, questioned-source images are usually captured\nin uncontrolled environments, with non-uniform lighting, and from\nnon-cooperative subjects. The poor quality of such material usually compromises\ntheir value as evidence in legal matters. On the other hand, in forensic\ncasework, multiple images of the person of interest are usually available. In\nthis paper, we propose to aggregate deep neural network embeddings from various\nimages of the same person to improve performance in facial verification. We\nobserve significant performance improvements, especially for very low-quality\nimages. Further improvements are obtained by aggregating embeddings of more\nimages and by applying quality-weighted aggregation. We demonstrate the\nbenefits of this approach in forensic evaluation settings with the development\nand validation of score-based likelihood ratio systems and report improvements\nin Cllr of up to 95% (from 0.249 to 0.012) for CCTV images and of up to 96%\n(from 0.083 to 0.003) for social media images.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-14.796335220336914, 8.66338062286377], "cluster": 8}, {"key": "ribeiro2023sketch", "year": "2023", "citations": "2", "title": "Sketch-an-anchor: Sub-epoch Fast Model Adaptation For Zero-shot Sketch-based Image Retrieval", "abstract": "<p>Sketch-an-Anchor is a novel method to train state-of-the-art Zero-shot\nSketch-based Image Retrieval (ZSSBIR) models in under an epoch. Most studies\nbreak down the problem of ZSSBIR into two parts: domain alignment between\nimages and sketches, inherited from SBIR, and generalization to unseen data,\ninherent to the zero-shot protocol. We argue one of these problems can be\nconsiderably simplified and re-frame the ZSSBIR problem around the\nalready-stellar yet underexplored Zero-shot Image-based Retrieval performance\nof off-the-shelf models. Our fast-converging model keeps the single-domain\nperformance while learning to extract similar representations from sketches. To\nthis end we introduce our Semantic Anchors \u2013 guiding embeddings learned from\nword-based semantic spaces and features from off-the-shelf models \u2013 and\ncombine them with our novel Anchored Contrastive Loss. Empirical evidence shows\nwe can achieve state-of-the-art performance on all benchmark datasets while\ntraining for 100x less iterations than other methods.</p>\n", "tags": ["Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-42.44744110107422, -17.69745635986328], "cluster": 5}, {"key": "rippel2014learning", "year": "2014", "citations": "49", "title": "Learning Ordered Representations With Nested Dropout", "abstract": "<p>In this paper, we study ordered representations of data in which different\ndimensions have different degrees of importance. To learn these representations\nwe introduce nested dropout, a procedure for stochastically removing coherent\nnested sets of hidden units in a neural network. We first present a sequence of\ntheoretical results in the simple case of a semi-linear autoencoder. We\nrigorously show that the application of nested dropout enforces identifiability\nof the units, which leads to an exact equivalence with PCA. We then extend the\nalgorithm to deep models and demonstrate the relevance of ordered\nrepresentations to a number of applications. Specifically, we use the ordered\nproperty of the learned codes to construct hash-based data structures that\npermit very fast retrieval, achieving retrieval in time logarithmic in the\ndatabase size and independent of the dimensionality of the representation. This\nallows codes that are hundreds of times longer than currently feasible for\nretrieval. We therefore avoid the diminished quality associated with short\ncodes, while still performing retrieval that is competitive in speed with\nexisting methods. We also show that ordered representations are a promising way\nto learn adaptive compression for efficient online data reconstruction.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [22.611003875732422, 5.1860270500183105], "cluster": 6}, {"key": "riyadh2024llm", "year": "2024", "citations": "0", "title": "Llm-assisted Vector Similarity Search", "abstract": "<p>As data retrieval demands become increasingly complex, traditional search\nmethods often fall short in addressing nuanced and conceptual queries. Vector\nsimilarity search has emerged as a promising technique for finding semantically\nsimilar information efficiently. However, its effectiveness diminishes when\nhandling intricate queries with contextual nuances. This paper explores a\nhybrid approach combining vector similarity search with Large Language Models\n(LLMs) to enhance search accuracy and relevance. The proposed two-step solution\nfirst employs vector similarity search to shortlist potential matches, followed\nby an LLM for context-aware ranking of the results. Experiments on structured\ndatasets demonstrate that while vector similarity search alone performs well\nfor straightforward queries, the LLM-assisted approach excels in processing\ncomplex queries involving constraints, negations, or conceptual requirements.\nBy leveraging the natural language understanding capabilities of LLMs, this\nmethod improves the accuracy of search results for complex tasks without\nsacrificing efficiency. We also discuss real-world applications and propose\ndirections for future research to refine and scale this technique for diverse\ndatasets and use cases.\n  Original article:\nhttps://engineering.grab.com/llm-assisted-vector-similarity-search</p>\n", "tags": ["Efficiency", "Similarity-Search", "Datasets"], "tsne_embedding": [3.289442539215088, -21.793319702148438], "cluster": 7}, {"key": "roald2024visual", "year": "2024", "citations": "0", "title": "Visual Navigation Of Digital Libraries: Retrieval And Classification Of Images In The National Library Of Norway's Digitised Book Collection", "abstract": "<p>Digital tools for text analysis have long been essential for the\nsearchability and accessibility of digitised library collections. Recent\ncomputer vision advances have introduced similar capabilities for visual\nmaterials, with deep learning-based embeddings showing promise for analysing\nvisual heritage. Given that many books feature visuals in addition to text,\ntaking advantage of these breakthroughs is critical to making library\ncollections open and accessible. In this work, we present a proof-of-concept\nimage search application for exploring images in the National Library of\nNorway\u2019s pre-1900 books, comparing Vision Transformer (ViT), Contrastive\nLanguage-Image Pre-training (CLIP), and Sigmoid loss for Language-Image\nPre-training (SigLIP) embeddings for image retrieval and classification. Our\nresults show that the application performs well for exact image retrieval, with\nSigLIP embeddings slightly outperforming CLIP and ViT in both retrieval and\nclassification tasks. Additionally, SigLIP-based image classification can aid\nin cleaning image datasets from a digitisation pipeline.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-23.407115936279297, -24.452638626098633], "cluster": 5}, {"key": "robberechts2022elastic", "year": "2022", "citations": "0", "title": "Elastic Product Quantization For Time Series", "abstract": "<p>Analyzing numerous or long time series is difficult in practice due to the\nhigh storage costs and computational requirements. Therefore, techniques have\nbeen proposed to generate compact similarity-preserving representations of time\nseries, enabling real-time similarity search on large in-memory data\ncollections. However, the existing techniques are not ideally suited for\nassessing similarity when sequences are locally out of phase. In this paper, we\npropose the use of product quantization for efficient similarity-based\ncomparison of time series under time warping. The idea is to first compress the\ndata by partitioning the time series into equal length sub-sequences which are\nrepresented by a short code. The distance between two time series can then be\nefficiently approximated by pre-computed elastic distances between their codes.\nThe partitioning into sub-sequences forces unwanted alignments, which we\naddress with a pre-alignment step using the maximal overlap discrete wavelet\ntransform (MODWT). To demonstrate the efficiency and accuracy of our method, we\nperform an extensive experimental evaluation on benchmark datasets in nearest\nneighbors classification and clustering applications. Overall, the proposed\nsolution emerges as a highly efficient (both in terms of memory usage and\ncomputation time) replacement for elastic measures in time series applications.</p>\n", "tags": ["Efficiency", "Quantization", "Similarity-Search", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [18.41486358642578, 35.052162170410156], "cluster": 4}, {"key": "rong2018locality", "year": "2018", "citations": "6", "title": "Locality-sensitive Hashing For Earthquake Detection: A Case Study Of Scaling Data-driven Science", "abstract": "<p>In this work, we report on a novel application of Locality Sensitive\nHashing (LSH) to seismic data at scale. Based on the high waveform similarity between reoccurring earthquakes, our application\nidentifies potential earthquakes by searching for similar time series\nsegments via LSH. However, a straightforward implementation of\nthis LSH-enabled application has difficulty scaling beyond 3 months\nof continuous time series data measured at a single seismic station.\nAs a case study of a data-driven science workflow, we illustrate how\ndomain knowledge can be incorporated into the workload to improve\nboth the efficiency and result quality. We describe several end-toend optimizations of the analysis pipeline from pre-processing to\npost-processing, which allow the application to scale to time series data measured at multiple seismic stations. Our optimizations\nenable an over 100\u00d7 speedup in the end-to-end analysis pipeline.\nThis improved scalability enabled seismologists to perform seismic\nanalysis on more than ten years of continuous time series data from\nover ten seismic stations, and has directly enabled the discovery of\n597 new earthquakes near the Diablo Canyon nuclear power plant\nin California and 6123 new earthquakes in New Zealand.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Scalability", "Re-Ranking"], "tsne_embedding": [-28.560035705566406, 38.045433044433594], "cluster": 8}, {"key": "rong2025locality", "year": "2018", "citations": "6", "title": "Locality-sensitive Hashing For Earthquake Detection: A Case Study Of Scaling Data-driven Science", "abstract": "<p>In this work, we report on a novel application of Locality Sensitive\nHashing (LSH) to seismic data at scale. Based on the high waveform similarity between reoccurring earthquakes, our application\nidentifies potential earthquakes by searching for similar time series\nsegments via LSH. However, a straightforward implementation of\nthis LSH-enabled application has difficulty scaling beyond 3 months\nof continuous time series data measured at a single seismic station.\nAs a case study of a data-driven science workflow, we illustrate how\ndomain knowledge can be incorporated into the workload to improve\nboth the efficiency and result quality. We describe several end-toend optimizations of the analysis pipeline from pre-processing to\npost-processing, which allow the application to scale to time series data measured at multiple seismic stations. Our optimizations\nenable an over 100\u00d7 speedup in the end-to-end analysis pipeline.\nThis improved scalability enabled seismologists to perform seismic\nanalysis on more than ten years of continuous time series data from\nover ten seismic stations, and has directly enabled the discovery of\n597 new earthquakes near the Diablo Canyon nuclear power plant\nin California and 6123 new earthquakes in New Zealand.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Scalability", "Re-Ranking"], "tsne_embedding": [-28.560035705566406, 38.045433044433594], "cluster": 8}, {"key": "rossetto2019query", "year": "2019", "citations": "5", "title": "Query By Semantic Sketch", "abstract": "<p>Sketch-based query formulation is very common in image and video retrieval as\nthese techniques often complement textual retrieval methods that are based on\neither manual or machine generated annotations. In this paper, we present a\nretrieval approach that allows to query visual media collections by sketching\nconcept maps, thereby merging sketch-based retrieval with the search for\nsemantic labels. Users can draw a spatial distribution of different concept\nlabels, such as \u201csky\u201d, \u201csea\u201d or \u201cperson\u201d and then use these sketches to find\nimages or video scenes that exhibit a similar distribution of these concepts.\nHence, this approach does not only take the semantic concepts themselves into\naccount, but also their semantic relations as well as their spatial context.\nThe efficient vector representation enables efficient retrieval even in large\nmultimedia collections. We have integrated the semantic sketch query mode into\nour retrieval engine vitrivr and demonstrated its effectiveness.</p>\n", "tags": ["Video-Retrieval", "Similarity-Search"], "tsne_embedding": [-24.063175201416016, -33.52400207519531], "cluster": 5}, {"key": "rossi2024relevance", "year": "2024", "citations": "2", "title": "Relevance Filtering For Embedding-based Retrieval", "abstract": "<p>In embedding-based retrieval, Approximate Nearest Neighbor (ANN) search\nenables efficient retrieval of similar items from large-scale datasets. While\nmaximizing recall of relevant items is usually the goal of retrieval systems, a\nlow precision may lead to a poor search experience. Unlike lexical retrieval,\nwhich inherently limits the size of the retrieved set through keyword matching,\ndense retrieval via ANN search has no natural cutoff. Moreover, the cosine\nsimilarity scores of embedding vectors are often optimized via contrastive or\nranking losses, which make them difficult to interpret. Consequently, relying\non top-K or cosine-similarity cutoff is often insufficient to filter out\nirrelevant results effectively. This issue is prominent in product search,\nwhere the number of relevant products is often small. This paper introduces a\nnovel relevance filtering component (called \u201cCosine Adapter\u201d) for\nembedding-based retrieval to address this challenge. Our approach maps raw\ncosine similarity scores to interpretable scores using a query-dependent\nmapping function. We then apply a global threshold on the mapped scores to\nfilter out irrelevant results. We are able to significantly increase the\nprecision of the retrieved set, at the expense of a small loss of recall. The\neffectiveness of our approach is demonstrated through experiments on both\npublic MS MARCO dataset and internal Walmart product search data. Furthermore,\nonline A/B testing on the Walmart site validates the practical value of our\napproach in real-world e-commerce settings.</p>\n", "tags": ["Distance-Metric-Learning", "CIKM", "Similarity-Search", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [22.60878562927246, 34.30644989013672], "cluster": 4}, {"key": "roth2019mic", "year": "2019", "citations": "92", "title": "MIC: Mining Interclass Characteristics For Improved Metric Learning", "abstract": "<p>Metric learning seeks to embed images of objects suchthat class-defined\nrelations are captured by the embeddingspace. However, variability in images is\nnot just due to different depicted object classes, but also depends on other\nlatent characteristics such as viewpoint or illumination. In addition to these\nstructured properties, random noise further obstructs the visual relations of\ninterest. The common approach to metric learning is to enforce a representation\nthat is invariant under all factors but the ones of interest. In contrast, we\npropose to explicitly learn the latent characteristics that are shared by and\ngo across object classes. We can then directly explain away structured visual\nvariability, rather than assuming it to be unknown random noise. We propose a\nnovel surrogate task to learn visual characteristics shared across classes with\na separate encoder. This encoder is trained jointly with the encoder for class\ninformation by reducing their mutual information. On five standard image\nretrieval benchmarks the approach significantly improves upon the\nstate-of-the-art.</p>\n", "tags": ["ICCV", "Distance-Metric-Learning"], "tsne_embedding": [-14.881608009338379, -18.77577018737793], "cluster": 1}, {"key": "roth2020s2sd", "year": "2020", "citations": "4", "title": "S2SD: Simultaneous Similarity-based Self-distillation For Deep Metric Learning", "abstract": "<p>Deep Metric Learning (DML) provides a crucial tool for visual similarity and\nzero-shot applications by learning generalizing embedding spaces, although\nrecent work in DML has shown strong performance saturation across training\nobjectives. However, generalization capacity is known to scale with the\nembedding space dimensionality. Unfortunately, high dimensional embeddings also\ncreate higher retrieval cost for downstream applications. To remedy this, we\npropose \\emph{Simultaneous Similarity-based Self-distillation (S2SD). S2SD\nextends DML with knowledge distillation from auxiliary, high-dimensional\nembedding and feature spaces to leverage complementary context during training\nwhile retaining test-time cost and with negligible changes to the training\ntime. Experiments and ablations across different objectives and standard\nbenchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while\nalso setting a new state-of-the-art. Code available at\nhttps://github.com/MLforHealth/S2SD.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning"], "tsne_embedding": [-17.122333526611328, -16.97750473022461], "cluster": 1}, {"key": "roy2016representing", "year": "2016", "citations": "11", "title": "Representing Documents And Queries As Sets Of Word Embedded Vectors For Information Retrieval", "abstract": "<p>A major difficulty in applying word vector embeddings in IR is in devising an\neffective and efficient strategy for obtaining representations of compound\nunits of text, such as whole documents, (in comparison to the atomic words),\nfor the purpose of indexing and scoring documents. Instead of striving for a\nsuitable method for obtaining a single vector representation of a large\ndocument of text, we rather aim for developing a similarity metric that makes\nuse of the similarities between the individual embedded word vectors in a\ndocument and a query. More specifically, we represent a document and a query as\nsets of word vectors, and use a standard notion of similarity measure between\nthese sets, computed as a function of the similarities between each constituent\nword pair from these sets. We then make use of this similarity measure in\ncombination with standard IR based similarities for document ranking. The\nresults of our initial experimental investigations shows that our proposed\nmethod improves MAP by up to \\(5.77%\\), in comparison to standard text-based\nlanguage model similarity, on the TREC ad-hoc dataset.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-37.651611328125, -22.327688217163086], "cluster": 5}, {"key": "roy2019metric", "year": "2020", "citations": "75", "title": "Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images", "abstract": "<p>Hashing methods have been recently found very effective in retrieval of\nremote sensing (RS) images due to their computational efficiency and fast\nsearch speed. The traditional hashing methods in RS usually exploit\nhand-crafted features to learn hash functions to obtain binary codes, which can\nbe insufficient to optimally represent the information content of RS images. To\novercome this problem, in this paper we introduce a metric-learning based\nhashing network, which learns: 1) a semantic-based metric space for effective\nfeature representation; and 2) compact binary hash codes for fast archive\nsearch. Our network considers an interplay of multiple loss functions that\nallows to jointly learn a metric based semantic space facilitating similar\nimages to be clustered together in that target space and at the same time\nproducing compact final activations that lose negligible information when\nbinarized. Experiments carried out on two benchmark RS archives point out that\nthe proposed network significantly improves the retrieval performance under the\nsame retrieval time when compared to the state-of-the-art hashing methods in\nRS.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Compact-Codes", "Evaluation"], "tsne_embedding": [-5.313404083251953, 11.151589393615723], "cluster": 8}, {"key": "roy2020zscrgan", "year": "2020", "citations": "3", "title": "ZSCRGAN: A Gan-based Expectation Maximization Model For Zero-shot Retrieval Of Images From Textual Descriptions", "abstract": "<p>Most existing algorithms for cross-modal Information Retrieval are based on a\nsupervised train-test setup, where a model learns to align the mode of the\nquery (e.g., text) to the mode of the documents (e.g., images) from a given\ntraining set. Such a setup assumes that the training set contains an exhaustive\nrepresentation of all possible classes of queries. In reality, a retrieval\nmodel may need to be deployed on previously unseen classes, which implies a\nzero-shot IR setup. In this paper, we propose a novel GAN-based model for\nzero-shot text to image retrieval. When given a textual description as the\nquery, our model can retrieve relevant images in a zero-shot setup. The\nproposed model is trained using an Expectation-Maximization framework.\nExperiments on multiple benchmark datasets show that our proposed model\ncomfortably outperforms several state-of-the-art zero-shot text to image\nretrieval models, as well as zero-shot classification and hashing models\nsuitably used for retrieval.</p>\n", "tags": ["Datasets", "Evaluation", "Tools-&-Libraries", "CIKM", "Image-Retrieval", "Hashing-Methods", "Supervised", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [-16.595003128051758, -21.1622371673584], "cluster": 5}, {"key": "royoletelier2018disambiguating", "year": "2018", "citations": "5", "title": "Disambiguating Music Artists At Scale With Audio Metric Learning", "abstract": "<p>We address the problem of disambiguating large scale catalogs through the\ndefinition of an unknown artist clustering task. We explore the use of metric\nlearning techniques to learn artist embeddings directly from audio, and using a\ndedicated homonym artists dataset, we compare our method with a recent approach\nthat learn similar embeddings using artist classifiers. While both systems have\nthe ability to disambiguate unknown artists relying exclusively on audio, we\nshow that our system is more suitable in the case when enough audio data is\navailable for each artist in the train dataset. We also propose a new negative\nsampling method for metric learning that takes advantage of side information\nsuch as music genre during the learning phase and shows promising results for\nthe artist clustering task.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [7.529828071594238, -45.261863708496094], "cluster": 3}, {"key": "ruan2022tricolo", "year": "2024", "citations": "4", "title": "Tricolo: Trimodal Contrastive Loss For Text To Shape Retrieval", "abstract": "<p>Text-to-shape retrieval is an increasingly relevant problem with the growth\nof 3D shape data. Recent work on contrastive losses for learning joint\nembeddings over multimodal data has been successful at tasks such as retrieval\nand classification. Thus far, work on joint representation learning for 3D\nshapes and text has focused on improving embeddings through modeling of complex\nattention between representations, or multi-task learning. We propose a\ntrimodal learning scheme over text, multi-view images and 3D shape voxels, and\nshow that with large batch contrastive learning we achieve good performance on\ntext-to-shape retrieval without complex attention mechanisms or losses. Our\nexperiments serve as a foundation for follow-up work on building trimodal\nembeddings for text-image-shape.</p>\n", "tags": ["Self-Supervised", "Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-38.12759780883789, -15.655485153198242], "cluster": 5}, {"key": "rubinstein2018hardness", "year": "2018", "citations": "75", "title": "Hardness Of Approximate Nearest Neighbor Search", "abstract": "<p>We prove conditional near-quadratic running time lower bounds for approximate\nBichromatic Closest Pair with Euclidean, Manhattan, Hamming, or edit distance.\nSpecifically, unless the Strong Exponential Time Hypothesis (SETH) is false,\nfor every \\(\\delta&gt;0\\) there exists a constant \\(\\epsilon&gt;0\\) such that computing a\n\\((1+\\epsilon)\\)-approximation to the Bichromatic Closest Pair requires\n\\(n^{2-\\delta}\\) time. In particular, this implies a near-linear query time for\nApproximate Nearest Neighbor search with polynomial preprocessing time.\n  Our reduction uses the Distributed PCP framework of [ARW\u201917], but obtains\nimproved efficiency using Algebraic Geometry (AG) codes. Efficient PCPs from AG\ncodes have been constructed in other settings before [BKKMS\u201916, BCGRS\u201917], but\nour construction is the first to yield new hardness results.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries"], "tsne_embedding": [17.32106590270996, 42.43756103515625], "cluster": 4}, {"key": "ruta2021aladin", "year": "2021", "citations": "28", "title": "ALADIN: All Layer Adaptive Instance Normalization For Fine-grained Style Similarity", "abstract": "<p>We present ALADIN (All Layer AdaIN); a novel architecture for searching\nimages based on the similarity of their artistic style. Representation learning\nis critical to visual search, where distance in the learned search embedding\nreflects image similarity. Learning an embedding that discriminates\nfine-grained variations in style is hard, due to the difficulty of defining and\nlabelling style. ALADIN takes a weakly supervised approach to learning a\nrepresentation for fine-grained style similarity of digital artworks,\nleveraging BAM-FG, a novel large-scale dataset of user generated content\ngroupings gathered from the web. ALADIN sets a new state of the art accuracy\nfor style-based visual search over both coarse labelled style data (BAM) and\nBAM-FG; a new 2.62 million image dataset of 310,000 fine-grained style\ngroupings also contributed by this work.</p>\n", "tags": ["ICCV", "Image-Retrieval", "Scalability", "Datasets", "Supervised"], "tsne_embedding": [-42.37163162231445, -22.271575927734375], "cluster": 5}, {"key": "ruta2022stylebabel", "year": "2022", "citations": "6", "title": "Stylebabel: Artistic Style Tagging And Captioning", "abstract": "<p>We present StyleBabel, a unique open access dataset of natural language\ncaptions and free-form tags describing the artistic style of over 135K digital\nartworks, collected via a novel participatory method from experts studying at\nspecialist art and design schools. StyleBabel was collected via an iterative\nmethod, inspired by `Grounded Theory\u2019: a qualitative approach that enables\nannotation while co-evolving a shared language for fine-grained artistic style\nattribute description. We demonstrate several downstream tasks for StyleBabel,\nadapting the recent ALADIN architecture for fine-grained style similarity, to\ntrain cross-modal embeddings for: 1) free-form tag generation; 2) natural\nlanguage description of artistic style; 3) fine-grained text search of style.\nTo do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and\ncross-modal representation learning, achieving a state of the art accuracy in\nfine-grained style retrieval.</p>\n", "tags": ["Text-Retrieval", "Datasets"], "tsne_embedding": [-21.274097442626953, -30.342222213745117], "cluster": 5}, {"key": "ryali2020bio", "year": "2020", "citations": "4", "title": "Bio-inspired Hashing For Unsupervised Similarity Search", "abstract": "<p>The fruit fly Drosophila\u2019s olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes, FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However, FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology, our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule, our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science, BioHash and BioConvHash are fast, scalable and yield compressed binary representations that are useful for similarity search.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Evaluation", "Unsupervised"], "tsne_embedding": [-10.234685897827148, 49.4592170715332], "cluster": 4}, {"key": "ryali2025bio", "year": "2020", "citations": "4", "title": "Bio-inspired Hashing For Unsupervised Similarity Search", "abstract": "<p>The fruit fly Drosophila\u2019s olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes, FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However, FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology, our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule, our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science, BioHash and BioConvHash are fast, scalable and yield compressed binary representations that are useful for similarity search.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Evaluation", "Unsupervised"], "tsne_embedding": [-10.234685897827148, 49.4592170715332], "cluster": 4}, {"key": "rychalska2020i", "year": "2020", "citations": "0", "title": "I Know Why You Like This Movie: Interpretable Efficient Multimodal Recommender", "abstract": "<p>Recently, the Efficient Manifold Density Estimator (EMDE) model has been\nintroduced. The model exploits Local Sensitive Hashing and Count-Min Sketch\nalgorithms, combining them with a neural network to achieve state-of-the-art\nresults on multiple recommender datasets. However, this model ingests a\ncompressed joint representation of all input items for each user/session, so\ncalculating attributions for separate items via gradient-based methods seems\nnot applicable. We prove that interpreting this model in a white-box setting is\npossible thanks to the properties of EMDE item retrieval method. By exploiting\nmultimodal flexibility of this model, we obtain meaningful results showing the\ninfluence of multiple modalities: text, categorical features, and images, on\nmovie recommendation output.</p>\n", "tags": ["Recommender-Systems", "Hashing-Methods", "Datasets"], "tsne_embedding": [-43.09090805053711, -29.24163246154785], "cluster": 5}, {"key": "rychalska2021t", "year": "2021", "citations": "0", "title": "T-EMDE: Sketching-based Global Similarity For Cross-modal Retrieval", "abstract": "<p>The key challenge in cross-modal retrieval is to find similarities between\nobjects represented with different modalities, such as image and text. However,\neach modality embeddings stem from non-related feature spaces, which causes the\nnotorious \u2018heterogeneity gap\u2019. Currently, many cross-modal systems try to\nbridge the gap with self-attention. However, self-attention has been widely\ncriticized for its quadratic complexity, which prevents many real-life\napplications. In response to this, we propose T-EMDE - a neural density\nestimator inspired by the recently introduced Efficient Manifold Density\nEstimator (EMDE) from the area of recommender systems. EMDE operates on\nsketches - representations especially suitable for multimodal operations.\nHowever, EMDE is non-differentiable and ingests precomputed, static embeddings.\nWith T-EMDE we introduce a trainable version of EMDE which allows full\nend-to-end training. In contrast to self-attention, the complexity of our\nsolution is linear to the number of tokens/segments. As such, T-EMDE is a\ndrop-in replacement for the self-attention module, with beneficial influence on\nboth speed and metric performance in cross-modal settings. It facilitates\ncommunication between modalities, as each global text/image representation is\nexpressed with a standardized sketch histogram which represents the same\nmanifold structures irrespective of the underlying modality. We evaluate T-EMDE\nby introducing it into two recent cross-modal SOTA models and achieving new\nstate-of-the-art results on multiple datasets and decreasing model latency by\nup to 20%.</p>\n", "tags": ["Recommender-Systems", "Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [-31.852338790893555, -12.167920112609863], "cluster": 5}, {"key": "rygl2017semantic", "year": "2017", "citations": "12", "title": "Semantic Vector Encoding And Similarity Search Using Fulltext Search Engines", "abstract": "<p>Vector representations and vector space modeling (VSM) play a central role in\nmodern machine learning. We propose a novel approach to `vector similarity\nsearching\u2019 over dense semantic representations of words and documents that can\nbe deployed on top of traditional inverted-index-based fulltext engines, taking\nadvantage of their robustness, stability, scalability and ubiquity.\n  We show that this approach allows the indexing and querying of dense vectors\nin text domains. This opens up exciting avenues for major efficiency gains,\nalong with simpler deployment, scaling and monitoring.\n  The end result is a fast and scalable vector database with a tunable\ntrade-off between vector search performance and quality, backed by a standard\nfulltext engine such as Elasticsearch.\n  We empirically demonstrate its querying performance and quality by applying\nthis solution to the task of semantic searching over a dense vector\nrepresentation of the entire English Wikipedia.</p>\n", "tags": ["Text-Retrieval", "Efficiency", "Scalability", "Similarity-Search", "Robustness", "Evaluation"], "tsne_embedding": [7.944753646850586, -31.283946990966797], "cluster": 7}, {"key": "r\u00f6der2024deep", "year": "2024", "citations": "0", "title": "Deep Transfer Hashing For Adaptive Learning On Federated Streaming Data", "abstract": "<p>This extended abstract explores the integration of federated learning with\ndeep transfer hashing for distributed prediction tasks, emphasizing\nresource-efficient client training from evolving data streams. Federated\nlearning allows multiple clients to collaboratively train a shared model while\nmaintaining data privacy - by incorporating deep transfer hashing,\nhigh-dimensional data can be converted into compact hash codes, reducing data\ntransmission size and network loads. The proposed framework utilizes transfer\nlearning, pre-training deep neural networks on a central server, and\nfine-tuning on clients to enhance model accuracy and adaptability. A selective\nhash code sharing mechanism using a privacy-preserving global memory bank\nfurther supports client fine-tuning. This approach addresses challenges in\nprevious research by improving computational efficiency and scalability.\nPractical applications include Car2X event predictions, where a shared model is\ncollectively trained to recognize traffic patterns, aiding in tasks such as\ntraffic density assessment and accident detection. The research aims to develop\na robust framework that combines federated learning, deep transfer hashing and\ntransfer learning for efficient and secure downstream task execution.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Hashing-Methods", "Scalability"], "tsne_embedding": [28.610668182373047, -6.546314716339111], "cluster": 6}, {"key": "s2020bag", "year": "2020", "citations": "1", "title": "A Bag Of Visual Words Model For Medical Image Retrieval", "abstract": "<p>Medical Image Retrieval is a challenging field in Visual information\nretrieval, due to the multi-dimensional and multi-modal context of the\nunderlying content. Traditional models often fail to take the intrinsic\ncharacteristics of data into consideration, and have thus achieved limited\naccuracy when applied to medical images. The Bag of Visual Words (BoVW) is a\ntechnique that can be used to effectively represent intrinsic image features in\nvector space, so that applications like image classification and similar-image\nsearch can be optimized. In this paper, we present a MedIR approach based on\nthe BoVW model for content-based medical image retrieval. As medical images as\nmulti-dimensional, they exhibit underlying cluster and manifold information\nwhich enhances semantic relevance and allows for label uniformity. Hence, the\nBoVW features extracted for each image are used to train a supervised machine\nlearning classifier based on positive and negative training images, for\nextending content based image retrieval. During experimental validation, the\nproposed model performed very well, achieving a Mean Average Precision of\n88.89% during top-3 image retrieval experiments.</p>\n", "tags": ["Supervised", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-49.986289978027344, 16.15639305114746], "cluster": 0}, {"key": "saberi2024drew", "year": "2024", "citations": "0", "title": "DREW : Towards Robust Data Provenance By Leveraging Error-controlled Watermarking", "abstract": "<p>Identifying the origin of data is crucial for data provenance, with\napplications including data ownership protection, media forensics, and\ndetecting AI-generated content. A standard approach involves embedding-based\nretrieval techniques that match query data with entries in a reference dataset.\nHowever, this method is not robust against benign and malicious edits. To\naddress this, we propose Data Retrieval with Error-corrected codes and\nWatermarking (DREW). DREW randomly clusters the reference dataset, injects\nunique error-controlled watermark keys into each cluster, and uses these keys\nat query time to identify the appropriate cluster for a given sample. After\nlocating the relevant cluster, embedding vector similarity retrieval is\nperformed within the cluster to find the most accurate matches. The integration\nof error control codes (ECC) ensures reliable cluster assignments, enabling the\nmethod to perform retrieval on the entire dataset in case the ECC algorithm\ncannot detect the correct cluster with high confidence. This makes DREW\nmaintain baseline performance, while also providing opportunities for\nperformance improvements due to the increased likelihood of correctly matching\nqueries to their origin when performing retrieval on a smaller subset of the\ndataset. Depending on the watermark technique used, DREW can provide\nsubstantial improvements in retrieval accuracy (up to 40% for some datasets\nand modification types) across multiple datasets and state-of-the-art embedding\nmodels (e.g., DinoV2, CLIP), making our method a promising solution for secure\nand reliable source identification. The code is available at\nhttps://github.com/mehrdadsaberi/DREW</p>\n", "tags": ["Efficiency", "Evaluation", "Similarity-Search", "Datasets"], "tsne_embedding": [-19.725494384765625, 42.93343734741211], "cluster": 8}, {"key": "sablayrolles2016how", "year": "2017", "citations": "97", "title": "How Should We Evaluate Supervised Hashing?", "abstract": "<p>Hashing produces compact representations for documents, to perform tasks like\nclassification or retrieval based on these short codes. When hashing is\nsupervised, the codes are trained using labels on the training data. This paper\nfirst shows that the evaluation protocols used in the literature for supervised\nhashing are not satisfactory: we show that a trivial solution that encodes the\noutput of a classifier significantly outperforms existing supervised or\nsemi-supervised methods, while using much shorter codes. We then propose two\nalternative protocols for supervised hashing: one based on retrieval on a\ndisjoint set of classes, and another based on transfer learning to new classes.\nWe provide two baseline methods for image-related tasks to assess the\nperformance of (semi-)supervised hashing: without coding and with unsupervised\ncodes. These baselines give a lower- and upper-bound on the performance of a\nsupervised hashing scheme.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "ICASSP", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [8.036580085754395, 0.29733237624168396], "cluster": 6}, {"key": "sadeh2019joint", "year": "2019", "citations": "4", "title": "Joint Visual-textual Embedding For Multimodal Style Search", "abstract": "<p>We introduce a multimodal visual-textual search refinement method for fashion\ngarments. Existing search engines do not enable intuitive, interactive,\nrefinement of retrieved results based on the properties of a particular\nproduct. We propose a method to retrieve similar items, based on a query item\nimage and textual refinement properties. We believe this method can be\nleveraged to solve many real-life customer scenarios, in which a similar item\nin a different color, pattern, length or style is desired. We employ a joint\nembedding training scheme in which product images and their catalog textual\nmetadata are mapped closely in a shared space. This joint visual-textual\nembedding space enables manipulating catalog images semantically, based on\ntextual refinement requirements. We propose a new training objective function,\nMini-Batch Match Retrieval, and demonstrate its superiority over the commonly\nused triplet loss. Additionally, we demonstrate the feasibility of adding an\nattribute extraction module, trained on the same catalog data, and demonstrate\nhow to integrate it within the multimodal search to boost its performance. We\nintroduce an evaluation protocol with an associated benchmark, and compare\nseveral approaches.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-17.25601577758789, -47.767578125], "cluster": 3}, {"key": "sain2020cross", "year": "2020", "citations": "21", "title": "Cross-modal Hierarchical Modelling For Fine-grained Sketch Based Image Retrieval", "abstract": "<p>Sketch as an image search query is an ideal alternative to text in capturing\nthe fine-grained visual details. Prior successes on fine-grained sketch-based\nimage retrieval (FG-SBIR) have demonstrated the importance of tackling the\nunique traits of sketches as opposed to photos, e.g., temporal vs. static,\nstrokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a\nfurther trait of sketches that has been overlooked to date, that is, they are\nhierarchical in terms of the levels of detail \u2013 a person typically sketches up\nto various extents of detail to depict an object. This hierarchical structure\nis often visually distinct. In this paper, we design a novel network that is\ncapable of cultivating sketch-specific hierarchies and exploiting them to match\nsketch with photo at corresponding hierarchical levels. In particular, features\nfrom a sketch and a photo are enriched using cross-modal co-attention, coupled\nwith hierarchical node fusion at every level to form a better embedding space\nto conduct retrieval. Experiments on common benchmarks show our method to\noutperform state-of-the-arts by a significant margin.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [-44.36518096923828, -19.99348258972168], "cluster": 5}, {"key": "sain2023exploiting", "year": "2023", "citations": "19", "title": "Exploiting Unlabelled Photos For Stronger Fine-grained SBIR", "abstract": "<p>This paper advances the fine-grained sketch-based image retrieval (FG-SBIR)\nliterature by putting forward a strong baseline that overshoots prior\nstate-of-the-arts by ~11%. This is not via complicated design though, but by\naddressing two critical issues facing the community (i) the gold standard\ntriplet loss does not enforce holistic latent space geometry, and (ii) there\nare never enough sketches to train a high accuracy model. For the former, we\npropose a simple modification to the standard triplet loss, that explicitly\nenforces separation amongst photos/sketch instances. For the latter, we put\nforward a novel knowledge distillation module can leverage photo data for model\ntraining. Both modules are then plugged into a novel plug-n-playable training\nparadigm that allows for more stable training. More specifically, for (i) we\nemploy an intra-modal triplet loss amongst sketches to bring sketches of the\nsame instance closer from others, and one more amongst photos to push away\ndifferent photo instances while bringing closer a structurally augmented\nversion of the same photo (offering a gain of ~4-6%). To tackle (ii), we first\npre-train a teacher on the large set of unlabelled photos over the\naforementioned intra-modal photo triplet loss. Then we distill the contextual\nsimilarity present amongst the instances in the teacher\u2019s embedding space to\nthat in the student\u2019s embedding space, by matching the distribution over\ninter-feature distances of respective samples in both embedding spaces\n(delivering a further gain of ~4-5%). Apart from outperforming prior arts\nsignificantly, our model also yields satisfactory results on generalising to\nnew classes. Project page: https://aneeshan95.github.io/Sketch_PVT/</p>\n", "tags": ["CVPR", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-26.708206176757812, -13.9301118850708], "cluster": 5}, {"key": "saket2023monitoring", "year": "2024", "citations": "1", "title": "Monitoring The Evolution Of Behavioural Embeddings In Social Media Recommendation", "abstract": "<p>Emerging short-video platforms like TikTok, Instagram Reels, and ShareChat\npresent unique challenges for recommender systems, primarily originating from a\ncontinuous stream of new content. ShareChat alone receives approximately 2\nmillion pieces of fresh content daily, complicating efforts to assess quality,\nlearn effective latent representations, and accurately match content with the\nappropriate user base, especially given limited user feedback. Embedding-based\napproaches are a popular choice for industrial recommender systems because they\ncan learn low-dimensional representations of items, leading to effective\nrecommendation that can easily scale to millions of items and users.\n  Our work characterizes the evolution of such embeddings in short-video\nrecommendation systems, comparing the effect of batch and real-time updates to\ncontent embeddings. We investigate <em>how</em> embeddings change with subsequent\nupdates, explore the relationship between embeddings and popularity bias, and\nhighlight their impact on user engagement metrics. Our study unveils the\ncontrast in the number of interactions needed to achieve mature embeddings in a\nbatch learning setup versus a real-time one, identifies the point of highest\ninformation updates, and explores the distribution of \\(\u2113\u2082\\)-norms across the\ntwo competing learning modes. Utilizing a production system deployed on a\nlarge-scale short-video app with over 180 million users, our findings offer\ninsights into designing effective recommendation systems and enhancing user\nsatisfaction and engagement in short-video applications.</p>\n", "tags": ["Efficiency", "SIGIR", "Recommender-Systems", "Scalability"], "tsne_embedding": [24.195547103881836, -29.970233917236328], "cluster": 7}, {"key": "salakhutdinov2008semantic", "year": "2008", "citations": "1272", "title": "Semantic Hashing", "abstract": "<p>We show how to learn a deep graphical model of the word-count\nvectors obtained from a large set of documents. The values of the\nlatent variables in the deepest layer are easy to infer and give a\nmuch better representation of each document than Latent Semantic\nAnalysis. When the deepest layer is forced to use a small number of\nbinary variables (e.g. 32), the graphical model performs \u201csemantic\nhashing\u201d: Documents are mapped to memory addresses in such a\nway that semantically similar documents are located at nearby addresses.\nDocuments similar to a query document can then be found\nby simply accessing all the addresses that differ by only a few bits\nfrom the address of the query document. This way of extending the\nefficiency of hash-coding to approximate matching is much faster\nthan locality sensitive hashing, which is the fastest current method.\nBy using semantic hashing to filter the documents given to TF-IDF,\nwe achieve higher accuracy than applying TF-IDF to the entire document\nset.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Text-Retrieval"], "tsne_embedding": [2.4139816761016846, 29.508771896362305], "cluster": 8}, {"key": "salakhutdinov2025semantic", "year": "2008", "citations": "1272", "title": "Semantic Hashing", "abstract": "<p>We show how to learn a deep graphical model of the word-count\nvectors obtained from a large set of documents. The values of the\nlatent variables in the deepest layer are easy to infer and give a\nmuch better representation of each document than Latent Semantic\nAnalysis. When the deepest layer is forced to use a small number of\nbinary variables (e.g. 32), the graphical model performs \u201csemantic\nhashing\u201d: Documents are mapped to memory addresses in such a\nway that semantically similar documents are located at nearby addresses.\nDocuments similar to a query document can then be found\nby simply accessing all the addresses that differ by only a few bits\nfrom the address of the query document. This way of extending the\nefficiency of hash-coding to approximate matching is much faster\nthan locality sensitive hashing, which is the fastest current method.\nBy using semantic hashing to filter the documents given to TF-IDF,\nwe achieve higher accuracy than applying TF-IDF to the entire document\nset.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Locality-Sensitive-Hashing", "Text-Retrieval"], "tsne_embedding": [2.4139862060546875, 29.508764266967773], "cluster": 8}, {"key": "salemi2023symmetric", "year": "2023", "citations": "13", "title": "A Symmetric Dual Encoding Dense Retrieval Framework For Knowledge-intensive Visual Question Answering", "abstract": "<p>Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a\nquestion about an image whose answer does not lie in the image. This paper\npresents a new pipeline for KI-VQA tasks, consisting of a retriever and a\nreader. First, we introduce DEDR, a symmetric dual encoding dense retrieval\nframework in which documents and queries are encoded into a shared embedding\nspace using uni-modal (textual) and multi-modal encoders. We introduce an\niterative knowledge distillation approach that bridges the gap between the\nrepresentation spaces in these two encoders. Extensive evaluation on two\nwell-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR\noutperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA,\nrespectively. Utilizing the passages retrieved by DEDR, we further introduce\nMM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating\na textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and\neach retrieved passage separately and uses all passages jointly in its decoder.\nCompared to competitive baselines in the literature, this approach leads to\n5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA\nand FVQA, respectively.</p>\n", "tags": ["SIGIR", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [-26.866626739501953, -29.325082778930664], "cluster": 5}, {"key": "salemohamed2024discovering", "year": "2024", "citations": "0", "title": "Discovering Data Structures: Nearest Neighbor Search And Beyond", "abstract": "<p>We propose a general framework for end-to-end learning of data structures.\nOur framework adapts to the underlying data distribution and provides\nfine-grained control over query and space complexity. Crucially, the data\nstructure is learned from scratch, and does not require careful initialization\nor seeding with candidate data structures/algorithms. We first apply this\nframework to the problem of nearest neighbor search. In several settings, we\nare able to reverse-engineer the learned data structures and query algorithms.\nFor 1D nearest neighbor search, the model discovers optimal distribution\n(in)dependent algorithms such as binary search and variants of interpolation\nsearch. In higher dimensions, the model learns solutions that resemble k-d\ntrees in some regimes, while in others, they have elements of\nlocality-sensitive hashing. The model can also learn useful representations of\nhigh-dimensional data and exploit them to design effective data structures. We\nalso adapt our framework to the problem of estimating frequencies over a data\nstream, and believe it could also be a powerful discovery tool for new\nproblems.</p>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods"], "tsne_embedding": [37.63240432739258, -6.105226516723633], "cluster": 9}, {"key": "salman2024robustness", "year": "2025", "citations": "0", "title": "On The Robustness Of Malware Detectors To Adversarial Samples", "abstract": "<p>Adversarial examples add imperceptible alterations to inputs with the\nobjective to induce misclassification in machine learning models. They have\nbeen demonstrated to pose significant challenges in domains like image\nclassification, with results showing that an adversarially perturbed image to\nevade detection against one classifier is most likely transferable to other\nclassifiers. Adversarial examples have also been studied in malware analysis.\nUnlike images, program binaries cannot be arbitrarily perturbed without\nrendering them non-functional. Due to the difficulty of crafting adversarial\nprogram binaries, there is no consensus on the transferability of adversarially\nperturbed programs to different detectors. In this work, we explore the\nrobustness of malware detectors against adversarially perturbed malware. We\ninvestigate the transferability of adversarial attacks developed against one\ndetector, against other machine learning-based malware detectors, and code\nsimilarity techniques, specifically, locality sensitive hashing-based\ndetectors. Our analysis reveals that adversarial program binaries crafted for\none detector are generally less effective against others. We also evaluate an\nensemble of detectors and show that they can potentially mitigate the impact of\nadversarial program binaries. Finally, we demonstrate that substantial program\nchanges made to evade detection may result in the transformation technique\nbeing identified, implying that the adversary must make minimal changes to the\nprogram binary.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Robustness"], "tsne_embedding": [29.368207931518555, -17.02706527709961], "cluster": 7}, {"key": "salvador2016faster", "year": "2016", "citations": "121", "title": "Faster R-CNN Features For Instance Search", "abstract": "<p>Image representations derived from pre-trained Convolutional Neural Networks\n(CNNs) have become the new state of the art in computer vision tasks such as\ninstance retrieval. This work explores the suitability for instance retrieval\nof image- and region-wise representations pooled from an object detection CNN\nsuch as Faster R-CNN. We take advantage of the object proposals learned by a\nRegion Proposal Network (RPN) and their associated CNN features to build an\ninstance search pipeline composed of a first filtering stage followed by a\nspatial reranking. We further investigate the suitability of Faster R-CNN\nfeatures when the network is fine-tuned for the same objects one wants to\nretrieve. We assess the performance of our proposed system with the Oxford\nBuildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013,\nachieving competitive results.</p>\n", "tags": ["CVPR", "Evaluation"], "tsne_embedding": [-53.61811447143555, 0.774338960647583], "cluster": 0}, {"key": "salvador2021revamping", "year": "2021", "citations": "61", "title": "Revamping Cross-modal Recipe Retrieval With Hierarchical Transformers And Self-supervised Learning", "abstract": "<p>Cross-modal recipe retrieval has recently gained substantial attention due to\nthe importance of food in people\u2019s lives, as well as the availability of vast\namounts of digital cooking recipes and food images to train machine learning\nmodels. In this work, we revisit existing approaches for cross-modal recipe\nretrieval and propose a simplified end-to-end model based on well established\nand high performing encoders for text and images. We introduce a hierarchical\nrecipe Transformer which attentively encodes individual recipe components\n(titles, ingredients and instructions). Further, we propose a self-supervised\nloss function computed on top of pairs of individual recipe components, which\nis able to leverage semantic relationships within recipes, and enables training\nusing both image-recipe and recipe-only samples. We conduct a thorough analysis\nand ablation studies to validate our design choices. As a result, our proposed\nmethod achieves state-of-the-art performance in the cross-modal recipe\nretrieval task on the Recipe1M dataset. We make code and models publicly\navailable.</p>\n", "tags": ["Self-Supervised", "CVPR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-42.655731201171875, 31.870973587036133], "cluster": 0}, {"key": "salvatori2024associative", "year": "2023", "citations": "1", "title": "Associative Memories In The Feature Space", "abstract": "<p>An autoassociative memory model is a function that, given a set of data\npoints, takes as input an arbitrary vector and outputs the most similar data\npoint from the memorized set. However, popular memory models fail to retrieve\nimages even when the corruption is mild and easy to detect for a human\nevaluator. This is because similarities are evaluated in the raw pixel space,\nwhich does not contain any semantic information about the images. This problem\ncan be easily solved by computing <em>similarities</em> in an embedding space\ninstead of the pixel space. We show that an effective way of computing such\nembeddings is via a network pretrained with a contrastive loss. As the\ndimension of embedding spaces is often significantly smaller than the pixel\nspace, we also have a faster computation of similarity scores. We test this\nmethod on complex datasets such as CIFAR10 and STL10. An additional drawback of\ncurrent models is the need of storing the whole dataset in the pixel space,\nwhich is often extremely large. We relax this condition and propose a class of\nmemory models that only stores low-dimensional semantic embeddings, and uses\nthem to retrieve similar, but not identical, memories. We demonstrate a proof\nof concept of this method on a simple task on the MNIST dataset.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [30.79030990600586, 21.628084182739258], "cluster": 2}, {"key": "salvi2016bloom", "year": "2016", "citations": "5", "title": "Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval", "abstract": "<p>This paper presents a novel method for efficient image retrieval, based on a\nsimple and effective hashing of CNN features and the use of an indexing\nstructure based on Bloom filters. These filters are used as gatekeepers for the\ndatabase of image features, allowing to avoid to perform a query if the query\nfeatures are not stored in the database and speeding up the query process,\nwithout affecting retrieval performance. Thanks to the limited memory\nrequirements the system is suitable for mobile applications and distributed\ndatabases, associating each filter to a distributed portion of the database.\nExperimental validation has been performed on three standard image retrieval\ndatasets, outperforming state-of-the-art hashing methods in terms of precision,\nwhile the proposed indexing method obtains a \\(2\\times\\) speedup.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-45.186214447021484, 2.1834988594055176], "cluster": 0}, {"key": "sami2022benchmarking", "year": "2022", "citations": "5", "title": "Benchmarking Human Face Similarity Using Identical Twins", "abstract": "<p>The problem of distinguishing identical twins and non-twin look-alikes in\nautomated facial recognition (FR) applications has become increasingly\nimportant with the widespread adoption of facial biometrics. Due to the high\nfacial similarity of both identical twins and look-alikes, these face pairs\nrepresent the hardest cases presented to facial recognition tools. This work\npresents an application of one of the largest twin datasets compiled to date to\naddress two FR challenges: 1) determining a baseline measure of facial\nsimilarity between identical twins and 2) applying this similarity measure to\ndetermine the impact of doppelgangers, or look-alikes, on FR performance for\nlarge face datasets. The facial similarity measure is determined via a deep\nconvolutional neural network. This network is trained on a tailored\nverification task designed to encourage the network to group together highly\nsimilar face pairs in the embedding space and achieves a test AUC of 0.9799.\nThe proposed network provides a quantitative similarity score for any two given\nfaces and has been applied to large-scale face datasets to identify similar\nface pairs. An additional analysis which correlates the comparison score\nreturned by a facial recognition tool and the similarity score returned by the\nproposed network has also been performed.</p>\n", "tags": ["Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-11.858527183532715, 5.514471054077148], "cluster": 1}, {"key": "sanakoyeu2019divide", "year": "2019", "citations": "122", "title": "Divide And Conquer The Embedding Space For Metric Learning", "abstract": "<p>Learning the embedding space, where semantically similar objects are located\nclose together and dissimilar objects far apart, is a cornerstone of many\ncomputer vision applications. Existing approaches usually learn a single metric\nin the embedding space for all available data points, which may have a very\ncomplex non-uniform distribution with different notions of similarity between\nobjects, e.g. appearance, shape, color or semantic meaning. Approaches for\nlearning a single distance metric often struggle to encode all different types\nof relationships and do not generalize well. In this work, we propose a novel\neasy-to-implement divide and conquer approach for deep metric learning, which\nsignificantly improves the state-of-the-art performance of metric learning. Our\napproach utilizes the embedding space more efficiently by jointly splitting the\nembedding space and data into \\(K\\) smaller sub-problems. It divides both, the\ndata and the embedding space into \\(K\\) subsets and learns \\(K\\) separate distance\nmetrics in the non-overlapping subspaces of the embedding space, defined by\ngroups of neurons in the embedding layer of the neural network. The proposed\napproach increases the convergence speed and improves generalization since the\ncomplexity of each sub-problem is reduced compared to the original one. We show\nthat our approach outperforms the state-of-the-art by a large margin in\nretrieval, clustering and re-identification tasks on CUB200-2011, CARS196,\nStanford Online Products, In-shop Clothes and PKU VehicleID datasets.</p>\n", "tags": ["CVPR", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-12.23946475982666, -14.535867691040039], "cluster": 1}, {"key": "sanakoyeu2021improving", "year": "2021", "citations": "16", "title": "Improving Deep Metric Learning By Divide And Conquer", "abstract": "<p>Deep metric learning (DML) is a cornerstone of many computer vision\napplications. It aims at learning a mapping from the input domain to an\nembedding space, where semantically similar objects are located nearby and\ndissimilar objects far from another. The target similarity on the training data\nis defined by user in form of ground-truth class labels. However, while the\nembedding space learns to mimic the user-provided similarity on the training\ndata, it should also generalize to novel categories not seen during training.\nBesides user-provided groundtruth training labels, a lot of additional visual\nfactors (such as viewpoint changes or shape peculiarities) exist and imply\ndifferent notions of similarity between objects, affecting the generalization\non the images unseen during training. However, existing approaches usually\ndirectly learn a single embedding space on all available training data,\nstruggling to encode all different types of relationships, and do not\ngeneralize well. We propose to build a more expressive representation by\njointly splitting the embedding space and the data hierarchically into smaller\nsub-parts. We successively focus on smaller subsets of the training data,\nreducing its variance and learning a different embedding subspace for each data\nsubset. Moreover, the subspaces are learned jointly to cover not only the\nintricacies, but the breadth of the data as well. Only after that, we build the\nfinal embedding from the subspaces in the conquering stage. The proposed\nalgorithm acts as a transparent wrapper that can be placed around arbitrary\nexisting DML methods. Our approach significantly improves upon the\nstate-of-the-art on image retrieval, clustering, and re-identification tasks\nevaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop\nClothes, and PKU VehicleID datasets.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-26.080821990966797, -14.062198638916016], "cluster": 5}, {"key": "sangkloy2022sketch", "year": "2022", "citations": "24", "title": "A Sketch Is Worth A Thousand Words: Image Retrieval With Text And Sketch", "abstract": "<p>We address the problem of retrieving images with both a sketch and a text\nquery. We present TASK-former (Text And SKetch transformer), an end-to-end\ntrainable model for image retrieval using a text description and a sketch as\ninput. We argue that both input modalities complement each other in a manner\nthat cannot be achieved easily by either one alone. TASK-former follows the\nlate-fusion dual-encoder approach, similar to CLIP, which allows efficient and\nscalable retrieval since the retrieval set can be indexed independently of the\nqueries. We empirically demonstrate that using an input sketch (even a poorly\ndrawn one) in addition to text considerably increases retrieval recall compared\nto traditional text-based image retrieval. To evaluate our approach, we collect\n5,000 hand-drawn sketches for images in the test set of the COCO dataset. The\ncollected sketches are available a https://janesjanes.github.io/tsbir/.</p>\n", "tags": ["Evaluation", "Large-Scale-Search", "Image-Retrieval", "Datasets"], "tsne_embedding": [-41.70074462890625, -18.697330474853516], "cluster": 5}, {"key": "sankar2019transferable", "year": "2019", "citations": "7", "title": "Transferable Neural Projection Representations", "abstract": "<p>Neural word representations are at the core of many state-of-the-art natural\nlanguage processing models. A widely used approach is to pre-train, store and\nlook up word or character embedding matrices. While useful, such\nrepresentations occupy huge memory making it hard to deploy on-device and often\ndo not generalize to unknown words due to vocabulary pruning.\n  In this paper, we propose a skip-gram based architecture coupled with\nLocality-Sensitive Hashing (LSH) projections to learn efficient dynamically\ncomputable representations. Our model does not need to store lookup tables as\nrepresentations are computed on-the-fly and require low memory footprint. The\nrepresentations can be trained in an unsupervised fashion and can be easily\ntransferred to other NLP tasks. For qualitative evaluation, we analyze the\nnearest neighbors of the word representations and discover semantically similar\nwords even with misspellings. For quantitative evaluation, we plug our\ntransferable projections into a simple LSTM and run it on multiple NLP tasks\nand show how our transferable projections achieve better performance compared\nto prior work.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Memory-Efficiency", "Evaluation", "Unsupervised"], "tsne_embedding": [22.308860778808594, 9.655669212341309], "cluster": 2}, {"key": "sankaranarayanan2016triplet", "year": "2016", "citations": "47", "title": "Triplet Similarity Embedding For Face Verification", "abstract": "<p>In this work, we present an unconstrained face verification algorithm and\nevaluate it on the recently released IJB-A dataset that aims to push the\nboundaries of face verification methods. The proposed algorithm couples a deep\nCNN-based approach with a low-dimensional discriminative embedding learnt using\ntriplet similarity constraints in a large margin fashion. Aside from yielding\nperformance improvement, this embedding provides significant advantages in\nterms of memory and post-processing operations like hashing and visualization.\nExperiments on the IJB-A dataset show that the proposed algorithm outperforms\nstate of the art methods in verification and identification metrics, while\nrequiring less training time.</p>\n", "tags": ["Re-Ranking", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [-41.711124420166016, 2.1020710468292236], "cluster": 0}, {"key": "sankowski2017approximate", "year": "2017", "citations": "1", "title": "Approximate Nearest Neighbors Search Without False Negatives For \\(l_2\\) For \\(c>\\sqrt{\\log\\log{n}}\\)", "abstract": "<p>In this paper, we report progress on answering the open problem presented by\nPagh~[14], who considered the nearest neighbor search without false negatives\nfor the Hamming distance. We show new data structures for solving the\n\\(c\\)-approximate nearest neighbors problem without false negatives for Euclidean\nhigh dimensional space \\(\\mathcal{R}^d\\). These data structures work for any \\(c =\n\\omega(\\sqrt{log{log{n}}})\\), where \\(n\\) is the number of points in the input\nset, with poly-logarithmic query time and polynomial preprocessing time. This\nimproves over the known algorithms, which require \\(c\\) to be \\(\u03a9(\\sqrt{d})\\).\n  This improvement is obtained by applying a sequence of reductions, which are\ninteresting on their own. First, we reduce the problem to \\(d\\) instances of\ndimension logarithmic in \\(n\\). Next, these instances are reduced to a number of\n\\(c\\)-approximate nearest neighbor search instances in \\(\\big(\\mathbb{R}^k\\big)^L\\)\nspace equipped with metric \\(m(x,y) = \\max_{1 \\le i \\le L}(\\lVert x_i -\ny_i\\rVert_2)\\).</p>\n", "tags": ["Efficiency"], "tsne_embedding": [21.583024978637695, 45.874385833740234], "cluster": 4}, {"key": "santhanam2021colbertv2", "year": "2022", "citations": "163", "title": "Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction", "abstract": "<p>Neural information retrieval (IR) has greatly advanced search and other\nknowledge-intensive language tasks. While many neural IR methods encode queries\nand documents into single-vector representations, late interaction models\nproduce multi-vector representations at the granularity of each token and\ndecompose relevance modeling into scalable token-level computations. This\ndecomposition has been shown to make late interaction more effective, but it\ninflates the space footprint of these models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\ncompression mechanism with a denoised supervision strategy to simultaneously\nimprove the quality and space footprint of late interaction. We evaluate\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\nquality within and outside the training domain while reducing the space\nfootprint of late interaction models by 6\u201310\\(\\times\\).</p>\n", "tags": ["Similarity-Search"], "tsne_embedding": [14.303040504455566, -26.583518981933594], "cluster": 7}, {"key": "sarafijanovicdjukic2020fast", "year": "2019", "citations": "32", "title": "Fast Distance-based Anomaly Detection In Images Using An Inception-like Autoencoder", "abstract": "<p>The goal of anomaly detection is to identify examples that deviate from\nnormal or expected behavior. We tackle this problem for images. We consider a\ntwo-phase approach. First, using normal examples, a convolutional autoencoder\n(CAE) is trained to extract a low-dimensional representation of the images.\nHere, we propose a novel architectural choice when designing the CAE, an\nInception-like CAE. It combines convolutional filters of different kernel sizes\nand it uses a Global Average Pooling (GAP) operation to extract the\nrepresentations from the CAE\u2019s bottleneck layer. Second, we employ a\ndistanced-based anomaly detector in the low-dimensional space of the learned\nrepresentation for the images. However, instead of computing the exact\ndistance, we compute an approximate distance using product quantization. This\nalleviates the high memory and prediction time costs of distance-based anomaly\ndetectors. We compare our proposed approach to a number of baselines and\nstate-of-the-art methods on four image datasets, and we find that our approach\nresulted in improved predictive performance.</p>\n", "tags": ["Quantization", "Evaluation", "Datasets"], "tsne_embedding": [-27.683956146240234, 7.82074499130249], "cluster": 0}, {"key": "sarfraz2017pose", "year": "2018", "citations": "521", "title": "A Pose-sensitive Embedding For Person Re-identification With Expanded Cross Neighborhood Re-ranking", "abstract": "<p>Person re identification is a challenging retrieval task that requires\nmatching a person\u2019s acquired image across non overlapping camera views. In this\npaper we propose an effective approach that incorporates both the fine and\ncoarse pose information of the person to learn a discriminative embedding. In\ncontrast to the recent direction of explicitly modeling body parts or\ncorrecting for misalignment based on these, we show that a rather\nstraightforward inclusion of acquired camera view and/or the detected joint\nlocations into a convolutional neural network helps to learn a very effective\nrepresentation. To increase retrieval performance, re-ranking techniques based\non computed distances have recently gained much attention. We propose a new\nunsupervised and automatic re-ranking framework that achieves state-of-the-art\nre-ranking performance. We show that in contrast to the current\nstate-of-the-art re-ranking methods our approach does not require to compute\nnew rank lists for each image pair (e.g., based on reciprocal neighbors) and\nperforms well by using simple direct rank list based comparison or even by just\nusing the already computed euclidean distances between the images. We show that\nboth our learned representation and our re-ranking method achieve\nstate-of-the-art performance on a number of challenging surveillance image and\nvideo datasets.\n  The code is available online at:\nhttps://github.com/pse-ecn/pose-sensitive-embedding</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Tools-&-Libraries", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation", "Unsupervised"], "tsne_embedding": [-32.374046325683594, 8.268202781677246], "cluster": 0}, {"key": "sarkar2024dual", "year": "2024", "citations": "0", "title": "Dual Pose-invariant Embeddings: Learning Category And Object-specific Discriminative Representations For Recognition And Retrieval", "abstract": "<p>In the context of pose-invariant object recognition and retrieval, we\ndemonstrate that it is possible to achieve significant improvements in\nperformance if both the category-based and the object-identity-based embeddings\nare learned simultaneously during training. In hindsight, that sounds intuitive\nbecause learning about the categories is more fundamental than learning about\nthe individual objects that correspond to those categories. However, to the\nbest of what we know, no prior work in pose-invariant learning has demonstrated\nthis effect. This paper presents an attention-based dual-encoder architecture\nwith specially designed loss functions that optimize the inter- and intra-class\ndistances simultaneously in two different embedding spaces, one for the\ncategory embeddings and the other for the object-level embeddings. The loss\nfunctions we have proposed are pose-invariant ranking losses that are designed\nto minimize the intra-class distances and maximize the inter-class distances in\nthe dual representation spaces. We demonstrate the power of our approach with\nthree challenging multi-view datasets, ModelNet-40, ObjectPI, and FG3D. With\nour dual approach, for single-view object recognition, we outperform the\nprevious best by 20.0% on ModelNet40, 2.0% on ObjectPI, and 46.5% on FG3D. On\nthe other hand, for single-view object retrieval, we outperform the previous\nbest by 33.7% on ModelNet40, 18.8% on ObjectPI, and 56.9% on FG3D.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-28.853801727294922, -20.718645095825195], "cluster": 5}, {"key": "satar2022exploiting", "year": "2022", "citations": "1", "title": "Exploiting Semantic Role Contextualized Video Features For Multi-instance Text-video Retrieval EPIC-KITCHENS-100 Multi-instance Retrieval Challenge 2022", "abstract": "<p>In this report, we present our approach for EPIC-KITCHENS-100 Multi-Instance\nRetrieval Challenge 2022. We first parse sentences into semantic roles\ncorresponding to verbs and nouns; then utilize self-attentions to exploit\nsemantic role contextualized video features along with textual features via\ntriplet losses in multiple embedding spaces. Our method overpasses the strong\nbaseline in normalized Discounted Cumulative Gain (nDCG), which is more\nvaluable for semantic similarity. Our submission is ranked 3rd for nDCG and\nranked 4th for mAP.</p>\n", "tags": ["Evaluation", "Video-Retrieval", "Distance-Metric-Learning"], "tsne_embedding": [-43.20552444458008, 34.48446273803711], "cluster": 0}, {"key": "sbai2018unsupervised", "year": "2020", "citations": "8", "title": "Unsupervised Image Decomposition In Vector Layers", "abstract": "<p>Deep image generation is becoming a tool to enhance artists and designers\ncreativity potential. In this paper, we aim at making the generation process\nmore structured and easier to interact with. Inspired by vector graphics\nsystems, we propose a new deep image reconstruction paradigm where the outputs\nare composed from simple layers, defined by their color and a vector\ntransparency mask. This presents a number of advantages compared to the\ncommonly used convolutional network architectures. In particular, our layered\ndecomposition allows simple user interaction, for example to update a given\nmask, or change the color of a selected layer. From a compact code, our\narchitecture also generates vector images with a virtually infinite resolution,\nthe color at each point in an image being a parametric function of its\ncoordinates. We validate the efficiency of our approach by comparing\nreconstructions with state-of-the-art baselines given similar memory resources\non CelebA and ImageNet datasets. Most importantly, we demonstrate several\napplications of our new image representation obtained in an unsupervised\nmanner, including editing, vectorization and image search.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Datasets", "Compact-Codes", "Unsupervised"], "tsne_embedding": [-39.00347137451172, 4.593668460845947], "cluster": 0}, {"key": "sbai2020unsupervised", "year": "2020", "citations": "8", "title": "Unsupervised Image Decomposition In Vector Layers", "abstract": "<p>Deep image generation is becoming a tool to enhance artists and designers\ncreativity potential. In this paper, we aim at making the generation process\nmore structured and easier to interact with. Inspired by vector graphics\nsystems, we propose a new deep image reconstruction paradigm where the outputs\nare composed from simple layers, defined by their color and a vector\ntransparency mask. This presents a number of advantages compared to the\ncommonly used convolutional network architectures. In particular, our layered\ndecomposition allows simple user interaction, for example to update a given\nmask, or change the color of a selected layer. From a compact code, our\narchitecture also generates vector images with a virtually infinite resolution,\nthe color at each point in an image being a parametric function of its\ncoordinates. We validate the efficiency of our approach by comparing\nreconstructions with state-of-the-art baselines given similar memory resources\non CelebA and ImageNet datasets. Most importantly, we demonstrate several\napplications of our new image representation obtained in an unsupervised\nmanner, including editing, vectorization and image search.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "Datasets", "Compact-Codes", "Unsupervised"], "tsne_embedding": [-39.00347137451172, 4.593668460845947], "cluster": 0}, {"key": "schall2019deep", "year": "2019", "citations": "7", "title": "Deep Metric Learning Using Similarities From Nonlinear Rank Approximations", "abstract": "<p>In recent years, deep metric learning has achieved promising results in\nlearning high dimensional semantic feature embeddings where the spatial\nrelationships of the feature vectors match the visual similarities of the\nimages. Similarity search for images is performed by determining the vectors\nwith the smallest distances to a query vector. However, high retrieval quality\ndoes not depend on the actual distances of the feature vectors, but rather on\nthe ranking order of the feature vectors from similar images. In this paper, we\nintroduce a metric learning algorithm that focuses on identifying and modifying\nthose feature vectors that most strongly affect the retrieval quality. We\ncompute normalized approximated ranks and convert them to similarities by\napplying a nonlinear transfer function. These similarities are used in a newly\nproposed loss function that better contracts similar and disperses dissimilar\nsamples. Experiments demonstrate significant improvement over existing deep\nfeature embedding methods on the CUB-200-2011, Cars196, and Stanford Online\nProducts data sets for all embedding sizes.</p>\n", "tags": ["Distance-Metric-Learning", "Similarity-Search"], "tsne_embedding": [-16.144079208374023, -10.966639518737793], "cluster": 1}, {"key": "schall2021gpr1200", "year": "2022", "citations": "18", "title": "GPR1200: A Benchmark For General-purpose Content-based Image Retrieval", "abstract": "<p>Even though it has extensively been shown that retrieval specific training of\ndeep neural networks is beneficial for nearest neighbor image search quality,\nmost of these models are trained and tested in the domain of landmarks images.\nHowever, some applications use images from various other domains and therefore\nneed a network with good generalization properties - a general-purpose CBIR\nmodel. To the best of our knowledge, no testing protocol has so far been\nintroduced to benchmark models with respect to general image retrieval quality.\nAfter analyzing popular image retrieval test sets we decided to manually curate\nGPR1200, an easy to use and accessible but challenging benchmark dataset with a\nbroad range of image categories. This benchmark is subsequently used to\nevaluate various pretrained models of different architectures on their\ngeneralization qualities. We show that large-scale pretraining significantly\nimproves retrieval performance and present experiments on how to further\nincrease these properties by appropriate fine-tuning. With these promising\nresults, we hope to increase interest in the research topic of general-purpose\nCBIR.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-11.619184494018555, -7.578479290008545], "cluster": 1}, {"key": "schall2024optimizing", "year": "2024", "citations": "1", "title": "Optimizing CLIP Models For Image Retrieval With Maintained Joint-embedding Alignment", "abstract": "<p>Contrastive Language and Image Pairing (CLIP), a transformative method in\nmultimedia retrieval, typically trains two neural networks concurrently to\ngenerate joint embeddings for text and image pairs. However, when applied\ndirectly, these models often struggle to differentiate between visually\ndistinct images that have similar captions, resulting in suboptimal performance\nfor image-based similarity searches. This paper addresses the challenge of\noptimizing CLIP models for various image-based similarity search scenarios,\nwhile maintaining their effectiveness in text-based search tasks such as\ntext-to-image retrieval and zero-shot classification. We propose and evaluate\ntwo novel methods aimed at refining the retrieval capabilities of CLIP without\ncompromising the alignment between text and image embeddings. The first method\ninvolves a sequential fine-tuning process: initially optimizing the image\nencoder for more precise image retrieval and subsequently realigning the text\nencoder to these optimized image embeddings. The second approach integrates\npseudo-captions during the retrieval-optimization phase to foster direct\nalignment within the embedding space. Through comprehensive experiments, we\ndemonstrate that these methods enhance CLIP\u2019s performance on various\nbenchmarks, including image retrieval, k-NN classification, and zero-shot\ntext-based classification, while maintaining robustness in text-to-image\nretrieval. Our optimized models permit maintaining a single embedding per\nimage, significantly simplifying the infrastructure needed for large-scale\nmulti-modal similarity search systems.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Similarity-Search", "Image-Retrieval", "Scalability", "Robustness", "Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [-31.796499252319336, -30.268512725830078], "cluster": 5}, {"key": "scheerer2025warp", "year": "2025", "citations": "0", "title": "WARP: An Efficient Engine For Multi-vector Retrieval", "abstract": "<p>Multi-vector retrieval methods such as ColBERT and its recent variant, the ConteXtualized Token Retriever (XTR), offer high accuracy but face efficiency challenges at scale. To address this, we present WARP, a retrieval engine that substantially improves the efficiency of retrievers trained with the XTR objective through three key innovations: (1) WARP\\(_\\text{SELECT}\\) for dynamic similarity imputation; (2) implicit decompression, avoiding costly vector reconstruction during retrieval; and (3) a two-stage reduction process for efficient score aggregation. Combined with highly-optimized C++ kernels, our system reduces end-to-end latency compared to XTR\u2019s reference implementation by 41x, and achieves a 3x speedup over the ColBERTv2/PLAID engine, while preserving retrieval quality.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [15.268819808959961, -23.529800415039062], "cluster": 7}, {"key": "schiavo2021sketches", "year": "2021", "citations": "0", "title": "Sketches Image Analysis: Web Image Search Engine Usinglsh Index And DNN Inceptionv3", "abstract": "<p>The adoption of an appropriate approximate similarity search method is an\nessential prereq-uisite for developing a fast and efficient CBIR system,\nespecially when dealing with large amount ofdata. In this study we implement a\nweb image search engine on top of a Locality Sensitive Hashing(LSH) Index to\nallow fast similarity search on deep features. Specifically, we exploit\ntransfer learningfor deep features extraction from images. Firstly, we adopt\nInceptionV3 pretrained on ImageNet asfeatures extractor, secondly, we try out\nseveral CNNs built on top of InceptionV3 as convolutionalbase fine-tuned on our\ndataset. In both of the previous cases we index the features extracted within\nourLSH index implementation so as to compare the retrieval performances with\nand without fine-tuning.In our approach we try out two different LSH\nimplementations: the first one working with real numberfeature vectors and the\nsecond one with the binary transposed version of those vectors.\nInterestingly,we obtain the best performances when using the binary LSH,\nreaching almost the same result, in termsof mean average precision, obtained by\nperforming sequential scan of the features, thus avoiding thebias introduced by\nthe LSH index. Lastly, we carry out a performance analysis class by class in\nterms ofrecall againstmAPhighlighting, as expected, a strong positive\ncorrelation between the two.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-51.43503189086914, -4.0701189041137695], "cluster": 0}, {"key": "schindler2020deep", "year": "2020", "citations": "1", "title": "Deep Learning For MIR Tutorial", "abstract": "<p>Deep Learning has become state of the art in visual computing and\ncontinuously emerges into the Music Information Retrieval (MIR) and audio\nretrieval domain. In order to bring attention to this topic we propose an\nintroductory tutorial on deep learning for MIR. Besides a general introduction\nto neural networks, the proposed tutorial covers a wide range of MIR relevant\ndeep learning approaches. \\textbf{Convolutional Neural Networks} are currently\na de-facto standard for deep learning based audio retrieval. \\textbf{Recurrent\nNeural Networks} have proven to be effective in onset detection tasks such as\nbeat or audio-event detection. \\textbf{Siamese Networks} have been shown\neffective in learning audio representations and distance functions specific for\nmusic similarity retrieval. We will incorporate both academic and industrial\npoints of view into the tutorial. Accompanying the tutorial, we will create a\nGithub repository for the content presented at the tutorial as well as\nreferences to state of the art work and literature for further reading. This\nrepository will remain public after the conference.</p>\n", "tags": ["Similarity-Search"], "tsne_embedding": [-7.318111896514893, -23.472591400146484], "cluster": 3}, {"key": "schlegel2018adding", "year": "2019", "citations": "6", "title": "Adding Cues To Binary Feature Descriptors For Visual Place Recognition", "abstract": "<p>In this paper we propose an approach to embed continuous and selector cues in\nbinary feature descriptors used for visual place recognition. The embedding is\nachieved by extending each feature descriptor with a binary string that encodes\na cue and supports the Hamming distance metric. Augmenting the descriptors in\nsuch a way has the advantage of being transparent to the procedure used to\ncompare them. We present two concrete applications of our methodology,\ndemonstrating the two considered types of cues. In addition to that, we\nconducted on these applications a broad quantitative and comparative evaluation\ncovering five benchmark datasets and several state-of-the-art image retrieval\napproaches in combination with various binary descriptor types.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-24.448822021484375, -30.817279815673828], "cluster": 5}, {"key": "schlegel2018hbst", "year": "2018", "citations": "54", "title": "HBST: A Hamming Distance Embedding Binary Search Tree For Visual Place Recognition", "abstract": "<p>Reliable and efficient Visual Place Recognition is a major building block of\nmodern SLAM systems. Leveraging on our prior work, in this paper we present a\nHamming Distance embedding Binary Search Tree (HBST) approach for binary\nDescriptor Matching and Image Retrieval. HBST allows for descriptor Search and\nInsertion in logarithmic time by exploiting particular properties of binary\nFeature descriptors. We support the idea behind our search structure with a\nthorough analysis on the exploited descriptor properties and their effects on\ncompleteness and complexity of search and insertion. To validate our claims we\nconducted comparative experiments for HBST and several state-of-the-art methods\non a broad range of publicly available datasets. HBST is available as a compact\nopen-source C++ header-only library.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-18.4195499420166, -35.51248550415039], "cluster": 3}, {"key": "schlemper2019deep", "year": "2019", "citations": "1", "title": "Deep Hashing Using Entropy Regularised Product Quantisation Network", "abstract": "<p>In large scale systems, approximate nearest neighbour search is a crucial\nalgorithm to enable efficient data retrievals. Recently, deep learning-based\nhashing algorithms have been proposed as a promising paradigm to enable data\ndependent schemes. Often their efficacy is only demonstrated on data sets with\nfixed, limited numbers of classes. In practical scenarios, those labels are not\nalways available or one requires a method that can handle a higher input\nvariability, as well as a higher granularity. To fulfil those requirements, we\nlook at more flexible similarity measures. In this work, we present a novel,\nflexible, end-to-end trainable network for large-scale data hashing. Our method\nworks by transforming the data distribution to behave as a uniform distribution\non a product of spheres. The transformed data is subsequently hashed to a\nbinary form in a way that maximises entropy of the output, (i.e. to fully\nutilise the available bit-rate capacity) while maintaining the correctness\n(i.e. close items hash to the same key in the map). We show that the method\noutperforms baseline approaches such as locality-sensitive hashing and product\nquantisation in the limited capacity regime.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Similarity-Search", "Scalability", "Evaluation"], "tsne_embedding": [17.111156463623047, 29.142724990844727], "cluster": 4}, {"key": "schmidt2024evaluating", "year": "2024", "citations": "0", "title": "Evaluating The Performance-deviation Of Itemknn In Recbole And Lenskit", "abstract": "<p>This study examines the performance of item-based k-Nearest Neighbors\n(ItemKNN) algorithms in the RecBole and LensKit recommender system libraries.\nUsing four data sets (Anime, Modcloth, ML-100K, and ML-1M), we assess each\nlibrary\u2019s efficiency, accuracy, and scalability, focusing primarily on\nnormalized discounted cumulative gain (nDCG). Our results show that RecBole\noutperforms LensKit on two of three metrics on the ML-100K data set: it\nachieved an 18% higher nDCG, 14% higher precision, and 35% lower recall. To\nensure a fair comparison, we adjusted LensKit\u2019s nDCG calculation to match\nRecBole\u2019s method. This alignment made the performance more comparable, with\nLensKit achieving an nDCG of 0.2540 and RecBole 0.2674. Differences in\nsimilarity matrix calculations were identified as the main cause of performance\ndeviations. After modifying LensKit to retain only the top K similar items,\nboth libraries showed nearly identical nDCG values across all data sets. For\ninstance, both achieved an nDCG of 0.2586 on the ML-1M data set with the same\nrandom seed. Initially, LensKit\u2019s original implementation only surpassed\nRecBole in the ModCloth dataset.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-29.566652297973633, 22.77385902404785], "cluster": 0}, {"key": "schroeder2020structured", "year": "2020", "citations": "49", "title": "Structured Query-based Image Retrieval Using Scene Graphs", "abstract": "<p>A structured query can capture the complexity of object interactions (e.g.\n\u2018woman rides motorcycle\u2019) unlike single objects (e.g. \u2018woman\u2019 or \u2018motorcycle\u2019).\nRetrieval using structured queries therefore is much more useful than single\nobject retrieval, but a much more challenging problem. In this paper we present\na method which uses scene graph embeddings as the basis for an approach to\nimage retrieval. We examine how visual relationships, derived from scene\ngraphs, can be used as structured queries. The visual relationships are\ndirected subgraphs of the scene graph with a subject and object as nodes\nconnected by a predicate relationship. Notably, we are able to achieve high\nrecall even on low to medium frequency objects found in the long-tailed\nCOCO-Stuff dataset, and find that adding a visual relationship-inspired loss\nboosts our recall by 10% in the best case.</p>\n", "tags": ["CVPR", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-2.461331844329834, -45.26565933227539], "cluster": 3}, {"key": "schroff2015facenet", "year": "2015", "citations": "8312", "title": "Facenet: A Unified Embedding For Face Recognition And Clustering", "abstract": "<p>Despite significant recent advances in the field of face recognition,\nimplementing face verification and recognition efficiently at scale presents\nserious challenges to current approaches. In this paper we present a system,\ncalled FaceNet, that directly learns a mapping from face images to a compact\nEuclidean space where distances directly correspond to a measure of face\nsimilarity. Once this space has been produced, tasks such as face recognition,\nverification and clustering can be easily implemented using standard techniques\nwith FaceNet embeddings as feature vectors.\n  Our method uses a deep convolutional network trained to directly optimize the\nembedding itself, rather than an intermediate bottleneck layer as in previous\ndeep learning approaches. To train, we use triplets of roughly aligned matching\n/ non-matching face patches generated using a novel online triplet mining\nmethod. The benefit of our approach is much greater representational\nefficiency: we achieve state-of-the-art face recognition performance using only\n128-bytes per face.\n  On the widely used Labeled Faces in the Wild (LFW) dataset, our system\nachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves\n95.12%. Our system cuts the error rate in comparison to the best published\nresult by 30% on both datasets.\n  We also introduce the concept of harmonic embeddings, and a harmonic triplet\nloss, which describe different versions of face embeddings (produced by\ndifferent networks) that are compatible to each other and allow for direct\ncomparison between each other.</p>\n", "tags": ["Efficiency", "CVPR", "Evaluation", "Datasets"], "tsne_embedding": [-12.117149353027344, 7.083433151245117], "cluster": 1}, {"key": "schubert2020graph", "year": "2021", "citations": "9", "title": "Graph-based Non-linear Least Squares Optimization For Visual Place Recognition In Changing Environments", "abstract": "<p>Visual place recognition is an important subproblem of mobile robot\nlocalization. Since it is a special case of image retrieval, the basic source\nof information is the pairwise similarity of image descriptors. However, the\nembedding of the image retrieval problem in this robotic task provides\nadditional structure that can be exploited, e.g. spatio-temporal consistency.\nSeveral algorithms exist to exploit this structure, e.g., sequence processing\napproaches or descriptor standardization approaches for changing environments.\nIn this paper, we propose a graph-based framework to systematically exploit\ndifferent types of additional structure and information. The graphical model is\nused to formulate a non-linear least squares problem that can be optimized with\nstandard tools. Beyond sequences and standardization, we propose the usage of\nintra-set similarities within the database and/or the query image set as\nadditional source of information. If available, our approach also allows to\nseamlessly integrate additional knowledge about poses of database images. We\nevaluate the system on a variety of standard place recognition datasets and\ndemonstrate performance improvements for a large number of different\nconfigurations including different sources of information, different types of\nconstraints, and online or offline place recognition setups.</p>\n", "tags": ["Graph-Based-Ann", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-35.97731018066406, -8.844075202941895], "cluster": 5}, {"key": "schubert2021triangle", "year": "2021", "citations": "20", "title": "A Triangle Inequality For Cosine Similarity", "abstract": "<p>Similarity search is a fundamental problem for many data analysis techniques.\nMany efficient search techniques rely on the triangle inequality of metrics,\nwhich allows pruning parts of the search space based on transitive bounds on\ndistances. Recently, Cosine similarity has become a popular alternative choice\nto the standard Euclidean metric, in particular in the context of textual data\nand neural network embeddings. Unfortunately, Cosine similarity is not metric\nand does not satisfy the standard triangle inequality. Instead, many search\ntechniques for Cosine rely on approximation techniques such as locality\nsensitive hashing. In this paper, we derive a triangle inequality for Cosine\nsimilarity that is suitable for efficient similarity search with many standard\nsearch structures (such as the VP-tree, Cover-tree, and M-tree); show that this\nbound is tight and discuss fast approximations for it. We hope that this spurs\nnew research on accelerating exact similarity search for cosine similarity, and\npossible other similarity measures beyond the existing work for distance\nmetrics.</p>\n", "tags": ["Tree-Based-Ann", "Hashing-Methods", "Distance-Metric-Learning", "Similarity-Search"], "tsne_embedding": [26.20008659362793, 33.13440704345703], "cluster": 4}, {"key": "schuhmann2021laion", "year": "2021", "citations": "312", "title": "LAION-400M: Open Dataset Of Clip-filtered 400 Million Image-text Pairs", "abstract": "<p>Multi-modal language-vision models trained on hundreds of millions of\nimage-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable\ncapability to perform zero- or few-shot learning and transfer even in absence\nof per-sample labels on target image data. Despite this trend, to date there\nhas been no publicly available datasets of sufficient scale for training such\nmodels from scratch. To address this issue, in a community effort we build and\nrelease for public LAION-400M, a dataset with CLIP-filtered 400 million\nimage-text pairs, their CLIP embeddings and kNN indices that allow efficient\nsimilarity search.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Similarity-Search", "Datasets"], "tsne_embedding": [-30.095687866210938, -33.5397834777832], "cluster": 5}, {"key": "schwengber2023deep", "year": "2023", "citations": "0", "title": "Deep Hashing Via Householder Quantization", "abstract": "<p>Hashing is at the heart of large-scale image similarity search, and recent\nmethods have been substantially improved through deep learning techniques. Such\nalgorithms typically learn continuous embeddings of the data. To avoid a\nsubsequent costly binarization step, a common solution is to employ loss\nfunctions that combine a similarity learning term (to ensure similar images are\ngrouped to nearby embeddings) and a quantization penalty term (to ensure that\nthe embedding entries are close to binarized entries, e.g., -1 or 1). Still,\nthe interaction between these two terms can make learning harder and the\nembeddings worse. We propose an alternative quantization strategy that\ndecomposes the learning problem in two stages: first, perform similarity\nlearning over the embedding space with no quantization; second, find an optimal\northogonal transformation of the embeddings so each coordinate of the embedding\nis close to its sign, and then quantize the transformed embedding through the\nsign function. In the second step, we parametrize orthogonal transformations\nusing Householder matrices to efficiently leverage stochastic gradient descent.\nSince similarity measures are usually invariant under orthogonal\ntransformations, this quantization strategy comes at no cost in terms of\nperformance. The resulting algorithm is unsupervised, fast, hyperparameter-free\nand can be run on top of any existing deep hashing or metric learning\nalgorithm. We provide extensive experimental results showing that this approach\nleads to state-of-the-art performance on widely used image datasets, and,\nunlike other quantization strategies, brings consistent improvements in\nperformance to existing deep hashing algorithms.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Neural-Hashing", "Quantization", "Scalability", "Similarity-Search", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [-6.697427272796631, 16.417387008666992], "cluster": 8}, {"key": "scribano2021all", "year": "2021", "citations": "7", "title": "All You Can Embed: Natural Language Based Vehicle Retrieval With Spatio-temporal Transformers", "abstract": "<p>Combining Natural Language with Vision represents a unique and interesting\nchallenge in the domain of Artificial Intelligence. The AI City Challenge Track\n5 for Natural Language-Based Vehicle Retrieval focuses on the problem of\ncombining visual and textual information, applied to a smart-city use case. In\nthis paper, we present All You Can Embed (AYCE), a modular solution to\ncorrelate single-vehicle tracking sequences with natural language. The main\nbuilding blocks of the proposed architecture are (i) BERT to provide an\nembedding of the textual descriptions, (ii) a convolutional backbone along with\na Transformer model to embed the visual information. For the training of the\nretrieval model, a variation of the Triplet Margin Loss is proposed to learn a\ndistance measure between the visual and language embeddings. The code is\npublicly available at https://github.com/cscribano/AYCE_2021.</p>\n", "tags": ["CVPR"], "tsne_embedding": [-17.45583724975586, -24.41127586364746], "cluster": 5}, {"key": "seemakhupt2024edgerag", "year": "2024", "citations": "0", "title": "Edgerag: Online-indexed RAG For Edge Devices", "abstract": "<p>Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.</p>\n", "tags": ["Vector-Indexing", "Datasets"], "tsne_embedding": [34.93195343017578, 15.864386558532715], "cluster": 2}, {"key": "seidenschwarz2021learning", "year": "2021", "citations": "23", "title": "Learning Intra-batch Connections For Deep Metric Learning", "abstract": "<p>The goal of metric learning is to learn a function that maps samples to a\nlower-dimensional space where similar samples lie closer than dissimilar ones.\nParticularly, deep metric learning utilizes neural networks to learn such a\nmapping. Most approaches rely on losses that only take the relations between\npairs or triplets of samples into account, which either belong to the same\nclass or two different classes. However, these methods do not explore the\nembedding space in its entirety. To this end, we propose an approach based on\nmessage passing networks that takes all the relations in a mini-batch into\naccount. We refine embedding vectors by exchanging messages among all samples\nin a given batch allowing the training process to be aware of its overall\nstructure. Since not all samples are equally important to predict a decision\nboundary, we use an attention mechanism during message passing to allow samples\nto weigh the importance of each neighbor accordingly. We achieve\nstate-of-the-art results on clustering and image retrieval on the CUB-200-2011,\nCars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate\nfurther research, we make available the code and the models at\nhttps://github.com/dvl-tum/intra_batch_connections.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-11.109600067138672, -21.00746726989746], "cluster": 1}, {"key": "senter2019unaligned", "year": "2019", "citations": "3", "title": "Unaligned Sequence Similarity Search Using Deep Learning", "abstract": "<p>Gene annotation has traditionally required direct comparison of DNA sequences\nbetween an unknown gene and a database of known ones using string comparison\nmethods. However, these methods do not provide useful information when a gene\ndoes not have a close match in the database. In addition, each comparison can\nbe costly when the database is large since it requires alignments and a series\nof string comparisons. In this work we propose a novel approach: using\nrecurrent neural networks to embed DNA or amino-acid sequences in a\nlow-dimensional space in which distances correlate with functional similarity.\nThis embedding space overcomes both shortcomings of the method of aligning\nsequences and comparing homology. First, it allows us to obtain information\nabout genes which do not have exact matches by measuring their similarity to\nother ones in the database. If our database is labeled this can provide labels\nfor a query gene as is done in traditional methods. However, even if the\ndatabase is unlabeled it allows us to find clusters and infer some\ncharacteristics of the gene population. In addition, each comparison is much\nfaster than traditional methods since the distance metric is reduced to the\nEuclidean distance, and thus efficient approximate nearest neighbor algorithms\ncan be used to find the best match. We present results showing the advantage of\nour algorithm. More specifically we show how our embedding can be useful for\nboth classification tasks when our labels are known, and clustering tasks where\nour sequences belong to classes which have not been seen before.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Similarity-Search"], "tsne_embedding": [2.427596092224121, 53.27463150024414], "cluster": 4}, {"key": "settle2017query", "year": "2017", "citations": "56", "title": "Query-by-example Search With Discriminative Neural Acoustic Word Embeddings", "abstract": "<p>Query-by-example search often uses dynamic time warping (DTW) for comparing\nqueries and proposed matching segments. Recent work has shown that comparing\nspeech segments by representing them as fixed-dimensional vectors \u2014 acoustic\nword embeddings \u2014 and measuring their vector distance (e.g., cosine distance)\ncan discriminate between words more accurately than DTW-based approaches. We\nconsider an approach to query-by-example search that embeds both the query and\ndatabase segments according to a neural model, followed by nearest-neighbor\nsearch to find the matching segments. Earlier work on embedding-based\nquery-by-example, using template-based acoustic word embeddings, achieved\ncompetitive performance. We find that our embeddings, based on recurrent neural\nnetworks trained to optimize word discrimination, achieve substantial\nimprovements in performance and run-time efficiency over the previous\napproaches.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [0.6528105735778809, -29.44034767150879], "cluster": 3}, {"key": "severo2024random", "year": "2024", "citations": "0", "title": "Random Cycle Coding: Lossless Compression Of Cluster Assignments Via Bits-back Coding", "abstract": "<p>We present an optimal method for encoding cluster assignments of arbitrary\ndata sets. Our method, Random Cycle Coding (RCC), encodes data sequentially and\nsends assignment information as cycles of the permutation defined by the order\nof encoded elements. RCC does not require any training and its worst-case\ncomplexity scales quasi-linearly with the size of the largest cluster. We\ncharacterize the achievable bit rates as a function of cluster sizes and number\nof elements, showing RCC consistently outperforms previous methods while\nrequiring less compute and memory resources. Experiments show RCC can save up\nto 2 bytes per element when applied to vector databases, and removes the need\nfor assigning integer ids to identify vectors, translating to savings of up to\n70% in vector database systems for similarity search applications.</p>\n", "tags": ["Similarity-Search"], "tsne_embedding": [30.535812377929688, 31.62725067138672], "cluster": 2}, {"key": "severo2025lossless", "year": "2025", "citations": "0", "title": "Lossless Compression Of Vector Ids For Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor search for vectors relies on indexes that are\nmost often accessed from RAM. Therefore, storage is the factor limiting the\nsize of the database that can be served from a machine. Lossy vector\ncompression, i.e., embedding quantization, has been applied extensively to\nreduce the size of indexes. However, for inverted file and graph-based indices,\nauxiliary data such as vector ids and links (edges) can represent most of the\nstorage cost. We introduce and evaluate lossless compression schemes for these\ncases. These approaches are based on asymmetric numeral systems or wavelet\ntrees that exploit the fact that the ordering of ids is irrelevant within the\ndata structures. In some settings, we are able to compress the vector ids by a\nfactor 7, with no impact on accuracy or search runtime. On billion-scale\ndatasets, this results in a reduction of 30% of the index size. Furthermore, we\nshow that for some datasets, these methods can also compress the quantized\nvector codes losslessly, by exploiting sub-optimalities in the original\nquantization algorithm. The source code for our approach available at\nhttps://github.com/facebookresearch/vector_db_id_compression.</p>\n", "tags": ["Graph-Based-Ann", "Quantization", "Vector-Indexing", "Scalability", "Memory-Efficiency", "Large-Scale-Search", "Datasets"], "tsne_embedding": [36.75741958618164, 19.090017318725586], "cluster": 2}, {"key": "shabanov2023stir", "year": "2024", "citations": "2", "title": "STIR: Siamese Transformer For Image Retrieval Postprocessing", "abstract": "<p>Current metric learning approaches for image retrieval are usually based on\nlearning a space of informative latent representations where simple approaches\nsuch as the cosine distance will work well. Recent state of the art methods\nsuch as HypViT move to more complex embedding spaces that may yield better\nresults but are harder to scale to production environments. In this work, we\nfirst construct a simpler model based on triplet loss with hard negatives\nmining that performs at the state of the art level but does not have these\ndrawbacks. Second, we introduce a novel approach for image retrieval\npostprocessing called Siamese Transformer for Image Retrieval (STIR) that\nreranks several top outputs in a single forward pass. Unlike previously\nproposed Reranking Transformers, STIR does not rely on global/local feature\nextraction and directly compares a query image and a retrieved candidate on\npixel level with the usage of attention mechanism. The resulting approach\ndefines a new state of the art on standard image retrieval datasets: Stanford\nOnline Products and DeepFashion In-shop. We also release the source code at\nhttps://github.com/OML-Team/open-metric-learning/tree/main/pipelines/postprocessing/\nand an interactive demo of our approach at\nhttps://dapladoc-oml-postprocessing-demo-srcappmain-pfh2g0.streamlit.app/</p>\n", "tags": ["CIKM", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-33.54656982421875, 3.5999701023101807], "cluster": 0}, {"key": "shafiei2021colored", "year": "2021", "citations": "1", "title": "Colored Kimia Path24 Dataset: Configurations And Benchmarks With Deep Embeddings", "abstract": "<p>The Kimia Path24 dataset has been introduced as a classification and\nretrieval dataset for digital pathology. Although it provides multi-class data,\nthe color information has been neglected in the process of extracting patches.\nThe staining information plays a major role in the recognition of tissue\npatterns. To address this drawback, we introduce the color version of Kimia\nPath24 by recreating sample patches from all 24 scans to propose Kimia Path24C.\nWe run extensive experiments to determine the best configuration for selected\npatches. To provide preliminary results for setting a benchmark for the new\ndataset, we utilize VGG16, InceptionV3 and DenseNet-121 model as feature\nextractors. Then, we use these feature vectors to retrieve test patches. The\naccuracy of image retrieval using DenseNet was 95.92% while the highest\naccuracy using InceptionV3 and VGG16 reached 92.45% and 92%, respectively. We\nalso experimented with \u201cdeep barcodes\u201d and established that with a small loss\nin accuracy (e.g., 93.43% for binarized features for DenseNet instead of 95.92%\nwhen the features themselves are used), the search operations can be\nsignificantly accelerated.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-48.88243103027344, 14.11290168762207], "cluster": 0}, {"key": "shah2023group", "year": "2024", "citations": "0", "title": "Group Testing For Accurate And Efficient Range-based Near Neighbor Search For Plagiarism Detection", "abstract": "<p>This work presents an adaptive group testing framework for the range-based\nhigh dimensional near neighbor search problem. Our method efficiently marks\neach item in a database as neighbor or non-neighbor of a query point, based on\na cosine distance threshold without exhaustive search. Like other methods for\nlarge scale retrieval, our approach exploits the assumption that most of the\nitems in the database are unrelated to the query. However, it does not assume a\nlarge difference between the cosine similarity of the query vector with the\nleast related neighbor and that with the least unrelated non-neighbor.\nFollowing a multi-stage adaptive group testing algorithm based on binary\nsplitting, we divide the set of items to be searched into half at each step,\nand perform dot product tests on smaller and smaller subsets, many of which we\nare able to prune away. We show that, using softmax-based features, our method\nachieves a more than ten-fold speed-up over exhaustive search with no loss of\naccuracy, on a variety of large datasets. Based on empirically verified models\nfor the distribution of cosine distances, we present a theoretical analysis of\nthe expected number of distance computations per query and the probability that\na pool will be pruned. Our method has the following features: (i) It implicitly\nexploits useful distributional properties of cosine distances unlike other\nmethods; (ii) All required data structures are created purely offline; (iii) It\ndoes not impose any strong assumptions on the number of true near neighbors;\n(iv) It is adaptable to streaming settings where new vectors are dynamically\nadded to the database; and (v) It does not require any parameter tuning. The\nhigh recall of our technique makes it particularly suited to plagiarism\ndetection scenarios where it is important to report every database item that is\nsufficiently similar item to the query.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [26.351430892944336, 27.80958366394043], "cluster": 2}, {"key": "shan2018recurrent", "year": "2018", "citations": "10", "title": "Recurrent Binary Embedding For Gpu-enabled Exhaustive Retrieval From Billion-scale Semantic Vectors", "abstract": "<p>Rapid advances in GPU hardware and multiple areas of Deep Learning open up a\nnew opportunity for billion-scale information retrieval with exhaustive search.\nBuilding on top of the powerful concept of semantic learning, this paper\nproposes a Recurrent Binary Embedding (RBE) model that learns compact\nrepresentations for real-time retrieval. The model has the unique ability to\nrefine a base binary vector by progressively adding binary residual vectors to\nmeet the desired accuracy. The refined vector enables efficient implementation\nof exhaustive similarity computation with bit-wise operations, followed by a\nnear- lossless k-NN selection algorithm, also proposed in this paper. The\nproposed algorithms are integrated into an end-to-end multi-GPU system that\nretrieves thousands of top items from over a billion candidates in real-time.\nThe RBE model and the retrieval system were evaluated with data from a major\npaid search engine. When measured against the state-of-the-art model for binary\nrepresentation and the full precision model for semantic embedding, RBE\nsignificantly outperformed the former, and filled in over 80% of the AUC gap\nin-between. Experiments comparing with our production retrieval system also\ndemonstrated superior performance. While the primary focus of this paper is to\nbuild RBE based on a particular class of semantic models, generalizing to other\ntypes is straightforward, as exemplified by two different models at the end of\nthe paper.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Large-Scale-Search", "Evaluation", "KDD"], "tsne_embedding": [24.400997161865234, 8.941871643066406], "cluster": 2}, {"key": "shand2020locality", "year": "2020", "citations": "0", "title": "Locality-sensitive Hashing In Function Spaces", "abstract": "<p>We discuss the problem of performing similarity search over function spaces.\nTo perform search over such spaces in a reasonable amount of time, we use {\\it\nlocality-sensitive hashing} (LSH). We present two methods that allow LSH\nfunctions on \\(\\mathbb{R}^N\\) to be extended to \\(L^p\\) spaces: one using function\napproximation in an orthonormal basis, and another using (quasi-)Monte\nCarlo-style techniques. We use the presented hashing schemes to construct an\nLSH family for Wasserstein distance over one-dimensional, continuous\nprobability distributions.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search"], "tsne_embedding": [14.768475532531738, 46.17433166503906], "cluster": 4}, {"key": "shanehsazzadeh2020fixed", "year": "2020", "citations": "0", "title": "Fixed-length Protein Embeddings Using Contextual Lenses", "abstract": "<p>The Basic Local Alignment Search Tool (BLAST) is currently the most popular\nmethod for searching databases of biological sequences. BLAST compares\nsequences via similarity defined by a weighted edit distance, which results in\nit being computationally expensive. As opposed to working with edit distance, a\nvector similarity approach can be accelerated substantially using modern\nhardware or hashing techniques. Such an approach would require fixed-length\nembeddings for biological sequences. There has been recent interest in learning\nfixed-length protein embeddings using deep learning models under the hypothesis\nthat the hidden layers of supervised or semi-supervised models could produce\npotentially useful vector embeddings. We consider transformer (BERT) protein\nlanguage models that are pretrained on the TrEMBL data set and learn\nfixed-length embeddings on top of them with contextual lenses. The embeddings\nare trained to predict the family a protein belongs to for sequences in the\nPfam database. We show that for nearest-neighbor family classification,\npretraining offers a noticeable boost in performance and that the corresponding\nlearned embeddings are competitive with BLAST. Furthermore, we show that the\nraw transformer embeddings, obtained via static pooling, do not perform well on\nnearest-neighbor family classification, which suggests that learning embeddings\nin a supervised manner via contextual lenses may be a compute-efficient\nalternative to fine-tuning.</p>\n", "tags": ["Supervised", "Evaluation", "Hashing-Methods"], "tsne_embedding": [0.7564858794212341, 54.57960510253906], "cluster": 4}, {"key": "shankar2017deep", "year": "2017", "citations": "59", "title": "Deep Learning Based Large Scale Visual Recommendation And Search For E-commerce", "abstract": "<p>In this paper, we present a unified end-to-end approach to build a large\nscale Visual Search and Recommendation system for e-commerce. Previous works\nhave targeted these problems in isolation. We believe a more effective and\nelegant solution could be obtained by tackling them together. We propose a\nunified Deep Convolutional Neural Network architecture, called VisNet, to learn\nembeddings to capture the notion of visual similarity, across several semantic\ngranularities. We demonstrate the superiority of our approach for the task of\nimage retrieval, by comparing against the state-of-the-art on the Exact\nStreet2Shop dataset. We then share the design decisions and trade-offs made\nwhile deploying the model to power Visual Recommendations across a catalog of\n50M products, supporting 2K queries a second at Flipkart, India\u2019s largest\ne-commerce company. The deployment of our solution has yielded a significant\nbusiness impact, as measured by the conversion-rate.</p>\n", "tags": ["Recommender-Systems", "Image-Retrieval", "Datasets"], "tsne_embedding": [-17.01222801208496, -45.49290466308594], "cluster": 3}, {"key": "shao2023global", "year": "2023", "citations": "21", "title": "Global Features Are All You Need For Image Retrieval And Reranking", "abstract": "<p>Image retrieval systems conventionally use a two-stage paradigm, leveraging\nglobal features for initial retrieval and local features for reranking.\nHowever, the scalability of this method is often limited due to the significant\nstorage and computation cost incurred by local feature matching in the\nreranking stage. In this paper, we present SuperGlobal, a novel approach that\nexclusively employs global features for both stages, improving efficiency\nwithout sacrificing accuracy. SuperGlobal introduces key enhancements to the\nretrieval system, specifically focusing on the global feature extraction and\nreranking processes. For extraction, we identify sub-optimal performance when\nthe widely-used ArcFace loss and Generalized Mean (GeM) pooling methods are\ncombined and propose several new modules to improve GeM pooling. In the\nreranking stage, we introduce a novel method to update the global features of\nthe query and top-ranked images by only considering feature refinement with a\nsmall set of images, thus being very compute and memory efficient. Our\nexperiments demonstrate substantial improvements compared to the state of the\nart in standard benchmarks. Notably, on the Revisited Oxford+1M Hard dataset,\nour single-stage results improve by 7.1%, while our two-stage gain reaches 3.7%\nwith a strong 64,865x speedup. Our two-stage system surpasses the current\nsingle-stage state-of-the-art by 16.3%, offering a scalable, accurate\nalternative for high-performing image retrieval systems with minimal time\noverhead. Code: https://github.com/ShihaoShao-GH/SuperGlobal.</p>\n", "tags": ["ICCV", "Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-34.46052169799805, 5.422060966491699], "cluster": 0}, {"key": "sharma2016stacked", "year": "2016", "citations": "32", "title": "Stacked Autoencoders For Medical Image Search", "abstract": "<p>Medical images can be a valuable resource for reliable information to support\nmedical diagnosis. However, the large volume of medical images makes it\nchallenging to retrieve relevant information given a particular scenario. To\nsolve this challenge, content-based image retrieval (CBIR) attempts to\ncharacterize images (or image regions) with invariant content information in\norder to facilitate image search. This work presents a feature extraction\ntechnique for medical images using stacked autoencoders, which encode images to\nbinary vectors. The technique is applied to the IRMA dataset, a collection of\n14,410 x-ray images in order to demonstrate the ability of autoencoders to\nretrieve similar x-rays given test queries. Using IRMA dataset as a benchmark,\nit was found that stacked autoencoders gave excellent results with a retrieval\nerror of 376 for 1,733 test images with a compression of 74.61%.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.143131256103516, 14.149301528930664], "cluster": 0}, {"key": "sharma2018improving", "year": "2018", "citations": "6", "title": "Improving Similarity Search With High-dimensional Locality-sensitive Hashing", "abstract": "<p>We propose a new class of data-independent locality-sensitive hashing (LSH)\nalgorithms based on the fruit fly olfactory circuit. The fundamental difference\nof this approach is that, instead of assigning hashes as dense points in a low\ndimensional space, hashes are assigned in a high dimensional space, which\nenhances their separability. We show theoretically and empirically that this\nnew family of hash functions is locality-sensitive and preserves rank\nsimilarity for inputs in any `p space. We then analyze different variations on\nthis strategy and show empirically that they outperform existing LSH methods\nfor nearest-neighbors search on six benchmark datasets. Finally, we propose a\nmulti-probe version of our algorithm that achieves higher performance for the\nsame query time, or conversely, that maintains performance of prior approaches\nwhile taking significantly less indexing time and memory. Overall, our approach\nleverages the advantages of separability provided by high-dimensional spaces,\nwhile still remaining computationally efficient</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [8.560121536254883, 34.74456787109375], "cluster": 4}, {"key": "sharma2019retrieving", "year": "2019", "citations": "20", "title": "Retrieving Similar E-commerce Images Using Deep Learning", "abstract": "<p>In this paper, we propose a deep convolutional neural network for learning\nthe embeddings of images in order to capture the notion of visual similarity.\nWe present a deep siamese architecture that when trained on positive and\nnegative pairs of images learn an embedding that accurately approximates the\nranking of images in order of visual similarity notion. We also implement a\nnovel loss calculation method using an angular loss metrics based on the\nproblems requirement. The final embedding of the image is combined\nrepresentation of the lower and top-level embeddings. We used fractional\ndistance matrix to calculate the distance between the learned embeddings in\nn-dimensional space. In the end, we compare our architecture with other\nexisting deep architecture and go on to demonstrate the superiority of our\nsolution in terms of image retrieval by testing the architecture on four\ndatasets. We also show how our suggested network is better than the other\ntraditional deep CNNs used for capturing fine-grained image similarities by\nlearning an optimum embedding.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [-7.207431316375732, -18.385082244873047], "cluster": 1}, {"key": "shen2016learning", "year": "2016", "citations": "0", "title": "Learning Binary Codes And Binary Weights For Efficient Classification", "abstract": "<p>This paper proposes a generic formulation that significantly expedites the\ntraining and deployment of image classification models, particularly under the\nscenarios of many image categories and high feature dimensions. As a defining\nproperty, our method represents both the images and learned classifiers using\nbinary hash codes, which are simultaneously learned from the training data.\nClassifying an image thereby reduces to computing the Hamming distance between\nthe binary codes of the image and classifiers and selecting the class with\nminimal Hamming distance. Conventionally, compact hash codes are primarily used\nfor accelerating image search. Our work is first of its kind to represent\nclassifiers using binary codes. Specifically, we formulate multi-class image\nclassification as an optimization problem over binary variables. The\noptimization alternatively proceeds over the binary classifiers and image hash\ncodes. Profiting from the special property of binary codes, we show that the\nsub-problems can be efficiently solved through either a binary quadratic\nprogram (BQP) or linear program. In particular, for attacking the BQP problem,\nwe propose a novel bit-flipping procedure which enjoys high efficacy and local\noptimality guarantee. Our formulation supports a large family of empirical loss\nfunctions and is here instantiated by exponential / hinge losses. Comprehensive\nevaluations are conducted on several representative image benchmarks. The\nexperiments consistently observe reduced complexities of model training and\ndeployment, without sacrifice of accuracies.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-12.067435264587402, 19.451250076293945], "cluster": 8}, {"key": "shen2017deep", "year": "2017", "citations": "366", "title": "Deep Binaries: Encoding Semantic-rich Cues For Efficient Textual-visual Cross Retrieval", "abstract": "<p>Cross-modal hashing is usually regarded as an effective technique for\nlarge-scale textual-visual cross retrieval, where data from different\nmodalities are mapped into a shared Hamming space for matching. Most of the\ntraditional textual-visual binary encoding methods only consider holistic image\nrepresentations and fail to model descriptive sentences. This renders existing\nmethods inappropriate to handle the rich semantics of informative cross-modal\ndata for quality textual-visual search tasks. To address the problem of hashing\ncross-modal data with semantic-rich cues, in this paper, a novel integrated\ndeep architecture is developed to effectively encode the detailed semantics of\ninformative images and long descriptive sentences, named as Textual-Visual Deep\nBinaries (TVDB). In particular, region-based convolutional networks with long\nshort-term memory units are introduced to fully explore image regional details\nwhile semantic cues of sentences are modeled by a text convolutional network.\nAdditionally, we propose a stochastic batch-wise training routine, where\nhigh-quality binary codes and deep encoding functions are efficiently optimized\nin an alternating manner. Experiments are conducted on three multimedia\ndatasets, i.e. Microsoft COCO, IAPR TC-12, and INRIA Web Queries, where the\nproposed TVDB model significantly outperforms state-of-the-art binary coding\nmethods in the task of cross-modal retrieval.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Multimodal-Retrieval", "Datasets", "Compact-Codes"], "tsne_embedding": [-16.805131912231445, -2.6225130558013916], "cluster": 1}, {"key": "shen2018matchable", "year": "2019", "citations": "49", "title": "Matchable Image Retrieval By Learning From Surface Reconstruction", "abstract": "<p>Convolutional Neural Networks (CNNs) have achieved superior performance on\nobject image retrieval, while Bag-of-Words (BoW) models with handcrafted local\nfeatures still dominate the retrieval of overlapping images in 3D\nreconstruction. In this paper, we narrow down this gap by presenting an\nefficient CNN-based method to retrieve images with overlaps, which we refer to\nas the matchable image retrieval problem. Different from previous methods that\ngenerates training data based on sparse reconstruction, we create a large-scale\nimage database with rich 3D geometrics and exploit information from surface\nreconstruction to obtain fine-grained training data. We propose a batched\ntriplet-based loss function combined with mesh re-projection to effectively\nlearn the CNN representation. The proposed method significantly accelerates the\nimage retrieval process in 3D reconstruction and outperforms the\nstate-of-the-art CNN-based and BoW methods for matchable image retrieval. The\ncode and data are available at https://github.com/hlzz/mirror.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Scalability"], "tsne_embedding": [-50.17750549316406, 0.6825286149978638], "cluster": 0}, {"key": "shen2018nash", "year": "2018", "citations": "65", "title": "NASH: Toward End-to-end Neural Architecture For Generative Semantic Hashing", "abstract": "<p>Semantic hashing has become a powerful paradigm for fast similarity search\nin many information retrieval systems.\nWhile fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present\nan end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary\nhashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent\nvariable to optimize the hash function.\nWe also draw connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on\nthree public datasets demonstrate that our\nmethod significantly outperforms several\nstate-of-the-art models on both unsupervised and supervised scenarios.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [27.309534072875977, -10.378313064575195], "cluster": 7}, {"key": "shen2018unsupervised", "year": "2018", "citations": "377", "title": "Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization", "abstract": "<p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing\nwith significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval\nperformance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve\nsatisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a\nsimple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds\nover three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from\nthe widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph\nmatrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise\nan effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive\nexperiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [15.418428421020508, 1.588890552520752], "cluster": 6}, {"key": "shen2018zero", "year": "2018", "citations": "155", "title": "Zero-shot Sketch-image Hashing", "abstract": "<p>Recent studies show that large-scale sketch-based image retrieval (SBIR) can\nbe efficiently tackled by cross-modal binary representation learning methods,\nwhere Hamming distance matching significantly speeds up the process of\nsimilarity search. Providing training and test data subjected to a fixed set of\npre-defined categories, the cutting-edge SBIR and cross-modal hashing works\nobtain acceptable retrieval performance. However, most of the existing methods\nfail when the categories of query sketches have never been seen during\ntraining. In this paper, the above problem is briefed as a novel but realistic\nzero-shot SBIR hashing task. We elaborate the challenges of this special task\nand accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An\nend-to-end three-network architecture is built, two of which are treated as the\nbinary encoders. The third network mitigates the sketch-image heterogeneity and\nenhances the semantic relations among data by utilizing the Kronecker fusion\nlayer and graph convolution, respectively. As an important part of ZSIH, we\nformulate a generative hashing scheme in reconstructing semantic knowledge\nrepresentations for zero-shot retrieval. To the best of our knowledge, ZSIH is\nthe first zero-shot hashing work suitable for SBIR and cross-modal search.\nComprehensive experiments are conducted on two extended datasets, i.e., Sketchy\nand TU-Berlin with a novel zero-shot train-test split. The proposed model\nremarkably outperforms related works.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Few-Shot-&-Zero-Shot", "Similarity-Search", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [3.1028919219970703, 9.95007610321045], "cluster": 6}, {"key": "shen2019embarrassingly", "year": "2019", "citations": "20", "title": "Embarrassingly Simple Binary Representation Learning", "abstract": "<p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash\u2019 paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>\n", "tags": ["Compact-Codes", "ICCV", "Hashing-Methods", "Scalability"], "tsne_embedding": [23.58979034423828, -1.7770941257476807], "cluster": 6}, {"key": "shen2020auto", "year": "2020", "citations": "113", "title": "Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [50.28215789794922, -0.8602146506309509], "cluster": 9}, {"key": "shen2022semicon", "year": "2022", "citations": "13", "title": "SEMICON: A Learning-to-hash Solution For Large-scale Fine-grained Image Retrieval", "abstract": "<p>In this paper, we propose Suppression-Enhancing Mask based attention and\nInteractive Channel transformatiON (SEMICON) to learn binary hash codes for\ndealing with large-scale fine-grained image retrieval tasks. In SEMICON, we\nfirst develop a suppression-enhancing mask (SEM) based attention to dynamically\nlocalize discriminative image regions. More importantly, different from\nexisting attention mechanism simply erasing previous discriminative regions,\nour SEM is developed to restrain such regions and then discover other\ncomplementary regions by considering the relation between activated regions in\na stage-by-stage fashion. In each stage, the interactive channel transformation\n(ICON) module is afterwards designed to exploit correlations across channels of\nattended activation tensors. Since channels could generally correspond to the\nparts of fine-grained objects, the part correlation can be also modeled\naccordingly, which further improves fine-grained retrieval accuracy. Moreover,\nto be computational economy, ICON is realized by an efficient two-step process.\nFinally, the hash learning of our SEMICON consists of both global- and\nlocal-level branches for better representing fine-grained objects and then\ngenerating binary hash codes explicitly corresponding to multiple levels.\nExperiments on five benchmark fine-grained datasets show our superiority over\ncompeting methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-23.715576171875, 6.2580742835998535], "cluster": 1}, {"key": "shen2023maru", "year": "2023", "citations": "0", "title": "Maru: A Manga Retrieval And Understanding System Connecting Vision And Language", "abstract": "<p>Manga, a widely celebrated Japanese comic art form, is renowned for its\ndiverse narratives and distinct artistic styles. However, the inherently visual\nand intricate structure of Manga, which comprises images housing multiple\npanels, poses significant challenges for content retrieval. To address this, we\npresent MaRU (Manga Retrieval and Understanding), a multi-staged system that\nconnects vision and language to facilitate efficient search of both dialogues\nand scenes within Manga frames. The architecture of MaRU integrates an object\ndetection model for identifying text and frame bounding boxes, a Vision\nEncoder-Decoder model for text recognition, a text encoder for embedding text,\nand a vision-text encoder that merges textual and visual information into a\nunified embedding space for scene retrieval. Rigorous evaluations reveal that\nMaRU excels in end-to-end dialogue retrieval and exhibits promising results for\nscene retrieval.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-32.49079895019531, -34.99668884277344], "cluster": 5}, {"key": "shen2025auto", "year": "2020", "citations": "113", "title": "Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [50.28215789794922, -0.8602146506309509], "cluster": 9}, {"key": "shen2025embarrassingly", "year": "2019", "citations": "20", "title": "Embarrassingly Simple Binary Representation Learning", "abstract": "<p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash\u2019 paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>\n", "tags": ["Compact-Codes", "ICCV", "Hashing-Methods", "Scalability"], "tsne_embedding": [23.58979034423828, -1.7770941257476807], "cluster": 6}, {"key": "shen2025mask", "year": "2025", "citations": "0", "title": "Mask-aware Text-to-image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "abstract": "<p>Text-to-image retrieval (TIR) aims to find relevant images based on a textual query, but existing approaches are primarily based on whole-image captions and lack interpretability. Meanwhile, referring expression segmentation (RES) enables precise object localization based on natural language descriptions but is computationally expensive when applied across large image collections. To bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies TIR and RES, requiring both efficient image search and accurate object segmentation. To address this task, we propose a two-stage framework, comprising a first stage for segmentation-aware image retrieval and a second stage for reranking and object grounding with a multimodal large language model (MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract region-level embeddings offline at first, enabling effective and scalable online retrieval. Secondly, MLLM is used to refine retrieval rankings and generate bounding boxes, which are matched to segmentation masks. We evaluate our approach on COCO and D\\(^3\\) datasets, demonstrating significant improvements in both retrieval accuracy and segmentation quality over previous methods.</p>\n", "tags": ["Multimodal-Retrieval", "Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-25.108566284179688, 1.70182204246521], "cluster": 1}, {"key": "shen2025nash", "year": "2018", "citations": "65", "title": "NASH: Toward End-to-end Neural Architecture For Generative Semantic Hashing", "abstract": "<p>Semantic hashing has become a powerful paradigm for fast similarity search\nin many information retrieval systems.\nWhile fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present\nan end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary\nhashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent\nvariable to optimize the hash function.\nWe also draw connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on\nthree public datasets demonstrate that our\nmethod significantly outperforms several\nstate-of-the-art models on both unsupervised and supervised scenarios.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [27.30956268310547, -10.378326416015625], "cluster": 7}, {"key": "shen2025structvpr", "year": "2025", "citations": "0", "title": "Structvpr++: Distill Structural And Semantic Knowledge With Weighting Samples For Visual Place Recognition", "abstract": "<p>Visual place recognition is a challenging task for autonomous driving and\nrobotics, which is usually considered as an image retrieval problem. A commonly\nused two-stage strategy involves global retrieval followed by re-ranking using\npatch-level descriptors. Most deep learning-based methods in an end-to-end\nmanner cannot extract global features with sufficient semantic information from\nRGB images. In contrast, re-ranking can utilize more explicit structural and\nsemantic information in one-to-one matching process, but it is time-consuming.\nTo bridge the gap between global retrieval and re-ranking and achieve a good\ntrade-off between accuracy and efficiency, we propose StructVPR++, a framework\nthat embeds structural and semantic knowledge into RGB global representations\nvia segmentation-guided distillation. Our key innovation lies in decoupling\nlabel-specific features from global descriptors, enabling explicit semantic\nalignment between image pairs without requiring segmentation during deployment.\nFurthermore, we introduce a sample-wise weighted distillation strategy that\nprioritizes reliable training pairs while suppressing noisy ones. Experiments\non four benchmarks demonstrate that StructVPR++ surpasses state-of-the-art\nglobal methods by 5-23% in Recall@1 and even outperforms many two-stage\napproaches, achieving real-time efficiency with a single RGB input.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Tools-&-Libraries", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-30.861474990844727, -4.562089920043945], "cluster": 0}, {"key": "shen2025unsupervised", "year": "2018", "citations": "377", "title": "Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization", "abstract": "<p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing\nwith significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval\nperformance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve\nsatisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a\nsimple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds\nover three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from\nthe widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph\nmatrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise\nan effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive\nexperiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [15.417631149291992, 1.5888811349868774], "cluster": 6}, {"key": "shenoy2017deduplication", "year": "2017", "citations": "1", "title": "Deduplication In A Massive Clinical Note Dataset", "abstract": "<p>Duplication, whether exact or partial, is a common issue in many datasets. In\nclinical notes data, duplication (and near duplication) can arise for many\nreasons, such as the pervasive use of templates, copy-pasting, or notes being\ngenerated by automated procedures. A key challenge in removing such near\nduplicates is the size of such datasets; our own dataset consists of more than\n10 million notes. To detect and correct such duplicates requires algorithms\nthat both accurate and highly scalable. We describe a solution based on\nMinhashing with Locality Sensitive Hashing. In this paper, we present the\ntheory behind this method and present a database-inspired approach to make the\nmethod scalable. We also present a clustering technique using disjoint sets to\nproduce dense clusters, which speeds up our algorithm.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [34.042301177978516, 12.090368270874023], "cluster": 2}, {"key": "sheynin2022knn", "year": "2022", "citations": "26", "title": "Knn-diffusion: Image Generation Via Large-scale Retrieval", "abstract": "<p>Recent text-to-image models have achieved impressive results. However, since\nthey require large-scale datasets of text-image pairs, it is impractical to\ntrain them on new domains where data is scarce or not labeled. In this work, we\npropose using large-scale retrieval methods, in particular, efficient\nk-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a\nsubstantially small and efficient text-to-image diffusion model without any\ntext, (2) generating out-of-distribution images by simply swapping the\nretrieval database at inference time, and (3) performing text-driven local\nsemantic manipulations while preserving object identity. To demonstrate the\nrobustness of our method, we apply our kNN approach on two state-of-the-art\ndiffusion backbones, and show results on several different datasets. As\nevaluated by human studies and automatic metrics, our method achieves\nstate-of-the-art results compared to existing approaches that train\ntext-to-image generation models using images only (without paired text data)</p>\n", "tags": ["Robustness", "Scalability", "Datasets"], "tsne_embedding": [-27.913814544677734, 0.26028844714164734], "cluster": 0}, {"key": "shi2018fast", "year": "2018", "citations": "1", "title": "Fast Locality Sensitive Hashing For Beam Search On GPU", "abstract": "<p>We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up\nbeam search for sequence models. We utilize the winner-take-all (WTA) hash,\nwhich is based on relative ranking order of hidden dimensions and thus\nresilient to perturbations in numerical values. Our algorithm is designed by\nfully considering the underling architecture of CUDA-enabled GPUs\n(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied\nfor LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are\nshared across beams to maximize the parallelism; 3) Top frequent words are\nmerged into candidate lists to improve performance. Experiments on 4\nlarge-scale neural machine translation models demonstrate that our algorithm\ncan achieve up to 4x speedup on softmax module, and 2x overall speedup without\nhurting BLEU on GPU.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Scalability", "Evaluation"], "tsne_embedding": [25.98461151123047, -3.816699504852295], "cluster": 6}, {"key": "shi2018scalable", "year": "2020", "citations": "11", "title": "A Scalable Optimization Mechanism For Pairwise Based Discrete Hashing", "abstract": "<p>Maintaining the pair similarity relationship among originally\nhigh-dimensional data into a low-dimensional binary space is a popular strategy\nto learn binary codes. One simiple and intutive method is to utilize two\nidentical code matrices produced by hash functions to approximate a pairwise\nreal label matrix. However, the resulting quartic problem is difficult to\ndirectly solve due to the non-convex and non-smooth nature of the objective. In\nthis paper, unlike previous optimization methods using various relaxation\nstrategies, we aim to directly solve the original quartic problem using a novel\nalternative optimization mechanism to linearize the quartic problem by\nintroducing a linear regression model. Additionally, we find that gradually\nlearning each batch of binary codes in a sequential mode, i.e. batch by batch,\nis greatly beneficial to the convergence of binary code learning. Based on this\nsignificant discovery and the proposed strategy, we introduce a scalable\nsymmetric discrete hashing algorithm that gradually and smoothly updates each\nbatch of binary codes. To further improve the smoothness, we also propose a\ngreedy symmetric discrete hashing algorithm to update each bit of batch binary\ncodes. Moreover, we extend the proposed optimization mechanism to solve the\nnon-convex optimization problems for binary code learning in many other\npairwise based hashing algorithms. Extensive experiments on benchmark\nsingle-label and multi-label databases demonstrate the superior performance of\nthe proposed mechanism over recent state-of-the-art methods.</p>\n", "tags": ["Compact-Codes", "Evaluation", "Hashing-Methods"], "tsne_embedding": [16.361196517944336, 2.5076828002929688], "cluster": 6}, {"key": "shi2019compositional", "year": "2020", "citations": "39", "title": "Compositional Embeddings Using Complementary Partitions For Memory-efficient Recommendation Systems", "abstract": "<p>Modern deep learning-based recommendation systems exploit hundreds to\nthousands of different categorical features, each with millions of different\ncategories ranging from clicks to posts. To respect the natural diversity\nwithin the categorical data, embeddings map each category to a unique dense\nrepresentation within an embedded space. Since each categorical feature could\ntake on as many as tens of millions of different possible categories, the\nembedding tables form the primary memory bottleneck during both training and\ninference. We propose a novel approach for reducing the embedding size in an\nend-to-end fashion by exploiting complementary partitions of the category set\nto produce a unique embedding vector for each category without explicit\ndefinition. By storing multiple smaller embedding tables based on each\ncomplementary partition and combining embeddings from each table, we define a\nunique embedding for each category at smaller memory cost. This approach may be\ninterpreted as using a specific fixed codebook to ensure uniqueness of each\ncategory\u2019s representation. Our experimental results demonstrate the\neffectiveness of our approach over the hashing trick for reducing the size of\nthe embedding tables in terms of model loss and accuracy, while retaining a\nsimilar reduction in the number of parameters.</p>\n", "tags": ["Evaluation", "Recommender-Systems", "Hashing-Methods", "KDD"], "tsne_embedding": [29.60878562927246, 15.557194709777832], "cluster": 2}, {"key": "shi2019variable", "year": "2019", "citations": "1", "title": "Variable-length Quantization Strategy For Hashing", "abstract": "<p>Hashing is widely used to solve fast Approximate Nearest Neighbor (ANN) search problems, involves converting the original real-valued samples to binary-valued representations. The conventional quantization strategies, such as Single-Bit Quantization and Multi-Bit quantization, are considered ineffective, because of their serious information loss. To address this issue, we propose a novel variable-length quantization (VLQ) strategy for hashing. In the proposed VLQ technique, we divide all samples into different regions in each dimension firstly given the real-valued features of samples. Then we compute the dispersion degrees of these regions. Subsequently, we attempt to optimally assign different number of bits to each dimensions to obtain the minimum dispersion degree. Our experiments show that the VLQ strategy achieves not only superior performance over the state-of-the-art methods, but also has a faster retrieval speed on public datasets.</p>\n", "tags": ["Quantization", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [10.539817810058594, 32.2371940612793], "cluster": 4}, {"key": "shi2022deep", "year": "2022", "citations": "1", "title": "Deep Manifold Hashing: A Divide-and-conquer Approach For Semi-paired Unsupervised Cross-modal Retrieval", "abstract": "<p>Hashing that projects data into binary codes has shown extraordinary talents\nin cross-modal retrieval due to its low storage usage and high query speed.\nDespite their empirical success on some scenarios, existing cross-modal hashing\nmethods usually fail to cross modality gap when fully-paired data with plenty\nof labeled information is nonexistent. To circumvent this drawback, motivated\nby the Divide-and-Conquer strategy, we propose Deep Manifold Hashing (DMH), a\nnovel method of dividing the problem of semi-paired unsupervised cross-modal\nretrieval into three sub-problems and building one simple yet efficiency model\nfor each sub-problem. Specifically, the first model is constructed for\nobtaining modality-invariant features by complementing semi-paired data based\non manifold learning, whereas the second model and the third model aim to learn\nhash codes and hash functions respectively. Extensive experiments on three\nbenchmarks demonstrate the superiority of our DMH compared with the\nstate-of-the-art fully-paired and semi-paired unsupervised cross-modal hashing\nmethods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "AAAI", "Multimodal-Retrieval", "Compact-Codes", "Unsupervised"], "tsne_embedding": [16.37700080871582, 6.161942481994629], "cluster": 6}, {"key": "shi2022efficient", "year": "2022", "citations": "2", "title": "Efficient Cross-modal Retrieval Via Deep Binary Hashing And Quantization", "abstract": "<p>Cross-modal retrieval aims to search for data with similar semantic meanings\nacross different content modalities. However, cross-modal retrieval requires\nhuge amounts of storage and retrieval time since it needs to process data in\nmultiple modalities. Existing works focused on learning single-source compact\nfeatures such as binary hash codes that preserve similarities between different\nmodalities. In this work, we propose a jointly learned deep hashing and\nquantization network (HQ) for cross-modal retrieval. We simultaneously learn\nbinary hash codes and quantization codes to preserve semantic information in\nmultiple modalities by an end-to-end deep learning architecture. At the\nretrieval step, binary hashing is used to retrieve a subset of items from the\nsearch space, then quantization is used to re-rank the retrieved items. We\ntheoretically and empirically show that this two-stage retrieval approach\nprovides faster retrieval results while preserving accuracy. Experimental\nresults on the NUS-WIDE, MIR-Flickr, and Amazon datasets demonstrate that HQ\nachieves boosts of more than 7% in precision compared to supervised neural\nnetwork-based compact coding models.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Quantization", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-3.273751974105835, -21.94434928894043], "cluster": 3}, {"key": "shi2022everyone", "year": "2022", "citations": "0", "title": "Everyone's Preference Changes Differently: Weighted Multi-interest Retrieval Model", "abstract": "<p>User embeddings (vectorized representations of a user) are essential in\nrecommendation systems. Numerous approaches have been proposed to construct a\nrepresentation for the user in order to find similar items for retrieval tasks,\nand they have been proven effective in industrial recommendation systems as\nwell. Recently people have discovered the power of using multiple embeddings to\nrepresent a user, with the hope that each embedding represents the user\u2019s\ninterest in a certain topic. With multi-interest representation, it\u2019s important\nto model the user\u2019s preference over the different topics and how the preference\nchange with time. However, existing approaches either fail to estimate the\nuser\u2019s affinity to each interest or unreasonably assume every interest of every\nuser fades with an equal rate with time, thus hurting the recall of candidate\nretrieval. In this paper, we propose the Multi-Interest Preference (MIP) model,\nan approach that not only produces multi-interest for users by using the user\u2019s\nsequential engagement more effectively but also automatically learns a set of\nweights to represent the preference over each embedding so that the candidates\ncan be retrieved from each interest proportionally. Extensive experiments have\nbeen done on various industrial-scale datasets to demonstrate the effectiveness\nof our approach.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [21.801799774169922, -27.68709373474121], "cluster": 7}, {"key": "shi2022information", "year": "2022", "citations": "0", "title": "Information-theoretic Hashing For Zero-shot Cross-modal Retrieval", "abstract": "<p>Zero-shot cross-modal retrieval (ZS-CMR) deals with the retrieval problem\namong heterogenous data from unseen classes. Typically, to guarantee\ngeneralization, the pre-defined class embeddings from natural language\nprocessing (NLP) models are used to build a common space. In this paper,\ninstead of using an extra NLP model to define a common space beforehand, we\nconsider a totally different way to construct (or learn) a common hamming space\nfrom an information-theoretic perspective. We term our model the\nInformation-Theoretic Hashing (ITH), which is composed of two cascading\nmodules: an Adaptive Information Aggregation (AIA) module; and a Semantic\nPreserving Encoding (SPE) module. Specifically, our AIA module takes the\ninspiration from the Principle of Relevant Information (PRI) to construct a\ncommon space that adaptively aggregates the intrinsic semantics of different\nmodalities of data and filters out redundant or irrelevant information. On the\nother hand, our SPE module further generates the hashing codes of different\nmodalities by preserving the similarity of intrinsic semantics with the\nelement-wise Kullback-Leibler (KL) divergence. A total correlation\nregularization term is also imposed to reduce the redundancy amongst different\ndimensions of hash codes. Sufficient experiments on three benchmark datasets\ndemonstrate the superiority of the proposed ITH in ZS-CMR. Source code is\navailable in the supplementary material.</p>\n", "tags": ["Hashing-Methods", "Few-Shot-&-Zero-Shot", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-11.483612060546875, 15.101808547973633], "cluster": 8}, {"key": "shi2022learning", "year": "2022", "citations": "1", "title": "Learning Similarity Preserving Binary Codes For Recommender Systems", "abstract": "<p>Hashing-based Recommender Systems (RSs) are widely studied to provide\nscalable services. The existing methods for the systems combine three modules\nto achieve efficiency: feature extraction, interaction modeling, and\nbinarization. In this paper, we study an unexplored module combination for the\nhashing-based recommender systems, namely Compact Cross-Similarity Recommender\n(CCSR). Inspired by cross-modal retrieval, CCSR utilizes Maximum a Posteriori\nsimilarity instead of matrix factorization and rating reconstruction to model\ninteractions between users and items. We conducted experiments on MovieLens1M,\nAmazon product review, Ichiba purchase dataset and confirmed CCSR outperformed\nthe existing matrix factorization-based methods. On the Movielens1M dataset,\nthe absolute performance improvements are up to 15.69% in NDCG and 4.29% in\nRecall. In addition, we extensively studied three binarization modules: \\(sign\\),\nscaled tanh, and sign-scaled tanh. The result demonstrated that although\ndifferentiable scaled tanh is popular in recent discrete feature learning\nliterature, a huge performance drop occurs when outputs of scaled \\(tanh\\) are\nforced to be binary.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Efficiency", "Recommender-Systems", "Multimodal-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-30.179101943969727, 22.706388473510742], "cluster": 0}, {"key": "shi2025scalable", "year": "2025", "citations": "0", "title": "Scalable Overload-aware Graph-based Index Construction For 10-billion-scale Vector Similarity Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is essential for modern\ndata-driven applications that require efficient retrieval of top-k results from\nmassive vector databases. Although existing graph-based ANNS algorithms achieve\na high recall rate on billion-scale datasets, their slow construction speed and\nlimited scalability hinder their applicability to large-scale industrial\nscenarios. In this paper, we introduce SOGAIC, the first Scalable\nOverload-Aware Graph-Based ANNS Index Construction system tailored for\nultra-large-scale vector databases: 1) We propose a dynamic data partitioning\nalgorithm with overload constraints that adaptively introduces overlaps among\nsubsets; 2) To enable efficient distributed subgraph construction, we employ a\nload-balancing task scheduling framework combined with an agglomerative merging\nstrategy; 3) Extensive experiments on various datasets demonstrate a reduction\nof 47.3% in average construction time compared to existing methods. The\nproposed method has also been successfully deployed in a real-world industrial\nsearch engine, managing over 10 billion daily updated vectors and serving\nhundreds of millions of users.</p>\n", "tags": ["Graph-Based-Ann", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [54.61113357543945, 14.646140098571777], "cluster": 9}, {"key": "shi2025variable", "year": "2019", "citations": "1", "title": "Variable-length Quantization Strategy For Hashing", "abstract": "<p>Hashing is widely used to solve fast Approximate Nearest Neighbor (ANN) search problems, involves converting the original real-valued samples to binary-valued representations. The conventional quantization strategies, such as Single-Bit Quantization and Multi-Bit quantization, are considered ineffective, because of their serious information loss. To address this issue, we propose a novel variable-length quantization (VLQ) strategy for hashing. In the proposed VLQ technique, we divide all samples into different regions in each dimension firstly given the real-valued features of samples. Then we compute the dispersion degrees of these regions. Subsequently, we attempt to optimally assign different number of bits to each dimensions to obtain the minimum dispersion degree. Our experiments show that the VLQ strategy achieves not only superior performance over the state-of-the-art methods, but also has a faster retrieval speed on public datasets.</p>\n", "tags": ["Quantization", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [10.539861679077148, 32.23737716674805], "cluster": 4}, {"key": "shimizu2022partial", "year": "2022", "citations": "0", "title": "Partial Visual-semantic Embedding: Fashion Intelligence System With Sensitive Part-by-part Learning", "abstract": "<p>In this study, we propose a technology called the Fashion Intelligence System\nbased on the visual-semantic embedding (VSE) model to quantify abstract and\ncomplex expressions unique to fashion, such as \u2018\u2018casual,\u2019\u2019 \u2018\u2018adult-casual,\u2019\u2019\nand \u2018\u2018office-casual,\u2019\u2019 and to support users\u2019 understanding of fashion. However,\nthe existing VSE model does not support the situations in which the image is\ncomposed of multiple parts such as hair, tops, pants, skirts, and shoes. We\npropose partial VSE, which enables sensitive learning for each part of the\nfashion coordinates. The proposed model partially learns embedded\nrepresentations. This helps retain the various existing practical\nfunctionalities and enables image-retrieval tasks in which changes are made\nonly to the specified parts and image reordering tasks that focus on the\nspecified parts. This was not possible with conventional models. Based on both\nthe qualitative and quantitative evaluation experiments, we show that the\nproposed model is superior to conventional models without increasing the\ncomputational complexity.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-22.74069595336914, -47.6318473815918], "cluster": 3}, {"key": "shin2019semi", "year": "2019", "citations": "9", "title": "Semi-supervised Feature-level Attribute Manipulation For Fashion Image Retrieval", "abstract": "<p>With a growing demand for the search by image, many works have studied the\ntask of fashion instance-level image retrieval (FIR). Furthermore, the recent\nworks introduce a concept of fashion attribute manipulation (FAM) which\nmanipulates a specific attribute (e.g color) of a fashion item while\nmaintaining the rest of the attributes (e.g shape, and pattern). In this way,\nusers can search not only \u201cthe same\u201d items but also \u201csimilar\u201d items with the\ndesired attributes. FAM is a challenging task in that the attributes are hard\nto define, and the unique characteristics of a query are hard to be preserved.\nAlthough both FIR and FAM are important in real-life applications, most of the\nprevious studies have focused on only one of these problem. In this study, we\naim to achieve competitive performance on both FIR and FAM. To do so, we\npropose a novel method that converts a query into a representation with the\ndesired attributes. We introduce a new idea of attribute manipulation at the\nfeature level, by matching the distribution of manipulated features with real\nfeatures. In this fashion, the attribute manipulation can be done independently\nfrom learning a representation from the image. By introducing the feature-level\nattribute manipulation, the previous methods for FIR can perform attribute\nmanipulation without sacrificing their retrieval performance.</p>\n", "tags": ["Supervised", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-24.218101501464844, -47.517459869384766], "cluster": 3}, {"key": "shlapentokhrothman2024region", "year": "2024", "citations": "3", "title": "Region-based Representations Revisited", "abstract": "<p>We investigate whether region-based representations are effective for\nrecognition. Regions were once a mainstay in recognition approaches, but pixel\nand patch-based features are now used almost exclusively. We show that recent\nclass-agnostic segmenters like SAM can be effectively combined with strong\nunsupervised representations like DINOv2 and used for a wide variety of tasks,\nincluding semantic segmentation, object-based image retrieval, and multi-image\nanalysis. Once the masks and features are extracted, these representations,\neven with linear decoders, enable competitive performance, making them well\nsuited to applications that require custom queries. The compactness of the\nrepresentation also makes it well-suited to video analysis and other problems\nrequiring inference across many images.</p>\n", "tags": ["CVPR", "Evaluation", "Image-Retrieval", "Unsupervised"], "tsne_embedding": [-19.04503631591797, 9.696756362915039], "cluster": 8}, {"key": "shohoud2023quranic", "year": "2023", "citations": "0", "title": "Quranic Conversations: Developing A Semantic Search Tool For The Quran Using Arabic NLP Techniques", "abstract": "<p>The Holy Book of Quran is believed to be the literal word of God (Allah) as\nrevealed to the Prophet Muhammad (PBUH) over a period of approximately 23\nyears. It is the book where God provides guidance on how to live a righteous\nand just life, emphasizing principles like honesty, compassion, charity and\njustice, as well as providing rules for personal conduct, family matters,\nbusiness ethics and much more. However, due to constraints related to the\nlanguage and the Quran organization, it is challenging for Muslims to get all\nrelevant ayahs (verses) pertaining to a matter or inquiry of interest. Hence,\nwe developed a Quran semantic search tool which finds the verses pertaining to\nthe user inquiry or prompt. To achieve this, we trained several models on a\nlarge dataset of over 30 tafsirs, where typically each tafsir corresponds to\none verse in the Quran and, using cosine similarity, obtained the tafsir tensor\nwhich is most similar to the prompt tensor of interest, which was then used to\nindex for the corresponding ayah in the Quran. Using the SNxLM model, we were\nable to achieve a cosine similarity score as high as 0.97 which corresponds to\nthe abdu tafsir for a verse relating to financial matters.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [16.693584442138672, -30.03042984008789], "cluster": 7}, {"key": "shon2018large", "year": "2019", "citations": "2", "title": "Large-scale Speaker Retrieval On Random Speaker Variability Subspace", "abstract": "<p>This paper describes a fast speaker search system to retrieve segments of the\nsame voice identity in the large-scale data. A recent study shows that Locality\nSensitive Hashing (LSH) enables quick retrieval of a relevant voice in the\nlarge-scale data in conjunction with i-vector while maintaining accuracy. In\nthis paper, we proposed Random Speaker-variability Subspace (RSS) projection to\nmap a data into LSH based hash tables. We hypothesized that rather than\nprojecting on completely random subspace without considering data, projecting\non randomly generated speaker variability space would give more chance to put\nthe same speaker representation into the same hash bins, so we can use less\nnumber of hash tables. Multiple RSS can be generated by randomly selecting a\nsubset of speakers from a large speaker cohort. From the experimental result,\nthe proposed approach shows 100 times and 7 times faster than the linear search\nand LSH, respectively</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "Evaluation"], "tsne_embedding": [9.494181632995605, 32.055389404296875], "cluster": 4}, {"key": "shore2024spagbol", "year": "2025", "citations": "0", "title": "Spagbol: Spatial-graph-based Orientated Localisation", "abstract": "<p>Cross-View Geo-Localisation within urban regions is challenging in part due\nto the lack of geo-spatial structuring within current datasets and techniques.\nWe propose utilising graph representations to model sequences of local\nobservations and the connectivity of the target location. Modelling as a graph\nenables generating previously unseen sequences by sampling with new parameter\nconfigurations. To leverage this newly available information, we propose a\nGNN-based architecture, producing spatially strong embeddings and improving\ndiscriminability over isolated image embeddings. We outline SpaGBOL,\nintroducing three novel contributions. 1) The first graph-structured dataset\nfor Cross-View Geo-Localisation, containing multiple streetview images per node\nto improve generalisation. 2) Introducing GNNs to the problem, we develop the\nfirst system that exploits the correlation between node proximity and feature\nsimilarity. 3) Leveraging the unique properties of the graph representation -\nwe demonstrate a novel retrieval filtering approach based on neighbourhood\nbearings. SpaGBOL achieves state-of-the-art accuracies on the unseen test graph</p>\n<ul>\n  <li>with relative Top-1 retrieval improvements on previous techniques of 11%, and\n50% when filtering with Bearing Vector Matching on the SpaGBOL dataset.</li>\n</ul>\n", "tags": ["Graph-Based-Ann", "Datasets"], "tsne_embedding": [50.07366943359375, -9.592580795288086], "cluster": 9}, {"key": "shrestha2023espn", "year": "2024", "citations": "3", "title": "ESPN: Memory-efficient Multi-vector Information Retrieval", "abstract": "<p>Recent advances in large language models have demonstrated remarkable\neffectiveness in information retrieval (IR) tasks. While many neural IR systems\nencode queries and documents into single-vector representations, multi-vector\nmodels elevate the retrieval quality by producing multi-vector representations\nand facilitating similarity searches at the granularity of individual tokens.\nHowever, these models significantly amplify memory and storage requirements for\nretrieval indices by an order of magnitude. This escalation in index size\nrenders the scalability of multi-vector IR models progressively challenging due\nto their substantial memory demands. We introduce Embedding from Storage\nPipelined Network (ESPN) where we offload the entire re-ranking embedding\ntables to SSDs and reduce the memory requirements by 5-16x. We design a\nsoftware prefetcher with hit rates exceeding 90%, improving SSD based retrieval\nup to 6.4x, and demonstrate that we can maintain near memory levels of query\nlatency even for large query batch sizes.</p>\n", "tags": ["Re-Ranking", "Hybrid-Ann-Methods", "Similarity-Search", "Scalability"], "tsne_embedding": [31.929336547851562, 14.035367965698242], "cluster": 2}, {"key": "shrivastava2014asymmetric", "year": "2014", "citations": "268", "title": "Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).", "abstract": "<p>We present the first provably sublinear time hashing algorithm for approximate\nMaximum Inner Product Search (MIPS). Searching with (un-normalized) inner\nproduct as the underlying similarity measure is a known difficult problem and\nfinding hashing schemes for MIPS was considered hard. While the existing Locality\nSensitive Hashing (LSH) framework is insufficient for solving MIPS, in this\npaper we extend the LSH framework to allow asymmetric hashing schemes. Our\nproposal is based on a key observation that the problem of finding maximum inner\nproducts, after independent asymmetric transformations, can be converted into\nthe problem of approximate near neighbor search in classical settings. This key\nobservation makes efficient sublinear hashing scheme for MIPS possible. Under\nthe extended asymmetric LSH (ALSH) framework, this paper provides an example\nof explicit construction of provably fast hashing scheme for MIPS. Our proposed\nalgorithm is simple and easy to implement. The proposed hashing scheme\nleads to significant computational savings over the two popular conventional LSH\nschemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable\ndistributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations\non Netflix and Movielens (10M) datasets.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [14.719525337219238, 38.118675231933594], "cluster": 4}, {"key": "shrivastava2014densifying", "year": "2014", "citations": "86", "title": "Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search", "abstract": "<p>The query complexity of locality sensitive hashing\n(LSH) based similarity search is dominated\nby the number of hash evaluations, and this number\ngrows with the data size (Indyk &amp; Motwani,\n1998). In industrial applications such as search\nwhere the data are often high-dimensional and\nbinary (e.g., text n-grams), minwise hashing is\nwidely adopted, which requires applying a large\nnumber of permutations on the data. This is\ncostly in computation and energy-consumption.\nIn this paper, we propose a hashing technique\nwhich generates all the necessary hash evaluations\nneeded for similarity search, using one\nsingle permutation. The heart of the proposed\nhash function is a \u201crotation\u201d scheme which densifies\nthe sparse sketches of one permutation\nhashing (Li et al., 2012) in an unbiased fashion\nthereby maintaining the LSH property. This\nmakes the obtained sketches suitable for hash table\nconstruction. This idea of rotation presented\nin this paper could be of independent interest for\ndensifying other types of sparse sketches.\nUsing our proposed hashing method, the query\ntime of a (K, L)-parameterized LSH is reduced\nfrom the typical O(dKL) complexity to merely\nO(KL + dL), where d is the number of nonzeros\nof the data vector, K is the number of hashes\nin each hash table, and L is the number of hash\ntables. Our experimental evaluation on real data\nconfirms that the proposed scheme significantly\nreduces the query processing time over minwise\nhashing without loss in retrieval accuracies.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [18.96767234802246, 16.903703689575195], "cluster": 2}, {"key": "shrivastava2016exact", "year": "2016", "citations": "7", "title": "Exact Weighted Minwise Hashing In Constant Time", "abstract": "<p>Weighted minwise hashing (WMH) is one of the fundamental subroutine, required\nby many celebrated approximation algorithms, commonly adopted in industrial\npractice for large scale-search and learning. The resource bottleneck of the\nalgorithms is the computation of multiple (typically a few hundreds to\nthousands) independent hashes of the data. The fastest hashing algorithm is by\nIoffe \\cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data\nvector, \\(O(d)\\) (\\(d\\) is the number of non-zeros), for computing one hash.\nHowever, the requirement of multiple hashes demands hundreds or thousands\npasses over the data. This is very costly for modern massive dataset.\n  In this work, we break this expensive barrier and show an expected constant\namortized time algorithm which computes \\(k\\) independent and unbiased WMH in\ntime \\(O(k)\\) instead of \\(O(dk)\\) required by Ioffe\u2019s method. Moreover, our\nproposal only needs a few bits (5 - 9 bits) of storage per hash value compared\nto around \\(64\\) bits required by the state-of-art-methodologies. Experimental\nevaluations, on real datasets, show that for computing 500 WMH, our proposal\ncan be 60000x faster than the Ioffe\u2019s method without losing any accuracy. Our\nmethod is also around 100x faster than approximate heuristics capitalizing on\nthe efficient \u201cdensified\u201d one permutation hashing schemes\n\\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its\nsignificant advantages, we hope that it will replace existing implementations\nin practice.</p>\n", "tags": ["Hashing-Methods", "Datasets"], "tsne_embedding": [14.04161262512207, 24.203235626220703], "cluster": 4}, {"key": "shrivastava2017optimal", "year": "2017", "citations": "27", "title": "Optimal Densification For Fast And Accurate Minwise Hashing", "abstract": "<p>Minwise hashing is a fundamental and one of the most successful hashing\nalgorithm in the literature. Recent advances based on the idea of\ndensification~\\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown\nthat it is possible to compute \\(k\\) minwise hashes, of a vector with \\(d\\)\nnonzeros, in mere \\((d + k)\\) computations, a significant improvement over the\nclassical \\(O(dk)\\). These advances have led to an algorithmic improvement in the\nquery complexity of traditional indexing algorithms based on minwise hashing.\nUnfortunately, the variance of the current densification techniques is\nunnecessarily high, which leads to significantly poor accuracy compared to\nvanilla minwise hashing, especially when the data is sparse. In this paper, we\nprovide a novel densification scheme which relies on carefully tailored\n2-universal hashes. We show that the proposed scheme is variance-optimal, and\nwithout losing the runtime efficiency, it is significantly more accurate than\nexisting densification techniques. As a result, we obtain a significantly\nefficient hashing scheme which has the same variance and collision probability\nas minwise hashing. Experimental evaluations on real sparse and\nhigh-dimensional datasets validate our claims. We believe that given the\nsignificant advantages, our method will replace minwise hashing implementations\nin practice.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Datasets"], "tsne_embedding": [11.590883255004883, 24.38156509399414], "cluster": 4}, {"key": "shrivastava2021clip", "year": "2021", "citations": "1", "title": "Clip-lite: Information Efficient Visual Representation Learning With Language Supervision", "abstract": "<p>We propose CLIP-Lite, an information efficient method for visual\nrepresentation learning by feature alignment with textual annotations. Compared\nto the previously proposed CLIP model, CLIP-Lite requires only one negative\nimage-text sample pair for every positive image-text sample during the\noptimization of its contrastive learning objective. We accomplish this by\ntaking advantage of an information efficient lower-bound to maximize the mutual\ninformation between the two input modalities. This allows CLIP-Lite to be\ntrained with significantly reduced amounts of data and batch sizes while\nobtaining better performance than CLIP at the same scale. We evaluate CLIP-Lite\nby pretraining on the COCO-Captions dataset and testing transfer learning to\nother datasets. CLIP-Lite obtains a +14.0% mAP absolute gain in performance on\nPascal VOC classification, and a +22.1% top-1 accuracy gain on ImageNet, while\nbeing comparable or superior to other, more complex, text-supervised models.\nCLIP-Lite is also superior to CLIP on image and text retrieval, zero-shot\nclassification, and visual grounding. Finally, we show that CLIP-Lite can\nleverage language semantics to encourage bias-free visual representations that\ncan be used in downstream tasks. Implementation:\nhttps://github.com/4m4n5/CLIP-Lite</p>\n", "tags": ["Self-Supervised", "Text-Retrieval", "Few-Shot-&-Zero-Shot", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-25.676227569580078, -21.05988121032715], "cluster": 5}, {"key": "shrivastava2025asymmetric", "year": "2014", "citations": "268", "title": "Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).", "abstract": "<p>We present the first provably sublinear time hashing algorithm for approximate\nMaximum Inner Product Search (MIPS). Searching with (un-normalized) inner\nproduct as the underlying similarity measure is a known difficult problem and\nfinding hashing schemes for MIPS was considered hard. While the existing Locality\nSensitive Hashing (LSH) framework is insufficient for solving MIPS, in this\npaper we extend the LSH framework to allow asymmetric hashing schemes. Our\nproposal is based on a key observation that the problem of finding maximum inner\nproducts, after independent asymmetric transformations, can be converted into\nthe problem of approximate near neighbor search in classical settings. This key\nobservation makes efficient sublinear hashing scheme for MIPS possible. Under\nthe extended asymmetric LSH (ALSH) framework, this paper provides an example\nof explicit construction of provably fast hashing scheme for MIPS. Our proposed\nalgorithm is simple and easy to implement. The proposed hashing scheme\nleads to significant computational savings over the two popular conventional LSH\nschemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable\ndistributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations\non Netflix and Movielens (10M) datasets.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [14.719496726989746, 38.11865234375], "cluster": 4}, {"key": "shrivastava2025densifying", "year": "2014", "citations": "86", "title": "Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search", "abstract": "<p>The query complexity of locality sensitive hashing\n(LSH) based similarity search is dominated\nby the number of hash evaluations, and this number\ngrows with the data size (Indyk &amp; Motwani,\n1998). In industrial applications such as search\nwhere the data are often high-dimensional and\nbinary (e.g., text n-grams), minwise hashing is\nwidely adopted, which requires applying a large\nnumber of permutations on the data. This is\ncostly in computation and energy-consumption.\nIn this paper, we propose a hashing technique\nwhich generates all the necessary hash evaluations\nneeded for similarity search, using one\nsingle permutation. The heart of the proposed\nhash function is a \u201crotation\u201d scheme which densifies\nthe sparse sketches of one permutation\nhashing (Li et al., 2012) in an unbiased fashion\nthereby maintaining the LSH property. This\nmakes the obtained sketches suitable for hash table\nconstruction. This idea of rotation presented\nin this paper could be of independent interest for\ndensifying other types of sparse sketches.\nUsing our proposed hashing method, the query\ntime of a (K, L)-parameterized LSH is reduced\nfrom the typical O(dKL) complexity to merely\nO(KL + dL), where d is the number of nonzeros\nof the data vector, K is the number of hashes\nin each hash table, and L is the number of hash\ntables. Our experimental evaluation on real data\nconfirms that the proposed scheme significantly\nreduces the query processing time over minwise\nhashing without loss in retrieval accuracies.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [18.96794319152832, 16.903892517089844], "cluster": 2}, {"key": "shu2017compressing", "year": "2017", "citations": "95", "title": "Compressing Word Embeddings Via Deep Compositional Code Learning", "abstract": "<p>Natural language processing (NLP) models often require a massive number of\nparameters for word embeddings, resulting in a large storage or memory\nfootprint. Deploying neural NLP models to mobile devices requires compressing\nthe word embeddings without any significant sacrifices in performance. For this\npurpose, we propose to construct the embeddings with few basis vectors. For\neach word, the composition of basis vectors is determined by a hash code. To\nmaximize the compression rate, we adopt the multi-codebook quantization\napproach instead of binary coding scheme. Each code is composed of multiple\ndiscrete numbers, such as (3, 2, 1, 8), where the value of each component is\nlimited to a fixed range. We propose to directly learn the discrete codes in an\nend-to-end neural network by applying the Gumbel-softmax trick. Experiments\nshow the compression rate achieves 98% in a sentiment analysis task and 94% ~\n99% in machine translation tasks without performance loss. In both tasks, the\nproposed method can improve the model performance by slightly lowering the\ncompression rate. Compared to other approaches such as character-level\nsegmentation, the proposed method is language-independent and does not require\nmodifications to the network architecture.</p>\n", "tags": ["Quantization", "Evaluation", "Hashing-Methods"], "tsne_embedding": [22.772655487060547, 10.91792106628418], "cluster": 2}, {"key": "shukor2022transformer", "year": "2022", "citations": "23", "title": "Transformer Decoders With Multimodal Regularization For Cross-modal Food Retrieval", "abstract": "<p>Cross-modal image-recipe retrieval has gained significant attention in recent\nyears. Most work focuses on improving cross-modal embeddings using unimodal\nencoders, that allow for efficient retrieval in large-scale databases, leaving\naside cross-attention between modalities which is more computationally\nexpensive. We propose a new retrieval framework, T-Food (Transformer Decoders\nwith MultiModal Regularization for Cross-Modal Food Retrieval) that exploits\nthe interaction between modalities in a novel regularization scheme, while\nusing only unimodal encoders at test time for efficient retrieval. We also\ncapture the intra-dependencies between recipe entities with a dedicated recipe\nencoder, and propose new variants of triplet losses with dynamic margins that\nadapt to the difficulty of the task. Finally, we leverage the power of the\nrecent Vision and Language Pretraining (VLP) models such as CLIP for the image\nencoder. Our approach outperforms existing approaches by a large margin on the\nRecipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6\nR@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code\nis available here:https://github.com/mshukor/TFood</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-41.12369918823242, 31.745845794677734], "cluster": 0}, {"key": "shvetsova2021everything", "year": "2022", "citations": "98", "title": "Everything At Once -- Multi-modal Fusion Transformer For Video Retrieval", "abstract": "<p>Multi-modal learning from video data has seen increased attention recently as\nit allows to train semantically meaningful embeddings without human annotation\nenabling tasks like zero-shot retrieval and classification. In this work, we\npresent a multi-modal, modality agnostic fusion transformer approach that\nlearns to exchange information between multiple modalities, such as video,\naudio, and text, and integrate them into a joined multi-modal representation to\nobtain an embedding that aggregates multi-modal temporal information. We\npropose to train the system with a combinatorial loss on everything at once,\nsingle modalities as well as pairs of modalities, explicitly leaving out any\nadd-ons such as position or modality encoding. At test time, the resulting\nmodel can process and fuse any number of input modalities. Moreover, the\nimplicit properties of the transformer allow to process inputs of different\nlengths. To evaluate the proposed approach, we train the model on the large\nscale HowTo100M dataset and evaluate the resulting embedding space on four\nchallenging benchmark datasets obtaining state-of-the-art results in zero-shot\nvideo retrieval and zero-shot video action localization.</p>\n", "tags": ["CVPR", "Few-Shot-&-Zero-Shot", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-12.810614585876465, -34.65330123901367], "cluster": 3}, {"key": "sidhu2024search", "year": "2024", "citations": "0", "title": "Search And Detect: Training-free Long Tail Object Detection Via Web-image Retrieval", "abstract": "<p>In this paper, we introduce SearchDet, a training-free long-tail object\ndetection framework that significantly enhances open-vocabulary object\ndetection performance. SearchDet retrieves a set of positive and negative\nimages of an object to ground, embeds these images, and computes an input\nimage-weighted query which is used to detect the desired concept in the image.\nOur proposed method is simple and training-free, yet achieves over 48.7% mAP\nimprovement on ODinW and 59.1% mAP improvement on LVIS compared to\nstate-of-the-art models such as GroundingDINO. We further show that our\napproach of basing object detection on a set of Web-retrieved exemplars is\nstable with respect to variations in the exemplars, suggesting a path towards\neliminating costly data annotation and training procedures.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-13.728690147399902, -23.281230926513672], "cluster": 3}, {"key": "sikka2019deep", "year": "2019", "citations": "1", "title": "Deep Unified Multimodal Embeddings For Understanding Both Content And Users In Social Media Networks", "abstract": "<p>There has been an explosion of multimodal content generated on social media\nnetworks in the last few years, which has necessitated a deeper understanding\nof social media content and user behavior. We present a novel\ncontent-independent content-user-reaction model for social multimedia content\nanalysis. Compared to prior works that generally tackle semantic content\nunderstanding and user behavior modeling in isolation, we propose a generalized\nsolution to these problems within a unified framework. We embed users, images\nand text drawn from open social media in a common multimodal geometric space,\nusing a novel loss function designed to cope with distant and disparate\nmodalities, and thereby enable seamless three-way retrieval. Our model not only\noutperforms unimodal embedding based methods on cross-modal retrieval tasks but\nalso shows improvements stemming from jointly solving the two tasks on Twitter\ndata. We also show that the user embeddings learned within our joint multimodal\nembedding model are better at predicting user interests compared to those\nlearned with unimodal content on Instagram data. Our framework thus goes beyond\nthe prior practice of using explicit leader-follower link information to\nestablish affiliations by extracting implicit content-centric affiliations from\nisolated users. We provide qualitative results to show that the user clusters\nemerging from learned embeddings have consistent semantics and the ability of\nour model to discover fine-grained semantics from noisy and unstructured data.\nOur work reveals that social multimodal content is inherently multimodal and\npossesses a consistent structure because in social networks meaning is created\nthrough interactions between users and content.</p>\n", "tags": ["Multimodal-Retrieval", "Tools-&-Libraries"], "tsne_embedding": [-7.396719455718994, -33.56987762451172], "cluster": 3}, {"key": "silavong2021deskew", "year": "2021", "citations": "1", "title": "Deskew-lsh Based Code-to-code Recommendation Engine", "abstract": "<p>Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with <em>Senatus</em>, a new code-to-code recommendation engine. At the core of Senatus is <em>De-Skew</em> LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus via automatic evaluation and with an expert developer user study and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example, on the CodeSearchNet dataset we show that Senatus improves performance by 6.7% F1 and query time 16x is faster compared to Facebook Aroma on the task of code-to-code recommendation.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Recommender-Systems", "Scalability", "Tree-Based-Ann", "Datasets", "Evaluation"], "tsne_embedding": [20.179235458374023, 13.112371444702148], "cluster": 2}, {"key": "silavong2025deskew", "year": "2021", "citations": "1", "title": "Deskew-lsh Based Code-to-code Recommendation Engine", "abstract": "<p>Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with <em>Senatus</em>, a new code-to-code recommendation engine. At the core of Senatus is <em>De-Skew</em> LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus via automatic evaluation and with an expert developer user study and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example, on the CodeSearchNet dataset we show that Senatus improves performance by 6.7% F1 and query time 16x is faster compared to Facebook Aroma on the task of code-to-code recommendation.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Recommender-Systems", "Scalability", "Tree-Based-Ann", "Datasets", "Evaluation"], "tsne_embedding": [20.179235458374023, 13.112371444702148], "cluster": 2}, {"key": "silcock2022noise", "year": "2022", "citations": "2", "title": "Noise-robust De-duplication At Scale", "abstract": "<p>Identifying near duplicates within large, noisy text corpora has a myriad of\napplications that range from de-duplicating training datasets, reducing privacy\nrisk, and evaluating test set leakage, to identifying reproduced news articles\nand literature within large corpora. Across these diverse applications, the\noverwhelming majority of work relies on N-grams. Limited efforts have been made\nto evaluate how well N-gram methods perform, in part because it is unclear how\none could create an unbiased evaluation dataset for a massive corpus. This\nstudy uses the unique timeliness of historical news wires to create a 27,210\ndocument dataset, with 122,876 positive duplicate pairs, for studying\nnoise-robust de-duplication. The time-sensitivity of news makes comprehensive\nhand labelling feasible - despite the massive overall size of the corpus - as\nduplicates occur within a narrow date range. The study then develops and\nevaluates a range of de-duplication methods: hashing and N-gram overlap (which\npredominate in the literature), a contrastively trained bi-encoder, and a\nre-rank style approach combining a bi- and cross-encoder. The neural approaches\nsignificantly outperform hashing and N-gram overlap. We show that the\nbi-encoder scales well, de-duplicating a 10 million article corpus on a single\nGPU card in a matter of hours. We also apply our pre-trained model to the\nRealNews and patent portions of C4 (Colossal Clean Crawled Corpus),\nillustrating that a neural approach can identify many near duplicates missed by\nhashing, in the presence of various types of noise. The public release of our\nNEWS-COPY de-duplication dataset, codebase, and the pre-trained models will\nfacilitate further research and applications.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [14.7408447265625, 22.014205932617188], "cluster": 2}, {"key": "simhadri2024results", "year": "2024", "citations": "0", "title": "Results Of The Big ANN: Neurips'23 Competition", "abstract": "<p>The 2023 Big ANN Challenge, held at NeurIPS 2023, focused on advancing the\nstate-of-the-art in indexing data structures and search algorithms for\npractical variants of Approximate Nearest Neighbor (ANN) search that reflect\nthe growing complexity and diversity of workloads. Unlike prior challenges that\nemphasized scaling up classical ANN search\n~\\cite{DBLP:conf/nips/SimhadriWADBBCH21}, this competition addressed filtered\nsearch, out-of-distribution data, sparse and streaming variants of ANNS.\nParticipants developed and submitted innovative solutions that were evaluated\non new standard datasets with constrained computational resources. The results\nshowcased significant improvements in search accuracy and efficiency over\nindustry-standard baselines, with notable contributions from both academic and\nindustrial teams. This paper summarizes the competition tracks, datasets,\nevaluation metrics, and the innovative approaches of the top-performing\nsubmissions, providing insights into the current advancements and future\ndirections in the field of approximate nearest neighbor search.</p>\n", "tags": ["Efficiency", "Evaluation", "Similarity-Search", "Datasets"], "tsne_embedding": [10.52979564666748, -31.569353103637695], "cluster": 7}, {"key": "sim\u00e9oni2017unsupervised", "year": "2018", "citations": "12", "title": "Unsupervised Object Discovery For Instance Recognition", "abstract": "<p>Severe background clutter is challenging in many computer vision tasks,\nincluding large-scale image retrieval. Global descriptors, that are popular due\nto their memory and search efficiency, are especially prone to corruption by\nsuch a clutter. Eliminating the impact of the clutter on the image descriptor\nincreases the chance of retrieving relevant images and prevents topic drift due\nto actually retrieving the clutter in the case of query expansion. In this\nwork, we propose a novel salient region detection method. It captures, in an\nunsupervised manner, patterns that are both discriminative and common in the\ndataset. Saliency is based on a centrality measure of a nearest neighbor graph\nconstructed from regional CNN representations of dataset images. The\ndescriptors derived from the salient regions improve particular object\nretrieval, most noticeably in a large collections containing small objects.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Unsupervised"], "tsne_embedding": [-47.09785461425781, 1.273856520652771], "cluster": 0}, {"key": "sim\u00e9oni2019local", "year": "2019", "citations": "70", "title": "Local Features And Visual Words Emerge In Activations", "abstract": "<p>We propose a novel method of deep spatial matching (DSM) for image retrieval.\nInitial ranking is based on image descriptors extracted from convolutional\nneural network activations by global pooling, as in recent state-of-the-art\nwork. However, the same sparse 3D activation tensor is also approximated by a\ncollection of local features. These local features are then robustly matched to\napproximate the optimal alignment of the tensors. This happens without any\nnetwork modification, additional layers or training. No local feature detection\nhappens on the original image. No local feature descriptors and no visual\nvocabulary are needed throughout the whole process.\n  We experimentally show that the proposed method achieves the state-of-the-art\nperformance on standard benchmarks across different network architectures and\ndifferent global pooling methods. The highest gain in performance is achieved\nwhen diffusion on the nearest-neighbor graph of global descriptors is initiated\nfrom spatially verified images.</p>\n", "tags": ["CVPR", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-17.6877498626709, 1.90665602684021], "cluster": 1}, {"key": "singh2016efficient", "year": "2016", "citations": "0", "title": "Efficient Document Indexing Using Pivot Tree", "abstract": "<p>We present a novel method for efficiently searching top-k neighbors for\ndocuments represented in high dimensional space of terms based on the cosine\nsimilarity. Mostly, documents are stored as bag-of-words tf-idf representation.\nOne of the most used ways of computing similarity between a pair of documents\nis cosine similarity between the vector representations, but cosine similarity\nis not a metric distance measure as it doesn\u2019t follow triangle inequality,\ntherefore most metric searching methods can not be applied directly. We propose\nan efficient method for indexing documents using a pivot tree that leads to\nefficient retrieval. We also study the relation between precision and\nefficiency for the proposed method and compare it with a state of the art in\nthe area of document searching based on inner product.</p>\n", "tags": ["Efficiency", "Evaluation", "Distance-Metric-Learning", "Similarity-Search"], "tsne_embedding": [27.537399291992188, 31.90413475036621], "cluster": 2}, {"key": "singh2016learning", "year": "2016", "citations": "4", "title": "Learning To Hash-tag Videos With Tag2vec", "abstract": "<p>User-given tags or labels are valuable resources for semantic understanding\nof visual media such as images and videos. Recently, a new type of labeling\nmechanism known as hash-tags have become increasingly popular on social media\nsites. In this paper, we study the problem of generating relevant and useful\nhash-tags for short video clips. Traditional data-driven approaches for tag\nenrichment and recommendation use direct visual similarity for label transfer\nand propagation. We attempt to learn a direct low-cost mapping from video to\nhash-tags using a two step training process. We first employ a natural language\nprocessing (NLP) technique, skip-gram models with neural network training to\nlearn a low-dimensional vector representation of hash-tags (Tag2Vec) using a\ncorpus of 10 million hash-tags. We then train an embedding function to map\nvideo features to the low-dimensional Tag2vec space. We learn this embedding\nfor 29 categories of short video clips with hash-tags. A query video without\nany tag-information can then be directly mapped to the vector space of tags\nusing the learned embedding and relevant tags can be found by performing a\nsimple nearest-neighbor retrieval in the Tag2Vec space. We validate the\nrelevance of the tags suggested by our system qualitatively and quantitatively\nwith a user study.</p>\n", "tags": ["Evaluation", "Recommender-Systems", "Hashing-Methods"], "tsne_embedding": [7.106109142303467, -8.526253700256348], "cluster": 6}, {"key": "singh2019adversarially", "year": "2019", "citations": "2", "title": "Adversarially Trained Deep Neural Semantic Hashing Scheme For Subjective Search In Fashion Inventory", "abstract": "<p>The simple approach of retrieving a closest match of a query image from one\nin the gallery, compares an image pair using sum of absolute difference in\npixel or feature space. The process is computationally expensive, ill-posed to\nillumination, background composition, pose variation, as well as inefficient to\nbe deployed on gallery sets with more than 1000 elements. Hashing is a faster\nalternative which involves representing images in reduced dimensional simple\nfeature spaces. Encoding images into binary hash codes enables similarity\ncomparison in an image-pair using the Hamming distance measure. The challenge,\nhowever, lies in encoding the images using a semantic hashing scheme that lets\nsubjective neighbors lie within the tolerable Hamming radius. This work\npresents a solution employing adversarial learning of a deep neural semantic\nhashing network for fashion inventory retrieval. It consists of a feature\nextracting convolutional neural network (CNN) learned to (i) minimize error in\nclassifying type of clothing, (ii) minimize hamming distance between semantic\nneighbors and maximize distance between semantically dissimilar images, (iii)\nmaximally scramble a discriminator\u2019s ability to identify the corresponding hash\ncode-image pair when processing a semantically similar query-gallery image\npair. Experimental validation for fashion inventory search yields a mean\naverage precision (mAP) of 90.65% in finding the closest match as compared to\n53.26% obtained by the prior art of deep Cauchy hashing for hamming space\nretrieval.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Text-Retrieval", "Robustness"], "tsne_embedding": [-26.7847900390625, 12.356096267700195], "cluster": 0}, {"key": "singh2019one", "year": "2019", "citations": "6", "title": "One Embedding To Do Them All", "abstract": "<p>Online shopping caters to the needs of millions of users daily. Search,\nrecommendations, personalization have become essential building blocks for\nserving customer needs. Efficacy of such systems is dependent on a thorough\nunderstanding of products and their representation. Multiple information\nsources and data types provide a complete picture of the product on the\nplatform. While each of these tasks shares some common characteristics,\ntypically product embeddings are trained and used in isolation.\n  In this paper, we propose a framework to combine multiple data sources and\nlearn unified embeddings for products on our e-commerce platform. Our product\nembeddings are built from three types of data sources - catalog text data, a\nuser\u2019s clickstream session data and product images. We use various techniques\nlike denoising auto-encoders for text, Bayesian personalized ranking (BPR) for\nclickstream data, Siamese neural network architecture for image data and\ncombined ensemble over the above methods for unified embeddings. Further, we\ncompare and analyze the performance of these embeddings across three unrelated\nreal-world e-commerce tasks specifically checking product attribute coverage,\nfinding similar products and predicting returns. We show that unified product\nembeddings perform uniformly well across all these tasks.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation"], "tsne_embedding": [-14.441625595092773, -45.19132995605469], "cluster": 3}, {"key": "singh2020ihashnet", "year": "2020", "citations": "5", "title": "Ihashnet: Iris Hashing Network Based On Efficient Multi-index Hashing", "abstract": "<p>Massive biometric deployments are pervasive in today\u2019s world. But despite the\nhigh accuracy of biometric systems, their computational efficiency degrades\ndrastically with an increase in the database size. Thus, it is essential to\nindex them. An ideal indexing scheme needs to generate codes that preserve the\nintra-subject similarity as well as inter-subject dissimilarity. Here, in this\npaper, we propose an iris indexing scheme using real-valued deep iris features\nbinarized to iris bar codes (IBC) compatible with the indexing structure.\nFirstly, for extracting robust iris features, we have designed a network\nutilizing the domain knowledge of ordinal filtering and learning their\nnonlinear combinations. Later these real-valued features are binarized.\nFinally, for indexing the iris dataset, we have proposed a loss that can\ntransform the binary feature into an improved feature compatible with the\nMulti-Index Hashing scheme. This loss function ensures the hamming distance\nequally distributed among all the contiguous disjoint sub-strings. To the best\nof our knowledge, this is the first work in the iris indexing domain that\npresents an end-to-end iris indexing structure. Experimental results on four\ndatasets are presented to depict the efficacy of the proposed approach.</p>\n", "tags": ["Efficiency", "Vector-Indexing", "Hashing-Methods", "Datasets"], "tsne_embedding": [-5.512535572052002, 13.031205177307129], "cluster": 8}, {"key": "singh2021freshdiskann", "year": "2021", "citations": "10", "title": "Freshdiskann: A Fast And Accurate Graph-based ANN Index For Streaming Similarity Search", "abstract": "<p>Approximate nearest neighbor search (ANNS) is a fundamental building block in\ninformation retrieval with graph-based indices being the current\nstate-of-the-art and widely used in the industry. Recent advances in\ngraph-based indices have made it possible to index and search billion-point\ndatasets with high recall and millisecond-level latency on a single commodity\nmachine with an SSD.\n  However, existing graph algorithms for ANNS support only static indices that\ncannot reflect real-time changes to the corpus required by many key real-world\nscenarios (e.g. index of sentences in documents, email, or a news index). To\novercome this drawback, the current industry practice for manifesting updates\ninto such indices is to periodically re-build these indices, which can be\nprohibitively expensive.\n  In this paper, we present the first graph-based ANNS index that reflects\ncorpus updates into the index in real-time without compromising on search\nperformance. Using update rules for this index, we design FreshDiskANN, a\nsystem that can index over a billion points on a workstation with an SSD and\nlimited memory, and support thousands of concurrent real-time inserts, deletes\nand searches per second each, while retaining \\(&gt;95%\\) 5-recall@5. This\nrepresents a 5-10x reduction in the cost of maintaining freshness in indices\nwhen compared to existing methods.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Vector-Indexing", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [56.71685028076172, 13.602856636047363], "cluster": 9}, {"key": "singh2021joint", "year": "2023", "citations": "4", "title": "Joint Triplet Autoencoder For Histopathological Colon Cancer Nuclei Retrieval", "abstract": "<p>Deep learning has shown a great improvement in the performance of visual\ntasks. Image retrieval is the task of extracting the visually similar images\nfrom a database for a query image. The feature matching is performed to rank\nthe images. Various hand-designed features have been derived in past to\nrepresent the images. Nowadays, the power of deep learning is being utilized\nfor automatic feature learning from data in the field of biomedical image\nanalysis. Autoencoder and Siamese networks are two deep learning models to\nlearn the latent space (i.e., features or embedding). Autoencoder works based\non the reconstruction of the image from latent space. Siamese network utilizes\nthe triplets to learn the intra-class similarity and inter-class dissimilarity.\nMoreover, Autoencoder is unsupervised, whereas Siamese network is supervised.\nWe propose a Joint Triplet Autoencoder Network (JTANet) by facilitating the\ntriplet learning in autoencoder framework. A joint supervised learning for\nSiamese network and unsupervised learning for Autoencoder is performed.\nMoreover, the Encoder network of Autoencoder is shared with Siamese network and\nreferred as the Siamcoder network. The features are extracted by using the\ntrained Siamcoder network for retrieval purpose. The experiments are performed\nover Histopathological Routine Colon Cancer dataset. We have observed the\npromising performance using the proposed JTANet model against the Autoencoder\nand Siamese models for colon cancer nuclei retrieval in histopathological\nimages.</p>\n", "tags": ["Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-6.6143083572387695, 2.6731786727905273], "cluster": 1}, {"key": "singh2022simultaneously", "year": "2023", "citations": "0", "title": "Simultaneously Learning Robust Audio Embeddings And Balanced Hash Codes For Query-by-example", "abstract": "<p>Audio fingerprinting systems must efficiently and robustly identify query\nsnippets in an extensive database. To this end, state-of-the-art systems use\ndeep learning to generate compact audio fingerprints. These systems deploy\nindexing methods, which quantize fingerprints to hash codes in an unsupervised\nmanner to expedite the search. However, these methods generate imbalanced hash\ncodes, leading to their suboptimal performance. Therefore, we propose a\nself-supervised learning framework to compute fingerprints and balanced hash\ncodes in an end-to-end manner to achieve both fast and accurate retrieval\nperformance. We model hash codes as a balanced clustering process, which we\nregard as an instance of the optimal transport problem. Experimental results\nindicate that the proposed approach improves retrieval efficiency while\npreserving high accuracy, particularly at high distortion levels, compared to\nthe competing methods. Moreover, our system is efficient and scalable in\ncomputational load and memory storage.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Efficiency", "ICASSP", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [14.922541618347168, 7.889492988586426], "cluster": 6}, {"key": "singh2023better", "year": "2024", "citations": "1", "title": "Better Generalization With Semantic Ids: A Case Study In Ranking For Recommendations", "abstract": "<p>Randomly-hashed item ids are used ubiquitously in recommendation models.\nHowever, the learned representations from random hashing prevents\ngeneralization across similar items, causing problems of learning unseen and\nlong-tail items, especially when item corpus is large, power-law distributed,\nand evolving dynamically. In this paper, we propose using content-derived\nfeatures as a replacement for random ids. We show that simply replacing ID\nfeatures with content-based embeddings can cause a drop in quality due to\nreduced memorization capability. To strike a good balance of memorization and\ngeneralization, we propose to use Semantic IDs \u2013 a compact discrete item\nrepresentation learned from frozen content embeddings using RQ-VAE that\ncaptures the hierarchy of concepts in items \u2013 as a replacement for random item\nids. Similar to content embeddings, the compactness of Semantic IDs poses a\nproblem of easy adaption in recommendation models. We propose novel methods for\nadapting Semantic IDs in industry-scale ranking models, through hashing\nsub-pieces of of the Semantic-ID sequences. In particular, we find that the\nSentencePiece model that is commonly used in LLM tokenization outperforms\nmanually crafted pieces such as N-grams. To the end, we evaluate our approaches\nin a real-world ranking model for YouTube recommendations. Our experiments\ndemonstrate that Semantic IDs can replace the direct use of video IDs by\nimproving the generalization ability on new and long-tail item slices without\nsacrificing overall model quality.</p>\n", "tags": ["Recommender-Systems", "Hashing-Methods"], "tsne_embedding": [6.312756538391113, 1.3789801597595215], "cluster": 6}, {"key": "sirnam2023preserving", "year": "2023", "citations": "1", "title": "Preserving Modality Structure Improves Multi-modal Learning", "abstract": "<p>Self-supervised learning on large-scale multi-modal datasets allows learning\nsemantically meaningful embeddings in a joint multi-modal representation space\nwithout relying on human annotations. These joint embeddings enable zero-shot\ncross-modal tasks like retrieval and classification. However, these methods\noften struggle to generalize well on out-of-domain data as they ignore the\nsemantic structure present in modality-specific embeddings. In this context, we\npropose a novel Semantic-Structure-Preserving Consistency approach to improve\ngeneralizability by preserving the modality-specific relationships in the joint\nembedding space. To capture modality-specific semantic relationships between\nsamples, we propose to learn multiple anchors and represent the multifaceted\nrelationship between samples with respect to their relationship with these\nanchors. To assign multiple anchors to each sample, we propose a novel\nMulti-Assignment Sinkhorn-Knopp algorithm. Our experimentation demonstrates\nthat our proposed approach learns semantically meaningful anchors in a\nself-supervised manner. Furthermore, our evaluation on MSR-VTT and YouCook2\ndatasets demonstrates that our proposed multi-anchor assignment based solution\nachieves state-of-the-art performance and generalizes to both inand\nout-of-domain datasets. Code: https://github.com/Swetha5/Multi_Sinkhorn_Knopp</p>\n", "tags": ["Self-Supervised", "ICCV", "Few-Shot-&-Zero-Shot", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [8.315173149108887, -1.4841197729110718], "cluster": 6}, {"key": "sivertsen2017fast", "year": "2017", "citations": "0", "title": "Fast Nearest Neighbor Preserving Embeddings", "abstract": "<p>We show an analog to the Fast Johnson-Lindenstrauss Transform for Nearest\nNeighbor Preserving Embeddings in \\(\u2113\u2082\\). These are sparse, randomized\nembeddings that preserve the (approximate) nearest neighbors. The\ndimensionality of the embedding space is bounded not by the size of the\nembedded set n, but by its doubling dimension {\\lambda}. For most large\nreal-world datasets this will mean a considerably lower-dimensional embedding\nspace than possible when preserving all distances. The resulting embeddings can\nbe used with existing approximate nearest neighbor data structures to yield\nspeed improvements.</p>\n", "tags": ["Datasets"], "tsne_embedding": [13.138314247131348, 42.646568298339844], "cluster": 4}, {"key": "sodani2021scalable", "year": "2021", "citations": "0", "title": "Scalable Reverse Image Search Engine For Nasaworldview", "abstract": "<p>Researchers often spend weeks sifting through decades of unlabeled satellite\nimagery(on NASA Worldview) in order to develop datasets on which they can start\nconducting research. We developed an interactive, scalable and fast image\nsimilarity search engine (which can take one or more images as the query image)\nthat automatically sifts through the unlabeled dataset reducing dataset\ngeneration time from weeks to minutes. In this work, we describe key components\nof the end to end pipeline. Our similarity search system was created to be able\nto identify similar images from a potentially petabyte scale database that are\nsimilar to an input image, and for this we had to break down each query image\ninto its features, which were generated by a classification layer stripped CNN\ntrained in a supervised manner. To store and search these features efficiently,\nwe had to make several scalability improvements. To improve the speed, reduce\nthe storage, and shrink memory requirements for embedding search, we add a\nfully connected layer to our CNN make all images into a 128 length vector\nbefore entering the classification layers. This helped us compress the size of\nour image features from 2048 (for ResNet, which was initially tried as our\nfeaturizer) to 128 for our new custom model. Additionally, we utilize existing\napproximate nearest neighbor search libraries to significantly speed up\nembedding search. Our system currently searches over our entire database of\nimages at 5 seconds per query on a single virtual machine in the cloud. In the\nfuture, we would like to incorporate a SimCLR based featurizing model which\ncould be trained without any labelling by a human (since the classification\naspect of the model is irrelevant to this use case).</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Similarity-Search", "Scalability", "Datasets", "Supervised"], "tsne_embedding": [-54.454010009765625, -4.735561370849609], "cluster": 0}, {"key": "sogi2024object", "year": "2024", "citations": "0", "title": "Object-aware Query Perturbation For Cross-modal Image-text Retrieval", "abstract": "<p>The pre-trained vision and language (V\\&amp;L) models have substantially improved\nthe performance of cross-modal image-text retrieval. In general, however, V\\&amp;L\nmodels have limited retrieval performance for small objects because of the\nrough alignment between words and the small objects in the image. In contrast,\nit is known that human cognition is object-centric, and we pay more attention\nto important objects, even if they are small. To bridge this gap between the\nhuman cognition and the V\\&amp;L model\u2019s capability, we propose a cross-modal\nimage-text retrieval framework based on ``object-aware query perturbation.\u2019\u2019\nThe proposed method generates a key feature subspace of the detected objects\nand perturbs the corresponding queries using this subspace to improve the\nobject awareness in the image. In our proposed method, object-aware cross-modal\nimage-text retrieval is possible while keeping the rich expressive power and\nretrieval performance of existing V\\&amp;L models without additional fine-tuning.\nComprehensive experiments on four public datasets show that our method\noutperforms conventional algorithms. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/query-perturbation.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-27.668949127197266, -24.398822784423828], "cluster": 5}, {"key": "somandepalli2020robust", "year": "2021", "citations": "6", "title": "Robust Character Labeling In Movie Videos: Data Resources And Self-supervised Feature Adaptation", "abstract": "<p>Robust face clustering is a vital step in enabling computational\nunderstanding of visual character portrayal in media. Face clustering for\nlong-form content is challenging because of variations in appearance and lack\nof supporting large-scale labeled data. Our work in this paper focuses on two\nkey aspects of this problem: the lack of domain-specific training or benchmark\ndatasets, and adapting face embeddings learned on web images to long-form\ncontent, specifically movies. First, we present a dataset of over 169,000 face\ntracks curated from 240 Hollywood movies with weak labels on whether a pair of\nface tracks belong to the same or a different character. We propose an offline\nalgorithm based on nearest-neighbor search in the embedding space to mine\nhard-examples from these tracks. We then investigate triplet-loss and multiview\ncorrelation-based methods for adapting face embeddings to hard-examples. Our\nexperimental results highlight the usefulness of weakly labeled data for\ndomain-specific feature adaptation. Overall, we find that multiview\ncorrelation-based adaptation yields more discriminative and robust face\nembeddings. Its performance on downstream face verification and clustering\ntasks is comparable to that of the state-of-the-art results in this domain. We\nalso present the SAIL-Movie Character Benchmark corpus developed to augment\nexisting benchmarks. It consists of racially diverse actors and provides\nface-quality labels for subsequent error analysis. We hope that the large-scale\ndatasets developed in this work can further advance automatic character\nlabeling in videos. All resources are available freely at\nhttps://sail.usc.edu/~ccmi/multiface.</p>\n", "tags": ["Self-Supervised", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-26.2050838470459, -5.339298248291016], "cluster": 1}, {"key": "song2013inter", "year": "2013", "citations": "619", "title": "Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources", "abstract": "<p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users\u2019 demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query\u2019s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods", "Large-Scale-Search", "Scalability", "Datasets"], "tsne_embedding": [12.954670906066895, -35.84012985229492], "cluster": 7}, {"key": "song2015deep", "year": "2016", "citations": "1656", "title": "Deep Metric Learning Via Lifted Structured Feature Embedding", "abstract": "<p>Learning the distance metric between pairs of examples is of great importance\nfor learning and visual recognition. With the remarkable success from the state\nof the art convolutional neural networks, recent works have shown promising\nresults on discriminatively training the networks to learn semantic feature\nembeddings where similar examples are mapped close to each other and dissimilar\nexamples are mapped farther apart. In this paper, we describe an algorithm for\ntaking full advantage of the training batches in the neural network training by\nlifting the vector of pairwise distances within the batch to the matrix of\npairwise distances. This step enables the algorithm to learn the state of the\nart feature embedding by optimizing a novel structured prediction objective on\nthe lifted problem. Additionally, we collected Online Products dataset: 120k\nimages of 23k classes of online products for metric learning. Our experiments\non the CUB-200-2011, CARS196, and Online Products datasets demonstrate\nsignificant improvement over existing deep feature embedding methods on all\nexperimented embedding sizes with the GoogLeNet network.</p>\n", "tags": ["CVPR", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-14.320112228393555, -26.481273651123047], "cluster": 3}, {"key": "song2015top", "year": "2015", "citations": "76", "title": "Top Rank Supervised Binary Coding For Visual Search", "abstract": "<p>In recent years, binary coding techniques are becoming\nincreasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been\ndemonstrated that supervised binary coding techniques that\nleverage supervised information can significantly enhance\nthe coding quality, and hence greatly benefit visual search\ntasks. Typically, a modern binary coding method seeks\nto learn a group of coding functions which compress data\nsamples into binary codes. However, few methods pursued\nthe coding functions such that the precision at the top of\na ranking list according to Hamming distances of the generated binary codes is optimized.\nIn this paper, we propose a novel supervised binary coding approach, namely\nTop Rank Supervised Binary Coding (Top-RSBC), which\nexplicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train\nthe disciplined coding functions, by which the mistakes at\nthe top of a Hamming-distance ranking list are penalized\nmore than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective\nwith a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online\nlearning algorithm to optimize the surrogate objective more\nefficiently. Empirical studies based upon three benchmark\nimage datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over\nthe state-of-the-arts.</p>\n", "tags": ["ICCV", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-8.316028594970703, 31.39313507080078], "cluster": 8}, {"key": "song2016deep", "year": "2017", "citations": "308", "title": "Deep Metric Learning Via Facility Location", "abstract": "<p>Learning the representation and the similarity metric in an end-to-end\nfashion with deep networks have demonstrated outstanding results for clustering\nand retrieval. However, these recent approaches still suffer from the\nperformance degradation stemming from the local metric training procedure which\nis unaware of the global structure of the embedding space.\n  We propose a global metric learning scheme for optimizing the deep metric\nembedding with the learnable clustering function and the clustering metric\n(NMI) in a novel structured prediction framework.\n  Our experiments on CUB200-2011, Cars196, and Stanford online products\ndatasets show state of the art performance both on the clustering and retrieval\ntasks measured in the NMI and Recall@K evaluation metrics.</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [41.20589065551758, -7.728880405426025], "cluster": 9}, {"key": "song2017binary", "year": "2018", "citations": "164", "title": "Binary Generative Adversarial Networks For Image Retrieval", "abstract": "<p>The most striking successes in image retrieval using deep hashing have mostly\ninvolved discriminative models, which require labels. In this paper, we use\nbinary generative adversarial networks (BGAN) to embed images to binary codes\nin an unsupervised way. By restricting the input noise variable of generative\nadversarial networks (GAN) to be binary and conditioned on the features of each\ninput image, BGAN can simultaneously learn a binary representation per image,\nand generate an image plausibly similar to the original one. In the proposed\nframework, we address two main problems: 1) how to directly generate binary\ncodes without relaxation? 2) how to equip the binary representation with the\nability of accurate image retrieval? We resolve these problems by proposing new\nsign-activation strategy and a loss function steering the learning process,\nwhich consists of new models for adversarial loss, a content loss, and a\nneighborhood structure loss. Experimental results on standard datasets\n(CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly\noutperforms existing hashing methods by up to 107% in terms of~mAP (See Table\ntab.res.map.comp) Our anonymous code is available at:\nhttps://github.com/htconquer/BGAN.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Robustness", "AAAI", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [-15.140897750854492, 18.399497985839844], "cluster": 8}, {"key": "song2017deep", "year": "2017", "citations": "308", "title": "Deep Discrete Hashing With Self-supervised Pairwise Labels", "abstract": "<p>Hashing methods have been widely used for applications of large-scale image\nretrieval and classification. Non-deep hashing methods using handcrafted\nfeatures have been significantly outperformed by deep hashing methods due to\ntheir better feature representation and end-to-end learning framework. However,\nthe most striking successes in deep hashing have mostly involved discriminative\nmodels, which require labels. In this paper, we propose a novel unsupervised\ndeep hashing method, named Deep Discrete Hashing (DDH), for large-scale image\nretrieval and classification. In the proposed framework, we address two main\nproblems: 1) how to directly learn discrete binary codes? 2) how to equip the\nbinary representation with the ability of accurate image retrieval and\nclassification in an unsupervised way? We resolve these problems by introducing\nan intermediate variable and a loss function steering the learning process,\nwhich is based on the neighborhood structure in the original space.\nExperimental results on standard datasets (CIFAR-10, NUS-WIDE, and Oxford-17)\ndemonstrate that our DDH significantly outperforms existing hashing methods by\nlarge margin in terms of~mAP for image retrieval and object recognition. Code\nis available at https://github.com/htconquer/ddh.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "CVPR", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-15.327345848083496, 18.570531845092773], "cluster": 8}, {"key": "song2018cross", "year": "2018", "citations": "7", "title": "Cross-modal Retrieval With Implicit Concept Association", "abstract": "<p>Traditional cross-modal retrieval assumes explicit association of concepts\nacross modalities, where there is no ambiguity in how the concepts are linked\nto each other, e.g., when we do the image search with a query \u201cdogs\u201d, we expect\nto see dog images. In this paper, we consider a different setting for\ncross-modal retrieval where data from different modalities are implicitly\nlinked via concepts that must be inferred by high-level reasoning; we call this\nsetting implicit concept association. To foster future research in this\nsetting, we present a new dataset containing 47K pairs of animated GIFs and\nsentences crawled from the web, in which the GIFs depict physical or emotional\nreactions to the scenarios described in the text (called \u201creaction GIFs\u201d). We\nreport on a user study showing that, despite the presence of implicit concept\nassociation, humans are able to identify video-sentence pairs with matching\nconcepts, suggesting the feasibility of our task. Furthermore, we propose a\nnovel visual-semantic embedding network based on multiple instance learning.\nUnlike traditional approaches, we compute multiple embeddings from each\nmodality, each representing different concepts, and measure their similarity by\nconsidering all possible combinations of visual-semantic embeddings in the\nframework of multiple instance learning. We evaluate our approach on two\nvideo-sentence datasets with explicit and implicit concept association and\nreport competitive results compared to existing approaches on cross-modal\nretrieval.</p>\n", "tags": ["Multimodal-Retrieval", "Tools-&-Libraries", "Image-Retrieval", "Datasets"], "tsne_embedding": [-34.81128692626953, -19.42544174194336], "cluster": 5}, {"key": "song2018self", "year": "2018", "citations": "165", "title": "Self-supervised Video Hashing With Hierarchical Binary Auto-encoder", "abstract": "<p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Compact-Codes", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Video-Retrieval", "Unsupervised"], "tsne_embedding": [-10.686958312988281, 0.3193299472332001], "cluster": 1}, {"key": "song2019deep", "year": "2019", "citations": "17", "title": "Deep Hashing Learning For Visual And Semantic Retrieval Of Remote Sensing Images", "abstract": "<p>Driven by the urgent demand for managing remote sensing big data, large-scale\nremote sensing image retrieval (RSIR) attracts increasing attention in the\nremote sensing field. In general, existing retrieval methods can be regarded as\nvisual-based retrieval approaches which search and return a set of similar\nimages from a database to a given query image. Although retrieval methods have\nachieved great success, there is still a question that needs to be responded\nto: Can we obtain the accurate semantic labels of the returned similar images\nto further help analyzing and processing imagery? Inspired by the above\nquestion, in this paper, we redefine the image retrieval problem as visual and\nsemantic retrieval of images. Specifically, we propose a novel deep hashing\nconvolutional neural network (DHCNN) to simultaneously retrieve the similar\nimages and classify their semantic labels in a unified framework. In more\ndetail, a convolutional neural network (CNN) is used to extract\nhigh-dimensional deep features. Then, a hash layer is perfectly inserted into\nthe network to transfer the deep features into compact hash codes. In addition,\na fully connected layer with a softmax function is performed on hash layer to\ngenerate class distribution. Finally, a loss function is elaborately designed\nto simultaneously consider the label loss of each image and similarity loss of\npairs of images. Experimental results on two remote sensing datasets\ndemonstrate that the proposed method achieves the state-of-art retrieval and\nclassification performance.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Tools-&-Libraries", "AAAI", "Image-Retrieval", "Hashing-Methods", "IJCAI", "Scalability"], "tsne_embedding": [-1.1922849416732788, 6.877899646759033], "cluster": 6}, {"key": "song2019polysemous", "year": "2019", "citations": "223", "title": "Polysemous Visual-semantic Embedding For Cross-modal Retrieval", "abstract": "<p>Visual-semantic embedding aims to find a shared latent space where related\nvisual and textual instances are close to each other. Most current methods\nlearn injective embedding functions that map an instance to a single point in\nthe shared space. Unfortunately, injective embedding cannot effectively handle\npolysemous instances with multiple possible meanings; at best, it would find an\naverage representation of different meanings. This hinders its use in\nreal-world scenarios where individual instances and their cross-modal\nassociations are often ambiguous. In this work, we introduce Polysemous\nInstance Embedding Networks (PIE-Nets) that compute multiple and diverse\nrepresentations of an instance by combining global context with locally-guided\nfeatures via multi-head self-attention and residual learning. To learn\nvisual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in\nthe multiple instance learning framework. Most existing work on cross-modal\nretrieval focuses on image-text data. Here, we also tackle a more challenging\ncase of video-text retrieval. To facilitate further research in video-text\nretrieval, we release a new dataset of 50K video-sentence pairs collected from\nsocial media, dubbed MRW (my reaction when). We demonstrate our approach on\nboth image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our\nnew MRW dataset.</p>\n", "tags": ["Text-Retrieval", "CVPR", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-17.464778900146484, -3.7685678005218506], "cluster": 1}, {"key": "song2020deep", "year": "2020", "citations": "0", "title": "Deep Robust Multilevel Semantic Cross-modal Hashing", "abstract": "<p>Hashing based cross-modal retrieval has recently made significant progress.\nBut straightforward embedding data from different modalities into a joint\nHamming space will inevitably produce false codes due to the intrinsic modality\ndiscrepancy and noises. We present a novel Robust Multilevel Semantic Hashing\n(RMSH) for more accurate cross-modal retrieval. It seeks to preserve\nfine-grained similarity among data with rich semantics, while explicitly\nrequire distances between dissimilar points to be larger than a specific value\nfor strong robustness. For this, we give an effective bound of this value based\non the information coding-theoretic analysis, and the above goals are embodied\ninto a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via\nfusing multiple hash codes to explore seldom-seen semantics, alleviating the\nsparsity problem of similarity information. Experiments on three benchmarks\nshow the validity of the derived bounds, and our method achieves\nstate-of-the-art performance.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Text-Retrieval", "Robustness", "Multimodal-Retrieval", "Evaluation"], "tsne_embedding": [8.54676342010498, 6.880814552307129], "cluster": 6}, {"key": "song2022asymmetric", "year": "2022", "citations": "44", "title": "Asymmetric Hash Code Learning For Remote Sensing Image Retrieval", "abstract": "<p>Remote sensing image retrieval (RSIR), aiming at searching for a set of\nsimilar items to a given query image, is a very important task in remote\nsensing applications. Deep hashing learning as the current mainstream method\nhas achieved satisfactory retrieval performance. On one hand, various deep\nneural networks are used to extract semantic features of remote sensing images.\nOn the other hand, the hashing techniques are subsequently adopted to map the\nhigh-dimensional deep features to the low-dimensional binary codes. This kind\nof methods attempts to learn one hash function for both the query and database\nsamples in a symmetric way. However, with the number of database samples\nincreasing, it is typically time-consuming to generate the hash codes of\nlarge-scale database images. In this paper, we propose a novel deep hashing\nmethod, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL\ngenerates the hash codes of query and database images in an asymmetric way. In\nmore detail, the hash codes of query images are obtained by binarizing the\noutput of the network, while the hash codes of database images are directly\nlearned by solving the designed objective function. In addition, we combine the\nsemantic information of each image and the similarity information of pairs of\nimages as supervised information to train a deep hashing network, which\nimproves the representation ability of deep features and hash codes. The\nexperimental results on three public datasets demonstrate that the proposed\nmethod outperforms symmetric methods in terms of retrieval accuracy and\nefficiency. The source code is available at\nhttps://github.com/weiweisong415/Demo AHCL for TGRS2022.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-0.7224392294883728, 7.018746852874756], "cluster": 6}, {"key": "song2022boosting", "year": "2023", "citations": "28", "title": "Boosting Vision Transformers For Image Retrieval", "abstract": "<p>Vision transformers have achieved remarkable progress in vision tasks such as\nimage classification and detection. However, in instance-level image retrieval,\ntransformers have not yet shown good performance compared to convolutional\nnetworks. We propose a number of improvements that make transformers outperform\nthe state of the art for the first time. (1) We show that a hybrid architecture\nis more effective than plain transformers, by a large margin. (2) We introduce\ntwo branches collecting global (classification token) and local (patch tokens)\ninformation, from which we form a global image representation. (3) In each\nbranch, we collect multi-layer features from the transformer encoder,\ncorresponding to skip connections across distant layers. (4) We enhance\nlocality of interactions at the deeper layers of the encoder, which is the\nrelative weakness of vision transformers. We train our model on all commonly\nused training sets and, for the first time, we make fair comparisons separately\nper training set. In all cases, we outperform previous models based on global\nrepresentation. Public code is available at\nhttps://github.com/dealicious-inc/DToP.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-37.40668487548828, 18.430137634277344], "cluster": 0}, {"key": "song2024biodeephash", "year": "2024", "citations": "0", "title": "Biodeephash: Mapping Biometrics Into A Stable Code", "abstract": "<p>With the wide application of biometrics, more and more attention has been\npaid to the security of biometric templates. However most of existing biometric\ntemplate protection (BTP) methods have some security problems, e.g. the problem\nthat protected templates leak part of the original biometric data (exists in\nCancelable Biometrics (CB)), the use of error-correcting codes (ECC) leads to\ndecodable attack, statistical attack (exists in Biometric Cryptosystems (BCS)),\nthe inability to achieve revocability (exists in methods using Neural Network\n(NN) to learn pre-defined templates), the inability to use cryptographic hash\nto guarantee strong security (exists in CB and methods using NN to learn latent\ntemplates). In this paper, we propose a framework called BioDeepHash based on\ndeep hashing and cryptographic hashing to address the above four problems,\nwhere different biometric data of the same user are mapped to a stable code\nusing deep hashing instead of predefined binary codes thus avoiding the use of\nECC. An application-specific binary string is employed to achieve revocability.\nThen cryptographic hashing is used to get the final protected template to\nensure strong security. Ultimately our framework achieves not storing any data\nthat would leak part of the original biometric data. We also conduct extensive\nexperiments on facial and iris datasets. Our method achieves an improvement of\n10.12\\(%\\) on the average Genuine Acceptance Rate (GAR) for iris data and\n3.12\\(%\\) for facial data compared to existing methods. In addition, BioDeepHash\nachieves extremely low False Acceptance Rate (FAR), i.e. 0\\(%\\) FAR on the iris\ndataset and the highest FAR on the facial dataset is only 0.0002\\(%\\).</p>\n", "tags": ["Hashing-Methods", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Neural-Hashing"], "tsne_embedding": [-11.52425479888916, 29.51785659790039], "cluster": 8}, {"key": "song2024comparing", "year": "2024", "citations": "0", "title": "Comparing Neighbors Together Makes It Easy: Jointly Comparing Multiple Candidates For Efficient And Effective Retrieval", "abstract": "<p>A common retrieve-and-rerank paradigm involves retrieving relevant candidates\nfrom a broad set using a fast bi-encoder (BE), followed by applying expensive\nbut accurate cross-encoders (CE) to a limited candidate set. However, relying\non this small subset is often susceptible to error propagation from the\nbi-encoders, which limits the overall performance. To address these issues, we\npropose the Comparing Multiple Candidates (CMC) framework. CMC compares a query\nand multiple embeddings of similar candidates (i.e., neighbors) through shallow\nself-attention layers, delivering rich representations contextualized to each\nother. Furthermore, CMC is scalable enough to handle multiple comparisons\nsimultaneously. For example, comparing ~10K candidates with CMC takes a similar\namount of time as comparing 16 candidates with CE. Experimental results on the\nZeSHEL dataset demonstrate that CMC, when plugged in between bi-encoders and\ncross-encoders as a seamless intermediate reranker (BE-CMC-CE), can effectively\nimprove recall@k (+4.8%-p, +3.5%-p for R@16, R@64) compared to using only\nbi-encoders (BE-CE), with negligible slowdown (&lt;7%). Additionally, to verify\nCMC\u2019s effectiveness as the final-stage reranker in improving top-1 accuracy, we\nconduct experiments on downstream tasks such as entity, passage, and dialogue\nranking. The results indicate that CMC is not only faster (11x) but also often\nmore effective than CE, with improved prediction accuracy in Wikipedia entity\nlinking (+0.7%-p) and DSTC7 dialogue ranking (+3.3%-p).</p>\n", "tags": ["EMNLP", "Tools-&-Libraries", "Datasets", "Re-Ranking", "Evaluation"], "tsne_embedding": [12.128971099853516, 15.368199348449707], "cluster": 6}, {"key": "song2025inter", "year": "2013", "citations": "619", "title": "Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources", "abstract": "<p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users\u2019 demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query\u2019s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods", "Large-Scale-Search", "Scalability", "Datasets"], "tsne_embedding": [12.95472526550293, -35.83974075317383], "cluster": 7}, {"key": "song2025self", "year": "2018", "citations": "165", "title": "Self-supervised Video Hashing With Hierarchical Binary Auto-encoder", "abstract": "<p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Compact-Codes", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Video-Retrieval", "Unsupervised"], "tsne_embedding": [-10.687193870544434, 0.3169157803058624], "cluster": 1}, {"key": "song2025top", "year": "2015", "citations": "76", "title": "Top Rank Supervised Binary Coding For Visual Search", "abstract": "<p>In recent years, binary coding techniques are becoming\nincreasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been\ndemonstrated that supervised binary coding techniques that\nleverage supervised information can significantly enhance\nthe coding quality, and hence greatly benefit visual search\ntasks. Typically, a modern binary coding method seeks\nto learn a group of coding functions which compress data\nsamples into binary codes. However, few methods pursued\nthe coding functions such that the precision at the top of\na ranking list according to Hamming distances of the generated binary codes is optimized.\nIn this paper, we propose a novel supervised binary coding approach, namely\nTop Rank Supervised Binary Coding (Top-RSBC), which\nexplicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train\nthe disciplined coding functions, by which the mistakes at\nthe top of a Hamming-distance ranking list are penalized\nmore than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective\nwith a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online\nlearning algorithm to optimize the surrogate objective more\nefficiently. Empirical studies based upon three benchmark\nimage datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over\nthe state-of-the-arts.</p>\n", "tags": ["ICCV", "Compact-Codes", "Efficiency", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-8.316028594970703, 31.39313316345215], "cluster": 8}, {"key": "soni2021rikonet", "year": "2021", "citations": "2", "title": "Rikonet: A Novel Anime Recommendation Engine", "abstract": "<p>Anime is quite well-received today, especially among the younger generations.\nWith many genres of available shows, more and more people are increasingly\ngetting attracted to this niche section of the entertainment industry. As anime\nhas recently garnered mainstream attention, we have insufficient information\nregarding users\u2019 penchant and watching habits. Therefore, it is an uphill task\nto build a recommendation engine for this relatively obscure entertainment\nmedium. In this attempt, we have built a novel hybrid recommendation system\nthat could act both as a recommendation system and as a means of exploring new\nanime genres and titles. We have analyzed the general trends in this field and\nthe users\u2019 watching habits for coming up with our efficacious solution. Our\nsolution employs deep autoencoders for the tasks of predicting ratings and\ngenerating embeddings. Following this, we formed clusters using the embeddings\nof the anime titles. These clusters form the search space for anime with\nsimilarities and are used to find anime similar to the ones liked and disliked\nby the user. This method, combined with the predicted ratings, forms the novel\nhybrid filter. In this article, we have demonstrated this idea and compared the\nperformance of our implemented model with the existing state-of-the-art\ntechniques.</p>\n", "tags": ["Recommender-Systems", "Evaluation"], "tsne_embedding": [24.162790298461914, -30.493038177490234], "cluster": 7}, {"key": "spaldingjamieson2025scalable", "year": "2025", "citations": "0", "title": "Scalable K-means Clustering For Large K Via Seeded Approximate Nearest-neighbor Search", "abstract": "<p>For very large values of \\(k\\), we consider methods for fast \\(k\\)-means\nclustering of massive datasets with \\(10^7\\sim10^9\\) points in high-dimensions\n(\\(d\\geq100\\)). All current practical methods for this problem have runtimes at\nleast \\(\u03a9(k^2)\\). We find that initialization routines are not a bottleneck\nfor this case. Instead, it is critical to improve the speed of Lloyd\u2019s\nlocal-search algorithm, particularly the step that reassigns points to their\nclosest center. Attempting to improve this step naturally leads us to leverage\napproximate nearest-neighbor search methods, although this alone is not enough\nto be practical. Instead, we propose a family of problems we call \u201cSeeded\nApproximate Nearest-Neighbor Search\u201d, for which we propose \u201cSeeded\nSearch-Graph\u201d methods as a solution.</p>\n", "tags": ["Datasets"], "tsne_embedding": [40.00383377075195, 25.8228816986084], "cluster": 2}, {"key": "spring2016scalable", "year": "2017", "citations": "108", "title": "Scalable And Sustainable Deep Learning Via Randomized Hashing", "abstract": "<p>Current deep learning architectures are growing larger in order to learn from\ncomplex datasets. These architectures require giant matrix multiplication\noperations to train millions of parameters. Conversely, there is another\ngrowing trend to bring deep learning to low-power, embedded devices. The matrix\noperations, associated with both training and testing of deep networks, are\nvery expensive from a computational and energy standpoint. We present a novel\nhashing based technique to drastically reduce the amount of computation needed\nto train and test deep networks. Our approach combines recent ideas from\nadaptive dropouts and randomized hashing for maximum inner product search to\nselect the nodes with the highest activation efficiently. Our new algorithm for\ndeep learning reduces the overall computational cost of forward and\nback-propagation by operating on significantly fewer (sparse) nodes. As a\nconsequence, our algorithm uses only 5% of the total multiplications, while\nkeeping on average within 1% of the accuracy of the original model. A unique\nproperty of the proposed hashing based back-propagation is that the updates are\nalways sparse. Due to the sparse gradient updates, our algorithm is ideally\nsuited for asynchronous and parallel training leading to near linear speedup\nwith increasing number of cores. We demonstrate the scalability and\nsustainability (energy efficiency) of our proposed algorithm via rigorous\nexperimental evaluations on several real datasets.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Datasets", "KDD"], "tsne_embedding": [30.55196189880371, 19.400800704956055], "cluster": 2}, {"key": "srinivas2018merging", "year": "2018", "citations": "7", "title": "Merging Datasets Through Deep Learning", "abstract": "<p>Merging datasets is a key operation for data analytics. A frequent\nrequirement for merging is joining across columns that have different surface\nforms for the same entity (e.g., the name of a person might be represented as\n\u201cDouglas Adams\u201d or \u201cAdams, Douglas\u201d). Similarly, ontology alignment can require\nrecognizing distinct surface forms of the same entity, especially when\nontologies are independently developed. However, data management systems are\ncurrently limited to performing merges based on string equality, or at best\nusing string similarity. We propose an approach to performing merges based on\ndeep learning models. Our approach depends on (a) creating a deep learning\nmodel that maps surface forms of an entity into a set of vectors such that\nalternate forms for the same entity are closest in vector space, (b) indexing\nthese vectors using a nearest neighbors algorithm to find the forms that can be\npotentially joined together. To build these models, we had to adapt techniques\nfrom metric learning due to the characteristics of the data; specifically we\ndescribe novel sample selection techniques and loss functions that work for\nthis problem. To evaluate our approach, we used Wikidata as ground truth and\nbuilt models from datasets with approximately 1.1M people\u2019s names (200K\nidentities) and 130K company names (70K identities). We developed models that\nallow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the\nmodels available for aligning people or companies across multiple datasets.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-1.7861416339874268, -13.991521835327148], "cluster": 1}, {"key": "srinivasan2022curriculum", "year": "2023", "citations": "3", "title": "Curriculum Learning For Data-efficient Vision-language Alignment", "abstract": "<p>Aligning image and text encoders from scratch using contrastive learning\nrequires large amounts of paired image-text data. We alleviate this need by\naligning individually pre-trained language and vision representation models\nusing a much smaller amount of paired data, augmented with a curriculum\nlearning algorithm to learn fine-grained vision-language alignments. TOnICS\n(Training with Ontology-Informed Contrastive Sampling) initially samples\nminibatches whose image-text pairs contain a wide variety of objects to learn\nobject-level alignment, and progressively samples minibatches where all\nimage-text pairs contain the same object to learn finer-grained contextual\nalignment. Aligning pre-trained BERT and VinVL models to each other using\nTOnICS outperforms CLIP on downstream zero-shot image retrieval while using\nless than 1% as much training data.</p>\n", "tags": ["Self-Supervised", "CVPR", "Few-Shot-&-Zero-Shot", "Image-Retrieval"], "tsne_embedding": [-24.885353088378906, -20.916501998901367], "cluster": 5}, {"key": "srivastava20163d", "year": "2016", "citations": "8", "title": "3D Binary Signatures", "abstract": "<p>In this paper, we propose a novel binary descriptor for 3D point clouds. The\nproposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the\nmatching efficiency of the binary descriptors for 2D images. 3DBS describes\nkeypoints from point clouds with a binary vector resulting in extremely fast\nmatching. The method uses keypoints from standard keypoint detectors. The\ndescriptor is built by constructing a Local Reference Frame and aligning a\nlocal surface patch accordingly. The local surface patch constitutes of\nidentifying nearest neighbours based upon an angular constraint among them. The\npoints are ordered with respect to the distance from the keypoints. The normals\nof the ordered pairs of these keypoints are projected on the axes and the\nrelative magnitude is used to assign a binary digit. The vector thus\nconstituted is used as a signature for representing the keypoints. The matching\nis done by using hamming distance. We show that 3DBS outperforms state of the\nart descriptors on various evaluation metrics.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [6.551004886627197, 47.92612075805664], "cluster": 4}, {"key": "srivastava20173d", "year": "2016", "citations": "8", "title": "3D Binary Signatures", "abstract": "<p>In this paper, we propose a novel binary descriptor for 3D point clouds. The\nproposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the\nmatching efficiency of the binary descriptors for 2D images. 3DBS describes\nkeypoints from point clouds with a binary vector resulting in extremely fast\nmatching. The method uses keypoints from standard keypoint detectors. The\ndescriptor is built by constructing a Local Reference Frame and aligning a\nlocal surface patch accordingly. The local surface patch constitutes of\nidentifying nearest neighbours based upon an angular constraint among them. The\npoints are ordered with respect to the distance from the keypoints. The normals\nof the ordered pairs of these keypoints are projected on the axes and the\nrelative magnitude is used to assign a binary digit. The vector thus\nconstituted is used as a signature for representing the keypoints. The matching\nis done by using hamming distance. We show that 3DBS outperforms state of the\nart descriptors on various evaluation metrics.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [6.551004886627197, 47.92612075805664], "cluster": 4}, {"key": "srivastava2023retailklip", "year": "2024", "citations": "1", "title": "Retailklip : Finetuning Openclip Backbone Using Metric Learning On A Single GPU For Zero-shot Retail Product Image Classification", "abstract": "<p>Retail product or packaged grocery goods images need to classified in various\ncomputer vision applications like self checkout stores, supply chain automation\nand retail execution evaluation. Previous works explore ways to finetune deep\nmodels for this purpose. But because of the fact that finetuning a large model\nor even linear layer for a pretrained backbone requires to run at least a few\nepochs of gradient descent for every new retail product added in classification\nrange, frequent retrainings are needed in a real world scenario. In this work,\nwe propose finetuning the vision encoder of a CLIP model in a way that its\nembeddings can be easily used for nearest neighbor based classification, while\nalso getting accuracy close to or exceeding full finetuning. A nearest neighbor\nbased classifier needs no incremental training for new products, thus saving\nresources and wait time.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning"], "tsne_embedding": [-12.185856819152832, -47.28347396850586], "cluster": 3}, {"key": "stanley2020sir", "year": "2020", "citations": "0", "title": "SIR: Similar Image Retrieval For Product Search In E-commerce", "abstract": "<p>We present a similar image retrieval (SIR) platform that is used to quickly\ndiscover visually similar products in a catalog of millions. Given the size,\ndiversity, and dynamism of our catalog, product search poses many challenges.\nIt can be addressed by building supervised models to tagging product images\nwith labels representing themes and later retrieving them by labels. This\napproach suffices for common and perennial themes like \u201cwhite shirt\u201d or\n\u201clifestyle image of TV\u201d. It does not work for new themes such as\n\u201ce-cigarettes\u201d, hard-to-define ones such as \u201cimage with a promotional badge\u201d,\nor the ones with short relevance span such as \u201cHalloween costumes\u201d. SIR is\nideal for such cases because it allows us to search by an example, not a\npre-defined theme. We describe the steps - embedding computation, encoding, and\nindexing - that power the approximate nearest neighbor search back-end. We also\nhighlight two applications of SIR. The first one is related to the detection of\nproducts with various types of potentially objectionable themes. This\napplication is run with a sense of urgency, hence the typical time frame to\ntrain and bootstrap a model is not permitted. Also, these themes are often\nshort-lived based on current trends, hence spending resources to build a\nlasting model is not justified. The second application is a variant item\ndetection system where SIR helps discover visual variants that are hard to find\nthrough text search. We analyze the performance of SIR in the context of these\napplications.</p>\n", "tags": ["Supervised", "Image-Retrieval", "Evaluation", "Text-Retrieval"], "tsne_embedding": [-9.810429573059082, -46.34547424316406], "cluster": 3}, {"key": "staszewski2020new", "year": "2021", "citations": "22", "title": "A New Approach To Descriptors Generation For Image Retrieval By Analyzing Activations Of Deep Neural Network Layers", "abstract": "<p>In this paper, we consider the problem of descriptors construction for the\ntask of content-based image retrieval using deep neural networks. The idea of\nneural codes, based on fully connected layers activations, is extended by\nincorporating the information contained in convolutional layers. It is known\nthat the total number of neurons in the convolutional part of the network is\nlarge and the majority of them have little influence on the final\nclassification decision. Therefore, in the paper we propose a novel algorithm\nthat allows us to extract the most significant neuron activations and utilize\nthis information to construct effective descriptors. The descriptors consisting\nof values taken from both the fully connected and convolutional layers\nperfectly represent the whole image content. The images retrieved using these\ndescriptors match semantically very well to the query image, and also they are\nsimilar in other secondary image characteristics, like background, textures or\ncolor distribution. These features of the proposed descriptors are verified\nexperimentally based on the IMAGENET1M dataset using the VGG16 neural network.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [-14.622568130493164, 2.070192337036133], "cluster": 1}, {"key": "stein2021self", "year": "2021", "citations": "13", "title": "Self-supervised Similarity Search For Large Scientific Datasets", "abstract": "<p>We present the use of self-supervised learning to explore and exploit large\nunlabeled datasets. Focusing on 42 million galaxy images from the latest data\nrelease of the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging\nSurveys, we first train a self-supervised model to distill low-dimensional\nrepresentations that are robust to symmetries, uncertainties, and noise in each\nimage. We then use the representations to construct and publicly release an\ninteractive semantic similarity search tool. We demonstrate how our tool can be\nused to rapidly discover rare objects given only a single example, increase the\nspeed of crowd-sourcing campaigns, and construct and improve training sets for\nsupervised applications. While we focus on images from sky surveys, the\ntechnique is straightforward to apply to any scientific dataset of any\ndimensionality. The similarity search web app can be found at\nhttps://github.com/georgestein/galaxy_search</p>\n", "tags": ["Supervised", "Self-Supervised", "Similarity-Search", "Datasets"], "tsne_embedding": [-17.044635772705078, -19.57605743408203], "cluster": 1}, {"key": "steorts2018probabilistic", "year": "2018", "citations": "1", "title": "Probabilistic Blocking With An Application To The Syrian Conflict", "abstract": "<p>Entity resolution seeks to merge databases as to remove duplicate entries\nwhere unique identifiers are typically unknown. We review modern blocking\napproaches for entity resolution, focusing on those based upon locality\nsensitive hashing (LSH). First, we introduce \\(k\\)-means locality sensitive\nhashing (KLSH), which is based upon the information retrieval literature and\nclusters similar records into blocks using a vector-space representation and\nprojections. Second, we introduce a subquadratic variant of LSH to the\nliterature, known as Densified One Permutation Hashing (DOPH). Third, we\npropose a weighted variant of DOPH. We illustrate each method on an application\nto a subset of the ongoing Syrian conflict, giving a discussion of each method.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [-4.252993106842041, -9.572519302368164], "cluster": 1}, {"key": "studer2019comprehensive", "year": "2019", "citations": "66", "title": "A Comprehensive Study Of Imagenet Pre-training For Historical Document Image Analysis", "abstract": "<p>Automatic analysis of scanned historical documents comprises a wide range of\nimage analysis tasks, which are often challenging for machine learning due to a\nlack of human-annotated learning samples. With the advent of deep neural\nnetworks, a promising way to cope with the lack of training data is to\npre-train models on images from a different domain and then fine-tune them on\nhistorical documents. In the current research, a typical example of such\ncross-domain transfer learning is the use of neural networks that have been\npre-trained on the ImageNet database for object recognition. It remains a\nmostly open question whether or not this pre-training helps to analyse\nhistorical documents, which have fundamentally different image properties when\ncompared with ImageNet. In this paper, we present a comprehensive empirical\nsurvey on the effect of ImageNet pre-training for diverse historical document\nanalysis tasks, including character recognition, style classification,\nmanuscript dating, semantic segmentation, and content-based retrieval. While we\nobtain mixed results for semantic segmentation at pixel-level, we observe a\nclear trend across different network architectures that ImageNet pre-training\nhas a positive effect on classification as well as content-based retrieval.</p>\n", "tags": ["Survey-Paper"], "tsne_embedding": [-27.963491439819336, -15.647127151489258], "cluster": 5}, {"key": "sturua2024jina", "year": "2024", "citations": "0", "title": "Jina-embeddings-v3: Multilingual Embeddings With Task Lora", "abstract": "<p>We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Evaluation on the MTEB benchmark\nshows that jina-embeddings-v3 outperforms the latest proprietary embeddings\nfrom OpenAI and Cohere on English tasks, while achieving superior performance\ncompared to multilingual-e5-large-instruct across all multilingual tasks. With\na default output dimension of 1024, users can flexibly reduce the embedding\ndimensions to as low as 32 without compromising performance, enabled by\nMatryoshka Representation Learning.</p>\n", "tags": ["Evaluation", "Text-Retrieval"], "tsne_embedding": [-2.7071776390075684, -40.021121978759766], "cluster": 3}, {"key": "stylianou2019visualizing", "year": "2019", "citations": "46", "title": "Visualizing Deep Similarity Networks", "abstract": "<p>For convolutional neural network models that optimize an image embedding, we\npropose a method to highlight the regions of images that contribute most to\npairwise similarity. This work is a corollary to the visualization tools\ndeveloped for classification networks, but applicable to the problem domains\nbetter suited to similarity learning. The visualization shows how similarity\nnetworks that are fine-tuned learn to focus on different features. We also\ngeneralize our approach to embedding networks that use different pooling\nstrategies and provide a simple mechanism to support image similarity searches\non objects or sub-regions in the query image.</p>\n", "tags": ["Similarity-Search"], "tsne_embedding": [-15.78155517578125, -8.30516529083252], "cluster": 1}, {"key": "su2018greedy", "year": "2018", "citations": "115", "title": "Greedy Hash: Towards Fast Optimization For Accurate Hash Coding In CNN", "abstract": "<p>To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Scalability", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [21.753923416137695, 6.0429792404174805], "cluster": 6}, {"key": "su2019deep", "year": "2019", "citations": "255", "title": "Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval", "abstract": "<p><img src=\"https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true\" alt=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" title=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" /></p>\n\n<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [6.910034656524658, 3.7741169929504395], "cluster": 6}, {"key": "su2020where", "year": "2020", "citations": "32", "title": "Where To Look And How To Describe: Fashion Image Retrieval With An Attentional Heterogeneous Bilinear Network", "abstract": "<p>Fashion products typically feature in compositions of a variety of styles at\ndifferent clothing parts. In order to distinguish images of different fashion\nproducts, we need to extract both appearance (i.e., \u201chow to describe\u201d) and\nlocalization (i.e.,\u201dwhere to look\u201d) information, and their interactions. To\nthis end, we propose a biologically inspired framework for image-based fashion\nproduct retrieval, which mimics the hypothesized twostream visual processing\nsystem of human brain. The proposed attentional heterogeneous bilinear network\n(AHBN) consists of two branches: a deep CNN branch to extract fine-grained\nappearance attributes and a fully convolutional branch to extract landmark\nlocalization information. A joint channel-wise attention mechanism is further\napplied to the extracted heterogeneous features to focus on important channels,\nfollowed by a compact bilinear pooling layer to model the interaction of the\ntwo streams. Our proposed framework achieves satisfactory performance on three\nimage-based fashion product retrieval benchmarks.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-21.62714385986328, -50.24320602416992], "cluster": 3}, {"key": "su2021hard", "year": "2021", "citations": "0", "title": "Hard Example Guided Hashing For Image Retrieval", "abstract": "<p>Compared with the traditional hashing methods, deep hashing methods generate\nhash codes with rich semantic information and greatly improves the performances\nin the image retrieval field. However, it is unsatisfied for current deep\nhashing methods to predict the similarity of hard examples. It exists two main\nfactors affecting the ability of learning hard examples, which are weak key\nfeatures extraction and the shortage of hard examples. In this paper, we give a\nnovel end-to-end model to extract the key feature from hard examples and obtain\nhash code with the accurate semantic information. In addition, we redesign a\nhard pair-wise loss function to assess the hard degree and update penalty\nweights of examples. It effectively alleviates the shortage problem in hard\nexamples. Experimental results on CIFAR-10 and NUS-WIDE demonstrate that our\nmodel outperformances the mainstream hashing-based image retrieval methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval"], "tsne_embedding": [-14.358550071716309, 9.773937225341797], "cluster": 8}, {"key": "su2023beyond", "year": "2023", "citations": "2", "title": "Beyond Two-tower Matching: Learning Sparse Retrievable Cross-interactions For Recommendation", "abstract": "<p>Two-tower models are a prevalent matching framework for recommendation, which\nhave been widely deployed in industrial applications. The success of two-tower\nmatching attributes to its efficiency in retrieval among a large number of\nitems, since the item tower can be precomputed and used for fast Approximate\nNearest Neighbor (ANN) search. However, it suffers two main challenges,\nincluding limited feature interaction capability and reduced accuracy in online\nserving. Existing approaches attempt to design novel late interactions instead\nof dot products, but they still fail to support complex feature interactions or\nlose retrieval efficiency. To address these challenges, we propose a new\nmatching paradigm named SparCode, which supports not only sophisticated feature\ninteractions but also efficient retrieval. Specifically, SparCode introduces an\nall-to-all interaction module to model fine-grained query-item interactions.\nBesides, we design a discrete code-based sparse inverted index jointly trained\nwith the model to achieve effective and efficient model inference. Extensive\nexperiments have been conducted on open benchmark datasets to demonstrate the\nsuperiority of our framework. The results show that SparCode significantly\nimproves the accuracy of candidate item matching while retaining the same level\nof retrieval efficiency with two-tower models. Our source code will be\navailable at MindSpore/models.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Similarity-Search", "SIGIR", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [25.03207015991211, -22.15917205810547], "cluster": 7}, {"key": "su2025deep", "year": "2019", "citations": "255", "title": "Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval", "abstract": "<p><img src=\"https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true\" alt=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" title=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" /></p>\n\n<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>\n", "tags": ["ICCV", "Hashing-Methods", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [6.9101409912109375, 3.773885488510132], "cluster": 6}, {"key": "su2025greedy", "year": "2018", "citations": "115", "title": "Greedy Hash: Towards Fast Optimization For Accurate Hash Coding In CNN", "abstract": "<p>To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Scalability", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [21.753782272338867, 6.043776512145996], "cluster": 6}, {"key": "subramanya2019diskann", "year": "2019", "citations": "36", "title": "Diskann: Fast Accurate Billion-point Nearest Neighbor Search On A Single Node", "abstract": "<p>Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index, store, and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom, we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN serves &gt; 5000 queries a second with &lt; 3ms mean latency and 95%+ 1-recall@1 on a 16 core machine, where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS and IVFOADC+G+P plateau at around 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can index and serve 5 \u2212 10x more points per node compared to state-of-the-art graph- based methods such as HNSW and NSG. Finally, as part of our overall DiskANN system, we introduce Vamana, a new graph-based ANNS index that is more versatile than the graph indices even for in-memory indices.</p>\n", "tags": ["Graph-Based-Ann", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [36.67066955566406, 21.24875259399414], "cluster": 2}, {"key": "subramanya2025diskann", "year": "2019", "citations": "36", "title": "Diskann: Fast Accurate Billion-point Nearest Neighbor Search On A Single Node", "abstract": "<p>Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index, store, and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom, we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN serves &gt; 5000 queries a second with &lt; 3ms mean latency and 95%+ 1-recall@1 on a 16 core machine, where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS and IVFOADC+G+P plateau at around 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can index and serve 5 \u2212 10x more points per node compared to state-of-the-art graph- based methods such as HNSW and NSG. Finally, as part of our overall DiskANN system, we introduce Vamana, a new graph-based ANNS index that is more versatile than the graph indices even for in-memory indices.</p>\n", "tags": ["Graph-Based-Ann", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [36.67066955566406, 21.24875259399414], "cluster": 2}, {"key": "suma2024ames", "year": "2024", "citations": "0", "title": "AMES: Asymmetric And Memory-efficient Similarity Estimation For Instance-level Retrieval", "abstract": "<p>This work investigates the problem of instance-level image retrieval\nre-ranking with the constraint of memory efficiency, ultimately aiming to limit\nmemory usage to 1KB per image. Departing from the prevalent focus on\nperformance enhancements, this work prioritizes the crucial trade-off between\nperformance and memory requirements. The proposed model uses a\ntransformer-based architecture designed to estimate image-to-image similarity\nby capturing interactions within and across images based on their local\ndescriptors. A distinctive property of the model is the capability for\nasymmetric similarity estimation. Database images are represented with a\nsmaller number of descriptors compared to query images, enabling performance\nimprovements without increasing memory consumption. To ensure adaptability\nacross different applications, a universal model is introduced that adjusts to\na varying number of local descriptors during the testing phase. Results on\nstandard benchmarks demonstrate the superiority of our approach over both\nhand-crafted and learned models. In particular, compared with current\nstate-of-the-art methods that overlook their memory footprint, our approach not\nonly attains superior performance but does so with a significantly reduced\nmemory footprint. The code and pretrained models are publicly available at:\nhttps://github.com/pavelsuma/ames</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Memory-Efficiency", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-35.20576858520508, 2.7759978771209717], "cluster": 0}, {"key": "sumbul2020deep", "year": "2021", "citations": "24", "title": "Deep Learning For Image Search And Retrieval In Large Remote Sensing Archives", "abstract": "<p>This chapter presents recent advances in content based image search and\nretrieval (CBIR) systems in remote sensing (RS) for fast and accurate\ninformation discovery from massive data archives. Initially, we analyze the\nlimitations of the traditional CBIR systems that rely on the hand-crafted RS\nimage descriptors. Then, we focus our attention on the advances in RS CBIR\nsystems for which deep learning (DL) models are at the forefront. In\nparticular, we present the theoretical properties of the most recent DL based\nCBIR systems for the characterization of the complex semantic content of RS\nimages. After discussing their strengths and limitations, we present the deep\nhashing based CBIR systems that have high time-efficient search capability\nwithin huge data archives. Finally, the most promising research directions in\nRS CBIR are discussed.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-10.342687606811523, -6.589277267456055], "cluster": 1}, {"key": "sumbul2022novel", "year": "2022", "citations": "1", "title": "A Novel Framework To Jointly Compress And Index Remote Sensing Images For Efficient Content-based Retrieval", "abstract": "<p>Remote sensing (RS) images are usually stored in compressed format to reduce\nthe storage size of the archives. Thus, existing content-based image retrieval\n(CBIR) systems in RS require decoding images before applying CBIR (which is\ncomputationally demanding in the case of large-scale CBIR problems). To address\nthis problem, in this paper, we present a joint framework that simultaneously\nlearns RS image compression and indexing. Thus, it eliminates the need for\ndecoding RS images before applying CBIR. The proposed framework is made up of\ntwo modules. The first module compresses RS images based on an auto-encoder\narchitecture. The second module produces hash codes with a high discrimination\ncapability by employing soft pairwise, bit-balancing and classification loss\nfunctions. We also introduce a two stage learning strategy with gradient\nmanipulation techniques to obtain image representations that are compatible\nwith both RS image indexing and compression. Experimental results show the\nefficacy of the proposed framework when compared to widely used approaches in\nRS. The code of the proposed framework is available at\nhttps://git.tu-berlin.de/rsim/RS-JCIF.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "Hashing-Methods", "Scalability"], "tsne_embedding": [-7.769588947296143, 13.960628509521484], "cluster": 8}, {"key": "sun2019geocapsnet", "year": "2019", "citations": "7", "title": "Geocapsnet: Aerial To Ground View Image Geo-localization Using Capsule Network", "abstract": "<p>The task of cross-view image geo-localization aims to determine the\ngeo-location (GPS coordinates) of a query ground-view image by matching it with\nthe GPS-tagged aerial (satellite) images in a reference dataset. Due to the\ndramatic changes of viewpoint, matching the cross-view images is challenging.\nIn this paper, we propose the GeoCapsNet based on the capsule network for\nground-to-aerial image geo-localization. The network first extracts features\nfrom both ground-view and aerial images via standard convolution layers and the\ncapsule layers further encode the features to model the spatial feature\nhierarchies and enhance the representation power. Moreover, we introduce a\nsimple and effective weighted soft-margin triplet loss with online batch hard\nsample mining, which can greatly improve image retrieval accuracy. Experimental\nresults show that our GeoCapsNet significantly outperforms the state-of-the-art\napproaches on two benchmark datasets.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-0.8719832897186279, 39.909942626953125], "cluster": 4}, {"key": "sun2019part", "year": "2018", "citations": "10", "title": "Part-based Multi-stream Model For Vehicle Searching", "abstract": "<p>Due to the enormous requirement in public security and intelligent\ntransportation system, searching an identical vehicle has become more and more\nimportant. Current studies usually treat vehicle as an integral object and then\ntrain a distance metric to measure the similarity among vehicles. However,\nthese raw images may be exactly similar to ones with different identification\nand include some pixels in background that may disturb the distance metric\nlearning. In this paper, we propose a novel and useful method to segment an\noriginal vehicle image into several discriminative foreground parts, and these\nparts consist of some fine grained regions that are named discriminative\npatches. After that, these parts combined with the raw image are fed into the\nproposed deep learning network. We can easily measure the similarity of two\nvehicle images by computing the Euclidean distance of the features from FC\nlayer. Two main contributions of this paper are as follows. Firstly, a method\nis proposed to estimate if a patch in a raw vehicle image is discriminative or\nnot. Secondly, a new Part-based Multi-Stream Model (PMSM) is designed and\noptimized for vehicle retrieval and re-identification tasks. We evaluate the\nproposed method on the VehicleID dataset, and the experimental results show\nthat our method can outperform the baseline.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-19.750450134277344, 6.170496463775635], "cluster": 1}, {"key": "sun2019supervised", "year": "2019", "citations": "48", "title": "Supervised Hierarchical Cross-modal Hashing", "abstract": "<p>Recently, due to the unprecedented growth of multimedia data,\ncross-modal hashing has gained increasing attention for the\nefficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but\noverlook the correlations among labels. Indeed, in many real-world\nscenarios, like the online fashion domain, instances (items) are\nlabeled with a set of categories correlated by certain hierarchy. In\nthis paper, we propose a new end-to-end solution for supervised\ncross-modal hashing, named HiCHNet, which explicitly exploits the\nhierarchical labels of instances. In particular, by the pre-established\nlabel hierarchy, we comprehensively characterize each modality\nof the instance with a set of layer-wise hash representations. In\nessence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also\nretain the hierarchical discriminative capabilities. Due to the lack\nof benchmark datasets, apart from adapting the existing dataset\nFashionVC from fashion domain, we create a dataset from the\nonline fashion platform Ssense consisting of 15, 696 image-text\npairs labeled by 32 hierarchical categories. Extensive experiments\non two real-world datasets demonstrate the superiority of our model\nover the state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "SIGIR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [17.02103614807129, -38.15203094482422], "cluster": 7}, {"key": "sun2020benchmarking", "year": "2020", "citations": "79", "title": "A Benchmarking Study Of Embedding-based Entity Alignment For Knowledge Graphs", "abstract": "<p>Entity alignment seeks to find entities in different knowledge graphs (KGs)\nthat refer to the same real-world object. Recent advancement in KG embedding\nimpels the advent of embedding-based entity alignment, which encodes entities\nin a continuous embedding space and measures entity similarities based on the\nlearned embeddings. In this paper, we conduct a comprehensive experimental\nstudy of this emerging field. We survey 23 recent embedding-based entity\nalignment approaches and categorize them based on their techniques and\ncharacteristics. We also propose a new KG sampling algorithm, with which we\ngenerate a set of dedicated benchmark datasets with various heterogeneity and\ndistributions for a realistic evaluation. We develop an open-source library\nincluding 12 representative embedding-based entity alignment approaches, and\nextensively evaluate these approaches, to understand their strengths and\nlimitations. Additionally, for several directions that have not been explored\nin current approaches, we perform exploratory experiments and report our\npreliminary findings for future studies. The benchmark datasets, open-source\nlibrary and experimental results are all accessible online and will be duly\nmaintained.</p>\n", "tags": ["Survey-Paper", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [6.552689075469971, -34.529544830322266], "cluster": 3}, {"key": "sun2020circle", "year": "2020", "citations": "787", "title": "Circle Loss: A Unified Perspective Of Pair Similarity Optimization", "abstract": "<p>This paper provides a pair similarity optimization viewpoint on deep feature\nlearning, aiming to maximize the within-class similarity \\(s_p\\) and minimize the\nbetween-class similarity \\(s_n\\). We find a majority of loss functions, including\nthe triplet loss and the softmax plus cross-entropy loss, embed \\(s_n\\) and \\(s_p\\)\ninto similarity pairs and seek to reduce \\((s_n-s_p)\\). Such an optimization\nmanner is inflexible, because the penalty strength on every single similarity\nscore is restricted to be equal. Our intuition is that if a similarity score\ndeviates far from the optimum, it should be emphasized. To this end, we simply\nre-weight each similarity to highlight the less-optimized similarity scores. It\nresults in a Circle loss, which is named due to its circular decision boundary.\nThe Circle loss has a unified formula for two elemental deep feature learning\napproaches, i.e. learning with class-level labels and pair-wise labels.\nAnalytically, we show that the Circle loss offers a more flexible optimization\napproach towards a more definite convergence target, compared with the loss\nfunctions optimizing \\((s_n-s_p)\\). Experimentally, we demonstrate the\nsuperiority of the Circle loss on a variety of deep feature learning tasks. On\nface recognition, person re-identification, as well as several fine-grained\nimage retrieval datasets, the achieved performance is on par with the state of\nthe art.</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [28.71467399597168, 49.503719329833984], "cluster": 4}, {"key": "sun2020multi", "year": "2019", "citations": "128", "title": "Multi-graph Convolution Collaborative Filtering", "abstract": "<p>Personalized recommendation is ubiquitous, playing an important role in many\nonline services. Substantial research has been dedicated to learning vector\nrepresentations of users and items with the goal of predicting a user\u2019s\npreference for an item based on the similarity of the representations.\nTechniques range from classic matrix factorization to more recent deep learning\nbased methods. However, we argue that existing methods do not make full use of\nthe information that is available from user-item interaction data and the\nsimilarities between user pairs and item pairs. In this work, we develop a\ngraph convolution-based recommendation framework, named Multi-Graph Convolution\nCollaborative Filtering (Multi-GCCF), which explicitly incorporates multiple\ngraphs in the embedding learning process. Multi-GCCF not only expressively\nmodels the high-order information via a partite user-item interaction graph,\nbut also integrates the proximal information by building and processing\nuser-user and item-item graphs. Furthermore, we consider the intrinsic\ndifference between user nodes and item nodes when performing graph convolution\non the bipartite graph. We conduct extensive experiments on four publicly\naccessible benchmarks, showing significant improvements relative to several\nstate-of-the-art collaborative filtering and graph neural network-based\nrecommendation models. Further experiments quantitatively verify the\neffectiveness of each component of our proposed model and demonstrate that the\nlearned embeddings capture the important relationship structure.</p>\n", "tags": ["Tools-&-Libraries", "Recommender-Systems"], "tsne_embedding": [55.115699768066406, -3.8563144207000732], "cluster": 9}, {"key": "sun20213rd", "year": "2021", "citations": "1", "title": "3rd Place: A Global And Local Dual Retrieval Solution To Facebook AI Image Similarity Challenge", "abstract": "<p>As a basic task of computer vision, image similarity retrieval is facing the\nchallenge of large-scale data and image copy attacks. This paper presents our\n3rd place solution to the matching track of Image Similarity Challenge (ISC)\n2021 organized by Facebook AI. We propose a multi-branch retrieval method of\ncombining global descriptors and local descriptors to cover all attack cases.\nSpecifically, we attempt many strategies to optimize global descriptors,\nincluding abundant data augmentations, self-supervised learning with a single\nTransformer model, overlay detection preprocessing. Moreover, we introduce the\nrobust SIFT feature and GPU Faiss for local retrieval which makes up for the\nshortcomings of the global retrieval. Finally, KNN-matching algorithm is used\nto judge the match and merge scores. We show some ablation experiments of our\nmethod, which reveals the complementary advantages of global and local\nfeatures.</p>\n", "tags": ["Self-Supervised", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Supervised"], "tsne_embedding": [-3.2115769386291504, -8.537032127380371], "cluster": 1}, {"key": "sun2021lightningdot", "year": "2021", "citations": "73", "title": "Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval", "abstract": "<p>Multimodal pre-training has propelled great advancement in\nvision-and-language research. These large-scale pre-trained models, although\nsuccessful, fatefully suffer from slow inference speed due to enormous\ncomputation cost mainly from cross-modal attention in Transformer architecture.\nWhen applied to real-life applications, such latency and computation demand\nseverely deter the practical use of pre-trained models. In this paper, we study\nImage-text retrieval (ITR), the most mature scenario of V+L application, which\nhas been widely studied even prior to the emergence of recent pre-trained\nmodels. We propose a simple yet highly effective approach, LightningDOT that\naccelerates the inference time of ITR by thousands of times, without\nsacrificing accuracy. LightningDOT removes the time-consuming cross-modal\nattention by pre-training on three novel learning objectives, extracting\nfeature indexes offline, and employing instant dot-product matching with\nfurther re-ranking, which significantly speeds up retrieval process. In fact,\nLightningDOT achieves new state of the art across multiple ITR benchmarks such\nas Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that\nconsume 1000x magnitude of computational hours. Code and pre-training\ncheckpoints are available at https://github.com/intersun/LightningDOT.</p>\n", "tags": ["Text-Retrieval", "Efficiency", "Scalability", "Hybrid-Ann-Methods", "Re-Ranking"], "tsne_embedding": [28.282142639160156, 7.207498073577881], "cluster": 2}, {"key": "sun2022deep", "year": "2022", "citations": "26", "title": "Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning", "abstract": "<p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [10.71604061126709, 11.69381046295166], "cluster": 6}, {"key": "sun2024soar", "year": "2024", "citations": "2", "title": "SOAR: Improved Indexing For Approximate Nearest Neighbor Search", "abstract": "<p>This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals,\na novel data indexing technique for approximate nearest neighbor (ANN) search.\nSOAR extends upon previous approaches to ANN search, such as spill trees, that\nutilize multiple redundant representations while partitioning the data to\nreduce the probability of missing a nearest neighbor during search. Rather than\ntraining and computing these redundant representations independently, however,\nSOAR uses an orthogonality-amplified residual loss, which optimizes each\nrepresentation to compensate for cases where other representations perform\npoorly. This drastically improves the overall index quality, resulting in\nstate-of-the-art ANN benchmark performance while maintaining fast indexing\ntimes and low memory consumption.</p>\n", "tags": ["Evaluation", "Similarity-Search"], "tsne_embedding": [11.342939376831055, 35.999088287353516], "cluster": 4}, {"key": "sun2025deep", "year": "2022", "citations": "26", "title": "Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning", "abstract": "<p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods", "CVPR", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [10.71604061126709, 11.69381046295166], "cluster": 6}, {"key": "sun2025supervised", "year": "2019", "citations": "48", "title": "Supervised Hierarchical Cross-modal Hashing", "abstract": "<p>Recently, due to the unprecedented growth of multimedia data,\ncross-modal hashing has gained increasing attention for the\nefficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but\noverlook the correlations among labels. Indeed, in many real-world\nscenarios, like the online fashion domain, instances (items) are\nlabeled with a set of categories correlated by certain hierarchy. In\nthis paper, we propose a new end-to-end solution for supervised\ncross-modal hashing, named HiCHNet, which explicitly exploits the\nhierarchical labels of instances. In particular, by the pre-established\nlabel hierarchy, we comprehensively characterize each modality\nof the instance with a set of layer-wise hash representations. In\nessence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also\nretain the hierarchical discriminative capabilities. Due to the lack\nof benchmark datasets, apart from adapting the existing dataset\nFashionVC from fashion domain, we create a dataset from the\nonline fashion platform Ssense consisting of 15, 696 image-text\npairs labeled by 32 hierarchical categories. Extensive experiments\non two real-world datasets demonstrate the superiority of our model\nover the state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "SIGIR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [17.02103614807129, -38.15203094482422], "cluster": 7}, {"key": "sundaram2013streaming", "year": "2013", "citations": "109", "title": "Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing", "abstract": "<p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,\nand many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an\napproximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel\nLSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several\nnovel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient\nalgorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration\nalgorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to\noptimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with\nhundreds of millions of new tweets per day, we can achieve query times of 1\u20132.5 ms. We show that this is an order of magnitude faster\nthan existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,\nwith table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Text-Retrieval", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [22.226064682006836, 23.94873046875], "cluster": 2}, {"key": "sundaram2025streaming", "year": "2013", "citations": "109", "title": "Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing", "abstract": "<p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,\nand many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an\napproximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel\nLSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several\nnovel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient\nalgorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration\nalgorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to\noptimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with\nhundreds of millions of new tweets per day, we can achieve query times of 1\u20132.5 ms. We show that this is an order of magnitude faster\nthan existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,\nwith table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Text-Retrieval", "Efficiency", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [22.226064682006836, 23.94873046875], "cluster": 2}, {"key": "suo2024knowledge", "year": "2024", "citations": "3", "title": "Knowledge-enhanced Dual-stream Zero-shot Composed Image Retrieval", "abstract": "<p>We study the zero-shot Composed Image Retrieval (ZS-CIR) task, which is to\nretrieve the target image given a reference image and a description without\ntraining on the triplet datasets. Previous works generate pseudo-word tokens by\nprojecting the reference image features to the text embedding space. However,\nthey focus on the global visual representation, ignoring the representation of\ndetailed attributes, e.g., color, object number and layout. To address this\nchallenge, we propose a Knowledge-Enhanced Dual-stream zero-shot composed image\nretrieval framework (KEDs). KEDs implicitly models the attributes of the\nreference images by incorporating a database. The database enriches the\npseudo-word tokens by providing relevant images and captions, emphasizing\nshared attribute information in various aspects. In this way, KEDs recognizes\nthe reference image from diverse perspectives. Moreover, KEDs adopts an extra\nstream that aligns pseudo-word tokens with textual concepts, leveraging\npseudo-triplets mined from image-text pairs. The pseudo-word tokens generated\nin this stream are explicitly aligned with fine-grained semantics in the text\nembedding space. Extensive experiments on widely used benchmarks, i.e.\nImageNet-R, COCO object, Fashion-IQ and CIRR, show that KEDs outperforms\nprevious zero-shot composed image retrieval methods.</p>\n", "tags": ["CVPR", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-10.668148040771484, 14.107552528381348], "cluster": 8}, {"key": "su\u00e1rez2024beblid", "year": "2020", "citations": "89", "title": "BEBLID: Boosted Efficient Binary Local Image Descriptor", "abstract": "<p>Efficient matching of local image features is a fundamental task in many\ncomputer vision applications. However, the real-time performance of top\nmatching algorithms is compromised in computationally limited devices, such as\nmobile phones or drones, due to the simplicity of their hardware and their\nfinite energy supply. In this paper we introduce BEBLID, an efficient learned\nbinary image descriptor. It improves our previous real-valued descriptor,\nBELID, making it both more efficient for matching and more accurate. To this\nend we use AdaBoost with an improved weak-learner training scheme that produces\nbetter local descriptions. Further, we binarize our descriptor by forcing all\nweak-learners to have the same weight in the strong learner combination and\ntrain it in an unbalanced data set to address the asymmetries arising in\nmatching and retrieval tasks. In our experiments BEBLID achieves an accuracy\nclose to SIFT and better computational efficiency than ORB, the fastest\nalgorithm in the literature.</p>\n", "tags": ["Efficiency", "Evaluation"], "tsne_embedding": [-35.827659606933594, -0.5201497673988342], "cluster": 0}, {"key": "svenstrup2017hash", "year": "2017", "citations": "29", "title": "Hash Embeddings For Efficient Word Representations", "abstract": "<p>We present hash embeddings, an efficient method for representing words in a\ncontinuous vector form. A hash embedding may be seen as an interpolation\nbetween a standard word embedding and a word embedding created using a random\nhash function (the hashing trick). In hash embeddings each token is represented\nby \\(k\\) \\(d\\)-dimensional embeddings vectors and one \\(k\\) dimensional weight\nvector. The final \\(d\\) dimensional representation of the token is the product of\nthe two. Rather than fitting the embedding vectors for each token these are\nselected by the hashing trick from a shared pool of \\(B\\) embedding vectors. Our\nexperiments show that hash embeddings can easily deal with huge vocabularies\nconsisting of millions of tokens. When using a hash embedding there is no need\nto create a dictionary before training nor to perform any kind of vocabulary\npruning after training. We show that models trained using hash embeddings\nexhibit at least the same level of performance as models trained using regular\nembeddings across a wide range of tasks. Furthermore, the number of parameters\nneeded by such an embedding is only a fraction of what is required by a regular\nembedding. Since standard embeddings and embeddings constructed using the\nhashing trick are actually just special cases of a hash embedding, hash\nembeddings can be considered an extension and improvement over the existing\nregular embedding types.</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [17.750877380371094, 15.8792724609375], "cluster": 2}, {"key": "szeto2016binary", "year": "2016", "citations": "11", "title": "Binary Codes For Tagging X-ray Images Via Deep De-noising Autoencoders", "abstract": "<p>A Content-Based Image Retrieval (CBIR) system which identifies similar\nmedical images based on a query image can assist clinicians for more accurate\ndiagnosis. The recent CBIR research trend favors the construction and use of\nbinary codes to represent images. Deep architectures could learn the non-linear\nrelationship among image pixels adaptively, allowing the automatic learning of\nhigh-level features from raw pixels. However, most of them require class\nlabels, which are expensive to obtain, particularly for medical images. The\nmethods which do not need class labels utilize a deep autoencoder for binary\nhashing, but the code construction involves a specific training algorithm and\nan ad-hoc regularization technique. In this study, we explored using a deep\nde-noising autoencoder (DDA), with a new unsupervised training scheme using\nonly backpropagation and dropout, to hash images into binary codes. We\nconducted experiments on more than 14,000 x-ray images. By using class labels\nonly for evaluating the retrieval results, we constructed a 16-bit DDA and a\n512-bit DDA independently. Comparing to other unsupervised methods, we\nsucceeded to obtain the lowest total error by using the 512-bit codes for\nretrieval via exhaustive search, and speed up 9.27 times with the use of the\n16-bit codes while keeping a comparable total error. We found that our new\ntraining scheme could reduce the total retrieval error significantly by 21.9%.\nTo further boost the image retrieval performance, we developed Radon\nAutoencoder Barcode (RABC) which are learned from the Radon projections of\nimages using a de-noising autoencoder. Experimental results demonstrated its\nsuperior performance in retrieval when it was combined with DDA binary codes.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Compact-Codes", "Evaluation", "Unsupervised"], "tsne_embedding": [-48.2706298828125, 15.1227445602417], "cluster": 0}, {"key": "szilagyi2025slag", "year": "2025", "citations": "0", "title": "SLAG: Scalable Language-augmented Gaussian Splatting", "abstract": "<p>Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [-38.7603645324707, 2.741905450820923], "cluster": 0}, {"key": "szilvasy2024vector", "year": "2024", "citations": "0", "title": "Vector Search With Small Radiuses", "abstract": "<p>In recent years, the dominant accuracy metric for vector search is the recall\nof a result list of fixed size (top-k retrieval), considering as ground truth\nthe exact vector retrieval results. Although convenient to compute, this metric\nis distantly related to the end-to-end accuracy of a full system that\nintegrates vector search. In this paper we focus on the common case where a\nhard decision needs to be taken depending on the vector retrieval results, for\nexample, deciding whether a query image matches a database image or not. We\nsolve this as a range search task, where all vectors within a certain radius\nfrom the query are returned.\n  We show that the value of a range search result can be modeled rigorously\nbased on the query-to-vector distance. This yields a metric for range search,\nRSM, that is both principled and easy to compute without running an end-to-end\nevaluation. We apply this metric to the case of image retrieval. We show that\nindexing methods that are adapted for top-k retrieval do not necessarily\nmaximize the RSM. In particular, for inverted file based indexes, we show that\nvisiting a limited set of clusters and encoding vectors compactly yields near\noptimal results.</p>\n", "tags": ["Vector-Indexing", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-18.245988845825195, 12.640787124633789], "cluster": 8}, {"key": "tabatabaei2024siamese", "year": "2024", "citations": "2", "title": "Siamese Content-based Search Engine For A More Transparent Skin And Breast Cancer Diagnosis Through Histological Imaging", "abstract": "<p>Computer Aid Diagnosis (CAD) has developed digital pathology with Deep\nLearning (DL)-based tools to assist pathologists in decision-making.\nContent-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek\nhighly correlated patches in terms of similarity in histopathological features.\nIn this work, we proposed two CBHIR approaches on breast (Breast-twins) and\nskin cancer (Skin-twins) data sets for robust and accurate patch-level\nretrieval, integrating a custom-built Siamese network as a feature extractor.\nThe proposed Siamese network is able to generalize for unseen images by\nfocusing on the similar histopathological features of the input pairs. The\nproposed CBHIR approaches are evaluated on the Breast (public) and Skin\n(private) data sets with top K accuracy. Finding the optimum amount of K is\nchallenging, but also, as much as K increases, the dissimilarity between the\nquery and the returned images increases which might mislead the pathologists.\nTo the best of the author\u2019s belief, this paper is tackling this issue for the\nfirst time on histopathological images by evaluating the top first retrieved\nimages. The Breast-twins model achieves 70% of the F1score at the top first,\nwhich exceeds the other state-of-the-art methods at a higher amount of K such\nas 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto\nEncoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model\ntackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential\n(STUMP) to assist pathologists with retrieving top K images and their\ncorresponding labels. So, this approach can offer a more explainable CAD tool\nto pathologists in terms of transparency, trustworthiness, or reliability among\nother characteristics.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-48.59502029418945, 17.048770904541016], "cluster": 0}, {"key": "taherkhani2020error", "year": "2020", "citations": "14", "title": "Error-corrected Margin-based Deep Cross-modal Hashing For Facial Image Retrieval", "abstract": "<p>Cross-modal hashing facilitates mapping of heterogeneous multimedia data into\na common Hamming space, which can beutilized for fast and flexible retrieval\nacross different modalities. In this paper, we propose a novel cross-modal\nhashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which\nuses a binary vector specifying the presence of certainfacial attributes as an\ninput query to retrieve relevant face images from a database. The DNDCMH\nnetwork consists of two separatecomponents: an attribute-based deep cross-modal\nhashing (ADCMH) module, which uses a margin (m)-based loss function\ntoefficiently learn compact binary codes to preserve similarity between\nmodalities in the Hamming space, and a neural error correctingdecoder (NECD),\nwhich is an error correcting decoder implemented with a neural network. The\ngoal of NECD network in DNDCMH isto error correct the hash codes generated by\nADCMH to improve the retrieval efficiency. The NECD network is trained such\nthat it hasan error correcting capability greater than or equal to the margin\n(m) of the margin-based loss function. This results in NECD cancorrect the\ncorrupted hash codes generated by ADCMH up to the Hamming distance of m. We\nhave evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing\nmethods on standard datasets to demonstrate the superiority of our method.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Efficiency", "Datasets"], "tsne_embedding": [-4.372733116149902, 6.69984769821167], "cluster": 8}, {"key": "tahmasebzadeh2020feature", "year": "2020", "citations": "0", "title": "A Feature Analysis For Multimodal News Retrieval", "abstract": "<p>Content-based information retrieval is based on the information contained in\ndocuments rather than using metadata such as keywords. Most information\nretrieval methods are either based on text or image. In this paper, we\ninvestigate the usefulness of multimodal features for cross-lingual news search\nin various domains: politics, health, environment, sport, and finance. To this\nend, we consider five feature types for image and text and compare the\nperformance of the retrieval system using different combinations. Experimental\nresults show that retrieval results can be improved when considering both\nvisual and textual information. In addition, it is observed that among textual\nfeatures entity overlap outperforms word embeddings, while geolocation\nembeddings achieve better performance among visual features in the retrieval\ntask.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-30.920034408569336, -38.49512481689453], "cluster": 5}, {"key": "takeshita2020secure", "year": "2020", "citations": "14", "title": "Secure Single-server Nearly-identical Image Deduplication", "abstract": "<p>Cloud computing is often utilized for file storage. Clients of cloud storage\nservices want to ensure the privacy of their data, and both clients and servers\nwant to use as little storage as possible. Cross-user deduplication is one\nmethod to reduce the amount of storage a server uses. Deduplication and privacy\nare naturally conflicting goals, especially for nearly-identical (``fuzzy\u2019\u2019)\ndeduplication, as some information about the data must be used to perform\ndeduplication. Prior solutions thus utilize multiple servers, or only function\nfor exact deduplication. In this paper, we present a single-server protocol for\ncross-user nearly-identical deduplication based on secure locality-sensitive\nhashing (SLSH). We formally define our ideal security, and rigorously prove our\nprotocol secure against fully malicious, colluding adversaries with a proof by\nsimulation. We show experimentally that the individual parts of the protocol\nare computationally feasible, and further discuss practical issues of security\nand efficiency.</p>\n", "tags": ["Hashing-Methods", "ICCV", "Efficiency"], "tsne_embedding": [35.731536865234375, 3.23087739944458], "cluster": 9}, {"key": "takeuchi2022introducing", "year": "2022", "citations": "1", "title": "Introducing Auxiliary Text Query-modifier To Content-based Audio Retrieval", "abstract": "<p>The amount of audio data available on public websites is growing rapidly, and\nan efficient mechanism for accessing the desired data is necessary. We propose\na content-based audio retrieval method that can retrieve a target audio that is\nsimilar to but slightly different from the query audio by introducing auxiliary\ntextual information which describes the difference between the query and target\naudio. While the range of conventional content-based audio retrieval is limited\nto audio that is similar to the query audio, the proposed method can adjust the\nretrieval range by adding an embedding of the auxiliary text query-modifier to\nthe embedding of the query sample audio in a shared latent space. To evaluate\nour method, we built a dataset comprising two different audio clips and the\ntext that describes the difference. The experimental results show that the\nproposed method retrieves the paired audio more accurately than the baseline.\nWe also confirmed based on visualization that the proposed method obtains the\nshared latent space in which the audio difference and the corresponding text\nare represented as similar embedding vectors.</p>\n", "tags": ["Datasets"], "tsne_embedding": [9.046852111816406, -48.48160934448242], "cluster": 3}, {"key": "talreja2019learning", "year": "2019", "citations": "19", "title": "Learning To Authenticate With Deep Multibiometric Hashing And Neural Network Decoding", "abstract": "<p>In this paper, we propose a novel multimodal deep hashing neural decoder\n(MDHND) architecture, which integrates a deep hashing framework with a neural\nnetwork decoder (NND) to create an effective multibiometric authentication\nsystem. The MDHND consists of two separate modules: a multimodal deep hashing\n(MDH) module, which is used for feature-level fusion and binarization of\nmultiple biometrics, and a neural network decoder (NND) module, which is used\nto refine the intermediate binary codes generated by the MDH and compensate for\nthe difference between enrollment and probe biometrics (variations in pose,\nillumination, etc.). Use of NND helps to improve the performance of the overall\nmultimodal authentication system. The MDHND framework is trained in 3 steps\nusing joint optimization of the two modules. In Step 1, the MDH parameters are\ntrained and learned to generate a shared multimodal latent code; in Step 2, the\nlatent codes from Step 1 are passed through a conventional error-correcting\ncode (ECC) decoder to generate the ground truth to train a neural network\ndecoder (NND); in Step 3, the NND decoder is trained using the ground truth\nfrom Step 2 and the MDH and NND are jointly optimized. Experimental results on\na standard multimodal dataset demonstrate the superiority of our method\nrelative to other current multimodal authentication systems</p>\n", "tags": ["Hashing-Methods", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation", "Neural-Hashing"], "tsne_embedding": [-11.022913932800293, 28.132766723632812], "cluster": 8}, {"key": "talreja2019using", "year": "2018", "citations": "25", "title": "Using Deep Cross Modal Hashing And Error Correcting Codes For Improving The Efficiency Of Attribute Guided Facial Image Retrieval", "abstract": "<p>With benefits of fast query speed and low storage cost, hashing-based image\nretrieval approaches have garnered considerable attention from the research\ncommunity. In this paper, we propose a novel Error-Corrected Deep Cross Modal\nHashing (CMH-ECC) method which uses a bitmap specifying the presence of certain\nfacial attributes as an input query to retrieve relevant face images from the\ndatabase. In this architecture, we generate compact hash codes using an\nend-to-end deep learning module, which effectively captures the inherent\nrelationships between the face and attribute modality. We also integrate our\ndeep learning module with forward error correction codes to further reduce the\ndistance between different modalities of the same subject. Specifically, the\nproperties of deep hashing and forward error correction codes are exploited to\ndesign a cross modal hashing framework with high retrieval performance.\nExperimental results using two standard datasets with facial attributes-image\nmodalities indicate that our CMH-ECC face image retrieval model outperforms\nmost of the current attribute-based face image retrieval approaches.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-12.784003257751465, 6.068032741546631], "cluster": 1}, {"key": "talreja2019zero", "year": "2019", "citations": "20", "title": "Zero-shot Deep Hashing And Neural Network Based Error Correction For Face Template Protection", "abstract": "<p>In this paper, we present a novel architecture that integrates a deep hashing\nframework with a neural network decoder (NND) for application to face template\nprotection. It improves upon existing face template protection techniques to\nprovide better matching performance with one-shot and multi-shot enrollment. A\nkey novelty of our proposed architecture is that the framework can also be used\nwith zero-shot enrollment. This implies that our architecture does not need to\nbe re-trained even if a new subject is to be enrolled into the system. The\nproposed architecture consists of two major components: a deep hashing (DH)\ncomponent, which is used for robust mapping of face images to their\ncorresponding intermediate binary codes, and a NND component, which corrects\nerrors in the intermediate binary codes that are caused by differences in the\nenrollment and probe biometrics due to factors such as variation in pose,\nillumination, and other factors. The final binary code generated by the NND is\nthen cryptographically hashed and stored as a secure face template in the\ndatabase. The efficacy of our approach with zero-shot, one-shot, and multi-shot\nenrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE\nface databases. With zero-shot enrollment, the system achieves approximately\n85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with\none-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at\n0.01% FAR, while providing a high level of template security.</p>\n", "tags": ["Hashing-Methods", "Few-Shot-&-Zero-Shot", "Tools-&-Libraries", "Compact-Codes", "Evaluation", "Neural-Hashing"], "tsne_embedding": [-11.378470420837402, 28.50917625427246], "cluster": 8}, {"key": "talreja2020deep", "year": "2020", "citations": "24", "title": "Deep Hashing For Secure Multimodal Biometrics", "abstract": "<p>When compared to unimodal systems, multimodal biometric systems have several\nadvantages, including lower error rate, higher accuracy, and larger population\ncoverage. However, multimodal systems have an increased demand for integrity\nand privacy because they must store multiple biometric traits associated with\neach user. In this paper, we present a deep learning framework for\nfeature-level fusion that generates a secure multimodal template from each\nuser\u2019s face and iris biometrics. We integrate a deep hashing (binarization)\ntechnique into the fusion architecture to generate a robust binary multimodal\nshared latent representation. Further, we employ a hybrid secure architecture\nby combining cancelable biometrics with secure sketch techniques and integrate\nit with a deep hashing framework, which makes it computationally prohibitive to\nforge a combination of multiple biometrics that pass the authentication. The\nefficacy of the proposed approach is shown using a multimodal database of face\nand iris and it is observed that the matching performance is improved due to\nthe fusion of multiple biometrics. Furthermore, the proposed approach also\nprovides cancelability and unlinkability of the templates along with improved\nprivacy of the biometric data. Additionally, we also test the proposed hashing\nfunction for an image retrieval application using a benchmark dataset. The main\ngoal of this paper is to develop a method for integrating multimodal fusion,\ndeep hashing, and biometric security, with an emphasis on structural data from\nmodalities like face and iris. The proposed approach is in no way a general\nbiometric security framework that can be applied to all biometric modalities,\nas further research is needed to extend the proposed framework to other\nunconstrained biometric modalities.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation", "Neural-Hashing"], "tsne_embedding": [-12.028388977050781, 28.958913803100586], "cluster": 8}, {"key": "tan2017supervised", "year": "2018", "citations": "4", "title": "Supervised Hashing With End-to-end Binary Deep Neural Network", "abstract": "<p>Image hashing is a popular technique applied to large scale content-based\nvisual retrieval due to its compact and efficient binary codes. Our work\nproposes a new end-to-end deep network architecture for supervised hashing\nwhich directly learns binary codes from input images and maintains good\nproperties over binary codes such as similarity preservation, independence, and\nbalancing. Furthermore, we also propose a new learning scheme that can cope\nwith the binary constrained loss function. The proposed algorithm not only is\nscalable for learning over large-scale datasets but also outperforms\nstate-of-the-art supervised hashing methods, which are illustrated throughout\nextensive experiments from various image retrieval benchmarks.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-6.305652141571045, -1.1824979782104492], "cluster": 1}, {"key": "tan2020learning", "year": "2020", "citations": "77", "title": "Learning To Hash With Graph Neural Networks For Recommender Systems", "abstract": "<p>Graph representation learning has attracted much attention in supporting high\nquality candidate search at scale. Despite its effectiveness in learning\nembedding vectors for objects in the user-item interaction network, the\ncomputational costs to infer users\u2019 preferences in continuous embedding space\nare tremendous. In this work, we investigate the problem of hashing with graph\nneural networks (GNNs) for high quality retrieval, and propose a simple yet\neffective discrete representation learning framework to jointly learn\ncontinuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN)\nis presented, which consists of two components, a GNN encoder for learning node\nrepresentations, and a hash layer for encoding representations to hash codes.\nThe whole architecture is trained end-to-end by jointly optimizing two losses,\ni.e., reconstruction loss from reconstructing observed links, and ranking loss\nfrom preserving the relative ordering of hash codes. A novel discrete\noptimization strategy based on straight through estimator (STE) with guidance\nis proposed. The principal idea is to avoid gradient magnification in\nback-propagation of STE with continuous embedding guidance, in which we begin\nfrom learning an easier network that mimic the continuous embedding and let it\nevolve during the training until it finally goes back to STE. Comprehensive\nexperiments over three publicly available and one real-world Alibaba company\ndatasets demonstrate that our model not only can achieve comparable performance\ncompared with its continuous counterpart but also runs multiple times faster\nduring inference.</p>\n", "tags": ["Hashing-Methods", "Recommender-Systems", "Tools-&-Libraries", "Datasets", "Evaluation", "Neural-Hashing"], "tsne_embedding": [49.79265213012695, 0.8842912316322327], "cluster": 9}, {"key": "tan2021bcd", "year": "2021", "citations": "0", "title": "BCD: A Cross-architecture Binary Comparison Database Experiment Using Locality Sensitive Hashing Algorithms", "abstract": "<p>Given a binary executable without source code, it is difficult to determine\nwhat each function in the binary does by reverse engineering it, and even\nharder without prior experience and context. In this paper, we performed a\ncomparison of different hashing functions\u2019 effectiveness at detecting similar\nlifted snippets of LLVM IR code, and present the design and implementation of a\nframework for cross-architecture binary code similarity search database using\nMinHash as the chosen hashing algorithm, over SimHash, SSDEEP and TLSH. The\nmotivation is to help reverse engineers to quickly gain context of functions in\nan unknown binary by comparing it against a database of known functions. The\ncode for this project is open source and can be found at\nhttps://github.com/h4sh5/bcddb</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [12.24377155303955, -8.95240306854248], "cluster": 6}, {"key": "tan2021fast", "year": "2022", "citations": "12", "title": "A Fast Partial Video Copy Detection Using KNN And Global Feature Database", "abstract": "<p>We propose a fast partial video copy detection framework in this paper. In\nthis framework all frame features of the reference videos are organized in a\nKNN searchable database. Instead of scanning all reference videos, the query\nvideo segment does a fast KNN search in the global feature database. The\nreturned results are used to generate a short list of candidate videos. A\nmodified temporal network is then used to localize the copy segment in the\ncandidate videos. We evaluate different choice of CNN features on the VCDB\ndataset. Our benchmark F1 score exceeds the state of the art by a big margin.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Similarity-Search", "Datasets"], "tsne_embedding": [-50.17667007446289, -7.290775775909424], "cluster": 0}, {"key": "tan2021instance", "year": "2021", "citations": "71", "title": "Instance-level Image Retrieval Using Reranking Transformers", "abstract": "<p>Instance-level image retrieval is the task of searching in a large database\nfor images that match an object in a query image. To address this task, systems\nusually rely on a retrieval step that uses global image descriptors, and a\nsubsequent step that performs domain-specific refinements or reranking by\nleveraging operations such as geometric verification based on local features.\nIn this work, we propose Reranking Transformers (RRTs) as a general model to\nincorporate both local and global features to rerank the matching images in a\nsupervised fashion and thus replace the relatively expensive process of\ngeometric verification. RRTs are lightweight and can be easily parallelized so\nthat reranking a set of top matching results can be performed in a single\nforward-pass. We perform extensive experiments on the Revisited Oxford and\nParis datasets, and the Google Landmarks v2 dataset, showing that RRTs\noutperform previous reranking approaches while using much fewer local\ndescriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs\ncan be optimized jointly with the feature extractor, which can lead to feature\nrepresentations tailored to downstream tasks and further accuracy improvements.\nThe code and trained models are publicly available at\nhttps://github.com/uvavision/RerankingTransformer.</p>\n", "tags": ["ICCV", "Image-Retrieval", "Datasets", "Supervised", "Re-Ranking"], "tsne_embedding": [-33.80196762084961, 5.442385673522949], "cluster": 0}, {"key": "tan2022multilingual", "year": "2023", "citations": "7", "title": "Multilingual Representation Distillation With Contrastive Learning", "abstract": "<p>Multilingual sentence representations from large models encode semantic\ninformation from two or more languages and can be used for different\ncross-lingual information retrieval and matching tasks. In this paper, we\nintegrate contrastive learning into multilingual representation distillation\nand use it for quality estimation of parallel sentences (i.e., find\nsemantically similar sentences that can be used as translations of each other).\nWe validate our approach with multilingual similarity search and corpus\nfiltering tasks. Experiments across different low-resource languages show that\nour method greatly outperforms previous sentence encoders such as LASER,\nLASER3, and LaBSE.</p>\n", "tags": ["Self-Supervised", "Similarity-Search"], "tsne_embedding": [-3.732572078704834, -36.46870040893555], "cluster": 3}, {"key": "tan2023fast", "year": "2025", "citations": "0", "title": "Fast Locality Sensitive Hashing With Theoretical Guarantee", "abstract": "<p>Locality-sensitive hashing (LSH) is an effective randomized technique widely\nused in many machine learning tasks. The cost of hashing is proportional to\ndata dimensions, and thus often the performance bottleneck when dimensionality\nis high and the number of hash functions involved is large. Surprisingly,\nhowever, little work has been done to improve the efficiency of LSH\ncomputation. In this paper, we design a simple yet efficient LSH scheme, named\nFastLSH, under l2 norm. By combining random sampling and random projection,\nFastLSH reduces the time complexity from O(n) to O(m) (m&lt;n), where n is the\ndata dimensionality and m is the number of sampled dimensions. Moreover,\nFastLSH has provable LSH property, which distinguishes it from the non-LSH fast\nsketches. We conduct comprehensive experiments over a collection of real and\nsynthetic datasets for the nearest neighbor search task. Experimental results\ndemonstrate that FastLSH is on par with the state-of-the-arts in terms of\nanswer quality, space occupation and query efficiency, while enjoying up to 80x\nspeedup in hash function evaluation. We believe that FastLSH is a promising\nalternative to the classic LSH scheme.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [31.602127075195312, 20.449541091918945], "cluster": 2}, {"key": "tan2023unfolded", "year": "2023", "citations": "1", "title": "Unfolded Self-reconstruction LSH: Towards Machine Unlearning In Approximate Nearest Neighbour Search", "abstract": "<p>Approximate nearest neighbour (ANN) search is an essential component of\nsearch engines, recommendation systems, etc. Many recent works focus on\nlearning-based data-distribution-dependent hashing and achieve good retrieval\nperformance. However, due to increasing demand for users\u2019 privacy and security,\nwe often need to remove users\u2019 data information from Machine Learning (ML)\nmodels to satisfy specific privacy and security requirements. This need\nrequires the ANN search algorithm to support fast online data deletion and\ninsertion. Current learning-based hashing methods need retraining the hash\nfunction, which is prohibitable due to the vast time-cost of large-scale data.\nTo address this problem, we propose a novel data-dependent hashing method named\nunfolded self-reconstruction locality-sensitive hashing (USR-LSH). Our USR-LSH\nunfolded the optimization update for instance-wise data reconstruction, which\nis better for preserving data information than data-independent LSH. Moreover,\nour USR-LSH supports fast online data deletion and insertion without\nretraining. To the best of our knowledge, we are the first to address the\nmachine unlearning of retrieval problems. Empirically, we demonstrate that\nUSR-LSH outperforms the state-of-the-art data-distribution-independent LSH in\nANN tasks in terms of precision and recall. We also show that USR-LSH has\nsignificantly faster data deletion and insertion time than learning-based\ndata-dependent hashing.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Recommender-Systems", "Similarity-Search", "Scalability", "Evaluation"], "tsne_embedding": [27.667743682861328, 4.775257110595703], "cluster": 2}, {"key": "tang2021improving", "year": "2021", "citations": "23", "title": "Improving Document Representations By Generating Pseudo Query Embeddings For Dense Retrieval", "abstract": "<p>Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.</p>\n", "tags": ["Text-Retrieval", "Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [26.540000915527344, 22.725191116333008], "cluster": 2}, {"key": "tang2021when", "year": "2021", "citations": "0", "title": "When Similarity Digest Meets Vector Management System: A Survey On Similarity Hash Function", "abstract": "<p>The booming vector manage system calls for feasible similarity hash function\nas a front-end to perform similarity analysis. In this paper, we make a\nsystematical survey on the existent well-known similarity hash functions to\ntease out the satisfied ones. We conclude that the similarity hash function\nMinHash and Nilsimsa can be directly marshaled into the pipeline of similarity\nanalysis using vector manage system. After that, we make a brief and empirical\ndiscussion on the performance, drawbacks of the these functions and highlight\nMinHash, the variant of SimHash and feature hashing are the best for vector\nmanagement system for large-scale similarity analysis.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Locality-Sensitive-Hashing", "Scalability", "Evaluation"], "tsne_embedding": [-21.74776840209961, 26.316442489624023], "cluster": 8}, {"key": "tang2023renderers", "year": "2023", "citations": "0", "title": "Renderers Are Good Zero-shot Representation Learners: Exploring Diffusion Latents For Metric Learning", "abstract": "<p>Can the latent spaces of modern generative neural rendering models serve as\nrepresentations for 3D-aware discriminative visual understanding tasks? We use\nretrieval as a proxy for measuring the metric learning properties of the latent\nspaces of Shap-E, including capturing view-independence and enabling the\naggregation of scene representations from the representations of individual\nimage views, and find that Shap-E representations outperform those of the\nclassical EfficientNet baseline representations zero-shot, and is still\ncompetitive when both methods are trained using a contrative loss. These\nfindings give preliminary indication that 3D-based rendering and generative\nmodels can yield useful representations for discriminative tasks in our\ninnately 3D-native world. Our code is available at\nhttps://github.com/michaelwilliamtang/golden-retriever.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Distance-Metric-Learning"], "tsne_embedding": [-35.77964782714844, -13.739664077758789], "cluster": 5}, {"key": "tang2024generative", "year": "2024", "citations": "0", "title": "Generative Retrieval Meets Multi-graded Relevance", "abstract": "<p>Generative retrieval represents a novel approach to information retrieval. It\nuses an encoder-decoder architecture to directly produce relevant document\nidentifiers (docids) for queries. While this method offers benefits, current\napproaches are limited to scenarios with binary relevance data, overlooking the\npotential for documents to have multi-graded relevance. Extending generative\nretrieval to accommodate multi-graded relevance poses challenges, including the\nneed to reconcile likelihood probabilities for docid pairs and the possibility\nof multiple relevant documents sharing the same identifier. To address these\nchallenges, we introduce a framework called GRaded Generative Retrieval\n(GR\\(^2\\)). GR\\(^2\\) focuses on two key components: ensuring relevant and distinct\nidentifiers, and implementing multi-graded constrained contrastive training.\nFirst, we create identifiers that are both semantically relevant and\nsufficiently distinct to represent individual documents effectively. This is\nachieved by jointly optimizing the relevance and distinctness of docids through\na combination of docid generation and autoencoder models. Second, we\nincorporate information about the relationship between relevance grades to\nguide the training process. We use a constrained contrastive training strategy\nto bring the representations of queries and the identifiers of their relevant\ndocuments closer together, based on their respective relevance grades.\nExtensive experiments on datasets with both multi-graded and binary relevance\ndemonstrate the effectiveness of GR\\(^2\\).</p>\n", "tags": ["Tools-&-Libraries", "Datasets"], "tsne_embedding": [-1.160697102546692, -25.241769790649414], "cluster": 3}, {"key": "tanioka2019fast", "year": "2019", "citations": "10", "title": "A Fast Content-based Image Retrieval Method Using Deep Visual Features", "abstract": "<p>Fast and scalable Content-Based Image Retrieval using visual features is\nrequired for document analysis, Medical image analysis, etc. in the present\nage. Convolutional Neural Network (CNN) activations as features achieved their\noutstanding performance in this area. Deep Convolutional representations using\nthe softmax function in the output layer are also ones among visual features.\nHowever, almost all the image retrieval systems hold their index of visual\nfeatures on main memory in order to high responsiveness, limiting their\napplicability for big data applications. In this paper, we propose a fast\ncalculation method of cosine similarity with L2 norm indexed in advance on\nElasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16\npre-trained model. The evaluation results show the effectiveness and efficiency\nof our proposed method.</p>\n", "tags": ["Distance-Metric-Learning", "Efficiency", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-50.78453063964844, 3.576366662979126], "cluster": 0}, {"key": "tarawneh2018deep", "year": "2019", "citations": "35", "title": "Deep Face Image Retrieval: A Comparative Study With Dictionary Learning", "abstract": "<p>Facial image retrieval is a challenging task since faces have many similar\nfeatures (areas), which makes it difficult for the retrieval systems to\ndistinguish faces of different people. With the advent of deep learning, deep\nnetworks are often applied to extract powerful features that are used in many\nareas of computer vision. This paper investigates the application of different\ndeep learning models for face image retrieval, namely, Alexlayer6, Alexlayer7,\nVGG16layer6, VGG16layer7, VGG19layer6, and VGG19layer7, with two types of\ndictionary learning techniques, namely \\(K\\)-means and \\(K\\)-SVD. We also\ninvestigate some coefficient learning techniques such as the Homotopy, Lasso,\nElastic Net and SSF and their effect on the face retrieval system. The\ncomparative results of the experiments conducted on three standard face image\ndatasets show that the best performers for face image retrieval are Alexlayer7\nwith \\(K\\)-means and SSF, Alexlayer6 with \\(K\\)-SVD and SSF, and Alexlayer6 with\n\\(K\\)-means and SSF. The APR and ARR of these methods were further compared to\nsome of the state of the art methods based on local descriptors. The\nexperimental results show that deep learning outperforms most of those methods\nand therefore can be recommended for use in practice of face image retrieval</p>\n", "tags": ["Survey-Paper", "Image-Retrieval", "Datasets"], "tsne_embedding": [-13.441792488098145, 4.8868937492370605], "cluster": 1}, {"key": "tasawong2023typo", "year": "2023", "citations": "2", "title": "Typo-robust Representation Learning For Dense Retrieval", "abstract": "<p>Dense retrieval is a basic building block of information retrieval\napplications. One of the main challenges of dense retrieval in real-world\nsettings is the handling of queries containing misspelled words. A popular\napproach for handling misspelled queries is minimizing the representations\ndiscrepancy between misspelled queries and their pristine ones. Unlike the\nexisting approaches, which only focus on the alignment between misspelled and\npristine queries, our method also improves the contrast between each misspelled\nquery and its surrounding queries. To assess the effectiveness of our proposed\nmethod, we compare it against the existing competitors using two benchmark\ndatasets and two base encoders. Our method outperforms the competitors in all\ncases with misspelled queries. Our code and models are available at\nhttps://github. com/panuthept/DST-DenseRetrieval.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [23.7174015045166, 21.127748489379883], "cluster": 2}, {"key": "tatsuno2024aisaq", "year": "2024", "citations": "0", "title": "Aisaq: All-in-storage ANNS With Product Quantization For Dram-free Information Retrieval", "abstract": "<p>Graph-based approximate nearest neighbor search (ANNS) algorithms work\neffectively against large-scale vector retrieval. Among such methods, DiskANN\nachieves good recall-speed tradeoffs using both DRAM and storage. DiskANN\nadopts product quantization (PQ) to reduce memory usage, which is still\nproportional to the scale of datasets. In this paper, we propose All-in-Storage\nANNS with Product Quantization (AiSAQ), which offloads compressed vectors to\nthe SSD index. Our method achieves \\(\\sim\\)10 MB memory usage in query search\nwith billion-scale datasets without critical latency degradation. AiSAQ also\nreduces the index load time for query search preparation, which enables fast\nswitch between muitiple billion-scale indices.This method can be applied to\nretrievers of retrieval-augmented generation (RAG) and be scaled out with\nmultiple-server systems for emerging datasets. Our DiskANN-based implementation\nis available on GitHub.</p>\n", "tags": ["Graph-Based-Ann", "Quantization", "Scalability", "Memory-Efficiency", "Large-Scale-Search", "Datasets", "Evaluation"], "tsne_embedding": [36.42157745361328, 19.040691375732422], "cluster": 2}, {"key": "tautkute2021i", "year": "2021", "citations": "4", "title": "I Want This Product But Different : Multimodal Retrieval With Synthetic Query Expansion", "abstract": "<p>This paper addresses the problem of media retrieval using a multimodal query\n(a query which combines visual input with additional semantic information in\nnatural language feedback). We propose a SynthTriplet GAN framework which\nresolves this task by expanding the multimodal query with a synthetically\ngenerated image that captures semantic information from both image and text\ninput. We introduce a novel triplet mining method that uses a synthetic image\nas an anchor to directly optimize for embedding distances of generated and\ntarget images. We demonstrate that apart from the added value of retrieval\nillustration with synthetic image with the focus on customization and user\nfeedback, the proposed method greatly surpasses other multimodal generation\nmethods and achieves state of the art results in the multimodal retrieval task.\nWe also show that in contrast to other retrieval methods, our method provides\nexplainable embeddings.</p>\n", "tags": ["Multimodal-Retrieval", "Tools-&-Libraries"], "tsne_embedding": [11.229744911193848, -36.11470031738281], "cluster": 7}, {"key": "tchayekondi2020new", "year": "2021", "citations": "5", "title": "A New Hashing Based Nearest Neighbors Selection Technique For Big Datasets", "abstract": "<p>KNN has the reputation to be the word simplest but efficient supervised\nlearning algorithm used for either classification or regression. KNN prediction\nefficiency highly depends on the size of its training data but when this\ntraining data grows KNN suffers from slowness in making decisions since it\nneeds to search nearest neighbors within the entire dataset at each decision\nmaking. This paper proposes a new technique that enables the selection of\nnearest neighbors directly in the neighborhood of a given observation. The\nproposed approach consists of dividing the data space into subcells of a\nvirtual grid built on top of data space. The mapping between the data points\nand subcells is performed using hashing. When it comes to select the nearest\nneighbors of a given observation, we firstly identify the cell the observation\nbelongs by using hashing, and then we look for nearest neighbors from that\ncentral cell and cells around it layer by layer. From our experiment\nperformance analysis on publicly available datasets, our algorithm outperforms\nthe original KNN in time efficiency with a prediction quality as good as that\nof KNN it also offers competitive performance with solutions like KDtree</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [45.48016357421875, 9.779640197753906], "cluster": 9}, {"key": "teichmann2018detect", "year": "2019", "citations": "118", "title": "Detect-to-retrieve: Efficient Regional Aggregation For Image Search", "abstract": "<p>Retrieving object instances among cluttered scenes efficiently requires\ncompact yet comprehensive regional image representations. Intuitively, object\nsemantics can help build the index that focuses on the most relevant regions.\nHowever, due to the lack of bounding-box datasets for objects of interest among\nretrieval benchmarks, most recent work on regional representations has focused\non either uniform or class-agnostic region selection. In this paper, we first\nfill the void by providing a new dataset of landmark bounding boxes, based on\nthe Google Landmarks dataset, that includes \\(86k\\) images with manually curated\nboxes from \\(15k\\) unique landmarks. Then, we demonstrate how a trained landmark\ndetector, using our new dataset, can be leveraged to index image regions and\nimprove retrieval accuracy while being much more efficient than existing\nregional methods. In addition, we introduce a novel regional aggregated\nselective match kernel (R-ASMK) to effectively combine information from\ndetected regions into an improved holistic image representation. R-ASMK boosts\nimage retrieval accuracy substantially with no dimensionality increase, while\neven outperforming systems that index image regions independently. Our complete\nimage retrieval system improves upon the previous state-of-the-art by\nsignificant margins on the Revisited Oxford and Paris datasets. Code and data\navailable at the project webpage:\nhttps://github.com/tensorflow/models/tree/master/research/delf.</p>\n", "tags": ["CVPR", "Image-Retrieval", "Datasets"], "tsne_embedding": [-34.22317886352539, 10.149698257446289], "cluster": 0}, {"key": "tellez2017scalable", "year": "2021", "citations": "2", "title": "A Scalable Solution To The Nearest Neighbor Search Problem Through Local-search Methods On Neighbor Graphs", "abstract": "<p>Near neighbor search (NNS) is a powerful abstraction for data access;\nhowever, data indexing is troublesome even for approximate indexes. For\nintrinsically high-dimensional data, high-quality fast searches demand either\nindexes with impractically large memory usage or preprocessing time.\n  In this paper, we introduce an algorithm to solve a nearest-neighbor query\n\\(q\\) by minimizing a kernel function defined by the distance from \\(q\\) to each\nobject in the database. The minimization is performed using metaheuristics to\nsolve the problem rapidly; even when some methods in the literature use this\nstrategy behind the scenes, our approach is the first one using it explicitly.\nWe also provide two approaches to select edges in the graph\u2019s construction\nstage that limit memory footprint and reduce the number of free parameters\nsimultaneously.\n  We carry out a thorough experimental comparison with state-of-the-art indexes\nthrough synthetic and real-world datasets; we found out that our contributions\nachieve competitive performances regarding speed, accuracy, and memory in\nalmost any of our benchmarks.</p>\n", "tags": ["Memory-Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [51.580684661865234, 9.749869346618652], "cluster": 9}, {"key": "teng2024mvbind", "year": "2024", "citations": "0", "title": "MVBIND: Self-supervised Music Recommendation For Videos Via Embedding Space Binding", "abstract": "<p>Recent years have witnessed the rapid development of short videos, which\nusually contain both visual and audio modalities. Background music is important\nto the short videos, which can significantly influence the emotions of the\nviewers. However, at present, the background music of short videos is generally\nchosen by the video producer, and there is a lack of automatic music\nrecommendation methods for short videos. This paper introduces MVBind, an\ninnovative Music-Video embedding space Binding model for cross-modal retrieval.\nMVBind operates as a self-supervised approach, acquiring inherent knowledge of\nintermodal relationships directly from data, without the need of manual\nannotations. Additionally, to compensate the lack of a corresponding\nmusical-visual pair dataset for short videos, we construct a dataset,\nSVM-10K(Short Video with Music-10K), which mainly consists of meticulously\nselected short videos. On this dataset, MVBind manifests significantly improved\nperformance compared to other baseline methods. The constructed dataset and\ncode will be released to facilitate future research.</p>\n", "tags": ["Self-Supervised", "Recommender-Systems", "Multimodal-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [2.3264596462249756, -48.70077896118164], "cluster": 3}, {"key": "teofili2019lucene", "year": "2019", "citations": "3", "title": "Lucene For Approximate Nearest-neighbors Search On Arbitrary Dense Vectors", "abstract": "<p>We demonstrate three approaches for adapting the open-source Lucene search\nlibrary to perform approximate nearest-neighbor search on arbitrary dense\nvectors, using similarity search on word embeddings as a case study. At its\ncore, Lucene is built around inverted indexes of a document collection\u2019s\n(sparse) term-document matrix, which is incompatible with the lower-dimensional\ndense vectors that are common in deep learning applications. We evaluate three\ntechniques to overcome these challenges that can all be natively integrated\ninto Lucene: the creation of documents populated with fake words, LSH applied\nto lexical realizations of dense vectors, and k-d trees coupled with\ndimensionality reduction. Experiments show that the \u201cfake words\u201d approach\nrepresents the best balance between effectiveness and efficiency. These\ntechniques are integrated into the Anserini open-source toolkit and made\navailable to the community.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Efficiency", "Similarity-Search", "Tools-&-Libraries", "Tree-Based-Ann"], "tsne_embedding": [36.590763092041016, -19.54873275756836], "cluster": 7}, {"key": "tepper2020procrustean", "year": "2020", "citations": "1", "title": "Procrustean Orthogonal Sparse Hashing", "abstract": "<p>Hashing is one of the most popular methods for similarity search because of\nits speed and efficiency. Dense binary hashing is prevalent in the literature.\nRecently, insect olfaction was shown to be structurally and functionally\nanalogous to sparse hashing [6]. Here, we prove that this biological mechanism\nis the solution to a well-posed optimization problem. Furthermore, we show that\northogonality increases the accuracy of sparse hashing. Next, we present a\nnovel method, Procrustean Orthogonal Sparse Hashing (POSH), that unifies these\nfindings, learning an orthogonal transform from training data compatible with\nthe sparse hashing mechanism. We provide theoretical evidence of the\nshortcomings of Optimal Sparse Lifting (OSL) [22] and BioHash [30], two related\nolfaction-inspired methods, and propose two new methods, Binary OSL and\nSphericalHash, to address these deficiencies. We compare POSH, Binary OSL, and\nSphericalHash to several state-of-the-art hashing methods and provide empirical\nresults for the superiority of the proposed methods across a wide range of\nstandard benchmarks and parameter settings.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Similarity-Search"], "tsne_embedding": [-10.681846618652344, 49.10956573486328], "cluster": 4}, {"key": "thakur2022injecting", "year": "2022", "citations": "6", "title": "Injecting Domain Adaptation With Learning-to-hash For Effective And Efficient Zero-shot Dense Retrieval", "abstract": "<p>Dense retrieval overcome the lexical gap and has shown great success in\nad-hoc information retrieval (IR). Despite their success, dense retrievers are\nexpensive to serve across practical use cases. For use cases requiring to\nsearch from millions of documents, the dense index becomes bulky and requires\nhigh memory usage for storing the index. More recently, learning-to-hash (LTH)\ntechniques, for e.g., BPR and JPQ, produce binary document vectors, thereby\nreducing the memory requirement to efficiently store the dense index. LTH\ntechniques are supervised and finetune the retriever using a ranking loss. They\noutperform their counterparts, i.e., traditional out-of-the-box vector\ncompression techniques such as PCA or PQ. A missing piece from prior work is\nthat existing techniques have been evaluated only in-domain, i.e., on a single\ndataset such as MS MARCO. In our work, we evaluate LTH and vector compression\ntechniques for improving the downstream zero-shot retrieval accuracy of the\nTAS-B dense retriever while maintaining efficiency at inference. Our results\ndemonstrate that, unlike prior work, LTH strategies when applied naively can\nunderperform the zero-shot TAS-B dense retriever on average by up to 14%\nnDCG@10 on the BEIR benchmark. To solve this limitation, in our work, we\npropose an easy yet effective solution of injecting domain adaptation with\nexisting supervised LTH techniques. We experiment with two well-known\nunsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptation\ninjection technique can improve the downstream zero-shot retrieval\neffectiveness for both BPR and JPQ variants of the TAS-B model by on average\n11.5% and 8.2% nDCG@10 while both maintaining 32\\(\\times\\) memory efficiency and\n14\\(\\times\\) and 2\\(\\times\\) speedup respectively in CPU retrieval latency on BEIR.\nAll our code, models, and data are publicly available at\nhttps://github.com/thakur-nandan/income.</p>\n", "tags": ["Efficiency", "Quantization", "Few-Shot-&-Zero-Shot", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [10.794042587280273, 18.319303512573242], "cluster": 6}, {"key": "tharani2018unsupervised", "year": "2018", "citations": "5", "title": "Unsupervised Deep Features For Remote Sensing Image Matching Via Discriminator Network", "abstract": "<p>The advent of deep perceptual networks brought about a paradigm shift in\nmachine vision and image perception. Image apprehension lately carried out by\nhand-crafted features in the latent space have been replaced by deep features\nacquired from supervised networks for improved understanding. However, such\ndeep networks require strict supervision with a substantial amount of the\nlabeled data for authentic training process. These methods perform poorly in\ndomains lacking labeled data especially in case of remote sensing image\nretrieval. Resolving this, we propose an unsupervised encoder-decoder feature\nfor remote sensing image matching (RSIM). Moreover, we replace the conventional\ndistance metrics with a deep discriminator network to identify the similarity\nof the image pairs. To the best of our knowledge, discriminator network has\nnever been used before for solving RSIM problem. Results have been validated\nwith two publicly available benchmark remote sensing image datasets. The\ntechnique has also been investigated for content-based remote sensing image\nretrieval (CBRSIR); one of the widely used applications of RSIM. Results\ndemonstrate that our technique supersedes the state-of-the-art methods used for\nunsupervised image matching with mean average precision (mAP) of 81%, and image\nretrieval with an overall improvement in mAP score of about 12%.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-44.226890563964844, -5.2027974128723145], "cluster": 0}, {"key": "thawakar2024composed", "year": "2024", "citations": "2", "title": "Composed Video Retrieval Via Enriched Context And Discriminative Embeddings", "abstract": "<p>Composed video retrieval (CoVR) is a challenging problem in computer vision\nwhich has recently highlighted the integration of modification text with visual\nqueries for more sophisticated video search in large databases. Existing works\npredominantly rely on visual queries combined with modification text to\ndistinguish relevant videos. However, such a strategy struggles to fully\npreserve the rich query-specific context in retrieved target videos and only\nrepresents the target video using visual embedding. We introduce a novel CoVR\nframework that leverages detailed language descriptions to explicitly encode\nquery-specific contextual information and learns discriminative embeddings of\nvision only, text only and vision-text for better alignment to accurately\nretrieve matched target videos. Our proposed framework can be flexibly employed\nfor both composed video (CoVR) and image (CoIR) retrieval tasks. Experiments on\nthree datasets show that our approach obtains state-of-the-art performance for\nboth CovR and zero-shot CoIR tasks, achieving gains as high as around 7% in\nterms of recall@K=1 score. Our code, models, detailed language descriptions for\nWebViD-CoVR dataset are available at\nhttps://github.com/OmkarThawakar/composed-video-retrieval</p>\n", "tags": ["CVPR", "Few-Shot-&-Zero-Shot", "Tools-&-Libraries", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-38.36271667480469, -30.922060012817383], "cluster": 5}, {"key": "theisen2023c", "year": "2024", "citations": "0", "title": "C-CLIP: Contrastive Image-text Encoders To Close The Descriptive-commentative Gap", "abstract": "<p>The interplay between the image and comment on a social media post is one of\nhigh importance for understanding its overall message. Recent strides in\nmultimodal embedding models, namely CLIP, have provided an avenue forward in\nrelating image and text. However the current training regime for CLIP models is\ninsufficient for matching content found on social media, regardless of site or\nlanguage. Current CLIP training data is based on what we call <code class=\"language-plaintext highlighter-rouge\">descriptive''\ntext: text in which an image is merely described. This is something rarely seen\non social media, where the vast majority of text content is</code>commentative\u2019\u2019 in\nnature. The captions provide commentary and broader context related to the\nimage, rather than describing what is in it. Current CLIP models perform poorly\non retrieval tasks where image-caption pairs display a commentative\nrelationship. Closing this gap would be beneficial for several important\napplication areas related to social media. For instance, it would allow groups\nfocused on Open-Source Intelligence Operations (OSINT) to further aid efforts\nduring disaster events, such as the ongoing Russian invasion of Ukraine, by\neasily exposing data to non-technical users for discovery and analysis. In\norder to close this gap we demonstrate that training contrastive image-text\nencoders on explicitly commentative pairs results in large improvements in\nretrieval results, with the results extending across a variety of non-English\nlanguages.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-34.900733947753906, -36.97151565551758], "cluster": 5}, {"key": "thoma2020geometrically", "year": "2020", "citations": "2", "title": "Geometrically Mappable Image Features", "abstract": "<p>Vision-based localization of an agent in a map is an important problem in\nrobotics and computer vision. In that context, localization by learning\nmatchable image features is gaining popularity due to recent advances in\nmachine learning. Features that uniquely describe the visual contents of images\nhave a wide range of applications, including image retrieval and understanding.\nIn this work, we propose a method that learns image features targeted for\nimage-retrieval-based localization. Retrieval-based localization has several\nbenefits, such as easy maintenance and quick computation. However, the\nstate-of-the-art features only provide visual similarity scores which do not\nexplicitly reveal the geometric distance between query and retrieved images.\nKnowing this distance is highly desirable for accurate localization, especially\nwhen the reference images are sparsely distributed in the scene. Therefore, we\npropose a novel loss function for learning image features which are both\nvisually representative and geometrically relatable. This is achieved by\nguiding the learning process such that the feature and geometric distances\nbetween images are directly proportional. In our experiments we show that our\nfeatures not only offer significantly better localization accuracy, but also\nallow to estimate the trajectory of a query sequence in absence of the\nreference images.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-30.112266540527344, -8.908256530761719], "cluster": 5}, {"key": "thomas2020preserving", "year": "2020", "citations": "27", "title": "Preserving Semantic Neighborhoods For Robust Cross-modal Retrieval", "abstract": "<p>The abundance of multimodal data (e.g. social media posts) has inspired\ninterest in cross-modal retrieval methods. Popular approaches rely on a variety\nof metric learning losses, which prescribe what the proximity of image and text\nshould be, in the learned space. However, most prior methods have focused on\nthe case where image and text convey redundant information; in contrast,\nreal-world image-text pairs convey complementary information with little\noverlap. Further, images in news articles and media portray topics in a\nvisually diverse fashion; thus, we need to take special care to ensure a\nmeaningful image representation. We propose novel within-modality losses which\nencourage semantic coherency in both the text and image subspaces, which does\nnot necessarily align with visual coherency. Our method ensures that not only\nare paired images and texts close, but the expected image-image and text-text\nrelationships are also observed. Our approach improves the results of\ncross-modal retrieval on four datasets compared to five baselines.</p>\n", "tags": ["Multimodal-Retrieval", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-22.16010856628418, -4.575699329376221], "cluster": 1}, {"key": "thomasian2024dimensionality", "year": "2024", "citations": "0", "title": "Dimensionality Reduced Clustered Data And Order Partition And Stepwise Dimensionality Increasing Indices", "abstract": "<p>One of the goals of NASA funded project at IBM T. J. Watson Research Center\nwas to build an index for similarity searching satellite images, which were\ncharacterized by high-dimensional feature image texture vectors. Reviewed is\nour effort on data clustering, dimensionality reduction via Singular Value\nDecomposition - SVD and indexing to build a smaller index and more efficient\nk-Nearest Neighbor - k-NN query processing for similarity search. k-NN queries\nbased on scanning of the feature vectors of all images is obviously too costly\nfor ever-increasing number of images. The ubiquitous multidimensional R-tree\nindex and its extensions were not an option given their limited scalability\ndimension-wise. The cost of processing k-NN queries was further reduced by\nbuilding memory resident Ordered Partition indices on dimensionality reduced\nclusters. Further research in a university setting included the following: (1)\nClustered SVD was extended to yield exact k-NN queries by issuing appropriate\nless costly range queries, (2) Stepwise Dimensionality Increasing - SDI index\noutperformed other known indices, (3) selection of optimal number of dimensions\nto reduce query processing cost, (4) two methods to make the OP-trees\npersistent and loadable as a single file access.</p>\n", "tags": ["Similarity-Search", "Scalability"], "tsne_embedding": [23.999448776245117, 28.508811950683594], "cluster": 2}, {"key": "thoreau2018deep", "year": "2018", "citations": "5", "title": "Deep Similarity Metric Learning For Real-time Pedestrian Tracking", "abstract": "<p>Tracking by detection is a common approach to solving the Multiple Object\nTracking problem. In this paper we show how learning a deep similarity metric\ncan improve three key aspects of pedestrian tracking on a multiple object\ntracking benchmark. We train a convolutional neural network to learn an\nembedding function in a Siamese configuration on a large person\nre-identification dataset. The offline-trained embedding network is integrated\nin to the tracking formulation to improve performance while retaining real-time\nperformance. The proposed tracker stores appearance metrics while detections\nare strong, using this appearance information to: prevent ID switches,\nassociate tracklets through occlusion, and propose new detections where\ndetector confidence is low. This method achieves competitive results in\nevaluation, especially among online, real-time approaches. We present an\nablative study showing the impact of each of the three uses of our deep\nappearance metric.</p>\n", "tags": ["Efficiency", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-30.894954681396484, -44.43565368652344], "cluster": 5}, {"key": "thottingal2025question", "year": "2025", "citations": "0", "title": "Question-to-question Retrieval For Hallucination-free Knowledge Access: An Approach For Wikipedia And Wikidata Question Answering", "abstract": "<p>This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \u201cquestion-to-question\u201d matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( &gt;\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering.</p>\n", "tags": ["Efficiency", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [7.599511623382568, -31.3541259765625], "cluster": 7}, {"key": "tian2017semi", "year": "2017", "citations": "2", "title": "Semi-supervised Multimodal Hashing", "abstract": "<p>Retrieving nearest neighbors across correlated data in multiple modalities,\nsuch as image-text pairs on Facebook and video-tag pairs on YouTube, has become\na challenging task due to the huge amount of data. Multimodal hashing methods\nthat embed data into binary codes can boost the retrieving speed and reduce\nstorage requirement. As unsupervised multimodal hashing methods are usually\ninferior to supervised ones, while the supervised ones requires too much\nmanually labeled data, the proposed method in this paper utilizes a part of\nlabels to design a semi-supervised multimodal hashing method. It first computes\nthe transformation matrices for data matrices and label matrix. Then, with\nthese transformation matrices, fuzzy logic is introduced to estimate a label\nmatrix for unlabeled data. Finally, it uses the estimated label matrix to learn\nhashing functions for data in each modality to generate a unified binary code\nmatrix. Experiments show that the proposed semi-supervised method with 50%\nlabels can get a medium performance among the compared supervised ones and\nachieve an approximate performance to the best supervised method with 90%\nlabels. With only 10% labels, the proposed method can still compete with the\nworst compared supervised one.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [3.8973467350006104, -2.571699857711792], "cluster": 6}, {"key": "tian2018learning", "year": "2018", "citations": "0", "title": "Learning Decorrelated Hashing Codes For Multimodal Retrieval", "abstract": "<p>In social networks, heterogeneous multimedia data correlate to each other,\nsuch as videos and their corresponding tags in YouTube and image-text pairs in\nFacebook. Nearest neighbor retrieval across multiple modalities on large data\nsets becomes a hot yet challenging problem. Hashing is expected to be an\nefficient solution, since it represents data as binary codes. As the bit-wise\nXOR operations can be fast handled, the retrieval time is greatly reduced. Few\nexisting multimodal hashing methods consider the correlation among hashing\nbits. The correlation has negative impact on hashing codes. When the hashing\ncode length becomes longer, the retrieval performance improvement becomes\nslower. In this paper, we propose a minimum correlation regularization (MCR)\nfor multimodal hashing. First, the sigmoid function is used to embed the data\nmatrices. Then, the MCR is applied on the output of sigmoid function. As the\noutput of sigmoid function approximates a binary code matrix, the proposed MCR\ncan efficiently decorrelate the hashing codes. Experiments show the superiority\nof the proposed method becomes greater as the code length increases.</p>\n", "tags": ["Compact-Codes", "Multimodal-Retrieval", "Evaluation", "Hashing-Methods"], "tsne_embedding": [14.379612922668457, 26.571998596191406], "cluster": 4}, {"key": "tian2019global", "year": "2016", "citations": "14", "title": "Global Hashing System For Fast Image Search", "abstract": "<p>Hashing methods have been widely investigated for fast approximate nearest\nneighbor searching in large data sets. Most existing methods use binary vectors\nin lower dimensional spaces to represent data points that are usually real\nvectors of higher dimensionality. We divide the hashing process into two steps.\nData points are first embedded in a low-dimensional space, and the global\npositioning system method is subsequently introduced but modified for binary\nembedding. We devise dataindependent and data-dependent methods to distribute\nthe satellites at appropriate locations. Our methods are based on finding the\ntradeoff between the information losses in these two steps. Experiments show\nthat our data-dependent method outperforms other methods in different-sized\ndata sets from 100k to 10M. By incorporating the orthogonality of the code\nmatrix, both our data-independent and data-dependent methods are particularly\nimpressive in experiments on longer bits.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval"], "tsne_embedding": [-0.525147020816803, 39.64328384399414], "cluster": 4}, {"key": "tian2019sosnet", "year": "2019", "citations": "320", "title": "Sosnet: Second Order Similarity Regularization For Local Descriptor Learning", "abstract": "<p>Despite the fact that Second Order Similarity (SOS) has been used with\nsignificant success in tasks such as graph matching and clustering, it has not\nbeen exploited for learning local descriptors. In this work, we explore the\npotential of SOS in the field of descriptor learning by building upon the\nintuition that a positive pair of matching points should exhibit similar\ndistances with respect to other points in the embedding space. Thus, we propose\na novel regularization term, named Second Order Similarity Regularization\n(SOSR), that follows this principle. By incorporating SOSR into training, our\nlearned descriptor achieves state-of-the-art performance on several challenging\nbenchmarks containing distinct tasks ranging from local patch retrieval to\nstructure from motion. Furthermore, by designing a von Mises-Fischer\ndistribution based evaluation method, we link the utilization of the descriptor\nspace to the matching performance, thus demonstrating the effectiveness of our\nproposed SOSR. Extensive experimental results, empirical evidence, and in-depth\nanalysis are provided, indicating that SOSR can significantly boost the\nmatching performance of the learned descriptor.</p>\n", "tags": ["CVPR", "Evaluation"], "tsne_embedding": [48.075469970703125, 1.5517113208770752], "cluster": 9}, {"key": "tian2020hynet", "year": "2020", "citations": "33", "title": "Hynet: Learning Local Descriptor With Hybrid Similarity Measure And Triplet Loss", "abstract": "<p>Recent works show that local descriptor learning benefits from the use of L2\nnormalisation, however, an in-depth analysis of this effect lacks in the\nliterature. In this paper, we investigate how L2 normalisation affects the\nback-propagated descriptor gradients during training. Based on our\nobservations, we propose HyNet, a new local descriptor that leads to\nstate-of-the-art results in matching. HyNet introduces a hybrid similarity\nmeasure for triplet margin loss, a regularisation term constraining the\ndescriptor norm, and a new network architecture that performs L2 normalisation\nof all intermediate feature maps and the output descriptors. HyNet surpasses\nprevious methods by a significant margin on standard benchmarks that include\npatch matching, verification, and retrieval, as well as outperforming full\nend-to-end methods on 3D reconstruction tasks.</p>\n", "tags": ["Distance-Metric-Learning"], "tsne_embedding": [-32.68516540527344, -6.493035316467285], "cluster": 0}, {"key": "tian2022learned", "year": "2022", "citations": "12", "title": "A Learned Index For Exact Similarity Search In Metric Spaces", "abstract": "<p>Indexing is an effective way to support efficient query processing in large\ndatabases. Recently the concept of learned index, which replaces or complements\ntraditional index structures with machine learning models, has been actively\nexplored to reduce storage and search costs. However, accurate and efficient\nsimilarity query processing in high-dimensional metric spaces remains to be an\nopen challenge. In this paper, we propose a novel indexing approach called LIMS\nthat uses data clustering, pivot-based data transformation techniques and\nlearned indexes to support efficient similarity query processing in metric\nspaces. In LIMS, the underlying data is partitioned into clusters such that\neach cluster follows a relatively uniform data distribution. Data\nredistribution is achieved by utilizing a small number of pivots for each\ncluster. Similar data are mapped into compact regions and the mapped values are\ntotally ordinal. Machine learning models are developed to approximate the\nposition of each data record on disk. Efficient algorithms are designed for\nprocessing range queries and nearest neighbor queries based on LIMS, and for\nindex maintenance with dynamic updates. Extensive experiments on real-world and\nsynthetic datasets demonstrate the superiority of LIMS compared with\ntraditional indexes and state-of-the-art learned indexes.</p>\n", "tags": ["Vector-Indexing", "Similarity-Search", "Datasets"], "tsne_embedding": [26.015382766723633, 19.895076751708984], "cluster": 2}, {"key": "tian2024fusionanns", "year": "2024", "citations": "0", "title": "Fusionanns: An Efficient CPU/GPU Cooperative Processing Architecture For Billion-scale Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor search (ANNS) has emerged as a crucial component\nof database and AI infrastructure. Ever-increasing vector datasets pose\nsignificant challenges in terms of performance, cost, and accuracy for ANNS\nservices. None of modern ANNS systems can address these issues simultaneously.\nWe present FusionANNS, a high-throughput, low-latency, cost-efficient, and\nhigh-accuracy ANNS system for billion-scale datasets using SSDs and only one\nentry-level GPU. The key idea of FusionANNS lies in CPU/GPU collaborative\nfiltering and re-ranking mechanisms, which significantly reduce I/O operations\nacross CPUs, GPU, and SSDs to break through the I/O performance bottleneck.\nSpecifically, we propose three novel designs: (1) multi-tiered indexing to\navoid data swapping between CPUs and GPU, (2) heuristic re-ranking to eliminate\nunnecessary I/Os and computations while guaranteeing high accuracy, and (3)\nredundant-aware I/O deduplication to further improve I/O efficiency. We\nimplement FusionANNS and compare it with the state-of-the-art SSD-based ANNS\nsystem \u2013 SPANN and GPU-accelerated in-memory ANNS system \u2013 RUMMY.\nExperimental results show that FusionANNS achieves 1) 9.4-13.1X higher query\nper second (QPS) and 5.7-8.8X higher cost efficiency compared with SPANN; 2)\nand 2-4.9X higher QPS and 2.3-6.8X higher cost efficiency compared with RUMMY,\nwhile guaranteeing low latency and high accuracy.</p>\n", "tags": ["Efficiency", "Scalability", "Large-Scale-Search", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [38.98933029174805, 19.317428588867188], "cluster": 2}, {"key": "tien2024improving", "year": "2025", "citations": "0", "title": "Improving Vietnamese Legal Document Retrieval Using Synthetic Data", "abstract": "<p>In the field of legal information retrieval, effective embedding-based models\nare essential for accurate question-answering systems. However, the scarcity of\nlarge annotated datasets poses a significant challenge, particularly for\nVietnamese legal texts. To address this issue, we propose a novel approach that\nleverages large language models to generate high-quality, diverse synthetic\nqueries for Vietnamese legal passages. This synthetic data is then used to\npre-train retrieval models, specifically bi-encoder and ColBERT, which are\nfurther fine-tuned using contrastive loss with mined hard negatives. Our\nexperiments demonstrate that these enhancements lead to strong improvement in\nretrieval accuracy, validating the effectiveness of synthetic data and\npre-training techniques in overcoming the limitations posed by the lack of\nlarge labeled datasets in the Vietnamese legal domain.</p>\n", "tags": ["Distance-Metric-Learning", "Text-Retrieval", "Datasets"], "tsne_embedding": [2.3534038066864014, -41.018272399902344], "cluster": 3}, {"key": "tien2025improving", "year": "2025", "citations": "0", "title": "Improving Vietnamese Legal Document Retrieval Using Synthetic Data", "abstract": "<p>In the field of legal information retrieval, effective embedding-based models\nare essential for accurate question-answering systems. However, the scarcity of\nlarge annotated datasets poses a significant challenge, particularly for\nVietnamese legal texts. To address this issue, we propose a novel approach that\nleverages large language models to generate high-quality, diverse synthetic\nqueries for Vietnamese legal passages. This synthetic data is then used to\npre-train retrieval models, specifically bi-encoder and ColBERT, which are\nfurther fine-tuned using contrastive loss with mined hard negatives. Our\nexperiments demonstrate that these enhancements lead to strong improvement in\nretrieval accuracy, validating the effectiveness of synthetic data and\npre-training techniques in overcoming the limitations posed by the lack of\nlarge labeled datasets in the Vietnamese legal domain.</p>\n", "tags": ["Distance-Metric-Learning", "Text-Retrieval", "Datasets"], "tsne_embedding": [2.3534038066864014, -41.018272399902344], "cluster": 3}, {"key": "tissier2018near", "year": "2019", "citations": "43", "title": "Near-lossless Binarization Of Word Embeddings", "abstract": "<p>Word embeddings are commonly used as a starting point in many NLP models to\nachieve state-of-the-art performances. However, with a large vocabulary and\nmany dimensions, these floating-point representations are expensive both in\nterms of memory and calculations which makes them unsuitable for use on\nlow-resource devices. The method proposed in this paper transforms real-valued\nembeddings into binary embeddings while preserving semantic information,\nrequiring only 128 or 256 bits for each vector. This leads to a small memory\nfootprint and fast vector operations. The model is based on an autoencoder\narchitecture, which also allows to reconstruct original vectors from the binary\nones. Experimental results on semantic similarity, text classification and\nsentiment analysis tasks show that the binarization of word embeddings only\nleads to a loss of ~2% in accuracy while vector size is reduced by 97%.\nFurthermore, a top-k benchmark demonstrates that using these binary vectors is\n30 times faster than using real-valued vectors.</p>\n", "tags": ["AAAI", "Evaluation", "Hashing-Methods"], "tsne_embedding": [29.703968048095703, 21.908443450927734], "cluster": 2}, {"key": "titus2018sig", "year": "2018", "citations": "12", "title": "SIG-DB: Leveraging Homomorphic Encryption To Securely Interrogate Privately Held Genomic Databases", "abstract": "<p>Genomic data are becoming increasingly valuable as we develop methods to\nutilize the information at scale and gain a greater understanding of how\ngenetic information relates to biological function. Advances in synthetic\nbiology and the decreased cost of sequencing are increasing the amount of\nprivately held genomic data. As the quantity and value of private genomic data\ngrows, so does the incentive to acquire and protect such data, which creates a\nneed to store and process these data securely. We present an algorithm for the\nSecure Interrogation of Genomic DataBases (SIG-DB). The SIG-DB algorithm\nenables databases of genomic sequences to be searched with an encrypted query\nsequence without revealing the query sequence to the Database Owner or any of\nthe database sequences to the Querier. SIG-DB is the first application of its\nkind to take advantage of locality-sensitive hashing and homomorphic encryption\nto allow generalized sequence-to-sequence comparisons of genomic data.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [-46.54095458984375, 22.635284423828125], "cluster": 0}, {"key": "tiwari2020visually", "year": "2020", "citations": "0", "title": "Visually Aware Skip-gram For Image Based Recommendations", "abstract": "<p>The visual appearance of a product significantly influences purchase\ndecisions on e-commerce websites. We propose a novel framework VASG (Visually\nAware Skip-Gram) for learning user and product representations in a common\nlatent space using product image features. Our model is an amalgamation of the\nSkip-Gram architecture and a deep neural network based Decoder. Here the\nSkip-Gram attempts to capture user preference by optimizing user-product\nco-occurrence in a Heterogeneous Information Network while the Decoder\nsimultaneously learns a mapping to transform product image features to the\nSkip-Gram embedding space. This architecture is jointly optimized in an\nend-to-end, multitask fashion. The proposed framework enables us to make\npersonalized recommendations for cold-start products which have no purchase\nhistory. Experiments conducted on large real-world datasets show that the\nlearned embeddings can generate effective recommendations using nearest\nneighbour searches.</p>\n", "tags": ["Tools-&-Libraries", "Datasets"], "tsne_embedding": [-15.317727088928223, -45.78126907348633], "cluster": 3}, {"key": "tizhoosh2016barcodes", "year": "2016", "citations": "16", "title": "Barcodes For Medical Image Retrieval Using Autoencoded Radon Transform", "abstract": "<p>Using content-based binary codes to tag digital images has emerged as a\npromising retrieval technology. Recently, Radon barcodes (RBCs) have been\nintroduced as a new binary descriptor for image search. RBCs are generated by\nbinarization of Radon projections and by assembling them into a vector, namely\nthe barcode. A simple local thresholding has been suggested for binarization.\nIn this paper, we put forward the idea of \u201cautoencoded Radon barcodes\u201d. Using\nimages in a training dataset, we autoencode Radon projections to perform\nbinarization on outputs of hidden layers. We employed the mini-batch stochastic\ngradient descent approach for the training. Each hidden layer of the\nautoencoder can produce a barcode using a threshold determined based on the\nrange of the logistic function used. The compressing capability of autoencoders\napparently reduces the redundancies inherent in Radon projections leading to\nmore accurate retrieval results. The IRMA dataset with 14,410 x-ray images is\nused to validate the performance of the proposed method. The experimental\nresults, containing comparison with RBCs, SURF and BRISK, show that autoencoded\nRadon barcode (ARBC) has the capacity to capture important information and to\nlearn richer representations resulting in lower retrieval errors for image\nretrieval measured with the accuracy of the first hit only.</p>\n", "tags": ["Compact-Codes", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-48.291133880615234, 11.862964630126953], "cluster": 0}, {"key": "tizhoosh2016minmax", "year": "2016", "citations": "38", "title": "Minmax Radon Barcodes For Medical Image Retrieval", "abstract": "<p>Content-based medical image retrieval can support diagnostic decisions by\nclinical experts. Examining similar images may provide clues to the expert to\nremove uncertainties in his/her final diagnosis. Beyond conventional feature\ndescriptors, binary features in different ways have been recently proposed to\nencode the image content. A recent proposal is \u201cRadon barcodes\u201d that employ\nbinarized Radon projections to tag/annotate medical images with content-based\nbinary vectors, called barcodes. In this paper, MinMax Radon barcodes are\nintroduced which are superior to \u201clocal thresholding\u201d scheme suggested in the\nliterature. Using IRMA dataset with 14,410 x-ray images from 193 different\nclasses, the advantage of using MinMax Radon barcodes over <em>thresholded</em>\nRadon barcodes are demonstrated. The retrieval error for direct search drops by\nmore than 15%. As well, SURF, as a well-established non-binary approach, and\nBRISK, as a recent binary method are examined to compare their results with\nMinMax Radon barcodes when retrieving images from IRMA dataset. The results\ndemonstrate that MinMax Radon barcodes are faster and more accurate when\napplied on IRMA images.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [-51.03110885620117, 13.120081901550293], "cluster": 0}, {"key": "tolias2017asymmetric", "year": "2017", "citations": "39", "title": "Asymmetric Feature Maps With Application To Sketch Based Retrieval", "abstract": "<p>We propose a novel concept of asymmetric feature maps (AFM), which allows to\nevaluate multiple kernels between a query and database entries without\nincreasing the memory requirements. To demonstrate the advantages of the AFM\nmethod, we derive a short vector image representation that, due to asymmetric\nfeature maps, supports efficient scale and translation invariant sketch-based\nimage retrieval. Unlike most of the short-code based retrieval systems, the\nproposed method provides the query localization in the retrieved image. The\nefficiency of the search is boosted by approximating a 2D translation search\nvia trigonometric polynomial of scores by 1D projections. The projections are a\nspecial case of AFM. An order of magnitude speed-up is achieved compared to\ntraditional trigonometric polynomials. The results are boosted by an\nimage-based average query expansion, exceeding significantly the state of the\nart on standard benchmarks.</p>\n", "tags": ["Efficiency", "CVPR", "Image-Retrieval"], "tsne_embedding": [10.01491641998291, 37.84649658203125], "cluster": 4}, {"key": "tolias2020learning", "year": "2020", "citations": "82", "title": "Learning And Aggregating Deep Local Descriptors For Instance-level Recognition", "abstract": "<p>We propose an efficient method to learn deep local descriptors for\ninstance-level recognition. The training only requires examples of positive and\nnegative image pairs and is performed as metric learning of sum-pooled global\nimage descriptors. At inference, the local descriptors are provided by the\nactivations of internal components of the network. We demonstrate why such an\napproach learns local descriptors that work well for image similarity\nestimation with classical efficient match kernel methods. The experimental\nvalidation studies the trade-off between performance and memory requirements of\nthe state-of-the-art image search approach based on match kernels. Compared to\nexisting local descriptors, the proposed ones perform better in two\ninstance-level recognition tasks and keep memory requirements lower. We\nexperimentally show that global descriptors are not effective enough at large\nscale and that local descriptors are essential. We achieve state-of-the-art\nperformance, in some cases even with a backbone network as small as ResNet18.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-35.808536529541016, -0.5135863423347473], "cluster": 0}, {"key": "tonellotto2021query", "year": "2021", "citations": "16", "title": "Query Embedding Pruning For Dense Retrieval", "abstract": "<p>Recent advances in dense retrieval techniques have offered the promise of\nbeing able not just to re-rank documents using contextualised language models\nsuch as BERT, but also to use such models to identify documents from the\ncollection in the first place. However, when using dense retrieval approaches\nthat use multiple embedded representations for each query, a large number of\ndocuments can be retrieved for each query, hindering the efficiency of the\nmethod. Hence, this work is the first to consider efficiency improvements in\nthe context of a dense retrieval approach (namely ColBERT), by pruning query\nterm embeddings that are estimated not to be useful for retrieving relevant\ndocuments. Our proposed query embeddings pruning reduces the cost of the dense\nretrieval operation, as well as reducing the number of documents that are\nretrieved and hence require to be fully scored. Experiments conducted on the\nMSMARCO passage ranking corpus demonstrate that, when reducing the number of\nquery embeddings used from 32 to 3 based on the collection frequency of the\ncorresponding tokens, query embedding pruning results in no statistically\nsignificant differences in effectiveness, while reducing the number of\ndocuments retrieved by 70%. In terms of mean response time for the end-to-end\nto end system, this results in a 2.65x speedup.</p>\n", "tags": ["CIKM", "Efficiency"], "tsne_embedding": [17.68435287475586, 22.610971450805664], "cluster": 2}, {"key": "tonioni2018deep", "year": "2018", "citations": "43", "title": "A Deep Learning Pipeline For Product Recognition On Store Shelves", "abstract": "<p>Recognition of grocery products in store shelves poses peculiar challenges.\nFirstly, the task mandates the recognition of an extremely high number of\ndifferent items, in the order of several thousands for medium-small shops, with\nmany of them featuring small inter and intra class variability. Then, available\nproduct databases usually include just one or a few studio-quality images per\nproduct (referred to herein as reference images), whilst at test time\nrecognition is performed on pictures displaying a portion of a shelf containing\nseveral products and taken in the store by cheap cameras (referred to as query\nimages). Moreover, as the items on sale in a store as well as their appearance\nchange frequently over time, a practical recognition system should handle\nseamlessly new products/packages. Inspired by recent advances in object\ndetection and image retrieval, we propose to leverage on state of the art\nobject detectors based on deep learning to obtain an initial productagnostic\nitem detection. Then, we pursue product recognition through a similarity search\nbetween global descriptors computed on reference and cropped query images. To\nmaximize performance, we learn an ad-hoc global descriptor by a CNN trained on\nreference images based on an image embedding loss. Our system is\ncomputationally expensive at training time but can perform recognition rapidly\nand accurately at test time.</p>\n", "tags": ["Evaluation", "Similarity-Search", "Image-Retrieval"], "tsne_embedding": [-12.16225814819336, -47.19338607788086], "cluster": 3}, {"key": "torres2021compact", "year": "2021", "citations": "12", "title": "Compact And Effective Representations For Sketch-based Image Retrieval", "abstract": "<p>Sketch-based image retrieval (SBIR) has undergone an increasing interest in\nthe community of computer vision bringing high impact in real applications. For\ninstance, SBIR brings an increased benefit to eCommerce search engines because\nit allows users to formulate a query just by drawing what they need to buy.\nHowever, current methods showing high precision in retrieval work in a high\ndimensional space, which negatively affects aspects like memory consumption and\ntime processing. Although some authors have also proposed compact\nrepresentations, these drastically degrade the performance in a low dimension.\nTherefore in this work, we present different results of evaluating methods for\nproducing compact embeddings in the context of sketch-based image retrieval.\nOur main interest is in strategies aiming to keep the local structure of the\noriginal space. The recent unsupervised local-topology preserving dimension\nreduction method UMAP fits our requirements and shows outstanding performance,\nimproving even the precision achieved by SOTA methods. We evaluate six methods\nin two different datasets. We use Flickr15K and eCommerce datasets; the latter\nis another contribution of this work. We show that UMAP allows us to have\nfeature vectors of 16 bytes improving precision by more than 35%.</p>\n", "tags": ["CVPR", "Image-Retrieval", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [-38.371212005615234, -4.634034156799316], "cluster": 0}, {"key": "torresxirau2015fast", "year": "2015", "citations": "3", "title": "Fast Approximate Nearest-neighbor Field By Cascaded Spherical Hashing", "abstract": "<p>We present an efficient and fast algorithm for computing approximate nearest neighbor fields between two images. Our method builds on the concept of Coherency-Sensitive Hashing (CSH), but uses a recent hashing scheme, Spherical Hashing (SpH), which is known to be better adapted to the nearest-neighbor problem for natural images. Cascaded Spherical Hashing concatenates different configurations of SpH to build larger Hash Tables with less elements in each bin to achieve higher selectivity. Our method amply outperforms existing techniques like PatchMatch and CSH, and the experimental results show that our algorithm is faster and more accurate than existing methods.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [7.8548808097839355, 37.62453842163086], "cluster": 4}, {"key": "torresxirau2025fast", "year": "2015", "citations": "3", "title": "Fast Approximate Nearest-neighbor Field By Cascaded Spherical Hashing", "abstract": "<p>We present an efficient and fast algorithm for computing approximate nearest neighbor fields between two images. Our method builds on the concept of Coherency-Sensitive Hashing (CSH), but uses a recent hashing scheme, Spherical Hashing (SpH), which is known to be better adapted to the nearest-neighbor problem for natural images. Cascaded Spherical Hashing concatenates different configurations of SpH to build larger Hash Tables with less elements in each bin to achieve higher selectivity. Our method amply outperforms existing techniques like PatchMatch and CSH, and the experimental results show that our algorithm is faster and more accurate than existing methods.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [7.8548808097839355, 37.62453842163086], "cluster": 4}, {"key": "touvron2020grafit", "year": "2021", "citations": "57", "title": "Grafit: Learning Fine-grained Image Representations With Coarse Labels", "abstract": "<p>This paper tackles the problem of learning a finer representation than the\none provided by training labels. This enables fine-grained category retrieval\nof images in a collection annotated with coarse labels only.\n  Our network is learned with a nearest-neighbor classifier objective, and an\ninstance loss inspired by self-supervised learning. By jointly leveraging the\ncoarse labels and the underlying fine-grained latent space, it significantly\nimproves the accuracy of category-level retrieval methods.\n  Our strategy outperforms all competing methods for retrieving or classifying\nimages at a finer granularity than that available at train time. It also\nimproves the accuracy for transfer learning tasks to fine-grained datasets,\nthereby establishing the new state of the art on five public benchmarks, like\niNaturalist-2018.</p>\n", "tags": ["Supervised", "ICCV", "Self-Supervised", "Datasets"], "tsne_embedding": [-15.610045433044434, -16.480134963989258], "cluster": 1}, {"key": "tran2016learning", "year": "2016", "citations": "1", "title": "Learning Deep Representation Of Multityped Objects And Tasks", "abstract": "<p>We introduce a deep multitask architecture to integrate multityped\nrepresentations of multimodal objects. This multitype exposition is less\nabstract than the multimodal characterization, but more machine-friendly, and\nthus is more precise to model. For example, an image can be described by\nmultiple visual views, which can be in the forms of bag-of-words (counts) or\ncolor/texture histograms (real-valued). At the same time, the image may have\nseveral social tags, which are best described using a sparse binary vector. Our\ndeep model takes as input multiple type-specific features, narrows the\ncross-modality semantic gaps, learns cross-type correlation, and produces a\nhigh-level homogeneous representation. At the same time, the model supports\nheterogeneously typed tasks. We demonstrate the capacity of the model on two\napplications: social image retrieval and multiple concept prediction. The deep\narchitecture produces more compact representation, naturally integrates\nmultiviews and multimodalities, exploits better side information, and most\nimportantly, performs competitively against baselines.</p>\n", "tags": ["Image-Retrieval"], "tsne_embedding": [0.7261580228805542, -15.133540153503418], "cluster": 1}, {"key": "tran2018device", "year": "2018", "citations": "38", "title": "On-device Scalable Image-based Localization Via Prioritized Cascade Search And Fast One-many RANSAC", "abstract": "<p>We present the design of an entire on-device system for large-scale urban\nlocalization using images. The proposed design integrates compact image\nretrieval and 2D-3D correspondence search to estimate the location in extensive\ncity regions. Our design is GPS agnostic and does not require network\nconnection. In order to overcome the resource constraints of mobile devices, we\npropose a system design that leverages the scalability advantage of image\nretrieval and accuracy of 3D model-based localization. Furthermore, we propose\na new hashing-based cascade search for fast computation of 2D-3D\ncorrespondences. In addition, we propose a new one-many RANSAC for accurate\npose estimation. The new one-many RANSAC addresses the challenge of repetitive\nbuilding structures (e.g. windows, balconies) in urban localization. Extensive\nexperiments demonstrate that our 2D-3D correspondence search achieves\nstate-of-the-art localization accuracy on multiple benchmark datasets.\nFurthermore, our experiments on a large Google Street View (GSV) image dataset\nshow the potential of large-scale localization entirely on a typical mobile\ndevice.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [-7.621427059173584, -53.16103744506836], "cluster": 3}, {"key": "tran2019searching", "year": "2019", "citations": "4", "title": "Searching For Apparel Products From Images In The Wild", "abstract": "<p>In this age of social media, people often look at what others are wearing. In\nparticular, Instagram and Twitter influencers often provide images of\nthemselves wearing different outfits and their followers are often inspired to\nbuy similar clothes.We propose a system to automatically find the closest\nvisually similar clothes in the online Catalog (street-to-shop searching). The\nproblem is challenging since the original images are taken under different pose\nand lighting conditions. The system initially localizes high-level descriptive\nregions (top, bottom, wristwear. . . ) using multiple CNN detectors such as\nYOLO and SSD that are trained specifically for apparel domain. It then\nclassifies these regions into more specific regions such as t-shirts, tunic or\ndresses. Finally, a feature embedding learned using a multi-task function is\nrecovered for every item and then compared with corresponding items in the\nonline Catalog database and ranked according to distance. We validate our\napproach component-wise using benchmark datasets and end-to-end using human\nevaluation.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-19.05221939086914, -48.34934616088867], "cluster": 3}, {"key": "tripathi2024honeybee", "year": "2024", "citations": "3", "title": "Honeybee: A Scalable Modular Framework For Creating Multimodal Oncology Datasets With Foundational Embedding Models", "abstract": "<p>Developing accurate machine learning models for oncology requires\nlarge-scale, high-quality multimodal datasets. However, creating such datasets\nremains challenging due to the complexity and heterogeneity of medical data. To\naddress this challenge, we introduce HoneyBee, a scalable modular framework for\nbuilding multimodal oncology datasets that leverages foundation models to\ngenerate representative embeddings. HoneyBee integrates various data\nmodalities, including clinical diagnostic and pathology imaging data, medical\nnotes, reports, records, and molecular data. It employs data preprocessing\ntechniques and foundation models to generate embeddings that capture the\nessential features and relationships within the raw medical data. The generated\nembeddings are stored in a structured format using Hugging Face datasets and\nPyTorch dataloaders for accessibility. Vector databases enable efficient\nquerying and retrieval for machine learning applications. We demonstrate the\neffectiveness of HoneyBee through experiments assessing the quality and\nrepresentativeness of these embeddings. The framework is designed to be\nextensible to other medical domains and aims to accelerate oncology research by\nproviding high-quality, machine learning-ready datasets. HoneyBee is an ongoing\nopen-source effort, and the code, datasets, and models are available at the\nproject repository.</p>\n", "tags": ["Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [-52.137020111083984, 17.44170379638672], "cluster": 0}, {"key": "trivigno2023divide", "year": "2023", "citations": "4", "title": "Divide&classify: Fine-grained Classification For City-wide Visual Place Recognition", "abstract": "<p>Visual Place recognition is commonly addressed as an image retrieval problem.\nHowever, retrieval methods are impractical to scale to large datasets, densely\nsampled from city-wide maps, since their dimension impact negatively on the\ninference time. Using approximate nearest neighbour search for retrieval helps\nto mitigate this issue, at the cost of a performance drop. In this paper we\ninvestigate whether we can effectively approach this task as a classification\nproblem, thus bypassing the need for a similarity search. We find that existing\nclassification methods for coarse, planet-wide localization are not suitable\nfor the fine-grained and city-wide setting. This is largely due to how the\ndataset is split into classes, because these methods are designed to handle a\nsparse distribution of photos and as such do not consider the visual aliasing\nproblem across neighbouring classes that naturally arises in dense scenarios.\nThus, we propose a partitioning scheme that enables a fast and accurate\ninference, preserving a simple learning procedure, and a novel inference\npipeline based on an ensemble of novel classifiers that uses the prototypes\nlearned via an angular margin loss. Our method, Divide&amp;Classify (D&amp;C), enjoys\nthe fast inference of classification solutions and an accuracy competitive with\nretrieval methods on the fine-grained, city-wide setting. Moreover, we show\nthat D&amp;C can be paired with existing retrieval pipelines to speed up\ncomputations by over 20 times while increasing their recall, leading to new\nstate-of-the-art results.</p>\n", "tags": ["ICCV", "Similarity-Search", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-5.607936382293701, 44.11043930053711], "cluster": 4}, {"key": "truong2023benchmarking", "year": "2024", "citations": "2", "title": "Benchmarking Pretrained Vision Embeddings For Near- And Duplicate Detection In Medical Images", "abstract": "<p>Near- and duplicate image detection is a critical concern in the field of\nmedical imaging. Medical datasets often contain similar or duplicate images\nfrom various sources, which can lead to significant performance issues and\nevaluation biases, especially in machine learning tasks due to data leakage\nbetween training and testing subsets. In this paper, we present an approach for\nidentifying near- and duplicate 3D medical images leveraging publicly available\n2D computer vision embeddings. We assessed our approach by comparing embeddings\nextracted from two state-of-the-art self-supervised pretrained models and two\ndifferent vector index structures for similarity retrieval. We generate an\nexperimental benchmark based on the publicly available Medical Segmentation\nDecathlon dataset. The proposed method yields promising results for near- and\nduplicate image detection achieving a mean sensitivity and specificity of\n0.9645 and 0.8559, respectively.</p>\n", "tags": ["Self-Supervised", "Vector-Indexing", "Similarity-Search", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-53.8068962097168, 13.558349609375], "cluster": 0}, {"key": "tsai2017learning", "year": "2017", "citations": "148", "title": "Learning Robust Visual-semantic Embeddings", "abstract": "<p>Many of the existing methods for learning joint embedding of images and text\nuse only supervised information from paired images and its textual attributes.\nTaking advantage of the recent success of unsupervised learning in deep neural\nnetworks, we propose an end-to-end learning framework that is able to extract\nmore robust multi-modal representations across domains. The proposed method\ncombines representation learning models (i.e., auto-encoders) together with\ncross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn\njoint embeddings for semantic and visual features. A novel technique of\nunsupervised-data adaptation inference is introduced to construct more\ncomprehensive embeddings for both labeled and unlabeled data. We evaluate our\nmethod on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with\na wide range of applications, including zero and few-shot image recognition and\nretrieval, from inductive to transductive settings. Empirically, we show that\nour framework improves over the current state of the art on many of the\nconsidered tasks.</p>\n", "tags": ["ICCV", "Few-Shot-&-Zero-Shot", "Tools-&-Libraries", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [-17.989742279052734, -22.18457794189453], "cluster": 5}, {"key": "tsang2022clustering", "year": "2022", "citations": "0", "title": "Clustering The Sketch: A Novel Approach To Embedding Table Compression", "abstract": "<p>Embedding tables are used by machine learning systems to work with\ncategorical features. In modern Recommendation Systems, these tables can be\nvery large, necessitating the development of new methods for fitting them in\nmemory, even during training. We suggest Clustered Compositional Embeddings\n(CCE) which combines clustering-based compression like quantization to\ncodebooks with dynamic methods like The Hashing Trick and Compositional\nEmbeddings (Shi et al., 2020). Experimentally CCE achieves the best of both\nworlds: The high compression rate of codebook-based quantization, but\n<em>dynamically</em> like hashing-based methods, so it can be used during training.\nTheoretically, we prove that CCE is guaranteed to converge to the optimal\ncodebook and give a tight bound for the number of iterations required.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Hashing-Methods"], "tsne_embedding": [26.492765426635742, -5.583637714385986], "cluster": 6}, {"key": "tseng2020parallel", "year": "2021", "citations": "4", "title": "Parallel Index-based Structural Graph Clustering And Its Approximation", "abstract": "<p>SCAN (Structural Clustering Algorithm for Networks) is a well-studied, widely\nused graph clustering algorithm. For large graphs, however, sequential SCAN\nvariants are prohibitively slow, and parallel SCAN variants do not effectively\nshare work among queries with different SCAN parameter settings. Since users of\nSCAN often explore many parameter settings to find good clusterings, it is\nworthwhile to precompute an index that speeds up queries.\n  This paper presents a practical and provably efficient parallel index-based\nSCAN algorithm based on GS<em>-Index, a recent sequential algorithm. Our parallel\nalgorithm improves upon the asymptotic work of the sequential algorithm by\nusing integer sorting. It is also highly parallel, achieving logarithmic span\n(parallel time) for both index construction and clustering queries.\nFurthermore, we apply locality-sensitive hashing (LSH) to design a novel\napproximate SCAN algorithm and prove guarantees for its clustering behavior.\n  We present an experimental evaluation of our algorithms on large real-world\ngraphs. On a 48-core machine with two-way hyper-threading, our parallel index\nconstruction achieves 50\u2013151\\(\\times\\) speedup over the construction of\nGS</em>-Index. In fact, even on a single thread, our index construction algorithm\nis faster than GS<em>-Index. Our parallel index query implementation achieves\n5\u201332\\(\\times\\) speedup over GS</em>-Index queries across a range of SCAN parameter\nvalues, and our implementation is always faster than ppSCAN, a state-of-the-art\nparallel SCAN algorithm. Moreover, our experiments show that applying LSH\nresults in faster index construction while maintaining good clustering quality.</p>\n", "tags": ["Efficiency", "Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods"], "tsne_embedding": [55.86403274536133, 9.978974342346191], "cluster": 9}, {"key": "tu2018object", "year": "2019", "citations": "10", "title": "Object Detection Based Deep Unsupervised Hashing", "abstract": "<p>Recently, similarity-preserving hashing methods have been extensively studied\nfor large-scale image retrieval. Compared with unsupervised hashing, supervised\nhashing methods for labeled data have usually better performance by utilizing\nsemantic label information. Intuitively, for unlabeled data, it will improve\nthe performance of unsupervised hashing methods if we can first mine some\nsupervised semantic \u2018label information\u2019 from unlabeled data and then\nincorporate the \u2018label information\u2019 into the training process. Thus, in this\npaper, we propose a novel Object Detection based Deep Unsupervised Hashing\nmethod (ODDUH). Specifically, a pre-trained object detection model is utilized\nto mining supervised \u2018label information\u2019, which is used to guide the learning\nprocess to generate high-quality hash codes.Extensive experiments on two public\ndatasets demonstrate that the proposed method outperforms the state-of-the-art\nunsupervised hashing methods in the image retrieval task.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "AAAI", "Image-Retrieval", "Hashing-Methods", "Supervised", "Unsupervised", "IJCAI", "Scalability"], "tsne_embedding": [0.8170385360717773, -4.519067764282227], "cluster": 6}, {"key": "tu2019deep", "year": "2020", "citations": "62", "title": "Deep Cross-modal Hashing With Hashing Functions And Unified Hash Codes Jointly Learning", "abstract": "<p>Due to their high retrieval efficiency and low storage cost, cross-modal\nhashing methods have attracted considerable attention. Generally, compared with\nshallow cross-modal hashing methods, deep cross-modal hashing methods can\nachieve a more satisfactory performance by integrating feature learning and\nhash codes optimizing into a same framework. However, most existing deep\ncross-modal hashing methods either cannot learn a unified hash code for the two\ncorrelated data-points of different modalities in a database instance or cannot\nguide the learning of unified hash codes by the feedback of hashing function\nlearning procedure, to enhance the retrieval accuracy. To address the issues\nabove, in this paper, we propose a novel end-to-end Deep Cross-Modal Hashing\nwith Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC).\nSpecifically, by an iterative optimization algorithm, DCHUC jointly learns\nunified hash codes for image-text pairs in a database and a pair of hash\nfunctions for unseen query image-text pairs. With the iterative optimization\nalgorithm, the learned unified hash codes can be used to guide the hashing\nfunction learning procedure; Meanwhile, the learned hashing functions can\nfeedback to guide the unified hash codes optimizing procedure. Extensive\nexperiments on three public datasets demonstrate that the proposed method\noutperforms the state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [5.153128147125244, 13.484748840332031], "cluster": 6}, {"key": "tu2020deep", "year": "2020", "citations": "2", "title": "Deep Cross-modal Hashing Via Margin-dynamic-softmax Loss", "abstract": "<p>Due to their high retrieval efficiency and low storage cost for cross-modal\nsearch task, cross-modal hashing methods have attracted considerable attention.\nFor the supervised cross-modal hashing methods, how to make the learned hash\ncodes preserve semantic information sufficiently contained in the label of\ndatapoints is the key to further enhance the retrieval performance. Hence,\nalmost all supervised cross-modal hashing methods usually depends on defining a\nsimilarity between datapoints with the label information to guide the hashing\nmodel learning fully or partly. However, the defined similarity between\ndatapoints can only capture the label information of datapoints partially and\nmisses abundant semantic information, then hinders the further improvement of\nretrieval performance. Thus, in this paper, different from previous works, we\npropose a novel cross-modal hashing method without defining the similarity\nbetween datapoints, called Deep Cross-modal Hashing via\n\\textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains\na proxy hashing network to transform each category information of a dataset\ninto a semantic discriminative hash code, called proxy hash code. Each proxy\nhash code can preserve the semantic information of its corresponding category\nwell. Next, without defining the similarity between datapoints to supervise the\ntraining process of the modality-specific hashing networks , we propose a novel\n\\textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing\ncodes as supervised information. Finally, by minimizing the novel\n\\textit{margin-dynamic-softmax loss}, the modality-specific hashing networks\ncan be trained to generate hash codes which can simultaneously preserve the\ncross-modal similarity and abundant semantic information well.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [3.482327461242676, 12.680575370788574], "cluster": 6}, {"key": "tu2022unsupervised", "year": "2023", "citations": "6", "title": "Unsupervised Hashing With Semantic Concept Mining", "abstract": "<p>Recently, to improve the unsupervised image retrieval performance, plenty of\nunsupervised hashing methods have been proposed by designing a semantic\nsimilarity matrix, which is based on the similarities between image features\nextracted by a pre-trained CNN model. However, most of these methods tend to\nignore high-level abstract semantic concepts contained in images. Intuitively,\nconcepts play an important role in calculating the similarity among images. In\nreal-world scenarios, each image is associated with some concepts, and the\nsimilarity between two images will be larger if they share more identical\nconcepts. Inspired by the above intuition, in this work, we propose a novel\nUnsupervised Hashing with Semantic Concept Mining, called UHSCM, which\nleverages a VLP model to construct a high-quality similarity matrix.\nSpecifically, a set of randomly chosen concepts is first collected. Then, by\nemploying a vision-language pretraining (VLP) model with the prompt engineering\nwhich has shown strong power in visual representation learning, the set of\nconcepts is denoised according to the training images. Next, the proposed\nmethod UHSCM applies the VLP model with prompting again to mine the concept\ndistribution of each image and construct a high-quality semantic similarity\nmatrix based on the mined concept distributions. Finally, with the semantic\nsimilarity matrix as guiding information, a novel hashing loss with a modified\ncontrastive loss based regularization item is proposed to optimize the hashing\nnetwork. Extensive experiments on three benchmark datasets show that the\nproposed method outperforms the state-of-the-art baselines in the image\nretrieval task.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Neural-Hashing", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-2.308586597442627, 7.9414896965026855], "cluster": 8}, {"key": "tuinhof2018image", "year": "2019", "citations": "65", "title": "Image Based Fashion Product Recommendation With Deep Learning", "abstract": "<p>We develop a two-stage deep learning framework that recommends fashion images\nbased on other input images of similar style. For that purpose, a neural\nnetwork classifier is used as a data-driven, visually-aware feature extractor.\nThe latter then serves as input for similarity-based recommendations using a\nranking algorithm. Our approach is tested on the publicly available Fashion\ndataset. Initialization strategies using transfer learning from larger product\ndatabases are presented. Combined with more traditional content-based\nrecommendation systems, our framework can help to increase robustness and\nperformance, for example, by better matching a particular customer style.</p>\n", "tags": ["Recommender-Systems", "Robustness", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-20.720346450805664, -50.132930755615234], "cluster": 3}, {"key": "uchida2016adaptive", "year": "2016", "citations": "2", "title": "Adaptive Substring Extraction And Modified Local NBNN Scoring For Binary Feature-based Local Mobile Visual Search Without False Positives", "abstract": "<p>In this paper, we propose a stand-alone mobile visual search system based on\nbinary features and the bag-of-visual words framework. The contribution of this\nstudy is three-fold: (1) We propose an adaptive substring extraction method\nthat adaptively extracts informative bits from the original binary vector and\nstores them in the inverted index. These substrings are used to refine visual\nword-based matching. (2) A modified local NBNN scoring method is proposed in\nthe context of image retrieval, which considers the density of binary features\nin scoring each feature matching. (3) In order to suppress false positives, we\nintroduce a convexity check step that imposes a convexity constraint on the\nconfiguration of a transformed reference image. The proposed system improves\nretrieval accuracy by 11% compared with a conventional method without\nincreasing the database size. Furthermore, our system with the convexity check\ndoes not lead to false positive results.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval"], "tsne_embedding": [-14.515089988708496, 20.75518798828125], "cluster": 8}, {"key": "uchida2016image", "year": "2013", "citations": "13", "title": "Image Retrieval With Fisher Vectors Of Binary Features", "abstract": "<p>Recently, the Fisher vector representation of local features has attracted\nmuch attention because of its effectiveness in both image classification and\nimage retrieval. Another trend in the area of image retrieval is the use of\nbinary features such as ORB, FREAK, and BRISK. Considering the significant\nperformance improvement for accuracy in both image classification and retrieval\nby the Fisher vector of continuous feature descriptors, if the Fisher vector\nwere also to be applied to binary features, we would receive similar benefits\nin binary feature based image retrieval and classification. In this paper, we\nderive the closed-form approximation of the Fisher vector of binary features\nmodeled by the Bernoulli mixture model. We also propose accelerating the Fisher\nvector by using the approximate value of posterior probability. Experiments\nshow that the Fisher vector representation significantly improves the accuracy\nof image retrieval compared with a bag of binary words approach.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-22.939167022705078, 20.01349639892578], "cluster": 8}, {"key": "ueki2021survey", "year": "2021", "citations": "2", "title": "Survey Of Visual-semantic Embedding Methods For Zero-shot Image Retrieval", "abstract": "<p>Visual-semantic embedding is an interesting research topic because it is\nuseful for various tasks, such as visual question answering (VQA), image-text\nretrieval, image captioning, and scene graph generation. In this paper, we\nfocus on zero-shot image retrieval using sentences as queries and present a\nsurvey of the technological trends in this area. First, we provide a\ncomprehensive overview of the history of the technology, starting with a\ndiscussion of the early studies of image-to-text matching and how the\ntechnology has evolved over time. In addition, a description of the datasets\ncommonly used in experiments and a comparison of the evaluation results of each\nmethod are presented. We also introduce the implementation available on github\nfor use in confirming the accuracy of experiments and for further improvement.\nWe hope that this survey paper will encourage researchers to further develop\ntheir research on bridging images and languages.</p>\n", "tags": ["Survey-Paper", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-20.729656219482422, -25.588584899902344], "cluster": 5}, {"key": "ufer2021object", "year": "2020", "citations": "2", "title": "Object Retrieval And Localization In Large Art Collections Using Deep Multi-style Feature Fusion And Iterative Voting", "abstract": "<p>The search for specific objects or motifs is essential to art history as both\nassist in decoding the meaning of artworks. Digitization has produced large art\ncollections, but manual methods prove to be insufficient to analyze them. In\nthe following, we introduce an algorithm that allows users to search for image\nregions containing specific motifs or objects and find similar regions in an\nextensive dataset, helping art historians to analyze large digitized art\ncollections. Computer vision has presented efficient methods for visual\ninstance retrieval across photographs. However, applied to art collections,\nthey reveal severe deficiencies because of diverse motifs and massive domain\nshifts induced by differences in techniques, materials, and styles. In this\npaper, we present a multi-style feature fusion approach that successfully\nreduces the domain gap and improves retrieval results without labelled data or\ncurated image collections. Our region-based voting with GPU-accelerated\napproximate nearest-neighbour search allows us to find and localize even small\nmotifs within an extensive dataset in a few seconds. We obtain state-of-the-art\nresults on the Brueghel dataset and demonstrate its generalization to\ninhomogeneous collections with a large number of distractors.</p>\n", "tags": ["Datasets"], "tsne_embedding": [-3.946928024291992, -46.695281982421875], "cluster": 3}, {"key": "ujiie2021biomedical", "year": "2021", "citations": "4", "title": "Biomedical Entity Linking With Contrastive Context Matching", "abstract": "<p>We introduce BioCoM, a contrastive learning framework for biomedical entity\nlinking that uses only two resources: a small-sized dictionary and a large\nnumber of raw biomedical articles. Specifically, we build the training\ninstances from raw PubMed articles by dictionary matching and use them to train\na context-aware entity linking model with contrastive learning. We predict the\nnormalized biomedical entity at inference time through a nearest-neighbor\nsearch. Results found that BioCoM substantially outperforms state-of-the-art\nmodels, especially in low-resource settings, by effectively using the context\nof the entities.</p>\n", "tags": ["Self-Supervised", "Tools-&-Libraries"], "tsne_embedding": [-47.96516036987305, 22.98162269592285], "cluster": 0}, {"key": "uno2009efficient", "year": "2009", "citations": "10", "title": "Efficient Construction Of Neighborhood Graphs By The Multiple Sorting Method", "abstract": "<p>Neighborhood graphs are gaining popularity as a concise data representation\nin machine learning. However, naive graph construction by pairwise distance\ncalculation takes \\(O(n^2)\\) runtime for \\(n\\) data points and this is\nprohibitively slow for millions of data points. For strings of equal length,\nthe multiple sorting method (Uno, 2008) can construct an \\(\\epsilon\\)-neighbor\ngraph in \\(O(n+m)\\) time, where \\(m\\) is the number of \\(\\epsilon\\)-neighbor pairs in\nthe data. To introduce this remarkably efficient algorithm to continuous\ndomains such as images, signals and texts, we employ a random projection method\nto convert vectors to strings. Theoretical results are presented to elucidate\nthe trade-off between approximation quality and computation time. Empirical\nresults show the efficiency of our method in comparison to fast nearest\nneighbor alternatives.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Evaluation", "Locality-Sensitive-Hashing"], "tsne_embedding": [51.4998893737793, 7.942480564117432], "cluster": 9}, {"key": "uy2018pointnetvlad", "year": "2018", "citations": "484", "title": "Pointnetvlad: Deep Point Cloud Based Retrieval For Large-scale Place Recognition", "abstract": "<p>Unlike its image based counterpart, point cloud based retrieval for place\nrecognition has remained as an unexplored and unsolved problem. This is largely\ndue to the difficulty in extracting local feature descriptors from a point\ncloud that can subsequently be encoded into a global descriptor for the\nretrieval task. In this paper, we propose the PointNetVLAD where we leverage on\nthe recent success of deep networks to solve point cloud based retrieval for\nplace recognition. Specifically, our PointNetVLAD is a combination/modification\nof the existing PointNet and NetVLAD, which allows end-to-end training and\ninference to extract the global descriptor from a given 3D point cloud.\nFurthermore, we propose the \u201clazy triplet and quadruplet\u201d loss functions that\ncan achieve more discriminative and generalizable global descriptors to tackle\nthe retrieval task. We create benchmark datasets for point cloud based\nretrieval for place recognition, and the experimental results on these datasets\nshow the feasibility of our PointNetVLAD. Our code and the link for the\nbenchmark dataset downloads are available in our project website.\nhttp://github.com/mikacuy/pointnetvlad/</p>\n", "tags": ["CVPR", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-32.63734436035156, 0.8518056273460388], "cluster": 0}, {"key": "vaccaro2020image", "year": "2020", "citations": "16", "title": "Image Retrieval Using Multi-scale CNN Features Pooling", "abstract": "<p>In this paper, we address the problem of image retrieval by learning images\nrepresentation based on the activations of a Convolutional Neural Network. We\npresent an end-to-end trainable network architecture that exploits a novel\nmulti-scale local pooling based on NetVLAD and a triplet mining procedure based\non samples difficulty to obtain an effective image representation. Extensive\nexperiments show that our approach is able to reach state-of-the-art results on\nthree standard datasets.</p>\n", "tags": ["Multimodal-Retrieval", "Datasets", "Image-Retrieval"], "tsne_embedding": [-25.215791702270508, -39.229557037353516], "cluster": 5}, {"key": "vanblokland2020indexing", "year": "2020", "citations": "14", "title": "An Indexing Scheme And Descriptor For 3D Object Retrieval Based On Local Shape Querying", "abstract": "<p>A binary descriptor indexing scheme based on Hamming distance called the\nHamming tree for local shape queries is presented. A new binary clutter\nresistant descriptor named Quick Intersection Count Change Image (QUICCI) is\nalso introduced. This local shape descriptor is extremely small and fast to\ncompare. Additionally, a novel distance function called Weighted Hamming\napplicable to QUICCI images is proposed for retrieval applications. The\neffectiveness of the indexing scheme and QUICCI is demonstrated on 828 million\nQUICCI images derived from the SHREC2017 dataset, while the clutter resistance\nof QUICCI is shown using the clutterbox experiment.</p>\n", "tags": ["Datasets"], "tsne_embedding": [-29.55497932434082, 17.66571807861328], "cluster": 0}, {"key": "vanblokland2021partial", "year": "2021", "citations": "6", "title": "Partial 3D Object Retrieval Using Local Binary QUICCI Descriptors And Dissimilarity Tree Indexing", "abstract": "<p>A complete pipeline is presented for accurate and efficient partial 3D object\nretrieval based on Quick Intersection Count Change Image (QUICCI) binary local\ndescriptors and a novel indexing tree. It is shown how a modification to the\nQUICCI query descriptor makes it ideal for partial retrieval. An indexing\nstructure called Dissimilarity Tree is proposed which can significantly\naccelerate searching the large space of local descriptors; this is applicable\nto QUICCI and other binary descriptors. The index exploits the distribution of\nbits within descriptors for efficient retrieval. The retrieval pipeline is\ntested on the artificial part of SHREC\u201916 dataset with near-ideal retrieval\nresults.</p>\n", "tags": ["Similarity-Search", "Datasets"], "tsne_embedding": [-21.448986053466797, -34.72919845581055], "cluster": 3}, {"key": "vasile2016meta", "year": "2016", "citations": "120", "title": "Meta-prod2vec - Product Embeddings Using Side-information For Recommendation", "abstract": "<p>We propose Meta-Prod2vec, a novel method to compute item similarities for\nrecommendation that leverages existing item metadata. Such scenarios are\nfrequently encountered in applications such as content recommendation, ad\ntargeting and web search. Our method leverages past user interactions with\nitems and their attributes to compute low-dimensional embeddings of items.\nSpecifically, the item metadata is in- jected into the model as side\ninformation to regularize the item embeddings. We show that the new item\nrepresenta- tions lead to better performance on recommendation tasks on an open\nmusic dataset.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [12.294397354125977, -42.68247985839844], "cluster": 3}, {"key": "vasudeva2021loop", "year": "2021", "citations": "12", "title": "Loop: Looking For Optimal Hard Negative Embeddings For Deep Metric Learning", "abstract": "<p>Deep metric learning has been effectively used to learn distance metrics for\ndifferent visual tasks like image retrieval, clustering, etc. In order to aid\nthe training process, existing methods either use a hard mining strategy to\nextract the most informative samples or seek to generate hard synthetics using\nan additional network. Such approaches face different challenges and can lead\nto biased embeddings in the former case, and (i) harder optimization (ii)\nslower training speed (iii) higher model complexity in the latter case. In\norder to overcome these challenges, we propose a novel approach that looks for\noptimal hard negatives (LoOp) in the embedding space, taking full advantage of\neach tuple by calculating the minimum distance between a pair of positives and\na pair of negatives. Unlike mining-based methods, our approach considers the\nentire space between pairs of embeddings to calculate the optimal hard\nnegatives. Extensive experiments combining our approach and representative\nmetric learning losses reveal a significant boost in performance on three\nbenchmark datasets.</p>\n", "tags": ["ICCV", "Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-19.156593322753906, -13.837045669555664], "cluster": 1}, {"key": "vaze2023genecis", "year": "2023", "citations": "7", "title": "Genecis: A Benchmark For General Conditional Image Similarity", "abstract": "<p>We argue that there are many notions of \u2018similarity\u2019 and that models, like\nhumans, should be able to adapt to these dynamically. This contrasts with most\nrepresentation learning methods, supervised or self-supervised, which learn a\nfixed embedding function and hence implicitly assume a single notion of\nsimilarity. For instance, models trained on ImageNet are biased towards object\ncategories, while a user might prefer the model to focus on colors, textures or\nspecific elements in the scene. In this paper, we propose the GeneCIS\n(\u2018genesis\u2019) benchmark, which measures models\u2019 ability to adapt to a range of\nsimilarity conditions. Extending prior work, our benchmark is designed for\nzero-shot evaluation only, and hence considers an open-set of similarity\nconditions. We find that baselines from powerful CLIP models struggle on\nGeneCIS and that performance on the benchmark is only weakly correlated with\nImageNet accuracy, suggesting that simply scaling existing methods is not\nfruitful. We further propose a simple, scalable solution based on automatically\nmining information from existing image-caption datasets. We find our method\noffers a substantial boost over the baselines on GeneCIS, and further improves\nzero-shot performance on related image retrieval benchmarks. In fact, though\nevaluated zero-shot, our model surpasses state-of-the-art supervised models on\nMIT-States. Project page at https://sgvaze.github.io/genecis/.</p>\n", "tags": ["Self-Supervised", "CVPR", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-33.4517707824707, -11.363597869873047], "cluster": 5}, {"key": "vecchiato2024learning", "year": "2024", "citations": "3", "title": "Learning Cluster Representatives For Approximate Nearest Neighbor Search", "abstract": "<p>Developing increasingly efficient and accurate algorithms for approximate\nnearest neighbor search is a paramount goal in modern information retrieval. A\nprimary approach to addressing this question is clustering, which involves\npartitioning the dataset into distinct groups, with each group characterized by\na representative data point. By this method, retrieving the top-k data points\nfor a query requires identifying the most relevant clusters based on their\nrepresentatives \u2013 a routing step \u2013 and then conducting a nearest neighbor\nsearch within these clusters only, drastically reducing the search space.\n  The objective of this thesis is not only to provide a comprehensive\nexplanation of clustering-based approximate nearest neighbor search but also to\nintroduce and delve into every aspect of our novel state-of-the-art method,\nwhich originated from a natural observation: The routing function solves a\nranking problem, making the function amenable to learning-to-rank. The\ndevelopment of this intuition and applying it to maximum inner product search\nhas led us to demonstrate that learning cluster representatives using a simple\nlinear function significantly boosts the accuracy of clustering-based\napproximate nearest neighbor search.</p>\n", "tags": ["SIGIR", "Datasets"], "tsne_embedding": [46.05471420288086, 6.35258150100708], "cluster": 9}, {"key": "veit2016conditional", "year": "2017", "citations": "168", "title": "Conditional Similarity Networks", "abstract": "<p>What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately.</p>\n", "tags": ["CVPR"], "tsne_embedding": [-19.922332763671875, -8.599618911743164], "cluster": 1}, {"key": "veit2017conditional", "year": "2017", "citations": "168", "title": "Conditional Similarity Networks", "abstract": "<p>What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately.</p>\n", "tags": ["CVPR"], "tsne_embedding": [-19.922332763671875, -8.599618911743164], "cluster": 1}, {"key": "veit2020improving", "year": "2020", "citations": "0", "title": "Improving Calibration In Deep Metric Learning With Cross-example Softmax", "abstract": "<p>Modern image retrieval systems increasingly rely on the use of deep neural\nnetworks to learn embedding spaces in which distance encodes the relevance\nbetween a given query and image. In this setting, existing approaches tend to\nemphasize one of two properties. Triplet-based methods capture top-\\(k\\)\nrelevancy, where all top-\\(k\\) scoring documents are assumed to be relevant to a\ngiven query Pairwise contrastive models capture threshold relevancy, where all\ndocuments scoring higher than some threshold are assumed to be relevant. In\nthis paper, we propose Cross-Example Softmax which combines the properties of\ntop-\\(k\\) and threshold relevancy. In each iteration, the proposed loss\nencourages all queries to be closer to their matching images than all queries\nare to all non-matching images. This leads to a globally more calibrated\nsimilarity metric and makes distance more interpretable as an absolute measure\nof relevance. We further introduce Cross-Example Negative Mining, in which each\npair is compared to the hardest negative comparisons across the entire batch.\nEmpirically, we show in a series of experiments on Conceptual Captions and\nFlickr30k, that the proposed method effectively improves global calibration and\nalso retrieval performance.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-19.397016525268555, -3.7052066326141357], "cluster": 1}, {"key": "vemulapalli2018compact", "year": "2019", "citations": "102", "title": "A Compact Embedding For Facial Expression Similarity", "abstract": "<p>Most of the existing work on automatic facial expression analysis focuses on\ndiscrete emotion recognition, or facial action unit detection. However, facial\nexpressions do not always fall neatly into pre-defined semantic categories.\nAlso, the similarity between expressions measured in the action unit space need\nnot correspond to how humans perceive expression similarity. Different from\nprevious work, our goal is to describe facial expressions in a continuous\nfashion using a compact embedding space that mimics human visual preferences.\nTo achieve this goal, we collect a large-scale faces-in-the-wild dataset with\nhuman annotations in the form: Expressions A and B are visually more similar\nwhen compared to expression C, and use this dataset to train a neural network\nthat produces a compact (16-dimensional) expression embedding. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications such as expression retrieval, photo album\nsummarization, and emotion recognition. We also show that the embedding learned\nusing the proposed dataset performs better than several other embeddings\nlearned using existing emotion or action unit datasets.</p>\n", "tags": ["CVPR", "Scalability", "Datasets"], "tsne_embedding": [-39.55712127685547, -39.952457427978516], "cluster": 5}, {"key": "vemulapalli2019compact", "year": "2019", "citations": "102", "title": "A Compact Embedding For Facial Expression Similarity", "abstract": "<p>Most of the existing work on automatic facial expression analysis focuses on\ndiscrete emotion recognition, or facial action unit detection. However, facial\nexpressions do not always fall neatly into pre-defined semantic categories.\nAlso, the similarity between expressions measured in the action unit space need\nnot correspond to how humans perceive expression similarity. Different from\nprevious work, our goal is to describe facial expressions in a continuous\nfashion using a compact embedding space that mimics human visual preferences.\nTo achieve this goal, we collect a large-scale faces-in-the-wild dataset with\nhuman annotations in the form: Expressions A and B are visually more similar\nwhen compared to expression C, and use this dataset to train a neural network\nthat produces a compact (16-dimensional) expression embedding. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications such as expression retrieval, photo album\nsummarization, and emotion recognition. We also show that the embedding learned\nusing the proposed dataset performs better than several other embeddings\nlearned using existing emotion or action unit datasets.</p>\n", "tags": ["CVPR", "Scalability", "Datasets"], "tsne_embedding": [-39.55712127685547, -39.952457427978516], "cluster": 5}, {"key": "venkatakeerthy2023vexir2vec", "year": "2023", "citations": "0", "title": "Vexir2vec: An Architecture-neutral Embedding Framework For Binary Similarity", "abstract": "<p>Binary similarity involves determining whether two binary programs exhibit\nsimilar functionality, often originating from the same source code. In this\nwork, we propose VexIR2Vec, an approach for binary similarity using VEX-IR, an\narchitecture-neutral Intermediate Representation (IR). We extract the\nembeddings from sequences of basic blocks, termed peepholes, derived by random\nwalks on the control-flow graph. The peepholes are normalized using\ntransformations inspired by compiler optimizations. The VEX-IR Normalization\nEngine mitigates, with these transformations, the architectural and\ncompiler-induced variations in binaries while exposing semantic similarities.\nWe then learn the vocabulary of representations at the entity level of the IR\nusing the knowledge graph embedding techniques in an unsupervised manner. This\nvocabulary is used to derive function embeddings for similarity assessment\nusing VexNet, a feed-forward Siamese network designed to position similar\nfunctions closely and separate dissimilar ones in an n-dimensional space. This\napproach is amenable for both diffing and searching tasks, ensuring robustness\nagainst Out-Of-Vocabulary (OOV) issues.\n  We evaluate VexIR2Vec on a dataset comprising 2.7M functions and 15.5K\nbinaries from 7 projects compiled across 12 compilers targeting x86 and ARM\narchitectures. In diffing experiments, VexIR2Vec outperforms the nearest\nbaselines by \\(40%\\), \\(18%\\), \\(21%\\), and \\(60%\\) in cross-optimization,\ncross-compilation, cross-architecture, and obfuscation settings, respectively.\nIn the searching experiment, VexIR2Vec achieves a mean average precision of\n\\(0.76\\), outperforming the nearest baseline by \\(46%\\). Our framework is highly\nscalable and is built as a lightweight, multi-threaded, parallel library using\nonly open-source tools. VexIR2Vec is \\(3.1\\)-\\(3.5 \\times\\) faster than the closest\nbaselines and orders-of-magnitude faster than other tools.</p>\n", "tags": ["Robustness", "Tools-&-Libraries", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [14.362861633300781, 14.03565502166748], "cluster": 6}, {"key": "venkataramanan2023integrating", "year": "2023", "citations": "2", "title": "Integrating Visual And Semantic Similarity Using Hierarchies For Image Retrieval", "abstract": "<p>Most of the research in content-based image retrieval (CBIR) focus on\ndeveloping robust feature representations that can effectively retrieve\ninstances from a database of images that are visually similar to a query.\nHowever, the retrieved images sometimes contain results that are not\nsemantically related to the query. To address this, we propose a method for\nCBIR that captures both visual and semantic similarity using a visual\nhierarchy. The hierarchy is constructed by merging classes with overlapping\nfeatures in the latent space of a deep neural network trained for\nclassification, assuming that overlapping classes share high visual and\nsemantic similarities. Finally, the constructed hierarchy is integrated into\nthe distance calculation metric for similarity search. Experiments on standard\ndatasets: CUB-200-2011 and CIFAR100, and a real-life use case using diatom\nmicroscopy images show that our method achieves superior performance compared\nto the existing methods on image retrieval.</p>\n", "tags": ["Evaluation", "Similarity-Search", "Image-Retrieval", "Datasets"], "tsne_embedding": [-15.88631534576416, -6.6783552169799805], "cluster": 1}, {"key": "venkateswara2017deep", "year": "2017", "citations": "1712", "title": "Deep Hashing Network For Unsupervised Domain Adaptation", "abstract": "<p>In recent years, deep neural networks have emerged as a dominant machine\nlearning tool for a wide variety of application domains. However, training a\ndeep neural network requires a large amount of labeled data, which is an\nexpensive process in terms of time, labor and human expertise. Domain\nadaptation or transfer learning algorithms address this challenge by leveraging\nlabeled data in a different, but related source domain, to develop a model for\nthe target domain. Further, the explosive growth of digital data has posed a\nfundamental challenge concerning its storage and retrieval. Due to its storage\nand retrieval efficiency, recent years have witnessed a wide application of\nhashing in a variety of computer vision applications. In this paper, we first\nintroduce a new dataset, Office-Home, to evaluate domain adaptation algorithms.\nThe dataset contains images of a variety of everyday objects from multiple\ndomains. We then propose a novel deep learning framework that can exploit\nlabeled source data and unlabeled target data to learn informative hash codes,\nto accurately classify unseen target data. To the best of our knowledge, this\nis the first research effort to exploit the feature learning capabilities of\ndeep neural networks to learn representative hash codes to address the domain\nadaptation problem. Our extensive empirical studies on multiple transfer tasks\ncorroborate the usefulness of the framework in learning efficient hash codes\nwhich outperform existing competitive baselines for unsupervised domain\nadaptation.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "CVPR", "Tools-&-Libraries", "Datasets", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [25.931962966918945, -13.237103462219238], "cluster": 7}, {"key": "vepakomma2021privatemail", "year": "2021", "citations": "0", "title": "Privatemail: Supervised Manifold Learning Of Deep Features With Differential Privacy For Image Retrieval", "abstract": "<p>Differential Privacy offers strong guarantees such as immutable privacy under\npost processing. Thus it is often looked to as a solution to learning on\nscattered and isolated data. This work focuses on supervised manifold learning,\na paradigm that can generate fine-tuned manifolds for a target use case. Our\ncontributions are two fold. 1) We present a novel differentially private method\n\\textit{PrivateMail} for supervised manifold learning, the first of its kind to\nour knowledge. 2) We provide a novel private geometric embedding scheme for our\nexperimental use case. We experiment on private \u201ccontent based image retrieval\u201d</p>\n<ul>\n  <li>embedding and querying the nearest neighbors of images in a private manner -\nand show extensive privacy-utility tradeoff results, as well as the\ncomputational efficiency and practicality of our methods.</li>\n</ul>\n", "tags": ["Supervised", "Image-Retrieval", "Efficiency", "Privacy-&-Security"], "tsne_embedding": [-17.39227867126465, 28.352720260620117], "cluster": 8}, {"key": "verma2019diversity", "year": "2018", "citations": "25", "title": "Diversity In Fashion Recommendation Using Semantic Parsing", "abstract": "<p>Developing recommendation system for fashion images is challenging due to the\ninherent ambiguity associated with what criterion a user is looking at.\nSuggesting multiple images where each output image is similar to the query\nimage on the basis of a different feature or part is one way to mitigate the\nproblem. Existing works for fashion recommendation have used Siamese or Triplet\nnetwork to learn features between a similar pair and a similar-dissimilar\ntriplet respectively. However, these methods do not provide basic information\nsuch as, how two clothing images are similar, or which parts present in the two\nimages make them similar. In this paper, we propose to recommend images by\nexplicitly learning and exploiting part based similarity. We propose a novel\napproach of learning discriminative features from weakly-supervised data by\nusing visual attention over the parts and a texture encoding network. We show\nthat the learned features surpass the state-of-the-art in retrieval task on\nDeepFashion dataset. We then use the proposed model to recommend fashion images\nhaving an explicit variation with respect to similarity of any of the parts.</p>\n", "tags": ["Supervised", "Recommender-Systems", "Datasets"], "tsne_embedding": [-21.87510108947754, -48.57252883911133], "cluster": 3}, {"key": "verma2021efficient", "year": "2022", "citations": "4", "title": "Efficient Binary Embedding Of Categorical Data Using Binsketch", "abstract": "<p>In this work, we present a dimensionality reduction algorithm, aka.\nsketching, for categorical datasets. Our proposed sketching algorithm Cabin\nconstructs low-dimensional binary sketches from high-dimensional categorical\nvectors, and our distance estimation algorithm Cham computes a close\napproximation of the Hamming distance between any two original vectors only\nfrom their sketches. The minimum dimension of the sketches required by Cham to\nensure a good estimation theoretically depends only on the sparsity of the data\npoints - making it useful for many real-life scenarios involving sparse\ndatasets. We present a rigorous theoretical analysis of our approach and\nsupplement it with extensive experiments on several high-dimensional real-world\ndata sets, including one with over a million dimensions. We show that the Cabin\nand Cham duo is a significantly fast and accurate approach for tasks such as\nRMSE, all-pairs similarity, and clustering when compared to working with the\nfull dataset and other dimensionality reduction techniques.</p>\n", "tags": ["Hashing-Methods", "Datasets"], "tsne_embedding": [49.27309036254883, -24.366134643554688], "cluster": 9}, {"key": "verma2022efficient", "year": "2022", "citations": "4", "title": "Efficient Binary Embedding Of Categorical Data Using Binsketch", "abstract": "<p>In this work, we present a dimensionality reduction algorithm, aka.\nsketching, for categorical datasets. Our proposed sketching algorithm Cabin\nconstructs low-dimensional binary sketches from high-dimensional categorical\nvectors, and our distance estimation algorithm Cham computes a close\napproximation of the Hamming distance between any two original vectors only\nfrom their sketches. The minimum dimension of the sketches required by Cham to\nensure a good estimation theoretically depends only on the sparsity of the data\npoints - making it useful for many real-life scenarios involving sparse\ndatasets. We present a rigorous theoretical analysis of our approach and\nsupplement it with extensive experiments on several high-dimensional real-world\ndata sets, including one with over a million dimensions. We show that the Cabin\nand Cham duo is a significantly fast and accurate approach for tasks such as\nRMSE, all-pairs similarity, and clustering when compared to working with the\nfull dataset and other dimensionality reduction techniques.</p>\n", "tags": ["Hashing-Methods", "Datasets"], "tsne_embedding": [49.27309036254883, -24.366134643554688], "cluster": 9}, {"key": "verma2024improving", "year": "2024", "citations": "0", "title": "Improving LSH Via Tensorized Random Projection", "abstract": "<p>Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by\ndata scientists for approximate nearest neighbour search problems that have\nbeen used extensively in many large scale data processing applications such as\nnear duplicate detection, nearest neighbour search, clustering, etc. In this\nwork, we aim to propose faster and space efficient locality sensitive hash\nfunctions for Euclidean distance and cosine similarity for tensor data.\nTypically, the naive approach for obtaining LSH for tensor data involves first\nreshaping the tensor into vectors, followed by applying existing LSH methods\nfor vector data \\(E2LSH\\) and \\(SRP\\). However, this approach becomes impractical\nfor higher order tensors because the size of the reshaped vector becomes\nexponential in the order of the tensor. Consequently, the size of LSH\nparameters increases exponentially. To address this problem, we suggest two\nmethods for LSH for Euclidean distance and cosine similarity, namely\n\\(CP-E2LSH\\), \\(TT-E2LSH\\), and \\(CP-SRP\\), \\(TT-SRP\\), respectively, building on \\(CP\\)\nand tensor train \\((TT)\\) decompositions techniques. Our approaches are space\nefficient and can be efficiently applied to low rank \\(CP\\) or \\(TT\\) tensors. We\nprovide a rigorous theoretical analysis of our proposal on their correctness\nand efficacy.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Distance-Metric-Learning"], "tsne_embedding": [17.914854049682617, 46.25559616088867], "cluster": 4}, {"key": "verma2025faster", "year": "2025", "citations": "0", "title": "Faster And Space Efficient Indexing For Locality Sensitive Hashing", "abstract": "<p>This work suggests faster and space-efficient index construction algorithms\nfor LSH for Euclidean distance (\\textit{a.k.a.}~\\ELSH) and cosine similarity\n(\\textit{a.k.a.}~\\SRP). The index construction step of these LSHs relies on\ngrouping data points into several bins of hash tables based on their hashcode.\nTo generate an \\(m\\)-dimensional hashcode of the \\(d\\)-dimensional data point,\nthese LSHs first project the data point onto a \\(d\\)-dimensional random Gaussian\nvector and then discretise the resulting inner product. The time and space\ncomplexity of both \\ELSH~and \\SRP~for computing an \\(m\\)-sized hashcode of a\n\\(d\\)-dimensional vector is \\(O(md)\\), which becomes impractical for large values\nof \\(m\\) and \\(d\\). To overcome this problem, we propose two alternative LSH\nhashcode generation algorithms both for Euclidean distance and cosine\nsimilarity, namely, \\CSELSH, \\HCSELSH~and \\CSSRP, \\HCSSRP, respectively.\n\\CSELSH~and \\CSSRP~are based on count sketch \\cite{count_sketch} and\n\\HCSELSH~and \\HCSSRP~utilize higher-order count sketch \\cite{shi2019higher}.\nThese proposals significantly reduce the hashcode computation time from \\(O(md)\\)\nto \\(O(d)\\). Additionally, both \\CSELSH~and \\CSSRP~reduce the space complexity\nfrom \\(O(md)\\) to \\(O(d)\\); ~and \\HCSELSH, \\HCSSRP~ reduce the space complexity\nfrom \\(O(md)\\) to \\(O(N \\sqrt[N]{d})\\) respectively, where \\(N\\geq 1\\) denotes the\nsize of the input/reshaped tensor. Our proposals are backed by strong\nmathematical guarantees, and we validate their performance through simulations\non various real-world datasets.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Datasets", "Evaluation"], "tsne_embedding": [19.589492797851562, 48.05023956298828], "cluster": 4}, {"key": "vilalta2017full", "year": "2017", "citations": "4", "title": "Full-network Embedding In A Multimodal Embedding Pipeline", "abstract": "<p>The current state-of-the-art for image annotation and image retrieval tasks\nis obtained through deep neural networks, which combine an image representation\nand a text representation into a shared embedding space. In this paper we\nevaluate the impact of using the Full-Network embedding in this setting,\nreplacing the original image representation in a competitive multimodal\nembedding generation scheme. Unlike the one-layer image embeddings typically\nused by most approaches, the Full-Network embedding provides a multi-scale\nrepresentation of images, which results in richer characterizations. To measure\nthe influence of the Full-Network embedding, we evaluate its performance on\nthree different datasets, and compare the results with the original multimodal\nembedding generation scheme when using a one-layer image embedding, and with\nthe rest of the state-of-the-art. Results for image annotation and image\nretrieval tasks indicate that the Full-Network embedding is consistently\nsuperior to the one-layer embedding. These results motivate the integration of\nthe Full-Network embedding on any multimodal embedding generation scheme,\nsomething feasible thanks to the flexibility of the approach.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-15.725736618041992, 2.419374704360962], "cluster": 1}, {"key": "visheratin2023nllb", "year": "2023", "citations": "0", "title": "NLLB-CLIP -- Train Performant Multilingual Image Retrieval Model On A Budget", "abstract": "<p>Today, the exponential rise of large models developed by academic and\nindustrial institutions with the help of massive computing resources raises the\nquestion of whether someone without access to such resources can make a\nvaluable scientific contribution. To explore this, we tried to solve the\nchallenging task of multilingual image retrieval having a limited budget of\n$1,000. As a result, we present NLLB-CLIP - CLIP model with a text encoder from\nthe NLLB model. To train the model, we used an automatically created dataset of\n106,246 good-quality images with captions in 201 languages derived from the\nLAION COCO dataset. We trained multiple models using image and text encoders of\nvarious sizes and kept different parts of the model frozen during the training.\nWe thoroughly analyzed the trained models using existing evaluation datasets\nand newly created XTD200 and Flickr30k-200 datasets. We show that NLLB-CLIP is\ncomparable in quality to state-of-the-art models and significantly outperforms\nthem on low-resource languages.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-20.823152542114258, -18.68464469909668], "cluster": 5}, {"key": "vo2016localizing", "year": "2016", "citations": "247", "title": "Localizing And Orienting Street Views Using Overhead Imagery", "abstract": "<p>In this paper we aim to determine the location and orientation of a\nground-level query image by matching to a reference database of overhead (e.g.\nsatellite) images. For this task we collect a new dataset with one million\npairs of street view and overhead images sampled from eleven U.S. cities. We\nexplore several deep CNN architectures for cross-domain matching \u2013\nClassification, Hybrid, Siamese, and Triplet networks. Classification and\nHybrid architectures are accurate but slow since they allow only partial\nfeature precomputation. We propose a new loss function which significantly\nimproves the accuracy of Siamese and Triplet embedding networks while\nmaintaining their applicability to large-scale retrieval tasks like image\ngeolocalization. This image matching task is challenging not just because of\nthe dramatic viewpoint difference between ground-level and overhead imagery but\nbecause the orientation (i.e. azimuth) of the street views is unknown making\ncorrespondence even more difficult. We examine several mechanisms to match in\nspite of this \u2013 training for rotation invariance, sampling possible rotations\nat query time, and explicitly predicting relative rotation of ground and\noverhead images with our deep networks. It turns out that explicit orientation\nsupervision also improves location prediction accuracy. Our best performing\narchitectures are roughly 2.5 times as accurate as the commonly used Siamese\nnetwork baseline.</p>\n", "tags": ["Efficiency", "Scalability", "Datasets"], "tsne_embedding": [-5.669633865356445, 45.09011459350586], "cluster": 4}, {"key": "vo2017revisiting", "year": "2017", "citations": "118", "title": "Revisiting IM2GPS In The Deep Learning Era", "abstract": "<p>Image geolocalization, inferring the geographic location of an image, is a\nchallenging computer vision problem with many potential applications. The\nrecent state-of-the-art approach to this problem is a deep image classification\napproach in which the world is spatially divided into cells and a deep network\nis trained to predict the correct cell for a given image. We propose to combine\nthis approach with the original Im2GPS approach in which a query image is\nmatched against a database of geotagged images and the location is inferred\nfrom the retrieved set. We estimate the geographic location of a query image by\napplying kernel density estimation to the locations of its nearest neighbors in\nthe reference database. Interestingly, we find that the best features for our\nretrieval task are derived from networks trained with classification loss even\nthough we do not use a classification approach at test time. Training with\nclassification loss outperforms several deep feature learning methods (e.g.\nSiamese networks with contrastive of triplet loss) more typical for retrieval\napplications. Our simple approach achieves state-of-the-art geolocalization\naccuracy while also requiring significantly less training data.</p>\n", "tags": ["ICCV", "Distance-Metric-Learning"], "tsne_embedding": [-30.392549514770508, 1.0312403440475464], "cluster": 0}, {"key": "vo2018composing", "year": "2019", "citations": "317", "title": "Composing Text And Image For Image Retrieval - An Empirical Odyssey", "abstract": "<p>In this paper, we study the task of image retrieval, where the input query is\nspecified in the form of an image plus some text that describes desired\nmodifications to the input image. For example, we may present an image of the\nEiffel tower, and ask the system to find images which are visually similar but\nare modified in small ways, such as being taken at nighttime instead of during\nthe day. To tackle this task, we learn a similarity metric between a target\nimage and a source image plus source text, an embedding and composing function\nsuch that target image feature is close to the source image plus text\ncomposition feature. We propose a new way to combine image and text using such\nfunction that is designed for the retrieval task. We show this outperforms\nexisting approaches on 3 different datasets, namely Fashion-200k, MIT-States\nand a new synthetic dataset we create based on CLEVR. We also show that our\napproach can be used to classify input queries, in addition to image retrieval.</p>\n", "tags": ["CVPR", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-23.08899688720703, -30.550092697143555], "cluster": 5}, {"key": "vo2018generalization", "year": "2018", "citations": "5", "title": "Generalization In Metric Learning: Should The Embedding Layer Be The Embedding Layer?", "abstract": "<p>This work studies deep metric learning under small to medium scale data as we\nbelieve that better generalization could be a contributing factor to the\nimprovement of previous fine-grained image retrieval methods; it should be\nconsidered when designing future techniques. In particular, we investigate\nusing other layers in a deep metric learning system (besides the embedding\nlayer) for feature extraction and analyze how well they perform on training\ndata and generalize to testing data. From this study, we suggest a new\nregularization practice where one can add or choose a more optimal layer for\nfeature extraction. State-of-the-art performance is demonstrated on 3\nfine-grained image retrieval benchmarks: Cars-196, CUB-200-2011, and Stanford\nOnline Product.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-15.438748359680176, -27.983850479125977], "cluster": 3}, {"key": "voutharoja2023malm", "year": "2023", "citations": "2", "title": "MALM: Mask Augmentation Based Local Matching For Food-recipe Retrieval", "abstract": "<p>Image-to-recipe retrieval is a challenging vision-to-language task of\nsignificant practical value. The main challenge of the task lies in the\nultra-high redundancy in the long recipe and the large variation reflected in\nboth food item combination and food item appearance. A de-facto idea to address\nthis task is to learn a shared feature embedding space in which a food image is\naligned better to its paired recipe than other recipes. However, such\nsupervised global matching is prone to supervision collapse, i.e., only partial\ninformation that is necessary for distinguishing training pairs can be\nidentified, while other information that is potentially useful in\ngeneralization could be lost. To mitigate such a problem, we propose a\nmask-augmentation-based local matching network (MALM), where an image-text\nmatching module and a masked self-distillation module benefit each other\nmutually to learn generalizable cross-modality representations. On one hand, we\nperform local matching between the tokenized representations of image and text\nto locate fine-grained cross-modality correspondence explicitly. We involve\nrepresentations of masked image patches in this process to alleviate\noverfitting resulting from local matching especially when some food items are\nunderrepresented. On the other hand, predicting the hidden representations of\nthe masked patches through self-distillation helps to learn general-purpose\nimage representations that are expected to generalize better. And the\nmulti-task nature of the model enables the representations of masked patches to\nbe text-aware and thus facilitates the lost information reconstruction.\nExperimental results on Recipe1M dataset show our method can clearly outperform\nstate-of-the-art (SOTA) methods. Our code will be available at\nhttps://github.com/MyFoodChoice/MALM_Mask_Augmentation_based_Local_Matching-_for-_Food_Recipe_Retrieval</p>\n", "tags": ["Supervised", "Datasets"], "tsne_embedding": [-40.862449645996094, 30.76723861694336], "cluster": 0}, {"key": "vu2016search", "year": "2017", "citations": "48", "title": "Search Personalization With Embeddings", "abstract": "<p>Recent research has shown that the performance of search personalization\ndepends on the richness of user profiles which normally represent the user\u2019s\ntopical interests. In this paper, we propose a new embedding approach to\nlearning user profiles, where users are embedded on a topical interest space.\nWe then directly utilize the user profiles for search personalization.\nExperiments on query logs from a major commercial web search engine demonstrate\nthat our embedding approach improves the performance of the search engine and\nalso achieves better search performance than other strong baselines.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [29.734067916870117, -29.882728576660156], "cluster": 7}, {"key": "vu2017search", "year": "2017", "citations": "48", "title": "Search Personalization With Embeddings", "abstract": "<p>Recent research has shown that the performance of search personalization\ndepends on the richness of user profiles which normally represent the user\u2019s\ntopical interests. In this paper, we propose a new embedding approach to\nlearning user profiles, where users are embedded on a topical interest space.\nWe then directly utilize the user profiles for search personalization.\nExperiments on query logs from a major commercial web search engine demonstrate\nthat our embedding approach improves the performance of the search engine and\nalso achieves better search performance than other strong baselines.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [29.734067916870117, -29.882728576660156], "cluster": 7}, {"key": "wang2010semi", "year": "2010", "citations": "626", "title": "Semi-supervised Deep Quantization For Cross-modal Search", "abstract": "<p>The problem of cross-modal similarity search, which aims at making efficient and accurate queries across multiple domains, has become a significant and important research topic. Composite quantization, a compact coding solution superior to hashing techniques, has shown its effectiveness for similarity search. However, most existing works utilizing composite quantization to search multi-domain content only consider either pairwise similarity information or class label information across different domains, which fails to tackle the semi-supervised problem in composite quantization. In this paper, we address the semi-supervised quantization problem by considering: (i) pairwise similarity information (without class label information) across different domains, which captures the intra-document relation, (ii) cross-domain data with class label which can help capture inter-document relation, and (iii) cross-domain data with neither pairwise similarity nor class label which enables the full use of abundant unlabelled information. To the best of our knowledge, we are the first to consider both supervised information (pairwise similarity + class label) and unsupervised information (neither pairwise similarity nor class label) simultaneously in composite quantization. A challenging problem arises: how can we jointly handle these three sorts of information across multiple domains in an efficient way? To tackle this challenge, we propose a novel semi-supervised deep quantization (SSDQ) model that takes both supervised and unsupervised information into account. The proposed SSDQ model is capable of incorporating the above three kinds of information into one single framework when utilizing composite quantization for accurate and efficient queries across different domains. More specifically, we employ a modified deep autoencoder for better latent representation and formulate pairwise similarity loss, supervised quantization loss as well as unsupervised distribution match loss to handle all three types of information. The extensive experiments demonstrate the significant improvement of SSDQ over several state-of-the-art methods on various datasets.</p>\n", "tags": ["Hashing-Methods", "Quantization", "CVPR", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [9.739940643310547, 12.221029281616211], "cluster": 6}, {"key": "wang2010sequential", "year": "2010", "citations": "328", "title": "Sequential Projection Learning For Hashing With Compact Codes", "abstract": "<p>Hashing based Approximate Nearest Neighbor\n(ANN) search has attracted much attention\ndue to its fast query time and drastically\nreduced storage. However, most of the hashing\nmethods either use random projections or\nextract principal directions from the data to\nderive hash functions. The resulting embedding\nsuffers from poor discrimination when\ncompact codes are used. In this paper, we\npropose a novel data-dependent projection\nlearning method such that each hash function\nis designed to correct the errors made by\nthe previous one sequentially. The proposed\nmethod easily adapts to both unsupervised\nand semi-supervised scenarios and shows significant\nperformance gains over the state-ofthe-art\nmethods on two large datasets containing\nup to 1 million points.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Compact-Codes", "Efficiency", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [12.560086250305176, 31.49847984313965], "cluster": 4}, {"key": "wang2014scalable", "year": "2015", "citations": "2", "title": "Scalable Similarity Learning Using Large Margin Neighborhood Embedding", "abstract": "<p>Classifying large-scale image data into object categories is an important\nproblem that has received increasing research attention. Given the huge amount\nof data, non-parametric approaches such as nearest neighbor classifiers have\nshown promising results, especially when they are underpinned by a learned\ndistance or similarity measurement. Although metric learning has been well\nstudied in the past decades, most existing algorithms are impractical to handle\nlarge-scale data sets. In this paper, we present an image similarity learning\nmethod that can scale well in both the number of images and the dimensionality\nof image descriptors. To this end, similarity comparison is restricted to each\nsample\u2019s local neighbors and a discriminative similarity measure is induced\nfrom large margin neighborhood embedding. We also exploit the ensemble of\nprojections so that high-dimensional features can be processed in a set of\nlower-dimensional subspaces in parallel without much performance compromise.\nThe similarity function is learned online using a stochastic gradient descent\nalgorithm in which the triplet sampling strategy is customized for quick\nconvergence of classification performance. The effectiveness of our proposed\nmodel is validated on several data sets with scales varying from tens of\nthousands to one million images. Recognition accuracies competitive with the\nstate-of-the-art performance are achieved with much higher efficiency and\nscalability.</p>\n", "tags": ["Efficiency", "Evaluation", "Distance-Metric-Learning", "Scalability"], "tsne_embedding": [22.87494659423828, 29.451894760131836], "cluster": 2}, {"key": "wang2015hamming", "year": "2015", "citations": "20", "title": "Hamming Compatible Quantization For Hashing", "abstract": "<p>Hashing is one of the effective techniques for fast\nApproximate Nearest Neighbour (ANN) search.\nTraditional single-bit quantization (SBQ) in most\nhashing methods incurs lots of quantization error\nwhich seriously degrades the search performance.\nTo address the limitation of SBQ, researchers have\nproposed promising multi-bit quantization (MBQ)\nmethods to quantize each projection dimension\nwith multiple bits. However, some MBQ methods\nneed to adopt specific distance for binary code\nmatching instead of the original Hamming distance,\nwhich would significantly decrease the retrieval\nspeed. Two typical MBQ methods Hierarchical\nQuantization and Double Bit Quantization\nretain the Hamming distance, but both of them only\nconsider the projection dimensions during quantization,\nignoring the neighborhood structure of raw\ndata inherent in Euclidean space. In this paper,\nwe propose a multi-bit quantization method named\nHamming Compatible Quantization (HCQ) to preserve\nthe capability of similarity metric between\nEuclidean space and Hamming space by utilizing\nthe neighborhood structure of raw data. Extensive\nexperiment results have shown our approach significantly\nimproves the performance of various stateof-the-art\nhashing methods while maintaining fast\nretrieval speed.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Quantization", "Similarity-Search", "Compact-Codes", "Evaluation"], "tsne_embedding": [-10.711970329284668, 37.398555755615234], "cluster": 8}, {"key": "wang2015semantic", "year": "2015", "citations": "149", "title": "Semantic Topic Multimodal Hashing For Cross-media Retrieval", "abstract": "<p>Multimodal hashing is essential to cross-media\nsimilarity search for its low storage cost and fast\nquery speed. Most existing multimodal hashing\nmethods embedded heterogeneous data into a common low-dimensional Hamming space, and then\nrounded the continuous embeddings to obtain the\nbinary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For\nthis purpose, a novel Semantic Topic Multimodal\nHashing (STMH) is developed by considering latent semantic information in coding procedure.\nIt\nfirst discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.\nThen the learned multimodal semantic features are\ntransformed into a common subspace by their correlations. Finally, each bit of unified hash code\ncan be generated directly by figuring out whether a\ntopic or concept is contained in a text or an image.\nTherefore, the obtained model by STMH is more\nsuitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method\noutperforms several state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Similarity-Search", "Memory-Efficiency", "Compact-Codes", "Evaluation"], "tsne_embedding": [8.993586540222168, -11.4234037399292], "cluster": 7}, {"key": "wang2016affinity", "year": "2016", "citations": "11", "title": "Affinity Preserving Quantization For Hashing: A Vector Quantization Approach To Learning Compact Binary Codes", "abstract": "<p>Hashing techniques are powerful for approximate nearest\nneighbour (ANN) search. Existing quantization methods in\nhashing are all focused on scalar quantization (SQ) which\nis inferior in utilizing the inherent data distribution. In this\npaper, we propose a novel vector quantization (VQ) method\nnamed affinity preserving quantization (APQ) to improve the\nquantization quality of projection values, which has significantly\nboosted the performance of state-of-the-art hashing\ntechniques. In particular, our method incorporates the neighbourhood\nstructure in the pre- and post-projection data space\ninto vector quantization. APQ minimizes the quantization errors\nof projection values as well as the loss of affinity property\nof original space. An effective algorithm has been proposed\nto solve the joint optimization problem in APQ, and\nthe extension to larger binary codes has been resolved by applying\nproduct quantization to APQ. Extensive experiments\nhave shown that APQ consistently outperforms the state-of-the-art\nquantization methods, and has significantly improved\nthe performance of various hashing techniques.</p>\n", "tags": ["Hashing-Methods", "Quantization", "AAAI", "Compact-Codes", "Evaluation"], "tsne_embedding": [11.61470890045166, 37.406471252441406], "cluster": 4}, {"key": "wang2016comprehensive", "year": "2016", "citations": "230", "title": "A Comprehensive Survey On Cross-modal Retrieval", "abstract": "<p>In recent years, cross-modal retrieval has drawn much attention due to the\nrapid growth of multimodal data. It takes one type of data as the query to\nretrieve relevant data of another type. For example, a user can use a text to\nretrieve relevant pictures or videos. Since the query and its retrieved results\ncan be of different modalities, how to measure the content similarity between\ndifferent modalities of data remains a challenge. Various methods have been\nproposed to deal with such a problem. In this paper, we first review a number\nof representative methods for cross-modal retrieval and classify them into two\nmain groups: 1) real-valued representation learning, and 2) binary\nrepresentation learning. Real-valued representation learning methods aim to\nlearn real-valued common representations for different modalities of data. To\nspeed up the cross-modal retrieval, a number of binary representation learning\nmethods are proposed to map different modalities of data into a common Hamming\nspace. Then, we introduce several multimodal datasets in the community, and\nshow the experimental results on two commonly used multimodal datasets. The\ncomparison reveals the characteristic of different kinds of cross-modal\nretrieval methods, which is expected to benefit both practical applications and\nfuture research. Finally, we discuss open problems and future research\ndirections.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-24.099246978759766, -36.7205810546875], "cluster": 5}, {"key": "wang2016contextual", "year": "2016", "citations": "7", "title": "Contextual Visual Similarity", "abstract": "<p>Measuring visual similarity is critical for image understanding. But what\nmakes two images similar? Most existing work on visual similarity assumes that\nimages are similar because they contain the same object instance or category.\nHowever, the reason why images are similar is much more complex. For example,\nfrom the perspective of category, a black dog image is similar to a white dog\nimage. However, in terms of color, a black dog image is more similar to a black\nhorse image than the white dog image. This example serves to illustrate that\nvisual similarity is ambiguous but can be made precise when given an explicit\ncontextual perspective. Based on this observation, we propose the concept of\ncontextual visual similarity. To be concrete, we examine the concept of\ncontextual visual similarity in the application domain of image search. Instead\nof providing only a single image for image similarity search (\\eg, Google image\nsearch), we require three images. Given a query image, a second positive image\nand a third negative image, dissimilar to the first two images, we define a\ncontextualized similarity search criteria. In particular, we learn feature\nweights over all the feature dimensions of each image such that the distance\nbetween the query image and the positive image is small and their distances to\nthe negative image are large after reweighting their features. The learned\nfeature weights encode the contextualized visual similarity specified by the\nuser and can be used for attribute specific image search. We also show the\nusefulness of our contextualized similarity weighting scheme for different\ntasks, such as answering visual analogy questions and unsupervised attribute\ndiscovery.</p>\n", "tags": ["Similarity-Search", "Image-Retrieval", "Unsupervised"], "tsne_embedding": [-31.161134719848633, 12.166328430175781], "cluster": 0}, {"key": "wang2016deep", "year": "2017", "citations": "192", "title": "Deep Supervised Hashing With Triplet Labels", "abstract": "<p>Hashing is one of the most popular and powerful approximate nearest neighbor\nsearch techniques for large-scale image retrieval. Most traditional hashing\nmethods first represent images as off-the-shelf visual features and then\nproduce hashing codes in a separate stage. However, off-the-shelf visual\nfeatures may not be optimally compatible with the hash code learning procedure,\nwhich may result in sub-optimal hash codes. Recently, deep hashing methods have\nbeen proposed to simultaneously learn image features and hash codes using deep\nneural networks and have shown superior performance over traditional hashing\nmethods. Most deep hashing methods are given supervised information in the form\nof pairwise labels or triplet labels. The current state-of-the-art deep hashing\nmethod DPSH~\\cite{li2015feature}, which is based on pairwise labels, performs\nimage feature learning and hash code learning simultaneously by maximizing the\nlikelihood of pairwise similarities. Inspired by DPSH~\\cite{li2015feature}, we\npropose a triplet label based deep hashing method which aims to maximize the\nlikelihood of the given triplet labels. Experimental results show that our\nmethod outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets,\nincluding the state-of-the-art method DPSH~\\cite{li2015feature} and all the\nprevious triplet label based deep hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-7.456912994384766, 5.866680145263672], "cluster": 1}, {"key": "wang2016learning", "year": "2016", "citations": "4", "title": "Learning A Deep \\(\\ell_\\infty\\) Encoder For Hashing", "abstract": "<p>We investigate the \\(\\ell_\\infty\\)-constrained representation which\ndemonstrates robustness to quantization errors, utilizing the tool of deep\nlearning. Based on the Alternating Direction Method of Multipliers (ADMM), we\nformulate the original convex minimization problem as a feed-forward neural\nnetwork, named \\textit{Deep \\(\\ell_\\infty\\) Encoder}, by introducing the novel\nBounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as\nnetwork biases. Such a structural prior acts as an effective network\nregularization, and facilitates the model initialization. We then investigate\nthe effective use of the proposed model in the application of hashing, by\ncoupling the proposed encoders under a supervised pairwise loss, to develop a\n\\textit{Deep Siamese \\(\\ell_\\infty\\) Network}, which can be optimized from end to\nend. Extensive experiments demonstrate the impressive performances of the\nproposed model. We also provide an in-depth analysis of its behaviors against\nthe competitors.</p>\n", "tags": ["Supervised", "Quantization", "Hashing-Methods", "Robustness"], "tsne_embedding": [17.899011611938477, -7.741804599761963], "cluster": 6}, {"key": "wang2016survey", "year": "2017", "citations": "929", "title": "A Survey On Learning To Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the\ndatabase such that the distances from them to the query point are the smallest.\nLearning to hash is one of the major solutions to this problem and has been\nwidely studied recently. In this paper, we present a comprehensive survey of\nthe learning to hash algorithms, categorize them according to the manners of\npreserving the similarities into: pairwise similarity preserving, multiwise\nsimilarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity\npreserving as the objective function is very different though quantization, as\nwe show, can be derived from preserving the pairwise similarities. In addition,\nwe present the evaluation protocols, and the general performance analysis, and\npoint out that the quantization algorithms perform superiorly in terms of\nsearch accuracy, search time cost, and space cost. Finally, we introduce a few\nemerging topics.</p>\n", "tags": ["Survey-Paper", "Quantization", "Evaluation", "Hashing-Methods"], "tsne_embedding": [12.896535873413086, 6.153784275054932], "cluster": 6}, {"key": "wang2016unsupervised", "year": "2016", "citations": "0", "title": "Unsupervised Cross-media Hashing With Structure Preservation", "abstract": "<p>Recent years have seen the exponential growth of heterogeneous multimedia\ndata. The need for effective and accurate data retrieval from heterogeneous\ndata sources has attracted much research interest in cross-media retrieval.\nHere, given a query of any media type, cross-media retrieval seeks to find\nrelevant results of different media types from heterogeneous data sources. To\nfacilitate large-scale cross-media retrieval, we propose a novel unsupervised\ncross-media hashing method. Our method incorporates local affinity and distance\nrepulsion constraints into a matrix factorization framework. Correspondingly,\nthe proposed method learns hash functions that generates unified hash codes\nfrom different media types, while ensuring intrinsic geometric structure of the\ndata distribution is preserved. These hash codes empower the similarity between\ndata of different media types to be evaluated directly. Experimental results on\ntwo large-scale multimedia datasets demonstrate the effectiveness of the\nproposed method, where we outperform the state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Tools-&-Libraries", "Datasets", "Unsupervised"], "tsne_embedding": [14.202760696411133, -36.35740661621094], "cluster": 7}, {"key": "wang2017attention", "year": "2017", "citations": "12", "title": "An Attention-based Deep Net For Learning To Rank", "abstract": "<p>In information retrieval, learning to rank constructs a machine-based ranking\nmodel which given a query, sorts the search results by their degree of\nrelevance or importance to the query. Neural networks have been successfully\napplied to this problem, and in this paper, we propose an attention-based deep\nneural network which better incorporates different embeddings of the queries\nand search results with an attention-based mechanism. This model also applies a\ndecoder mechanism to learn the ranks of the search results in a listwise\nfashion. The embeddings are trained with convolutional neural networks or the\nword2vec model. We demonstrate the performance of this model with image\nretrieval and text querying data sets.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-4.964666843414307, -14.942855834960938], "cluster": 1}, {"key": "wang2017composite", "year": "2018", "citations": "27", "title": "Composite Quantization", "abstract": "<p>This paper studies the compact coding approach to approximate nearest\nneighbor search. We introduce a composite quantization framework. It uses the\ncomposition of several (\\(M\\)) elements, each of which is selected from a\ndifferent dictionary, to accurately approximate a \\(D\\)-dimensional vector, thus\nyielding accurate search, and represents the data vector by a short code\ncomposed of the indices of the selected elements in the corresponding\ndictionaries. Our key contribution lies in introducing a near-orthogonality\nconstraint, which makes the search efficiency is guaranteed as the cost of the\ndistance computation is reduced to \\(O(M)\\) from \\(O(D)\\) through a distance table\nlookup scheme. The resulting approach is called near-orthogonal composite\nquantization. We theoretically justify the equivalence between near-orthogonal\ncomposite quantization and minimizing an upper bound of a function formed by\njointly considering the quantization error and the search cost according to a\ngeneralized triangle inequality. We empirically show the efficacy of the\nproposed approach over several benchmark datasets. In addition, we demonstrate\nthe superior performances in other three applications: combination with\ninverted multi-index, quantizing the query for mobile search, and inner-product\nsimilarity search.</p>\n", "tags": ["Efficiency", "Quantization", "Vector-Indexing", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [13.831772804260254, 44.54387283325195], "cluster": 4}, {"key": "wang2017flash", "year": "2017", "citations": "9", "title": "FLASH: Randomized Algorithms Accelerated Over CPU-GPU For Ultra-high Dimensional Similarity Search", "abstract": "<p>We present FLASH (\\textbf{F}ast \\textbf{L}SH \\textbf{A}lgorithm for\n\\textbf{S}imilarity search accelerated with \\textbf{H}PC), a similarity search\nsystem for ultra-high dimensional datasets on a single machine, that does not\nrequire similarity computations and is tailored for high-performance computing\nplatforms. By leveraging a LSH style randomized indexing procedure and\ncombining it with several principled techniques, such as reservoir sampling,\nrecent advances in one-pass minwise hashing, and count based estimations, we\nreduce the computational and parallelization costs of similarity search, while\nretaining sound theoretical guarantees.\n  We evaluate FLASH on several real, high-dimensional datasets from different\ndomains, including text, malicious URL, click-through prediction, social\nnetworks, etc. Our experiments shed new light on the difficulties associated\nwith datasets having several million dimensions. Current state-of-the-art\nimplementations either fail on the presented scale or are orders of magnitude\nslower than FLASH. FLASH is capable of computing an approximate k-NN graph,\nfrom scratch, over the full webspam dataset (1.3 billion nonzeros) in less than\n10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam\ndataset, using brute-force (\\(n^2D\\)), will require at least 20 teraflops. We\nprovide CPU and GPU implementations of FLASH for replicability of our results.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [38.6494026184082, 31.46286964416504], "cluster": 2}, {"key": "wang2017learning", "year": "2018", "citations": "529", "title": "Learning Two-branch Neural Networks For Image-text Matching Tasks", "abstract": "<p>Image-language matching tasks have recently attracted a lot of attention in\nthe computer vision field. These tasks include image-sentence matching, i.e.,\ngiven an image query, retrieving relevant sentences and vice versa, and\nregion-phrase matching or visual grounding, i.e., matching a phrase to relevant\nregions. This paper investigates two-branch neural networks for learning the\nsimilarity between these two data modalities. We propose two network structures\nthat produce different output representations. The first one, referred to as an\nembedding network, learns an explicit shared latent embedding space with a\nmaximum-margin ranking loss and novel neighborhood constraints. Compared to\nstandard triplet sampling, we perform improved neighborhood sampling that takes\nneighborhood information into consideration while constructing mini-batches.\nThe second network structure, referred to as a similarity network, fuses the\ntwo branches via element-wise product and is trained with regression loss to\ndirectly predict a similarity score. Extensive experiments show that our\nnetworks achieve high accuracies for phrase localization on the Flickr30K\nEntities dataset and for bi-directional image-sentence retrieval on Flickr30K\nand MSCOCO datasets.</p>\n", "tags": ["Datasets"], "tsne_embedding": [-13.496421813964844, -6.3064422607421875], "cluster": 1}, {"key": "wang2017subspace", "year": "2017", "citations": "0", "title": "Subspace Approximation For Approximate Nearest Neighbor Search In NLP", "abstract": "<p>Most natural language processing tasks can be formulated as the approximated\nnearest neighbor search problem, such as word analogy, document similarity,\nmachine translation. Take the question-answering task as an example, given a\nquestion as the query, the goal is to search its nearest neighbor in the\ntraining dataset as the answer. However, existing methods for approximate\nnearest neighbor search problem may not perform well owing to the following\npractical challenges: 1) there are noise in the data; 2) the large scale\ndataset yields a huge retrieval space and high search time complexity.\n  In order to solve these problems, we propose a novel approximate nearest\nneighbor search framework which i) projects the data to a subspace based\nspectral analysis which eliminates the influence of noise; ii) partitions the\ntraining dataset to different groups in order to reduce the search space.\nSpecifically, the retrieval space is reduced from \\(O(n)\\) to \\(O(log n)\\) (where\n\\(n\\) is the number of data points in the training dataset). We prove that the\nretrieved nearest neighbor in the projected subspace is the same as the one in\nthe original feature space. We demonstrate the outstanding performance of our\nframework on real-world natural language processing tasks.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [-27.975982666015625, -11.306546211242676], "cluster": 5}, {"key": "wang2017supervised", "year": "2018", "citations": "26", "title": "Supervised Deep Hashing For Hierarchical Labeled Data", "abstract": "<p>Recently, hashing methods have been widely used in large-scale image\nretrieval. However, most existing hashing methods did not consider the\nhierarchical relation of labels, which means that they ignored the rich\ninformation stored in the hierarchy. Moreover, most of previous works treat\neach bit in a hash code equally, which does not meet the scenario of\nhierarchical labeled data. In this paper, we propose a novel deep hashing\nmethod, called supervised hierarchical deep hashing (SHDH), to perform hash\ncode learning for hierarchical labeled data. Specifically, we define a novel\nsimilarity formula for hierarchical labeled data by weighting each layer, and\ndesign a deep convolutional neural network to obtain a hash code for each data\npoint. Extensive experiments on several real-world public datasets show that\nthe proposed method outperforms the state-of-the-art baselines in the image\nretrieval task.</p>\n", "tags": ["Hashing-Methods", "Scalability", "AAAI", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-3.53367018699646, -5.150185585021973], "cluster": 1}, {"key": "wang2017survey", "year": "2017", "citations": "929", "title": "A Survey On Learning To Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the\nquery point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this\npaper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving\nthe similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different\nthough quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation\nprotocols, and the general performance analysis, and point out that the quantization algori</p>\n", "tags": ["Survey-Paper", "Quantization", "Evaluation", "Hashing-Methods"], "tsne_embedding": [13.000655174255371, 6.2062153816223145], "cluster": 6}, {"key": "wang2018adaptive", "year": "2018", "citations": "4", "title": "Adaptive Co-weighting Deep Convolutional Features For Object Retrieval", "abstract": "<p>Aggregating deep convolutional features into a global image vector has\nattracted sustained attention in image retrieval. In this paper, we propose an\nefficient unsupervised aggregation method that uses an adaptive Gaussian filter\nand an elementvalue sensitive vector to co-weight deep features. Specifically,\nthe Gaussian filter assigns large weights to features of region-of-interests\n(RoI) by adaptively determining the RoI\u2019s center, while the element-value\nsensitive channel vector suppresses burstiness phenomenon by assigning small\nweights to feature maps with large sum values of all locations. Experimental\nresults on benchmark datasets validate the proposed two weighting schemes both\neffectively improve the discrimination power of image vectors. Furthermore,\nwith the same experimental setting, our method outperforms other very recent\naggregation approaches by a considerable margin.</p>\n", "tags": ["Unsupervised", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-22.225536346435547, 17.316085815429688], "cluster": 8}, {"key": "wang2018deep", "year": "2019", "citations": "45", "title": "Deep Metric Learning By Online Soft Mining And Class-aware Attention", "abstract": "<p>Deep metric learning aims to learn a deep embedding that can capture the\nsemantic similarity of data points. Given the availability of massive training\nsamples, deep metric learning is known to suffer from slow convergence due to a\nlarge fraction of trivial samples. Therefore, most existing methods generally\nresort to sample mining strategies for selecting nontrivial samples to\naccelerate convergence and improve performance. In this work, we identify two\ncritical limitations of the sample mining methods, and provide solutions for\nboth of them. First, previous mining methods assign one binary score to each\nsample, i.e., dropping or keeping it, so they only selects a subset of relevant\nsamples in a mini-batch. Therefore, we propose a novel sample mining method,\ncalled Online Soft Mining (OSM), which assigns one continuous score to each\nsample to make use of all samples in the mini-batch. OSM learns extended\nmanifolds that preserve useful intraclass variances by focusing on more similar\npositives. Second, the existing methods are easily influenced by outliers as\nthey are generally included in the mined subset. To address this, we introduce\nClass-Aware Attention (CAA) that assigns little attention to abnormal data\nsamples. Furthermore, by combining OSM and CAA, we propose a novel weighted\ncontrastive loss to learn discriminative embeddings. Extensive experiments on\ntwo fine-grained visual categorisation datasets and two video-based person\nre-identification benchmarks show that our method significantly outperforms the\nstate-of-the-art.</p>\n", "tags": ["AAAI", "Evaluation", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-17.888534545898438, -13.960742950439453], "cluster": 1}, {"key": "wang2019camp", "year": "2019", "citations": "296", "title": "CAMP: Cross-modal Adaptive Message Passing For Text-image Retrieval", "abstract": "<p>Text-image cross-modal retrieval is a challenging task in the field of\nlanguage and vision. Most previous approaches independently embed images and\nsentences into a joint embedding space and compare their similarities. However,\nprevious approaches rarely explore the interactions between images and\nsentences before calculating similarities in the joint space. Intuitively, when\nmatching between images and sentences, human beings would alternatively attend\nto regions in images and words in sentences, and select the most salient\ninformation considering the interaction between both modalities. In this paper,\nwe propose Cross-modal Adaptive Message Passing (CAMP), which adaptively\ncontrols the information flow for message passing across modalities. Our\napproach not only takes comprehensive and fine-grained cross-modal interactions\ninto account, but also properly handles negative pairs and irrelevant\ninformation with an adaptive gating scheme. Moreover, instead of conventional\njoint embedding approaches for text-image matching, we infer the matching score\nbased on the fused features, and propose a hardest negative binary\ncross-entropy loss for training. Results on COCO and Flickr30k significantly\nsurpass state-of-the-art methods, demonstrating the effectiveness of our\napproach.</p>\n", "tags": ["ICCV", "Multimodal-Retrieval", "Image-Retrieval"], "tsne_embedding": [-21.877656936645508, -7.52955436706543], "cluster": 1}, {"key": "wang2019cluster", "year": "2020", "citations": "25", "title": "Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search", "abstract": "<p>Large-scale cross-modal hashing similarity retrieval has attracted more and\nmore attention in modern search applications such as search engines and\nautopilot, showing great superiority in computation and storage. However,\ncurrent unsupervised cross-modal hashing methods still have some limitations:\n(1)many methods relax the discrete constraints to solve the optimization\nobjective which may significantly degrade the retrieval performance;(2)most\nexisting hashing model project heterogenous data into a common latent space,\nwhich may always lose sight of diversity in heterogenous data;(3)transforming\nreal-valued data point to binary codes always results in abundant loss of\ninformation, producing the suboptimal continuous latent space. To overcome\nabove problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)\nmethod is proposed. Specifically, CUH jointly performs the multi-view\nclustering that projects the original data points from different modalities\ninto its own low-dimensional latent semantic space and finds the cluster\ncentroid points and the common clustering indicators in its own low-dimensional\nspace, and learns the compact hash codes and the corresponding linear hash\nfunctions. An discrete optimization framework is developed to learn the unified\nbinary codes across modalities under the guidance cluster-wise code-prototypes.\nThe reasonableness and effectiveness of CUH is well demonstrated by\ncomprehensive experiments on diverse benchmark datasets.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Neural-Hashing", "Compact-Codes", "Tools-&-Libraries", "Hashing-Methods", "Supervised", "Unsupervised", "Similarity-Search", "Scalability"], "tsne_embedding": [8.265368461608887, 14.16634750366211], "cluster": 6}, {"key": "wang2019cross", "year": "2020", "citations": "219", "title": "Cross-modal Scene Graph Matching For Relationship-aware Image-text Retrieval", "abstract": "<p>Image-text retrieval of natural scenes has been a popular research topic.\nSince image and text are heterogeneous cross-modal data, one of the key\nchallenges is how to learn comprehensive yet unified representations to express\nthe multi-modal data. A natural scene image mainly involves two kinds of visual\nconcepts, objects and their relationships, which are equally essential to\nimage-text retrieval. Therefore, a good representation should account for both\nof them. In the light of recent success of scene graph in many CV and NLP tasks\nfor describing complex natural scenes, we propose to represent image and text\nwith two kinds of scene graphs: visual scene graph (VSG) and textual scene\ngraph (TSG), each of which is exploited to jointly characterize objects and\nrelationships in the corresponding modality. The image-text retrieval task is\nthen naturally formulated as cross-modal scene graph matching. Specifically, we\ndesign two particular scene graph encoders in our model for VSG and TSG, which\ncan refine the representation of each node on the graph by aggregating\nneighborhood information. As a result, both object-level and relationship-level\ncross-modal features can be obtained, which favorably enables us to evaluate\nthe similarity of image and text in the two levels in a more plausible way. We\nachieve state-of-the-art results on Flickr30k and MSCOCO, which verifies the\nadvantages of our graph matching based approach for image-text retrieval.</p>\n", "tags": ["Text-Retrieval"], "tsne_embedding": [56.036922454833984, -9.867879867553711], "cluster": 9}, {"key": "wang2019deep", "year": "2019", "citations": "2", "title": "Deep Collaborative Discrete Hashing With Semantic-invariant Structure", "abstract": "<p>Existing deep hashing approaches fail to fully explore semantic correlations\nand neglect the effect of linguistic context on visual attention learning,\nleading to inferior performance. This paper proposes a dual-stream learning\nframework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs\na discriminative common discrete space by collaboratively incorporating the\nshared and individual semantics deduced from visual features and semantic\nlabels. Specifically, the context-aware representations are generated by\nemploying the outer product of visual embeddings and semantic encodings.\nMoreover, we reconstruct the labels and introduce the focal loss to take\nadvantage of frequent and rare concepts. The common binary code space is built\non the joint learning of the visual representations attended by language, the\nsemantic-invariant structure construction and the label distribution\ncorrection. Extensive experiments demonstrate the superiority of our method.</p>\n", "tags": ["Evaluation", "Neural-Hashing", "Tools-&-Libraries", "Compact-Codes", "Hashing-Methods", "Multimodal-Retrieval"], "tsne_embedding": [-23.06521224975586, -22.145957946777344], "cluster": 5}, {"key": "wang2019fusion", "year": "2019", "citations": "15", "title": "Fusion-supervised Deep Cross-modal Hashing", "abstract": "<p>Deep hashing has recently received attention in cross-modal retrieval for its\nimpressive advantages. However, existing hashing methods for cross-modal\nretrieval cannot fully capture the heterogeneous multi-modal correlation and\nexploit the semantic information. In this paper, we propose a novel\n<em>Fusion-supervised Deep Cross-modal Hashing</em> (FDCH) approach. Firstly,\nFDCH learns unified binary codes through a fusion hash network with paired\nsamples as input, which effectively enhances the modeling of the correlation of\nheterogeneous multi-modal data. Then, these high-quality unified hash codes\nfurther supervise the training of the modality-specific hash networks for\nencoding out-of-sample queries. Meanwhile, both pair-wise similarity\ninformation and classification information are embedded in the hash networks\nunder one stream framework, which simultaneously preserves cross-modal\nsimilarity and keeps semantic consistency. Experimental results on two\nbenchmark datasets demonstrate the state-of-the-art performance of FDCH.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [8.835403442382812, 4.331428527832031], "cluster": 6}, {"key": "wang2019learning", "year": "2019", "citations": "127", "title": "Learning Cross-modal Embeddings With Adversarial Networks For Cooking Recipes And Food Images", "abstract": "<p>Food computing is playing an increasingly important role in human daily life,\nand has found tremendous applications in guiding human behavior towards smart\nfood consumption and healthy lifestyle. An important task under the\nfood-computing umbrella is retrieval, which is particularly helpful for health\nrelated applications, where we are interested in retrieving important\ninformation about food (e.g., ingredients, nutrition, etc.). In this paper, we\ninvestigate an open research task of cross-modal retrieval between cooking\nrecipes and food images, and propose a novel framework Adversarial Cross-Modal\nEmbedding (ACME) to resolve the cross-modal retrieval task in food domains.\nSpecifically, the goal is to learn a common embedding feature space between the\ntwo modalities, in which our approach consists of several novel ideas: (i)\nlearning by using a new triplet loss scheme together with an effective sampling\nstrategy, (ii) imposing modality alignment using an adversarial learning\nstrategy, and (iii) imposing cross-modal translation consistency such that the\nembedding of one modality is able to recover some important information of\ncorresponding instances in the other modality. ACME achieves the\nstate-of-the-art performance on the benchmark Recipe1M dataset, validating the\nefficacy of the proposed technique.</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Robustness", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-42.094783782958984, 31.078960418701172], "cluster": 0}, {"key": "wang2019memory", "year": "2019", "citations": "33", "title": "A Memory-efficient Sketch Method For Estimating High Similarities In Streaming Sets", "abstract": "<p>Estimating set similarity and detecting highly similar sets are fundamental\nproblems in areas such as databases, machine learning, and information\nretrieval. MinHash is a well-known technique for approximating Jaccard\nsimilarity of sets and has been successfully used for many applications such as\nsimilarity search and large scale learning. Its two compressed versions, b-bit\nMinHash and Odd Sketch, can significantly reduce the memory usage of the\noriginal MinHash method, especially for estimating high similarities (i.e.,\nsimilarities around 1). Although MinHash can be applied to static sets as well\nas streaming sets, of which elements are given in a streaming fashion and\ncardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd\nSketch fail to deal with streaming data. To solve this problem, we design a\nmemory efficient sketch method, MaxLogHash, to accurately estimate Jaccard\nsimilarities in streaming sets. Compared to MinHash, our method uses smaller\nsized registers (each register consists of less than 7 bits) to build a compact\nsketch for each set. We also provide a simple yet accurate estimator for\ninferring Jaccard similarity from MaxLogHash sketches. In addition, we derive\nformulas for bounding the estimation error and determine the smallest necessary\nmemory usage (i.e., the number of registers used for a MaxLogHash sketch) for\nthe desired accuracy. We conduct experiments on a variety of datasets, and\nexperimental results show that our method MaxLogHash is about 5 times more\nmemory efficient than MinHash with the same accuracy and computational cost for\nestimating high similarities.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Similarity-Search", "Memory-Efficiency", "Datasets", "KDD"], "tsne_embedding": [21.857057571411133, 32.09434509277344], "cluster": 4}, {"key": "wang2019multi", "year": "2019", "citations": "736", "title": "Multi-similarity Loss With General Pair Weighting For Deep Metric Learning", "abstract": "<p>A family of loss functions built on pair-based computation have been proposed\nin the literature which provide a myriad of solutions for deep metric learning.\nIn this paper, we provide a general weighting framework for understanding\nrecent pair-based loss functions. Our contributions are three-fold: (1) we\nestablish a General Pair Weighting (GPW) framework, which casts the sampling\nproblem of deep metric learning into a unified view of pair weighting through\ngradient analysis, providing a powerful tool for understanding recent\npair-based loss functions; (2) we show that with GPW, various existing\npair-based methods can be compared and discussed comprehensively, with clear\ndifferences and key limitations identified; (3) we propose a new loss called\nmulti-similarity loss (MS loss) under the GPW, which is implemented in two\niterative steps (i.e., mining and weighting). This allows it to fully consider\nthree similarities for pair weighting, providing a more principled approach for\ncollecting and weighting informative pairs. Finally, the proposed MS loss\nobtains new state-of-the-art performance on four image retrieval benchmarks,\nwhere it outperforms the most recent approaches, such as\nABE\\cite{Kim_2018_ECCV} and HTL by a large margin: 60.6% to 65.7% on CUB200,\nand 80.9% to 88.0% on In-Shop Clothes Retrieval dataset at Recall@1. Code is\navailable at https://github.com/MalongTech/research-ms-loss.</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-4.876289367675781, -20.423904418945312], "cluster": 1}, {"key": "wang2019ranked", "year": "2019", "citations": "233", "title": "Ranked List Loss For Deep Metric Learning", "abstract": "<p>The objective of deep metric learning (DML) is to learn embeddings that can\ncapture semantic similarity and dissimilarity information among data points.\nExisting pairwise or tripletwise loss functions used in DML are known to suffer\nfrom slow convergence due to a large proportion of trivial pairs or triplets as\nthe model improves. To improve this, ranking-motivated structured losses are\nproposed recently to incorporate multiple examples and exploit the structured\ninformation among them. They converge faster and achieve state-of-the-art\nperformance. In this work, we unveil two limitations of existing\nranking-motivated structured losses and propose a novel ranked list loss to\nsolve both of them. First, given a query, only a fraction of data points is\nincorporated to build the similarity structure. Consequently, some useful\nexamples are ignored and the structure is less informative. To address this, we\npropose to build a set-based similarity structure by exploiting all instances\nin the gallery. The learning setting can be interpreted as few-shot retrieval:\ngiven a mini-batch, every example is iteratively used as a query, and the rest\nones compose the gallery to search, i.e., the support set in few-shot setting.\nThe rest examples are split into a positive set and a negative set. For every\nmini-batch, the learning objective of ranked list loss is to make the query\ncloser to the positive set than to the negative set by a margin. Second,\nprevious methods aim to pull positive pairs as close as possible in the\nembedding space. As a result, the intraclass data distribution tends to be\nextremely compressed. In contrast, we propose to learn a hypersphere for each\nclass in order to preserve useful similarity structure inside it, which\nfunctions as regularisation. Extensive experiments demonstrate the superiority\nof our proposal by comparing with the state-of-the-art methods.</p>\n", "tags": ["CVPR", "Evaluation", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning"], "tsne_embedding": [-10.777486801147461, -13.750746726989746], "cluster": 1}, {"key": "wang2019supervised", "year": "2016", "citations": "82", "title": "Supervised Quantization For Similarity Search", "abstract": "<p>In this paper, we address the problem of searching for semantically similar\nimages from a large database. We present a compact coding approach, supervised\nquantization. Our approach simultaneously learns feature selection that\nlinearly transforms the database points into a low-dimensional discriminative\nsubspace, and quantizes the data points in the transformed space. The\noptimization criterion is that the quantized points not only approximate the\ntransformed points accurately, but also are semantically separable: the points\nbelonging to a class lie in a cluster that is not overlapped with other\nclusters corresponding to other classes, which is formulated as a\nclassification problem. The experiments on several standard datasets show the\nsuperiority of our approach over the state-of-the art supervised hashing and\nunsupervised quantization algorithms.</p>\n", "tags": ["Hashing-Methods", "Quantization", "CVPR", "Similarity-Search", "Datasets", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [22.906343460083008, 18.48953628540039], "cluster": 2}, {"key": "wang2020asymmetric", "year": "2021", "citations": "21", "title": "Asymmetric Correlation Quantization Hashing For Cross-modal Retrieval", "abstract": "<p>Due to the superiority in similarity computation and database storage for\nlarge-scale multiple modalities data, cross-modal hashing methods have\nattracted extensive attention in similarity retrieval across the heterogeneous\nmodalities. However, there are still some limitations to be further taken into\naccount: (1) most current CMH methods transform real-valued data points into\ndiscrete compact binary codes under the binary constraints, limiting the\ncapability of representation for original data on account of abundant loss of\ninformation and producing suboptimal hash codes; (2) the discrete binary\nconstraint learning model is hard to solve, where the retrieval performance may\ngreatly reduce by relaxing the binary constraints for large quantization error;\n(3) handling the learning problem of CMH in a symmetric framework, leading to\ndifficult and complex optimization objective. To address above challenges, in\nthis paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method\nis proposed. Specifically, ACQH learns the projection matrixs of heterogeneous\nmodalities data points for transforming query into a low-dimensional\nreal-valued vector in latent semantic space and constructs the stacked\ncompositional quantization embedding in a coarse-to-fine manner for indicating\ndatabase points by a series of learnt real-valued codeword in the codebook with\nthe help of pointwise label information regression simultaneously. Besides, the\nunified hash codes across modalities can be directly obtained by the discrete\niterative optimization framework devised in the paper. Comprehensive\nexperiments on diverse three benchmark datasets have shown the effectiveness\nand rationality of ACQH.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Scalability", "Similarity-Search", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [10.599377632141113, 14.837767601013184], "cluster": 6}, {"key": "wang2020consensus", "year": "2020", "citations": "149", "title": "Consensus-aware Visual-semantic Embedding For Image-text Matching", "abstract": "<p>Image-text matching plays a central role in bridging vision and language.\nMost existing approaches only rely on the image-text instance pair to learn\ntheir representations, thereby exploiting their matching relationships and\nmaking the corresponding alignments. Such approaches only exploit the\nsuperficial associations contained in the instance pairwise data, with no\nconsideration of any external commonsense knowledge, which may hinder their\ncapabilities to reason the higher-level relationships between image and text.\nIn this paper, we propose a Consensus-aware Visual-Semantic Embedding (CVSE)\nmodel to incorporate the consensus information, namely the commonsense\nknowledge shared between both modalities, into image-text matching.\nSpecifically, the consensus information is exploited by computing the\nstatistical co-occurrence correlations between the semantic concepts from the\nimage captioning corpus and deploying the constructed concept correlation graph\nto yield the consensus-aware concept (CAC) representations. Afterwards, CVSE\nlearns the associations and alignments between image and text based on the\nexploited consensus as well as the instance-level representations for both\nmodalities. Extensive experiments conducted on two public datasets verify that\nthe exploited consensus makes significant contributions to constructing more\nmeaningful visual-semantic embeddings, with the superior performances over the\nstate-of-the-art approaches on the bidirectional image and text retrieval task.\nOur code of this paper is available at: https://github.com/BruceW91/CVSE.</p>\n", "tags": ["Text-Retrieval", "Datasets"], "tsne_embedding": [-20.89460563659668, -9.000632286071777], "cluster": 1}, {"key": "wang2020cosea", "year": "2020", "citations": "3", "title": "COSEA: Convolutional Code Search With Layer-wise Attention", "abstract": "<p>Semantic code search, which aims to retrieve code snippets relevant to a\ngiven natural language query, has attracted many research efforts with the\npurpose of accelerating software development. The huge amount of online\npublicly available code repositories has prompted the employment of deep\nlearning techniques to build state-of-the-art code search models. Particularly,\nthey leverage deep neural networks to embed codes and queries into a unified\nsemantic vector space and then use the similarity between code\u2019s and query\u2019s\nvectors to approximate the semantic correlation between code and the query.\nHowever, most existing studies overlook the code\u2019s intrinsic structural logic,\nwhich indeed contains a wealth of semantic information, and fails to capture\nintrinsic features of codes. In this paper, we propose a new deep learning\narchitecture, COSEA, which leverages convolutional neural networks with\nlayer-wise attention to capture the valuable code\u2019s intrinsic structural logic.\nTo further increase the learning efficiency of COSEA, we propose a variant of\ncontrastive loss for training the code search model, where the ground-truth\ncode should be distinguished from the most similar negative sample. We have\nimplemented a prototype of COSEA. Extensive experiments over existing public\ndatasets of Python and SQL have demonstrated that COSEA can achieve significant\nimprovements over state-of-the-art methods on code search tasks.</p>\n", "tags": ["Efficiency", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [19.137908935546875, -16.367080688476562], "cluster": 7}, {"key": "wang2020cross", "year": "2020", "citations": "219", "title": "Cross-modal Food Retrieval: Learning A Joint Embedding Of Food Images And Recipes With Semantic Consistency And Attention Mechanism", "abstract": "<p>Food retrieval is an important task to perform analysis of food-related\ninformation, where we are interested in retrieving relevant information about\nthe queried food item such as ingredients, cooking instructions, etc. In this\npaper, we investigate cross-modal retrieval between food images and cooking\nrecipes. The goal is to learn an embedding of images and recipes in a common\nfeature space, such that the corresponding image-recipe embeddings lie close to\none another. Two major challenges in addressing this problem are 1) large\nintra-variance and small inter-variance across cross-modal food data; and 2)\ndifficulties in obtaining discriminative recipe representations. To address\nthese two problems, we propose Semantic-Consistent and Attention-based Networks\n(SCAN), which regularize the embeddings of the two modalities through aligning\noutput semantic probabilities. Besides, we exploit a self-attention mechanism\nto improve the embedding of recipes. We evaluate the performance of the\nproposed method on the large-scale Recipe1M dataset, and show that we can\noutperform several state-of-the-art cross-modal retrieval strategies for food\nimages and cooking recipes by a significant margin.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-41.92808532714844, 30.96540069580078], "cluster": 0}, {"key": "wang2020deep", "year": "2020", "citations": "9", "title": "Deep Collaborative Discrete Hashing With Semantic-invariant Structure", "abstract": "<p>Existing deep hashing approaches fail to fully explore semantic correlations and neglect the effect of linguistic context on visual attention learning, leading to inferior performance. This paper proposes a dual-stream learning framework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantic labels. Specifically, the context-aware representations are generated by employing the outer product of visual embeddings and semantic encodings. Moreover, we reconstruct the labels and introduce the focal loss to take advantage of frequent and rare concepts. The common binary code space is built on the joint learning of the visual representations attended by language, the semantic-invariant structure construction and the label distribution correction. Extensive experiments demonstrate the superiority of our method.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [-23.066007614135742, -22.14589500427246], "cluster": 5}, {"key": "wang2020distilling", "year": "2021", "citations": "21", "title": "Distilling Knowledge By Mimicking Features", "abstract": "<p>Knowledge distillation (KD) is a popular method to train efficient networks\n(\u201cstudent\u201d) with the help of high-capacity networks (\u201cteacher\u201d). Traditional\nmethods use the teacher\u2019s soft logits as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher\u2019s features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature\u2019s magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy. We provide theoretical analyses of how LSH facilitates\nfeature direction mimicking, and further extend feature mimicking to\nmulti-label recognition and object detection.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [27.07362937927246, -38.903804779052734], "cluster": 7}, {"key": "wang2020faster", "year": "2020", "citations": "54", "title": "Faster Person Re-identification", "abstract": "<p>Fast person re-identification (ReID) aims to search person images quickly and\naccurately. The main idea of recent fast ReID methods is the hashing algorithm,\nwhich learns compact binary codes and performs fast Hamming distance and\ncounting sort. However, a very long code is needed for high accuracy (e.g.\n2048), which compromises search speed. In this work, we introduce a new\nsolution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code\nsearch strategy, which complementarily uses short and long codes, achieving\nboth faster speed and better accuracy. It uses shorter codes to coarsely rank\nbroad matching similarities and longer codes to refine only a few top\ncandidates for more accurate instance ReID. Specifically, we design an\nAll-in-One (AiO) framework together with a Distance Threshold Optimization\n(DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of\ndifferent lengths in a single model. It learns multiple codes in a pyramid\nstructure, and encourage shorter codes to mimic longer codes by\nself-distillation. DTO solves a complex threshold search problem by a simple\noptimization process, and the balance between accuracy and speed is easily\ncontrolled by a single parameter. It formulates the optimization target as a\n\\(F_{\\beta}\\) score that can be optimised by Gaussian cumulative distribution\nfunctions. Experimental results on 2 datasets show that our proposed method\n(CtF) is not only 8% more accurate but also 5x faster than contemporary hashing\nReID methods. Compared with non-hashing ReID methods, CtF is \\(50\\times\\) faster\nwith comparable accuracy. Code is available at\nhttps://github.com/wangguanan/light-reid.</p>\n", "tags": ["Compact-Codes", "Tools-&-Libraries", "Hashing-Methods", "Datasets"], "tsne_embedding": [4.311660289764404, 23.85145378112793], "cluster": 8}, {"key": "wang2020online", "year": "2020", "citations": "47", "title": "Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval", "abstract": "<p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "SIGIR", "Evaluation"], "tsne_embedding": [28.840089797973633, 3.1371724605560303], "cluster": 2}, {"key": "wang2021comprehensive", "year": "2021", "citations": "119", "title": "A Comprehensive Survey And Experimental Comparison Of Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor search (ANNS) constitutes an important operation\nin a multitude of applications, including recommendation systems, information\nretrieval, and pattern recognition. In the past decade, graph-based ANNS\nalgorithms have been the leading paradigm in this domain, with dozens of\ngraph-based ANNS algorithms proposed. Such algorithms aim to provide effective,\nefficient solutions for retrieving the nearest neighbors for a given query.\nNevertheless, these efforts focus on developing and optimizing algorithms with\ndifferent approaches, so there is a real need for a comprehensive survey about\nthe approaches\u2019 relative performance, strengths, and pitfalls. Thus here we\nprovide a thorough comparative analysis and experimental evaluation of 13\nrepresentative graph-based ANNS algorithms via a new taxonomy and fine-grained\npipeline. We compared each algorithm in a uniform test environment on eight\nreal-world datasets and 12 synthetic datasets with varying sizes and\ncharacteristics. Our study yields novel discoveries, offerings several useful\nprinciples to improve algorithms, thus designing an optimized method that\noutperforms the state-of-the-art algorithms. This effort also helped us\npinpoint algorithms\u2019 working portions, along with rule-of-thumb recommendations\nabout promising research directions and suitable algorithms for practitioners\nin different fields.</p>\n", "tags": ["Survey-Paper", "Graph-Based-Ann", "Recommender-Systems", "Datasets", "Evaluation"], "tsne_embedding": [53.883548736572266, 12.546785354614258], "cluster": 9}, {"key": "wang2021cross", "year": "2021", "citations": "31", "title": "Cross-modal Zero-shot Hashing By Label Attributes Embedding", "abstract": "<p>Cross-modal hashing (CMH) is one of the most promising methods in cross-modal\napproximate nearest neighbor search. Most CMH solutions ideally assume the\nlabels of training and testing set are identical. However, the assumption is\noften violated, causing a zero-shot CMH problem. Recent efforts to address this\nissue focus on transferring knowledge from the seen classes to the unseen ones\nusing label attributes. However, the attributes are isolated from the features\nof multi-modal data. To reduce the information gap, we introduce an approach\ncalled LAEH (Label Attributes Embedding for zero-shot cross-modal Hashing).\nLAEH first gets the initial semantic attribute vectors of labels by word2vec\nmodel and then uses a transformation network to transform them into a common\nsubspace. Next, it leverages the hash vectors and the feature similarity matrix\nto guide the feature extraction network of different modalities. At the same\ntime, LAEH uses the attribute similarity as the supplement of label similarity\nto rectify the label embedding and common subspace. Experiments show that LAEH\noutperforms related representative zero-shot and cross-modal hashing methods.</p>\n", "tags": ["SIGIR", "Hashing-Methods", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [2.1404340267181396, -4.498101234436035], "cluster": 6}, {"key": "wang2021domain", "year": "2021", "citations": "28", "title": "Domain-smoothing Network For Zero-shot Sketch-based Image Retrieval", "abstract": "<p>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal\nretrieval task, where abstract sketches are used as queries to retrieve natural\nimages under zero-shot scenario. Most existing methods regard ZS-SBIR as a\ntraditional classification problem and employ a cross-entropy or triplet-based\nloss to achieve retrieval, which neglect the problems of the domain gap between\nsketches and natural images and the large intra-class diversity in sketches.\nToward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.\nSpecifically, a cross-modal contrastive method is proposed to learn generalized\nrepresentations to smooth the domain gap by mining relations with additional\naugmented samples. Furthermore, a category-specific memory bank with sketch\nfeatures is explored to reduce intra-class diversity in the sketch domain.\nExtensive experiments demonstrate that our approach notably outperforms the\nstate-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source\ncode is publicly available at https://github.com/haowang1992/DSN.</p>\n", "tags": ["Datasets", "AAAI", "Image-Retrieval", "IJCAI", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [-42.910789489746094, -17.550941467285156], "cluster": 5}, {"key": "wang2021mathematical", "year": "2021", "citations": "0", "title": "Mathematical Models For Local Sensing Hashes", "abstract": "<p>As data volumes continue to grow, searches in data are becoming increasingly\ntime-consuming. Classical index structures for neighbor search are no longer\nsustainable due to the \u201ccurse of dimensionality\u201d. Instead, approximated index\nstructures offer a good opportunity to significantly accelerate the neighbor\nsearch for clustering and outlier detection and to have the lowest possible\nerror rate in the results of the algorithms. Local sensing hashes is one of\nthose. We indicate directions to mathematically model the properties of it.</p>\n", "tags": ["Vector-Indexing"], "tsne_embedding": [54.49528884887695, 19.530105590820312], "cluster": 9}, {"key": "wang2021meta", "year": "2021", "citations": "1", "title": "Meta Cross-modal Hashing On Long-tailed Data", "abstract": "<p>Due to the advantage of reducing storage while speeding up query time on big\nheterogeneous data, cross-modal hashing has been extensively studied for\napproximate nearest neighbor search of multi-modal data. Most hashing methods\nassume that training data is class-balanced.However, in practice, real world\ndata often have a long-tailed distribution. In this paper, we introduce a\nmeta-learning based cross-modal hashing method (MetaCMH) to handle long-tailed\ndata. Due to the lack of training samples in the tail classes, MetaCMH first\nlearns direct features from data in different modalities, and then introduces\nan associative memory module to learn the memory features of samples of the\ntail classes. It then combines the direct and memory features to obtain meta\nfeatures for each sample. For samples of the head classes of the long tail\ndistribution, the weight of the direct features is larger, because there are\nenough training data to learn them well; while for rare classes, the weight of\nthe memory features is larger. Finally, MetaCMH uses a likelihood loss function\nto preserve the similarity in different modalities and learns hash functions in\nan end-to-end fashion. Experiments on long-tailed datasets show that MetaCMH\nperforms significantly better than state-of-the-art methods, especially on the\ntail classes.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Datasets"], "tsne_embedding": [24.07437515258789, -9.298847198486328], "cluster": 7}, {"key": "wang2021prototype", "year": "2021", "citations": "43", "title": "Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing", "abstract": "<p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "CVPR", "Scalability", "Image-Retrieval", "Robustness", "Tools-&-Libraries", "Supervised", "Evaluation"], "tsne_embedding": [6.080043315887451, -10.881379127502441], "cluster": 6}, {"key": "wang2021pseudo", "year": "2021", "citations": "54", "title": "Pseudo-relevance Feedback For Multiple Representation Dense Retrieval", "abstract": "<p>Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,\nhave shown the usefulness of expanding and reweighting the users\u2019 initial\nqueries using information occurring in an initial set of retrieved documents,\nknown as the pseudo-relevant set. Recently, dense retrieval \u2013 through the use\nof neural contextual language models such as BERT for analysing the documents\u2019\nand queries\u2019 contents and computing their relevance scores \u2013 has shown a\npromising performance on several information retrieval tasks still relying on\nthe traditional inverted index for identifying documents relevant to a query.\nTwo different dense retrieval families have emerged: the use of single embedded\nrepresentations for each passage and query (e.g. using BERT\u2019s [CLS] token), or\nvia multiple representations (e.g. using an embedding for each token of the\nquery and document). In this work, we conduct the first study into the\npotential for multiple representation dense retrieval to be enhanced using\npseudo-relevance feedback. In particular, based on the pseudo-relevant set of\ndocuments identified using a first-pass dense retrieval, we extract\nrepresentative feedback embeddings (using KMeans clustering) \u2013 while ensuring\nthat these embeddings discriminate among passages (based on IDF) \u2013 which are\nthen added to the query representation. These additional feedback embeddings\nare shown to both enhance the effectiveness of a reranking as well as an\nadditional dense retrieval operation. Indeed, experiments on the MSMARCO\npassage ranking dataset show that MAP can be improved by upto 26% on the TREC\n2019 query set and 10% on the TREC 2020 query set by the application of our\nproposed ColBERT-PRF method on a ColBERT dense retrieval approach.</p>\n", "tags": ["SIGIR", "Evaluation", "Datasets"], "tsne_embedding": [8.658794403076172, -25.745561599731445], "cluster": 7}, {"key": "wang2021scene", "year": "2021", "citations": "41", "title": "Scene Text Retrieval Via Joint Text Detection And Similarity Learning", "abstract": "<p>Scene text retrieval aims to localize and search all text instances from an\nimage gallery, which are the same or similar to a given query text. Such a task\nis usually realized by matching a query text to the recognized words, outputted\nby an end-to-end scene text spotter. In this paper, we address this problem by\ndirectly learning a cross-modal similarity between a query text and each text\ninstance from natural images. Specifically, we establish an end-to-end\ntrainable network, jointly optimizing the procedures of scene text detection\nand cross-modal similarity learning. In this way, scene text retrieval can be\nsimply performed by ranking the detected text instances with the learned\nsimilarity. Experiments on three benchmark datasets demonstrate our method\nconsistently outperforms the state-of-the-art scene text spotting/retrieval\napproaches. In particular, the proposed framework of joint detection and\nsimilarity learning achieves significantly better performance than separated\nmethods. Code is available at: https://github.com/lanfeng4659/STR-TDSL.</p>\n", "tags": ["Text-Retrieval", "CVPR", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-20.12093734741211, -10.159940719604492], "cluster": 1}, {"key": "wang2021semantic", "year": "2021", "citations": "0", "title": "A Semantic Indexing Structure For Image Retrieval", "abstract": "<p>In large-scale image retrieval, many indexing methods have been proposed to\nnarrow down the searching scope of retrieval. The features extracted from\nimages usually are of high dimensions or unfixed sizes due to the existence of\nkey points. Most of existing index structures suffer from the dimension curse,\nthe unfixed feature size and/or the loss of semantic similarity. In this paper\na new classification-based indexing structure, called Semantic Indexing\nStructure (SIS), is proposed, in which we utilize the semantic categories\nrather than clustering centers to create database partitions, such that the\nproposed index SIS can be combined with feature extractors without the\nrestriction of dimensions. Besides, it is observed that the size of each\nsemantic partition is positively correlated with the semantic distribution of\ndatabase. Along this way, we found that when the partition number is normalized\nto five, the proposed algorithm performed very well in all the tests. Compared\nwith state-of-the-art models, SIS achieves outstanding performance.</p>\n", "tags": ["Text-Retrieval", "Vector-Indexing", "Scalability", "Image-Retrieval", "Evaluation"], "tsne_embedding": [-7.6705756187438965, -3.3434553146362305], "cluster": 1}, {"key": "wang2021towards", "year": "2021", "citations": "0", "title": "Towards A Model For LSH", "abstract": "<p>As data volumes continue to grow, clustering and outlier detection algorithms\nare becoming increasingly time-consuming. Classical index structures for\nneighbor search are no longer sustainable due to the \u201ccurse of dimensionality\u201d.\nInstead, approximated index structures offer a good opportunity to\nsignificantly accelerate the neighbor search for clustering and outlier\ndetection and to have the lowest possible error rate in the results of the\nalgorithms. Locality-sensitive hashing is one of those. We indicate directions\nto model the properties of LSH.</p>\n", "tags": ["Vector-Indexing", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [54.49435806274414, 19.53890037536621], "cluster": 9}, {"key": "wang2022anchor", "year": "2022", "citations": "2", "title": "Anchor Graph Structure Fusion Hashing For Cross-modal Similarity Search", "abstract": "<p>Cross-modal hashing still has some challenges needed to address: (1) most\nexisting CMH methods take graphs as input to model data distribution. These\nmethods omit to consider the correlation of graph structure among multiple\nmodalities; (2) most existing CMH methods ignores considering the fusion\naffinity among multi-modalities data; (3) most existing CMH methods relax the\ndiscrete constraints to solve the optimization objective, significantly\ndegrading the retrieval performance. To solve the above limitations, we propose\na novel Anchor Graph Structure Fusion Hashing (AGSFH). AGSFH constructs the\nanchor graph structure fusion matrix from different anchor graphs of multiple\nmodalities with the Hadamard product, which can fully exploit the geometric\nproperty of underlying data structure. Based on the anchor graph structure\nfusion matrix, AGSFH attempts to directly learn an intrinsic anchor graph,\nwhere the structure of the intrinsic anchor graph is adaptively tuned so that\nthe number of components of the intrinsic graph is exactly equal to the number\nof clusters. Besides, AGSFH preserves the anchor fusion affinity into the\ncommon binary Hamming space. Furthermore, a discrete optimization framework is\ndesigned to learn the unified binary codes. Extensive experimental results on\nthree public social datasets demonstrate the superiority of AGSFH.</p>\n", "tags": ["Hashing-Methods", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [53.42652130126953, -1.4981086254119873], "cluster": 9}, {"key": "wang2022binary", "year": "2022", "citations": "18", "title": "Binary Representation Via Jointly Personalized Sparse Hashing", "abstract": "<p>Unsupervised hashing has attracted much attention for binary representation\nlearning due to the requirement of economical storage and efficiency of binary\ncodes. It aims to encode high-dimensional features in the Hamming space with\nsimilarity preservation between instances. However, most existing methods learn\nhash functions in manifold-based approaches. Those methods capture the local\ngeometric structures (i.e., pairwise relationships) of data, and lack\nsatisfactory performance in dealing with real-world scenarios that produce\nsimilar features (e.g. color and shape) with different semantic information. To\naddress this challenge, in this work, we propose an effective unsupervised\nmethod, namely Jointly Personalized Sparse Hashing (JPSH), for binary\nrepresentation learning. To be specific, firstly, we propose a novel\npersonalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different\npersonalized subspaces are constructed to reflect category-specific attributes\nfor different clusters, adaptively mapping instances within the same cluster to\nthe same Hamming space. In addition, we deploy sparse constraints for different\npersonalized subspaces to select important features. We also collect the\nstrengths of the other clusters to build the PSH module with avoiding\nover-fitting. Then, to simultaneously preserve semantic and pairwise\nsimilarities in our JPSH, we incorporate the PSH and manifold-based hash\nlearning into the seamless formulation. As such, JPSH not only distinguishes\nthe instances from different clusters, but also preserves local neighborhood\nstructures within the cluster. Finally, an alternating optimization algorithm\nis adopted to iteratively capture analytical solutions of the JPSH model.\nExtensive experiments on four benchmark datasets verify that the JPSH\noutperforms several hashing algorithms on the similarity search task.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Similarity-Search", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [8.040406227111816, 15.569984436035156], "cluster": 6}, {"key": "wang2022cgat", "year": "2023", "citations": "4", "title": "Cgat: Center-guided Adversarial Training For Deep Hashing-based Retrieval", "abstract": "<p>Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61%, 12.35%, and 11.56% on FLICKR-25K, NUS-WIDE, and MS-COCO,\nrespectively. The code is available at https://github.com/xunguangwang/CgAT.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [2.8145341873168945, 16.97506332397461], "cluster": 8}, {"key": "wang2022coder", "year": "2022", "citations": "17", "title": "CODER: Coupled Diversity-sensitive Momentum Contrastive Learning For Image-text Retrieval", "abstract": "<p>Image-Text Retrieval (ITR) is challenging in bridging visual and lingual\nmodalities. Contrastive learning has been adopted by most prior arts. Except\nfor limited amount of negative image-text pairs, the capability of constrastive\nlearning is restricted by manually weighting negative pairs as well as\nunawareness of external knowledge. In this paper, we propose our novel Coupled\nDiversity-Sensitive Momentum Constrastive Learning (CODER) for improving\ncross-modal representation. Firstly, a novel diversity-sensitive contrastive\nlearning (DCL) architecture is invented. We introduce dynamic dictionaries for\nboth modalities to enlarge the scale of image-text pairs, and\ndiversity-sensitiveness is achieved by adaptive negative pair weighting.\nFurthermore, two branches are designed in CODER. One learns instance-level\nembeddings from image/text, and it also generates pseudo online clustering\nlabels for its input image/text based on their embeddings. Meanwhile, the other\nbranch learns to query from commonsense knowledge graph to form concept-level\ndescriptors for both modalities. Afterwards, both branches leverage DCL to\nalign the cross-modal embedding spaces while an extra pseudo clustering label\nprediction loss is utilized to promote concept-level representation learning\nfor the second branch. Extensive experiments conducted on two popular\nbenchmarks, i.e. MSCOCO and Flicker30K, validate CODER remarkably outperforms\nthe state-of-the-art approaches.</p>\n", "tags": ["Self-Supervised", "Text-Retrieval"], "tsne_embedding": [-26.255016326904297, -18.90887451171875], "cluster": 5}, {"key": "wang2022contrastive", "year": "2022", "citations": "146", "title": "Contrastive Masked Autoencoders For Self-supervised Video Hashing", "abstract": "<p>Self-Supervised Video Hashing (SSVH) models learn to generate short binary\nrepresentations for videos without ground-truth supervision, facilitating\nlarge-scale video retrieval efficiency and attracting increasing research\nattention. The success of SSVH lies in the understanding of video content and\nthe ability to capture the semantic relation among unlabeled videos. Typically,\nstate-of-the-art SSVH methods consider these two points in a two-stage training\npipeline, where they firstly train an auxiliary network by instance-wise\nmask-and-predict tasks and secondly train a hashing model to preserve the\npseudo-neighborhood structure transferred from the auxiliary network. This\nconsecutive training strategy is inflexible and also unnecessary. In this\npaper, we propose a simple yet effective one-stage SSVH method called ConMH,\nwhich incorporates video semantic information and video similarity relationship\nunderstanding in a single stage. To capture video semantic information for\nbetter hashing learning, we adopt an encoder-decoder structure to reconstruct\nthe video from its temporal-masked frames. Particularly, we find that a higher\nmasking ratio helps video understanding. Besides, we fully exploit the\nsimilarity relationship between videos by maximizing agreement between two\naugmented views of a video, which contributes to more discriminative and robust\nhash codes. Extensive experiments on three large-scale video datasets (i.e.,\nFCVID, ActivityNet and YFCC) indicate that ConMH achieves state-of-the-art\nresults. Code is available at https://github.com/huangmozhi9527/ConMH.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Efficiency", "Scalability", "Datasets", "Supervised", "Video-Retrieval"], "tsne_embedding": [-11.216157913208008, 1.0286532640457153], "cluster": 1}, {"key": "wang2022cross", "year": "2022", "citations": "18", "title": "Cross-lingual Cross-modal Retrieval With Noise-robust Learning", "abstract": "<p>Despite the recent developments in the field of cross-modal retrieval, there\nhas been less research focusing on low-resource languages due to the lack of\nmanually annotated datasets. In this paper, we propose a noise-robust\ncross-lingual cross-modal retrieval method for low-resource languages. To this\nend, we use Machine Translation (MT) to construct pseudo-parallel sentence\npairs for low-resource languages. However, as MT is not perfect, it tends to\nintroduce noise during translation, rendering textual embeddings corrupted and\nthereby compromising the retrieval performance. To alleviate this, we introduce\na multi-view self-distillation method to learn noise-robust target-language\nrepresentations, which employs a cross-attention module to generate soft\npseudo-targets to provide direct supervision from the similarity-based view and\nfeature-based view. Besides, inspired by the back-translation in unsupervised\nMT, we minimize the semantic discrepancies between origin sentences and\nback-translated sentences to further improve the noise robustness of the\ntextual encoder. Extensive experiments are conducted on three video-text and\nimage-text cross-modal retrieval benchmarks across different languages, and the\nresults demonstrate that our method significantly improves the overall\nperformance without using extra human-labeled data. In addition, equipped with\na pre-trained visual encoder from a recent vision-and-language pre-training\nframework, i.e., CLIP, our model achieves a significant performance gain,\nshowing that our method is compatible with popular pre-training models. Code\nand data are available at https://github.com/HuiGuanLab/nrccr.</p>\n", "tags": ["Robustness", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [-20.688833236694336, -0.876254677772522], "cluster": 1}, {"key": "wang2022english", "year": "2022", "citations": "9", "title": "English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings", "abstract": "<p>Universal cross-lingual sentence embeddings map semantically similar\ncross-lingual sentences into a shared embedding space. Aligning cross-lingual\nsentence embeddings usually requires supervised cross-lingual parallel\nsentences. In this work, we propose mSimCSE, which extends SimCSE to\nmultilingual settings and reveal that contrastive learning on English data can\nsurprisingly learn high-quality universal cross-lingual sentence embeddings\nwithout any parallel data. In unsupervised and weakly supervised settings,\nmSimCSE significantly improves previous sentence embedding methods on\ncross-lingual retrieval and multilingual STS tasks. The performance of\nunsupervised mSimCSE is comparable to fully supervised methods in retrieving\nlow-resource languages and multilingual STS. The performance can be further\nenhanced when cross-lingual NLI data is available. Our code is publicly\navailable at https://github.com/yaushian/mSimCSE.</p>\n", "tags": ["Self-Supervised", "EMNLP", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-4.084894180297852, -38.88117980957031], "cluster": 3}, {"key": "wang2022hcfrec", "year": "2022", "citations": "1", "title": "Hcfrec: Hash Collaborative Filtering Via Normalized Flow With Structural Consensus For Efficient Recommendation", "abstract": "<p>The ever-increasing data scale of user-item interactions makes it challenging\nfor an effective and efficient recommender system. Recently, hash-based\ncollaborative filtering (Hash-CF) approaches employ efficient Hamming distance\nof learned binary representations of users and items to accelerate\nrecommendations. However, Hash-CF often faces two challenging problems, i.e.,\noptimization on discrete representations and preserving semantic information in\nlearned representations. To address the above two challenges, we propose\nHCFRec, a novel Hash-CF approach for effective and efficient recommendations.\nSpecifically, HCFRec not only innovatively introduces normalized flow to learn\nthe optimal hash code by efficiently fit a proposed approximate mixture\nmultivariate normal distribution, a continuous but approximately discrete\ndistribution, but also deploys a cluster consistency preserving mechanism to\npreserve the semantic structure in representations for more accurate\nrecommendations. Extensive experiments conducted on six real-world datasets\ndemonstrate the superiority of our HCFRec compared to the state-of-art methods\nin terms of effectiveness and efficiency.</p>\n", "tags": ["Datasets", "AAAI", "Efficiency", "Recommender-Systems", "Hashing-Methods", "IJCAI"], "tsne_embedding": [14.068697929382324, 8.331900596618652], "cluster": 6}, {"key": "wang2022hybrid", "year": "2022", "citations": "5", "title": "Hybrid Contrastive Quantization For Efficient Cross-view Video Retrieval", "abstract": "<p>With the recent boom of video-based social platforms (e.g., YouTube and\nTikTok), video retrieval using sentence queries has become an important demand\nand attracts increasing research attention. Despite the decent performance,\nexisting text-video retrieval models in vision and language communities are\nimpractical for large-scale Web search because they adopt brute-force search\nbased on high-dimensional embeddings. To improve efficiency, Web search engines\nwidely apply vector compression libraries (e.g., FAISS) to post-process the\nlearned embeddings. Unfortunately, separate compression from feature encoding\ndegrades the robustness of representations and incurs performance decay. To\npursue a better balance between performance and efficiency, we propose the\nfirst quantized representation learning method for cross-view video retrieval,\nnamely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both\ncoarse-grained and fine-grained quantizations with transformers, which provide\ncomplementary understandings for texts and videos and preserve comprehensive\nsemantic information. By performing Asymmetric-Quantized Contrastive Learning\n(AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and\nmultiple fine-grained levels. This hybrid-grained learning strategy serves as\nstrong supervision on the cross-view video quantization model, where\ncontrastive learning at different levels can be mutually promoted. Extensive\nexperiments on three Web video benchmark datasets demonstrate that HCQ achieves\ncompetitive performance with state-of-the-art non-compressed retrieval methods\nwhile showing high efficiency in storage and computation. Code and\nconfigurations are available at https://github.com/gimpong/WWW22-HCQ.</p>\n", "tags": ["Self-Supervised", "Efficiency", "Quantization", "Scalability", "Robustness", "Tools-&-Libraries", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-38.99174499511719, -28.908916473388672], "cluster": 5}, {"key": "wang2022inverted", "year": "2022", "citations": "0", "title": "Inverted Semantic-index For Image Retrieval", "abstract": "<p>This paper addresses the construction of inverted index for large-scale image\nretrieval. The inverted index proposed by J. Sivic brings a significant\nacceleration by reducing distance computations with only a small fraction of\nthe database. The state-of-the-art inverted indices aim to build finer\npartitions that produce a concise and accurate candidate list. However,\npartitioning in these frameworks is generally achieved by unsupervised\nclustering methods which ignore the semantic information of images. In this\npaper, we replace the clustering method with image classification, during the\nconstruction of codebook. We then propose a merging and splitting method to\nsolve the problem that the number of partitions is unchangeable in the inverted\nsemantic-index. Next, we combine our semantic-index with the product\nquantization (PQ) so as to alleviate the accuracy loss caused by PQ\ncompression. Finally, we evaluate our model on large-scale image retrieval\nbenchmarks. Experiment results demonstrate that our model can significantly\nimprove the retrieval accuracy by generating high-quality candidate lists.</p>\n", "tags": ["Quantization", "Image-Retrieval", "Scalability", "Unsupervised"], "tsne_embedding": [-22.118968963623047, 9.577075004577637], "cluster": 8}, {"key": "wang2022navigable", "year": "2022", "citations": "4", "title": "Navigable Proximity Graph-driven Native Hybrid Queries With Structured And Unstructured Constraints", "abstract": "<p>As research interest surges, vector similarity search is applied in multiple\nfields, including data mining, computer vision, and information retrieval.\n{Given a set of objects (e.g., a set of images) and a query object, we can\neasily transform each object into a feature vector and apply the vector\nsimilarity search to retrieve the most similar objects. However, the original\nvector similarity search cannot well support \\textit{hybrid queries}, where\nusers not only input unstructured query constraint (i.e., the feature vector of\nquery object) but also structured query constraint (i.e., the desired\nattributes of interest). Hybrid query processing aims at identifying these\nobjects with similar feature vectors to query object and satisfying the given\nattribute constraints. Recent efforts have attempted to answer a hybrid query\nby performing attribute filtering and vector similarity search separately and\nthen merging the results later, which limits efficiency and accuracy because\nthey are not purpose-built for hybrid queries.} In this paper, we propose a\nnative hybrid query (NHQ) framework based on proximity graph (PG), which\nprovides the specialized \\textit{composite index and joint pruning} modules for\nhybrid queries. We easily deploy existing various PGs on this framework to\nprocess hybrid queries efficiently. Moreover, we present two novel navigable\nPGs (NPGs) with optimized edge selection and routing strategies, which obtain\nbetter overall performance than existing PGs. After that, we deploy the\nproposed NPGs in NHQ to form two hybrid query methods, which significantly\noutperform the state-of-the-art competitors on all experimental datasets\n(10\\(\\times\\) faster under the same \\textit{Recall}), including eight public and\none in-house real-world datasets. Our code and datasets have been released at\nhttps://github.com/AshenOn3/NHQ.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [4.382645606994629, 26.817293167114258], "cluster": 8}, {"key": "wang2022neural", "year": "2022", "citations": "43", "title": "A Neural Corpus Indexer For Document Retrieval", "abstract": "<p>Current state-of-the-art document retrieval solutions mainly follow an\nindex-retrieve paradigm, where the index is hard to be directly optimized for\nthe final retrieval target. In this paper, we aim to show that an end-to-end\ndeep neural network unifying training and indexing stages can significantly\nimprove the recall performance of traditional methods. To this end, we propose\nNeural Corpus Indexer (NCI), a sequence-to-sequence network that generates\nrelevant document identifiers directly for a designated query. To optimize the\nrecall performance of NCI, we invent a prefix-aware weight-adaptive decoder\narchitecture, and leverage tailored techniques including query generation,\nsemantic document identifiers, and consistency-based regularization. Empirical\nstudies demonstrated the superiority of NCI on two commonly used academic\nbenchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on\nNQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to\nthe best baseline method.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [5.744388103485107, -26.721372604370117], "cluster": 7}, {"key": "wang2022survey", "year": "2022", "citations": "0", "title": "A Survey On Efficient Processing Of Similarity Queries Over Neural Embeddings", "abstract": "<p>Similarity query is the family of queries based on some similarity metrics.\nUnlike the traditional database queries which are mostly based on value\nequality, similarity queries aim to find targets \u201csimilar enough to\u201d the given\ndata objects, depending on some similarity metric, e.g., Euclidean distance,\ncosine similarity and so on. To measure the similarity between data objects,\ntraditional methods normally work on low level or syntax features(e.g., basic\nvisual features on images or bag-of-word features of text), which makes them\nweak to compute the semantic similarities between objects. So for measuring\ndata similarities semantically, neural embedding is applied. Embedding\ntechniques work by representing the raw data objects as vectors (so called\n\u201cembeddings\u201d or \u201cneural embeddings\u201d since they are mostly generated by neural\nnetwork models) that expose the hidden semantics of the raw data, based on\nwhich embeddings do show outstanding effectiveness on capturing data\nsimilarities, making it one of the most widely used and studied techniques in\nthe state-of-the-art similarity query processing research. But there are still\nmany open challenges on the efficiency of embedding based similarity query\nprocessing, which are not so well-studied as the effectiveness. In this survey,\nwe first provide an overview of the \u201csimilarity query\u201d and \u201csimilarity query\nprocessing\u201d problems. Then we talk about recent approaches on designing the\nindexes and operators for highly efficient similarity query processing on top\nof embeddings (or more generally, high dimensional data). Finally, we\ninvestigate the specific solutions with and without using embeddings in\nselected application domains of similarity queries, including entity resolution\nand information retrieval. By comparing the solutions, we show how neural\nembeddings benefit those applications.</p>\n", "tags": ["Efficiency", "Distance-Metric-Learning", "Survey-Paper"], "tsne_embedding": [2.152343273162842, -17.892135620117188], "cluster": 7}, {"key": "wang2022text", "year": "2022", "citations": "74", "title": "Text Embeddings By Weakly-supervised Contrastive Pre-training", "abstract": "<p>This paper presents E5, a family of state-of-the-art text embeddings that\ntransfer well to a wide range of tasks. The model is trained in a contrastive\nmanner with weak supervision signals from our curated large-scale text pair\ndataset (called CCPairs). E5 can be readily used as a general-purpose embedding\nmodel for any tasks requiring a single-vector representation of texts such as\nretrieval, clustering, and classification, achieving strong performance in both\nzero-shot and fine-tuned settings. We conduct extensive evaluations on 56\ndatasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the\nfirst model that outperforms the strong BM25 baseline on the BEIR retrieval\nbenchmark without using any labeled data. When fine-tuned, E5 obtains the best\nresults on the MTEB benchmark, beating existing embedding models with 40x more\nparameters.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-1.9208824634552002, -40.5211181640625], "cluster": 3}, {"key": "wang2022using", "year": "2023", "citations": "2", "title": "Using Multiple Instance Learning To Build Multimodal Representations", "abstract": "<p>Image-text multimodal representation learning aligns data across modalities\nand enables important medical applications, e.g., image classification, visual\ngrounding, and cross-modal retrieval. In this work, we establish a connection\nbetween multimodal representation learning and multiple instance learning.\nBased on this connection, we propose a generic framework for constructing\npermutation-invariant score functions with many existing multimodal\nrepresentation learning approaches as special cases. Furthermore, we use the\nframework to derive a novel contrastive learning approach and demonstrate that\nour method achieves state-of-the-art results in several downstream tasks.</p>\n", "tags": ["Self-Supervised", "Multimodal-Retrieval", "Tools-&-Libraries"], "tsne_embedding": [-54.25509262084961, 15.177433013916016], "cluster": 0}, {"key": "wang2022v", "year": "2022", "citations": "0", "title": "V\\(^2\\)L: Leveraging Vision And Vision-language Models Into Large-scale Product Retrieval", "abstract": "<p>Product retrieval is of great importance in the ecommerce domain. This paper\nintroduces our 1st-place solution in eBay eProduct Visual Search Challenge\n(FGVC9), which is featured for an ensemble of about 20 models from vision\nmodels and vision-language models. While model ensemble is common, we show that\ncombining the vision models and vision-language models brings particular\nbenefits from their complementarity and is a key factor to our superiority.\nSpecifically, for the vision models, we use a two-stage training pipeline which\nfirst learns from the coarse labels provided in the training set and then\nconducts fine-grained self-supervised training, yielding a coarse-to-fine\nmetric learning manner. For the vision-language models, we use the textual\ndescription of the training image as the supervision signals for fine-tuning\nthe image-encoder (feature extractor). With these designs, our solution\nachieves 0.7623 MAR@10, ranking the first place among all the competitors. The\ncode is available at: \\href{https://github.com/WangWenhao0716/V2L}{V\\(^2\\)L}.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Image-Retrieval", "Scalability", "Supervised"], "tsne_embedding": [-11.924229621887207, -24.478361129760742], "cluster": 3}, {"key": "wang2023balance", "year": "2023", "citations": "4", "title": "Balance Act: Mitigating Hubness In Cross-modal Retrieval With Query And Gallery Banks", "abstract": "<p>In this work, we present a post-processing solution to address the hubness\nproblem in cross-modal retrieval, a phenomenon where a small number of gallery\ndata points are frequently retrieved, resulting in a decline in retrieval\nperformance. We first theoretically demonstrate the necessity of incorporating\nboth the gallery and query data for addressing hubness as hubs always exhibit\nhigh similarity with gallery and query data. Second, building on our\ntheoretical results, we propose a novel framework, Dual Bank Normalization\n(DBNorm). While previous work has attempted to alleviate hubness by only\nutilizing the query samples, DBNorm leverages two banks constructed from the\nquery and gallery samples to reduce the occurrence of hubs during inference.\nNext, to complement DBNorm, we introduce two novel methods, dual inverted\nsoftmax and dual dynamic inverted softmax, for normalizing similarity based on\nthe two banks. Specifically, our proposed methods reduce the similarity between\nhubs and queries while improving the similarity between non-hubs and queries.\nFinally, we present extensive experimental results on diverse language-grounded\nbenchmarks, including text-image, text-video, and text-audio, demonstrating the\nsuperior performance of our approaches compared to previous methods in\naddressing hubness and boosting retrieval performance. Our code is available at\nhttps://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.</p>\n", "tags": ["EMNLP", "Multimodal-Retrieval", "Tools-&-Libraries", "Re-Ranking", "Evaluation"], "tsne_embedding": [4.665496826171875, -17.78504753112793], "cluster": 7}, {"key": "wang2023graph", "year": "2023", "citations": "0", "title": "Graph-collaborated Auto-encoder Hashing For Multi-view Binary Clustering", "abstract": "<p>Unsupervised hashing methods have attracted widespread attention with the\nexplosive growth of large-scale data, which can greatly reduce storage and\ncomputation by learning compact binary codes. Existing unsupervised hashing\nmethods attempt to exploit the valuable information from samples, which fails\nto take the local geometric structure of unlabeled samples into consideration.\nMoreover, hashing based on auto-encoders aims to minimize the reconstruction\nloss between the input data and binary codes, which ignores the potential\nconsistency and complementarity of multiple sources data. To address the above\nissues, we propose a hashing algorithm based on auto-encoders for multi-view\nbinary clustering, which dynamically learns affinity graphs with low-rank\nconstraints and adopts collaboratively learning between auto-encoders and\naffinity graphs to learn a unified binary code, called Graph-Collaborated\nAuto-Encoder Hashing for Multi-view Binary Clustering (GCAE). Specifically, we\npropose a multi-view affinity graphs learning model with low-rank constraint,\nwhich can mine the underlying geometric information from multi-view data. Then,\nwe design an encoder-decoder paradigm to collaborate the multiple affinity\ngraphs, which can learn a unified binary code effectively. Notably, we impose\nthe decorrelation and code balance constraints on binary codes to reduce the\nquantization errors. Finally, we utilize an alternating iterative optimization\nscheme to obtain the multi-view clustering results. Extensive experimental\nresults on \\(5\\) public datasets are provided to reveal the effectiveness of the\nalgorithm and its superior performance over other state-of-the-art\nalternatives.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Scalability", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [49.522029876708984, 0.2915106415748596], "cluster": 9}, {"key": "wang2023learning", "year": "2023", "citations": "4", "title": "Learning Efficient Representations For Image-based Patent Retrieval", "abstract": "<p>Patent retrieval has been attracting tremendous interest from researchers in\nintellectual property and information retrieval communities in the past\ndecades. However, most existing approaches rely on textual and metadata\ninformation of the patent, and content-based image-based patent retrieval is\nrarely investigated. Based on traits of patent drawing images, we present a\nsimple and lightweight model for this task. Without bells and whistles, this\napproach significantly outperforms other counterparts on a large-scale\nbenchmark and noticeably improves the state-of-the-art by 33.5% with the mean\naverage precision (mAP) score. Further experiments reveal that this model can\nbe elaborately scaled up to achieve a surprisingly high mAP of 93.5%. Our\nmethod ranks first in the ECCV 2022 Patent Diagram Image Retrieval Challenge.</p>\n", "tags": ["Image-Retrieval", "Evaluation", "Scalability"], "tsne_embedding": [-1.2111374139785767, -31.426599502563477], "cluster": 3}, {"key": "wang2023multi", "year": "2024", "citations": "1", "title": "Multi-granularity Representation Learning For Sketch-based Dynamic Face Image Retrieval", "abstract": "<p>In specific scenarios, face sketch can be used to identify a person. However,\ndrawing a face sketch often requires exceptional skill and is time-consuming,\nlimiting its widespread applications in actual scenarios. The new framework of\nsketch less face image retrieval (SLFIR)[1] attempts to overcome the barriers\nby providing a means for humans and machines to interact during the drawing\nprocess. Considering SLFIR problem, there is a large gap between a partial\nsketch with few strokes and any whole face photo, resulting in poor performance\nat the early stages. In this study, we propose a multigranularity (MG)\nrepresentation learning (MGRL) method to address the SLFIR problem, in which we\nlearn the representation of different granularity regions for a partial sketch,\nand then, by combining all MG regions of the sketches and images, the final\ndistance was determined. In the experiments, our method outperformed\nstate-of-the-art baselines in terms of early retrieval on two accessible\ndatasets. Codes are available at https://github.com/ddw2AIGROUP2CQUPT/MGRL.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-45.97309112548828, -18.64961814880371], "cluster": 5}, {"key": "wang2023must", "year": "2024", "citations": "1", "title": "MUST: An Effective And Scalable Framework For Multimodal Search Of Target Modality", "abstract": "<p>We investigate the problem of multimodal search of target modality, where the\ntask involves enhancing a query in a specific target modality by integrating\ninformation from auxiliary modalities. The goal is to retrieve relevant objects\nwhose contents in the target modality match the specified multimodal query. The\npaper first introduces two baseline approaches that integrate techniques from\nthe Database, Information Retrieval, and Computer Vision communities. These\nbaselines either merge the results of separate vector searches for each\nmodality or perform a single-channel vector search by fusing all modalities.\nHowever, both baselines have limitations in terms of efficiency and accuracy as\nthey fail to adequately consider the varying importance of fusing information\nacross modalities. To overcome these limitations, the paper proposes a novel\nframework, called MUST. Our framework employs a hybrid fusion mechanism,\ncombining different modalities at multiple stages. Notably, we leverage vector\nweight learning to determine the importance of each modality, thereby enhancing\nthe accuracy of joint similarity measurement. Additionally, the proposed\nframework utilizes a fused proximity graph index, enabling efficient joint\nsearch for multimodal queries. MUST offers several other advantageous\nproperties, including pluggable design to integrate any advanced embedding\ntechniques, user flexibility to customize weight preferences, and modularized\nindex construction. Extensive experiments on real-world datasets demonstrate\nthe superiority of MUST over the baselines in terms of both search accuracy and\nefficiency. Our framework achieves over 10x faster search times while attaining\nan average of 93% higher accuracy. Furthermore, MUST exhibits scalability to\ndatasets containing more than 10 million data elements.</p>\n", "tags": ["Graph-Based-Ann", "Efficiency", "Scalability", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [4.885046482086182, 27.07196044921875], "cluster": 4}, {"key": "wang2023note", "year": "2023", "citations": "0", "title": "A Note On \"efficient Task-specific Data Valuation For Nearest Neighbor Algorithms\"", "abstract": "<p>Data valuation is a growing research field that studies the influence of\nindividual data points for machine learning (ML) models. Data Shapley, inspired\nby cooperative game theory and economics, is an effective method for data\nvaluation. However, it is well-known that the Shapley value (SV) can be\ncomputationally expensive. Fortunately, Jia et al. (2019) showed that for\nK-Nearest Neighbors (KNN) models, the computation of Data Shapley is\nsurprisingly simple and efficient.\n  In this note, we revisit the work of Jia et al. (2019) and propose a more\nnatural and interpretable utility function that better reflects the performance\nof KNN models. We derive the corresponding calculation procedure for the Data\nShapley of KNN classifiers/regressors with the new utility functions. Our new\napproach, dubbed soft-label KNN-SV, achieves the same time complexity as the\noriginal method. We further provide an efficient approximation algorithm for\nsoft-label KNN-SV based on locality sensitive hashing (LSH). Our experimental\nresults demonstrate that Soft-label KNN-SV outperforms the original method on\nmost datasets in the task of mislabeled data detection, making it a better\nbaseline for future work on data valuation.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [18.149620056152344, 28.332124710083008], "cluster": 4}, {"key": "wang2023reliable", "year": "2023", "citations": "0", "title": "Reliable And Efficient Evaluation Of Adversarial Robustness For Deep Hashing-based Retrieval", "abstract": "<p>Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [3.559389114379883, 17.861236572265625], "cluster": 8}, {"key": "wang2023scene", "year": "2023", "citations": "2", "title": "Scene Graph Based Fusion Network For Image-text Retrieval", "abstract": "<p>A critical challenge to image-text retrieval is how to learn accurate\ncorrespondences between images and texts. Most existing methods mainly focus on\ncoarse-grained correspondences based on co-occurrences of semantic objects,\nwhile failing to distinguish the fine-grained local correspondences. In this\npaper, we propose a novel Scene Graph based Fusion Network (dubbed SGFN), which\nenhances the images\u2019/texts\u2019 features through intra- and cross-modal fusion for\nimage-text retrieval. To be specific, we design an intra-modal hierarchical\nattention fusion to incorporate semantic contexts, such as objects, attributes,\nand relationships, into images\u2019/texts\u2019 feature vectors via scene graphs, and a\ncross-modal attention fusion to combine the contextual semantics and local\nfusion via contextual vectors. Extensive experiments on public datasets\nFlickr30K and MSCOCO show that our SGFN performs better than quite a few SOTA\nimage-text retrieval methods.</p>\n", "tags": ["Text-Retrieval", "Datasets"], "tsne_embedding": [-32.11455535888672, -37.59897232055664], "cluster": 5}, {"key": "wang2023unified", "year": "2023", "citations": "20", "title": "Unified Coarse-to-fine Alignment For Video-text Retrieval", "abstract": "<p>The canonical approach to video-text retrieval leverages a coarse-grained or\nfine-grained alignment between visual and textual information. However,\nretrieving the correct video according to the text query is often challenging\nas it requires the ability to reason about both high-level (scene) and\nlow-level (object) visual clues and how they relate to the text query. To this\nend, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.\nSpecifically, our model captures the cross-modal similarity information at\ndifferent granularity levels. To alleviate the effect of irrelevant visual\nclues, we also apply an Interactive Similarity Aggregation module (ISA) to\nconsider the importance of different visual features while aggregating the\ncross-modal similarity to obtain a similarity score for each granularity.\nFinally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of\neach level before summing them, alleviating over- and under-representation\nissues at different levels. By jointly considering the crossmodal similarity of\ndifferent granularity, UCoFiA allows the effective unification of multi-grained\nalignments. Empirically, UCoFiA outperforms previous state-of-the-art\nCLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,\n1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,\nActivity-Net, and DiDeMo, respectively. Our code is publicly available at\nhttps://github.com/Ziyang412/UCoFiA.</p>\n", "tags": ["ICCV", "Video-Retrieval", "Text-Retrieval"], "tsne_embedding": [-33.39820098876953, -24.62493133544922], "cluster": 5}, {"key": "wang2023video", "year": "2023", "citations": "4", "title": "Video-text Retrieval By Supervised Sparse Multi-grained Learning", "abstract": "<p>While recent progress in video-text retrieval has been advanced by the\nexploration of better representation learning, in this paper, we present a\nnovel multi-grained sparse learning framework, S3MA, to learn an aligned sparse\nspace shared between the video and the text for video-text retrieval. The\nshared sparse space is initialized with a finite number of sparse concepts,\neach of which refers to a number of words. With the text data at hand, we learn\nand update the shared sparse space in a supervised manner using the proposed\nsimilarity and alignment losses. Moreover, to enable multi-grained alignment,\nwe incorporate frame representations for better modeling the video modality and\ncalculating fine-grained and coarse-grained similarities. Benefiting from the\nlearned shared sparse space and multi-grained similarities, extensive\nexperiments on several video-text retrieval benchmarks demonstrate the\nsuperiority of S3MA over existing methods. Our code is available at\nhttps://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.</p>\n", "tags": ["EMNLP", "Text-Retrieval", "Tools-&-Libraries", "Supervised"], "tsne_embedding": [-39.70443344116211, -26.221113204956055], "cluster": 5}, {"key": "wang2024efficient", "year": "2024", "citations": "0", "title": "Efficient Self-supervised Video Hashing With Selective State Spaces", "abstract": "<p>Self-supervised video hashing (SSVH) is a practical task in video indexing\nand retrieval. Although Transformers are predominant in SSVH for their\nimpressive temporal modeling capabilities, they often suffer from computational\nand memory inefficiencies. Drawing inspiration from Mamba, an advanced\nstate-space model, we explore its potential in SSVH to achieve a better balance\nbetween efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing\nmodel with an improved self-supervised learning paradigm. Specifically, we\ndesign bidirectional Mamba layers for both the encoder and decoder, which are\neffective and efficient in capturing temporal relationships thanks to the\ndata-dependent selective scanning mechanism with linear complexity. In our\nlearning strategy, we transform global semantics in the feature space into\nsemantically consistent and discriminative hash centers, followed by a center\nalignment loss as a global learning signal. Our self-local-global (SLG)\nparadigm significantly improves learning efficiency, leading to faster and\nbetter convergence. Extensive experiments demonstrate S5VH\u2019s improvements over\nstate-of-the-art methods, superior transferability, and scalable advantages in\ninference efficiency. Code is available at\nhttps://github.com/gimpong/AAAI25-S5VH.</p>\n", "tags": ["Supervised", "Self-Supervised", "Hashing-Methods", "Efficiency"], "tsne_embedding": [2.392578601837158, 3.0355372428894043], "cluster": 6}, {"key": "wang2024enhancing", "year": "2024", "citations": "1", "title": "Enhancing Image-text Matching With Adaptive Feature Aggregation", "abstract": "<p>Image-text matching aims to find matched cross-modal pairs accurately. While\ncurrent methods often rely on projecting cross-modal features into a common\nembedding space, they frequently suffer from imbalanced feature representations\nacross different modalities, leading to unreliable retrieval results. To\naddress these limitations, we introduce a novel Feature Enhancement Module that\nadaptively aggregates single-modal features for more balanced and robust\nimage-text retrieval. Additionally, we propose a new loss function that\novercomes the shortcomings of original triplet ranking loss, thereby\nsignificantly improving retrieval performance. The proposed model has been\nevaluated on two public datasets and achieves competitive retrieval performance\nwhen compared with several state-of-the-art models. Implementation codes can be\nfound here.</p>\n", "tags": ["ICASSP", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-22.410402297973633, -5.7289958000183105], "cluster": 1}, {"key": "wang2024neural", "year": "2024", "citations": "0", "title": "Neural Locality Sensitive Hashing For Entity Blocking", "abstract": "<p>Locality-sensitive hashing (LSH) is a fundamental algorithmic technique\nwidely employed in large-scale data processing applications, such as\nnearest-neighbor search, entity resolution, and clustering. However, its\napplicability in some real-world scenarios is limited due to the need for\ncareful design of hashing functions that align with specific metrics. Existing\nLSH-based Entity Blocking solutions primarily rely on generic similarity\nmetrics such as Jaccard similarity, whereas practical use cases often demand\ncomplex and customized similarity rules surpassing the capabilities of generic\nsimilarity metrics. Consequently, designing LSH functions for these customized\nsimilarity rules presents considerable challenges. In this research, we propose\na neuralization approach to enhance locality-sensitive hashing by training deep\nneural networks to serve as hashing functions for complex metrics. We assess\nthe effectiveness of this approach within the context of the entity resolution\nproblem, which frequently involves the use of task-specific metrics in\nreal-world applications. Specifically, we introduce NLSHBlock (Neural-LSH\nBlock), a novel blocking methodology that leverages pre-trained language\nmodels, fine-tuned with a novel LSH-based loss function. Through extensive\nevaluations conducted on a diverse range of real-world datasets, we demonstrate\nthe superiority of NLSHBlock over existing methods, exhibiting significant\nperformance improvements. Furthermore, we showcase the efficacy of NLSHBlock in\nenhancing the performance of the entity matching phase, particularly within the\nsemi-supervised setting.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Distance-Metric-Learning", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [29.479501724243164, -13.877347946166992], "cluster": 7}, {"key": "wang2024partial", "year": "2024", "citations": "0", "title": "Partial Scene Text Retrieval", "abstract": "<p>The task of partial scene text retrieval involves localizing and searching\nfor text instances that are the same or similar to a given query text from an\nimage gallery. However, existing methods can only handle text-line instances,\nleaving the problem of searching for partial patches within these text-line\ninstances unsolved due to a lack of patch annotations in the training data. To\naddress this issue, we propose a network that can simultaneously retrieve both\ntext-line instances and their partial patches. Our method embeds the two types\nof data (query text and scene text instances) into a shared feature space and\nmeasures their cross-modal similarities. To handle partial patches, our\nproposed approach adopts a Multiple Instance Learning (MIL) approach to learn\ntheir similarities with query text, without requiring extra annotations.\nHowever, constructing bags, which is a standard step of conventional MIL\napproaches, can introduce numerous noisy samples for training, and lower\ninference speed. To address this issue, we propose a Ranking MIL (RankMIL)\napproach to adaptively filter those noisy samples. Additionally, we present a\nDynamic Partial Match Algorithm (DPMA) that can directly search for the target\npartial patch from a text-line instance during the inference stage, without\nrequiring bags. This greatly improves the search efficiency and the performance\nof retrieving partial patches. The source code and dataset are available at\nhttps://github.com/lanfeng4659/PSTR.</p>\n", "tags": ["Efficiency", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-33.65914535522461, -3.306248188018799], "cluster": 0}, {"key": "wang2024pdsr", "year": "2024", "citations": "0", "title": "PDSR: A Privacy-preserving Diversified Service Recommendation Method On Distributed Data", "abstract": "<p>The last decade has witnessed a tremendous growth of service computing, while\nefficient service recommendation methods are desired to recommend high-quality\nservices to users. It is well known that collaborative filtering is one of the\nmost popular methods for service recommendation based on QoS, and many existing\nproposals focus on improving recommendation accuracy, i.e., recommending\nhigh-quality redundant services. Nevertheless, users may have different\nrequirements on QoS, and hence diversified recommendation has been attracting\nincreasing attention in recent years to fulfill users\u2019 diverse demands and to\nexplore potential services. Unfortunately, the recommendation performances\nrelies on a large volume of data (e.g., QoS data), whereas the data may be\ndistributed across multiple platforms. Therefore, to enable data sharing across\nthe different platforms for diversified service recommendation, we propose a\nPrivacy-preserving Diversified Service Recommendation (PDSR) method.\nSpecifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH)\nmechanism such that privacy-preserved data sharing across different platforms\nis enabled to construct a service similarity graph. Based on the similarity\ngraph, we propose a novel accuracy-diversity metric and design a\n\\(2\\)-approximation algorithm to select \\(K\\) services to recommend by maximizing\nthe accuracy-diversity measure. Extensive experiments on real datasets are\nconducted to verify the efficacy of our PDSR method.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Recommender-Systems", "Hashing-Methods", "Datasets"], "tsne_embedding": [13.672623634338379, 9.516261100769043], "cluster": 6}, {"key": "wang2024rreh", "year": "2024", "citations": "1", "title": "RREH: Reconstruction Relations Embedded Hashing For Semi-paired Cross-modal Retrieval", "abstract": "<p>Known for efficient computation and easy storage, hashing has been\nextensively explored in cross-modal retrieval. The majority of current hashing\nmodels are predicated on the premise of a direct one-to-one mapping between\ndata points. However, in real practice, data correspondence across modalities\nmay be partially provided. In this research, we introduce an innovative\nunsupervised hashing technique designed for semi-paired cross-modal retrieval\ntasks, named Reconstruction Relations Embedded Hashing (RREH). RREH assumes\nthat multi-modal data share a common subspace. For paired data, RREH explores\nthe latent consistent information of heterogeneous modalities by seeking a\nshared representation. For unpaired data, to effectively capture the latent\ndiscriminative features, the high-order relationships between unpaired data and\nanchors are embedded into the latent subspace, which are computed by efficient\nlinear reconstruction. The anchors are sampled from paired data, which improves\nthe efficiency of hash learning. The RREH trains the underlying features and\nthe binary encodings in a unified framework with high-order reconstruction\nrelations preserved. With the well devised objective function and discrete\noptimization algorithm, RREH is designed to be scalable, making it suitable for\nlarge-scale datasets and facilitating efficient cross-modal retrieval. In the\nevaluation process, the proposed is tested with partially paired data to\nestablish its superiority over several existing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [9.090618133544922, 8.507529258728027], "cluster": 6}, {"key": "wang2024tpch", "year": "2025", "citations": "0", "title": "TPCH: Tensor-interacted Projection And Cooperative Hashing For Multi-view Clustering", "abstract": "<p>In recent years, anchor and hash-based multi-view clustering methods have\ngained attention for their efficiency and simplicity in handling large-scale\ndata. However, existing methods often overlook the interactions among\nmulti-view data and higher-order cooperative relationships during projection,\nnegatively impacting the quality of hash representation in low-dimensional\nspaces, clustering performance, and sensitivity to noise. To address this\nissue, we propose a novel approach named Tensor-Interacted Projection and\nCooperative Hashing for Multi-View Clustering(TPCH). TPCH stacks multiple\nprojection matrices into a tensor, taking into account the synergies and\ncommunications during the projection process. By capturing higher-order\nmulti-view information through dual projection and Hamming space, TPCH employs\nan enhanced tensor nuclear norm to learn more compact and distinguishable hash\nrepresentations, promoting communication within and between views. Experimental\nresults demonstrate that this refined method significantly outperforms\nstate-of-the-art methods in clustering on five large-scale multi-view datasets.\nMoreover, in terms of CPU time, TPCH achieves substantial acceleration compared\nto the most advanced current methods. The code is available at\n\\textcolor{red}{https://github.com/jankin-wang/TPCH}.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "AAAI", "Datasets", "Evaluation"], "tsne_embedding": [6.77143669128418, 27.73451805114746], "cluster": 4}, {"key": "wang2024weakly", "year": "2021", "citations": "9", "title": "Weakly Supervised Deep Hyperspherical Quantization For Image Retrieval", "abstract": "<p>Deep quantization methods have shown high efficiency on large-scale image\nretrieval. However, current models heavily rely on ground-truth information,\nhindering the application of quantization in label-hungry scenarios. A more\nrealistic demand is to learn from inexhaustible uploaded images that are\nassociated with informal tags provided by amateur users. Though such sketchy\ntags do not obviously reveal the labels, they actually contain useful semantic\ninformation for supervising deep quantization. To this end, we propose\nWeakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first\nwork to learn deep quantization from weakly tagged images. Specifically, 1) we\nuse word embeddings to represent the tags and enhance their semantic\ninformation based on a tag correlation graph. 2) To better preserve semantic\ninformation in quantization codes and reduce quantization error, we jointly\nlearn semantics-preserving embeddings and supervised quantizer on hypersphere\nby employing a well-designed fusion layer and tailor-made loss functions.\nExtensive experiments show that WSDHQ can achieve state-of-art performance on\nweakly-supervised compact coding. Code is available at\nhttps://github.com/gimpong/AAAI21-WSDHQ.</p>\n", "tags": ["Efficiency", "Quantization", "Scalability", "Image-Retrieval", "AAAI", "Supervised", "Evaluation"], "tsne_embedding": [4.953455924987793, -8.106599807739258], "cluster": 6}, {"key": "wang2024xling", "year": "2024", "citations": "0", "title": "Xling: A Learned Filter Framework For Accelerating High-dimensional Approximate Similarity Join", "abstract": "<p>Similarity join finds all pairs of close points within a given distance\nthreshold. Many similarity join methods have been proposed, but they are\nusually not efficient on high-dimensional space due to the curse of\ndimensionality and data-unawareness. We investigate the possibility of using\nmetric space Bloom filter (MSBF), a family of data structures checking if a\nquery point has neighbors in a multi-dimensional space, to speed up similarity\njoin. However, there are several challenges when applying MSBF to similarity\njoin, including excessive information loss, data-unawareness and hard\nconstraint on the distance metric. In this paper, we propose Xling, a generic\nframework to build a learning-based metric space filter with any existing\nregression model, aiming at accurately predicting whether a query point has\nenough number of neighbors. The framework provides a suite of optimization\nstrategies to further improve the prediction quality based on the learning\nmodel, which has demonstrated significantly higher prediction quality than\nexisting MSBF. We also propose XJoin, one of the first filter-based similarity\njoin methods, based on Xling. By predicting and skipping those queries without\nenough neighbors, XJoin can effectively reduce unnecessary neighbor searching\nand therefore it achieves a remarkable acceleration. Benefiting from the\ngeneralization capability of deep learning models, XJoin can be easily\ntransferred onto new dataset (in similar distribution) without re-training.\nFurthermore, Xling is not limited to being applied in XJoin, instead, it acts\nas a flexible plugin that can be inserted to any loop-based similarity join\nmethods for a speedup.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [21.6513729095459, 30.007478713989258], "cluster": 4}, {"key": "wang2025affinity", "year": "2016", "citations": "11", "title": "Affinity Preserving Quantization For Hashing: A Vector Quantization Approach To Learning Compact Binary Codes", "abstract": "<p>Hashing techniques are powerful for approximate nearest\nneighbour (ANN) search. Existing quantization methods in\nhashing are all focused on scalar quantization (SQ) which\nis inferior in utilizing the inherent data distribution. In this\npaper, we propose a novel vector quantization (VQ) method\nnamed affinity preserving quantization (APQ) to improve the\nquantization quality of projection values, which has significantly\nboosted the performance of state-of-the-art hashing\ntechniques. In particular, our method incorporates the neighbourhood\nstructure in the pre- and post-projection data space\ninto vector quantization. APQ minimizes the quantization errors\nof projection values as well as the loss of affinity property\nof original space. An effective algorithm has been proposed\nto solve the joint optimization problem in APQ, and\nthe extension to larger binary codes has been resolved by applying\nproduct quantization to APQ. Extensive experiments\nhave shown that APQ consistently outperforms the state-of-the-art\nquantization methods, and has significantly improved\nthe performance of various hashing techniques.</p>\n", "tags": ["Hashing-Methods", "Quantization", "AAAI", "Compact-Codes", "Evaluation"], "tsne_embedding": [11.614707946777344, 37.406734466552734], "cluster": 4}, {"key": "wang2025deep", "year": "2020", "citations": "9", "title": "Deep Hashing With Active Pairwise Supervision", "abstract": "<p>In this paper, we propose a Deep Hashing method with Active Pairwise Supervision(DH-APS). Conventional methods with passive\npairwise supervision obtain labeled data for training and require large\namount of annotations to reach their full potential, which are not feasible in realistic retrieval tasks. On the contrary, we actively select a small\nquantity of informative samples for annotation to provide effective pairwise supervision so that discriminative hash codes can be obtained with\nlimited annotation budget. Specifically, we generalize the structural risk\nminimization principle and obtain three criteria for the pairwise supervision acquisition: uncertainty, representativeness and diversity. Accordingly, samples involved in the following training pairs should be labeled:\npairs with most uncertain similarity, pairs that minimize the discrepancy\nbetween labeled and unlabeled data, and pairs which are most different\nfrom the annotated data, so that the discriminality and generalization ability of the learned hash codes are significantly strengthened. Moreover,\nour DH-APS can also be employed as a plug-and-play module for semisupervised hashing methods to further enhance the performance. Experiments demonstrate that the presented DH-APS achieves the accuracy\nof supervised hashing methods with only 30% labeled training samples\nand improves the semi-supervised binary codes by a sizable margin.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Supervised", "Evaluation"], "tsne_embedding": [8.18937873840332, -5.682315349578857], "cluster": 6}, {"key": "wang2025focus", "year": "2025", "citations": "0", "title": "Focus On Local: Finding Reliable Discriminative Regions For Visual Place Recognition", "abstract": "<p>Visual Place Recognition (VPR) is aimed at predicting the location of a query\nimage by referencing a database of geotagged images. For VPR task, often fewer\ndiscriminative local regions in an image produce important effects while\nmundane background regions do not contribute or even cause perceptual aliasing\nbecause of easy overlap. However, existing methods lack precisely modeling and\nfull exploitation of these discriminative regions. In this paper, we propose\nthe Focus on Local (FoL) approach to stimulate the performance of image\nretrieval and re-ranking in VPR simultaneously by mining and exploiting\nreliable discriminative local regions in images and introducing\npseudo-correlation supervision. First, we design two losses,\nExtraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background\nContrast Enhancement Loss (CEL), to explicitly model reliable discriminative\nlocal regions and use them to guide the generation of global representations\nand efficient re-ranking. Second, we introduce a weakly-supervised local\nfeature training strategy based on pseudo-correspondences obtained from\naggregating global features to alleviate the lack of local correspondences\nground truth for the VPR task. Third, we suggest an efficient re-ranking\npipeline that is efficiently and precisely based on discriminative region\nguidance. Finally, experimental results show that our FoL achieves the\nstate-of-the-art on multiple VPR benchmarks in both image retrieval and\nre-ranking stages and also significantly outperforms existing two-stage VPR\nmethods in terms of computational efficiency. Code and models are available at\nhttps://github.com/chenshunpeng/FoL</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Hybrid-Ann-Methods", "AAAI", "Datasets", "Supervised", "Re-Ranking", "Evaluation"], "tsne_embedding": [-29.09245491027832, 5.787177562713623], "cluster": 0}, {"key": "wang2025hamming", "year": "2015", "citations": "20", "title": "Hamming Compatible Quantization For Hashing", "abstract": "<p>Hashing is one of the effective techniques for fast\nApproximate Nearest Neighbour (ANN) search.\nTraditional single-bit quantization (SBQ) in most\nhashing methods incurs lots of quantization error\nwhich seriously degrades the search performance.\nTo address the limitation of SBQ, researchers have\nproposed promising multi-bit quantization (MBQ)\nmethods to quantize each projection dimension\nwith multiple bits. However, some MBQ methods\nneed to adopt specific distance for binary code\nmatching instead of the original Hamming distance,\nwhich would significantly decrease the retrieval\nspeed. Two typical MBQ methods Hierarchical\nQuantization and Double Bit Quantization\nretain the Hamming distance, but both of them only\nconsider the projection dimensions during quantization,\nignoring the neighborhood structure of raw\ndata inherent in Euclidean space. In this paper,\nwe propose a multi-bit quantization method named\nHamming Compatible Quantization (HCQ) to preserve\nthe capability of similarity metric between\nEuclidean space and Hamming space by utilizing\nthe neighborhood structure of raw data. Extensive\nexperiment results have shown our approach significantly\nimproves the performance of various stateof-the-art\nhashing methods while maintaining fast\nretrieval speed.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Quantization", "Similarity-Search", "Compact-Codes", "Evaluation"], "tsne_embedding": [-10.711970329284668, 37.398555755615234], "cluster": 8}, {"key": "wang2025online", "year": "2020", "citations": "47", "title": "Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval", "abstract": "<p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "SIGIR", "Evaluation"], "tsne_embedding": [28.840089797973633, 3.1371724605560303], "cluster": 2}, {"key": "wang2025prototype", "year": "2021", "citations": "43", "title": "Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing", "abstract": "<p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "CVPR", "Scalability", "Image-Retrieval", "Robustness", "Tools-&-Libraries", "Supervised", "Evaluation"], "tsne_embedding": [6.080043315887451, -10.881379127502441], "cluster": 6}, {"key": "wang2025semantic", "year": "2015", "citations": "149", "title": "Semantic Topic Multimodal Hashing For Cross-media Retrieval", "abstract": "<p>Multimodal hashing is essential to cross-media\nsimilarity search for its low storage cost and fast\nquery speed. Most existing multimodal hashing\nmethods embedded heterogeneous data into a common low-dimensional Hamming space, and then\nrounded the continuous embeddings to obtain the\nbinary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For\nthis purpose, a novel Semantic Topic Multimodal\nHashing (STMH) is developed by considering latent semantic information in coding procedure.\nIt\nfirst discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.\nThen the learned multimodal semantic features are\ntransformed into a common subspace by their correlations. Finally, each bit of unified hash code\ncan be generated directly by figuring out whether a\ntopic or concept is contained in a text or an image.\nTherefore, the obtained model by STMH is more\nsuitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method\noutperforms several state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Similarity-Search", "Memory-Efficiency", "Compact-Codes", "Evaluation"], "tsne_embedding": [8.993571281433105, -11.423409461975098], "cluster": 7}, {"key": "wang2025semi", "year": "2010", "citations": "626", "title": "Semi-supervised Hashing For Scalable Image Retrieval", "abstract": "<p>Large scale image search has recently attracted considerable\nattention due to easy availability of huge amounts of\ndata. Several hashing methods have been proposed to allow\napproximate but highly efficient search. Unsupervised\nhashing methods show good performance with metric distances\nbut, in image search, semantic similarity is usually\ngiven in terms of labeled pairs of images. There exist supervised\nhashing methods that can handle such semantic similarity\nbut they are prone to overfitting when labeled data\nis small or noisy. Moreover, these methods are usually very\nslow to train. In this work, we propose a semi-supervised\nhashing method that is formulated as minimizing empirical\nerror on the labeled data while maximizing variance\nand independence of hash bits over the labeled and unlabeled\ndata. The proposed method can handle both metric as\nwell as semantic similarity. The experimental results on two\nlarge datasets (up to one million samples) demonstrate its\nsuperior performance over state-of-the-art supervised and\nunsupervised methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "CVPR", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-2.9755475521087646, 22.475332260131836], "cluster": 8}, {"key": "wang2025sequential", "year": "2010", "citations": "328", "title": "Sequential Projection Learning For Hashing With Compact Codes", "abstract": "<p>Hashing based Approximate Nearest Neighbor\n(ANN) search has attracted much attention\ndue to its fast query time and drastically\nreduced storage. However, most of the hashing\nmethods either use random projections or\nextract principal directions from the data to\nderive hash functions. The resulting embedding\nsuffers from poor discrimination when\ncompact codes are used. In this paper, we\npropose a novel data-dependent projection\nlearning method such that each hash function\nis designed to correct the errors made by\nthe previous one sequentially. The proposed\nmethod easily adapts to both unsupervised\nand semi-supervised scenarios and shows significant\nperformance gains over the state-ofthe-art\nmethods on two large datasets containing\nup to 1 million points.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Compact-Codes", "Efficiency", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [12.560175895690918, 31.49854850769043], "cluster": 4}, {"key": "wang2025survey", "year": "2017", "citations": "929", "title": "A Survey On Learning To Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the\nquery point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this\npaper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving\nthe similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different\nthough quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation\nprotocols, and the general performance analysis, and point out that the quantization algori</p>\n", "tags": ["Survey-Paper", "Quantization", "Evaluation", "Hashing-Methods"], "tsne_embedding": [13.000639915466309, 6.206272125244141], "cluster": 6}, {"key": "wei2016selective", "year": "2017", "citations": "434", "title": "Selective Convolutional Descriptor Aggregation For Fine-grained Image Retrieval", "abstract": "<p>Deep convolutional neural network models pre-trained for the ImageNet\nclassification task have been successfully adopted to tasks in other domains,\nsuch as texture description and object proposal generation, but these tasks\nrequire annotations for images in the new domain. In this paper, we focus on a\nnovel and challenging task in the pure unsupervised setting: fine-grained image\nretrieval. Even with image labels, fine-grained images are difficult to\nclassify, let alone the unsupervised retrieval task. We propose the Selective\nConvolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes the\nmain object in fine-grained images, a step that discards the noisy background\nand keeps useful deep descriptors. The selected descriptors are then aggregated\nand dimensionality reduced into a short feature vector using the best practices\nwe found. SCDA is unsupervised, using no image label or bounding box\nannotation. Experiments on six fine-grained datasets confirm the effectiveness\nof SCDA for fine-grained image retrieval. Besides, visualization of the SCDA\nfeatures shows that they correspond to visual attributes (even subtle ones),\nwhich might explain SCDA\u2019s high mean average precision in fine-grained\nretrieval. Moreover, on general image retrieval datasets, SCDA achieves\ncomparable retrieval results with state-of-the-art general image retrieval\napproaches.</p>\n", "tags": ["Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-31.688568115234375, -1.1829819679260254], "cluster": 0}, {"key": "wei2018optimal", "year": "2018", "citations": "0", "title": "Optimal Las Vegas Approximate Near Neighbors In \\(\\ell_p\\)", "abstract": "<p>We show that approximate near neighbor search in high dimensions can be\nsolved in a Las Vegas fashion (i.e., without false negatives) for \\(\\ell_p\\)\n(\\(1\\le p\\le 2\\)) while matching the performance of optimal locality-sensitive\nhashing. Specifically, we construct a data-independent Las Vegas data structure\nwith query time \\(O(dn^{\\rho})\\) and space usage \\(O(dn^{1+\\rho})\\) for \\((r, c\nr)\\)-approximate near neighbors in \\(\\mathbb{R}^{d}\\) under the \\(\\ell_p\\) norm,\nwhere \\(\\rho = 1/c^p + o(1)\\). Furthermore, we give a Las Vegas\nlocality-sensitive filter construction for the unit sphere that can be used\nwith the data-dependent data structure of Andoni et al. (SODA 2017) to achieve\noptimal space-time tradeoffs in the data-dependent setting. For the symmetric\ncase, this gives us a data-dependent Las Vegas data structure with query time\n\\(O(dn^{\\rho})\\) and space usage \\(O(dn^{1+\\rho})\\) for \\((r, c r)\\)-approximate near\nneighbors in \\(\\mathbb{R}^{d}\\) under the \\(\\ell_p\\) norm, where \\(\\rho = 1/(2c^p -\n1) + o(1)\\).\n  Our data-independent construction improves on the recent Las Vegas data\nstructure of Ahle (FOCS 2017) for \\(\\ell_p\\) when \\(1 &lt; p\\le 2\\). Our\ndata-dependent construction does even better for \\(\\ell_p\\) for all \\(p\\in [1, 2]\\)\nand is the first Las Vegas approximate near neighbors data structure to make\nuse of data-dependent approaches. We also answer open questions of Indyk (SODA\n2000), Pagh (SODA 2016), and Ahle by showing that for approximate near\nneighbors, Las Vegas data structures can match state-of-the-art Monte Carlo\ndata structures in performance for both the data-independent and data-dependent\nsettings and across space-time tradeoffs.</p>\n", "tags": ["Efficiency", "Evaluation", "Hashing-Methods"], "tsne_embedding": [23.276817321777344, 52.3211669921875], "cluster": 4}, {"key": "wei2021net", "year": "2021", "citations": "7", "title": "A-net: Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-13.825222969055176, 14.202783584594727], "cluster": 8}, {"key": "wei2021pp", "year": "2021", "citations": "6", "title": "Pp-shitu: A Practical Lightweight Image Recognition System", "abstract": "<p>In recent years, image recognition applications have developed rapidly. A\nlarge number of studies and techniques have emerged in different fields, such\nas face recognition, pedestrian and vehicle re-identification, landmark\nretrieval, and product recognition. In this paper, we propose a practical\nlightweight image recognition system, named PP-ShiTu, consisting of the\nfollowing 3 modules, mainbody detection, feature extraction and vector search.\nWe introduce popular strategies including metric learning, deep hash, knowledge\ndistillation and model quantization to improve accuracy and inference speed.\nWith strategies above, PP-ShiTu works well in different scenarios with a set of\nmodels trained on a mixed dataset. Experiments on different datasets and\nbenchmarks show that the system is widely effective in different domains of\nimage recognition. All the above mentioned models are open-sourced and the code\nis available in the GitHub repository PaddleClas on PaddlePaddle.</p>\n", "tags": ["Quantization", "Neural-Hashing", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-18.514314651489258, -35.584312438964844], "cluster": 3}, {"key": "wei2022accurate", "year": "2022", "citations": "5", "title": "Accurate Instance-level CAD Model Retrieval In A Large-scale Database", "abstract": "<p>We present a new solution to the fine-grained retrieval of clean CAD models\nfrom a large-scale database in order to recover detailed object shape\ngeometries for RGBD scans. Unlike previous work simply indexing into a\nmoderately small database using an object shape descriptor and accepting the\ntop retrieval result, we argue that in the case of a large-scale database a\nmore accurate model may be found within a neighborhood of the descriptor. More\nimportantly, we propose that the distinctiveness deficiency of shape\ndescriptors at the instance level can be compensated by a geometry-based\nre-ranking of its neighborhood. Our approach first leverages the discriminative\npower of learned representations to distinguish between different categories of\nmodels and then uses a novel robust point set distance metric to re-rank the\nCAD neighborhood, enabling fine-grained retrieval in a large shape database.\nEvaluation on a real-world dataset shows that our geometry-based re-ranking is\na conceptually simple but highly effective method that can lead to a\nsignificant improvement in retrieval accuracy compared to the state-of-the-art.</p>\n", "tags": ["Distance-Metric-Learning", "Scalability", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-38.84735870361328, -13.593302726745605], "cluster": 5}, {"key": "wei2022hyperbolic", "year": "2022", "citations": "0", "title": "Hyperbolic Hierarchical Contrastive Hashing", "abstract": "<p>Hierarchical semantic structures, naturally existing in real-world datasets,\ncan assist in capturing the latent distribution of data to learn robust hash\ncodes for retrieval systems. Although hierarchical semantic structures can be\nsimply expressed by integrating semantically relevant data into a high-level\ntaxon with coarser-grained semantics, the construction, embedding, and\nexploitation of the structures remain tricky for unsupervised hash learning. To\ntackle these problems, we propose a novel unsupervised hashing method named\nHyperbolic Hierarchical Contrastive Hashing (HHCH). We propose to embed\ncontinuous hash codes into hyperbolic space for accurate semantic expression\nsince embedding hierarchies in hyperbolic space generates less distortion than\nin hyper-sphere space and Euclidean space. In addition, we extend the K-Means\nalgorithm to hyperbolic space and perform the proposed hierarchical hyperbolic\nK-Means algorithm to construct hierarchical semantic structures adaptively. To\nexploit the hierarchical semantic structures in hyperbolic space, we designed\nthe hierarchical contrastive learning algorithm, including hierarchical\ninstance-wise and hierarchical prototype-wise contrastive learning. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed method\noutperforms the state-of-the-art unsupervised hashing methods. Codes will be\nreleased.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [4.924291133880615, 3.0165021419525146], "cluster": 6}, {"key": "wei2023attribute", "year": "2023", "citations": "16", "title": "Attribute-aware Deep Hashing With Self-consistency For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as\nranking the images depicting the concept of interests (i.e., the same\nsub-category labels) highest based on the fine-grained details in the query. It\nis desirable to alleviate the challenges of both fine-grained nature of small\ninter-class variations with large intra-class variations and explosive growth\nof fine-grained data for such a practical task. In this paper, we propose\nattribute-aware hashing networks with self-consistency for generating\nattribute-aware hash codes to not only make the retrieval process efficient,\nbut also establish explicit correspondences between hash codes and visual\nattributes. Specifically, based on the captured visual representations by\nattention, we develop an encoder-decoder structure network of a reconstruction\ntask to unsupervisedly distill high-level attribute-specific vectors from the\nappearance-specific visual representations without attribute annotations. Our\nmodels are also equipped with a feature decorrelation constraint upon these\nattribute vectors to strengthen their representative abilities. Then, driven by\npreserving original entities\u2019 similarity, the required hash codes can be\ngenerated from these attribute-specific vectors and thus become\nattribute-aware. Furthermore, to combat simplicity bias in deep hashing, we\nconsider the model design from the perspective of the self-consistency\nprinciple and propose to further enhance models\u2019 self-consistency by equipping\nan additional image reconstruction path. Comprehensive quantitative experiments\nunder diverse empirical settings on six fine-grained retrieval datasets and two\ngeneric retrieval datasets show the superiority of our models over competing\nmethods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Neural-Hashing"], "tsne_embedding": [-13.864215850830078, 14.273920059204102], "cluster": 8}, {"key": "wei2023chain", "year": "2023", "citations": "5", "title": "CHAIN: Exploring Global-local Spatio-temporal Information For Improved Self-supervised Video Hashing", "abstract": "<p>Compressing videos into binary codes can improve retrieval speed and reduce\nstorage overhead. However, learning accurate hash codes for video retrieval can\nbe challenging due to high local redundancy and complex global dependencies\nbetween video frames, especially in the absence of labels. Existing\nself-supervised video hashing methods have been effective in designing\nexpressive temporal encoders, but have not fully utilized the temporal dynamics\nand spatial appearance of videos due to less challenging and unreliable\nlearning tasks. To address these challenges, we begin by utilizing the\ncontrastive learning task to capture global spatio-temporal information of\nvideos for hashing. With the aid of our designed augmentation strategies, which\nfocus on spatial and temporal variations to create positive pairs, the learning\nframework can generate hash codes that are invariant to motion, scale, and\nviewpoint. Furthermore, we incorporate two collaborative learning tasks, i.e.,\nframe order verification and scene change regularization, to capture local\nspatio-temporal details within video frames, thereby enhancing the perception\nof temporal structure and the modeling of spatio-temporal relationships. Our\nproposed Contrastive Hashing with Global-Local Spatio-temporal Information\n(CHAIN) outperforms state-of-the-art self-supervised video hashing methods on\nfour video benchmark datasets. Our codes will be released.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-11.874415397644043, 0.4959338307380676], "cluster": 1}, {"key": "wei2023dynamic", "year": "2023", "citations": "0", "title": "Dynamic Visual Semantic Sub-embeddings And Fast Re-ranking", "abstract": "<p>The core of cross-modal matching is to accurately measure the similarity\nbetween different modalities in a unified representation space. However,\ncompared to textual descriptions of a certain perspective, the visual modality\nhas more semantic variations. So, images are usually associated with multiple\ntextual captions in databases. Although popular symmetric embedding methods\nhave explored numerous modal interaction approaches, they often learn toward\nincreasing the average expression probability of multiple semantic variations\nwithin image embeddings. Consequently, information entropy in embeddings is\nincreased, resulting in redundancy and decreased accuracy. In this work, we\npropose a Dynamic Visual Semantic Sub-Embeddings framework (DVSE) to reduce the\ninformation entropy. Specifically, we obtain a set of heterogeneous visual\nsub-embeddings through dynamic orthogonal constraint loss. To encourage the\ngenerated candidate embeddings to capture various semantic variations, we\nconstruct a mixed distribution and employ a variance-aware weighting loss to\nassign different weights to the optimization process. In addition, we develop a\nFast Re-ranking strategy (FR) to efficiently evaluate the retrieval results and\nenhance the performance. We compare the performance with existing set-based\nmethod using four image feature encoders and two text feature encoders on three\nbenchmark datasets: MSCOCO, Flickr30K and CUB Captions. We also show the role\nof different components by ablation studies and perform a sensitivity analysis\nof the hyperparameters. The qualitative analysis of visualized bidirectional\nretrieval and attention maps further demonstrates the ability of our method to\nencode semantic variations.</p>\n", "tags": ["Tools-&-Libraries", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-24.52433204650879, 9.700258255004883], "cluster": 0}, {"key": "wei2024breaking", "year": "2025", "citations": "0", "title": "Breaking The Frame: Visual Place Recognition By Overlap Prediction", "abstract": "<p>Visual place recognition methods struggle with occlusions and partial visual\noverlaps. We propose a novel visual place recognition approach based on overlap\nprediction, called VOP, shifting from traditional reliance on global image\nsimilarities and local features to image overlap prediction. VOP proceeds\nco-visible image sections by obtaining patch-level embeddings using a Vision\nTransformer backbone and establishing patch-to-patch correspondences without\nrequiring expensive feature detection and matching. Our approach uses a voting\nmechanism to assess overlap scores for potential database images. It provides a\nnuanced image retrieval metric in challenging scenarios. Experimental results\nshow that VOP leads to more accurate relative pose estimation and localization\nresults on the retrieved image pairs than state-of-the-art baselines on a\nnumber of large-scale, real-world indoor and outdoor benchmarks. The code is\navailable at https://github.com/weitong8591/vop.git.</p>\n", "tags": ["Image-Retrieval", "Scalability"], "tsne_embedding": [-31.326379776000977, 3.2771353721618652], "cluster": 0}, {"key": "wei2024contrastive", "year": "2024", "citations": "0", "title": "Contrastive Masked Auto-encoders Based Self-supervised Hashing For 2D Image And 3D Point Cloud Cross-modal Retrieval", "abstract": "<p>Implementing cross-modal hashing between 2D images and 3D point-cloud data is\na growing concern in real-world retrieval systems. Simply applying existing\ncross-modal approaches to this new task fails to adequately capture latent\nmulti-modal semantics and effectively bridge the modality gap between 2D and\n3D. To address these issues without relying on hand-crafted labels, we propose\ncontrastive masked autoencoders based self-supervised hashing (CMAH) for\nretrieval between images and point-cloud data. We start by contrasting 2D-3D\npairs and explicitly constraining them into a joint Hamming space. This\ncontrastive learning process ensures robust discriminability for the generated\nhash codes and effectively reduces the modality gap. Moreover, we utilize\nmulti-modal auto-encoders to enhance the model\u2019s understanding of multi-modal\nsemantics. By completing the masked image/point-cloud data modeling task, the\nmodel is encouraged to capture more localized clues. In addition, the proposed\nmulti-modal fusion block facilitates fine-grained interactions among different\nmodalities. Extensive experiments on three public datasets demonstrate that the\nproposed CMAH significantly outperforms all baseline methods.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Multimodal-Retrieval", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-11.571795463562012, 12.161614418029785], "cluster": 8}, {"key": "wei2024snap", "year": "2024", "citations": "0", "title": "Snap And Diagnose: An Advanced Multimodal Retrieval System For Identifying Plant Diseases In The Wild", "abstract": "<p>Plant disease recognition is a critical task that ensures crop health and\nmitigates the damage caused by diseases. A handy tool that enables farmers to\nreceive a diagnosis based on query pictures or the text description of\nsuspicious plants is in high demand for initiating treatment before potential\ndiseases spread further. In this paper, we develop a multimodal plant disease\nimage retrieval system to support disease search based on either image or text\nprompts. Specifically, we utilize the largest in-the-wild plant disease dataset\nPlantWild, which includes over 18,000 images across 89 categories, to provide a\ncomprehensive view of potential diseases relating to the query. Furthermore,\ncross-modal retrieval is achieved in the developed system, facilitated by a\nnovel CLIP-based vision-language model that encodes both disease descriptions\nand disease images into the same latent space. Built on top of the retriever,\nour retrieval system allows users to upload either plant disease images or\ndisease descriptions to retrieve the corresponding images with similar\ncharacteristics from the disease dataset to suggest candidate diseases for end\nusers\u2019 consideration.</p>\n", "tags": ["Multimodal-Retrieval", "Image-Retrieval", "Datasets"], "tsne_embedding": [-56.24644470214844, 21.52701759338379], "cluster": 0}, {"key": "wei2025net", "year": "2021", "citations": "7", "title": "A-net: Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-13.826271057128906, 14.204439163208008], "cluster": 8}, {"key": "weinzaepfel2022learning", "year": "2022", "citations": "11", "title": "Learning Super-features For Image Retrieval", "abstract": "<p>Methods that combine local and global features have recently shown excellent\nperformance on multiple challenging deep image retrieval benchmarks, but their\nuse of local features raises at least two issues. First, these local features\nsimply boil down to the localized map activations of a neural network, and\nhence can be extremely redundant. Second, they are typically trained with a\nglobal loss that only acts on top of an aggregation of local features; by\ncontrast, testing is based on local feature matching, which creates a\ndiscrepancy between training and testing. In this paper, we propose a novel\narchitecture for deep image retrieval, based solely on mid-level features that\nwe call Super-features. These Super-features are constructed by an iterative\nattention module and constitute an ordered set in which each element focuses on\na localized and discriminant image pattern. For training, they require only\nimage labels. A contrastive loss operates directly at the level of\nSuper-features and focuses on those that match across images. A second\ncomplementary loss encourages diversity. Experiments on common landmark\nretrieval benchmarks validate that Super-features substantially outperform\nstate-of-the-art methods when using the same number of features, and only\nrequire a significantly smaller memory footprint to match their performance.\nCode and models are available at: https://github.com/naver/FIRe.</p>\n", "tags": ["Memory-Efficiency", "Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-32.06410217285156, -4.4172186851501465], "cluster": 0}, {"key": "weiss2008spectral", "year": "2008", "citations": "2154", "title": "Spectral Hashing", "abstract": "<p>Semantic hashing seeks compact binary codes of data-points so that the\nHamming distance between codewords correlates with semantic similarity.\nIn this paper, we show that the problem of finding a best code for a given\ndataset is closely related to the problem of graph partitioning and can\nbe shown to be NP hard. By relaxing the original problem, we obtain a\nspectral method whose solutions are simply a subset of thresholded eigenvectors\nof the graph Laplacian. By utilizing recent results on convergence\nof graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of\nmanifolds, we show how to efficiently calculate the code of a novel datapoint.\nTaken together, both learning the code and applying it to a novel\npoint are extremely simple. Our experiments show that our codes outperform\nthe state-of-the art.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Text-Retrieval", "Datasets"], "tsne_embedding": [52.442108154296875, 4.557441711425781], "cluster": 9}, {"key": "weiss2012multidimensional", "year": "2012", "citations": "162", "title": "Multidimensional Spectral Hashing", "abstract": "<p>en a surge of interest in methods based on \u201csemantic hashing\u201d,\ni.e. compact binary codes of data-points so that the Hamming distance\nbetween codewords correlates with similarity. In reviewing and\ncomparing existing methods, we show that their relative performance can\nchange drastically depending on the definition of ground-truth neighbors.\nMotivated by this finding, we propose a new formulation for learning binary\ncodes which seeks to reconstruct the affinity between datapoints,\nrather than their distances. We show that this criterion is intractable\nto solve exactly, but a spectral relaxation gives an algorithm where the\nbits correspond to thresholded eigenvectors of the affinity matrix, and\nas the number of datapoints goes to infinity these eigenvectors converge\nto eigenfunctions of Laplace-Beltrami operators, similar to the recently\nproposed Spectral Hashing (SH) method. Unlike SH whose performance\nmay degrade as the number of bits increases, the optimal code using\nour formulation is guaranteed to faithfully reproduce the affinities as\nthe number of bits increases. We show that the number of eigenfunctions\nneeded may increase exponentially with dimension, but introduce a \u201ckernel\ntrick\u201d to allow us to compute with an exponentially large number of\nbits but using only memory and computation that grows linearly with\ndimension. Experiments shows that MDSH outperforms the state-of-the\nart, especially in the challenging regime of small distance thresholds.</p>\n", "tags": ["Compact-Codes", "Evaluation", "Hashing-Methods", "Text-Retrieval"], "tsne_embedding": [16.494617462158203, 36.9911994934082], "cluster": 4}, {"key": "weiss2025multidimensional", "year": "2012", "citations": "162", "title": "Multidimensional Spectral Hashing", "abstract": "<p>en a surge of interest in methods based on \u201csemantic hashing\u201d,\ni.e. compact binary codes of data-points so that the Hamming distance\nbetween codewords correlates with similarity. In reviewing and\ncomparing existing methods, we show that their relative performance can\nchange drastically depending on the definition of ground-truth neighbors.\nMotivated by this finding, we propose a new formulation for learning binary\ncodes which seeks to reconstruct the affinity between datapoints,\nrather than their distances. We show that this criterion is intractable\nto solve exactly, but a spectral relaxation gives an algorithm where the\nbits correspond to thresholded eigenvectors of the affinity matrix, and\nas the number of datapoints goes to infinity these eigenvectors converge\nto eigenfunctions of Laplace-Beltrami operators, similar to the recently\nproposed Spectral Hashing (SH) method. Unlike SH whose performance\nmay degrade as the number of bits increases, the optimal code using\nour formulation is guaranteed to faithfully reproduce the affinities as\nthe number of bits increases. We show that the number of eigenfunctions\nneeded may increase exponentially with dimension, but introduce a \u201ckernel\ntrick\u201d to allow us to compute with an exponentially large number of\nbits but using only memory and computation that grows linearly with\ndimension. Experiments shows that MDSH outperforms the state-of-the\nart, especially in the challenging regime of small distance thresholds.</p>\n", "tags": ["Compact-Codes", "Evaluation", "Hashing-Methods", "Text-Retrieval"], "tsne_embedding": [16.494617462158203, 36.9911994934082], "cluster": 4}, {"key": "weiss2025spectral", "year": "2008", "citations": "2154", "title": "Spectral Hashing", "abstract": "<p>Semantic hashing seeks compact binary codes of data-points so that the\nHamming distance between codewords correlates with semantic similarity.\nIn this paper, we show that the problem of finding a best code for a given\ndataset is closely related to the problem of graph partitioning and can\nbe shown to be NP hard. By relaxing the original problem, we obtain a\nspectral method whose solutions are simply a subset of thresholded eigenvectors\nof the graph Laplacian. By utilizing recent results on convergence\nof graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of\nmanifolds, we show how to efficiently calculate the code of a novel datapoint.\nTaken together, both learning the code and applying it to a novel\npoint are extremely simple. Our experiments show that our codes outperform\nthe state-of-the art.</p>\n", "tags": ["Compact-Codes", "Hashing-Methods", "Text-Retrieval", "Datasets"], "tsne_embedding": [52.4421272277832, 4.557410717010498], "cluster": 9}, {"key": "weissman2014identifying", "year": "2015", "citations": "10", "title": "Identifying Duplicate And Contradictory Information In Wikipedia", "abstract": "<p>Our study identifies sentences in Wikipedia articles that are either\nidentical or highly similar by applying techniques for near-duplicate detection\nof web pages. This is accomplished with a MapReduce implementation of minhash\nto identify clusters of sentences with high Jaccard similarity. We show that\nthese clusters can be categorized into six different types, two of which are\nparticularly interesting: identical sentences quantify the extent to which\ncontent in Wikipedia is copied and pasted, and near-duplicate sentences that\nstate contradictory facts point to quality issues in Wikipedia.</p>\n", "tags": ["Locality-Sensitive-Hashing"], "tsne_embedding": [36.98847198486328, -23.503816604614258], "cluster": 7}, {"key": "wen2019adversarial", "year": "2019", "citations": "18", "title": "Adversarial Cross-modal Retrieval Via Learning And Transferring Single-modal Similarities", "abstract": "<p>Cross-modal retrieval aims to retrieve relevant data across different\nmodalities (e.g., texts vs. images). The common strategy is to apply\nelement-wise constraints between manually labeled pair-wise items to guide the\ngenerators to learn the semantic relationships between the modalities, so that\nthe similar items can be projected close to each other in the common\nrepresentation subspace. However, such constraints often fail to preserve the\nsemantic structure between unpaired but semantically similar items (e.g. the\nunpaired items with the same class label are more similar than items with\ndifferent labels). To address the above problem, we propose a novel cross-modal\nsimilarity transferring (CMST) method to learn and preserve the semantic\nrelationships between unpaired items in an unsupervised way. The key idea is to\nlearn the quantitative similarities in single-modal representation subspace,\nand then transfer them to the common representation subspace to establish the\nsemantic relationships between unpaired items across modalities. Experiments\nshow that our method outperforms the state-of-the-art approaches both in the\nclass-based and pair-based retrieval tasks.</p>\n", "tags": ["Multimodal-Retrieval", "Unsupervised", "Robustness"], "tsne_embedding": [-20.267868041992188, -6.4262895584106445], "cluster": 1}, {"key": "weng2019efficient", "year": "2020", "citations": "3", "title": "Efficient Querying From Weighted Binary Codes", "abstract": "<p>Binary codes are widely used to represent the data due to their small storage\nand efficient computation. However, there exists an ambiguity problem that lots\nof binary codes share the same Hamming distance to a query. To alleviate the\nambiguity problem, weighted binary codes assign different weights to each bit\nof binary codes and compare the binary codes by the weighted Hamming distance.\nTill now, performing the querying from the weighted binary codes efficiently is\nstill an open issue. In this paper, we propose a new method to rank the\nweighted binary codes and return the nearest weighted binary codes of the query\nefficiently. In our method, based on the multi-index hash tables, two\nalgorithms, the table bucket finding algorithm and the table merging algorithm,\nare proposed to select the nearest weighted binary codes of the query in a\nnon-exhaustive and accurate way. The proposed algorithms are justified by\nproving their theoretic properties. The experiments on three large-scale\ndatasets validate both the search efficiency and the search accuracy of our\nmethod. Especially for the number of weighted binary codes up to one billion,\nour method shows a great improvement of more than 1000 times faster than the\nlinear scan.</p>\n", "tags": ["Efficiency", "Vector-Indexing", "Scalability", "AAAI", "Datasets", "Compact-Codes"], "tsne_embedding": [16.837196350097656, 7.124932765960693], "cluster": 6}, {"key": "weng2019online", "year": "2020", "citations": "15", "title": "Online Hashing With Efficient Updating Of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the\nstreaming data. However, when the hash functions change, the binary codes for\nthe database have to be recomputed to guarantee the retrieval accuracy.\nRecomputing the binary codes by accumulating the whole database brings a\ntimeliness challenge to the online retrieval process. In this paper, we propose\na novel online hashing framework to update the binary codes efficiently without\naccumulating the whole database. In our framework, the hash functions are fixed\nand the projection functions are introduced to learn online from the streaming\ndata. Therefore, inefficient updating of the binary codes by accumulating the\nwhole database can be transformed to efficient updating of the binary codes by\nprojecting the binary codes into another binary space. The queries and the\nbinary code database are projected asymmetrically to further improve the\nretrieval accuracy. The experiments on two multi-label image databases\ndemonstrate the effectiveness and the efficiency of our method for multi-label\nimage retrieval.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "AAAI", "Tools-&-Libraries", "Compact-Codes"], "tsne_embedding": [31.53652572631836, 5.013258457183838], "cluster": 2}, {"key": "weng2020fast", "year": "2020", "citations": "0", "title": "Fast Search On Binary Codes By Weighted Hamming Distance", "abstract": "<p>Weighted Hamming distance, as a similarity measure between binary codes and\nbinary queries, provides superior accuracy in search tasks than Hamming\ndistance. However, how to efficiently and accurately find \\(K\\) binary codes that\nhave the smallest weighted Hamming distance to the query remains an open issue.\nIn this paper, a fast search algorithm is proposed to perform the\nnon-exhaustive search for \\(K\\) nearest binary codes by weighted Hamming\ndistance. By using binary codes as direct bucket indices in a hash table, the\nsearch algorithm generates a sequence to probe the buckets based on the\nindependence characteristic of the weights for each bit. Furthermore, a fast\nsearch framework based on the proposed search algorithm is designed to solve\nthe problem of long binary codes. Specifically, long binary codes are split\ninto substrings and multiple hash tables are built on them. Then, the search\nalgorithm probes the buckets to obtain candidates according to the generated\nsubstring indices, and a merging algorithm is proposed to find the nearest\nbinary codes by merging the candidates. Theoretical analysis and experimental\nresults demonstrate that the search algorithm improves the search accuracy\ncompared to other non-exhaustive algorithms and provides orders-of-magnitude\nfaster search than the linear scan baseline.</p>\n", "tags": ["Compact-Codes", "Tools-&-Libraries"], "tsne_embedding": [-1.058664321899414, 30.00631332397461], "cluster": 8}, {"key": "weng2020online", "year": "2020", "citations": "15", "title": "Online Hashing With Efficient Updating Of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "AAAI", "Tools-&-Libraries", "Compact-Codes"], "tsne_embedding": [31.53658103942871, 5.013228893280029], "cluster": 2}, {"key": "weng2020random", "year": "2020", "citations": "1", "title": "Random VLAD Based Deep Hashing For Efficient Image Retrieval", "abstract": "<p>Image hash algorithms generate compact binary representations that can be\nquickly matched by Hamming distance, thus become an efficient solution for\nlarge-scale image retrieval. This paper proposes RV-SSDH, a deep image hash\nalgorithm that incorporates the classical VLAD (vector of locally aggregated\ndescriptors) architecture into neural networks. Specifically, a novel neural\nnetwork component is formed by coupling a random VLAD layer with a latent hash\nlayer through a transform layer. This component can be combined with\nconvolutional layers to realize a hash algorithm. We implement RV-SSDH as a\npoint-wise algorithm that can be efficiently trained by minimizing\nclassification error and quantization loss. Comprehensive experiments show this\nnew architecture significantly outperforms baselines such as NetVLAD and SSDH,\nand offers a cost-effective trade-off in the state-of-the-art. In addition, the\nproposed random VLAD layer leads to satisfactory accuracy with low complexity,\nthus shows promising potentials as an alternative to NetVLAD.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Image-Retrieval", "Scalability", "Neural-Hashing"], "tsne_embedding": [1.1116548776626587, 19.192399978637695], "cluster": 8}, {"key": "weng2021online", "year": "2021", "citations": "0", "title": "Online Hashing With Similarity Learning", "abstract": "<p>Online hashing methods usually learn the hash functions online, aiming to\nefficiently adapt to the data variations in the streaming environment. However,\nwhen the hash functions are updated, the binary codes for the whole database\nhave to be updated to be consistent with the hash functions, resulting in the\ninefficiency in the online image retrieval process. In this paper, we propose a\nnovel online hashing framework without updating binary codes. In the proposed\nframework, the hash functions are fixed and a parametric similarity function\nfor the binary codes is learnt online to adapt to the streaming data.\nSpecifically, a parametric similarity function that has a bilinear form is\nadopted and a metric learning algorithm is proposed to learn the similarity\nfunction online based on the characteristics of the hashing methods. The\nexperiments on two multi-label image datasets show that our method is\ncompetitive or outperforms the state-of-the-art online hashing methods in terms\nof both accuracy and efficiency for multi-label image retrieval.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Compact-Codes"], "tsne_embedding": [32.1368293762207, 4.70613956451416], "cluster": 2}, {"key": "weng2023constant", "year": "2023", "citations": "0", "title": "Constant Sequence Extension For Fast Search Using Weighted Hamming Distance", "abstract": "<p>Representing visual data using compact binary codes is attracting increasing\nattention as binary codes are used as direct indices into hash table(s) for\nfast non-exhaustive search. Recent methods show that ranking binary codes using\nweighted Hamming distance (WHD) rather than Hamming distance (HD) by generating\nquery-adaptive weights for each bit can better retrieve query-related items.\nHowever, search using WHD is slower than that using HD. One main challenge is\nthat the complexity of extending a monotone increasing sequence using WHD to\nprobe buckets in hash table(s) for existing methods is at least proportional to\nthe square of the sequence length, while that using HD is proportional to the\nsequence length. To overcome this challenge, we propose a novel fast\nnon-exhaustive search method using WHD. The key idea is to design a constant\nsequence extension algorithm to perform each sequence extension in constant\ncomputational complexity and the total complexity is proportional to the\nsequence length, which is justified by theoretical analysis. Experimental\nresults show that our method is faster than other WHD-based search methods.\nAlso, compared with the HD-based non-exhaustive search method, our method has\ncomparable efficiency but retrieves more query-related items for the dataset of\nup to one billion items.</p>\n", "tags": ["Compact-Codes", "Efficiency", "Datasets"], "tsne_embedding": [9.145095825195312, 41.14697265625], "cluster": 4}, {"key": "weng2025online", "year": "2020", "citations": "15", "title": "Online Hashing With Efficient Updating Of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Image-Retrieval", "AAAI", "Tools-&-Libraries", "Compact-Codes"], "tsne_embedding": [31.53659439086914, 5.013202667236328], "cluster": 2}, {"key": "westermann2021sentence", "year": "2020", "citations": "22", "title": "Sentence Embeddings And High-speed Similarity Search For Fast Computer Assisted Annotation Of Legal Documents", "abstract": "<p>Human-performed annotation of sentences in legal documents is an important\nprerequisite to many machine learning based systems supporting legal tasks.\nTypically, the annotation is done sequentially, sentence by sentence, which is\noften time consuming and, hence, expensive. In this paper, we introduce a\nproof-of-concept system for annotating sentences \u201claterally.\u201d The approach is\nbased on the observation that sentences that are similar in meaning often have\nthe same label in terms of a particular type system. We use this observation in\nallowing annotators to quickly view and annotate sentences that are\nsemantically similar to a given sentence, across an entire corpus of documents.\nHere, we present the interface of the system and empirically evaluate the\napproach. The experiments show that lateral annotation has the potential to\nmake the annotation process quicker and more consistent.</p>\n", "tags": ["Similarity-Search"], "tsne_embedding": [2.947252035140991, -40.32991409301758], "cluster": 3}, {"key": "weyand2020google", "year": "2020", "citations": "235", "title": "Google Landmarks Dataset V2 -- A Large-scale Benchmark For Instance-level Recognition And Retrieval", "abstract": "<p>While image retrieval and instance recognition techniques are progressing\nrapidly, there is a need for challenging datasets to accurately measure their\nperformance \u2013 while posing novel challenges that are relevant for practical\napplications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new\nbenchmark for large-scale, fine-grained instance recognition and image\nretrieval in the domain of human-made and natural landmarks. GLDv2 is the\nlargest such dataset to date by a large margin, including over 5M images and\n200k distinct instance labels. Its test set consists of 118k images with ground\ntruth annotations for both the retrieval and recognition tasks. The ground\ntruth construction involved over 800 hours of human annotator work. Our new\ndataset has several challenging properties inspired by real world applications\nthat previous datasets did not consider: An extremely long-tailed class\ndistribution, a large fraction of out-of-domain test photos and large\nintra-class variability. The dataset is sourced from Wikimedia Commons, the\nworld\u2019s largest crowdsourced collection of landmark photos. We provide baseline\nresults for both recognition and retrieval tasks based on state-of-the-art\nmethods as well as competitive results from a public challenge. We further\ndemonstrate the suitability of the dataset for transfer learning by showing\nthat image embeddings trained on it achieve competitive retrieval performance\non independent datasets. The dataset images, ground-truth and metric scoring\ncode are available at https://github.com/cvdfoundation/google-landmark.</p>\n", "tags": ["CVPR", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-35.35395050048828, 9.178929328918457], "cluster": 0}, {"key": "wieczorek2021unreasonable", "year": "2021", "citations": "84", "title": "On The Unreasonable Effectiveness Of Centroids In Image Retrieval", "abstract": "<p>Image retrieval task consists of finding similar images to a query image from\na set of gallery (database) images. Such systems are used in various\napplications e.g. person re-identification (ReID) or visual product search.\nDespite active development of retrieval models it still remains a challenging\ntask mainly due to large intra-class variance caused by changes in view angle,\nlighting, background clutter or occlusion, while inter-class variance may be\nrelatively low. A large portion of current research focuses on creating more\nrobust features and modifying objective functions, usually based on Triplet\nLoss. Some works experiment with using centroid/proxy representation of a class\nto alleviate problems with computing speed and hard samples mining used with\nTriplet Loss. However, these approaches are used for training alone and\ndiscarded during the retrieval stage. In this paper we propose to use the mean\ncentroid representation both during training and retrieval. Such an aggregated\nrepresentation is more robust to outliers and assures more stable features. As\neach class is represented by a single embedding - the class centroid - both\nretrieval time and storage requirements are reduced significantly. Aggregating\nmultiple embeddings results in a significant reduction of the search space due\nto lowering the number of candidate target vectors, which makes the method\nespecially suitable for production deployments. Comprehensive experiments\nconducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness\nof our method, which outperforms the current state-of-the-art. We propose\ncentroid training and retrieval as a viable method for both Fashion Retrieval\nand ReID applications.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-29.03154182434082, -5.456885814666748], "cluster": 1}, {"key": "wieschollek2017efficient", "year": "2016", "citations": "55", "title": "Efficient Large-scale Approximate Nearest Neighbor Search On The GPU", "abstract": "<p>We present a new approach for efficient approximate nearest neighbor (ANN)\nsearch in high dimensional spaces, extending the idea of Product Quantization.\nWe propose a two-level product and vector quantization tree that reduces the\nnumber of vector comparisons required during tree traversal. Our approach also\nincludes a novel highly parallelizable re-ranking method for candidate vectors\nby efficiently reusing already computed intermediate values. Due to its small\nmemory footprint during traversal, the method lends itself to an efficient,\nparallel GPU implementation. This Product Quantization Tree (PQT) approach\nsignificantly outperforms recent state of the art methods for high dimensional\nnearest neighbor queries on standard reference datasets. Ours is the first work\nthat demonstrates GPU performance superior to CPU performance on high\ndimensional, large scale ANN problems in time-critical real-world applications,\nlike loop-closing in videos.</p>\n", "tags": ["Quantization", "CVPR", "Scalability", "Memory-Efficiency", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [46.87173080444336, 16.812454223632812], "cluster": 9}, {"key": "wiggers2019deep", "year": "2019", "citations": "4", "title": "Deep Learning Approaches For Image Retrieval And Pattern Spotting In Ancient Documents", "abstract": "<p>This paper describes two approaches for content-based image retrieval and\npattern spotting in document images using deep learning. The first approach\nuses a pre-trained CNN model to cope with the lack of training data, which is\nfine-tuned to achieve a compact yet discriminant representation of queries and\nimage candidates. The second approach uses a Siamese Convolution Neural Network\ntrained on a previously prepared subset of image pairs from the ImageNet\ndataset to provide the similarity-based feature maps. In both methods, the\nlearned representation scheme considers feature maps of different sizes which\nare evaluated in terms of retrieval performance. A robust experimental protocol\nusing two public datasets (Tobacoo-800 and DocExplore) has shown that the\nproposed methods compare favorably against state-of-the-art document image\nretrieval and pattern spotting methods.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-39.59313201904297, -0.2432699352502823], "cluster": 0}, {"key": "wiggers2019image", "year": "2019", "citations": "37", "title": "Image Retrieval And Pattern Spotting Using Siamese Neural Network", "abstract": "<p>This paper presents a novel approach for image retrieval and pattern spotting\nin document image collections. The manual feature engineering is avoided by\nlearning a similarity-based representation using a Siamese Neural Network\ntrained on a previously prepared subset of image pairs from the ImageNet\ndataset. The learned representation is used to provide the similarity-based\nfeature maps used to find relevant image candidates in the data collection\ngiven an image query. A robust experimental protocol based on the public\nTobacco800 document image collection shows that the proposed method compares\nfavorably against state-of-the-art document image retrieval methods, reaching\n0.94 and 0.83 of mean average precision (mAP) for retrieval and pattern\nspotting (IoU=0.7), respectively. Besides, we have evaluated the proposed\nmethod considering feature maps of different sizes, showing the impact of\nreducing the number of features in the retrieval performance and\ntime-consuming.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-33.4580078125, -7.9792799949646], "cluster": 5}, {"key": "winata2024miners", "year": "2024", "citations": "0", "title": "MINERS: Multilingual Language Models As Semantic Retrievers", "abstract": "<p>Words have been represented in a high-dimensional vector space that encodes\ntheir semantic similarities, enabling downstream applications such as\nretrieving synonyms, antonyms, and relevant contexts. However, despite recent\nadvances in multilingual language models (LMs), the effectiveness of these\nmodels\u2019 representations in semantic retrieval contexts has not been\ncomprehensively explored. To fill this gap, this paper introduces the MINERS, a\nbenchmark designed to evaluate the ability of multilingual LMs in semantic\nretrieval tasks, including bitext mining and classification via\nretrieval-augmented contexts. We create a comprehensive framework to assess the\nrobustness of LMs in retrieving samples across over 200 diverse languages,\nincluding extremely low-resource languages in challenging cross-lingual and\ncode-switching settings. Our results demonstrate that by solely retrieving\nsemantically similar embeddings yields performance competitive with\nstate-of-the-art approaches, without requiring any fine-tuning.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Robustness"], "tsne_embedding": [-1.2195727825164795, -16.43865966796875], "cluster": 1}, {"key": "witherspoon2020seec", "year": "2020", "citations": "1", "title": "SEEC: Semantic Vector Federation Across Edge Computing Environments", "abstract": "<p>Semantic vector embedding techniques have proven useful in learning semantic\nrepresentations of data across multiple domains. A key application enabled by\nsuch techniques is the ability to measure semantic similarity between given\ndata samples and find data most similar to a given sample. State-of-the-art\nembedding approaches assume all data is available on a single site. However, in\nmany business settings, data is distributed across multiple edge locations and\ncannot be aggregated due to a variety of constraints. Hence, the applicability\nof state-of-the-art embedding approaches is limited to freely shared datasets,\nleaving out applications with sensitive or mission-critical data. This paper\naddresses this gap by proposing novel unsupervised algorithms called\n<em>SEEC</em> for learning and applying semantic vector embedding in a variety of\ndistributed settings. Specifically, for scenarios where multiple edge locations\ncan engage in joint learning, we adapt the recently proposed federated learning\ntechniques for semantic vector embedding. Where joint learning is not possible,\nwe propose novel semantic vector translation algorithms to enable semantic\nquery across multiple edge locations, each with its own semantic vector-space.\nExperimental results on natural language as well as graph datasets show that\nthis may be a promising new direction.</p>\n", "tags": ["Unsupervised", "Datasets"], "tsne_embedding": [34.8005256652832, -7.861720561981201], "cluster": 9}, {"key": "won2020multimodal", "year": "2021", "citations": "28", "title": "Multimodal Metric Learning For Tag-based Music Retrieval", "abstract": "<p>Tag-based music retrieval is crucial to browse large-scale music libraries\nefficiently. Hence, automatic music tagging has been actively explored, mostly\nas a classification task, which has an inherent limitation: a fixed vocabulary.\nOn the other hand, metric learning enables flexible vocabularies by using\npretrained word embeddings as side information. Also, metric learning has\nalready proven its suitability for cross-modal retrieval tasks in other domains\n(e.g., text-to-image) by jointly learning a multimodal embedding space. In this\npaper, we investigate three ideas to successfully introduce multimodal metric\nlearning for tag-based music retrieval: elaborate triplet sampling, acoustic\nand cultural music information, and domain-specific word embeddings. Our\nexperimental results show that the proposed ideas enhance the retrieval system\nquantitatively, and qualitatively. Furthermore, we release the MSD500, a subset\nof the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually\nannotated tag categories, and user taste profiles.</p>\n", "tags": ["Distance-Metric-Learning", "Scalability", "Multimodal-Retrieval", "ICASSP", "Datasets"], "tsne_embedding": [9.721941947937012, -44.97414779663086], "cluster": 3}, {"key": "won2021emotion", "year": "2021", "citations": "4", "title": "Emotion Embedding Spaces For Matching Music To Stories", "abstract": "<p>Content creators often use music to enhance their stories, as it can be a\npowerful tool to convey emotion. In this paper, our goal is to help creators\nfind music to match the emotion of their story. We focus on text-based stories\nthat can be auralized (e.g., books), use multiple sentences as input queries,\nand automatically retrieve matching music. We formalize this task as a\ncross-modal text-to-music retrieval problem. Both the music and text domains\nhave existing datasets with emotion labels, but mismatched emotion vocabularies\nprevent us from using mood or emotion annotations directly for matching. To\naddress this challenge, we propose and investigate several emotion embedding\nspaces, both manually defined (e.g., valence/arousal) and data-driven (e.g.,\nWord2Vec and metric learning) to bridge this gap. Our experiments show that by\nleveraging these embedding spaces, we are able to successfully bridge the gap\nbetween modalities to facilitate cross modal retrieval. We show that our method\ncan leverage the well established valence-arousal space, but that it can also\nachieve our goal via data-driven embedding spaces. By leveraging data-driven\nembeddings, our approach has the potential of being generalized to other\nretrieval tasks that require broader or completely different vocabularies.</p>\n", "tags": ["Multimodal-Retrieval", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [11.088769912719727, -44.67815399169922], "cluster": 3}, {"key": "wong2024shotit", "year": "2024", "citations": "0", "title": "Shotit: Compute-efficient Image-to-video Search Engine For The Cloud", "abstract": "<p>With the rapid growth of information technology, users are exposed to a\nmassive amount of data online, including image, music, and video. This has led\nto strong needs to provide effective corresponsive search services such as\nimage, music, and video search services. Most of them are operated based on\nkeywords, namely using keywords to find related image, music, and video.\nAdditionally, there are image-to-image search services that enable users to\nfind similar images using one input image. Given that videos are essentially\ncomposed of image frames, then similar videos can be searched by one input\nimage or screenshot. We want to target this scenario and provide an efficient\nmethod and implementation in this paper.\n  We present Shotit, a cloud-native image-to-video search engine that tailors\nthis search scenario in a compute-efficient approach. One main limitation faced\nin this scenario is the scale of its dataset. A typical image-to-image search\nengine only handles one-to-one relationships, colloquially, one image\ncorresponds to another single image. But image-to-video proliferates. Take a\n24-min length video as an example, it will generate roughly 20,000 image\nframes. As the number of videos grows, the scale of the dataset explodes\nexponentially. In this case, a compute-efficient approach ought to be\nconsidered, and the system design should cater to the cloud-native trend.\nChoosing an emerging technology - vector database as its backbone, Shotit fits\nthese two metrics performantly. Experiments for two different datasets, a 50\nthousand-scale Blender Open Movie dataset, and a 50 million-scale proprietary\nTV genre dataset at a 4 Core 32GB RAM Intel Xeon Gold 6271C cloud machine with\nobject storage reveal the effectiveness of Shotit. A demo regarding the Blender\nOpen Movie dataset is illustrated within this paper.</p>\n", "tags": ["Video-Retrieval", "Image-Retrieval", "Datasets"], "tsne_embedding": [-40.884830474853516, -31.169050216674805], "cluster": 5}, {"key": "woodbridge2018detecting", "year": "2018", "citations": "34", "title": "Detecting Homoglyph Attacks With A Siamese Neural Network", "abstract": "<p>A homoglyph (name spoofing) attack is a common technique used by adversaries\nto obfuscate file and domain names. This technique creates process or domain\nnames that are visually similar to legitimate and recognized names. For\ninstance, an attacker may create malware with the name svch0st.exe so that in a\nvisual inspection of running processes or a directory listing, the process or\nfile name might be mistaken as the Windows system process svchost.exe. There\nhas been limited published research on detecting homoglyph attacks. Current\napproaches rely on string comparison algorithms (such as Levenshtein distance)\nthat result in computationally heavy solutions with a high number of false\npositives. In addition, there is a deficiency in the number of publicly\navailable datasets for reproducible research, with most datasets focused on\nphishing attacks, in which homoglyphs are not always used. This paper presents\na fundamentally different solution to this problem using a Siamese\nconvolutional neural network (CNN). Rather than leveraging similarity based on\ncharacter swaps and deletions, this technique uses a learned metric on strings\nrendered as images: a CNN learns features that are optimized to detect visual\nsimilarity of the rendered strings. The trained model is used to convert\nthousands of potentially targeted process or domain names to feature vectors.\nThese feature vectors are indexed using randomized KD-Trees to make similarity\nsearches extremely fast with minimal computational processing. This technique\nshows a considerable 13% to 45% improvement over baseline techniques in terms\nof area under the receiver operating characteristic curve (ROC AUC). In\naddition, we provide both code and data to further future research.</p>\n", "tags": ["Tree-Based-Ann", "Evaluation", "Datasets"], "tsne_embedding": [-1.281361699104309, 14.773274421691895], "cluster": 8}, {"key": "wray2019fine", "year": "2019", "citations": "130", "title": "Fine-grained Action Retrieval Through Multiple Parts-of-speech Embeddings", "abstract": "<p>We address the problem of cross-modal fine-grained action retrieval between\ntext and video. Cross-modal retrieval is commonly achieved through learning a\nshared embedding space, that can indifferently embed modalities. In this paper,\nwe propose to enrich the embedding by disentangling parts-of-speech (PoS) in\nthe accompanying captions. We build a separate multi-modal embedding space for\neach PoS tag. The outputs of multiple PoS embeddings are then used as input to\nan integrated multi-modal space, where we perform action retrieval. All\nembeddings are trained jointly through a combination of PoS-aware and\nPoS-agnostic losses. Our proposal enables learning specialised embedding spaces\nthat offer multiple views of the same embedded entities.\n  We report the first retrieval results on fine-grained actions for the\nlarge-scale EPIC dataset, in a generalised zero-shot setting. Results show the\nadvantage of our approach for both video-to-text and text-to-video action\nretrieval. We also demonstrate the benefit of disentangling the PoS for the\ngeneric task of cross-modal video retrieval on the MSR-VTT dataset.</p>\n", "tags": ["ICCV", "Few-Shot-&-Zero-Shot", "Scalability", "Multimodal-Retrieval", "Datasets", "Video-Retrieval"], "tsne_embedding": [-13.378005981445312, -34.304073333740234], "cluster": 3}, {"key": "wu2016robust", "year": "2016", "citations": "49", "title": "Robust Hashing For Multi-view Data: Jointly Learning Low-rank Kernelized Similarity Consensus And Hash Functions", "abstract": "<p>Learning hash functions/codes for similarity search over multi-view data is\nattracting increasing attention, where similar hash codes are assigned to the\ndata objects characterizing consistently neighborhood relationship across\nviews. Traditional methods in this category inherently suffer three\nlimitations: 1) they commonly adopt a two-stage scheme where similarity matrix\nis first constructed, followed by a subsequent hash function learning; 2) these\nmethods are commonly developed on the assumption that data samples with\nmultiple representations are noise-free,which is not practical in real-life\napplications; 3) they often incur cumbersome training model caused by the\nneighborhood graph construction using all \\(N\\) points in the database (\\(O(N)\\)).\nIn this paper, we motivate the problem of jointly and efficiently training the\nrobust hash functions over data objects with multi-feature representations\nwhich may be noise corrupted. To achieve both the robustness and training\nefficiency, we propose an approach to effectively and efficiently learning\nlow-rank kernelized \\footnote{We use kernelized similarity rather than kernel,\nas it is not a squared symmetric matrix for data-landmark affinity matrix.}\nhash functions shared across views. Specifically, we utilize landmark graphs to\nconstruct tractable similarity matrices in multi-views to automatically\ndiscover neighborhood structure in the data. To learn robust hash functions, a\nlatent low-rank kernel function is used to construct hash functions in order to\naccommodate linearly inseparable data. In particular, a latent kernelized\nsimilarity matrix is recovered by rank minimization on multiple kernel-based\nsimilarity matrices. Extensive experiments on real-world multi-view datasets\nvalidate the efficacy of our method in the presence of error corruptions.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Efficiency", "Similarity-Search", "Robustness", "Datasets"], "tsne_embedding": [42.83818817138672, 2.1173315048217773], "cluster": 9}, {"key": "wu2017deep", "year": "2017", "citations": "67", "title": "Deep Incremental Hashing Network For Efficient Image Retrieval", "abstract": "<p>Hashing has shown great potential in large-scale image retrieval due to its storage and computation efficiency, especially the recent deep supervised hashing methods. To achieve promising performance, deep supervised hashing methods require a large amount of training data from different classes. However, when images of new categories emerge, existing deep hashing methods have to retrain the CNN model and generate hash codes for all the database images again, which is impractical for large-scale retrieval system.\nIn this paper, we propose a novel deep hashing framework, called Deep Incremental Hashing Network (DIHN), for learning hash codes in an incremental manner. DIHN learns the hash codes for the new coming images directly, while keeping the old ones unchanged. Simultaneously, a deep hash function for query set is learned by preserving the similarities between training points. Extensive experiments on two widely used image retrieval benchmarks demonstrate that the proposed DIHN framework can significantly decrease the training time while keeping the state-of-the-art retrieval accuracy.</p>\n", "tags": ["Evaluation", "Neural-Hashing", "Tools-&-Libraries", "Efficiency", "Image-Retrieval", "Hashing-Methods", "Supervised", "Multimodal-Retrieval", "Scalability"], "tsne_embedding": [-1.977414846420288, -2.329388380050659], "cluster": 1}, {"key": "wu2017sampling", "year": "2017", "citations": "863", "title": "Sampling Matters In Deep Embedding Learning", "abstract": "<p>Deep embeddings answer one simple question: How similar are two images?\nLearning these embeddings is the bedrock of verification, zero-shot learning,\nand visual search. The most prominent approaches optimize a deep convolutional\nnetwork with a suitable loss function, such as contrastive loss or triplet\nloss. While a rich line of work focuses solely on the loss functions, we show\nin this paper that selecting training examples plays an equally important role.\nWe propose distance weighted sampling, which selects more informative and\nstable examples than traditional approaches. In addition, we show that a simple\nmargin based loss is sufficient to outperform all other loss functions. We\nevaluate our approach on the Stanford Online Products, CAR196, and the\nCUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset\nfor face verification. Our method achieves state-of-the-art performance on all\nof them.</p>\n", "tags": ["ICCV", "Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-14.711410522460938, -29.234495162963867], "cluster": 3}, {"key": "wu2017starspace", "year": "2018", "citations": "140", "title": "Starspace: Embed All The Things!", "abstract": "<p>We present StarSpace, a general-purpose neural embedding model that can solve\na wide variety of problems: labeling tasks such as text classification, ranking\ntasks such as information retrieval/web search, collaborative filtering-based\nor content-based recommendation, embedding of multi-relational graphs, and\nlearning word, sentence or document level embeddings. In each case the model\nworks by embedding those entities comprised of discrete features and comparing\nthem against each other \u2013 learning similarities dependent on the task.\nEmpirical results on a number of tasks show that StarSpace is highly\ncompetitive with existing methods, whilst also being generally applicable to\nnew cases where those methods are not.</p>\n", "tags": ["AAAI", "Recommender-Systems"], "tsne_embedding": [7.5197577476501465, -38.10353088378906], "cluster": 3}, {"key": "wu2017structured", "year": "2017", "citations": "71", "title": "Structured Deep Hashing With Convolutional Neural Networks For Fast Person Re-identification", "abstract": "<p>Given a pedestrian image as a query, the purpose of person re-identification\nis to identify the correct match from a large collection of gallery images\ndepicting the same person captured by disjoint camera views. The critical\nchallenge is how to construct a robust yet discriminative feature\nrepresentation to capture the compounded variations in pedestrian appearance.\nTo this end, deep learning methods have been proposed to extract hierarchical\nfeatures against extreme variability of appearance. However, existing methods\nin this category generally neglect the efficiency in the matching stage whereas\nthe searching speed of a re-identification system is crucial in real-world\napplications. In this paper, we present a novel deep hashing framework with\nConvolutional Neural Networks (CNNs) for fast person re-identification.\nTechnically, we simultaneously learn both CNN features and hash functions/codes\nto get robust yet discriminative features and similarity-preserving hash codes.\nThereby, person re-identification can be resolved by efficiently computing and\nranking the Hamming distances between images. A structured loss function\ndefined over positive pairs and hard negatives is proposed to formulate a novel\noptimization problem so that fast convergence and more stable optimized\nsolution can be obtained. Extensive experiments on two benchmarks CUHK03\n\\cite{FPNN} and Market-1501 \\cite{Market1501} show that the proposed deep\narchitecture is efficacy over state-of-the-arts.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Hashing-Methods", "Neural-Hashing"], "tsne_embedding": [-26.748586654663086, 13.74794864654541], "cluster": 0}, {"key": "wu2018cycle", "year": "2018", "citations": "189", "title": "Cycle-consistent Deep Generative Hashing For Cross-modal Retrieval", "abstract": "<p>In this paper, we propose a novel deep generative approach to cross-modal\nretrieval to learn hash functions in the absence of paired training samples\nthrough the cycle consistency loss. Our proposed approach employs adversarial\ntraining scheme to lean a couple of hash functions enabling translation between\nmodalities while assuming the underlying semantic relationship. To induce the\nhash codes with semantics to the input-output pair, cycle consistency loss is\nfurther proposed upon the adversarial training to strengthen the correlations\nbetween inputs and corresponding outputs. Our approach is generative to learn\nhash functions such that the learned hash codes can maximally correlate each\ninput-output correspondence, meanwhile can also regenerate the inputs so as to\nminimize the information loss. The learning to hash embedding is thus performed\nto jointly optimize the parameters of the hash functions across modalities as\nwell as the associated generative models. Extensive experiments on a variety of\nlarge-scale cross-modal data sets demonstrate that our proposed method achieves\nbetter retrieval results than the state-of-the-arts.</p>\n", "tags": ["Multimodal-Retrieval", "Hashing-Methods", "Scalability", "Robustness"], "tsne_embedding": [15.021528244018555, 0.096413753926754], "cluster": 6}, {"key": "wu2018learning", "year": "2018", "citations": "1", "title": "Learning Product Codebooks Using Vector Quantized Autoencoders For Image Retrieval", "abstract": "<p>Vector-Quantized Variational Autoencoders (VQ-VAE)[1] provide an unsupervised\nmodel for learning discrete representations by combining vector quantization\nand autoencoders. In this paper, we study the use of VQ-VAE for representation\nlearning for downstream tasks, such as image retrieval. We first describe the\nVQ-VAE in the context of an information-theoretic framework. We show that the\nregularization term on the learned representation is determined by the size of\nthe embedded codebook before the training and it affects the generalization\nability of the model. As a result, we introduce a hyperparameter to balance the\nstrength of the vector quantizer and the reconstruction error. By tuning the\nhyperparameter, the embedded bottleneck quantizer is used as a regularizer that\nforces the output of the encoder to share a constrained coding space such that\nlearned latent features preserve the similarity relations of the data space. In\naddition, we provide a search range for finding the best hyperparameter.\nFinally, we incorporate the product quantization into the bottleneck stage of\nVQ-VAE and propose an end-to-end unsupervised learning model for the image\nretrieval task. The product quantizer has the advantage of generating\nlarge-size codebooks. Fast retrieval can be achieved by using the lookup tables\nthat store the distance between any pair of sub-codewords. State-of-the-art\nretrieval results are achieved by the learned codebooks.</p>\n", "tags": ["Efficiency", "Quantization", "Image-Retrieval", "Tools-&-Libraries", "Unsupervised"], "tsne_embedding": [-19.713726043701172, -16.584426879882812], "cluster": 1}, {"key": "wu2018local", "year": "2018", "citations": "7", "title": "Local Density Estimation In High Dimensions", "abstract": "<p>An important question that arises in the study of high dimensional vector\nrepresentations learned from data is: given a set \\(\\mathcal{D}\\) of vectors and\na query \\(q\\), estimate the number of points within a specified distance\nthreshold of \\(q\\). We develop two estimators, LSH Count and Multi-Probe Count\nthat use locality sensitive hashing to preprocess the data to accurately and\nefficiently estimate the answers to such questions via importance sampling. A\nkey innovation is the ability to maintain a small number of hash tables via\npreprocessing data structures and algorithms that sample from multiple buckets\nin each hash table. We give bounds on the space requirements and sample\ncomplexity of our schemes, and demonstrate their effectiveness in experiments\non a standard word embedding dataset.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Datasets"], "tsne_embedding": [26.946950912475586, 12.621031761169434], "cluster": 2}, {"key": "wu2018review", "year": "2020", "citations": "31", "title": "A Review For Weighted Minhash Algorithms", "abstract": "<p>Data similarity (or distance) computation is a fundamental research topic\nwhich underpins many high-level applications based on similarity measures in\nmachine learning and data mining. However, in large-scale real-world scenarios,\nthe exact similarity computation has become daunting due to \u201c3V\u201d nature\n(volume, velocity and variety) of big data. In such cases, the hashing\ntechniques have been verified to efficiently conduct similarity estimation in\nterms of both theory and practice. Currently, MinHash is a popular technique\nfor efficiently estimating the Jaccard similarity of binary sets and\nfurthermore, weighted MinHash is generalized to estimate the generalized\nJaccard similarity of weighted sets. This review focuses on categorizing and\ndiscussing the existing works of weighted MinHash algorithms. In this review,\nwe mainly categorize the Weighted MinHash algorithms into quantization-based\napproaches, \u201cactive index\u201d-based ones and others, and show the evolution and\ninherent connection of the weighted MinHash algorithms, from the integer\nweighted MinHash algorithms to real-valued weighted MinHash ones (particularly\nthe Consistent Weighted Sampling scheme). Also, we have developed a python\ntoolbox for the algorithms, and released it in our github. Based on the\ntoolbox, we experimentally conduct a comprehensive comparative study of the\nstandard MinHash algorithm and the weighted MinHash ones.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Locality-Sensitive-Hashing", "Quantization", "Scalability"], "tsne_embedding": [11.989371299743652, 10.484573364257812], "cluster": 6}, {"key": "wu2019saec", "year": "2019", "citations": "4", "title": "Saec: Similarity-aware Embedding Compression In Recommendation Systems", "abstract": "<p>Production recommendation systems rely on embedding methods to represent\nvarious features. An impeding challenge in practice is that the large embedding\nmatrix incurs substantial memory footprint in serving as the number of features\ngrows over time. We propose a similarity-aware embedding matrix compression\nmethod called Saec to address this challenge. Saec clusters similar features\nwithin a field to reduce the embedding matrix size. Saec also adopts a fast\nclustering optimization based on feature frequency to drastically improve\nclustering time. We implement and evaluate Saec on Numerous, the production\ndistributed machine learning system in Tencent, with 10-day worth of feature\ndata from QQ mobile browser. Testbed experiments show that Saec reduces the\nnumber of embedding vectors by two orders of magnitude, compresses the\nembedding size by ~27x, and delivers the same AUC and log loss performance.</p>\n", "tags": ["Recommender-Systems", "Memory-Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [38.793766021728516, -1.9998548030853271], "cluster": 9}, {"key": "wu2019scalable", "year": "2020", "citations": "290", "title": "Scalable Zero-shot Entity Linking With Dense Entity Retrieval", "abstract": "<p>This paper introduces a conceptually simple, scalable, and highly effective\nBERT-based entity linking model, along with an extensive evaluation of its\naccuracy-speed trade-off. We present a two-stage zero-shot linking algorithm,\nwhere each entity is defined only by a short textual description. The first\nstage does retrieval in a dense space defined by a bi-encoder that\nindependently embeds the mention context and the entity descriptions. Each\ncandidate is then re-ranked with a cross-encoder, that concatenates the mention\nand entity text. Experiments demonstrate that this approach is state of the art\non recent zero-shot benchmarks (6 point absolute gains) and also on more\nestablished non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative\nsimplicity (e.g. no explicit entity embeddings or manually engineered mention\ntables). We also show that bi-encoder linking is very fast with nearest\nneighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds),\nand that much of the accuracy gain from the more expensive cross-encoder can be\ntransferred to the bi-encoder via knowledge distillation. Our code and models\nare available at https://github.com/facebookresearch/BLINK.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "EMNLP"], "tsne_embedding": [23.14540672302246, -15.129796981811523], "cluster": 7}, {"key": "wu2019univse", "year": "2019", "citations": "4", "title": "Univse: Robust Visual Semantic Embeddings Via Structured Semantic Representations", "abstract": "<p>We propose Unified Visual-Semantic Embeddings (UniVSE) for learning a joint\nspace of visual and textual concepts. The space unifies the concepts at\ndifferent levels, including objects, attributes, relations, and full scenes. A\ncontrastive learning approach is proposed for the fine-grained alignment from\nonly image-caption pairs. Moreover, we present an effective approach for\nenforcing the coverage of semantic components that appear in the sentence. We\ndemonstrate the robustness of Unified VSE in defending text-domain adversarial\nattacks on cross-modal retrieval tasks. Such robustness also empowers the use\nof visual cues to resolve word dependencies in novel sentences.</p>\n", "tags": ["Self-Supervised", "Multimodal-Retrieval", "Robustness"], "tsne_embedding": [-27.624156951904297, -31.663150787353516], "cluster": 5}, {"key": "wu2020nearest", "year": "2020", "citations": "1", "title": "Nearest Neighbor Search For Hyperbolic Embeddings", "abstract": "<p>Embedding into hyperbolic space is emerging as an effective representation\ntechnique for datasets that exhibit hierarchical structure. This development\nmotivates the need for algorithms that are able to effectively extract\nknowledge and insights from datapoints embedded in negatively curved spaces. We\nfocus on the problem of nearest neighbor search, a fundamental problem in data\nanalysis. We present efficient algorithmic solutions that build upon\nestablished methods for nearest neighbor search in Euclidean space, allowing\nfor easy adoption and integration with existing systems. We prove theoretical\nguarantees for our techniques and our experiments demonstrate the effectiveness\nof our approach on real datasets over competing algorithms.</p>\n", "tags": ["Datasets"], "tsne_embedding": [0.07862157374620438, -18.66002655029297], "cluster": 1}, {"key": "wu2021hashing", "year": "2021", "citations": "30", "title": "Hashing-accelerated Graph Neural Networks For Link Prediction", "abstract": "<p>Networks are ubiquitous in the real world. Link prediction, as one of the key\nproblems for network-structured data, aims to predict whether there exists a\nlink between two nodes. The traditional approaches are based on the explicit\nsimilarity computation between the compact node representation by embedding\neach node into a low-dimensional space. In order to efficiently handle the\nintensive similarity computation in link prediction, the hashing technique has\nbeen successfully used to produce the node representation in the Hamming space.\nHowever, the hashing-based link prediction algorithms face accuracy loss from\nthe randomized hashing techniques or inefficiency from the learning to hash\ntechniques in the embedding process. Currently, the Graph Neural Network (GNN)\nframework has been widely applied to the graph-related tasks in an end-to-end\nmanner, but it commonly requires substantial computational resources and memory\ncosts due to massive parameter learning, which makes the GNN-based algorithms\nimpractical without the help of a powerful workhorse. In this paper, we propose\na simple and effective model called #GNN, which balances the trade-off between\naccuracy and efficiency. #GNN is able to efficiently acquire node\nrepresentation in the Hamming space for link prediction by exploiting the\nrandomized hashing technique to implement message passing and capture\nhigh-order proximity in the GNN framework. Furthermore, we characterize the\ndiscriminative power of #GNN in probability. The extensive experimental results\ndemonstrate that the proposed #GNN algorithm achieves accuracy comparable to\nthe learning-based algorithms and outperforms the randomized algorithm, while\nrunning significantly faster than the learning-based algorithms. Also, the\nproposed algorithm shows excellent scalability on a large-scale network with\nthe limited resources.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Hashing-Methods", "Scalability"], "tsne_embedding": [53.353023529052734, 1.943480372428894], "cluster": 9}, {"key": "wu2021online", "year": "2022", "citations": "5", "title": "Online Enhanced Semantic Hashing: Towards Effective And Efficient Retrieval For Streaming Multi-modal Data", "abstract": "<p>With the vigorous development of multimedia equipment and applications,\nefficient retrieval of large-scale multi-modal data has become a trendy\nresearch topic. Thereinto, hashing has become a prevalent choice due to its\nretrieval efficiency and low storage cost. Although multi-modal hashing has\ndrawn lots of attention in recent years, there still remain some problems. The\nfirst point is that existing methods are mainly designed in batch mode and not\nable to efficiently handle streaming multi-modal data. The second point is that\nall existing online multi-modal hashing methods fail to effectively handle\nunseen new classes which come continuously with streaming data chunks. In this\npaper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).\nWe design novel semantic-enhanced representation for data, which could help\nhandle the new coming classes, and thereby construct the enhanced semantic\nobjective function. An efficient and effective discrete online optimization\nalgorithm is further proposed for OASIS. Extensive experiments show that our\nmethod can exceed the state-of-the-art models. For good reproducibility and\nbenefiting the community, our code and data are already available in\nsupplementary material and will be made publicly available.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Efficiency", "Scalability", "Similarity-Search", "AAAI", "Memory-Efficiency"], "tsne_embedding": [15.491499900817871, -10.721968650817871], "cluster": 7}, {"key": "wu2022hierarchical", "year": "2022", "citations": "0", "title": "Hierarchical Locality Sensitive Hashing For Structured Data: A Survey", "abstract": "<p>Data similarity (or distance) computation is a fundamental research topic\nwhich fosters a variety of similarity-based machine learning and data mining\napplications. In big data analytics, it is impractical to compute the exact\nsimilarity of data instances due to high computational cost. To this end, the\nLocality Sensitive Hashing (LSH) technique has been proposed to provide\naccurate estimators for various similarity measures between sets or vectors in\nan efficient manner without the learning process. Structured data (e.g.,\nsequences, trees and graphs), which are composed of elements and relations\nbetween the elements, are commonly seen in the real world, but the traditional\nLSH algorithms cannot preserve the structure information represented as\nrelations between elements. In order to conquer the issue, researchers have\nbeen devoted to the family of the hierarchical LSH algorithms. In this paper,\nwe explore the present progress of the research into hierarchical LSH from the\nfollowing perspectives: 1) Data structures, where we review various\nhierarchical LSH algorithms for three typical data structures and uncover their\ninherent connections; 2) Applications, where we review the hierarchical LSH\nalgorithms in multiple application scenarios; 3) Challenges, where we discuss\nsome potential challenges as future directions.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [30.07301139831543, -13.323302268981934], "cluster": 7}, {"key": "wu2022hqann", "year": "2022", "citations": "8", "title": "HQANN: Efficient And Robust Similarity Search For Hybrid Queries With Structured And Unstructured Constraints", "abstract": "<p>The in-memory approximate nearest neighbor search (ANNS) algorithms have\nachieved great success for fast high-recall query processing, but are extremely\ninefficient when handling hybrid queries with unstructured (i.e., feature\nvectors) and structured (i.e., related attributes) constraints. In this paper,\nwe present HQANN, a simple yet highly efficient hybrid query processing\nframework which can be easily embedded into existing proximity graph-based ANNS\nalgorithms. We guarantee both low latency and high recall by leveraging\nnavigation sense among attributes and fusing vector similarity search with\nattribute filtering. Experimental results on both public and in-house datasets\ndemonstrate that HQANN is 10x faster than the state-of-the-art hybrid ANNS\nsolutions to reach the same recall quality and its performance is hardly\naffected by the complexity of attributes. It can reach 99% recall@10 in just\naround 50 microseconds On GLOVE-1.2M with thousands of attribute constraints.</p>\n", "tags": ["Datasets", "Evaluation", "Graph-Based-Ann", "Tools-&-Libraries", "CIKM", "Efficiency", "Hybrid-Ann-Methods", "Similarity-Search"], "tsne_embedding": [31.27762794494629, 29.108383178710938], "cluster": 2}, {"key": "wu2022learning", "year": "2022", "citations": "6", "title": "Learning Deep Semantic Model For Code Search Using Codesearchnet Corpus", "abstract": "<p>Semantic code search is the task of retrieving relevant code snippet given a\nnatural language query. Different from typical information retrieval tasks,\ncode search requires to bridge the semantic gap between the programming\nlanguage and natural language, for better describing intrinsic concepts and\nsemantics. Recently, deep neural network for code search has been a hot\nresearch topic. Typical methods for neural code search first represent the code\nsnippet and query text as separate embeddings, and then use vector distance\n(e.g. dot-product or cosine) to calculate the semantic similarity between them.\nThere exist many different ways for aggregating the variable length of code or\nquery tokens into a learnable embedding, including bi-encoder, cross-encoder,\nand poly-encoder. The goal of the query encoder and code encoder is to produce\nembeddings that are close with each other for a related pair of query and the\ncorresponding desired code snippet, in which the choice and design of encoder\nis very significant.\n  In this paper, we propose a novel deep semantic model which makes use of the\nutilities of not only the multi-modal sources, but also feature extractors such\nas self-attention, the aggregated vectors, combination of the intermediate\nrepresentations. We apply the proposed model to tackle the CodeSearchNet\nchallenge about semantic code search. We align cross-lingual embedding for\nmulti-modality learning with large batches and hard example mining, and combine\ndifferent learned representations for better enhancing the representation\nlearning. Our model is trained on CodeSearchNet corpus and evaluated on the\nheld-out data, the final model achieves 0.384 NDCG and won the first place in\nthis benchmark. Models and code are available at\nhttps://github.com/overwindows/SemanticCodeSearch.git.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [19.50546646118164, -17.462017059326172], "cluster": 7}, {"key": "wu2022retrievalguard", "year": "2022", "citations": "3", "title": "Retrievalguard: Provably Robust 1-nearest Neighbor Image Retrieval", "abstract": "<p>Recent research works have shown that image retrieval models are vulnerable\nto adversarial attacks, where slightly modified test inputs could lead to\nproblematic retrieval results. In this paper, we aim to design a provably\nrobust image retrieval model which keeps the most important evaluation metric\nRecall@1 invariant to adversarial perturbation. We propose the first 1-nearest\nneighbor (NN) image retrieval algorithm, RetrievalGuard, which is provably\nrobust against adversarial perturbations within an \\(\u2113\u2082\\) ball of calculable\nradius. The challenge is to design a provably robust algorithm that takes into\nconsideration the 1-NN search and the high-dimensional nature of the embedding\nspace. Algorithmically, given a base retrieval model and a query sample, we\nbuild a smoothed retrieval model by carefully analyzing the 1-NN search\nprocedure in the high-dimensional embedding space. We show that the smoothed\nretrieval model has bounded Lipschitz constant and thus the retrieval score is\ninvariant to \\(\u2113\u2082\\) adversarial perturbations. Experiments on image retrieval\ntasks validate the robustness of our RetrievalGuard method.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Robustness"], "tsne_embedding": [-18.062780380249023, 14.026314735412598], "cluster": 8}, {"key": "wu2022self", "year": "2022", "citations": "2", "title": "Self-supervised Consistent Quantization For Fully Unsupervised Image Retrieval", "abstract": "<p>Unsupervised image retrieval aims to learn an efficient retrieval system\nwithout expensive data annotations, but most existing methods rely heavily on\nhandcrafted feature descriptors or pre-trained feature extractors. To minimize\nhuman supervision, recent advance proposes deep fully unsupervised image\nretrieval aiming at training a deep model from scratch to jointly optimize\nvisual features and quantization codes. However, existing approach mainly\nfocuses on instance contrastive learning without considering underlying\nsemantic structure information, resulting in sub-optimal performance. In this\nwork, we propose a novel self-supervised consistent quantization approach to\ndeep fully unsupervised image retrieval, which consists of part consistent\nquantization and global consistent quantization. In part consistent\nquantization, we devise part neighbor semantic consistency learning with\ncodeword diversity regularization. This allows to discover underlying neighbor\nstructure information of sub-quantized representations as self-supervision. In\nglobal consistent quantization, we employ contrastive learning for both\nembedding and quantized representations and fuses these representations for\nconsistent contrastive regularization between instances. This can make up for\nthe loss of useful representation information during quantization and\nregularize consistency between instances. With a unified learning objective of\npart and global consistent quantization, our approach exploits richer\nself-supervision cues to facilitate model learning. Extensive experiments on\nthree benchmark datasets show the superiority of our approach over the\nstate-of-the-art methods.</p>\n", "tags": ["Self-Supervised", "Quantization", "Similarity-Search", "Image-Retrieval", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-33.38030242919922, -2.28236722946167], "cluster": 0}, {"key": "wu2023forb", "year": "2023", "citations": "1", "title": "FORB: A Flat Object Retrieval Benchmark For Universal Image Embedding", "abstract": "<p>Image retrieval is a fundamental task in computer vision. Despite recent\nadvances in this field, many techniques have been evaluated on a limited number\nof domains, with a small number of instance categories. Notably, most existing\nworks only consider domains like 3D landmarks, making it difficult to\ngeneralize the conclusions made by these works to other domains, e.g., logo and\nother 2D flat objects. To bridge this gap, we introduce a new dataset for\nbenchmarking visual search methods on flat images with diverse patterns. Our\nflat object retrieval benchmark (FORB) supplements the commonly adopted 3D\nobject domain, and more importantly, it serves as a testbed for assessing the\nimage embedding quality on out-of-distribution domains. In this benchmark we\ninvestigate the retrieval accuracy of representative methods in terms of\ncandidate ranks, as well as matching score margin, a viewpoint which is largely\nignored by many works. Our experiments not only highlight the challenges and\nrich heterogeneity of FORB, but also reveal the hidden properties of different\nretrieval strategies. The proposed benchmark is a growing project and we expect\nto expand in both quantity and variety of objects. The dataset and supporting\ncodes are available at https://github.com/pxiangwu/FORB/.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-37.59840774536133, -11.31839656829834], "cluster": 5}, {"key": "wu2023mofi", "year": "2023", "citations": "3", "title": "MOFI: Learning Image Representations From Noisy Entity Annotated Images", "abstract": "<p>We present MOFI, Manifold OF Images, a new vision foundation model designed\nto learn image representations from noisy entity annotated images. MOFI differs\nfrom previous work in two key aspects: (i) pre-training data, and (ii) training\nrecipe. Regarding data, we introduce a new approach to automatically assign\nentity labels to images from noisy image-text pairs. Our approach involves\nemploying a named entity recognition model to extract entities from the\nalt-text, and then using a CLIP model to select the correct entities as labels\nof the paired image. It\u2019s a simple, cost-effective method that can scale to\nhandle billions of web-mined image-text pairs. Through this method, we have\ncreated Image-to-Entities (I2E), a new dataset with 1 billion images and 2\nmillion distinct entities, covering rich visual concepts in the wild. Building\nupon the I2E dataset, we study different training recipes like supervised\npre-training, contrastive pre-training, and multi-task learning. For\ncontrastive pre-training, we treat entity names as free-form text, and further\nenrich them with entity descriptions. Experiments show that supervised\npre-training with large-scale fine-grained entity labels is highly effective\nfor image retrieval tasks, and multi-task training further improves the\nperformance. The final MOFI model achieves 86.66% mAP on the challenging\nGPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19%\nfrom OpenAI\u2019s CLIP model. Further experiments on zero-shot and linear probe\nimage classification also show that MOFI outperforms a CLIP model trained on\nthe original image-text data, demonstrating the effectiveness of the I2E\ndataset in learning strong image representations. We release our code and model\nweights at https://github.com/apple/ml-mofi.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-19.241863250732422, -24.431156158447266], "cluster": 5}, {"key": "wu2023pix2map", "year": "2023", "citations": "3", "title": "Pix2map: Cross-modal Retrieval For Inferring Street Maps From Images", "abstract": "<p>Self-driving vehicles rely on urban street maps for autonomous navigation. In\nthis paper, we introduce Pix2Map, a method for inferring urban street map\ntopology directly from ego-view images, as needed to continually update and\nexpand existing maps. This is a challenging task, as we need to infer a complex\nurban road topology directly from raw image data. The main insight of this\npaper is that this problem can be posed as cross-modal retrieval by learning a\njoint, cross-modal embedding space for images and existing maps, represented as\ndiscrete graphs that encode the topological layout of the visual surroundings.\nWe conduct our experimental evaluation using the Argoverse dataset and show\nthat it is indeed possible to accurately retrieve street maps corresponding to\nboth seen and unseen roads solely from image data. Moreover, we show that our\nretrieved maps can be used to update or expand existing maps and even show\nproof-of-concept results for visual localization and image retrieval from\nspatial graphs.</p>\n", "tags": ["CVPR", "Image-Retrieval", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-7.124598503112793, -53.70752716064453], "cluster": 3}, {"key": "wu2024com3d", "year": "2024", "citations": "0", "title": "COM3D: Leveraging Cross-view Correspondence And Cross-modal Mining For 3D Retrieval", "abstract": "<p>In this paper, we investigate an open research task of cross-modal retrieval\nbetween 3D shapes and textual descriptions. Previous approaches mainly rely on\npoint cloud encoders for feature extraction, which may ignore key inherent\nfeatures of 3D shapes, including depth, spatial hierarchy, geometric\ncontinuity, etc. To address this issue, we propose COM3D, making the first\nattempt to exploit the cross-view correspondence and cross-modal mining to\nenhance the retrieval performance. Notably, we augment the 3D features through\na scene representation transformer, to generate cross-view correspondence\nfeatures of 3D shapes, which enrich the inherent features and enhance their\ncompatibility with text matching. Furthermore, we propose to optimize the\ncross-modal matching process based on the semi-hard negative example mining\nmethod, in an attempt to improve the learning efficiency. Extensive\nquantitative and qualitative experiments demonstrate the superiority of our\nproposed COM3D, achieving state-of-the-art results on the Text2Shape dataset.</p>\n", "tags": ["Efficiency", "Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [-38.17926025390625, -15.334528923034668], "cluster": 5}, {"key": "wu2024graphhash", "year": "2025", "citations": "0", "title": "Graphhash: Graph Clustering Enables Parameter Efficiency In Recommender Systems", "abstract": "<p>Deep recommender systems rely heavily on large embedding tables to handle\nhigh-cardinality categorical features such as user/item identifiers, and face\nsignificant memory constraints at scale. To tackle this challenge, hashing\ntechniques are often employed to map multiple entities to the same embedding\nand thus reduce the size of the embedding tables. Concurrently, graph-based\ncollaborative signals have emerged as powerful tools in recommender systems,\nyet their potential for optimizing embedding table reduction remains\nunexplored. This paper introduces GraphHash, the first graph-based approach\nthat leverages modularity-based bipartite graph clustering on user-item\ninteraction graphs to reduce embedding table sizes. We demonstrate that the\nmodularity objective has a theoretical connection to message-passing, which\nprovides a foundation for our method. By employing fast clustering algorithms,\nGraphHash serves as a computationally efficient proxy for message-passing\nduring preprocessing and a plug-and-play graph-based alternative to traditional\nID hashing. Extensive experiments show that GraphHash substantially outperforms\ndiverse hashing baselines on both retrieval and click-through-rate prediction\ntasks. In particular, GraphHash achieves on average a 101.52% improvement in\nrecall when reducing the embedding table size by more than 75%, highlighting\nthe value of graph-based collaborative information for model reduction. Our\ncode is available at https://github.com/snap-research/GraphHash.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Efficiency", "Recommender-Systems", "Evaluation"], "tsne_embedding": [53.767242431640625, -0.29621422290802], "cluster": 9}, {"key": "wu2024interpretable", "year": "2020", "citations": "30", "title": "Interpretable Embedding For Ad-hoc Video Search", "abstract": "<p>Answering query with semantic concepts has long been the mainstream approach\nfor video search. Until recently, its performance is surpassed by concept-free\napproach, which embeds queries in a joint space as videos. Nevertheless, the\nembedded features as well as search results are not interpretable, hindering\nsubsequent steps in video browsing and query reformulation. This paper\nintegrates feature embedding and concept interpretation into a neural network\nfor unified dual-task learning. In this way, an embedding is associated with a\nlist of semantic concepts as an interpretation of video content. This paper\nempirically demonstrates that, by using either the embedding features or\nconcepts, considerable search improvement is attainable on TRECVid benchmarked\ndatasets. Concepts are not only effective in pruning false positive videos, but\nalso highly complementary to concept-free search, leading to large margin of\nimprovement compared to state-of-the-art approaches.</p>\n", "tags": ["Evaluation", "Video-Retrieval", "Datasets"], "tsne_embedding": [-11.327713012695312, -3.806755304336548], "cluster": 1}, {"key": "wu2024plms", "year": "2024", "citations": "0", "title": "A Plms Based Protein Retrieval Framework", "abstract": "<p>Protein retrieval, which targets the deconstruction of the relationship\nbetween sequences, structures and functions, empowers the advancing of biology.\nBasic Local Alignment Search Tool (BLAST), a sequence-similarity-based\nalgorithm, has proved the efficiency of this field. Despite the existing tools\nfor protein retrieval, they prioritize sequence similarity and probably\noverlook proteins that are dissimilar but share homology or functionality. In\norder to tackle this problem, we propose a novel protein retrieval framework\nthat mitigates the bias towards sequence similarity. Our framework initiatively\nharnesses protein language models (PLMs) to embed protein sequences within a\nhigh-dimensional feature space, thereby enhancing the representation capacity\nfor subsequent analysis. Subsequently, an accelerated indexed vector database\nis constructed to facilitate expedited access and retrieval of dense vectors.\nExtensive experiments demonstrate that our framework can equally retrieve both\nsimilar and dissimilar proteins. Moreover, this approach enables the\nidentification of proteins that conventional methods fail to uncover. This\nframework will effectively assist in protein mining and empower the development\nof biology.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries"], "tsne_embedding": [0.733595073223114, 54.57343292236328], "cluster": 4}, {"key": "wu2024sign", "year": "2024", "citations": "0", "title": "Sign-guided Bipartite Graph Hashing For Hamming Space Search", "abstract": "<p>Bipartite graph hashing (BGH) is extensively used for Top-K search in Hamming\nspace at low storage and inference costs. Recent research adopts graph\nconvolutional hashing for BGH and has achieved the state-of-the-art\nperformance. However, the contributions of its various influencing factors to\nhashing performance have not been explored in-depth, including the\nsame/different sign count between two binary embeddings during Hamming space\nsearch (sign property), the contribution of sub-embeddings at each layer (model\nproperty), the contribution of different node types in the bipartite graph\n(node property), and the combination of augmentation methods. In this work, we\nbuild a lightweight graph convolutional hashing model named LightGCH by mainly\nremoving the augmentation methods of the state-of-the-art model BGCH. By\nanalyzing the contributions of each layer and node type to performance, as well\nas analyzing the Hamming similarity statistics at each layer, we find that the\nactual neighbors in the bipartite graph tend to have low Hamming similarity at\nthe shallow layer, and all nodes tend to have high Hamming similarity at the\ndeep layers in LightGCH. To tackle these problems, we propose a novel\nsign-guided framework SGBGH to make improvement, which uses sign-guided\nnegative sampling to improve the Hamming similarity of neighbors, and uses\nsign-aware contrastive learning to help nodes learn more uniform\nrepresentations. Experimental results show that SGBGH outperforms BGCH and\nLightGCH significantly in embedding quality.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Similarity-Search", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [49.61543655395508, -2.178480386734009], "cluster": 9}, {"key": "wu2024sketchql", "year": "2024", "citations": "1", "title": "Sketchql Demonstration: Zero-shot Video Moment Querying With Sketches", "abstract": "<p>In this paper, we will present SketchQL, a video database management system\n(VDBMS) for retrieving video moments with a sketch-based query interface. This\nnovel interface allows users to specify object trajectory events with simple\nmouse drag-and-drop operations. Users can use trajectories of single objects as\nbuilding blocks to compose complex events. Using a pre-trained model that\nencodes trajectory similarity, SketchQL achieves zero-shot video moments\nretrieval by performing similarity searches over the video to identify clips\nthat are the most similar to the visual query. In this demonstration, we\nintroduce the graphic user interface of SketchQL and detail its functionalities\nand interaction mechanisms. We also demonstrate the end-to-end usage of\nSketchQL from query composition to video moments retrieval using real-world\nscenarios.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Similarity-Search"], "tsne_embedding": [-43.89335250854492, -25.500579833984375], "cluster": 5}, {"key": "wu2025deep", "year": "2017", "citations": "67", "title": "Deep Supervised Hashing For Multi-label And Large-scale Image Retrieval", "abstract": "<p>One of the most challenging tasks in large-scale multi-label image retrieval is to map images into binary codes while preserving multilevel semantic similarity. Recently, several deep supervised hashing methods have been proposed to learn hash functions that preserve multilevel semantic similarity with deep convolutional neural networks. However, these triplet label based methods try to preserve the ranking order of images according to their similarity degrees to the queries while not putting direct constraints on the distance between the codes of very similar images. Besides, the current evaluation criteria are not able to measure the performance of existing hashing methods on preserving fine-grained multilevel semantic similarity. To tackle these issues, we propose a novel Deep Multilevel Semantic Similarity Preserving Hashing (DMSSPH) method to learn compact similarity-preserving binary codes for the huge body of multi-label image data with deep convolutional neural networks. In our approach, we make the best of the supervised information in the form of pairwise labels to maximize the discriminability of output binary codes. Extensive evaluations conducted on several benchmark datasets demonstrate that the proposed method significantly outperforms the state-of-the-art supervised and unsupervised hashing methods at the accuracies of top returned images, especially for shorter binary codes. Meanwhile, the proposed method shows better performance on preserving fine-grained multilevel semantic similarity according to the results under the Jaccard coefficient based evaluation criteria we propose.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Compact-Codes", "Image-Retrieval", "Hashing-Methods", "Supervised", "Unsupervised", "Multimodal-Retrieval", "Scalability"], "tsne_embedding": [-6.685640335083008, 6.9237141609191895], "cluster": 8}, {"key": "wurzer2016randomised", "year": "2016", "citations": "0", "title": "Randomised Relevance Model", "abstract": "<p>Relevance Models are well-known retrieval models and capable of producing\ncompetitive results. However, because they use query expansion they can be very\nslow. We address this slowness by incorporating two variants of locality\nsensitive hashing (LSH) into the query expansion process. Results on two\ndocument collections suggest that we can obtain large reductions in the amount\nof work, with a small reduction in effectiveness. Our approach is shown to be\nadditive when pruning query terms.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [28.8790225982666, 23.66558837890625], "cluster": 2}, {"key": "wygocki2017fast", "year": "2017", "citations": "2", "title": "On Fast Bounded Locality Sensitive Hashing", "abstract": "<p>In this paper, we examine the hash functions expressed as scalar products,\ni.e., \\(f(x)=&lt;v,x&gt;\\), for some bounded random vector \\(v\\). Such hash functions\nhave numerous applications, but often there is a need to optimize the choice of\nthe distribution of \\(v\\). In the present work, we focus on so-called\nanti-concentration bounds, i.e. the upper bounds of \\(\\mathbb{P}\\left[|&lt;v,x&gt;| &lt;\n\\alpha \\right]\\). In many applications, \\(v\\) is a vector of independent random\nvariables with standard normal distribution. In such case, the distribution of\n\\(&lt;v,x&gt;\\) is also normal and it is easy to approximate \\(\\mathbb{P}\\left[|&lt;v,x&gt;| &lt;\n\\alpha \\right]\\). Here, we consider two bounded distributions in the context of\nthe anti-concentration bounds. Particularly, we analyze \\(v\\) being a random\nvector from the unit ball in \\(l_{\\infty}\\) and \\(v\\) being a random vector from\nthe unit sphere in \\(l_{2}\\). We show optimal up to a constant anti-concentration\nmeasures for functions \\(f(x)=&lt;v,x&gt;\\).\n  As a consequence of our research, we obtain new best results for \\newline\n\\textit{\\(c\\)-approximate nearest neighbors without false negatives} for \\(l_p\\) in\nhigh dimensional space for all \\(p\\in[1,\\infty]\\), for\n\\(c=\u03a9(\\max\\{\\sqrt{d},d^{1/p}\\})\\). These results improve over those\npresented in [16]. Finally, our paper reports progress on answering the open\nproblem by Pagh~[17], who considered the nearest neighbor search without false\nnegatives for the Hamming distance.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing"], "tsne_embedding": [21.042604446411133, 49.280704498291016], "cluster": 4}, {"key": "xia2016unsupervised", "year": "2016", "citations": "17", "title": "Unsupervised Deep Hashing For Large-scale Visual Search", "abstract": "<p>Learning based hashing plays a pivotal role in large-scale visual search.\nHowever, most existing hashing algorithms tend to learn shallow models that do\nnot seek representative binary codes. In this paper, we propose a novel hashing\napproach based on unsupervised deep learning to hierarchically transform\nfeatures into hash codes. Within the heterogeneous deep hashing framework, the\nautoencoder layers with specific constraints are considered to model the\nnonlinear mapping between features and binary codes. Then, a Restricted\nBoltzmann Machine (RBM) layer with constraints is utilized to reduce the\ndimension in the hamming space. Extensive experiments on the problem of visual\nsearch demonstrate the competitiveness of our proposed approach compared to\nstate-of-the-art.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Compact-Codes", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-1.0776458978652954, -8.439685821533203], "cluster": 1}, {"key": "xia2021searchgcn", "year": "2021", "citations": "3", "title": "Searchgcn: Powering Embedding Retrieval By Graph Convolution Networks For E-commerce Search", "abstract": "<p>Graph convolution networks (GCN), which recently becomes new state-of-the-art\nmethod for graph node classification, recommendation and other applications,\nhas not been successfully applied to industrial-scale search engine yet. In\nthis proposal, we introduce our approach, namely SearchGCN, for embedding-based\ncandidate retrieval in one of the largest e-commerce search engine in the\nworld. Empirical studies demonstrate that SearchGCN learns better embedding\nrepresentations than existing methods, especially for long tail queries and\nitems. Thus, SearchGCN has been deployed into JD.com\u2019s search production since\nJuly 2020.</p>\n", "tags": ["SIGIR", "Recommender-Systems"], "tsne_embedding": [47.38961410522461, 13.222871780395508], "cluster": 9}, {"key": "xian2024bert", "year": "2024", "citations": "0", "title": "Bert-enhanced Retrieval Tool For Homework Plagiarism Detection System", "abstract": "<p>Text plagiarism detection task is a common natural language processing task\nthat aims to detect whether a given text contains plagiarism or copying from\nother texts. In existing research, detection of high level plagiarism is still\na challenge due to the lack of high quality datasets. In this paper, we propose\na plagiarized text data generation method based on GPT-3.5, which produces\n32,927 pairs of text plagiarism detection datasets covering a wide range of\nplagiarism methods, bridging the gap in this part of research. Meanwhile, we\npropose a plagiarism identification method based on Faiss with BERT with high\nefficiency and high accuracy. Our experiments show that the performance of this\nmodel outperforms other models in several metrics, including 98.86%, 98.90%,\n98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively.\nAt the end, we also provide a user-friendly demo platform that allows users to\nupload a text library and intuitively participate in the plagiarism analysis.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Evaluation", "Datasets"], "tsne_embedding": [16.167387008666992, 22.256990432739258], "cluster": 2}, {"key": "xiao2020deeply", "year": "2022", "citations": "5", "title": "Deeply Activated Salient Region For Instance Search", "abstract": "<p>The performance of instance search depends heavily on the ability to locate\nand describe a wide variety of object instances in a video/image collection.\nDue to the lack of proper mechanism in locating instances and deriving feature\nrepresentation, instance search is generally only effective for retrieving\ninstances of known object categories. In this paper, a simple but effective\ninstance-level feature representation is presented. Different from other\napproaches, the issues in class-agnostic instance localization and distinctive\nfeature representation are considered. The former is achieved by detecting\nsalient instance regions from an image by a layer-wise back-propagation\nprocess. The back-propagation starts from the last convolution layer of a\npre-trained CNN that is originally used for classification. The\nback-propagation proceeds layer-by-layer until it reaches the input layer. This\nallows the salient instance regions in the input image from both known and\nunknown categories to be activated. Each activated salient region covers the\nfull or more usually a major range of an instance. The distinctive feature\nrepresentation is produced by average-pooling on the feature map of certain\nlayer with the detected instance region. Experiments show that such kind of\nfeature representation demonstrates considerably better performance over most\nof the existing approaches. In addition, we show that the proposed feature\ndescriptor is also suitable for content-based image search.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-47.03605651855469, -2.1199111938476562], "cluster": 0}, {"key": "xiao2021neural", "year": "2021", "citations": "7", "title": "Neural Pathsim For Inductive Similarity Search In Heterogeneous Information Networks", "abstract": "<p>PathSim is a widely used meta-path-based similarity in heterogeneous\ninformation networks. Numerous applications rely on the computation of PathSim,\nincluding similarity search and clustering. Computing PathSim scores on large\ngraphs is computationally challenging due to its high time and storage\ncomplexity. In this paper, we propose to transform the problem of approximating\nthe ground truth PathSim scores into a learning problem. We design an\nencoder-decoder based framework, NeuPath, where the algorithmic structure of\nPathSim is considered. Specifically, the encoder module identifies Top T\noptimized path instances, which can approximate the ground truth PathSim, and\nmaps each path instance to an embedding vector. The decoder transforms each\nembedding vector into a scalar respectively, which identifies the similarity\nscore. We perform extensive experiments on two real-world datasets in different\ndomains, ACM and IMDB. Our results demonstrate that NeuPath performs better\nthan state-of-the-art baselines in the PathSim approximation task and\nsimilarity search task.</p>\n", "tags": ["Similarity-Search", "Tools-&-Libraries", "Datasets", "CIKM"], "tsne_embedding": [49.10838317871094, 4.433562755584717], "cluster": 9}, {"key": "xiao2022attribute", "year": "2024", "citations": "2", "title": "Attribute-guided Multi-level Attention Network For Fine-grained Fashion Retrieval", "abstract": "<p>Fine-grained fashion retrieval searches for items that share a similar\nattribute with the query image. Most existing methods use a pre-trained feature\nextractor (e.g., ResNet 50) to capture image representations. However, a\npre-trained feature backbone is typically trained for image classification and\nobject detection, which are fundamentally different tasks from fine-grained\nfashion retrieval. Therefore, existing methods suffer from a feature gap\nproblem when directly using the pre-trained backbone for fine-tuning. To solve\nthis problem, we introduce an attribute-guided multi-level attention network\n(AG-MAN). Specifically, we first enhance the pre-trained feature extractor to\ncapture multi-level image embedding, thereby enriching the low-level features\nwithin these representations. Then, we propose a classification scheme where\nimages with the same attribute, albeit with different values, are categorized\ninto the same class. This can further alleviate the feature gap problem by\nperturbing object-centric feature learning. Moreover, we propose an improved\nattribute-guided attention module for extracting more accurate\nattribute-specific representations. Our model consistently outperforms existing\nattention based methods when assessed on the FashionAI (62.8788% in MAP),\nDeepFashion (8.9804% in MAP), and Zappos50k datasets (93.32% in Prediction\naccuracy). Especially, ours improves the most typical ASENet_V2 model by 2.12%,\n0.31%, and 0.78% points in FashionAI, DeepFashion, and Zappos50k datasets,\nrespectively. The source code is available in\nhttps://github.com/Dr-LingXiao/AG-MAN.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-28.22938346862793, -23.34393882751465], "cluster": 5}, {"key": "xiao2022deeply", "year": "2022", "citations": "5", "title": "Deeply Activated Salient Region For Instance Search", "abstract": "<p>The performance of instance search depends heavily on the ability to locate\nand describe a wide variety of object instances in a video/image collection.\nDue to the lack of proper mechanism in locating instances and deriving feature\nrepresentation, instance search is generally only effective for retrieving\ninstances of known object categories. In this paper, a simple but effective\ninstance-level feature representation is presented. Different from other\napproaches, the issues in class-agnostic instance localization and distinctive\nfeature representation are considered. The former is achieved by detecting\nsalient instance regions from an image by a layer-wise back-propagation\nprocess. The back-propagation starts from the last convolution layer of a\npre-trained CNN that is originally used for classification. The\nback-propagation proceeds layer-by-layer until it reaches the input layer. This\nallows the salient instance regions in the input image from both known and\nunknown categories to be activated. Each activated salient region covers the\nfull or more usually a major range of an instance. The distinctive feature\nrepresentation is produced by average-pooling on the feature map of certain\nlayer with the detected instance region. Experiments show that such kind of\nfeature representation demonstrates considerably better performance over most\nof the existing approaches. In addition, we show that the proposed feature\ndescriptor is also suitable for content-based image search.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-47.036128997802734, -2.1199111938476562], "cluster": 0}, {"key": "xiao2022progressively", "year": "2022", "citations": "14", "title": "Progressively Optimized Bi-granular Document Representation For Scalable Embedding Based Retrieval", "abstract": "<p>Ad-hoc search calls for the selection of appropriate answers from a\nmassive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a\npromising solution, where deep learning based document representation and ANN\nsearch techniques are allied to handle this task. However, a major challenge is\nthat the ANN index can be too large to fit into memory, given the considerable\nsize of answer corpus. In this work, we tackle this problem with Bi-Granular\nDocument Representation, where the lightweight sparse embeddings are indexed\nand standby in memory for coarse-grained candidate search, and the heavyweight\ndense embeddings are hosted in disk for fine-grained post verification. For the\nbest of retrieval accuracy, a Progressive Optimization framework is designed.\nThe sparse embeddings are learned ahead for high-quality search of candidates.\nConditioned on the candidate distribution induced by the sparse embeddings, the\ndense embeddings are continuously learned to optimize the discrimination of\nground-truth from the shortlisted candidates. Besides, two techniques: the\ncontrastive quantization and the locality-centric sampling are introduced for\nthe learning of sparse and dense embeddings, which substantially contribute to\ntheir performances. Thanks to the above features, our method effectively\nhandles massive-scale EBR with strong advantages in accuracy: with up to +4.3%\nrecall gain on million-scale corpus, and up to +17.5% recall gain on\nbillion-scale corpus. Besides, Our method is applied to a major sponsored\nsearch platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and\nCTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.</p>\n", "tags": ["Quantization", "Vector-Indexing", "Scalability", "Tools-&-Libraries", "Large-Scale-Search", "Evaluation"], "tsne_embedding": [22.980934143066406, 14.383058547973633], "cluster": 2}, {"key": "xiao2023unsupervised", "year": "2023", "citations": "0", "title": "Unsupervised Multi-criteria Adversarial Detection In Deep Image Retrieval", "abstract": "<p>The vulnerability in the algorithm supply chain of deep learning has imposed\nnew challenges to image retrieval systems in the downstream. Among a variety of\ntechniques, deep hashing is gaining popularity. As it inherits the algorithmic\nbackend from deep learning, a handful of attacks are recently proposed to\ndisrupt normal image retrieval. Unfortunately, the defense strategies in\nsoftmax classification are not readily available to be applied in the image\nretrieval domain. In this paper, we propose an efficient and unsupervised\nscheme to identify unique adversarial behaviors in the hamming space. In\nparticular, we design three criteria from the perspectives of hamming distance,\nquantization loss and denoising to defend against both untargeted and targeted\nattacks, which collectively limit the adversarial space. The extensive\nexperiments on four datasets demonstrate 2-23% improvements of detection rates\nwith minimum computational overhead for real-time image queries.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Image-Retrieval", "Robustness", "Datasets", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [-8.874804496765137, -5.091838836669922], "cluster": 1}, {"key": "xiao2024flair", "year": "2024", "citations": "0", "title": "FLAIR: VLM With Fine-grained Language-informed Image Representations", "abstract": "<p>CLIP has shown impressive results in aligning images and texts at scale.\nHowever, its ability to capture detailed visual features remains limited\nbecause CLIP matches images and texts at a global level. To address this issue,\nwe propose FLAIR, Fine-grained Language-informed Image Representations, an\napproach that utilizes long and detailed image descriptions to learn localized\nimage embeddings. By sampling diverse sub-captions that describe fine-grained\ndetails about an image, we train our vision-language model to produce not only\nglobal embeddings but also text-specific image representations. Our model\nintroduces text-conditioned attention pooling on top of local image tokens to\nproduce fine-grained image representations that excel at retrieving detailed\nimage content. We achieve state-of-the-art performance on both, existing\nmultimodal retrieval benchmarks, as well as, our newly introduced fine-grained\nretrieval task which evaluates vision-language models\u2019 ability to retrieve\npartial image content. Furthermore, our experiments demonstrate the\neffectiveness of FLAIR trained on 30M image-text pairs in capturing\nfine-grained visual information, including zero-shot semantic segmentation,\noutperforming models trained on billions of pairs. Code is available at\nhttps://github.com/ExplainableML/flair .</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Few-Shot-&-Zero-Shot"], "tsne_embedding": [-32.077430725097656, -32.20090103149414], "cluster": 5}, {"key": "xie2021efficient", "year": "2021", "citations": "2", "title": "Efficient Deep Feature Calibration For Cross-modal Joint Embedding Learning", "abstract": "<p>This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.</p>\n", "tags": ["Tools-&-Libraries", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-18.655622482299805, -29.89354133605957], "cluster": 3}, {"key": "xie2021learning", "year": "2021", "citations": "25", "title": "Learning Text-image Joint Embedding For Efficient Cross-modal Retrieval With Deep Feature Engineering", "abstract": "<p>This paper introduces a two-phase deep feature engineering framework for\nefficient learning of semantics enhanced joint embedding, which clearly\nseparates the deep feature engineering in data preprocessing from training the\ntext-image joint embedding model. We use the Recipe1M dataset for the technical\ndescription and empirical validation. In preprocessing, we perform deep feature\nengineering by combining deep feature engineering with semantic context\nfeatures derived from raw text-image input data. We leverage LSTM to identify\nkey terms, deep NLP models from the BERT family, TextRank, or TF-IDF to produce\nranking scores for key terms before generating the vector representation for\neach key term by using word2vec. We leverage wideResNet50 and word2vec to\nextract and encode the image category semantics of food images to help semantic\nalignment of the learned recipe and image embeddings in the joint latent space.\nIn joint embedding learning, we perform deep feature engineering by optimizing\nthe batch-hard triplet loss function with soft-margin and double negative\nsampling, taking into account also the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with deep feature engineering significantly outperforms the\nstate-of-the-art approaches.</p>\n", "tags": ["Multimodal-Retrieval", "Tools-&-Libraries", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-41.74595642089844, 29.88848114013672], "cluster": 0}, {"key": "xin2021zero", "year": "2022", "citations": "17", "title": "Zero-shot Dense Retrieval With Momentum Adversarial Domain Invariant Representations", "abstract": "<p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts\nin the embedding space and then matching them by nearest neighbor search. This\nrequires strong locality properties from the representation space, i.e, the\nclose allocations of each small group of relevant texts, which are hard to\ngeneralize to domains without sufficient training data. In this paper, we aim\nto improve the generalization ability of DR models from source training domains\nwith rich supervision signals to target domains without any relevant labels, in\nthe zero-shot setting. To achieve that, we propose Momentum adversarial Domain\nInvariant Representation learning (MoDIR), which introduces a momentum method\nin the DR training process to train a domain classifier distinguishing source\nversus target, and then adversarially updates the DR encoder to learn domain\ninvariant representations. Our experiments show that MoDIR robustly outperforms\nits baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot\nsetup, with more than 10% relative gains on datasets with enough sensitivity\nfor DR models\u2019 evaluation. Source code of this paper will be released.</p>\n", "tags": ["Text-Retrieval", "Few-Shot-&-Zero-Shot", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-6.812083721160889, -26.17130470275879], "cluster": 3}, {"key": "xing2022multimorbidity", "year": "2022", "citations": "0", "title": "Multimorbidity Content-based Medical Image Retrieval Using Proxies", "abstract": "<p>Content-based medical image retrieval is an important diagnostic tool that\nimproves the explainability of computer-aided diagnosis systems and provides\ndecision making support to healthcare professionals. Medical imaging data, such\nas radiology images, are often multimorbidity; a single sample may have more\nthan one pathology present. As such, image retrieval systems for the medical\ndomain must be designed for the multi-label scenario. In this paper, we propose\na novel multi-label metric learning method that can be used for both\nclassification and content-based image retrieval. In this way, our model is\nable to support diagnosis by predicting the presence of diseases and provide\nevidence for these predictions by returning samples with similar pathological\ncontent to the user. In practice, the retrieved images may also be accompanied\nby pathology reports, further assisting in the diagnostic process. Our method\nleverages proxy feature vectors, enabling the efficient learning of a robust\nfeature space in which the distance between feature vectors can be used as a\nmeasure of the similarity of those samples. Unlike existing proxy-based\nmethods, training samples are able to assign to multiple proxies that span\nmultiple class labels. This multi-label proxy assignment results in a feature\nspace that encodes the complex relationships between diseases present in\nmedical imaging data. Our method outperforms state-of-the-art image retrieval\nsystems and a set of baseline approaches. We demonstrate the efficacy of our\napproach to both classification and content-based image retrieval on two\nmultimorbidity radiology datasets.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.52267074584961, 16.033811569213867], "cluster": 0}, {"key": "xing2025esans", "year": "2025", "citations": "0", "title": "ESANS: Effective And Semantic-aware Negative Sampling For Large-scale Retrieval Systems", "abstract": "<p>Industrial recommendation systems typically involve a two-stage process:\nretrieval and ranking, which aims to match users with millions of items. In the\nretrieval stage, classic embedding-based retrieval (EBR) methods depend on\neffective negative sampling techniques to enhance both performance and\nefficiency. However, existing techniques often suffer from false negatives,\nhigh cost for ensuring sampling quality and semantic information deficiency. To\naddress these limitations, we propose Effective and Semantic-Aware Negative\nSampling (ESANS), which integrates two key components: Effective Dense\nInterpolation Strategy (EDIS) and Multimodal Semantic-Aware Clustering (MSAC).\nEDIS generates virtual samples within the low-dimensional embedding space to\nimprove the diversity and density of the sampling distribution while minimizing\ncomputational costs. MSAC refines the negative sampling distribution by\nhierarchically clustering item representations based on multimodal information\n(visual, textual, behavioral), ensuring semantic consistency and reducing false\nnegatives. Extensive offline and online experiments demonstrate the superior\nefficiency and performance of ESANS.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Evaluation", "Scalability"], "tsne_embedding": [25.275691986083984, -22.439422607421875], "cluster": 7}, {"key": "xiong2014adaptive", "year": "2014", "citations": "13", "title": "Adaptive Quantization For Hashing: An Information-based Approach To Learning Binary Codes", "abstract": "<p>Large-scale data mining and retrieval applications have\nincreasingly turned to compact binary data representations\nas a way to achieve both fast queries and efficient\ndata storage; many algorithms have been proposed for\nlearning effective binary encodings. Most of these algorithms\nfocus on learning a set of projection hyperplanes\nfor the data and simply binarizing the result from each\nhyperplane, but this neglects the fact that informativeness\nmay not be uniformly distributed across the projections.\nIn this paper, we address this issue by proposing\na novel adaptive quantization (AQ) strategy that\nadaptively assigns varying numbers of bits to different\nhyperplanes based on their information content. Our\nmethod provides an information-based schema that preserves\nthe neighborhood structure of data points, and\nwe jointly find the globally optimal bit-allocation for\nall hyperplanes. In our experiments, we compare with\nstate-of-the-art methods on four large-scale datasets\nand find that our adaptive quantization approach significantly\nimproves on traditional hashing methods.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Scalability", "Datasets", "Compact-Codes"], "tsne_embedding": [0.4383697211742401, 36.6116943359375], "cluster": 4}, {"key": "xiong2020approximate", "year": "2021", "citations": "359", "title": "Approximate Nearest Neighbor Negative Contrastive Learning For Dense Text Retrieval", "abstract": "<p>Conducting text retrieval in a dense learned representation space has many\nintriguing advantages over sparse retrieval. Yet the effectiveness of dense\nretrieval (DR) often requires combination with sparse retrieval. In this paper,\nwe identify that the main bottleneck is in the training mechanisms, where the\nnegative instances used in training are not representative of the irrelevant\ndocuments in testing. This paper presents Approximate nearest neighbor Negative\nContrastive Estimation (ANCE), a training mechanism that constructs negatives\nfrom an Approximate Nearest Neighbor (ANN) index of the corpus, which is\nparallelly updated with the learning process to select more realistic negative\ntraining instances. This fundamentally resolves the discrepancy between the\ndata distribution used in the training and testing of DR. In our experiments,\nANCE boosts the BERT-Siamese DR model to outperform all competitive dense and\nsparse retrieval baselines. It nearly matches the accuracy of\nsparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned\nrepresentation space and provides almost 100x speed-up.</p>\n", "tags": ["Self-Supervised", "Text-Retrieval"], "tsne_embedding": [-6.646904945373535, -26.21260643005371], "cluster": 3}, {"key": "xiong2025adaptive", "year": "2014", "citations": "13", "title": "Adaptive Quantization For Hashing: An Information-based Approach To Learning Binary Codes", "abstract": "<p>Large-scale data mining and retrieval applications have\nincreasingly turned to compact binary data representations\nas a way to achieve both fast queries and efficient\ndata storage; many algorithms have been proposed for\nlearning effective binary encodings. Most of these algorithms\nfocus on learning a set of projection hyperplanes\nfor the data and simply binarizing the result from each\nhyperplane, but this neglects the fact that informativeness\nmay not be uniformly distributed across the projections.\nIn this paper, we address this issue by proposing\na novel adaptive quantization (AQ) strategy that\nadaptively assigns varying numbers of bits to different\nhyperplanes based on their information content. Our\nmethod provides an information-based schema that preserves\nthe neighborhood structure of data points, and\nwe jointly find the globally optimal bit-allocation for\nall hyperplanes. In our experiments, we compare with\nstate-of-the-art methods on four large-scale datasets\nand find that our adaptive quantization approach significantly\nimproves on traditional hashing methods.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Scalability", "Datasets", "Compact-Codes"], "tsne_embedding": [0.4386049509048462, 36.611637115478516], "cluster": 4}, {"key": "xu2013harmonious", "year": "2013", "citations": "49", "title": "Harmonious Hashing", "abstract": "<p>Hashing-based fast nearest neighbor search technique\nhas attracted great attention in both research\nand industry areas recently. Many existing hashing\napproaches encode data with projection-based hash\nfunctions and represent each projected dimension\nby 1-bit. However, the dimensions with high variance\nhold large energy or information of data but\ntreated equivalently as dimensions with low variance,\nwhich leads to a serious information loss. In\nthis paper, we introduce a novel hashing algorithm\ncalled Harmonious Hashing which aims at learning\nhash functions with low information loss. Specifically,\nwe learn a set of optimized projections to\npreserve the maximum cumulative energy and meet\nthe constraint of equivalent variance on each dimension\nas much as possible. In this way, we could\nminimize the information loss after binarization.\nDespite the extreme simplicity, our method outperforms\nsuperiorly to many state-of-the-art hashing\nmethods in large-scale and high-dimensional nearest\nneighbor search experiments.</p>\n", "tags": ["Hashing-Methods", "Scalability"], "tsne_embedding": [14.768529891967773, 30.5906982421875], "cluster": 4}, {"key": "xu2015convolutional", "year": "2015", "citations": "33", "title": "Convolutional Neural Networks For Text Hashing", "abstract": "<p>Hashing, as a popular approximate nearest neighbor\nsearch, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning\nmethods are utilized to learn similarity-preserving\nbinary codes. However, most of them directly encode the explicit features, keywords, which fail to\npreserve the accurate semantic similarities in binary code beyond keyword matching, especially on\nshort texts. Here we propose a novel text hashing\nframework with convolutional neural networks. In\nparticular, we first embed the keyword features into\ncompact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to\nlearn the implicit features which are further incorporated with the explicit features to fit the pre-trained\nbinary code. Such base method can be successfully\naccomplished without any external tags/labels, and\nother three model variations are designed to integrate tags/labels. Experimental results show the\nsuperiority of our proposed approach over several\nstate-of-the-art hashing methods when tested on one\nshort text dataset as well as one normal text dataset.</p>\n", "tags": ["Hashing-Methods", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Datasets", "Compact-Codes"], "tsne_embedding": [10.941181182861328, -9.343489646911621], "cluster": 6}, {"key": "xu2016binary", "year": "2016", "citations": "2", "title": "Binary Subspace Coding For Query-by-image Video Retrieval", "abstract": "<p>The query-by-image video retrieval (QBIVR) task has been attracting\nconsiderable research attention recently. However, most existing methods\nrepresent a video by either aggregating or projecting all its frames into a\nsingle datum point, which may easily cause severe information loss. In this\npaper, we propose an efficient QBIVR framework to enable an effective and\nefficient video search with image query. We first define a\nsimilarity-preserving distance metric between an image and its orthogonal\nprojection in the subspace of the video, which can be equivalently transformed\nto a Maximum Inner Product Search (MIPS) problem.\n  Besides, to boost the efficiency of solving the MIPS problem, we propose two\nasymmetric hashing schemes, which bridge the domain gap of images and videos.\nThe first approach, termed Inner-product Binary Coding (IBC), preserves the\ninner relationships of images and videos in a common Hamming space. To further\nimprove the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)\napproach, which employs compact bilinear projections instead of a single large\nprojection matrix. Extensive experiments have been conducted on four real-world\nvideo datasets to verify the effectiveness of our proposed approaches as\ncompared to the state-of-the-arts.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Tools-&-Libraries", "Datasets", "Video-Retrieval"], "tsne_embedding": [-37.26235580444336, -27.51095962524414], "cluster": 5}, {"key": "xu2017cross", "year": "2017", "citations": "82", "title": "Cross-modal Subspace Learning For Fine-grained Sketch-based Image Retrieval", "abstract": "<p>Sketch-based image retrieval (SBIR) is challenging due to the inherent\ndomain-gap between sketch and photo. Compared with pixel-perfect depictions of\nphotos, sketches are iconic renderings of the real world with highly abstract.\nTherefore, matching sketch and photo directly using low-level visual clues are\nunsufficient, since a common low-level subspace that traverses semantically\nacross the two modalities is non-trivial to establish. Most existing SBIR\nstudies do not directly tackle this cross-modal problem. This naturally\nmotivates us to explore the effectiveness of cross-modal retrieval methods in\nSBIR, which have been applied in the image-text matching successfully. In this\npaper, we introduce and compare a series of state-of-the-art cross-modal\nsubspace learning methods and benchmark them on two recently released\nfine-grained SBIR datasets. Through thorough examination of the experimental\nresults, we have demonstrated that the subspace learning can effectively model\nthe sketch-photo domain-gap. In addition we draw a few key insights to drive\nfuture research.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-45.32539749145508, -16.840566635131836], "cluster": 5}, {"key": "xu2017iterative", "year": "2018", "citations": "21", "title": "Iterative Manifold Embedding Layer Learned By Incomplete Data For Large-scale Image Retrieval", "abstract": "<p>Existing manifold learning methods are not appropriate for image retrieval\ntask, because most of them are unable to process query image and they have much\nadditional computational cost especially for large scale database. Therefore,\nwe propose the iterative manifold embedding (IME) layer, of which the weights\nare learned off-line by unsupervised strategy, to explore the intrinsic\nmanifolds by incomplete data. On the large scale database that contains 27000\nimages, IME layer is more than 120 times faster than other manifold learning\nmethods to embed the original representations at query time. We embed the\noriginal descriptors of database images which lie on manifold in a high\ndimensional space into manifold-based representations iteratively to generate\nthe IME representations in off-line learning stage. According to the original\ndescriptors and the IME representations of database images, we estimate the\nweights of IME layer by ridge regression. In on-line retrieval stage, we employ\nthe IME layer to map the original representation of query image with ignorable\ntime cost (2 milliseconds). We experiment on five public standard datasets for\nimage retrieval. The proposed IME layer significantly outperforms related\ndimension reduction methods and manifold learning methods. Without\npost-processing, Our IME layer achieves a boost in performance of\nstate-of-the-art image retrieval methods with post-processing on most datasets,\nand needs less computational cost.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Re-Ranking", "Evaluation", "Unsupervised"], "tsne_embedding": [-2.813310384750366, 18.1419677734375], "cluster": 8}, {"key": "xu2017neural", "year": "2017", "citations": "545", "title": "Neural Network-based Graph Embedding For Cross-platform Binary Code Similarity Detection", "abstract": "<p>The problem of cross-platform binary code similarity detection aims at\ndetecting whether two binary functions coming from different platforms are\nsimilar or not. It has many security applications, including plagiarism\ndetection, malware detection, vulnerability search, etc. Existing approaches\nrely on approximate graph matching algorithms, which are inevitably slow and\nsometimes inaccurate, and hard to adapt to a new task. To address these issues,\nin this work, we propose a novel neural network-based approach to compute the\nembedding, i.e., a numeric vector, based on the control flow graph of each\nbinary function, then the similarity detection can be done efficiently by\nmeasuring the distance between the embeddings for two functions. We implement a\nprototype called Gemini. Our extensive evaluation shows that Gemini outperforms\nthe state-of-the-art approaches by large margins with respect to similarity\ndetection accuracy. Further, Gemini can speed up prior art\u2019s embedding\ngeneration time by 3 to 4 orders of magnitude and reduce the required training\ntime from more than 1 week down to 30 minutes to 10 hours. Our real world case\nstudies demonstrate that Gemini can identify significantly more vulnerable\nfirmware images than the state-of-the-art, i.e., Genius. Our research showcases\na successful application of deep learning on computer security problems.</p>\n", "tags": ["Compact-Codes", "Evaluation"], "tsne_embedding": [16.40153694152832, 13.022055625915527], "cluster": 6}, {"key": "xu2017non", "year": "2017", "citations": "0", "title": "Non-iterative Label Propagation In Optimal Leading Forest", "abstract": "<p>Graph based semi-supervised learning (GSSL) has intuitive representation and\ncan be improved by exploiting the matrix calculation. However, it has to\nperform iterative optimization to achieve a preset objective, which usually\nleads to low efficiency. Another inconvenience lying in GSSL is that when new\ndata come, the graph construction and the optimization have to be conducted all\nover again. We propose a sound assumption, arguing that: the neighboring data\npoints are not in peer-to-peer relation, but in a partial-ordered relation\ninduced by the local density and distance between the data; and the label of a\ncenter can be regarded as the contribution of its followers. Starting from the\nassumption, we develop a highly efficient non-iterative label propagation\nalgorithm based on a novel data structure named as optimal leading forest\n(LaPOLeaF). The major weaknesses of the traditional GSSL are addressed by this\nstudy. We further scale LaPOLeaF to accommodate big data by utilizing block\ndistance matrix technique, parallel computing, and Locality-Sensitive Hashing\n(LSH). Experiments on large datasets have shown the promising results of the\nproposed methods.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Datasets", "Supervised"], "tsne_embedding": [49.507938385009766, 2.1879096031188965], "cluster": 9}, {"key": "xu2017unsupervised", "year": "2018", "citations": "48", "title": "Unsupervised Part-based Weighting Aggregation Of Deep Convolutional Features For Image Retrieval", "abstract": "<p>In this paper, we propose a simple but effective semantic part-based\nweighting aggregation (PWA) for image retrieval. The proposed PWA utilizes the\ndiscriminative filters of deep convolutional layers as part detectors.\nMoreover, we propose the effective unsupervised strategy to select some part\ndetectors to generate the \u201cprobabilistic proposals\u201d, which highlight certain\ndiscriminative parts of objects and suppress the noise of background. The final\nglobal PWA representation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \u201cprobabilistic proposals\u201d\ncorresponding to various semantic content. We conduct comprehensive experiments\non four standard datasets and show that our unsupervised PWA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods. Code is\navailable at https://github.com/XJhaoren/PWA.</p>\n", "tags": ["Image-Retrieval", "AAAI", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [-26.383968353271484, 17.484073638916016], "cluster": 8}, {"key": "xu2018adversarial", "year": "2018", "citations": "1", "title": "Adversarial Soft-detection-based Aggregation Network For Image Retrieval", "abstract": "<p>In recent year, the compact representations based on activations of\nConvolutional Neural Network (CNN) achieve remarkable performance in image\nretrieval. However, retrieval of some interested object that only takes up a\nsmall part of the whole image is still a challenging problem. Therefore, it is\nsignificant to extract the discriminative representations that contain regional\ninformation of the pivotal small object. In this paper, we propose a novel\nadversarial soft-detection-based aggregation (ASDA) method free from bounding\nbox annotations for image retrieval, based on adversarial detector and soft\nregion proposal layer. Our trainable adversarial detector generates semantic\nmaps based on adversarial erasing strategy to preserve more discriminative and\ndetailed information. Computed based on semantic maps corresponding to various\ndiscriminative patterns and semantic contents, our soft region proposal is\narbitrary shape rather than only rectangle and it reflects the significance of\nobjects. The aggregation based on trainable soft region proposal highlights\ndiscriminative semantic contents and suppresses the noise of background.\n  We conduct comprehensive experiments on standard image retrieval datasets.\nOur weakly supervised ASDA method achieves state-of-the-art performance on most\ndatasets. The results demonstrate that the proposed ASDA method is effective\nfor image retrieval.</p>\n", "tags": ["Image-Retrieval", "Robustness", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-47.404518127441406, -0.16904354095458984], "cluster": 0}, {"key": "xu2018error", "year": "2018", "citations": "0", "title": "Error Correction Maximization For Deep Image Hashing", "abstract": "<p>We propose to use the concept of the Hamming bound to derive the optimal\ncriteria for learning hash codes with a deep network. In particular, when the\nnumber of binary hash codes (typically the number of image categories) and code\nlength are known, it is possible to derive an upper bound on the minimum\nHamming distance between the hash codes. This upper bound can then be used to\ndefine the loss function for learning hash codes. By encouraging the margin\n(minimum Hamming distance) between the hash codes of different image categories\nto match the upper bound, we are able to learn theoretically optimal hash\ncodes. Our experiments show that our method significantly outperforms competing\ndeep learning-based approaches and obtains top performance on benchmark\ndatasets.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Image-Retrieval", "Datasets"], "tsne_embedding": [-4.679043769836426, 1.1505043506622314], "cluster": 1}, {"key": "xu2018gpu", "year": "2018", "citations": "1", "title": "GPU Accelerated Cascade Hashing Image Matching For Large Scale 3D Reconstruction", "abstract": "<p>Image feature point matching is a key step in Structure from Motion(SFM).\nHowever, it is becoming more and more time consuming because the number of\nimages is getting larger and larger. In this paper, we proposed a GPU\naccelerated image matching method with improved Cascade Hashing. Firstly, we\npropose a Disk-Memory-GPU data exchange strategy and optimize the load order of\ndata, so that the proposed method can deal with big data. Next, we parallelize\nthe Cascade Hashing method on GPU. An improved parallel reduction and an\nimproved parallel hashing ranking are proposed to fulfill this task. Finally,\nextensive experiments show that our image matching is about 20 times faster\nthan SiftGPU on the same graphics card, nearly 100 times faster than the CPU\nCasHash method and hundreds of times faster than the CPU Kd-Tree based matching\nmethod. Further more, we introduce the epipolar constraint to the proposed\nmethod, and use the epipolar geometry to guide the feature matching procedure,\nwhich further reduces the matching cost.</p>\n", "tags": ["Tree-Based-Ann", "Hashing-Methods"], "tsne_embedding": [38.7366943359375, 31.934085845947266], "cluster": 2}, {"key": "xu2018sketchmate", "year": "2018", "citations": "128", "title": "Sketchmate: Deep Hashing For Million-scale Human Sketch Retrieval", "abstract": "<p>We propose a deep hashing framework for sketch retrieval that, for the first\ntime, works on a multi-million scale human sketch dataset. Leveraging on this\nlarge dataset, we explore a few sketch-specific traits that were otherwise\nunder-studied in prior literature. Instead of following the conventional sketch\nrecognition task, we introduce the novel problem of sketch hashing retrieval\nwhich is not only more challenging, but also offers a better testbed for\nlarge-scale sketch analysis, since: (i) more fine-grained sketch feature\nlearning is required to accommodate the large variations in style and\nabstraction, and (ii) a compact binary code needs to be learned at the same\ntime to enable efficient retrieval. Key to our network design is the embedding\nof unique characteristics of human sketch, where (i) a two-branch CNN-RNN\narchitecture is adapted to explore the temporal ordering of strokes, and (ii) a\nnovel hashing loss is specifically designed to accommodate both the temporal\nand abstract traits of sketches. By working with a 3.8M sketch dataset, we show\nthat state-of-the-art hashing models specifically engineered for static images\nfail to perform well on temporal sketch data. Our network on the other hand not\nonly offers the best retrieval performance on various code sizes, but also\nyields the best generalization performance under a zero-shot setting and when\nre-purposed for sketch recognition. Such superior performances effectively\ndemonstrate the benefit of our sketch-specific design.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "CVPR", "Few-Shot-&-Zero-Shot", "Scalability", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-44.07469940185547, -18.391231536865234], "cluster": 5}, {"key": "xu2019hashing", "year": "2020", "citations": "8", "title": "Hashing Based Answer Selection", "abstract": "<p>Answer selection is an important subtask of question answering (QA), where\ndeep models usually achieve better performance. Most deep models adopt\nquestion-answer interaction mechanisms, such as attention, to get vector\nrepresentations for answers. When these interaction based deep models are\ndeployed for online prediction, the representations of all answers need to be\nrecalculated for each question. This procedure is time-consuming for deep\nmodels with complex encoders like BERT which usually have better accuracy than\nsimple encoders. One possible solution is to store the matrix representation\n(encoder output) of each answer in memory to avoid recalculation. But this will\nbring large memory cost. In this paper, we propose a novel method, called\nhashing based answer selection (HAS), to tackle this problem. HAS adopts a\nhashing strategy to learn a binary matrix representation for each answer, which\ncan dramatically reduce the memory cost for storing the matrix representations\nof answers. Hence, HAS can adopt complex encoders like BERT in the model, but\nthe online prediction of HAS is still fast with a low memory cost. Experimental\nresults on three popular answer selection datasets show that HAS can outperform\nexisting models to achieve state-of-the-art performance.</p>\n", "tags": ["AAAI", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [26.628250122070312, 5.517973899841309], "cluster": 2}, {"key": "xu2019semantic", "year": "2019", "citations": "4", "title": "Semantic Adversarial Network For Zero-shot Sketch-based Image Retrieval", "abstract": "<p>Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal\nretrieval task for retrieving natural images with free-hand sketches under\nzero-shot scenario. Previous works mostly focus on modeling the correspondence\nbetween images and sketches or synthesizing image features with sketch\nfeatures. However, both of them ignore the large intra-class variance of\nsketches, thus resulting in unsatisfactory retrieval performance. In this\npaper, we propose a novel end-to-end semantic adversarial approach for ZS-SBIR.\nSpecifically, we devise a semantic adversarial module to maximize the\nconsistency between learned semantic features and category-level word vectors.\nMoreover, to preserve the discriminability of synthesized features within each\ntraining category, a triplet loss is employed for the generative module.\nAdditionally, the proposed model is trained in an end-to-end strategy to\nexploit better semantic features suitable for ZS-SBIR. Extensive experiments\nconducted on two large-scale popular datasets demonstrate that our proposed\napproach remarkably outperforms state-of-the-art approaches by more than 12%\non Sketchy dataset and about 3% on TU-Berlin dataset in the retrieval.</p>\n", "tags": ["Distance-Metric-Learning", "Few-Shot-&-Zero-Shot", "Image-Retrieval", "Scalability", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [-33.67250061035156, -12.16762638092041], "cluster": 5}, {"key": "xu2020learning", "year": "2020", "citations": "4", "title": "On Learning Semantic Representations For Million-scale Free-hand Sketches", "abstract": "<p>In this paper, we study learning semantic representations for million-scale\nfree-hand sketches. This is highly challenging due to the domain-unique traits\nof sketches, e.g., diverse, sparse, abstract, noisy. We propose a dual-branch\nCNNRNN network architecture to represent sketches, which simultaneously encodes\nboth the static and temporal patterns of sketch strokes. Based on this\narchitecture, we further explore learning the sketch-oriented semantic\nrepresentations in two challenging yet practical settings, i.e., hashing\nretrieval and zero-shot recognition on million-scale sketches. Specifically, we\nuse our dual-branch architecture as a universal representation framework to\ndesign two sketch-specific deep models: (i) We propose a deep hashing model for\nsketch retrieval, where a novel hashing loss is specifically designed to\naccommodate both the abstract and messy traits of sketches. (ii) We propose a\ndeep embedding model for sketch zero-shot recognition, via collecting a\nlarge-scale edge-map dataset and proposing to extract a set of semantic vectors\nfrom edge-maps as the semantic knowledge for sketch zero-shot domain alignment.\nBoth deep models are evaluated by comprehensive experiments on million-scale\nsketches and outperform the state-of-the-art competitors.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Few-Shot-&-Zero-Shot", "Scalability", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-43.93412399291992, -18.286582946777344], "cluster": 5}, {"key": "xu2020multi", "year": "2020", "citations": "21", "title": "Multi-feature Discrete Collaborative Filtering For Fast Cold-start Recommendation", "abstract": "<p>Hashing is an effective technique to address the large-scale recommendation\nproblem, due to its high computation and storage efficiency on calculating the\nuser preferences on items. However, existing hashing-based recommendation\nmethods still suffer from two important problems: 1) Their recommendation\nprocess mainly relies on the user-item interactions and single specific content\nfeature. When the interaction history or the content feature is unavailable\n(the cold-start problem), their performance will be seriously deteriorated. 2)\nExisting methods learn the hash codes with relaxed optimization or adopt\ndiscrete coordinate descent to directly solve binary hash codes, which results\nin significant quantization loss or consumes considerable computation time. In\nthis paper, we propose a fast cold-start recommendation method, called\nMulti-Feature Discrete Collaborative Filtering (MFDCF), to solve these\nproblems. Specifically, a low-rank self-weighted multi-feature fusion module is\ndesigned to adaptively project the multiple content features into binary yet\ninformative hash codes by fully exploiting their complementarity. Additionally,\nwe develop a fast discrete optimization algorithm to directly compute the\nbinary hash codes with simple operations. Experiments on two public\nrecommendation datasets demonstrate that MFDCF outperforms the\nstate-of-the-arts on various aspects.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Recommender-Systems", "Scalability", "AAAI", "Datasets", "Evaluation"], "tsne_embedding": [9.259172439575195, 21.442914962768555], "cluster": 6}, {"key": "xu20213rd", "year": "2021", "citations": "4", "title": "3rd Place Solution To Google Landmark Recognition Competition 2021", "abstract": "<p>In this paper, we show our solution to the Google Landmark Recognition 2021\nCompetition. Firstly, embeddings of images are extracted via various\narchitectures (i.e. CNN-, Transformer- and hybrid-based), which are optimized\nby ArcFace loss. Then we apply an efficient pipeline to re-rank predictions by\nadjusting the retrieval score with classification logits and non-landmark\ndistractors. Finally, the ensembled model scores 0.489 on the private\nleaderboard, achieving the 3rd place in the 2021 edition of the Google Landmark\nRecognition Competition.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-14.972203254699707, -57.14902877807617], "cluster": 3}, {"key": "xu2021hhf", "year": "2022", "citations": "16", "title": "HHF: Hashing-guided Hinge Function For Deep Hashing Retrieval", "abstract": "<p>Deep hashing has shown promising performance in large-scale image retrieval.\nHowever, latent codes extracted by Deep Neural Networks (DNNs) will inevitably\nlose semantic information during the binarization process, which damages the\nretrieval accuracy and makes it challenging. Although many existing approaches\nperform regularization to alleviate quantization errors, we figure out an\nincompatible conflict between metric learning and quantization learning. The\nmetric loss penalizes the inter-class distances to push different classes\nunconstrained far away. Worse still, it tends to map the latent code deviate\nfrom ideal binarization point and generate severe ambiguity in the binarization\nprocess. Based on the minimum distance of the binary linear code, we creatively\npropose Hashing-guided Hinge Function (HHF) to avoid such conflict. In detail,\nthe carefully-designed inflection point, which relies on the hash bit length\nand category numbers, is explicitly adopted to balance the metric term and\nquantization term. Such a modification prevents the network from falling into\nlocal metric optimal minima in deep hashing. Extensive experiments in CIFAR-10,\nCIFAR-100, ImageNet, and MS-COCO show that HHF consistently outperforms\nexisting techniques, and is robust and flexible to transplant into other\nmethods. Code is available at https://github.com/JerryXu0129/HHF.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Neural-Hashing", "Quantization", "Scalability", "Image-Retrieval", "Evaluation"], "tsne_embedding": [-4.638647079467773, 16.487016677856445], "cluster": 8}, {"key": "xu2021videoclip", "year": "2021", "citations": "266", "title": "Videoclip: Contrastive Pre-training For Zero-shot Video-text Understanding", "abstract": "<p>We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/MMPT.</p>\n", "tags": ["EMNLP", "Few-Shot-&-Zero-Shot", "Supervised", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-12.1538724899292, -35.39845275878906], "cluster": 3}, {"key": "xu2022hyp", "year": "2022", "citations": "2", "title": "Hyp\\(^2\\) Loss: Beyond Hypersphere Metric Space For Multi-label Image Retrieval", "abstract": "<p>Image retrieval has become an increasingly appealing technique with broad\nmultimedia application prospects, where deep hashing serves as the dominant\nbranch towards low storage and efficient retrieval. In this paper, we carried\nout in-depth investigations on metric learning in deep hashing for establishing\na powerful metric space in multi-label scenarios, where the pair loss suffers\nhigh computational overhead and converge difficulty, while the proxy loss is\ntheoretically incapable of expressing the profound label dependencies and\nexhibits conflicts in the constructed hypersphere space. To address the\nproblems, we propose a novel metric learning framework with Hybrid Proxy-Pair\nLoss (HyP\\(^2\\) Loss) that constructs an expressive metric space with efficient\ntraining complexity w.r.t. the whole dataset. The proposed HyP\\(^2\\) Loss focuses\non optimizing the hypersphere space by learnable proxies and excavating\ndata-to-data correlations of irrelevant pairs, which integrates sufficient data\ncorrespondence of pair-based methods and high-efficiency of proxy-based\nmethods. Extensive experiments on four standard multi-label benchmarks justify\nthe proposed method outperforms the state-of-the-art, is robust among different\nhash bits and achieves significant performance gains with a faster, more stable\nconvergence speed. Our code is available at\nhttps://github.com/JerryXu0129/HyP2-Loss.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Neural-Hashing", "Efficiency", "Similarity-Search", "Image-Retrieval", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [-1.548747181892395, -10.03345775604248], "cluster": 1}, {"key": "xu2022laprador", "year": "2022", "citations": "22", "title": "Laprador: Unsupervised Pretrained Dense Retriever For Zero-shot Text Retrieval", "abstract": "<p>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever\nthat does not require any supervised data for training. Specifically, we first\npresent Iterative Contrastive Learning (ICoL) that iteratively trains the query\nand document encoders with a cache mechanism. ICoL not only enlarges the number\nof negative instances but also keeps representations of cached examples in the\nsame hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a\nsimple yet effective way to enhance dense retrieval with lexical matching. We\nevaluate LaPraDoR on the recently proposed BEIR benchmark, including 18\ndatasets of 9 zero-shot text retrieval tasks. Experimental results show that\nLaPraDoR achieves state-of-the-art performance compared with supervised dense\nretrieval models, and further analysis reveals the effectiveness of our\ntraining strategy and objectives. Compared to re-ranking, our lexicon-enhanced\napproach can be run in milliseconds (22.5x faster) while achieving superior\nperformance.</p>\n", "tags": ["Self-Supervised", "Text-Retrieval", "Few-Shot-&-Zero-Shot", "Datasets", "Supervised", "Hybrid-Ann-Methods", "Evaluation", "Re-Ranking", "Unsupervised"], "tsne_embedding": [-5.0519185066223145, -27.996034622192383], "cluster": 3}, {"key": "xu2023deep", "year": "2024", "citations": "1", "title": "Deep Lifelong Cross-modal Hashing", "abstract": "<p>Hashing methods have made significant progress in cross-modal retrieval tasks\nwith fast query speed and low storage cost. Among them, deep learning-based\nhashing achieves better performance on large-scale data due to its excellent\nextraction and representation ability for nonlinear heterogeneous features.\nHowever, there are still two main challenges in catastrophic forgetting when\ndata with new categories arrive continuously, and time-consuming for\nnon-continuous hashing retrieval to retrain for updating. To this end, we, in\nthis paper, propose a novel deep lifelong cross-modal hashing to achieve\nlifelong hashing retrieval instead of re-training hash function repeatedly when\nnew data arrive. Specifically, we design lifelong learning strategy to update\nhash functions by directly training the incremental data instead of retraining\nnew hash functions using all the accumulated data, which significantly reduce\ntraining time. Then, we propose lifelong hashing loss to enable original hash\ncodes participate in lifelong learning but remain invariant, and further\npreserve the similarity and dis-similarity among original and incremental hash\ncodes to maintain performance. Additionally, considering distribution\nheterogeneity when new data arriving continuously, we introduce multi-label\nsemantic similarity to supervise hash learning, and it has been proven that the\nsimilarity improves performance with detailed analysis. Experimental results on\nbenchmark datasets show that the proposed methods achieves comparative\nperformance comparing with recent state-of-the-art cross-modal hashing methods,\nand it yields substantial average increments over 20% in retrieval accuracy\nand almost reduces over 80% training time when new data arrives continuously.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Multimodal-Retrieval", "Memory-Efficiency", "Datasets", "Evaluation"], "tsne_embedding": [27.55268096923828, 2.2157392501831055], "cluster": 6}, {"key": "xu2024bi", "year": "2024", "citations": "0", "title": "A Bi-metric Framework For Fast Similarity Search", "abstract": "<p>We propose a new \u201cbi-metric\u201d framework for designing nearest neighbor data\nstructures. Our framework assumes two dissimilarity functions: a ground-truth\nmetric that is accurate but expensive to compute, and a proxy metric that is\ncheaper but less accurate. In both theory and practice, we show how to\nconstruct data structures using only the proxy metric such that the query\nprocedure achieves the accuracy of the expensive metric, while only using a\nlimited number of calls to both metrics. Our theoretical results instantiate\nthis framework for two popular nearest neighbor search algorithms: DiskANN and\nCover Tree. In both cases we show that, as long as the proxy metric used to\nconstruct the data structure approximates the ground-truth metric up to a\nbounded factor, our data structure achieves arbitrarily good approximation\nguarantees with respect to the ground-truth metric. On the empirical side, we\napply the framework to the text retrieval problem with two dissimilarity\nfunctions evaluated by ML models with vastly different computational costs. We\nobserve that for almost all data sets in the MTEB benchmark, our approach\nachieves a considerably better accuracy-efficiency tradeoff than the\nalternatives, such as re-ranking.</p>\n", "tags": ["Text-Retrieval", "Efficiency", "Similarity-Search", "Tools-&-Libraries", "Tree-Based-Ann", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [18.751447677612305, 40.86321258544922], "cluster": 4}, {"key": "xu2025convolutional", "year": "2015", "citations": "33", "title": "Convolutional Neural Networks For Text Hashing", "abstract": "<p>Hashing, as a popular approximate nearest neighbor\nsearch, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning\nmethods are utilized to learn similarity-preserving\nbinary codes. However, most of them directly encode the explicit features, keywords, which fail to\npreserve the accurate semantic similarities in binary code beyond keyword matching, especially on\nshort texts. Here we propose a novel text hashing\nframework with convolutional neural networks. In\nparticular, we first embed the keyword features into\ncompact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to\nlearn the implicit features which are further incorporated with the explicit features to fit the pre-trained\nbinary code. Such base method can be successfully\naccomplished without any external tags/labels, and\nother three model variations are designed to integrate tags/labels. Experimental results show the\nsuperiority of our proposed approach over several\nstate-of-the-art hashing methods when tested on one\nshort text dataset as well as one normal text dataset.</p>\n", "tags": ["Hashing-Methods", "Similarity-Search", "Scalability", "Tools-&-Libraries", "Datasets", "Compact-Codes"], "tsne_embedding": [10.941232681274414, -9.343491554260254], "cluster": 6}, {"key": "xu2025harmonious", "year": "2013", "citations": "49", "title": "Harmonious Hashing", "abstract": "<p>Hashing-based fast nearest neighbor search technique\nhas attracted great attention in both research\nand industry areas recently. Many existing hashing\napproaches encode data with projection-based hash\nfunctions and represent each projected dimension\nby 1-bit. However, the dimensions with high variance\nhold large energy or information of data but\ntreated equivalently as dimensions with low variance,\nwhich leads to a serious information loss. In\nthis paper, we introduce a novel hashing algorithm\ncalled Harmonious Hashing which aims at learning\nhash functions with low information loss. Specifically,\nwe learn a set of optimized projections to\npreserve the maximum cumulative energy and meet\nthe constraint of equivalent variance on each dimension\nas much as possible. In this way, we could\nminimize the information loss after binarization.\nDespite the extreme simplicity, our method outperforms\nsuperiorly to many state-of-the-art hashing\nmethods in large-scale and high-dimensional nearest\nneighbor search experiments.</p>\n", "tags": ["Hashing-Methods", "Scalability"], "tsne_embedding": [14.768640518188477, 30.590682983398438], "cluster": 4}, {"key": "xu2025place", "year": "2025", "citations": "0", "title": "In-place Updates Of A Graph Index For Streaming Approximate Nearest Neighbor Search", "abstract": "<p>Indices for approximate nearest neighbor search (ANNS) are a basic component\nfor information retrieval and widely used in database, search, recommendation\nand RAG systems. In these scenarios, documents or other objects are inserted\ninto and deleted from the working set at a high rate, requiring a stream of\nupdates to the vector index. Algorithms based on proximity graph indices are\nthe most efficient indices for ANNS, winning many benchmark competitions.\nHowever, it is challenging to update such graph index at a high rate, while\nsupporting stable recall after many updates. Since the graph is singly-linked,\ndeletions are hard because there is no fast way to find in-neighbors of a\ndeleted vertex. Therefore, to update the graph, state-of-the-art algorithms\nsuch as FreshDiskANN accumulate deletions in a batch and periodically\nconsolidate, removing edges to deleted vertices and modifying the graph to\nensure recall stability. In this paper, we present IP-DiskANN\n(InPlaceUpdate-DiskANN), the first algorithm to avoid batch consolidation by\nefficiently processing each insertion and deletion in-place. Our experiments\nusing standard benchmarks show that IP-DiskANN has stable recall over various\nlengthy update patterns in both high-recall and low-recall regimes. Further,\nits query throughput and update speed are better than using the batch\nconsolidation algorithm and HNSW.</p>\n", "tags": ["Graph-Based-Ann", "Recommender-Systems", "Vector-Indexing", "Evaluation"], "tsne_embedding": [56.687171936035156, 13.431870460510254], "cluster": 9}, {"key": "xuan2018deep", "year": "2018", "citations": "124", "title": "Deep Randomized Ensembles For Metric Learning", "abstract": "<p>Learning embedding functions, which map semantically related inputs to nearby\nlocations in a feature space supports a variety of classification and\ninformation retrieval tasks. In this work, we propose a novel, generalizable\nand fast method to define a family of embedding functions that can be used as\nan ensemble to give improved results. Each embedding function is learned by\nrandomly bagging the training labels into small subsets. We show experimentally\nthat these embedding ensembles create effective embedding functions. The\nensemble output defines a metric space that improves state of the art\nperformance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes\nRetrieval and VehicleID.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-15.685441970825195, -25.25560760498047], "cluster": 3}, {"key": "xuan2019improved", "year": "2020", "citations": "131", "title": "Improved Embeddings With Easy Positive Triplet Mining", "abstract": "<p>Deep metric learning seeks to define an embedding where semantically similar\nimages are embedded to nearby locations, and semantically dissimilar images are\nembedded to distant locations. Substantial work has focused on loss functions\nand strategies to learn these embeddings by pushing images from the same class\nas close together in the embedding space as possible. In this paper, we propose\nan alternative, loosened embedding strategy that requires the embedding\nfunction only map each training image to the most similar examples from the\nsame class, an approach we call \u201cEasy Positive\u201d mining. We provide a collection\nof experiments and visualizations that highlight that this Easy Positive mining\nleads to embeddings that are more flexible and generalize better to new unseen\ndata. This simple mining strategy yields recall performance that exceeds state\nof the art approaches (including those with complicated loss functions and\nensemble methods) on image retrieval datasets including CUB, Stanford Online\nProducts, In-Shop Clothes and Hotels-50K.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-16.995824813842773, -14.398999214172363], "cluster": 1}, {"key": "xuan2022dissecting", "year": "2022", "citations": "1", "title": "Dissecting The Impact Of Different Loss Functions With Gradient Surgery", "abstract": "<p>Pair-wise loss is an approach to metric learning that learns a semantic\nembedding by optimizing a loss function that encourages images from the same\nsemantic class to be mapped closer than images from different classes. The\nliterature reports a large and growing set of variations of the pair-wise loss\nstrategies. Here we decompose the gradient of these loss functions into\ncomponents that relate to how they push the relative feature positions of the\nanchor-positive and anchor-negative pairs. This decomposition allows the\nunification of a large collection of current pair-wise loss functions.\nAdditionally, explicitly constructing pair-wise gradient updates to separate\nout these effects gives insights into which have the biggest impact, and leads\nto a simple algorithm that beats the state of the art for image retrieval on\nthe CAR, CUB and Stanford Online products datasets.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-15.293830871582031, -11.742870330810547], "cluster": 1}, {"key": "xue2022cross", "year": "2022", "citations": "5", "title": "Cross-scale Context Extracted Hashing For Fine-grained Image Binary Encoding", "abstract": "<p>Deep hashing has been widely applied to large-scale image retrieval tasks\nowing to efficient computation and low storage cost by encoding\nhigh-dimensional image data into binary codes. Since binary codes do not\ncontain as much information as float features, the essence of binary encoding\nis preserving the main context to guarantee retrieval quality. However, the\nexisting hashing methods have great limitations on suppressing redundant\nbackground information and accurately encoding from Euclidean space to Hamming\nspace by a simple sign function. In order to solve these problems, a\nCross-Scale Context Extracted Hashing Network (CSCE-Net) is proposed in this\npaper. Firstly, we design a two-branch framework to capture fine-grained local\ninformation while maintaining high-level global semantic information. Besides,\nAttention guided Information Extraction module (AIE) is introduced between two\nbranches, which suppresses areas of low context information cooperated with\nglobal sliding windows. Unlike previous methods, our CSCE-Net learns a\ncontent-related Dynamic Sign Function (DSF) to replace the original simple sign\nfunction. Therefore, the proposed CSCE-Net is context-sensitive and able to\nperform well on accurate image binary encoding. We further demonstrate that our\nCSCE-Net is superior to the existing hashing methods, which improves retrieval\nperformance on standard benchmarks.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [-25.111120223999023, 4.16644811630249], "cluster": 1}, {"key": "yadav2022efficient", "year": "2022", "citations": "3", "title": "Efficient Nearest Neighbor Search For Cross-encoder Models Using Matrix Factorization", "abstract": "<p>Efficient k-nearest neighbor search is a fundamental task, foundational for\nmany problems in NLP. When the similarity is measured by dot-product between\ndual-encoder vectors or \\(\u2113\u2082\\)-distance, there already exist many scalable\nand efficient search methods. But not so when similarity is measured by more\naccurate and expensive black-box neural similarity models, such as\ncross-encoders, which jointly encode the query and candidate neighbor. The\ncross-encoders\u2019 high computational cost typically limits their use to reranking\ncandidates retrieved by a cheaper model, such as dual encoder or TF-IDF.\nHowever, the accuracy of such a two-stage approach is upper-bounded by the\nrecall of the initial candidate set, and potentially requires additional\ntraining to align the auxiliary retrieval model with the cross-encoder model.\nIn this paper, we present an approach that avoids the use of a dual-encoder for\nretrieval, relying solely on the cross-encoder. Retrieval is made efficient\nwith CUR decomposition, a matrix decomposition approach that approximates all\npairwise cross-encoder distances from a small subset of rows and columns of the\ndistance matrix. Indexing items using our approach is computationally cheaper\nthan training an auxiliary dual-encoder model through distillation.\nEmpirically, for k &gt; 10, our approach provides test-time\nrecall-vs-computational cost trade-offs superior to the current widely-used\nmethods that re-rank items retrieved using a dual-encoder or TF-IDF.</p>\n", "tags": ["Evaluation", "EMNLP"], "tsne_embedding": [23.73981475830078, 34.16912841796875], "cluster": 4}, {"key": "yadav2023efficient", "year": "2023", "citations": "0", "title": "Efficient K-nn Search With Cross-encoders Using Adaptive Multi-round CUR Decomposition", "abstract": "<p>Cross-encoder models, which jointly encode and score a query-item pair, are\nprohibitively expensive for direct k-nearest neighbor (k-NN) search.\nConsequently, k-NN search typically employs a fast approximate retrieval (e.g.\nusing BM25 or dual-encoder vectors), followed by reranking with a\ncross-encoder; however, the retrieval approximation often has detrimental\nrecall regret. This problem is tackled by ANNCUR (Yadav et al., 2022), a recent\nwork that employs a cross-encoder only, making search efficient using a\nrelatively small number of anchor items, and a CUR matrix factorization. While\nANNCUR\u2019s one-time selection of anchors tends to approximate the cross-encoder\ndistances on average, doing so forfeits the capacity to accurately estimate\ndistances to items near the query, leading to regret in the crucial end-task:\nrecall of top-k items. In this paper, we propose ADACUR, a method that\nadaptively, iteratively, and efficiently minimizes the approximation error for\nthe practically important top-k neighbors. It does so by iteratively performing\nk-NN search using the anchors available so far, then adding these retrieved\nnearest neighbors to the anchor set for the next round. Empirically, on\nmultiple datasets, in comparison to previous traditional and state-of-the-art\nmethods such as ANNCUR and dual-encoder-based retrieve-and-rerank, our proposed\napproach ADACUR consistently reduces recall error-by up to 70% on the important\nk = 1 setting-while using no more compute than its competitors.</p>\n", "tags": ["EMNLP", "Datasets", "Re-Ranking", "Evaluation"], "tsne_embedding": [23.301488876342773, 34.19575500488281], "cluster": 4}, {"key": "yadav2024adaptive", "year": "2024", "citations": "0", "title": "Adaptive Retrieval And Scalable Indexing For K-nn Search With Cross-encoders", "abstract": "<p>Cross-encoder (CE) models which compute similarity by jointly encoding a\nquery-item pair perform better than embedding-based models (dual-encoders) at\nestimating query-item relevance. Existing approaches perform k-NN search with\nCE by approximating the CE similarity with a vector embedding space fit either\nwith dual-encoders (DE) or CUR matrix factorization. DE-based\nretrieve-and-rerank approaches suffer from poor recall on new domains and the\nretrieval with DE is decoupled from the CE. While CUR-based approaches can be\nmore accurate than the DE-based approach, they require a prohibitively large\nnumber of CE calls to compute item embeddings, thus making it impractical for\ndeployment at scale. In this paper, we address these shortcomings with our\nproposed sparse-matrix factorization based method that efficiently computes\nlatent query and item embeddings to approximate CE scores and performs k-NN\nsearch with the approximate CE similarity. We compute item embeddings offline\nby factorizing a sparse matrix containing query-item CE scores for a set of\ntrain queries. Our method produces a high-quality approximation while requiring\nonly a fraction of CE calls as compared to CUR-based methods, and allows for\nleveraging DE to initialize the embedding space while avoiding compute- and\nresource-intensive finetuning of DE via distillation. At test time, the item\nembeddings remain fixed and retrieval occurs over rounds, alternating between\na) estimating the test query embedding by minimizing error in approximating CE\nscores of items retrieved thus far, and b) using the updated test query\nembedding for retrieving more items. Our k-NN search method improves recall by\nup to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our\nindexing approach achieves a speedup of up to 100x over CUR-based and 5x over\nDE distillation methods, while matching or improving k-NN search recall over\nbaselines.</p>\n", "tags": ["Re-Ranking", "Efficiency", "Evaluation", "Scalability"], "tsne_embedding": [23.376087188720703, 35.387603759765625], "cluster": 4}, {"key": "yamada2021efficient", "year": "2021", "citations": "46", "title": "Efficient Passage Retrieval With Hashing For Open-domain Question Answering", "abstract": "<p>Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes"], "tsne_embedding": [22.800872802734375, 8.745745658874512], "cluster": 6}, {"key": "yan2018norm", "year": "2018", "citations": "24", "title": "Norm-range Partition: A Universal Catalyst For LSH Based Maximum Inner Product Search (MIPS)", "abstract": "<p>Recently, locality sensitive hashing (LSH) was shown to be effective for MIPS\nand several algorithms including \\(L_2\\)-ALSH, Sign-ALSH and Simple-LSH have been\nproposed. In this paper, we introduce the norm-range partition technique, which\npartitions the original dataset into sub-datasets containing items with similar\n2-norms and builds hash index independently for each sub-dataset. We prove that\nnorm-range partition reduces the query processing complexity for all existing\nLSH based MIPS algorithms under mild conditions. The key to performance\nimprovement is that norm-range partition allows to use smaller normalization\nfactor most sub-datasets. For efficient query processing, we also formulate a\nunified framework to rank the buckets from the hash indexes of different\nsub-datasets. Experiments on real datasets show that norm-range partition\nsignificantly reduces the number of probed for LSH based MIPS algorithms when\nachieving the same recall.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [27.8187313079834, 19.279090881347656], "cluster": 2}, {"key": "yan2019deep", "year": "2019", "citations": "27", "title": "Deep Hashing By Discriminating Hard Examples", "abstract": "<p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Neural-Hashing"], "tsne_embedding": [1.5252578258514404, 21.689651489257812], "cluster": 8}, {"key": "yan2020deep", "year": "2020", "citations": "413", "title": "Deep Multi-view Enhancement Hashing For Image Retrieval", "abstract": "<p>Hashing is an efficient method for nearest neighbor search in large-scale\ndata space by embedding high-dimensional feature descriptors into a similarity\npreserving Hamming space with a low dimension. However, large-scale high-speed\nretrieval through binary code has a certain degree of reduction in retrieval\naccuracy compared to traditional retrieval methods. We have noticed that\nmulti-view methods can well preserve the diverse characteristics of data.\nTherefore, we try to introduce the multi-view deep neural network into the hash\nlearning field, and design an efficient and innovative retrieval model, which\nhas achieved a significant improvement in retrieval performance. In this paper,\nwe propose a supervised multi-view hash model which can enhance the multi-view\ninformation through neural networks. This is a completely new hash learning\nmethod that combines multi-view and deep learning methods. The proposed method\nutilizes an effective view stability evaluation method to actively explore the\nrelationship among views, which will affect the optimization direction of the\nentire network. We have also designed a variety of multi-data fusion methods in\nthe Hamming space to preserve the advantages of both convolution and\nmulti-view. In order to avoid excessive computing resources on the enhancement\nprocedure during retrieval, we set up a separate structure called memory\nnetwork which participates in training together. The proposed method is\nsystematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and\nthe results show that our method significantly outperforms the state-of-the-art\nsingle-view and multi-view hashing methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [3.1584250926971436, 4.146723747253418], "cluster": 6}, {"key": "yan2021binary", "year": "2021", "citations": "10", "title": "Binary Code Based Hash Embedding For Web-scale Applications", "abstract": "<p>Nowadays, deep learning models are widely adopted in web-scale applications\nsuch as recommender systems, and online advertising. In these applications,\nembedding learning of categorical features is crucial to the success of deep\nlearning models. In these models, a standard method is that each categorical\nfeature value is assigned a unique embedding vector which can be learned and\noptimized. Although this method can well capture the characteristics of the\ncategorical features and promise good performance, it can incur a huge memory\ncost to store the embedding table, especially for those web-scale applications.\nSuch a huge memory cost significantly holds back the effectiveness and\nusability of EDRMs. In this paper, we propose a binary code based hash\nembedding method which allows the size of the embedding table to be reduced in\narbitrary scale without compromising too much performance. Experimental\nevaluation results show that one can still achieve 99% performance even if the\nembedding table size is reduced 1000\\(\\times\\) smaller than the original one with\nour proposed method.</p>\n", "tags": ["Evaluation", "Compact-Codes", "CIKM", "Recommender-Systems", "Large-Scale-Search", "Scalability"], "tsne_embedding": [24.305997848510742, 4.177294731140137], "cluster": 6}, {"key": "yan2021hierarchical", "year": "2021", "citations": "32", "title": "Hierarchical Attention Fusion For Geo-localization", "abstract": "<p>Geo-localization is a critical task in computer vision. In this work, we cast\nthe geo-localization as a 2D image retrieval task. Current state-of-the-art\nmethods for 2D geo-localization are not robust to locate a scene with drastic\nscale variations because they only exploit features from one semantic level for\nimage representations. To address this limitation, we introduce a hierarchical\nattention fusion network using multi-scale features for geo-localization. We\nextract the hierarchical feature maps from a convolutional neural network (CNN)\nand organically fuse the extracted features for image representations. Our\ntraining is self-supervised using adaptive weights to control the attention of\nfeature emphasis from each hierarchical level. Evaluation results on the image\nretrieval and the large-scale geo-localization benchmarks indicate that our\nmethod outperforms the existing state-of-the-art methods. Code is available\nhere: https://github.com/YanLiqi/HAF.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Scalability", "ICASSP", "Supervised", "Evaluation"], "tsne_embedding": [-36.781280517578125, -12.800557136535645], "cluster": 5}, {"key": "yan2025deep", "year": "2019", "citations": "27", "title": "Deep Hashing By Discriminating Hard Examples", "abstract": "<p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Scalability", "Datasets", "Neural-Hashing"], "tsne_embedding": [1.525199294090271, 21.689605712890625], "cluster": 8}, {"key": "yang2016zero", "year": "2016", "citations": "139", "title": "Zero-shot Hashing Via Transferring Supervised Knowledge", "abstract": "<p>Hashing has shown its efficiency and effectiveness in facilitating\nlarge-scale multimedia applications. Supervised knowledge e.g. semantic labels\nor pair-wise relationship) associated to data is capable of significantly\nimproving the quality of hash codes and hash functions. However, confronted\nwith the rapid growth of newly-emerging concepts and multimedia data on the\nWeb, existing supervised hashing approaches may easily suffer from the scarcity\nand validity of supervised information due to the expensive cost of manual\nlabelling. In this paper, we propose a novel hashing scheme, termed\n<em>zero-shot hashing</em> (ZSH), which compresses images of \u201cunseen\u201d categories\nto binary codes with hash functions learned from limited training data of\n\u201cseen\u201d categories. Specifically, we project independent data labels i.e.\n0/1-form label vectors) into semantic embedding space, where semantic\nrelationships among all the labels can be precisely characterized and thus seen\nsupervised knowledge can be transferred to unseen classes. Moreover, in order\nto cope with the semantic shift problem, we rotate the embedded space to more\nsuitably align the embedded semantics with the low-level visual feature space,\nthereby alleviating the influence of semantic gap. In the meantime, to exert\npositive effects on learning high-quality hash functions, we further propose to\npreserve local structural property and discrete nature in binary codes.\nBesides, we develop an efficient alternating algorithm to solve the ZSH model.\nExtensive experiments conducted on various real-life datasets show the superior\nzero-shot image retrieval performance of ZSH as compared to several\nstate-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Efficiency", "Few-Shot-&-Zero-Shot", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [9.49520492553711, -0.7689440250396729], "cluster": 6}, {"key": "yang2017utilizing", "year": "2017", "citations": "3", "title": "Utilizing Embeddings For Ad-hoc Retrieval By Document-to-document Similarity", "abstract": "<p>Latent semantic representations of words or paragraphs, namely the\nembeddings, have been widely applied to information retrieval (IR). One of the\ncommon approaches of utilizing embeddings for IR is to estimate the\ndocument-to-query (D2Q) similarity in their embeddings. As words with similar\nsyntactic usage are usually very close to each other in the embeddings space,\nalthough they are not semantically similar, the D2Q similarity approach may\nsuffer from the problem of \u201cmultiple degrees of similarity\u201d. To this end, this\npaper proposes a novel approach that estimates a semantic relevance score (SEM)\nbased on document-to-document (D2D) similarity of embeddings. As Word or\nPara2Vec generates embeddings by the context of words/paragraphs, the D2D\nsimilarity approach turns the task of document ranking into the estimation of\nsimilarity between content within different documents. Experimental results on\nstandard TREC test collections show that our proposed approach outperforms\nstrong baselines.</p>\n", "tags": ["Uncategorized"], "tsne_embedding": [-37.62101745605469, -22.2166748046875], "cluster": 5}, {"key": "yang2018deep", "year": "2019", "citations": "17", "title": "Deep Attention-guided Hashing", "abstract": "<p>With the rapid growth of multimedia data (e.g., image, audio and video etc.)\non the web, learning-based hashing techniques such as Deep Supervised Hashing\n(DSH) have proven to be very efficient for large-scale multimedia search. The\nrecent successes seen in Learning-based hashing methods are largely due to the\nsuccess of deep learning-based hashing methods. However, there are some\nlimitations to previous learning-based hashing methods (e.g., the learned hash\ncodes containing repetitive and highly correlated information). In this paper,\nwe propose a novel learning-based hashing method, named Deep Attention-guided\nHashing (DAgH). DAgH is implemented using two stream frameworks. The core idea\nis to use guided hash codes which are generated by the hashing network of the\nfirst stream framework (called first hashing network) to guide the training of\nthe hashing network of the second stream framework (called second hashing\nnetwork). Specifically, in the first network, it leverages an attention network\nand hashing network to generate the attention-guided hash codes from the\noriginal images. The loss function we propose contains two components: the\nsemantic loss and the attention loss. The attention loss is used to punish the\nattention network to obtain the salient region from pairs of images; in the\nsecond network, these attention-guided hash codes are used to guide the\ntraining of the second hashing network (i.e., these codes are treated as\nsupervised labels to train the second network). By doing this, DAgH can make\nfull use of the most critical information contained in images to guide the\nsecond hashing network in order to learn efficient hash codes in a true\nend-to-end fashion. Results from our experiments demonstrate that DAgH can\ngenerate high quality hash codes and it outperforms current state-of-the-art\nmethods on three benchmark datasets, CIFAR-10, NUS-WIDE, and ImageNet.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [1.323330283164978, 4.752940654754639], "cluster": 6}, {"key": "yang2018efficient", "year": "2019", "citations": "49", "title": "Efficient Image Retrieval Via Decoupling Diffusion Into Online And Offline Processing", "abstract": "<p>Diffusion is commonly used as a ranking or re-ranking method in retrieval\ntasks to achieve higher retrieval performance, and has attracted lots of\nattention in recent years. A downside to diffusion is that it performs slowly\nin comparison to the naive k-NN search, which causes a non-trivial online\ncomputational cost on large datasets. To overcome this weakness, we propose a\nnovel diffusion technique in this paper. In our work, instead of applying\ndiffusion to the query, we pre-compute the diffusion results of each element in\nthe database, making the online search a simple linear combination on top of\nthe k-NN search process. Our proposed method becomes 10~ times faster in terms\nof online search speed. Moreover, we propose to use late truncation instead of\nearly truncation in previous works to achieve better retrieval performance.</p>\n", "tags": ["Image-Retrieval", "AAAI", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [34.53340148925781, 9.885598182678223], "cluster": 2}, {"key": "yang2018gb", "year": "2019", "citations": "18", "title": "GB-KMV: An Augmented KMV Sketch For Approximate Containment Similarity Search", "abstract": "<p>In this paper, we study the problem of approximate containment similarity\nsearch. Given two records Q and X, the containment similarity between Q and X\nwith respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of\nrecords S, the containment similarity search finds a set of records from S\nwhose containment similarity regarding Q are not less than the given threshold.\nThis problem has many important applications in commercial and scientific\nfields such as record matching and domain search. Existing solution relies on\nthe asymmetric LSH method by transforming the containment similarity to\nwell-studied Jaccard similarity. In this paper, we use a different framework by\ntransforming the containment similarity to set intersection. We propose a novel\naugmented KMV sketch technique, namely GB-KMV, which is data-dependent and can\nachieve a good trade-off between the sketch size and the accuracy. We provide a\nset of theoretical analysis to underpin the proposed augmented KMV sketch\ntechnique, and show that it outperforms the state-of-the-art technique LSH-E in\nterms of estimation accuracy under practical assumption. Our comprehensive\nexperiments on real-life datasets verify that GB-KMV is superior to LSH-E in\nterms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch\nconstruction time. For instance, with similar estimation accuracy (F-1 score),\nGB-KMV is over 100 times faster than LSH-E on some real-life dataset.</p>\n", "tags": ["Tools-&-Libraries", "Locality-Sensitive-Hashing", "Similarity-Search", "Datasets"], "tsne_embedding": [13.667570114135742, 39.72550964355469], "cluster": 4}, {"key": "yang20192", "year": "2019", "citations": "4", "title": "2-bit Model Compression Of Deep Convolutional Neural Network On ASIC Engine For Image Retrieval", "abstract": "<p>Image retrieval utilizes image descriptors to retrieve the most similar\nimages to a given query image. Convolutional neural network (CNN) is becoming\nthe dominant approach to extract image descriptors for image retrieval. For\nlow-power hardware implementation of image retrieval, the drawback of CNN-based\nfeature descriptor is that it requires hundreds of megabytes of storage. To\naddress this problem, this paper applies deep model quantization and\ncompression to CNN in ASIC chip for image retrieval. It is demonstrated that\nthe CNN-based features descriptor can be extracted using as few as 2-bit\nweights quantization to deliver a similar performance as floating-point model\nfor image retrieval. In addition, to implement CNN in ASIC, especially for\nlarge scale images, the limited buffer size of chips should be considered. To\nretrieve large scale images, we propose an improved pooling strategy, region\nnested invariance pooling (RNIP), which uses cropped sub-images for CNN.\nTesting results on chip show that integrating RNIP with the proposed 2-bit CNN\nmodel compression approach is capable of retrieving large scale images.</p>\n", "tags": ["Quantization", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-45.714481353759766, 0.7919595241546631], "cluster": 0}, {"key": "yang2019adaptive", "year": "2019", "citations": "8", "title": "Adaptive Labeling For Deep Learning To Hash", "abstract": "<p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary\nhash function learning approach via deep neural networks\nin this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward\nnetwork training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class\nlabel representations together with the network weights are\nupdated in the learning process. As the label representations (or referred to as codewords in this work), are learned\nfrom data, semantically similar classes will be assigned\nwith the codewords that are close to each other in terms\nof Hamming distance in the label space. The codewords\nthen serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which\ncan jointly learn label representations and infer compact\nbinary codes from data. It is applicable to both supervised\nand semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance\nof AdaLabelHash.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "CVPR", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [35.07741165161133, -11.775190353393555], "cluster": 9}, {"key": "yang2019asymmetric", "year": "2019", "citations": "5", "title": "Asymmetric Deep Semantic Quantization For Image Retrieval", "abstract": "<p>Due to its fast retrieval and storage efficiency capabilities, hashing has\nbeen widely used in nearest neighbor retrieval tasks. By using deep learning\nbased techniques, hashing can outperform non-learning based hashing technique\nin many applications. However, we argue that the current deep learning based\nhashing methods ignore some critical problems (e.g., the learned hash codes are\nnot discriminative due to the hashing methods being unable to discover rich\nsemantic information and the training strategy having difficulty optimizing the\ndiscrete binary codes). In this paper, we propose a novel image hashing method,\ntermed as \\textbf{\\underline{A}}symmetric \\textbf{\\underline{D}}eep\n\\textbf{\\underline{S}}emantic \\textbf{\\underline{Q}}uantization\n(\\textbf{ADSQ}). \\textbf{ADSQ} is implemented using three stream frameworks,\nwhich consist of one <em>LabelNet</em> and two <em>ImgNets</em>. The\n<em>LabelNet</em> leverages the power of three fully-connected layers, which are\nused to capture rich semantic information between image pairs. For the two\n<em>ImgNets</em>, they each adopt the same convolutional neural network\nstructure, but with different weights (i.e., asymmetric convolutional neural\nnetworks). The two <em>ImgNets</em> are used to generate discriminative compact\nhash codes. Specifically, the function of the <em>LabelNet</em> is to capture\nrich semantic information that is used to guide the two <em>ImgNets</em> in\nminimizing the gap between the real-continuous features and the discrete binary\ncodes. Furthermore, \\textbf{ADSQ} can utilize the most critical semantic\ninformation to guide the feature learning process and consider the consistency\nof the common semantic space and Hamming space. Experimental results on three\nbenchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the\nproposed \\textbf{ADSQ} can outperforms current state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Image-Retrieval", "Compact-Codes"], "tsne_embedding": [3.8562660217285156, 14.381234169006348], "cluster": 6}, {"key": "yang2019distillhash", "year": "2019", "citations": "149", "title": "Distillhash: Unsupervised Deep Hashing By Distilling Data Pairs", "abstract": "<p>Due to the high storage and search efficiency, hashing\nhas become prevalent for large-scale similarity search. Particularly, deep hashing methods have greatly improved the\nsearch performance under supervised scenarios. In contrast, unsupervised deep hashing models can hardly achieve\nsatisfactory performance due to the lack of reliable supervisory similarity signals.\n To address this issue, we propose\na novel deep unsupervised hashing model, dubbed DistillHash, which can learn a distilled data set consisted of data\npairs, which have confidence similarity signals. Specifically, we investigate the relationship between the initial\nnoisy similarity signals learned from local structures and\nthe semantic similarity labels assigned by a Bayes optimal\nclassifier. We show that under a mild assumption, some\ndata pairs, of which labels are consistent with those assigned by the Bayes optimal classifier, can be potentially\ndistilled. Inspired by this fact, we design a simple yet effective strategy to distill data pairs automatically and further\nadopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the\nproposed DistillHash consistently accomplishes the stateof-the-art search performance.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "CVPR", "Scalability", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [4.668554782867432, -0.6873703002929688], "cluster": 6}, {"key": "yang2019feature", "year": "2019", "citations": "22", "title": "Feature Pyramid Hashing", "abstract": "<p>In recent years, deep-networks-based hashing has become a leading approach\nfor large-scale image retrieval. Most deep hashing approaches use the high\nlayer to extract the powerful semantic representations. However, these methods\nhave limited ability for fine-grained image retrieval because the semantic\nfeatures extracted from the high layer are difficult in capturing the subtle\ndifferences. To this end, we propose a novel two-pyramid hashing architecture\nto learn both the semantic information and the subtle appearance details for\nfine-grained image search. Inspired by the feature pyramids of convolutional\nneural network, a vertical pyramid is proposed to capture the high-layer\nfeatures and a horizontal pyramid combines multiple low-layer features with\nstructural information to capture the subtle differences. To fuse the low-level\nfeatures, a novel combination strategy, called consensus fusion, is proposed to\ncapture all subtle information from several low-layers for finer retrieval.\nExtensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford\nDogs demonstrate that the proposed method achieves significant performance\ncompared with the state-of-art baselines.</p>\n", "tags": ["Datasets", "Evaluation", "Neural-Hashing", "Image-Retrieval", "Hashing-Methods", "Multimodal-Retrieval", "Scalability"], "tsne_embedding": [-9.71770191192627, 3.4561564922332764], "cluster": 1}, {"key": "yang2019learning", "year": "2020", "citations": "28", "title": "Learning Shared Semantic Space With Correlation Alignment For Cross-modal Event Retrieval", "abstract": "<p>In this paper, we propose to learn shared semantic space with correlation\nalignment (\\({S}^{3}CA\\)) for multimodal data representations, which aligns\nnonlinear correlations of multimodal data distributions in deep neural networks\ndesigned for heterogeneous data. In the context of cross-modal (event)\nretrieval, we design a neural network with convolutional layers and\nfully-connected layers to extract features for images, including images on\nFlickr-like social media. Simultaneously, we exploit a fully-connected neural\nnetwork to extract semantic features for texts, including news articles from\nnews media. In particular, nonlinear correlations of layer activations in the\ntwo neural networks are aligned with correlation alignment during the joint\ntraining of the networks. Furthermore, we project the multimodal data into a\nshared semantic space for cross-modal (event) retrieval, where the distances\nbetween heterogeneous data samples can be measured directly. In addition, we\ncontribute a Wiki-Flickr Event dataset, where the multimodal data samples are\nnot describing each other in pairs like the existing paired datasets, but all\nof them are describing semantic events. Extensive experiments conducted on both\npaired and unpaired datasets manifest the effectiveness of \\({S}^{3}CA\\),\noutperforming the state-of-the-art methods.</p>\n", "tags": ["Datasets"], "tsne_embedding": [-2.8704445362091064, -19.72890853881836], "cluster": 1}, {"key": "yang2019shared", "year": "2018", "citations": "151", "title": "Shared Predictive Cross-modal Deep Quantization", "abstract": "<p>With explosive growth of data volume and ever-increasing diversity of data\nmodalities, cross-modal similarity search, which conducts nearest neighbor\nsearch across different modalities, has been attracting increasing interest.\nThis paper presents a deep compact code learning solution for efficient\ncross-modal similarity search. Many recent studies have proven that\nquantization-based approaches perform generally better than hashing-based\napproaches on single-modal similarity search. In this paper, we propose a deep\nquantization approach, which is among the early attempts of leveraging deep\nneural networks into quantization-based cross-modal similarity search. Our\napproach, dubbed shared predictive deep quantization (SPDQ), explicitly\nformulates a shared subspace across different modalities and two private\nsubspaces for individual modalities, and representations in the shared subspace\nand the private subspaces are learned simultaneously by embedding them to a\nreproducing kernel Hilbert space, where the mean embedding of different\nmodality distributions can be explicitly compared. In addition, in the shared\nsubspace, a quantizer is learned to produce the semantics preserving compact\ncodes with the help of label alignment. Thanks to this novel network\narchitecture in cooperation with supervised quantization training, SPDQ can\npreserve intramodal and intermodal similarities as much as possible and greatly\nreduce quantization error. Experiments on two popular benchmarks corroborate\nthat our approach outperforms state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Quantization", "Similarity-Search", "Supervised"], "tsne_embedding": [-2.414443254470825, -19.1842098236084], "cluster": 1}, {"key": "yang2020camera", "year": "2020", "citations": "0", "title": "Camera-based Piano Sheet Music Identification", "abstract": "<p>This paper presents a method for large-scale retrieval of piano sheet music\nimages. Our work differs from previous studies on sheet music retrieval in two\nways. First, we investigate the problem at a much larger scale than previous\nstudies, using all solo piano sheet music images in the entire IMSLP dataset as\na searchable database. Second, we use cell phone images of sheet music as our\ninput queries, which lends itself to a practical, user-facing application. We\nshow that a previously proposed fingerprinting method for sheet music retrieval\nis far too slow for a real-time application, and we diagnose its shortcomings.\nWe propose a novel hashing scheme called dynamic n-gram fingerprinting that\nsignificantly reduces runtime while simultaneously boosting retrieval accuracy.\nIn experiments on IMSLP data, our proposed method achieves a mean reciprocal\nrank of 0.85 and an average runtime of 0.98 seconds per query.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [36.76240539550781, 12.361162185668945], "cluster": 2}, {"key": "yang2020nonlinear", "year": "2020", "citations": "22", "title": "Nonlinear Robust Discrete Hashing For Cross-modal Retrieval", "abstract": "<p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Similarity-Search", "SIGIR", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [5.985929012298584, 7.281187057495117], "cluster": 6}, {"key": "yang2020tree", "year": "2020", "citations": "123", "title": "Tree-augmented Cross-modal Encoding For Complex-query Video Retrieval", "abstract": "<p>The rapid growth of user-generated videos on the Internet has intensified the\nneed for text-based video retrieval systems. Traditional methods mainly favor\nthe concept-based paradigm on retrieval with simple queries, which are usually\nineffective for complex queries that carry far more complex semantics.\nRecently, embedding-based paradigm has emerged as a popular approach. It aims\nto map the queries and videos into a shared embedding space where\nsemantically-similar texts and videos are much closer to each other. Despite\nits simplicity, it forgoes the exploitation of the syntactic structure of text\nqueries, making it suboptimal to model the complex queries.\n  To facilitate video retrieval with complex queries, we propose a\nTree-augmented Cross-modal Encoding method by jointly learning the linguistic\nstructure of queries and the temporal representation of videos. Specifically,\ngiven a complex user query, we first recursively compose a latent semantic tree\nto structurally describe the text query. We then design a tree-augmented query\nencoder to derive structure-aware query representation and a temporal attentive\nvideo encoder to model the temporal characteristics of videos. Finally, both\nthe query and videos are mapped into a joint embedding space for matching and\nranking. In this approach, we have a better understanding and modeling of the\ncomplex queries, thereby achieving a better video retrieval performance.\nExtensive experiments on large scale video retrieval benchmark datasets\ndemonstrate the effectiveness of our approach.</p>\n", "tags": ["SIGIR", "Evaluation", "Video-Retrieval", "Datasets"], "tsne_embedding": [-39.5466194152832, -33.53839874267578], "cluster": 5}, {"key": "yang2021dolg", "year": "2021", "citations": "110", "title": "DOLG: Single-stage Image Retrieval With Deep Orthogonal Fusion Of Local And Global Features", "abstract": "<p>Image Retrieval is a fundamental task of obtaining images similar to the\nquery one from a database. A common image retrieval practice is to firstly\nretrieve candidate images via similarity search using global image features and\nthen re-rank the candidates by leveraging their local features. Previous\nlearning-based studies mainly focus on either global or local image\nrepresentation learning to tackle the retrieval task. In this paper, we abandon\nthe two-stage paradigm and seek to design an effective single-stage solution by\nintegrating local and global information inside images into compact image\nrepresentations. Specifically, we propose a Deep Orthogonal Local and Global\n(DOLG) information fusion framework for end-to-end image retrieval. It\nattentively extracts representative local information with multi-atrous\nconvolutions and self-attention at first. Components orthogonal to the global\nimage representation are then extracted from the local information. At last,\nthe orthogonal components are concatenated with the global representation as a\ncomplementary, and then aggregation is performed to generate the final\nrepresentation. The whole framework is end-to-end differentiable and can be\ntrained with image-level labels. Extensive experimental results validate the\neffectiveness of our solution and show that our model achieves state-of-the-art\nimage retrieval performances on Revisited Oxford and Paris datasets.</p>\n", "tags": ["ICCV", "Similarity-Search", "Image-Retrieval", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-28.173267364501953, -3.775911331176758], "cluster": 1}, {"key": "yang2022compact", "year": "2022", "citations": "3", "title": "Compact Token Representations With Contextual Quantization For Efficient Document Re-ranking", "abstract": "<p>Transformer based re-ranking models can achieve high search relevance through\ncontext-aware soft matching of query tokens with document tokens. To alleviate\nruntime complexity of such inference, previous work has adopted a late\ninteraction architecture with pre-computed contextual token representations at\nthe cost of a large online storage. This paper proposes contextual quantization\nof token embeddings by decoupling document-specific and document-independent\nranking contributions during codebook-based compression. This allows effective\nonline decompression and embedding composition for better search relevance.\nThis paper presents an evaluation of the above compact token representation\nmodel in terms of relevance and space efficiency.</p>\n", "tags": ["Efficiency", "Quantization", "Hybrid-Ann-Methods", "Evaluation", "Re-Ranking"], "tsne_embedding": [12.730326652526855, -22.473222732543945], "cluster": 7}, {"key": "yang2022fedhap", "year": "2023", "citations": "4", "title": "Fedhap: Federated Hashing With Global Prototypes For Cross-silo Retrieval", "abstract": "<p>Deep hashing has been widely applied in large-scale data retrieval due to its\nsuperior retrieval efficiency and low storage cost. However, data are often\nscattered in data silos with privacy concerns, so performing centralized data\nstorage and retrieval is not always possible. Leveraging the concept of\nfederated learning (FL) to perform deep hashing is a recent research trend.\nHowever, existing frameworks mostly rely on the aggregation of the local deep\nhashing models, which are trained by performing similarity learning with local\nskewed data only. Therefore, they cannot work well for non-IID clients in a\nreal federated environment. To overcome these challenges, we propose a novel\nfederated hashing framework that enables participating clients to jointly train\nthe shared deep hashing model by leveraging the prototypical hash codes for\neach class. Globally, the transmission of global prototypes with only one\nprototypical hash code per class will minimize the impact of communication cost\nand privacy risk. Locally, the use of global prototypes are maximized by\njointly training a discriminator network and the local hashing network.\nExtensive experiments on benchmark datasets are conducted to demonstrate that\nour method can significantly improve the performance of the deep hashing model\nin the federated environments with non-IID data distributions.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [37.07218551635742, 2.0705738067626953], "cluster": 9}, {"key": "yang2022transformer", "year": "2023", "citations": "6", "title": "Transformer-based Cross-modal Recipe Embeddings With Large Batch Training", "abstract": "<p>In this paper, we present a cross-modal recipe retrieval framework,\nTransformer-based Network for Large Batch Training (TNLBT), which is inspired\nby ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).\nTNLBT aims to accomplish retrieval tasks while generating images from recipe\nembeddings. We apply the Hierarchical Transformer-based recipe text encoder,\nthe Vision Transformer~(ViT)-based recipe image encoder, and an adversarial\nnetwork architecture to enable better cross-modal embedding learning for recipe\ntexts and images. In addition, we use self-supervised learning to exploit the\nrich information in the recipe texts having no corresponding images. Since\ncontrastive learning could benefit from a larger batch size according to the\nrecent literature on self-supervised learning, we adopt a large batch size\nduring training and have validated its effectiveness. In the experiments, the\nproposed framework significantly outperformed the current state-of-the-art\nframeworks in both cross-modal recipe retrieval and image generation tasks on\nthe benchmark Recipe1M. This is the first work which confirmed the\neffectiveness of large batch training on cross-modal recipe embeddings.</p>\n", "tags": ["Self-Supervised", "Robustness", "Tools-&-Libraries", "Supervised", "Evaluation"], "tsne_embedding": [-40.40079116821289, 32.65960693359375], "cluster": 0}, {"key": "yang2023atomic", "year": "2023", "citations": "4", "title": "Atomic: An Image/text Retrieval Test Collection To Support Multimedia Content Creation", "abstract": "<p>This paper presents the AToMiC (Authoring Tools for Multimedia Content)\ndataset, designed to advance research in image/text cross-modal retrieval.\nWhile vision-language pretrained transformers have led to significant\nimprovements in retrieval effectiveness, existing research has relied on\nimage-caption datasets that feature only simplistic image-text relationships\nand underspecified user models of retrieval tasks. To address the gap between\nthese oversimplified settings and real-world applications for multimedia\ncontent creation, we introduce a new approach for building retrieval test\ncollections. We leverage hierarchical structures and diverse domains of texts,\nstyles, and types of images, as well as large-scale image-document associations\nembedded in Wikipedia. We formulate two tasks based on a realistic user model\nand validate our dataset through retrieval experiments using baseline models.\nAToMiC offers a testbed for scalable, diverse, and reproducible multimedia\nretrieval research. Finally, the dataset provides the basis for a dedicated\ntrack at the 2023 Text Retrieval Conference (TREC), and is publicly available\nat https://github.com/TREC-AToMiC/AToMiC.</p>\n", "tags": ["Text-Retrieval", "Scalability", "SIGIR", "Multimodal-Retrieval", "Datasets"], "tsne_embedding": [5.293142795562744, -32.96068572998047], "cluster": 3}, {"key": "yang2023towards", "year": "2023", "citations": "60", "title": "Towards Unified Text-based Person Retrieval: A Large-scale Multi-attribute And Language Search Benchmark", "abstract": "<p>In this paper, we introduce a large Multi-Attribute and Language Search\ndataset for text-based person retrieval, called MALS, and explore the\nfeasibility of performing pre-training on both attribute recognition and\nimage-text matching tasks in one stone. In particular, MALS contains 1,510,330\nimage-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,\nand all images are annotated with 27 attributes. Considering the privacy\nconcerns and annotation costs, we leverage the off-the-shelf diffusion models\nto generate the dataset. To verify the feasibility of learning from the\ngenerated data, we develop a new joint Attribute Prompt Learning and Text\nMatching Learning (APTM) framework, considering the shared knowledge between\nattribute and text. As the name implies, APTM contains an attribute prompt\nlearning stream and a text matching learning stream. (1) The attribute prompt\nlearning leverages the attribute prompts for image-attribute alignment, which\nenhances the text matching learning. (2) The text matching learning facilitates\nthe representation learning on fine-grained details, and in turn, boosts the\nattribute prompt learning. Extensive experiments validate the effectiveness of\nthe pre-training on MALS, achieving state-of-the-art retrieval performance via\nAPTM on three challenging real-world benchmarks. In particular, APTM achieves a\nconsistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy on\nCUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-30.072284698486328, -25.109169006347656], "cluster": 5}, {"key": "yang2025adaptive", "year": "2019", "citations": "8", "title": "Adaptive Labeling For Deep Learning To Hash", "abstract": "<p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary\nhash function learning approach via deep neural networks\nin this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward\nnetwork training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class\nlabel representations together with the network weights are\nupdated in the learning process. As the label representations (or referred to as codewords in this work), are learned\nfrom data, semantically similar classes will be assigned\nwith the codewords that are close to each other in terms\nof Hamming distance in the label space. The codewords\nthen serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which\ncan jointly learn label representations and infer compact\nbinary codes from data. It is applicable to both supervised\nand semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance\nof AdaLabelHash.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "CVPR", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [35.0772819519043, -11.774967193603516], "cluster": 9}, {"key": "yang2025distillhash", "year": "2019", "citations": "149", "title": "Distillhash: Unsupervised Deep Hashing By Distilling Data Pairs", "abstract": "<p>Due to the high storage and search efficiency, hashing\nhas become prevalent for large-scale similarity search. Particularly, deep hashing methods have greatly improved the\nsearch performance under supervised scenarios. In contrast, unsupervised deep hashing models can hardly achieve\nsatisfactory performance due to the lack of reliable supervisory similarity signals.\n To address this issue, we propose\na novel deep unsupervised hashing model, dubbed DistillHash, which can learn a distilled data set consisted of data\npairs, which have confidence similarity signals. Specifically, we investigate the relationship between the initial\nnoisy similarity signals learned from local structures and\nthe semantic similarity labels assigned by a Bayes optimal\nclassifier. We show that under a mild assumption, some\ndata pairs, of which labels are consistent with those assigned by the Bayes optimal classifier, can be potentially\ndistilled. Inspired by this fact, we design a simple yet effective strategy to distill data pairs automatically and further\nadopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the\nproposed DistillHash consistently accomplishes the stateof-the-art search performance.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "CVPR", "Scalability", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [4.668418884277344, -0.6873201131820679], "cluster": 6}, {"key": "yang2025lstm", "year": "2025", "citations": "0", "title": "Lstm-based Selective Dense Text Retrieval Guided By Sparse Lexical Retrieval", "abstract": "<p>This paper studies fast fusion of dense retrieval and sparse lexical\nretrieval, and proposes a cluster-based selective dense retrieval method called\nCluSD guided by sparse lexical retrieval. CluSD takes a lightweight\ncluster-based approach and exploits the overlap of sparse retrieval results and\nembedding clusters in a two-stage selection process with an LSTM model to\nquickly identify relevant clusters while incurring limited extra memory space\noverhead. CluSD triggers partial dense retrieval and performs cluster-based\nblock disk I/O if needed. This paper evaluates CluSD and compares it with\nseveral baselines for searching in-memory and on-disk MS MARCO and BEIR\ndatasets.</p>\n", "tags": ["Text-Retrieval", "Datasets"], "tsne_embedding": [31.943586349487305, -20.895395278930664], "cluster": 7}, {"key": "yang2025nonlinear", "year": "2020", "citations": "22", "title": "Nonlinear Robust Discrete Hashing For Cross-modal Retrieval", "abstract": "<p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Similarity-Search", "SIGIR", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [5.985903739929199, 7.281176567077637], "cluster": 6}, {"key": "yao2017one", "year": "2017", "citations": "22", "title": "One-shot Fine-grained Instance Retrieval", "abstract": "<p>Fine-Grained Visual Categorization (FGVC) has achieved significant progress\nrecently. However, the number of fine-grained species could be huge and\ndynamically increasing in real scenarios, making it difficult to recognize\nunseen objects under the current FGVC framework. This raises an open issue to\nperform large-scale fine-grained identification without a complete training\nset. Aiming to conquer this issue, we propose a retrieval task named One-Shot\nFine-Grained Instance Retrieval (OSFGIR). \u201cOne-Shot\u201d denotes the ability of\nidentifying unseen objects through a fine-grained retrieval task assisted with\nan incomplete auxiliary training set. This paper first presents the detailed\ndescription to OSFGIR task and our collected OSFGIR-378K dataset. Next, we\npropose the Convolutional and Normalization Networks (CN-Nets) learned on the\nauxiliary dataset to generate a concise and discriminative representation.\nFinally, we present a coarse-to-fine retrieval framework consisting of three\ncomponents, i.e., coarse retrieval, fine-grained retrieval, and query\nexpansion, respectively. The framework progressively retrieves images with\nsimilar semantics, and performs fine-grained identification. Experiments show\nour OSFGIR framework achieves significantly better accuracy and efficiency than\nexisting FGVC and image retrieval methods, thus could be a better solution for\nlarge-scale fine-grained object identification.</p>\n", "tags": ["Efficiency", "Image-Retrieval", "Scalability", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-29.803258895874023, -0.40390148758888245], "cluster": 0}, {"key": "yao2019efficient", "year": "2019", "citations": "29", "title": "Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval", "abstract": "<p>Supervised cross-modal hashing has gained increasing research interest on\nlarge-scale retrieval task owning to its satisfactory performance and\nefficiency. However, it still has some challenging issues to be further\nstudied: 1) most of them fail to well preserve the semantic correlations in\nhash codes because of the large heterogenous gap; 2) most of them relax the\ndiscrete constraint on hash codes, leading to large quantization error and\nconsequent low performance; 3) most of them suffer from relatively high memory\ncost and computational complexity during training procedure, which makes them\nunscalable. In this paper, to address above issues, we propose a supervised\ncross-modal hashing method based on matrix factorization dubbed Efficient\nDiscrete Supervised Hashing (EDSH). Specifically, collective matrix\nfactorization on heterogenous features and semantic embedding with class labels\nare seamlessly integrated to learn hash codes. Therefore, the feature based\nsimilarities and semantic correlations can be both preserved in hash codes,\nwhich makes the learned hash codes more discriminative. Then an efficient\ndiscrete optimal algorithm is proposed to handle the scalable issue. Instead of\nlearning hash codes bit-by-bit, hash codes matrix can be obtained directly\nwhich is more efficient. Extensive experimental results on three public\nreal-world datasets demonstrate that EDSH produces a superior performance in\nboth accuracy and scalability over some existing cross-modal hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Quantization", "Scalability", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [12.028511047363281, 0.5219746828079224], "cluster": 6}, {"key": "yavuz2022segment", "year": "2022", "citations": "0", "title": "Segment Augmentation And Differentiable Ranking For Logo Retrieval", "abstract": "<p>Logo retrieval is a challenging problem since the definition of similarity is\nmore subjective compared to image retrieval tasks and the set of known\nsimilarities is very scarce. To tackle this challenge, in this paper, we\npropose a simple but effective segment-based augmentation strategy to introduce\nartificially similar logos for training deep networks for logo retrieval. In\nthis novel augmentation strategy, we first find segments in a logo and apply\ntransformations such as rotation, scaling, and color change, on the segments,\nunlike the conventional image-level augmentation strategies. Moreover, we\nevaluate whether the recently introduced ranking-based loss function,\nSmooth-AP, is a better approach for learning similarity for logo retrieval. On\nthe large scale METU Trademark Dataset, we show that (i) our segment-based\naugmentation strategy improves retrieval performance compared to the baseline\nmodel or image-level augmentation strategies, and (ii) Smooth-AP indeed\nperforms better than conventional losses for logo retrieval.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.37116241455078, -13.895464897155762], "cluster": 5}, {"key": "ye2020unsupervised", "year": "2020", "citations": "4", "title": "Unsupervised Few-bits Semantic Hashing With Implicit Topics Modeling", "abstract": "<p>Semantic hashing is a powerful paradigm for\nrepresenting texts as compact binary hash\ncodes. The explosion of short text data has\nspurred the demand of few-bits hashing. However, the performance of existing semantic\nhashing methods cannot be guaranteed when\napplied to few-bits hashing because of severe\ninformation loss. In this paper, we present a\nsimple but effective unsupervised neural generative semantic hashing method with a focus on\nfew-bits hashing. Our model is built upon variational autoencoder and represents each hash\nbit as a Bernoulli variable, which allows the\nmodel to be end-to-end trainable. To address\nthe issue of information loss, we introduce a\nset of auxiliary implicit topic vectors. With\nthe aid of these topic vectors, the generated\nhash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves\nsignificant improvements over state-of-the-art\nsemantic hashing methods in few-bits hashing.</p>\n", "tags": ["Hashing-Methods", "EMNLP", "Text-Retrieval", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [14.444937705993652, -12.961923599243164], "cluster": 7}, {"key": "ye2025unsupervised", "year": "2020", "citations": "4", "title": "Unsupervised Few-bits Semantic Hashing With Implicit Topics Modeling", "abstract": "<p>Semantic hashing is a powerful paradigm for\nrepresenting texts as compact binary hash\ncodes. The explosion of short text data has\nspurred the demand of few-bits hashing. However, the performance of existing semantic\nhashing methods cannot be guaranteed when\napplied to few-bits hashing because of severe\ninformation loss. In this paper, we present a\nsimple but effective unsupervised neural generative semantic hashing method with a focus on\nfew-bits hashing. Our model is built upon variational autoencoder and represents each hash\nbit as a Bernoulli variable, which allows the\nmodel to be end-to-end trainable. To address\nthe issue of information loss, we introduce a\nset of auxiliary implicit topic vectors. With\nthe aid of these topic vectors, the generated\nhash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves\nsignificant improvements over state-of-the-art\nsemantic hashing methods in few-bits hashing.</p>\n", "tags": ["Hashing-Methods", "EMNLP", "Text-Retrieval", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [14.444926261901855, -12.961930274963379], "cluster": 7}, {"key": "yeh2022embedding", "year": "2022", "citations": "13", "title": "Embedding Compression With Hashing For Efficient Representation Learning In Large-scale Graph", "abstract": "<p>Graph neural networks (GNNs) are deep learning models designed specifically\nfor graph data, and they typically rely on node features as the input to the\nfirst layer. When applying such a type of network on the graph without node\nfeatures, one can extract simple graph-based node features (e.g., number of\ndegrees) or learn the input node representations (i.e., embeddings) when\ntraining the network. While the latter approach, which trains node embeddings,\nmore likely leads to better performance, the number of parameters associated\nwith the embeddings grows linearly with the number of nodes. It is therefore\nimpractical to train the input node embeddings together with GNNs within\ngraphics processing unit (GPU) memory in an end-to-end fashion when dealing\nwith industrial-scale graph data. Inspired by the embedding compression methods\ndeveloped for natural language processing (NLP) tasks, we develop a node\nembedding compression method where each node is compactly represented with a\nbit vector instead of a floating-point vector. The parameters utilized in the\ncompression method can be trained together with GNNs. We show that the proposed\nnode embedding compression method achieves superior performance compared to the\nalternatives.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Scalability", "Evaluation", "KDD"], "tsne_embedding": [54.35548782348633, 2.3377346992492676], "cluster": 9}, {"key": "yesiler2020less", "year": "2020", "citations": "4", "title": "Less Is More: Faster And Better Music Version Identification With Embedding Distillation", "abstract": "<p>Version identification systems aim to detect different renditions of the same\nunderlying musical composition (loosely called cover songs). By learning to\nencode entire recordings into plain vector embeddings, recent systems have made\nsignificant progress in bridging the gap between accuracy and scalability,\nwhich has been a key challenge for nearly two decades. In this work, we propose\nto further narrow this gap by employing a set of data distillation techniques\nthat reduce the embedding dimensionality of a pre-trained state-of-the-art\nmodel. We compare a wide range of techniques and propose new ones, from\nclassical dimensionality reduction to more sophisticated distillation schemes.\nWith those, we obtain 99% smaller embeddings that, moreover, yield up to a 3%\naccuracy increase. Such small embeddings can have an important impact in\nretrieval time, up to the point of making a real-world system practical on a\nstandalone laptop.</p>\n", "tags": ["Scalability"], "tsne_embedding": [36.66733169555664, 12.373041152954102], "cluster": 2}, {"key": "yi2014deep", "year": "2014", "citations": "143", "title": "Deep Metric Learning For Practical Person Re-identification", "abstract": "<p>Various hand-crafted features and metric learning methods prevail in the\nfield of person re-identification. Compared to these methods, this paper\nproposes a more general way that can learn a similarity metric from image\npixels directly. By using a \u201csiamese\u201d deep neural network, the proposed method\ncan jointly learn the color feature, texture feature and metric in a unified\nframework. The network has a symmetry structure with two sub-networks which are\nconnected by Cosine function. To deal with the big variations of person images,\nbinomial deviance is used to evaluate the cost between similarities and labels,\nwhich is proved to be robust to outliers.\n  Compared to existing researches, a more practical setting is studied in the\nexperiments that is training and test on different datasets (cross dataset\nperson re-identification). Both in \u201cintra dataset\u201d and \u201ccross dataset\u201d\nsettings, the superiorities of the proposed method are illustrated on VIPeR and\nPRID.</p>\n", "tags": ["Tools-&-Libraries", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-17.37624740600586, -7.9417500495910645], "cluster": 1}, {"key": "yi2015binary", "year": "2015", "citations": "19", "title": "Binary Embedding: Fundamental Limits And Fast Algorithm", "abstract": "<p>Binary embedding is a nonlinear dimension reduction methodology where high\ndimensional data are embedded into the Hamming cube while preserving the\nstructure of the original space. Specifically, for an arbitrary \\(N\\) distinct\npoints in \\(\\mathbb{S}^{p-1}\\), our goal is to encode each point using\n\\(m\\)-dimensional binary strings such that we can reconstruct their geodesic\ndistance up to \\(\\delta\\) uniform distortion. Existing binary embedding\nalgorithms either lack theoretical guarantees or suffer from running time\n\\(O\\big(mp\\big)\\). We make three contributions: (1) we establish a lower bound\nthat shows any binary embedding oblivious to the set of points requires \\(m =\n\u03a9(\\frac{1}{\\delta^2}log{N})\\) bits and a similar lower bound for\nnon-oblivious embeddings into Hamming distance; (2) [DELETED, see comment]; (3)\nwe also provide an analytic result about embedding a general set of points \\(K\n\\subseteq \\mathbb{S}^{p-1}\\) with even infinite size. Our theoretical findings\nare supported through experiments on both synthetic and real data sets.</p>\n", "tags": ["Hashing-Methods"], "tsne_embedding": [11.278459548950195, 51.607486724853516], "cluster": 4}, {"key": "yim2018one", "year": "2018", "citations": "2", "title": "One-shot Item Search With Multimodal Data", "abstract": "<p>In the task of near similar image search, features from Deep Neural Network\nis often used to compare images and measure similarity. In the past, we only\nfocused visual search in image dataset without text data. However, since deep\nneural network emerged, the performance of visual search becomes high enough to\napply it in many industries from 3D data to multimodal data. Compared to the\nneeds of multimodal search, there has not been sufficient researches.\n  In this paper, we present a method of near similar search with image and text\nmultimodal dataset. Earlier time, similar image search, especially when\nsearching shopping items, treated image and text separately to search similar\nitems and reorder the results. This regards two tasks of image search and text\nmatching as two different tasks. Our method, however, explore the vast data to\ncompute k-nearest neighbors using both image and text.\n  In our experiment of similar item search, our system using multimodal data\nshows better performance than single data while it only increases minute\ncomputing time. For the experiment, we collected more than 15 million of\naccessory and six million of digital product items from online shopping\nwebsites, in which the product item comprises item images, titles, categories,\nand descriptions. Then we compare the performance of multimodal searching to\nsingle space searching in these datasets.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [42.24864196777344, 28.465341567993164], "cluster": 2}, {"key": "yin2022rapid", "year": "2022", "citations": "2", "title": "Rapid Person Re-identification Via Sub-space Consistency Regularization", "abstract": "<p>Person Re-Identification (ReID) matches pedestrians across disjoint cameras.\nExisting ReID methods adopting real-value feature descriptors have achieved\nhigh accuracy, but they are low in efficiency due to the slow Euclidean\ndistance computation as well as complex quick-sort algorithms. Recently, some\nworks propose to yield binary encoded person descriptors which instead only\nrequire fast Hamming distance computation and simple counting-sort algorithms.\nHowever, the performances of such binary encoded descriptors, especially with\nshort code (e.g., 32 and 64 bits), are hardly satisfactory given the sparse\nbinary space. To strike a balance between the model accuracy and efficiency, we\npropose a novel Sub-space Consistency Regularization (SCR) algorithm that can\nspeed up the ReID procedure by \\(0.25\\) times than real-value features under the\nsame dimensions whilst maintaining a competitive accuracy, especially under\nshort codes. SCR transforms real-value features vector (e.g., 2048 float32)\nwith short binary codes (e.g., 64 bits) by first dividing real-value features\nvector into \\(M\\) sub-spaces, each with \\(C\\) clustered centroids. Thus the\ndistance between two samples can be expressed as the summation of the\nrespective distance to the centroids, which can be sped up by offline\ncalculation and maintained via a look-up table. On the other side, these\nreal-value centroids help to achieve significantly higher accuracy than using\nbinary code. Lastly, we convert the distance look-up table to be integer and\napply the counting-sort algorithm to speed up the ranking stage.\n  We also propose a novel consistency regularization with an iterative\nframework. Experimental results on Market-1501 and DukeMTMC-reID show promising\nand exciting results. Under short code, our proposed SCR enjoys\nReal-value-level accuracy and Hashing-level speed.</p>\n", "tags": ["Compact-Codes", "Tools-&-Libraries", "Hashing-Methods", "Efficiency"], "tsne_embedding": [6.337223529815674, 23.061189651489258], "cluster": 8}, {"key": "yin2024list", "year": "2025", "citations": "0", "title": "LIST: Learning To Index Spatio-textual Data For Embedding Based Spatial Keyword Queries", "abstract": "<p>With the proliferation of spatio-textual data, Top-k KNN spatial keyword\nqueries (TkQs), which return a list of objects based on a ranking function that\nconsiders both spatial and textual relevance, have found many real-life\napplications. To efficiently handle TkQs, many indexes have been developed, but\nthe effectiveness of TkQ is limited. To improve effectiveness, several deep\nlearning models have recently been proposed, but they suffer severe efficiency\nissues and there are no efficient indexes specifically designed to accelerate\nthe top-k search process for these deep learning models. To tackle these\nissues, we consider embedding based spatial keyword queries, which capture the\nsemantic meaning of query keywords and object descriptions in two separate\nembeddings to evaluate textual relevance. Although various models can be used\nto generate these embeddings, no indexes have been specifically designed for\nsuch queries. To fill this gap, we propose LIST, a novel machine learning based\nApproximate Nearest Neighbor Search index that Learns to Index the\nSpatio-Textual data. LIST utilizes a new learning-to-cluster technique to group\nrelevant queries and objects together while separating irrelevant queries and\nobjects. There are two key challenges in building an effective and efficient\nindex, i.e., the absence of high-quality labels and the unbalanced clustering\nresults. We develop a novel pseudo-label generation technique to address the\ntwo challenges. Additionally, we introduce a learning based spatial relevance\nmodel that can integrates with various text relevance models to form a\nlightweight yet effective relevance for reranking objects retrieved by LIST.</p>\n", "tags": ["Efficiency", "Similarity-Search"], "tsne_embedding": [17.476858139038086, -14.200434684753418], "cluster": 7}, {"key": "ying2018graph", "year": "2018", "citations": "3167", "title": "Graph Convolutional Neural Networks For Web-scale Recommender Systems", "abstract": "<p>Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. However,\nmaking these methods practical and scalable to web-scale recommendation tasks\nwith billions of items and hundreds of millions of users remains a challenge.\nHere we describe a large-scale deep recommendation engine that we developed and\ndeployed at Pinterest. We develop a data-efficient Graph Convolutional Network\n(GCN) algorithm PinSage, which combines efficient random walks and graph\nconvolutions to generate embeddings of nodes (i.e., items) that incorporate\nboth graph structure as well as node feature information. Compared to prior GCN\napproaches, we develop a novel method based on highly efficient random walks to\nstructure the convolutions and design a novel training strategy that relies on\nharder-and-harder training examples to improve robustness and convergence of\nthe model. We also develop an efficient MapReduce model inference algorithm to\ngenerate embeddings using a trained model. We deploy PinSage at Pinterest and\ntrain it on 7.5 billion examples on a graph with 3 billion nodes representing\npins and boards, and 18 billion edges. According to offline metrics, user\nstudies and A/B tests, PinSage generates higher-quality recommendations than\ncomparable deep learning and graph-based alternatives. To our knowledge, this\nis the largest application of deep graph embeddings to date and paves the way\nfor a new generation of web-scale recommender systems based on graph\nconvolutional architectures.</p>\n", "tags": ["Graph-Based-Ann", "Recommender-Systems", "Scalability", "Robustness", "Large-Scale-Search", "Evaluation", "KDD"], "tsne_embedding": [55.66050338745117, -1.915888786315918], "cluster": 9}, {"key": "yingfan2021revisiting", "year": "2021", "citations": "2", "title": "Revisiting \\(k\\)-nearest Neighbor Graph Construction On High-dimensional Data : Experiments And Analyses", "abstract": "<p>The \\(k\\)-nearest neighbor graph (KNNG) on high-dimensional data is a data\nstructure widely used in many applications such as similarity search, dimension\nreduction and clustering. Due to its increasing popularity, several methods\nunder the same framework have been proposed in the past decade. This framework\ncontains two steps, i.e. building an initial KNNG (denoted as \\texttt{INIT})\nand then refining it by neighborhood propagation (denoted as \\texttt{NBPG}).\nHowever, there remain several questions to be answered. First, it lacks a\ncomprehensive experimental comparison among representative solutions in the\nliterature. Second, some recently proposed indexing structures, e.g., SW and\nHNSW, have not been used or tested for building an initial KNNG. Third, the\nrelationship between the data property and the effectiveness of \\texttt{NBPG}\nis still not clear. To address these issues, we comprehensively compare the\nrepresentative approaches on real-world high-dimensional data sets to provide\npractical and insightful suggestions for users. As the first attempt, we take\nSW and HNSW as the alternatives of \\texttt{INIT} in our experiments. Moreover,\nwe investigate the effectiveness of \\texttt{NBPG} and find the strong\ncorrelation between the huness phenomenon and the performance of \\texttt{NBPG}.</p>\n", "tags": ["Graph-Based-Ann", "Tools-&-Libraries", "Evaluation", "Similarity-Search"], "tsne_embedding": [46.57080078125, 27.03898048400879], "cluster": 2}, {"key": "yokoo2020two", "year": "2020", "citations": "20", "title": "Two-stage Discriminative Re-ranking For Large-scale Landmark Retrieval", "abstract": "<p>We propose an efficient pipeline for large-scale landmark image retrieval\nthat addresses the diversity of the dataset through two-stage discriminative\nre-ranking. Our approach is based on embedding the images in a feature-space\nusing a convolutional neural network trained with a cosine softmax loss. Due to\nthe variance of the images, which include extreme viewpoint changes such as\nhaving to retrieve images of the exterior of a landmark from images of the\ninterior, this is very challenging for approaches based exclusively on visual\nsimilarity. Our proposed re-ranking approach improves the results in two steps:\nin the sort-step, \\(k\\)-nearest neighbor search with soft-voting to sort the\nretrieved results based on their label similarity to the query images, and in\nthe insert-step, we add additional samples from the dataset that were not\nretrieved by image-similarity. This approach allows overcoming the low visual\ndiversity in retrieved images. In-depth experimental results show that the\nproposed approach significantly outperforms existing approaches on the\nchallenging Google Landmarks Datasets. Using our methods, we achieved 1st place\nin the Google Landmark Retrieval 2019 challenge and 3rd place in the Google\nLandmark Recognition 2019 challenge on Kaggle. Our code is publicly available\nhere: https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution</p>\n", "tags": ["CVPR", "Image-Retrieval", "Scalability", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods"], "tsne_embedding": [-28.916841506958008, 10.659217834472656], "cluster": 0}, {"key": "yoon2023search", "year": "2024", "citations": "0", "title": "Search-adaptor: Embedding Customization For Information Retrieval", "abstract": "<p>Embeddings extracted by pre-trained Large Language Models (LLMs) have\nsignificant potential to improve information retrieval and search. Beyond the\nzero-shot setup in which they are being conventionally used, being able to take\nadvantage of the information from the relevant query-corpus paired data can\nfurther boost the LLM capabilities. In this paper, we propose a novel method,\nSearch-Adaptor, for customizing LLMs for information retrieval in an efficient\nand robust way. Search-Adaptor modifies the embeddings generated by pre-trained\nLLMs, and can be integrated with any LLM, including those only available via\nprediction APIs. On multiple English, multilingual, and multimodal retrieval\ndatasets, we show consistent and significant performance benefits for\nSearch-Adaptor \u2013 e.g., more than 5% improvements for Google Embedding APIs in\nnDCG@10 averaged over 14 BEIR datasets.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Multimodal-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-3.2553060054779053, -30.619539260864258], "cluster": 3}, {"key": "yoon2024matryoshka", "year": "2024", "citations": "0", "title": "Matryoshka-adaptor: Unsupervised And Supervised Tuning For Smaller Embedding Dimensions", "abstract": "<p>Embeddings from Large Language Models (LLMs) have emerged as critical\ncomponents in various applications, particularly for information retrieval.\nWhile high-dimensional embeddings generally demonstrate superior performance as\nthey contain more salient information, their practical application is\nfrequently hindered by elevated computational latency and the associated higher\ncost. To address these challenges, we propose Matryoshka-Adaptor, a novel\ntuning framework designed for the customization of LLM embeddings.\nMatryoshka-Adaptor facilitates substantial dimensionality reduction while\nmaintaining comparable performance levels, thereby achieving a significant\nenhancement in computational efficiency and cost-effectiveness. Our framework\ndirectly modifies the embeddings from pre-trained LLMs which is designed to be\nseamlessly integrated with any LLM architecture, encompassing those accessible\nexclusively through black-box APIs. Also, it exhibits efficacy in both\nunsupervised and supervised learning settings. A rigorous evaluation conducted\nacross a diverse corpus of English, multilingual, and multimodal datasets\nconsistently reveals substantial gains with Matryoshka-Adaptor. Notably, with\nGoogle and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in\ndimensionality ranging from two- to twelve-fold without compromising\nperformance across multiple BEIR datasets.</p>\n", "tags": ["EMNLP", "Efficiency", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-4.090428352355957, -30.846899032592773], "cluster": 3}, {"key": "yu2014circulant", "year": "2014", "citations": "132", "title": "Circulant Binary Embedding", "abstract": "<p>Binary embedding of high-dimensional data requires\nlong codes to preserve the discriminative\npower of the input space. Traditional binary coding\nmethods often suffer from very high computation\nand storage costs in such a scenario. To\naddress this problem, we propose Circulant Binary\nEmbedding (CBE) which generates binary\ncodes by projecting the data with a circulant matrix.\nThe circulant structure enables the use of\nFast Fourier Transformation to speed up the computation.\nCompared to methods that use unstructured\nmatrices, the proposed method improves\nthe time complexity from O(d^2\n) to O(d log d),\nand the space complexity from O(d^2) to O(d)\nwhere d is the input dimensionality. We also\npropose a novel time-frequency alternating optimization\nto learn data-dependent circulant projections,\nwhich alternatively minimizes the objective\nin original and Fourier domains. We show\nby extensive experiments that the proposed approach\ngives much better performance than the\nstate-of-the-art approaches for fixed time, and\nprovides much faster computation with no performance\ndegradation for fixed number of bits.</p>\n", "tags": ["Memory-Efficiency", "Evaluation", "Hashing-Methods"], "tsne_embedding": [8.245488166809082, 41.18101119995117], "cluster": 4}, {"key": "yu2017devil", "year": "2017", "citations": "74", "title": "The Devil Is In The Middle: Exploiting Mid-level Representations For Cross-domain Instance Matching", "abstract": "<p>Many vision problems require matching images of object instances across\ndifferent domains. These include fine-grained sketch-based image retrieval\n(FG-SBIR) and Person Re-identification (person ReID). Existing approaches\nattempt to learn a joint embedding space where images from different domains\ncan be directly compared. In most cases, this space is defined by the output of\nthe final layer of a deep neural network (DNN), which primarily contains\nfeatures of a high semantic level. In this paper, we argue that both high and\nmid-level features are relevant for cross-domain instance matching (CDIM).\nImportantly, mid-level features already exist in earlier layers of the DNN.\nThey just need to be extracted, represented, and fused properly with the final\nlayer. Based on this simple but powerful idea, we propose a unified framework\nfor CDIM. Instantiating our framework for FG-SBIR and ReID, we show that our\nsimple models can easily beat the state-of-the-art models, which are often\nequipped with much more elaborate architectures.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval"], "tsne_embedding": [-17.656150817871094, -32.338497161865234], "cluster": 3}, {"key": "yu2017hyperminhash", "year": "2020", "citations": "16", "title": "Hyperminhash: Minhash In Loglog Space", "abstract": "<p>In this extended abstract, we describe and analyze a lossy compression of\nMinHash from buckets of size \\(O(log n)\\) to buckets of size \\(O(loglog n)\\) by\nencoding using floating-point notation. This new compressed sketch, which we\ncall HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a\ndrop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting\nalgorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash\nretains MinHash\u2019s features of streaming updates, unions, and cardinality\nestimation. For a multiplicative approximation error \\(1+ \\epsilon\\) on a Jaccard\nindex \\( t \\), given a random oracle, HyperMinHash needs \\(O\\left(\\epsilon^{-2}\n\\left( loglog n + log \\frac{1}{ t \\epsilon} \\right)\\right)\\) space.\nHyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on\nthe order of \\(10^{19}\\) with relative error of around 10% using 64KiB of\nmemory; MinHash can only estimate Jaccard indices for cardinalities of\n\\(10^{10}\\) with the same memory consumption.</p>\n", "tags": ["Locality-Sensitive-Hashing"], "tsne_embedding": [17.36155891418457, 50.66862869262695], "cluster": 4}, {"key": "yu2018discriminative", "year": "2019", "citations": "15", "title": "Discriminative Supervised Hashing For Cross-modal Similarity Search", "abstract": "<p>With the advantage of low storage cost and high retrieval efficiency, hashing\ntechniques have recently been an emerging topic in cross-modal similarity\nsearch. As multiple modal data reflect similar semantic content, many\nresearches aim at learning unified binary codes. However, discriminative\nhashing features learned by these methods are not adequate. This results in\nlower accuracy and robustness. We propose a novel hashing learning framework\nwhich jointly performs classifier learning, subspace learning and matrix\nfactorization to preserve class-specific semantic content, termed\nDiscriminative Supervised Hashing (DSH), to learn the discrimative unified\nbinary codes for multi-modal data. Besides, reducing the loss of information\nand preserving the non-linear structure of data, DSH non-linearly projects\ndifferent modalities into the common space in which the similarity among\nheterogeneous data points can be measured. Extensive experiments conducted on\nthe three publicly available datasets demonstrate that the framework proposed\nin this paper outperforms several state-of -the-art methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Similarity-Search", "Robustness", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [9.432372093200684, 6.1851396560668945], "cluster": 6}, {"key": "yu2018learning", "year": "2020", "citations": "4", "title": "Learning Discriminative Hashing Codes For Cross-modal Retrieval Based On Multi-view Features", "abstract": "<p>Hashing techniques have been applied broadly in retrieval tasks due to their\nlow storage requirements and high speed of processing. Many hashing methods\nbased on a single view have been extensively studied for information retrieval.\nHowever, the representation capacity of a single view is insufficient and some\ndiscriminative information is not captured, which results in limited\nimprovement. In this paper, we employ multiple views to represent images and\ntexts for enriching the feature information. Our framework exploits the\ncomplementary information among multiple views to better learn the\ndiscriminative compact hash codes. A discrete hashing learning framework that\njointly performs classifier learning and subspace learning is proposed to\ncomplete multiple search tasks simultaneously. Our framework includes two\nstages, namely a kernelization process and a quantization process.\nKernelization aims to find a common subspace where multi-view features can be\nfused. The quantization stage is designed to learn discriminative unified\nhashing codes. Extensive experiments are performed on single-label datasets\n(WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE) and the\nexperimental results indicate the superiority of our method compared with the\nstate-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [15.618152618408203, -6.934574604034424], "cluster": 6}, {"key": "yu2018modeling", "year": "2018", "citations": "44", "title": "Modeling Text With Graph Convolutional Network For Cross-modal Information Retrieval", "abstract": "<p>Cross-modal information retrieval aims to find heterogeneous data of various\nmodalities from a given query of one modality. The main challenge is to map\ndifferent modalities into a common semantic space, in which distance between\nconcepts in different modalities can be well modeled. For cross-modal\ninformation retrieval between images and texts, existing work mostly uses\noff-the-shelf Convolutional Neural Network (CNN) for image feature extraction.\nFor texts, word-level features such as bag-of-words or word2vec are employed to\nbuild deep learning models to represent texts. Besides word-level semantics,\nthe semantic relations between words are also informative but less explored. In\nthis paper, we model texts by graphs using similarity measure based on\nword2vec. A dual-path neural network model is proposed for couple feature\nlearning in cross-modal information retrieval. One path utilizes Graph\nConvolutional Network (GCN) for text modeling based on graph representations.\nThe other path uses a neural network with layers of nonlinearities for image\nmodeling based on off-the-shelf features. The model is trained by a pairwise\nsimilarity loss function to maximize the similarity of relevant text-image\npairs and minimize the similarity of irrelevant pairs. Experimental results\nshow that the proposed model outperforms the state-of-the-art methods\nsignificantly, with 17% improvement on accuracy for the best case.</p>\n", "tags": ["Evaluation"], "tsne_embedding": [48.532283782958984, -5.510652542114258], "cluster": 9}, {"key": "yu2018semi", "year": "2018", "citations": "10", "title": "Semi-supervised Hashing For Semi-paired Cross-view Retrieval", "abstract": "<p>Recently, hashing techniques have gained importance in large-scale retrieval\ntasks because of their retrieval speed. Most of the existing cross-view\nframeworks assume that data are well paired. However, the fully-paired\nmultiview situation is not universal in real applications. The aim of the\nmethod proposed in this paper is to learn the hashing function for semi-paired\ncross-view retrieval tasks. To utilize the label information of partial data,\nwe propose a semi-supervised hashing learning framework which jointly performs\nfeature extraction and classifier learning. The experimental results on two\ndatasets show that our method outperforms several state-of-the-art methods in\nterms of retrieval accuracy.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Tools-&-Libraries", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [28.158756256103516, -2.857396125793457], "cluster": 6}, {"key": "yu2018vlase", "year": "2018", "citations": "41", "title": "VLASE: Vehicle Localization By Aggregating Semantic Edges", "abstract": "<p>In this paper, we propose VLASE, a framework to use semantic edge features\nfrom images to achieve on-road localization. Semantic edge features denote edge\ncontours that separate pairs of distinct objects such as building-sky, road-\nsidewalk, and building-ground. While prior work has shown promising results by\nutilizing the boundary between prominent classes such as sky and building using\nskylines, we generalize this approach to consider semantic edge features that\narise from 19 different classes. Our localization algorithm is simple, yet very\npowerful. We extract semantic edge features using a recently introduced CASENet\narchitecture and utilize VLAD framework to perform image retrieval. Our\nexperiments show that we achieve improvement over some of the state-of-the-art\nlocalization algorithms such as SIFT-VLAD and its deep variant NetVLAD. We use\nablation study to study the importance of different semantic classes and show\nthat our unified approach achieves better performance compared to individual\nprominent features such as skylines.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-38.24207305908203, 12.295513153076172], "cluster": 0}, {"key": "yu2019unsupervised", "year": "2021", "citations": "9", "title": "Unsupervised Multi-modal Hashing For Cross-modal Retrieval", "abstract": "<p>With the advantage of low storage cost and high efficiency, hashing learning\nhas received much attention in the domain of Big Data. In this paper, we\npropose a novel unsupervised hashing learning method to cope with this open\nproblem to directly preserve the manifold structure by hashing. To address this\nproblem, both the semantic correlation in textual space and the locally\ngeometric structure in the visual space are explored simultaneously in our\nframework. Besides, the `2;1-norm constraint is imposed on the projection\nmatrices to learn the discriminative hash function for each modality. Extensive\nexperiments are performed to evaluate the proposed method on the three publicly\navailable datasets and the experimental results show that our method can\nachieve superior performance over the state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Multimodal-Retrieval", "Tools-&-Libraries", "Memory-Efficiency", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [27.661035537719727, -8.885843276977539], "cluster": 7}, {"key": "yu2020comprehensive", "year": "2020", "citations": "0", "title": "Comprehensive Graph-conditional Similarity Preserving Network For Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently.\nCurrent UCMH focuses on exploring data similarities. However, current UCMH\nmethods calculate the similarity between two data, mainly relying on the two\ndata\u2019s cross-modal features. These methods suffer from inaccurate similarity\nproblems that result in a suboptimal retrieval Hamming space, because the\ncross-modal features between the data are not sufficient to describe the\ncomplex data relationships, such as situations where two data have different\nfeature representations but share the inherent concepts. In this paper, we\ndevise a deep graph-neighbor coherence preserving network (DGCPN).\nSpecifically, DGCPN stems from graph models and explores graph-neighbor\ncoherence by consolidating the information between data and their neighbors.\nDGCPN regulates comprehensive similarity preserving losses by exploiting three\ntypes of data similarities (i.e., the graph-neighbor coherence, the coexistent\nsimilarity, and the intra- and inter-modality consistency) and designs a\nhalf-real and half-binary optimization strategy to reduce the quantization\nerrors during hashing. Essentially, DGCPN addresses the inaccurate similarity\nproblem by exploring and exploiting the data\u2019s intrinsic relationships in a\ngraph. We conduct extensive experiments on three public UCMH datasets. The\nexperimental results demonstrate the superiority of DGCPN, e.g., by improving\nthe mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit\nhashing codes to retrieve texts from images. We will release the source code\npackage and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [48.0765380859375, -3.4359817504882812], "cluster": 9}, {"key": "yu2020encode", "year": "2021", "citations": "2", "title": "Encode The Unseen: Predictive Video Hashing For Scalable Mid-stream Retrieval", "abstract": "<p>This paper tackles a new problem in computer vision: mid-stream\nvideo-to-video retrieval. This task, which consists in searching a database for\ncontent similar to a video right as it is playing, e.g. from a live stream,\nexhibits challenging characteristics. Only the beginning part of the video is\navailable as query and new frames are constantly added as the video plays out.\nTo perform retrieval in this demanding situation, we propose an approach based\non a binary encoder that is both predictive and incremental in order to (1)\naccount for the missing video content at query time and (2) keep up with\nrepeated, continuously evolving queries throughout the streaming. In\nparticular, we present the first hashing framework that infers the unseen\nfuture content of a currently playing video. Experiments on FCVID and\nActivityNet demonstrate the feasibility of this task. Our approach also yields\na significant mAP@20 performance increase compared to a baseline adapted from\nthe literature for this task, for instance 7.4% (2.6%) increase at 20% (50%) of\nelapsed runtime on FCVID using bitcodes of size 192 bits.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Tools-&-Libraries", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-12.285240173339844, -1.9468293190002441], "cluster": 1}, {"key": "yu2020retrieval", "year": "2020", "citations": "13", "title": "Retrieval Of Family Members Using Siamese Neural Network", "abstract": "<p>Retrieval of family members in the wild aims at finding family members of the\ngiven subject in the dataset, which is useful in finding the lost children and\nanalyzing the kinship. However, due to the diversity in age, gender, pose and\nillumination of the collected data, this task is always challenging. To solve\nthis problem, we propose our solution with deep Siamese neural network. Our\nsolution can be divided into two parts: similarity computation and ranking. In\ntraining procedure, the Siamese network firstly takes two candidate images as\ninput and produces two feature vectors. And then, the similarity between the\ntwo vectors is computed with several fully connected layers. While in inference\nprocedure, we try another similarity computing method by dropping the followed\nseveral fully connected layers and directly computing the cosine similarity of\nthe two feature vectors. After similarity computation, we use the ranking\nalgorithm to merge the similarity scores with the same identity and output the\nordered list according to their similarities. To gain further improvement, we\ntry different combinations of backbones, training methods and similarity\ncomputing methods. Finally, we submit the best combination as our solution and\nour team(ustc-nelslip) obtains favorable result in the track3 of the RFIW2020\nchallenge with the first runner-up, which verifies the effectiveness of our\nmethod. Our code is available at: https://github.com/gniknoil/FG2020-kinship</p>\n", "tags": ["Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-1.5354682207107544, -13.736470222473145], "cluster": 1}, {"key": "yu2020self", "year": "2020", "citations": "0", "title": "Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint", "abstract": "<p>Due to its effectivity and efficiency, deep hashing approaches are widely\nused for large-scale visual search. However, it is still challenging to produce\ncompact and discriminative hash codes for images associated with multiple\nsemantics for two main reasons, 1) similarity constraints designed in most of\nthe existing methods are based upon an oversimplified similarity\nassignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs\nsharing at least 1 label), 2) the exploration in multi-semantic relevance are\ninsufficient or even neglected in many of the existing methods. These problems\nsignificantly limit the discrimination of generated hash codes. In this paper,\nwe propose a novel self-supervised asymmetric deep hashing method with a\nmargin-scalable constraint(SADH) approach to cope with these problems. SADH\nimplements a self-supervised network to sufficiently preserve semantic\ninformation in a semantic feature dictionary and a semantic code dictionary for\nthe semantics of the given dataset, which efficiently and precisely guides a\nfeature learning network to preserve multilabel semantic information using an\nasymmetric learning strategy. By further exploiting semantic dictionaries, a\nnew margin-scalable constraint is employed for both precise similarity\nsearching and robust hash code generation. Extensive empirical research on four\npopular benchmarks validates the proposed method and shows it outperforms\nseveral state-of-the-art approaches.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Image-Retrieval", "Scalability", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [2.0888631343841553, 12.305225372314453], "cluster": 6}, {"key": "yu2021deep", "year": "2021", "citations": "136", "title": "Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data\u2019s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data\u2019s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["Hashing-Methods", "Quantization", "AAAI", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [48.04623794555664, -3.316274642944336], "cluster": 9}, {"key": "yu2021improving", "year": "2021", "citations": "44", "title": "Improving Query Representations For Dense Retrieval With Pseudo Relevance Feedback", "abstract": "<p>Dense retrieval systems conduct first-stage retrieval using embedded\nrepresentations and simple similarity metrics to match a query to documents.\nIts effectiveness depends on encoded embeddings to capture the semantics of\nqueries and documents, a challenging task due to the shortness and ambiguity of\nsearch queries. This paper proposes ANCE-PRF, a new query encoder that uses\npseudo relevance feedback (PRF) to improve query representations for dense\nretrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top\nretrieved documents from a dense retrieval model, ANCE, and it learns to\nproduce better query embeddings directly from relevance labels. It also keeps\nthe document index unchanged to reduce overhead. ANCE-PRF significantly\noutperforms ANCE and other recent dense retrieval systems on several datasets.\nAnalysis shows that the PRF encoder effectively captures the relevant and\ncomplementary information from PRF documents, while ignoring the noise with its\nlearned attention mechanism.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets", "CIKM"], "tsne_embedding": [6.041253089904785, -23.96968650817871], "cluster": 7}, {"key": "yu2022learning", "year": "2022", "citations": "2", "title": "Learning To Hash Naturally Sorts", "abstract": "<p>Learning to hash pictures a list-wise sorting problem. Its testing metrics,\ne.g., mean-average precision, count on a sorted candidate list ordered by\npair-wise code similarity. However, scarcely does one train a deep hashing\nmodel with the sorted results end-to-end because of the non-differentiable\nnature of the sorting operation. This inconsistency in the objectives of\ntraining and test may lead to sub-optimal performance since the training loss\noften fails to reflect the actual retrieval metric. In this paper, we tackle\nthis problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming\ndistances of samples\u2019 hash codes and accordingly gather their latent\nrepresentations for self-supervised training. Thanks to the recent advances in\ndifferentiable sorting approximations, the hash head receives gradients from\nthe sorter so that the hash encoder can be optimized along with the training\nprocedure. Additionally, we describe a novel Sorted Noise-Contrastive\nEstimation (SortedNCE) loss that selectively picks positive and negative\nsamples for contrastive learning, which allows NSH to mine data semantic\nrelations during training in an unsupervised manner. Our extensive experiments\nshow the proposed NSH model significantly outperforms the existing unsupervised\nhashing methods on three benchmarked datasets.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [14.26236343383789, -5.224725246429443], "cluster": 6}, {"key": "yu2022live", "year": "2023", "citations": "5", "title": "Live Laparoscopic Video Retrieval With Compressed Uncertainty", "abstract": "<p>Searching through large volumes of medical data to retrieve relevant\ninformation is a challenging yet crucial task for clinical care. However the\nprimitive and most common approach to retrieval, involving text in the form of\nkeywords, is severely limited when dealing with complex media formats.\nContent-based retrieval offers a way to overcome this limitation, by using rich\nmedia as the query itself. Surgical video-to-video retrieval in particular is a\nnew and largely unexplored research problem with high clinical value,\nespecially in the real-time case: using real-time video hashing, search can be\nachieved directly inside of the operating room. Indeed, the process of hashing\nconverts large data entries into compact binary arrays or hashes, enabling\nlarge-scale search operations at a very fast rate. However, due to fluctuations\nover the course of a video, not all bits in a given hash are equally reliable.\nIn this work, we propose a method capable of mitigating this uncertainty while\nmaintaining a light computational footprint. We present superior retrieval\nresults (3-4 % top 10 mean average precision) on a multi-task evaluation\nprotocol for surgery, using cholecystectomy phases, bypass phases, and coming\nfrom an entirely new dataset introduced here, critical events across six\ndifferent surgery types. Success on this multi-task benchmark shows the\ngeneralizability of our approach for surgical video retrieval.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Large-Scale-Search", "Datasets", "Evaluation", "Video-Retrieval"], "tsne_embedding": [-45.80983352661133, 17.999755859375], "cluster": 0}, {"key": "yu2022weighted", "year": "2022", "citations": "0", "title": "Weighted Contrastive Hashing", "abstract": "<p>The development of unsupervised hashing is advanced by the recent popular\ncontrastive learning paradigm. However, previous contrastive learning-based\nworks have been hampered by (1) insufficient data similarity mining based on\nglobal-only image representations, and (2) the hash code semantic loss caused\nby the data augmentation. In this paper, we propose a novel method, namely\nWeighted Contrative Hashing (WCH), to take a step towards solving these two\nproblems. We introduce a novel mutual attention module to alleviate the problem\nof information asymmetry in network features caused by the missing image\nstructure during contrative augmentation. Furthermore, we explore the\nfine-grained semantic relations between images, i.e., we divide the images into\nmultiple patches and calculate similarities between patches. The aggregated\nweighted similarities, which reflect the deep image relations, are distilled to\nfacilitate the hash codes learning with a distillation loss, so as to obtain\nbetter retrieval performance. Extensive experiments show that the proposed WCH\nsignificantly outperforms existing unsupervised hashing methods on three\nbenchmark datasets.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [0.09003518521785736, -1.1700118780136108], "cluster": 6}, {"key": "yu2023contextually", "year": "2023", "citations": "0", "title": "Contextually Affinitive Neighborhood Refinery For Deep Clustering", "abstract": "<p>Previous endeavors in self-supervised learning have enlightened the research\nof deep clustering from an instance discrimination perspective. Built upon this\nfoundation, recent studies further highlight the importance of grouping\nsemantically similar instances. One effective method to achieve this is by\npromoting the semantic structure preserved by neighborhood consistency.\nHowever, the samples in the local neighborhood may be limited due to their\nclose proximity to each other, which may not provide substantial and diverse\nsupervision signals. Inspired by the versatile re-ranking methods in the\ncontext of image retrieval, we propose to employ an efficient online re-ranking\nprocess to mine more informative neighbors in a Contextually Affinitive\n(ConAff) Neighborhood, and then encourage the cross-view neighborhood\nconsistency. To further mitigate the intrinsic neighborhood noises near cluster\nboundaries, we propose a progressively relaxed boundary filtering strategy to\ncircumvent the issues brought by noisy neighbors. Our method can be easily\nintegrated into the generic self-supervised frameworks and outperforms the\nstate-of-the-art methods on several popular benchmarks.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Hybrid-Ann-Methods", "Supervised", "Re-Ranking"], "tsne_embedding": [6.3644700050354, -43.0160026550293], "cluster": 3}, {"key": "yu2023learning", "year": "2015", "citations": "8", "title": "Learning Cross Space Mapping Via DNN Using Large Scale Click-through Logs", "abstract": "<p>The gap between low-level visual signals and high-level semantics has been\nprogressively bridged by continuous development of deep neural network (DNN).\nWith recent progress of DNN, almost all image classification tasks have\nachieved new records of accuracy. To extend the ability of DNN to image\nretrieval tasks, we proposed a unified DNN model for image-query similarity\ncalculation by simultaneously modeling image and query in one network. The\nunified DNN is named the cross space mapping (CSM) model, which contains two\nparts, a convolutional part and a query-embedding part. The image and query are\nmapped to a common vector space via these two parts respectively, and\nimage-query similarity is naturally defined as an inner product of their\nmappings in the space. To ensure good generalization ability of the DNN, we\nlearn weights of the DNN from a large number of click-through logs which\nconsists of 23 million clicked image-query pairs between 1 million images and\n11.7 million queries. Both the qualitative results and quantitative results on\nan image retrieval evaluation task with 1000 queries demonstrate the\nsuperiority of the proposed method.</p>\n", "tags": ["Evaluation", "Image-Retrieval"], "tsne_embedding": [-4.266237258911133, -18.264978408813477], "cluster": 1}, {"key": "yu2023pecann", "year": "2023", "citations": "1", "title": "PECANN: Parallel Efficient Clustering With Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>This paper studies density-based clustering of point sets. These methods use dense regions of points to detect clusters of arbitrary shapes. In particular, we study variants of density peaks clustering, a popular type of algorithm that has been shown to work well in practice. Our goal is to cluster large high-dimensional datasets, which are prevalent in practice. Prior solutions are either sequential, and cannot scale to large data, or are specialized for low-dimensional data.\n  This paper unifies the different variants of density peaks clustering into a single framework, PECANN, by abstracting out several key steps common to this class of algorithms. One such key step is to find nearest neighbors that satisfy a predicate function, and one of the main contributions of this paper is an efficient way to do this predicate search using graph-based approximate nearest neighbor search (ANNS). To provide ample parallelism, we propose a doubling search technique that enables points to find an approximate nearest neighbor satisfying the predicate in a small number of rounds. Our technique can be applied to many existing graph-based ANNS algorithms, which can all be plugged into PECANN.\n  We implement five clustering algorithms with PECANN and evaluate them on synthetic and real-world datasets with up to 1.28 million points and up to 1024 dimensions on a 30-core machine with two-way hyper-threading. Compared to the state-of-the-art FASTDP algorithm for high-dimensional density peaks clustering, which is sequential, our best algorithm is 45x-734x faster while achieving competitive ARI scores. Compared to the state-of-the-art parallel DPC-based algorithm, which is optimized for low dimensions, we show that PECANN is two orders of magnitude faster. As far as we know, our work is the first to evaluate DPC variants on large high-dimensional real-world image and text embedding datasets.</p>\n", "tags": ["Graph-Based-Ann", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [56.20216369628906, 10.406131744384766], "cluster": 9}, {"key": "yu2025circulant", "year": "2014", "citations": "132", "title": "Circulant Binary Embedding", "abstract": "<p>Binary embedding of high-dimensional data requires\nlong codes to preserve the discriminative\npower of the input space. Traditional binary coding\nmethods often suffer from very high computation\nand storage costs in such a scenario. To\naddress this problem, we propose Circulant Binary\nEmbedding (CBE) which generates binary\ncodes by projecting the data with a circulant matrix.\nThe circulant structure enables the use of\nFast Fourier Transformation to speed up the computation.\nCompared to methods that use unstructured\nmatrices, the proposed method improves\nthe time complexity from O(d^2\n) to O(d log d),\nand the space complexity from O(d^2) to O(d)\nwhere d is the input dimensionality. We also\npropose a novel time-frequency alternating optimization\nto learn data-dependent circulant projections,\nwhich alternatively minimizes the objective\nin original and Fourier domains. We show\nby extensive experiments that the proposed approach\ngives much better performance than the\nstate-of-the-art approaches for fixed time, and\nprovides much faster computation with no performance\ndegradation for fixed number of bits.</p>\n", "tags": ["Memory-Efficiency", "Evaluation", "Hashing-Methods"], "tsne_embedding": [8.245488166809082, 41.18101119995117], "cluster": 4}, {"key": "yu2025deep", "year": "2021", "citations": "136", "title": "Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data\u2019s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data\u2019s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["Hashing-Methods", "Quantization", "AAAI", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [48.04623794555664, -3.316274642944336], "cluster": 9}, {"key": "yuan2019central", "year": "2020", "citations": "288", "title": "Central Similarity Quantization For Efficient Image And Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn hash functions from\npairwise or triplet data relationships, which only capture the data similarity\nlocally, and often suffer from low learning efficiency and low collision rate.\nIn this work, we propose a new <em>global</em> similarity metric, termed as\n<em>central similarity</em>, with which the hash codes of similar data pairs are\nencouraged to approach a common center and those for dissimilar pairs to\nconverge to different centers, to improve hash learning efficiency and\nretrieval accuracy. We principally formulate the computation of the proposed\ncentral similarity metric by introducing a new concept, i.e., <em>hash\ncenter</em> that refers to a set of data points scattered in the Hamming space with\na sufficient mutual distance between each other. We then provide an efficient\nmethod to construct well separated hash centers by leveraging the Hadamard\nmatrix and Bernoulli distributions. Finally, we propose the Central Similarity\nQuantization (CSQ) that optimizes the central similarity between data points\nw.r.t.\\ their hash centers instead of optimizing the local similarity. CSQ is\ngeneric and applicable to both image and video hashing scenarios. Extensive\nexperiments on large-scale image and video retrieval tasks demonstrate that CSQ\ncan generate cohesive hash codes for similar data pairs and dispersed hash\ncodes for dissimilar pairs, achieving a noticeable boost in retrieval\nperformance, i.e. 3%-20% in mAP over the previous state-of-the-arts. The code\nis at: https://github.com/yuanli2333/Hadamard-Matrix-for-hashing</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Quantization", "CVPR", "Scalability", "Evaluation", "Video-Retrieval"], "tsne_embedding": [7.298595428466797, 11.20103645324707], "cluster": 6}, {"key": "yuan2019signal", "year": "2019", "citations": "81", "title": "Signal-to-noise Ratio: A Robust Distance Metric For Deep Metric Learning", "abstract": "<p>Deep metric learning, which learns discriminative features to process image\nclustering and retrieval tasks, has attracted extensive attention in recent\nyears. A number of deep metric learning methods, which ensure that similar\nexamples are mapped close to each other and dissimilar examples are mapped\nfarther apart, have been proposed to construct effective structures for loss\nfunctions and have shown promising results. In this paper, different from the\napproaches on learning the loss structures, we propose a robust SNR distance\nmetric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of\nimage pairs for deep metric learning. By exploring the properties of our SNR\ndistance metric from the view of geometry space and statistical theory, we\nanalyze the properties of our metric and show that it can preserve the semantic\nsimilarity between image pairs, which well justify its suitability for deep\nmetric learning. Compared with Euclidean distance metric, our SNR distance\nmetric can further jointly reduce the intra-class distances and enlarge the\ninter-class distances for learned features. Leveraging our SNR distance metric,\nwe propose Deep SNR-based Metric Learning (DSML) to generate discriminative\nfeature embeddings. By extensive experiments on three widely adopted\nbenchmarks, including CARS196, CUB200-2011 and CIFAR10, our DSML has shown its\nsuperiority over other state-of-the-art methods. Additionally, we extend our\nSNR distance metric to deep hashing learning, and conduct experiments on two\nbenchmarks, including CIFAR10 and NUS-WIDE, to demonstrate the effectiveness\nand generality of our SNR distance metric.</p>\n", "tags": ["CVPR", "Hashing-Methods", "Neural-Hashing", "Distance-Metric-Learning"], "tsne_embedding": [-16.690387725830078, -11.581693649291992], "cluster": 1}, {"key": "yuan2020central", "year": "2020", "citations": "288", "title": "Central Similarity Hashing For Efficient Image And Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn\nhash functions from the pairwise or triplet data relationships, which only capture the data similarity locally, and\noften suffer low learning efficiency and low collision rate.\nIn this work, we propose a new global similarity metric,\ntermed as central similarity, with which the hash codes for\nsimilar data pairs are encouraged to approach a common\ncenter and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e. hash center that refers to a set\nof data points scattered in the Hamming space with sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash\ncenters by leveraging the Hadamard matrix and Bernoulli\ndistributions. Finally, we propose the Central Similarity\nHashing (CSH) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The CSH is generic and applicable to both image and video hashing. Extensive experiments on large-scale image and video retrieval demonstrate CSH can generate cohesive hash codes for similar\ndata pairs and dispersed hash codes for dissimilar pairs,\nand achieve noticeable boost in retrieval performance, i.e.\n3%-20% in mAP over the previous state-of-the-art. The\ncodes are in: https://github.com/yuanli2333/\nHadamard-Matrix-for-hashing</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "CVPR", "Scalability", "Evaluation", "Video-Retrieval"], "tsne_embedding": [7.375890254974365, 11.069867134094238], "cluster": 6}, {"key": "yuan2021conversational", "year": "2021", "citations": "32", "title": "Conversational Fashion Image Retrieval Via Multiturn Natural Language Feedback", "abstract": "<p>We study the task of conversational fashion image retrieval via multiturn\nnatural language feedback. Most previous studies are based on single-turn\nsettings. Existing models on multiturn conversational fashion image retrieval\nhave limitations, such as employing traditional models, and leading to\nineffective performance. We propose a novel framework that can effectively\nhandle conversational fashion image retrieval with multiturn natural language\nfeedback texts. One characteristic of the framework is that it searches for\ncandidate images based on exploitation of the encoded reference image and\nfeedback text information together with the conversation history. Furthermore,\nthe image fashion attribute information is leveraged via a mutual attention\nstrategy. Since there is no existing fashion dataset suitable for the multiturn\nsetting of our task, we derive a large-scale multiturn fashion dataset via\nadditional manual annotation efforts on an existing single-turn dataset. The\nexperiments show that our proposed model significantly outperforms existing\nstate-of-the-art methods.</p>\n", "tags": ["Image-Retrieval", "Scalability", "SIGIR", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-23.73523712158203, -47.121734619140625], "cluster": 3}, {"key": "yuan2021multimodal", "year": "2021", "citations": "125", "title": "Multimodal Contrastive Training For Visual Representation Learning", "abstract": "<p>We develop an approach to learning visual representations that embraces\nmultimodal data, driven by a combination of intra- and inter-modal similarity\npreservation objectives. Unlike existing visual pre-training methods, which\nsolve a proxy prediction task in a single domain, our method exploits intrinsic\ndata properties within each modality and semantic information from cross-modal\ncorrelation simultaneously, hence improving the quality of learned visual\nrepresentations. By including multimodal training in a unified framework with\ndifferent types of contrastive losses, our method can learn more powerful and\ngeneric visual features. We first train our model on COCO and evaluate the\nlearned visual representations on various downstream tasks including image\nclassification, object detection, and instance segmentation. For example, the\nvisual representations pre-trained on COCO by our method achieve\nstate-of-the-art top-1 validation accuracy of \\(55.3%\\) on ImageNet\nclassification, under the common transfer protocol. We also evaluate our method\non the large-scale Stock images dataset and show its effectiveness on\nmulti-label image tagging, and cross-modal retrieval tasks.</p>\n", "tags": ["Distance-Metric-Learning", "CVPR", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-25.593286514282227, -22.867429733276367], "cluster": 5}, {"key": "yuan2022exploring", "year": "2021", "citations": "110", "title": "Exploring A Fine-grained Multiscale Method For Cross-modal Remote Sensing Image Retrieval", "abstract": "<p>Remote sensing (RS) cross-modal text-image retrieval has attracted extensive\nattention for its advantages of flexible input and efficient query. However,\ntraditional methods ignore the characteristics of multi-scale and redundant\ntargets in RS image, leading to the degradation of retrieval accuracy. To cope\nwith the problem of multi-scale scarcity and target redundancy in RS multimodal\nretrieval task, we come up with a novel asymmetric multimodal feature matching\nnetwork (AMFMN). Our model adapts to multi-scale feature inputs, favors\nmulti-source retrieval methods, and can dynamically filter redundant features.\nAMFMN employs the multi-scale visual self-attention (MVSA) module to extract\nthe salient features of RS image and utilizes visual features to guide the text\nrepresentation. Furthermore, to alleviate the positive samples ambiguity caused\nby the strong intraclass similarity in RS image, we propose a triplet loss\nfunction with dynamic variable margin based on prior similarity of sample\npairs. Finally, unlike the traditional RS image-text dataset with coarse text\nand higher intraclass similarity, we construct a fine-grained and more\nchallenging Remote sensing Image-Text Match dataset (RSITMD), which supports RS\nimage retrieval through keywords and sentence separately and jointly.\nExperiments on four RS text-image datasets demonstrate that the proposed model\ncan achieve state-of-the-art performance in cross-modal RS text-image retrieval\ntask.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning", "Image-Retrieval", "Datasets"], "tsne_embedding": [-22.619813919067383, 4.194581508636475], "cluster": 1}, {"key": "yuan2023semantic", "year": "2023", "citations": "13", "title": "Semantic-aware Adversarial Training For Reliable Deep Hashing Retrieval", "abstract": "<p>Deep hashing has been intensively studied and successfully applied in\nlarge-scale image retrieval systems due to its efficiency and effectiveness.\nRecent studies have recognized that the existence of adversarial examples poses\na security threat to deep hashing models, that is, adversarial vulnerability.\nNotably, it is challenging to efficiently distill reliable semantic\nrepresentatives for deep hashing to guide adversarial learning, and thereby it\nhinders the enhancement of adversarial robustness of deep hashing-based\nretrieval models. Moreover, current researches on adversarial training for deep\nhashing are hard to be formalized into a unified minimax structure. In this\npaper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the\nadversarial robustness of deep hashing models. Specifically, we conceive a\ndiscriminative mainstay features learning (DMFL) scheme to construct semantic\nrepresentatives for guiding adversarial learning in deep hashing. Particularly,\nour DMFL with the strict theoretical guarantee is adaptively optimized in a\ndiscriminative learning manner, where both discriminative and semantic\nproperties are jointly considered. Moreover, adversarial examples are\nfabricated by maximizing the Hamming distance between the hash codes of\nadversarial samples and mainstay features, the efficacy of which is validated\nin the adversarial attack trials. Further, we, for the first time, formulate\nthe formalized adversarial training of deep hashing into a unified minimax\noptimization under the guidance of the generated mainstay codes. Extensive\nexperiments on benchmark datasets show superb attack performance against the\nstate-of-the-art algorithms, meanwhile, the proposed adversarial training can\neffectively eliminate adversarial perturbations for trustworthy deep\nhashing-based retrieval. Our code is available at\nhttps://github.com/xandery-geek/SAAT.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Scalability", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [2.7562458515167236, 16.778484344482422], "cluster": 8}, {"key": "yuan2025central", "year": "2020", "citations": "288", "title": "Central Similarity Quantization For Efficient Image And Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships, which only capture the data similarity locally, and often suffer from low learning efficiency and low collision rate. In this work, we propose a new global similarity metric, termed as central similarity, with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e., hash center that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally, we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs, achieving a noticeable boost in retrieval performance, i.e. 3%-20% in mAP over the previous state-of-the-arts.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Quantization", "CVPR", "Scalability", "Evaluation", "Video-Retrieval"], "tsne_embedding": [7.39821720123291, 11.0286865234375], "cluster": 6}, {"key": "yuqi20212nd", "year": "2021", "citations": "0", "title": "2nd Place Solution To Google Landmark Retrieval 2021", "abstract": "<p>This paper presents the 2nd place solution to the Google Landmark Retrieval\n2021 Competition on Kaggle. The solution is based on a baseline with training\ntricks from person re-identification, a continent-aware sampling strategy is\npresented to select training images according to their country tags and a\nLandmark-Country aware reranking is proposed for the retrieval task. With these\ncontributions, we achieve 0.52995 mAP@100 on private leaderboard. Code\navailable at\nhttps://github.com/WesleyZhang1991/Google_Landmark_Retrieval_2021_2nd_Place_Solution</p>\n", "tags": ["Evaluation"], "tsne_embedding": [-13.539804458618164, -57.04781723022461], "cluster": 3}, {"key": "zadeh2012dimension", "year": "2013", "citations": "53", "title": "Dimension Independent Similarity Computation", "abstract": "<p>We present a suite of algorithms for Dimension Independent Similarity\nComputation (DISCO) to compute all pairwise similarities between very high\ndimensional sparse vectors. All of our results are provably independent of\ndimension, meaning apart from the initial cost of trivially reading in the\ndata, all subsequent operations are independent of the dimension, thus the\ndimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard\nsimilarity measures. For Jaccard similiarity we include an improved version of\nMinHash. Our results are geared toward the MapReduce framework. We empirically\nvalidate our theorems at large scale using data from the social networking site\nTwitter. At time of writing, our algorithms are live in production at\ntwitter.com.</p>\n", "tags": ["Tools-&-Libraries", "Locality-Sensitive-Hashing"], "tsne_embedding": [21.42609214782715, 19.989643096923828], "cluster": 2}, {"key": "zamani2023multivariate", "year": "2023", "citations": "4", "title": "Multivariate Representation Learning For Information Retrieval", "abstract": "<p>Dense retrieval models use bi-encoder network architectures for learning\nquery and document representations. These representations are often in the form\nof a vector representation and their similarities are often computed using the\ndot product function. In this paper, we propose a new representation learning\nframework for dense retrieval. Instead of learning a vector for each query and\ndocument, our framework learns a multivariate distribution and uses negative\nmultivariate KL divergence to compute the similarity between distributions. For\nsimplicity and efficiency reasons, we assume that the distributions are\nmultivariate normals and then train large language models to produce mean and\nvariance vectors for these distributions. We provide a theoretical foundation\nfor the proposed framework and show that it can be seamlessly integrated into\nthe existing approximate nearest neighbor algorithms to perform retrieval\nefficiently. We conduct an extensive suite of experiments on a wide range of\ndatasets, and demonstrate significant improvements compared to competitive\ndense retrieval models.</p>\n", "tags": ["Efficiency", "SIGIR", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [0.9624283313751221, -18.9940128326416], "cluster": 7}, {"key": "zaratiana2021contrastive", "year": "2021", "citations": "1", "title": "Contrastive String Representation Learning Using Synthetic Data", "abstract": "<p>String representation Learning (SRL) is an important task in the field of\nNatural Language Processing, but it remains under-explored. The goal of SRL is\nto learn dense and low-dimensional vectors (or embeddings) for encoding\ncharacter sequences. The learned representation from this task can be used in\nmany downstream application tasks such as string similarity matching or lexical\nnormalization. In this paper, we propose a new method for to train a SRL model\nby only using synthetic data. Our approach makes use of Contrastive Learning in\norder to maximize similarity between related strings while minimizing it for\nunrelated strings. We demonstrate the effectiveness of our approach by\nevaluating the learned representation on the task of string similarity\nmatching. Codes, data and pretrained models will be made publicly available.</p>\n", "tags": ["Self-Supervised"], "tsne_embedding": [14.220362663269043, -16.495410919189453], "cluster": 7}, {"key": "zeighami2024nudge", "year": "2024", "citations": "0", "title": "NUDGE: Lightweight Non-parametric Fine-tuning Of Embeddings For Retrieval", "abstract": "<p>\\(k\\)-Nearest Neighbor search on dense vector embeddings (\\(k\\)-NN retrieval)\nfrom pre-trained embedding models is the predominant retrieval method for text\nand images, as well as Retrieval-Augmented Generation (RAG) pipelines. In\npractice, application developers often fine-tune the embeddings to improve\ntheir accuracy on the dataset and query workload in hand. Existing approaches\neither fine-tune the pre-trained model itself or, more efficiently, but at the\ncost of accuracy, train adaptor models to transform the output of the\npre-trained model. We present NUDGE, a family of novel non-parametric embedding\nfine-tuning approaches that are significantly more accurate and efficient than\nboth sets of existing approaches. NUDGE directly modifies the embeddings of\ndata records to maximize the accuracy of \\(k\\)-NN retrieval. We present a\nthorough theoretical and experimental study of NUDGE\u2019s non-parametric approach.\nWe show that even though the underlying problem is NP-Hard, constrained\nvariations can be solved efficiently. These constraints additionally ensure\nthat the changes to the embeddings are modest, avoiding large distortions to\nthe semantics learned during pre-training. In experiments across five\npre-trained models and nine standard text and image retrieval datasets, NUDGE\nruns in minutes and often improves NDCG@10 by more than 10% over existing\nfine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase\nin accuracy and runs 200x and 3x faster, respectively, over fine-tuning the\npre-trained model and training adaptors.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [9.399916648864746, 19.232770919799805], "cluster": 6}, {"key": "zemene2017large", "year": "2018", "citations": "38", "title": "Large-scale Image Geo-localization Using Dominant Sets", "abstract": "<p>This paper presents a new approach for the challenging problem of\ngeo-locating an image using image matching in a structured database of\ncity-wide reference images with known GPS coordinates. We cast the\ngeo-localization as a clustering problem on local image features. Akin to\nexisting approaches on the problem, our framework builds on low-level features\nwhich allow partial matching between images. For each local feature in the\nquery image, we find its approximate nearest neighbors in the reference set.\nNext, we cluster the features from reference images using Dominant Set\nclustering, which affords several advantages over existing approaches. First,\nit permits variable number of nodes in the cluster which we use to dynamically\nselect the number of nearest neighbors (typically coming from multiple\nreference images) for each query feature based on its discrimination value.\nSecond, as we also quantify in our experiments, this approach is several orders\nof magnitude faster than existing approaches. Thus, we obtain multiple clusters\n(different local maximizers) and obtain a robust final solution to the problem\nusing multiple weak solutions through constrained Dominant Set clustering on\nglobal image features, where we enforce the constraint that the query image\nmust be included in the cluster. This second level of clustering also bypasses\nheuristic approaches to voting and selecting the reference image that matches\nto the query. We evaluated the proposed framework on an existing dataset of\n102k street view images as well as a new dataset of 300k images, and show that\nit outperforms the state-of-the-art by 20% and 7%, respectively, on the two\ndatasets.</p>\n", "tags": ["Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [29.356101989746094, 35.876121520996094], "cluster": 4}, {"key": "zeng2019audio", "year": "2019", "citations": "0", "title": "Audio-visual Embedding For Cross-modal Musicvideo Retrieval Through Supervised Deep CCA", "abstract": "<p>Deep learning has successfully shown excellent performance in learning joint\nrepresentations between different data modalities. Unfortunately, little\nresearch focuses on cross-modal correlation learning where temporal structures\nof different data modalities, such as audio and video, should be taken into\naccount. Music video retrieval by given musical audio is a natural way to\nsearch and interact with music contents. In this work, we study cross-modal\nmusic video retrieval in terms of emotion similarity. Particularly, audio of an\narbitrary length is used to retrieve a longer or full-length music video. To\nthis end, we propose a novel audio-visual embedding algorithm by Supervised\nDeep CanonicalCorrelation Analysis (S-DCCA) that projects audio and video into\na shared space to bridge the semantic gap between audio and video. This also\npreserves the similarity between audio and visual contents from different\nvideos with the same class label and the temporal structure. The contribution\nof our approach is mainly manifested in the two aspects: i) We propose to\nselect top k audio chunks by attention-based Long Short-Term Memory\n(LSTM)model, which can represent good audio summarization with local\nproperties. ii) We propose an end-to-end deep model for cross-modal\naudio-visual learning where S-DCCA is trained to learn the semantic correlation\nbetween audio and visual modalities. Due to the lack of music video dataset, we\nconstruct 10K music video dataset from YouTube 8M dataset. Some promising\nresults such as MAP and precision-recall show that our proposed model can be\napplied to music video retrieval.</p>\n", "tags": ["Supervised", "Evaluation", "Video-Retrieval", "Datasets"], "tsne_embedding": [3.2447383403778076, -48.18698501586914], "cluster": 3}, {"key": "zeng2019modal", "year": "2019", "citations": "0", "title": "Modal-aware Features For Multimodal Hashing", "abstract": "<p>Many retrieval applications can benefit from multiple modalities, e.g., text\nthat contains images on Wikipedia, for which how to represent multimodal data\nis the critical component. Most deep multimodal learning methods typically\ninvolve two steps to construct the joint representations: 1) learning of\nmultiple intermediate features, with each intermediate feature corresponding to\na modality, using separate and independent deep models; 2) merging the\nintermediate features into a joint representation using a fusion strategy.\nHowever, in the first step, these intermediate features do not have previous\nknowledge of each other and cannot fully exploit the information contained in\nthe other modalities. In this paper, we present a modal-aware operation as a\ngeneric building block to capture the non-linear dependences among the\nheterogeneous intermediate features that can learn the underlying correlation\nstructures in other multimodal data as soon as possible. The modal-aware\noperation consists of a kernel network and an attention network. The kernel\nnetwork is utilized to learn the non-linear relationships with other\nmodalities. Then, to learn better representations for binary hash codes, we\npresent an attention network that finds the informative regions of these\nmodal-aware features that are favorable for retrieval. Experiments conducted on\nthree public benchmark datasets demonstrate significant improvements in the\nperformance of our method relative to state-of-the-art methods.</p>\n", "tags": ["Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [14.340709686279297, -7.910396099090576], "cluster": 6}, {"key": "zeng2019simultaneous", "year": "2019", "citations": "5", "title": "Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval", "abstract": "<p>Fine-grained image hashing is a challenging problem due to the difficulties\nof discriminative region localization and hash code generation. Most existing\ndeep hashing approaches solve the two tasks independently. While these two\ntasks are correlated and can reinforce each other. In this paper, we propose a\ndeep fine-grained hashing to simultaneously localize the discriminative regions\nand generate the efficient binary codes. The proposed approach consists of a\nregion localization module and a hash coding module. The region localization\nmodule aims to provide informative regions to the hash coding module. The hash\ncoding module aims to generate effective binary codes and give feedback for\nlearning better localizer. Moreover, to better capture subtle differences,\nmulti-scale regions at different layers are learned without the need of\nbounding-box/part annotations. Extensive experiments are conducted on two\npublic benchmark fine-grained datasets. The results demonstrate significant\nimprovements in the performance of our method relative to other fine-grained\nhashing algorithms.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-8.85530948638916, 19.030563354492188], "cluster": 8}, {"key": "zeng2021phpq", "year": "2021", "citations": "0", "title": "PHPQ: Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval", "abstract": "<p>Deep hashing approaches, including deep quantization and deep binary hashing,\nhave become a common solution to large-scale image retrieval due to their high\ncomputation and storage efficiency. Most existing hashing methods cannot\nproduce satisfactory results for fine-grained retrieval, because they usually\nadopt the outputs of the last CNN layer to generate binary codes. Since deeper\nlayers tend to summarize visual clues, e.g., texture, into abstract semantics,\ne.g., dogs and cats, the feature produced by the last CNN layer is less\neffective in capturing subtle but discriminative visual details that mostly\nexist in shallow layers. To improve fine-grained image hashing, we propose\nPyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid\nHybrid Pooling (PHP) module to capture and preserve fine-grained semantic\ninformation from multi-level features, which emphasizes the subtle\ndiscrimination of different sub-categories. Besides, we propose a learnable\nquantization module with a partial codebook attention mechanism, which helps to\noptimize the most relevant codewords and improves the quantization.\nComprehensive experiments on two widely-used public benchmarks, i.e.,\nCUB-200-2011 and Stanford Dogs, demonstrate that PHPQ outperforms\nstate-of-the-art methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Quantization", "Scalability", "Image-Retrieval", "Compact-Codes", "Neural-Hashing"], "tsne_embedding": [-4.002079010009766, 8.755573272705078], "cluster": 8}, {"key": "zeng2023cascading", "year": "2023", "citations": "2", "title": "Cascading Hierarchical Networks With Multi-task Balanced Loss For Fine-grained Hashing", "abstract": "<p>With the explosive growth in the number of fine-grained images in the\nInternet era, it has become a challenging problem to perform fast and efficient\nretrieval from large-scale fine-grained images. Among the many retrieval\nmethods, hashing methods are widely used due to their high efficiency and small\nstorage space occupation. Fine-grained hashing is more challenging than\ntraditional hashing problems due to the difficulties such as low inter-class\nvariances and high intra-class variances caused by the characteristics of\nfine-grained images. To improve the retrieval accuracy of fine-grained hashing,\nwe propose a cascaded network to learn compact and highly semantic hash codes,\nand introduce an attention-guided data augmentation method. We refer to this\nnetwork as a cascaded hierarchical data augmentation network. We also propose a\nnovel approach to coordinately balance the loss of multi-task learning. We do\nextensive experiments on some common fine-grained visual classification\ndatasets. The experimental results demonstrate that our proposed method\noutperforms several state-of-art hashing methods and can effectively improve\nthe accuracy of fine-grained retrieval. The source code is publicly available:\nhttps://github.com/kaiba007/FG-CNET.</p>\n", "tags": ["Efficiency", "Hashing-Methods", "Scalability", "Datasets"], "tsne_embedding": [0.19876977801322937, 0.6027765274047852], "cluster": 6}, {"key": "zeng2024candy", "year": "2024", "citations": "0", "title": "CANDY: A Benchmark For Continuous Approximate Nearest Neighbor Search With Dynamic Data Ingestion", "abstract": "<p>Approximate K Nearest Neighbor (AKNN) algorithms play a pivotal role in\nvarious AI applications, including information retrieval, computer vision, and\nnatural language processing. Although numerous AKNN algorithms and benchmarks\nhave been developed recently to evaluate their effectiveness, the dynamic\nnature of real-world data presents significant challenges that existing\nbenchmarks fail to address. Traditional benchmarks primarily assess retrieval\neffectiveness in static contexts and often overlook update efficiency, which is\ncrucial for handling continuous data ingestion. This limitation results in an\nincomplete assessment of an AKNN algorithms ability to adapt to changing data\npatterns, thereby restricting insights into their performance in dynamic\nenvironments. To address these gaps, we introduce CANDY, a benchmark tailored\nfor Continuous Approximate Nearest Neighbor Search with Dynamic Data Ingestion.\nCANDY comprehensively assesses a wide range of AKNN algorithms, integrating\nadvanced optimizations such as machine learning-driven inference to supplant\ntraditional heuristic scans, and improved distance computation methods to\nreduce computational overhead. Our extensive evaluations across diverse\ndatasets demonstrate that simpler AKNN baselines often surpass more complex\nalternatives in terms of recall and latency. These findings challenge\nestablished beliefs about the necessity of algorithmic complexity for high\nperformance. Furthermore, our results underscore existing challenges and\nilluminate future research opportunities. We have made the datasets and\nimplementation methods available at: https://github.com/intellistream/candy.</p>\n", "tags": ["Efficiency", "Evaluation", "Datasets"], "tsne_embedding": [33.58365249633789, 24.295333862304688], "cluster": 2}, {"key": "zerveas2021coder", "year": "2022", "citations": "6", "title": "CODER: An Efficient Framework For Improving Retrieval Through Contextual Document Embedding Reranking", "abstract": "<p>Contrastive learning has been the dominant approach to training dense\nretrieval models. In this work, we investigate the impact of ranking context -\nan often overlooked aspect of learning dense retrieval models. In particular,\nwe examine the effect of its constituent parts: jointly scoring a large number\nof negatives per query, using retrieved (query-specific) instead of random\nnegatives, and a fully list-wise loss. To incorporate these factors into\ntraining, we introduce Contextual Document Embedding Reranking (CODER), a\nhighly efficient retrieval framework. When reranking, it incurs only a\nnegligible computational overhead on top of a first-stage method at run time\n(delay per query in the order of milliseconds), allowing it to be easily\ncombined with any state-of-the-art dual encoder method. After fine-tuning\nthrough CODER, which is a lightweight and fast process, models can also be used\nas stand-alone retrievers. Evaluating CODER in a large set of experiments on\nthe MS~MARCO and TripClick collections, we show that the contextual reranking\nof precomputed document embeddings leads to a significant improvement in\nretrieval performance. This improvement becomes even more pronounced when more\nrelevance information per query is available, shown in the TripClick\ncollection, where we establish new state-of-the-art results by a large margin.</p>\n", "tags": ["Self-Supervised", "EMNLP", "Similarity-Search", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [28.341228485107422, 9.3660888671875], "cluster": 2}, {"key": "zerveas2023enhancing", "year": "2023", "citations": "0", "title": "Enhancing The Ranking Context Of Dense Retrieval Methods Through Reciprocal Nearest Neighbors", "abstract": "<p>Sparse annotation poses persistent challenges to training dense retrieval\nmodels; for example, it distorts the training signal when unlabeled relevant\ndocuments are used spuriously as negatives in contrastive learning. To\nalleviate this problem, we introduce evidence-based label smoothing, a novel,\ncomputationally efficient method that prevents penalizing the model for\nassigning high relevance to false negatives. To compute the target relevance\ndistribution over candidate documents within the ranking context of a given\nquery, we assign a non-zero relevance probability to those candidates most\nsimilar to the ground truth based on the degree of their similarity to the\nground-truth document(s).\n  To estimate relevance we leverage an improved similarity metric based on\nreciprocal nearest neighbors, which can also be used independently to rerank\ncandidates in post-processing. Through extensive experiments on two large-scale\nad hoc text retrieval datasets, we demonstrate that reciprocal nearest\nneighbors can improve the ranking effectiveness of dense retrieval models, both\nwhen used for label smoothing, as well as for reranking. This indicates that by\nconsidering relationships between documents and queries beyond simple geometric\ndistance we can effectively enhance the ranking context.</p>\n", "tags": ["Self-Supervised", "Distance-Metric-Learning", "Text-Retrieval", "Scalability", "Datasets", "Re-Ranking"], "tsne_embedding": [-27.587881088256836, 24.207599639892578], "cluster": 8}, {"key": "zhai2018classification", "year": "2018", "citations": "132", "title": "Classification Is A Strong Baseline For Deep Metric Learning", "abstract": "<p>Deep metric learning aims to learn a function mapping image pixels to\nembedding feature vectors that model the similarity between images. Two major\napplications of metric learning are content-based image retrieval and face\nverification. For the retrieval tasks, the majority of current state-of-the-art\n(SOTA) approaches are triplet-based non-parametric training. For the face\nverification tasks, however, recent SOTA approaches have adopted\nclassification-based parametric training. In this paper, we look into the\neffectiveness of classification based approaches on image retrieval datasets.\nWe evaluate on several standard retrieval datasets such as CAR-196,\nCUB-200-2011, Stanford Online Product, and In-Shop datasets for image retrieval\nand clustering, and establish that our classification-based approach is\ncompetitive across different feature dimensions and base feature networks. We\nfurther provide insights into the performance effects of subsampling classes\nfor scalable classification-based training, and the effects of binarization,\nenabling efficient storage and computation for practical applications.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-16.34037208557129, -28.751813888549805], "cluster": 3}, {"key": "zhai2019learning", "year": "2019", "citations": "33", "title": "Learning A Unified Embedding For Visual Search At Pinterest", "abstract": "<p>At Pinterest, we utilize image embeddings throughout our search and\nrecommendation systems to help our users navigate through visual content by\npowering experiences like browsing of related content and searching for exact\nproducts for shopping. In this work we describe a multi-task deep metric\nlearning system to learn a single unified image embedding which can be used to\npower our multiple visual search products. The solution we present not only\nallows us to train for multiple application objectives in a single deep neural\nnetwork architecture, but takes advantage of correlated information in the\ncombination of all training data from each application to generate a unified\nembedding that outperforms all specialized embeddings previously deployed for\neach product. We discuss the challenges of handling images from different\ndomains such as camera photos, high quality web images, and clean product\ncatalog images. We also detail how to jointly train for multiple product\nobjectives and how to leverage both engagement data and human labeled data. In\naddition, our trained embeddings can also be binarized for efficient storage\nand retrieval without compromising precision and recall. Through comprehensive\nevaluations on offline metrics, user studies, and online A/B experiments, we\ndemonstrate that our proposed unified embedding improves both relevance and\nengagement of our visual search products for both browsing and searching\npurposes when compared to existing specialized embeddings. Finally, the\ndeployment of the unified embedding at Pinterest has drastically reduced the\noperation and engineering cost of maintaining multiple embeddings while\nimproving quality.</p>\n", "tags": ["Recommender-Systems", "Image-Retrieval", "Compact-Codes", "Evaluation", "KDD"], "tsne_embedding": [-16.616558074951172, -29.087987899780273], "cluster": 3}, {"key": "zhan2020repbert", "year": "2020", "citations": "60", "title": "Repbert: Contextualized Text Embeddings For First-stage Retrieval", "abstract": "<p>Although exact term match between queries and documents is the dominant\nmethod to perform first-stage retrieval, we propose a different approach,\ncalled RepBERT, to represent documents and queries with fixed-length\ncontextualized embeddings. The inner products of query and document embeddings\nare regarded as relevance scores. On MS MARCO Passage Ranking task, RepBERT\nachieves state-of-the-art results among all initial retrieval techniques. And\nits efficiency is comparable to bag-of-words methods.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [5.6637701988220215, -24.572885513305664], "cluster": 7}, {"key": "zhan2020weakly", "year": "2021", "citations": "9", "title": "Weakly-supervised Online Hashing", "abstract": "<p>With the rapid development of social websites, recent years have witnessed an\nexplosive growth of social images with user-provided tags which continuously\narrive in a streaming fashion. Due to the fast query speed and low storage\ncost, hashing-based methods for image search have attracted increasing\nattention. However, existing hashing methods for social image retrieval are\nbased on batch mode which violates the nature of social images, i.e., social\nimages are usually generated periodically or collected in a stream fashion.\nAlthough there exist many online image hashing methods, they either adopt\nunsupervised learning which ignore the relevant tags, or are designed in the\nsupervised manner which needs high-quality labels. In this paper, to overcome\nthe above limitations, we propose a new method named Weakly-supervised Online\nHashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak\nsupervision by considering the semantics of tags and removing the noise.\nBesides, We develop a discrete online optimization algorithm for WOH, which is\nefficient and scalable. Extensive experiments conducted on two real-world\ndatasets demonstrate the superiority of WOH compared with several\nstate-of-the-art hashing baselines.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [6.013611316680908, -5.6576714515686035], "cluster": 6}, {"key": "zhan2021jointly", "year": "2021", "citations": "58", "title": "Jointly Optimizing Query Encoder And Product Quantization To Improve Retrieval Performance", "abstract": "<p>Recently, Information Retrieval community has witnessed fast-paced advances\nin Dense Retrieval (DR), which performs first-stage retrieval with\nembedding-based search. Despite the impressive ranking performance, previous\nstudies usually adopt brute-force search to acquire candidates, which is\nprohibitive in practical Web search scenarios due to its tremendous memory\nusage and time cost. To overcome these problems, vector compression methods\nhave been adopted in many practical embedding-based retrieval applications. One\nof the most popular methods is Product Quantization (PQ). However, although\nexisting vector compression methods including PQ can help improve the\nefficiency of DR, they incur severely decayed retrieval performance due to the\nseparation between encoding and compression. To tackle this problem, we present\nJPQ, which stands for Joint optimization of query encoding and Product\nQuantization. It trains the query encoder and PQ index jointly in an end-to-end\nmanner based on three optimization strategies, namely ranking-oriented loss, PQ\ncentroid optimization, and end-to-end negative sampling. We evaluate JPQ on two\npublicly available retrieval benchmarks. Experimental results show that JPQ\nsignificantly outperforms popular vector compression methods. Compared with\nprevious DR models that use brute-force search, JPQ almost matches the best\nretrieval performance with 30x compression on index size. The compressed index\nfurther brings 10x speedup on CPU and 2x speedup on GPU in query latency.</p>\n", "tags": ["Quantization", "CIKM", "Efficiency", "Evaluation"], "tsne_embedding": [13.639819145202637, 19.486398696899414], "cluster": 2}, {"key": "zhan2021learning", "year": "2022", "citations": "34", "title": "Learning Discrete Representations Via Constrained Clustering For Effective And Efficient Dense Retrieval", "abstract": "<p>Dense Retrieval (DR) has achieved state-of-the-art first-stage ranking\neffectiveness. However, the efficiency of most existing DR models is limited by\nthe large memory cost of storing dense vectors and the time-consuming nearest\nneighbor search (NNS) in vector space. Therefore, we present RepCONC, a novel\nretrieval model that learns discrete Representations via CONstrained\nClustering. RepCONC jointly trains dual-encoders and the Product Quantization\n(PQ) method to learn discrete document representations and enables fast\napproximate NNS with compact indexes. It models quantization as a constrained\nclustering process, which requires the document embeddings to be uniformly\nclustered around the quantization centroids and supports end-to-end\noptimization of the quantization method and dual-encoders. We theoretically\ndemonstrate the importance of the uniform clustering constraint in RepCONC and\nderive an efficient approximate solution for constrained clustering by reducing\nit to an instance of the optimal transport problem. Besides constrained\nclustering, RepCONC further adopts a vector-based inverted file system (IVF) to\nsupport highly efficient vector search on CPUs. Extensive experiments on two\npopular ad-hoc retrieval benchmarks show that RepCONC achieves better ranking\neffectiveness than competitive vector quantization baselines under different\ncompression ratio settings. It also substantially outperforms a wide range of\nexisting retrieval models in terms of retrieval effectiveness, memory\nefficiency, and time efficiency.</p>\n", "tags": ["Efficiency", "Quantization", "Vector-Indexing"], "tsne_embedding": [26.57719612121582, 21.206953048706055], "cluster": 2}, {"key": "zhang2010self", "year": "2010", "citations": "354", "title": "Self-taught Hashing For Fast Similarity Search", "abstract": "<p>The ability of fast similarity search at large scale is of great\nimportance to many Information Retrieval (IR) applications.\nA promising way to accelerate similarity search is semantic\nhashing which designs compact binary codes for a large number\nof documents so that semantically similar documents\nare mapped to similar codes (within a short Hamming distance).\nAlthough some recently proposed techniques are\nable to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents\nremains to be a very challenging problem. In this\npaper, we emphasise this issue and propose a novel SelfTaught\nHashing (STH) approach to semantic hashing: we\nfirst find the optimal l-bit binary codes for all documents in\nthe given corpus via unsupervised learning, and then train\nl classifiers via supervised learning to predict the l-bit code\nfor any query document unseen before. Our experiments on\nthree real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and\nlinear Support Vector Machine (SVM) outperforms stateof-the-art\ntechniques significantly.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Compact-Codes", "Similarity-Search", "SIGIR", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [15.052294731140137, -16.67589569091797], "cluster": 7}, {"key": "zhang2011composite", "year": "2011", "citations": "221", "title": "Composite Hashing With Multiple Information Sources", "abstract": "<p>Similarity search applications with a large amount of text\nand image data demands an efficient and effective solution.\nOne useful strategy is to represent the examples in databases\nas compact binary codes through semantic hashing, which\nhas attracted much attention due to its fast query/search\nspeed and drastically reduced storage requirement. All of\nthe current semantic hashing methods only deal with the\ncase when each example is represented by one type of features.\nHowever, examples are often described from several\ndifferent information sources in many real world applications.\nFor example, the characteristics of a webpage can be\nderived from both its content part and its associated links.\nTo address the problem of learning good hashing codes in\nthis scenario, we propose a novel research problem \u2013 Composite\nHashing with Multiple Information Sources (CHMIS).\nThe focus of the new research problem is to design an algorithm\nfor incorporating the features from different information\nsources into the binary hashing codes efficiently and\neffectively. In particular, we propose an algorithm CHMISAW\n(CHMIS with Adjusted Weights) for learning the codes.\nThe proposed algorithm integrates information from several\ndifferent sources into the binary hashing codes by adjusting\nthe weights on each individual source for maximizing\nthe coding performance, and enables fast conversion from\nquery examples to their binary hashing codes. Experimental\nresults on five different datasets demonstrate the superior\nperformance of the proposed method against several other\nstate-of-the-art semantic hashing techniques.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Similarity-Search", "SIGIR", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [13.318633079528809, 12.074615478515625], "cluster": 6}, {"key": "zhang2013binary", "year": "2013", "citations": "98", "title": "Binary Code Ranking With Weighted Hamming Distance", "abstract": "<p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most\nexisting binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or\nsimilarity of two points are approximated by the Hamming\ndistance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are\noften lots of results sharing the same Hamming distance to\na query, which makes this distance measure ambiguous and\nposes a critical issue for similarity search where ranking is\nimportant. In this paper, we propose a weighted Hamming\ndistance ranking algorithm (WhRank) to rank the binary\ncodes of hashing methods. By assigning different bit-level\nweights to different hash bits, the returned binary codes\nare ranked at a finer-grained binary code level. We give\nan algorithm to learn the data-adaptive and query-sensitive\nweight for each hash bit. Evaluations on two large-scale\nimage data sets demonstrate the efficacy of our weighted\nHamming distance for binary code ranking.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "CVPR", "Scalability", "Similarity-Search", "Compact-Codes"], "tsne_embedding": [-2.4041080474853516, 28.57634735107422], "cluster": 8}, {"key": "zhang2014large", "year": "2014", "citations": "619", "title": "Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for similarity search in multimedia\ndata. In particular, more and more attentions\nhave been payed to multimodal hashing for search in\nmultimedia data with multiple modalities, such as images\nwith tags. Typically, supervised information of semantic\nlabels is also available for the data points in\nmany real applications. Hence, many supervised multimodal\nhashing (SMH) methods have been proposed\nto utilize such semantic labels to further improve the\nsearch accuracy. However, the training time complexity\nof most existing SMH methods is too high, which\nmakes them unscalable to large-scale datasets. In this\npaper, a novel SMH method, called semantic correlation\nmaximization (SCM), is proposed to seamlessly integrate\nsemantic labels into the hashing learning procedure\nfor large-scale data modeling. Experimental results\non two real-world datasets show that SCM can signifi-\ncantly outperform the state-of-the-art SMH methods, in\nterms of both accuracy and scalability.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Similarity-Search", "AAAI", "Memory-Efficiency", "Datasets", "Supervised"], "tsne_embedding": [5.993285655975342, -4.120081901550293], "cluster": 6}, {"key": "zhang2014supervised", "year": "2014", "citations": "286", "title": "Supervised Hashing With Latent Factor Models", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for approximate nearest neighbor\nsearch in large-scale datasets. Traditional hashing methods\ntry to learn the hash codes in an unsupervised way where\nthe metric (Euclidean) structure of the training data is preserved.\nVery recently, supervised hashing methods, which\ntry to preserve the semantic structure constructed from the\nsemantic labels of the training points, have exhibited higher\naccuracy than unsupervised methods. In this paper, we\npropose a novel supervised hashing method, called latent\nfactor hashing (LFH), to learn similarity-preserving binary\ncodes based on latent factor models. An algorithm with\nconvergence guarantee is proposed to learn the parameters\nof LFH. Furthermore, a linear-time variant with stochastic\nlearning is proposed for training LFH on large-scale datasets.\nExperimental results on two large datasets with semantic\nlabels show that LFH can achieve superior accuracy than\nstate-of-the-art methods with comparable training time.</p>\n", "tags": ["Hashing-Methods", "Scalability", "SIGIR", "Memory-Efficiency", "Datasets", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [18.724925994873047, 2.6640539169311523], "cluster": 6}, {"key": "zhang2015bit", "year": "2015", "citations": "418", "title": "Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification", "abstract": "<p>Extracting informative image features and learning\neffective approximate hashing functions are two crucial steps in\nimage retrieval . Conventional methods often study these two\nsteps separately, e.g., learning hash functions from a predefined\nhand-crafted feature space. Meanwhile, the bit lengths of output\nhashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised\nlearning framework to generate compact and bit-scalable hashing\ncodes directly from raw images. We pose hashing learning as\na problem of regularized similarity learning. Specifically, we\norganize the training images into a batch of triplet samples,\neach sample containing two images with the same label and one\nwith a different label. With these triplet samples, we maximize\nthe margin between matched pairs and mismatched pairs in the\nHamming space. In addition, a regularization term is introduced\nto enforce the adjacency consistency, i.e., images of similar\nappearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end\nfashion, where discriminative image features and hash functions\nare simultaneously optimized. Furthermore, each bit of our\nhashing codes is unequally weighted so that we can manipulate\nthe code lengths by truncating the insignificant bits. Our\nframework outperforms state-of-the-arts on public benchmarks\nof similar image search and also achieves promising results in\nthe application of person re-identification in surveillance. It is\nalso shown that the generated bit-scalable hashing codes well\npreserve the discriminative powers with shorter code lengths.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Tools-&-Libraries", "Supervised", "Neural-Hashing"], "tsne_embedding": [-7.243453502655029, 2.0529520511627197], "cluster": 1}, {"key": "zhang2015zero", "year": "2015", "citations": "581", "title": "Zero-shot Learning Via Joint Latent Similarity Embedding", "abstract": "<p>Zero-shot recognition (ZSR) deals with the problem of predicting class labels\nfor target domain instances based on source domain side information (e.g.\nattributes) of unseen classes. We formulate ZSR as a binary prediction problem.\nOur resulting classifier is class-independent. It takes an arbitrary pair of\nsource and target domain instances as input and predicts whether or not they\ncome from the same class, i.e. whether there is a match. We model the posterior\nprobability of a match since it is a sufficient statistic and propose a latent\nprobabilistic model in this context. We develop a joint discriminative learning\nframework based on dictionary learning to jointly learn the parameters of our\nmodel for both domains, which ultimately leads to our class-independent\nclassifier. Many of the existing embedding methods can be viewed as special\ncases of our probabilistic model. On ZSR our method shows 4.90% improvement\nover the state-of-the-art in accuracy averaged across four benchmark datasets.\nWe also adapt ZSR method for zero-shot retrieval and show 22.45% improvement\naccordingly in mean average precision (mAP).</p>\n", "tags": ["ICCV", "Few-Shot-&-Zero-Shot", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [9.773283958435059, -4.599472522735596], "cluster": 6}, {"key": "zhang2016efficient", "year": "2016", "citations": "112", "title": "Efficient Training Of Very Deep Neural Networks For Supervised Hashing", "abstract": "<p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively \u201cshallow\u201d networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "CVPR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [31.624061584472656, -5.349379539489746], "cluster": 9}, {"key": "zhang2016query", "year": "2018", "citations": "55", "title": "Query-adaptive Image Retrieval By Deep Weighted Hashing", "abstract": "<p>Hashing methods have attracted much attention for large scale image\nretrieval. Some deep hashing methods have achieved promising results by taking\nadvantage of the strong representation power of deep networks recently.\nHowever, existing deep hashing methods treat all hash bits equally. On one\nhand, a large number of images share the same distance to a query image due to\nthe discrete Hamming distance, which raises a critical issue of image retrieval\nwhere fine-grained rankings are very important. On the other hand, different\nhash bits actually contribute to the image retrieval differently, and treating\nthem equally greatly affects the retrieval accuracy of image. To address the\nabove two problems, we propose the query-adaptive deep weighted hashing (QaDWH)\napproach, which can perform fine-grained ranking for different queries by\nweighted Hamming distance. First, a novel deep hashing network is proposed to\nlearn the hash codes and corresponding class-wise weights jointly, so that the\nlearned weights can reflect the importance of different hash bits for different\nimage classes. Second, a query-adaptive image retrieval method is proposed,\nwhich rapidly generates hash bit weights for different query images by fusing\nits semantic probability and the learned class-wise weights. Fine-grained image\nretrieval is then performed by the weighted Hamming distance, which can provide\nmore accurate ranking than the traditional Hamming distance. Experiments on\nfour widely used datasets show that the proposed approach outperforms eight\nstate-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Datasets"], "tsne_embedding": [-7.251551628112793, 6.227656364440918], "cluster": 1}, {"key": "zhang2016scalable", "year": "2016", "citations": "4", "title": "Scalable Discrete Supervised Hash Learning With Asymmetric Matrix Factorization", "abstract": "<p>Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, and it has received a broad attention due to its low storage cost and\nfast retrieval speed. However, the existing limitations make the present\nalgorithms difficult to deal with large-scale datasets: (1) discrete\nconstraints are involved in the learning of the hash function; (2) pairwise or\ntriplet similarity is adopted to generate efficient hashcodes, resulting both\ntime and space complexity are greater than O(n^2). To address these issues, we\npropose a novel discrete supervised hash learning framework which can be\nscalable to large-scale datasets. First, the discrete learning procedure is\ndecomposed into a binary classifier learning scheme and binary codes learning\nscheme, which makes the learning procedure more efficient. Second, we adopt the\nAsymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based\nBatch Coordinate Descent method, such that the time and space complexity is\nreduced to O(n). The proposed framework also provides a flexible paradigm to\nincorporate with arbitrary hash function, including deep neural networks and\nkernel methods. Experiments on large-scale datasets demonstrate that the\nproposed method is superior or comparable with state-of-the-art hashing\nalgorithms.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [22.2877140045166, 3.6588118076324463], "cluster": 6}, {"key": "zhang2016ssdh", "year": "2017", "citations": "150", "title": "SSDH: Semi-supervised Deep Hashing For Large Scale Image Retrieval", "abstract": "<p>Hashing methods have been widely used for efficient similarity retrieval on\nlarge scale image database. Traditional hashing methods learn hash functions to\ngenerate binary codes from hand-crafted features, which achieve limited\naccuracy since the hand-crafted features cannot optimally represent the image\ncontent and preserve the semantic similarity. Recently, several deep hashing\nmethods have shown better performance because the deep architectures generate\nmore discriminative feature representations. However, these deep hashing\nmethods are mainly designed for supervised scenarios, which only exploit the\nsemantic similarity information, but ignore the underlying data structures. In\nthis paper, we propose the semi-supervised deep hashing (SSDH) approach, to\nperform more effective hash function learning by simultaneously preserving\nsemantic similarity and underlying data structures. The main contributions are\nas follows: (1) We propose a semi-supervised loss to jointly minimize the\nempirical error on labeled data, as well as the embedding error on both labeled\nand unlabeled data, which can preserve the semantic similarity and capture the\nmeaningful neighbors on the underlying data structures for effective hashing.\n(2) A semi-supervised deep hashing network is designed to extensively exploit\nboth labeled and unlabeled data, in which we propose an online graph\nconstruction method to benefit from the evolving deep features during training\nto better capture semantic neighbors. To the best of our knowledge, the\nproposed deep network is the first deep hashing method that can perform hash\ncode learning and feature learning simultaneously in a semi-supervised fashion.\nExperimental results on 5 widely-used datasets show that our proposed approach\noutperforms the state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Similarity-Search", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [2.539609432220459, 12.249144554138184], "cluster": 6}, {"key": "zhang2017effective", "year": "2019", "citations": "24", "title": "Effective Image Retrieval Via Multilinear Multi-index Fusion", "abstract": "<p>Multi-index fusion has demonstrated impressive performances in retrieval task\nby integrating different visual representations in a unified framework.\nHowever, previous works mainly consider propagating similarities via neighbor\nstructure, ignoring the high order information among different visual\nrepresentations. In this paper, we propose a new multi-index fusion scheme for\nimage retrieval. By formulating this procedure as a multilinear based\noptimization problem, the complementary information hidden in different indexes\ncan be explored more thoroughly. Specially, we first build our multiple indexes\nfrom various visual representations. Then a so-called index-specific functional\nmatrix, which aims to propagate similarities, is introduced for updating the\noriginal index. The functional matrices are then optimized in a unified tensor\nspace to achieve a refinement, such that the relevant images can be pushed more\ncloser. The optimization problem can be efficiently solved by the augmented\nLagrangian method with theoretical convergence guarantee. Unlike the\ntraditional multi-index fusion scheme, our approach embeds the multi-index\nsubspace structure into the new indexes with sparse constraint, thus it has\nlittle additional memory consumption in online query stage. Experimental\nevaluation on three benchmark datasets reveals that the proposed approach\nachieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP\n94.1% on Holiday and 62.39% on Market-1501.</p>\n", "tags": ["Vector-Indexing", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [5.818140983581543, 27.687938690185547], "cluster": 4}, {"key": "zhang2017hashgan", "year": "2017", "citations": "16", "title": "Hashgan:attention-aware Deep Adversarial Hashing For Cross Modal Retrieval", "abstract": "<p>As the rapid growth of multi-modal data, hashing methods for cross-modal\nretrieval have received considerable attention. Deep-networks-based cross-modal\nhashing methods are appealing as they can integrate feature learning and hash\ncoding into end-to-end trainable frameworks. However, it is still challenging\nto find content similarities between different modalities of data due to the\nheterogeneity gap. To further address this problem, we propose an adversarial\nhashing network with attention mechanism to enhance the measurement of content\nsimilarities by selectively focusing on informative parts of multi-modal data.\nThe proposed new adversarial network, HashGAN, consists of three building\nblocks: 1) the feature learning module to obtain feature representations, 2)\nthe generative attention module to generate an attention mask, which is used to\nobtain the attended (foreground) and the unattended (background) feature\nrepresentations, 3) the discriminative hash coding module to learn hash\nfunctions that preserve the similarities between different modalities. In our\nframework, the generative module and the discriminative module are trained in\nan adversarial way: the generator is learned to make the discriminator cannot\npreserve the similarities of multi-modal data w.r.t. the background feature\nrepresentations, while the discriminator aims to preserve the similarities of\nmulti-modal data w.r.t. both the foreground and the background feature\nrepresentations. Extensive evaluations on several benchmark datasets\ndemonstrate that the proposed HashGAN brings substantial improvements over\nother state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Hashing-Methods", "Robustness", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-0.19857561588287354, 4.993699073791504], "cluster": 6}, {"key": "zhang2017unsupervised", "year": "2018", "citations": "203", "title": "Unsupervised Generative Adversarial Cross-modal Hashing", "abstract": "<p>Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Unsupervised cross-modal hashing is more flexible and applicable\nthan supervised methods, since no intensive labeling work is involved. However,\nexisting unsupervised methods learn hashing functions by preserving inter and\nintra correlations, while ignoring the underlying manifold structure across\ndifferent modalities, which is extremely helpful to capture meaningful nearest\nneighbors of different modalities for cross-modal retrieval. To address the\nabove problem, in this paper we propose an Unsupervised Generative Adversarial\nCross-modal Hashing approach (UGACH), which makes full use of GAN\u2019s ability for\nunsupervised representation learning to exploit the underlying manifold\nstructure of cross-modal data. The main contributions can be summarized as\nfollows: (1) We propose a generative adversarial network to model cross-modal\nhashing in an unsupervised fashion. In the proposed UGACH, given a data of one\nmodality, the generative model tries to fit the distribution over the manifold\nstructure, and select informative data of another modality to challenge the\ndiscriminative model. The discriminative model learns to distinguish the\ngenerated data and the true positive data sampled from correlation graph to\nachieve better retrieval accuracy. These two models are trained in an\nadversarial way to improve each other and promote hashing function learning.\n(2) We propose a correlation graph based approach to capture the underlying\nmanifold structure across different modalities, so that data of different\nmodalities but within the same manifold can have smaller Hamming distance and\npromote retrieval accuracy. Extensive experiments compared with 6\nstate-of-the-art methods verify the effectiveness of our proposed approach.</p>\n", "tags": ["Hashing-Methods", "Robustness", "AAAI", "Multimodal-Retrieval", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [3.300502300262451, 7.543087959289551], "cluster": 6}, {"key": "zhang2018highly", "year": "2018", "citations": "49", "title": "Highly-economized Multi-view Binary Compression For Scalable Image Clustering", "abstract": "<p>How to economically cluster large-scale multi-view images is a long-standing\nproblem in computer vision. To tackle this challenge, we introduce a novel\napproach named Highly-economized Scalable Image Clustering (HSIC) that\nradically surpasses conventional image clustering methods via binary\ncompression. We intuitively unify the binary representation learning and\nefficient binary cluster structure learning into a joint framework. In\nparticular, common binary representations are learned by exploiting both\nsharable and individual information across multiple views to capture their\nunderlying correlations. Meanwhile, cluster assignment with robust binary\ncentroids is also performed via effective discrete optimization under L21-norm\nconstraint. By this means, heavy continuous-valued Euclidean distance\ncomputations can be successfully reduced by efficient binary XOR operations\nduring the clustering procedure. To our best knowledge, HSIC is the first\nbinary clustering work specifically designed for scalable multi-view image\nclustering. Extensive experimental results on four large-scale image datasets\nshow that HSIC consistently outperforms the state-of-the-art approaches, whilst\nsignificantly reducing computational time and memory footprint.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [-5.196020603179932, 37.91625213623047], "cluster": 8}, {"key": "zhang2018improved", "year": "2019", "citations": "144", "title": "Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval", "abstract": "<p>Hash coding has been widely used in the approximate nearest neighbor search\nfor large-scale image retrieval. Recently, many deep hashing methods have been\nproposed and shown largely improved performance over traditional\nfeature-learning-based methods. Most of these methods examine the pairwise\nsimilarity on the semantic-level labels, where the pairwise similarity is\ngenerally defined in a hard-assignment way. That is, the pairwise similarity is\n\u20181\u2019 if they share no less than one class label and \u20180\u2019 if they do not share\nany. However, such similarity definition cannot reflect the similarity ranking\nfor pairwise images that hold multiple labels. In this paper, a new deep\nhashing method is proposed for multi-label image retrieval by re-defining the\npairwise similarity into an instance similarity, where the instance similarity\nis quantified into a percentage based on the normalized semantic labels. Based\non the instance similarity, a weighted cross-entropy loss and a minimum mean\nsquare error loss are tailored for loss-function construction, and are\nefficiently used for simultaneous feature learning and hash coding. Experiments\non three popular datasets demonstrate that, the proposed method outperforms the\ncompeting methods and achieves the state-of-the-art performance in multi-label\nimage retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Evaluation"], "tsne_embedding": [-9.124702453613281, 6.175615310668945], "cluster": 1}, {"key": "zhang2018relationnet2", "year": "2018", "citations": "17", "title": "Relationnet2: Deep Comparison Columns For Few-shot Learning", "abstract": "<p>Few-shot deep learning is a topical challenge area for scaling visual\nrecognition to open ended growth of unseen new classes with limited labeled\nexamples. A promising approach is based on metric learning, which trains a deep\nembedding to support image similarity matching. Our insight is that effective\ngeneral purpose matching requires non-linear comparison of features at multiple\nabstraction levels. We thus propose a new deep comparison network comprised of\nembedding and relation modules that learn multiple non-linear distance metrics\nbased on different levels of features simultaneously. Furthermore, to reduce\nover-fitting and enable the use of deeper embeddings, we represent images as\ndistributions rather than vectors via learning parameterized Gaussian noise\nregularization. The resulting network achieves excellent performance on both\nminiImageNet and tieredImageNet.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Distance-Metric-Learning"], "tsne_embedding": [-13.095475196838379, -18.898386001586914], "cluster": 1}, {"key": "zhang2018sch", "year": "2018", "citations": "127", "title": "SCH-GAN: Semi-supervised Cross-modal Hashing By Generative Adversarial Network", "abstract": "<p>Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Supervised cross-modal hashing methods have achieved considerable\nprogress by incorporating semantic side information. However, they mainly have\ntwo limitations: (1) Heavily rely on large-scale labeled cross-modal training\ndata which are labor intensive and hard to obtain. (2) Ignore the rich\ninformation contained in the large amount of unlabeled data across different\nmodalities, especially the margin examples that are easily to be incorrectly\nretrieved, which can help to model the correlations. To address these problems,\nin this paper we propose a novel Semi-supervised Cross-Modal Hashing approach\nby Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN\u2019s\nability for modeling data distributions to promote cross-modal hashing learning\nin an adversarial way. The main contributions can be summarized as follows: (1)\nWe propose a novel generative adversarial network for cross-modal hashing. In\nour proposed SCH-GAN, the generative model tries to select margin examples of\none modality from unlabeled data when giving a query of another modality. While\nthe discriminative model tries to distinguish the selected examples and true\npositive examples of the query. These two models play a minimax game so that\nthe generative model can promote the hashing performance of discriminative\nmodel. (2) We propose a reinforcement learning based algorithm to drive the\ntraining of proposed SCH-GAN. The generative model takes the correlation score\npredicted by discriminative model as a reward, and tries to select the examples\nclose to the margin to promote discriminative model by maximizing the margin\nbetween positive and negative data. Experiments on 3 widely-used datasets\nverify the effectiveness of our proposed approach.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Robustness", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [2.989612340927124, 7.195013523101807], "cluster": 6}, {"key": "zhang2018semantic", "year": "2019", "citations": "13", "title": "Semantic Cluster Unary Loss For Efficient Deep Hashing", "abstract": "<p>Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, which has received a broad attention due to its low storage cost and\nfast retrieval speed. With the rapid development of deep learning, deep hashing\nmethods have achieved promising results in efficient information retrieval.\nMost of the existing deep hashing methods adopt pairwise or triplet losses to\ndeal with similarities underlying the data, but the training is difficult and\nless efficient because \\(O(n^2)\\) data pairs and \\(O(n^3)\\) triplets are involved.\nTo address these issues, we propose a novel deep hashing algorithm with unary\nloss which can be trained very efficiently. We first of all introduce a Unary\nUpper Bound of the traditional triplet loss, thus reducing the complexity to\n\\(O(n)\\) and bridging the classification-based unary loss and the triplet loss.\nSecond, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by\nintroducing a modified Unary Upper Bound loss, named Semantic Cluster Unary\nLoss (SCUL). The resultant hashcodes form several compact clusters, which means\nhashcodes in the same cluster have similar semantic information. We also\ndemonstrate that the proposed SCDH is easy to be extended to semi-supervised\nsettings by incorporating the state-of-the-art semi-supervised learning\nalgorithms. Experiments on large-scale datasets show that the proposed method\nis superior to state-of-the-art hashing algorithms.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Scalability", "Memory-Efficiency", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [19.774219512939453, 4.460603713989258], "cluster": 6}, {"key": "zhang2018trquery", "year": "2018", "citations": "1", "title": "Trquery: An Embedding-based Framework For Recommanding SPARQL Queries", "abstract": "<p>In this paper, we present an embedding-based framework (TrQuery) for\nrecommending solutions of a SPARQL query, including approximate solutions when\nexact querying solutions are not available due to incompleteness or\ninconsistencies of real-world RDF data. Within this framework, embedding is\napplied to score solutions together with edit distance so that we could obtain\nmore fine-grained recommendations than those recommendations via edit distance.\nFor instance, graphs of two querying solutions with a similar structure can be\ndistinguished in our proposed framework while the edit distance depending on\nstructural difference becomes unable. To this end, we propose a novel score\nmodel built on vector space generated in embedding system to compute the\nsimilarity between an approximate subgraph matching and a whole graph matching.\nFinally, we evaluate our approach on large RDF datasets DBpedia and YAGO, and\nexperimental results show that TrQuery exhibits an excellent behavior in terms\nof both effectiveness and efficiency.</p>\n", "tags": ["Efficiency", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [49.997467041015625, 4.670829772949219], "cluster": 9}, {"key": "zhang2019collaborative", "year": "2016", "citations": "62", "title": "Collaborative Quantization For Cross-modal Similarity Search", "abstract": "<p>Cross-modal similarity search is a problem about designing a search system\nsupporting querying across content modalities, e.g., using an image to search\nfor texts or using a text to search for images. This paper presents a compact\ncoding solution for efficient search, with a focus on the quantization approach\nwhich has already shown the superior performance over the hashing solutions in\nthe single-modal similarity search. We propose a cross-modal quantization\napproach, which is among the early attempts to introduce quantization into\ncross-modal search. The major contribution lies in jointly learning the\nquantizers for both modalities through aligning the quantized representations\nfor each pair of image and text belonging to a document. In addition, our\napproach simultaneously learns the common space for both modalities in which\nquantization is conducted to enable efficient and effective search using the\nEuclidean distance computed in the common space with fast distance table\nlookup. Experimental results compared with several competitive algorithms over\nthree benchmark datasets demonstrate that the proposed approach achieves the\nstate-of-the-art performance.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Quantization", "CVPR", "Similarity-Search", "Datasets", "Evaluation"], "tsne_embedding": [-28.53862190246582, -29.971017837524414], "cluster": 5}, {"key": "zhang2019generic", "year": "2019", "citations": "33", "title": "Generic Intent Representation In Web Search", "abstract": "<p>This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a\ndistributed representation space for user intent in search. Leveraging large\nscale user clicks from Bing search logs as weak supervision of user intent, GEN\nEncoder learns to map queries with shared clicks into similar embeddings\nend-to-end and then finetunes on multiple paraphrase tasks. Experimental\nresults on an intrinsic evaluation task - query intent similarity modeling -\ndemonstrate GEN Encoder\u2019s robust and significant advantages over previous\nrepresentation methods. Ablation studies reveal the crucial role of learning\nfrom implicit user feedback in representing user intent and the contributions\nof multi-task learning in representation generality. We also demonstrate that\nGEN Encoder alleviates the sparsity of tail search traffic and cuts down half\nof the unseen queries by using an efficient approximate nearest neighbor search\nto effectively identify previous queries with the same search intent. Finally,\nwe demonstrate distances between GEN encodings reflect certain information\nseeking behaviors in search sessions.</p>\n", "tags": ["SIGIR", "Evaluation"], "tsne_embedding": [20.596355438232422, -26.65098762512207], "cluster": 7}, {"key": "zhang2019joint", "year": "2019", "citations": "4", "title": "Joint Cluster Unary Loss For Efficient Cross-modal Hashing", "abstract": "<p>With the rapid growth of various types of multimodal data, cross-modal deep\nhashing has received broad attention for solving cross-modal retrieval problems\nefficiently. Most cross-modal hashing methods follow the traditional supervised\nhashing framework in which the \\(O(n^2)\\) data pairs and \\(O(n^3)\\) data triplets\nare generated for training, but the training procedure is less efficient\nbecause the complexity is high for large-scale dataset. To address these\nissues, we propose a novel and efficient cross-modal hashing algorithm in which\nthe unary loss is introduced. First of all, We introduce the Cross-Modal Unary\nLoss (CMUL) with \\(O(n)\\) complexity to bridge the traditional triplet loss and\nclassification-based unary loss. A more accurate bound of the triplet loss for\nstructured multilabel data is also proposed in CMUL. Second, we propose the\nnovel Joint Cluster Cross-Modal Hashing (JCCH) algorithm for efficient hash\nlearning, in which the CMUL is involved. The resultant hashcodes form several\nclusters in which the hashcodes in the same cluster share similar semantic\ninformation, and the heterogeneity gap on different modalities is diminished by\nsharing the clusters. The proposed algorithm is able to be applied to various\ntypes of data, and experiments on large-scale datasets show that the proposed\nmethod is superior over or comparable with state-of-the-art cross-modal hashing\nmethods, and training with the proposed method is more efficient than others.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Scalability", "Multimodal-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [19.824237823486328, 4.489902019500732], "cluster": 6}, {"key": "zhang2019pairwise", "year": "2019", "citations": "7", "title": "Pairwise Teacher-student Network For Semi-supervised Hashing", "abstract": "<p>Hashing method maps similar high-dimensional data to binary hashcodes with\nsmaller hamming distance, and it has received broad attention due to its low\nstorage cost and fast retrieval speed. Pairwise similarity is easily obtained\nand widely used for retrieval, and most supervised hashing algorithms are\ncarefully designed for the pairwise supervisions. As labeling all data pairs is\ndifficult, semi-supervised hashing is proposed which aims at learning efficient\ncodes with limited labeled pairs and abundant unlabeled ones. Existing methods\nbuild graphs to capture the structure of dataset, but they are not working well\nfor complex data as the graph is built based on the data representations and\ndetermining the representations of complex data is difficult. In this paper, we\npropose a novel teacher-student semi-supervised hashing framework in which the\nstudent is trained with the pairwise information produced by the teacher\nnetwork. The network follows the smoothness assumption, which achieves\nconsistent distances for similar data pairs so that the retrieval results are\nsimilar for neighborhood queries. Experiments on large-scale datasets show that\nthe proposed method reaches impressive gain over the supervised baselines and\nis superior to state-of-the-art semi-supervised hashing methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "CVPR", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [38.708473205566406, -15.41224479675293], "cluster": 9}, {"key": "zhang2019part", "year": "2020", "citations": "56", "title": "Part-guided Attention Learning For Vehicle Instance Retrieval", "abstract": "<p>Vehicle instance retrieval often requires one to recognize the fine-grained\nvisual differences between vehicles. Besides the holistic appearance of\nvehicles which is easily affected by the viewpoint variation and distortion,\nvehicle parts also provide crucial cues to differentiate near-identical\nvehicles. Motivated by these observations, we introduce a Part-Guided Attention\nNetwork (PGAN) to pinpoint the prominent part regions and effectively combine\nthe global and part information for discriminative feature learning. PGAN first\ndetects the locations of different part components and salient regions\nregardless of the vehicle identity, which serve as the bottom-up attention to\nnarrow down the possible searching regions. To estimate the importance of\ndetected parts, we propose a Part Attention Module (PAM) to adaptively locate\nthe most discriminative regions with high-attention weights and suppress the\ndistraction of irrelevant parts with relatively low weights. The PAM is guided\nby the instance retrieval loss and therefore provides top-down attention that\nenables attention to be calculated at the level of car parts and other salient\nregions. Finally, we aggregate the global appearance and part features to\nimprove the feature performance further. The PGAN combines part-guided\nbottom-up and top-down attention, global and part visual features in an\nend-to-end framework. Extensive experiments demonstrate that the proposed\nmethod achieves new state-of-the-art vehicle instance retrieval performance on\nfour large-scale benchmark datasets.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Scalability", "Datasets"], "tsne_embedding": [-21.578516006469727, 5.70070219039917], "cluster": 1}, {"key": "zhang2019pcan", "year": "2019", "citations": "221", "title": "PCAN: 3D Attention Map Learning Using Contextual Information For Point Cloud Based Retrieval", "abstract": "<p>Point cloud based retrieval for place recognition is an emerging problem in\nvision field. The main challenge is how to find an efficient way to encode the\nlocal features into a discriminative global descriptor. In this paper, we\npropose a Point Contextual Attention Network (PCAN), which can predict the\nsignificance of each local point feature based on point context. Our network\nmakes it possible to pay more attention to the task-relevent features when\naggregating local features. Experiments on various benchmark datasets show that\nthe proposed network can provide outperformance than current state-of-the-art\napproaches.</p>\n", "tags": ["CVPR", "Evaluation", "Datasets"], "tsne_embedding": [-32.37785720825195, 1.463742971420288], "cluster": 0}, {"key": "zhang2019sadih", "year": "2019", "citations": "17", "title": "SADIH: Semantic-aware Discrete Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, hashing has been recognized\nto accomplish similarity search in large-scale multimedia retrieval\napplications. Particularly supervised hashing has recently received\nconsiderable research attention by leveraging the label information to preserve\nthe pairwise similarities of data points in the Hamming space. However, there\nstill remain two crucial bottlenecks: 1) the learning process of the full\npairwise similarity preservation is computationally unaffordable and unscalable\nto deal with big data; 2) the available category information of data are not\nwell-explored to learn discriminative hash functions. To overcome these\nchallenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH)\nframework, which aims to directly embed the transformed semantic information\ninto the asymmetric similarity approximation and discriminative hashing\nfunction learning. Specifically, a semantic-aware latent embedding is\nintroduced to asymmetrically preserve the full pairwise similarities while\nskillfully handle the cumbersome n times n pairwise similarity matrix.\nMeanwhile, a semantic-aware autoencoder is developed to jointly preserve the\ndata structures in the discriminative latent semantic space and perform data\nreconstruction. Moreover, an efficient alternating optimization algorithm is\nproposed to solve the resulting discrete optimization problem. Extensive\nexperimental results on multiple large-scale datasets demonstrate that our\nSADIH can clearly outperform the state-of-the-art baselines with the additional\nbenefit of lower computational costs.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Similarity-Search", "AAAI", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [4.011345386505127, 12.450996398925781], "cluster": 6}, {"key": "zhang2019search", "year": "2021", "citations": "2", "title": "Search Efficient Binary Network Embedding", "abstract": "<p>Traditional network embedding primarily focuses on learning a continuous\nvector representation for each node, preserving network structure and/or node\ncontent information, such that off-the-shelf machine learning algorithms can be\neasily applied to the vector-format node representations for network analysis.\nHowever, the learned continuous vector representations are inefficient for\nlarge-scale similarity search, which often involves finding nearest neighbors\nmeasured by distance or similarity in a continuous vector space. In this paper,\nwe propose a search efficient binary network embedding algorithm called\nBinaryNE to learn a binary code for each node, by simultaneously modeling node\ncontext relations and node attribute relations through a three-layer neural\nnetwork. BinaryNE learns binary node representations through a stochastic\ngradient descent based online learning algorithm. The learned binary encoding\nnot only reduces memory usage to represent each node, but also allows fast\nbit-wise comparisons to support faster node similarity search than using\nEuclidean distance or other distance measures. Extensive experiments and\ncomparisons demonstrate that BinaryNE not only delivers more than 25 times\nfaster search speed, but also provides comparable or better search quality than\ntraditional continuous vector based network embedding methods. The binary codes\nlearned by BinaryNE also render competitive performance on node classification\nand node clustering tasks. The source code of this paper is available at\nhttps://github.com/daokunzhang/BinaryNE.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Scalability", "Similarity-Search", "Memory-Efficiency", "Compact-Codes", "Evaluation"], "tsne_embedding": [34.86970520019531, -5.487929344177246], "cluster": 9}, {"key": "zhang2019semantic", "year": "2019", "citations": "3", "title": "Semantic Hierarchy Preserving Deep Hashing For Large-scale Image Retrieval", "abstract": "<p>Deep hashing models have been proposed as an efficient method for large-scale\nsimilarity search. However, most existing deep hashing methods only utilize\nfine-level labels for training while ignoring the natural semantic hierarchy\nstructure. This paper presents an effective method that preserves the classwise\nsimilarity of full-level semantic hierarchy for large-scale image retrieval.\nExperiments on two benchmark datasets show that our method helps improve the\nfine-level retrieval performance. Moreover, with the help of the semantic\nhierarchy, it can produce significantly better binary codes for hierarchical\nretrieval, which indicates its potential of providing more user-desired\nretrieval results.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "Similarity-Search", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [-2.8410873413085938, -4.545039176940918], "cluster": 1}, {"key": "zhang2020collaborative", "year": "2020", "citations": "6", "title": "Collaborative Generative Hashing For Marketing And Fast Cold-start Recommendation", "abstract": "<p>Cold-start has being a critical issue in recommender systems with the\nexplosion of data in e-commerce. Most existing studies proposed to alleviate\nthe cold-start problem are also known as hybrid recommender systems that learn\nrepresentations of users and items by combining user-item interactive and\nuser/item content information. However, previous hybrid methods regularly\nsuffered poor efficiency bottlenecking in online recommendations with\nlarge-scale items, because they were designed to project users and items into\ncontinuous latent space where the online recommendation is expensive. To this\nend, we propose a collaborative generated hashing (CGH) framework to improve\nthe efficiency by denoting users and items as binary codes, then fast hashing\nsearch techniques can be used to speed up the online recommendation. In\naddition, the proposed CGH can generate potential users or items for marketing\napplication where the generative network is designed with the principle of\nMinimum Description Length (MDL), which is used to learn compact and\ninformative binary codes. Extensive experiments on two public datasets show the\nadvantages for recommendations in various settings over competing baselines and\nanalyze its feasibility in marketing application.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Recommender-Systems", "Scalability", "Tools-&-Libraries", "Datasets", "Compact-Codes"], "tsne_embedding": [26.341794967651367, -26.682037353515625], "cluster": 7}, {"key": "zhang2020deep", "year": "2020", "citations": "9", "title": "Deep Pairwise Hashing For Cold-start Recommendation", "abstract": "<p>Recommendation efficiency and data sparsity problems have been regarded as\ntwo challenges of improving performance for online recommendation. Most of the\nprevious related work focus on improving recommendation accuracy instead of\nefficiency. In this paper, we propose a Deep Pairwise Hashing (DPH) to map\nusers and items to binary vectors in Hamming space, where a user\u2019s preference\nfor an item can be efficiently calculated by Hamming distance, which\nsignificantly improves the efficiency of online recommendation. To alleviate\ndata sparsity and cold-start problems, the user-item interactive information\nand item content information are unified to learn effective representations of\nitems and users. Specifically, we first pre-train robust item representation\nfrom item content data by a Denoising Auto-encoder instead of other\ndeterministic deep learning frameworks; then we finetune the entire framework\nby adding a pairwise loss objective with discrete constraints; moreover, DPH\naims to minimize a pairwise ranking loss that is consistent with the ultimate\ngoal of recommendation. Finally, we adopt the alternating optimization method\nto optimize the proposed model with discrete constraints. Extensive experiments\non three different datasets show that DPH can significantly advance the\nstate-of-the-art frameworks regarding data sparsity and item cold-start\nrecommendation.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Recommender-Systems", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [20.584890365600586, -24.446407318115234], "cluster": 7}, {"key": "zhang2020faster", "year": "2020", "citations": "1", "title": "Faster Binary Embeddings For Preserving Euclidean Distances", "abstract": "<p>We propose a fast, distance-preserving, binary embedding algorithm to\ntransform a high-dimensional dataset \\(\\mathcal{T}\\subseteq\\mathbb{R}^n\\) into\nbinary sequences in the cube \\(\\{\\pm 1\\}^m\\). When \\(\\mathcal{T}\\) consists of\nwell-spread (i.e., non-sparse) vectors, our embedding method applies a stable\nnoise-shaping quantization scheme to \\(A x\\) where \\(A\\in\\mathbb{R}^{m\\times n}\\)\nis a sparse Gaussian random matrix. This contrasts with most binary embedding\nmethods, which usually use \\(x\\mapsto \\mathrm{sign}(Ax)\\) for the embedding.\nMoreover, we show that Euclidean distances among the elements of \\(\\mathcal{T}\\)\nare approximated by the \\(\\ell_1\\) norm on the images of \\(\\{\\pm 1\\}^m\\) under a\nfast linear transformation. This again contrasts with standard methods, where\nthe Hamming distance is used instead. Our method is both fast and memory\nefficient, with time complexity \\(O(m)\\) and space complexity \\(O(m)\\). Further, we\nprove that the method is accurate and its associated error is comparable to\nthat of a continuous valued Johnson-Lindenstrauss embedding plus a quantization\nerror that admits a polynomial decay as the embedding dimension \\(m\\) increases.\nThus the length of the binary codes required to achieve a desired accuracy is\nquite small, and we show it can even be compressed further without compromising\nthe accuracy. To illustrate our results, we test the proposed method on natural\nimages and show that it achieves strong performance.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Quantization", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [12.774893760681152, 51.84617614746094], "cluster": 4}, {"key": "zhang2020learning", "year": "2020", "citations": "23", "title": "Learning To Represent Image And Text With Denotation Graph", "abstract": "<p>Learning to fuse vision and language information and representing them is an\nimportant research problem with many applications. Recent progresses have\nleveraged the ideas of pre-training (from language modeling) and attention\nlayers in Transformers to learn representation from datasets containing images\naligned with linguistic expressions that describe the images. In this paper, we\npropose learning representations from a set of implied, visually grounded\nexpressions between image and text, automatically mined from those datasets. In\nparticular, we use denotation graphs to represent how specific concepts (such\nas sentences describing images) can be linked to abstract and generic concepts\n(such as short phrases) that are also visually grounded. This type of\ngeneric-to-specific relations can be discovered using linguistic analysis\ntools. We propose methods to incorporate such relations into learning\nrepresentation. We show that state-of-the-art multimodal learning models can be\nfurther improved by leveraging automatically harvested structural relations.\nThe representations lead to stronger empirical results on downstream tasks of\ncross-modal image retrieval, referring expression, and compositional\nattribute-object recognition. Both our codes and the extracted denotation\ngraphs on the Flickr30K and the COCO datasets are publically available on\nhttps://sha-lab.github.io/DG.</p>\n", "tags": ["EMNLP", "Image-Retrieval", "Datasets"], "tsne_embedding": [-20.063623428344727, -27.057485580444336], "cluster": 5}, {"key": "zhang2020leveraging", "year": "2021", "citations": "13", "title": "Leveraging Local And Global Descriptors In Parallel To Search Correspondences For Visual Localization", "abstract": "<p>Visual localization to compute 6DoF camera pose from a given image has wide\napplications such as in robotics, virtual reality, augmented reality, etc. Two\nkinds of descriptors are important for the visual localization. One is global\ndescriptors that extract the whole feature from each image. The other is local\ndescriptors that extract the local feature from each image patch usually\nenclosing a key point. More and more methods of the visual localization have\ntwo stages: at first to perform image retrieval by global descriptors and then\nfrom the retrieval feedback to make 2D-3D point correspondences by local\ndescriptors. The two stages are in serial for most of the methods. This simple\ncombination has not achieved superiority of fusing local and global\ndescriptors. The 3D points obtained from the retrieval feedback are as the\nnearest neighbor candidates of the 2D image points only by global descriptors.\nEach of the 2D image points is also called a query local feature when\nperforming the 2D-3D point correspondences. In this paper, we propose a novel\nparallel search framework, which leverages advantages of both local and global\ndescriptors to get nearest neighbor candidates of a query local feature.\nSpecifically, besides using deep learning based global descriptors, we also\nutilize local descriptors to construct random tree structures for obtaining\nnearest neighbor candidates of the query local feature. We propose a new\nprobabilistic model and a new deep learning based local descriptor when\nconstructing the random trees. A weighted Hamming regularization term to keep\ndiscriminativeness after binarization is given in the loss function for the\nproposed local descriptor. The loss function co-trains both real and binary\ndescriptors of which the results are integrated into the random trees.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval", "CVPR"], "tsne_embedding": [-26.176546096801758, 6.464483737945557], "cluster": 0}, {"key": "zhang2020survey", "year": "2020", "citations": "1", "title": "A Survey On Deep Hashing For Image Retrieval", "abstract": "<p>Hashing has been widely used in approximate nearest search for large-scale\ndatabase retrieval for its computation and storage efficiency. Deep hashing,\nwhich devises convolutional neural network architecture to exploit and extract\nthe semantic information or feature of images, has received increasing\nattention recently. In this survey, several deep supervised hashing methods for\nimage retrieval are evaluated and I conclude three main different directions\nfor deep supervised hashing methods. Several comments are made at the end.\nMoreover, to break through the bottleneck of the existing hashing methods, I\npropose a Shadow Recurrent Hashing(SRH) method as a try. Specifically, I devise\na CNN architecture to extract the semantic features of images and design a loss\nfunction to encourage similar images projected close. To this end, I propose a\nconcept: shadow of the CNN output. During optimization process, the CNN output\nand its shadow are guiding each other so as to achieve the optimal solution as\nmuch as possible. Several experiments on dataset CIFAR-10 show the satisfying\nperformance of SRH.</p>\n", "tags": ["Survey-Paper", "Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-49.47819900512695, -1.9395838975906372], "cluster": 0}, {"key": "zhang2021deep", "year": "2021", "citations": "21", "title": "Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval", "abstract": "<p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Neural-Hashing", "Tools-&-Libraries", "Compact-Codes", "Efficiency", "Image-Retrieval", "Memory-Efficiency", "Hashing-Methods", "Scalability"], "tsne_embedding": [-5.082295894622803, -1.5498651266098022], "cluster": 1}, {"key": "zhang2021fast", "year": "2021", "citations": "9", "title": "Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization", "abstract": "<p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Multimodal-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [18.12259864807129, -10.693266868591309], "cluster": 7}, {"key": "zhang2021given", "year": "2021", "citations": "0", "title": "Given Users Recommendations Based On Reviews On Yelp", "abstract": "<p>In our project, we focus on NLP-based hybrid recommendation systems. Our data\nis from Yelp Data. For our hybrid recommendation system, we have two major\ncomponents: the first part is to embed the reviews with the Bert model and\nword2vec model; the second part is the implementation of an item-based\ncollaborative filtering algorithm to compute the similarity of each review\nunder different categories of restaurants. In the end, with the help of\nsimilarity scores, we are able to recommend users the most matched restaurant\nbased on their recorded reviews. The coding work is split into several parts:\nselecting samples and data cleaning, processing, embedding, computing\nsimilarity, and computing prediction and error. Due to the size of the data,\neach part will generate one or more JSON files as the milestone to reduce the\npressure on memory and the communication between each part.</p>\n", "tags": ["Survey-Paper", "Recommender-Systems"], "tsne_embedding": [-40.871192932128906, 27.755279541015625], "cluster": 0}, {"key": "zhang2021hierarchical", "year": "2021", "citations": "2", "title": "Hierarchical Deep Hashing For Fast Large Scale Image Retrieval", "abstract": "<p>Fast image retrieval is of great importance in many computer vision tasks and especially practical applications. Deep hashing, the state-of-the-art fast image retrieval scheme, introduces deep learning to learn the hash functions and generate binary hash codes, and outperforms the other image retrieval methods in terms of accuracy. However, all the existing deep hashing methods could only generate one level hash codes and require a linear traversal of all the hash codes to figure out the closest one when a new query arrives, which is very time-consuming and even intractable for large scale applications. In this work, we propose a Hierarchical Deep Hashing(HDHash) scheme to speed up the state-of-the-art deep hashing methods. More specifically, hierarchical deep hash codes of multiple levels can be generated and indexed with tree structures rather than linear ones, and pruning irrelevant branches can sharply decrease the retrieval time. To our best knowledge, this is the first work to introduce hierarchical indexed deep hashing for fast large scale image retrieval. Extensive experimental results on three benchmark datasets demonstrate that the proposed HDHash scheme achieves better or comparable accuracy with significantly improved efficiency and reduced memory as compared to state-of- the-art fast image retrieval schemes.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-1.7019833326339722, 19.449649810791016], "cluster": 8}, {"key": "zhang2021high", "year": "2021", "citations": "62", "title": "High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval", "abstract": "<p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods", "Multimodal-Retrieval", "Compact-Codes", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [9.101985931396484, 9.735011100769043], "cluster": 6}, {"key": "zhang2021improved", "year": "2021", "citations": "8", "title": "Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval", "abstract": "<p>Deep supervised hashing for image retrieval has attracted researchers\u2019\nattention due to its high efficiency and superior retrieval performance. Most\nexisting deep supervised hashing works, which are based on pairwise/triplet\nlabels, suffer from the expensive computational cost and insufficient\nutilization of the semantics information. Recently, deep classwise hashing\nintroduced a classwise loss supervised by class labels information\nalternatively; however, we find it still has its drawback. In this paper, we\npropose an improved deep classwise hashing, which enables hashing learning and\nclass centers learning simultaneously. Specifically, we design a two-step\nstrategy on center similarity learning. It interacts with the classwise loss to\nattract the class center to concentrate on the intra-class samples while\npushing other class centers as far as possible. The centers similarity learning\ncontributes to generating more compact and discriminative hashing codes. We\nconduct experiments on three benchmark datasets. It shows that the proposed\nmethod effectively surpasses the original method and outperforms\nstate-of-the-art baselines under various commonly-used evaluation metrics for\nimage retrieval.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-5.769750595092773, -2.701383590698242], "cluster": 1}, {"key": "zhang2021instance", "year": "2021", "citations": "2", "title": "Instance-weighted Central Similarity For Multi-label Image Retrieval", "abstract": "<p>Deep hashing has been widely applied to large-scale image retrieval by\nencoding high-dimensional data points into binary codes for efficient\nretrieval. Compared with pairwise/triplet similarity based hash learning,\ncentral similarity based hashing can more efficiently capture the global data\ndistribution. For multi-label image retrieval, however, previous methods only\nuse multiple hash centers with equal weights to generate one centroid as the\nlearning target, which ignores the relationship between the weights of hash\ncenters and the proportion of instance regions in the image. To address the\nabove issue, we propose a two-step alternative optimization approach,\nInstance-weighted Central Similarity (ICS), to automatically learn the center\nweight corresponding to a hash code. Firstly, we apply the maximum entropy\nregularizer to prevent one hash center from dominating the loss function, and\ncompute the center weights via projection gradient descent. Secondly, we update\nneural network parameters by standard back-propagation with fixed center\nweights. More importantly, the learned center weights can well reflect the\nproportion of foreground instances in the image. Our method achieves the\nstate-of-the-art performance on the image retrieval benchmarks, and especially\nimproves the mAP by 1.6%-6.4% on the MS COCO dataset.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Image-Retrieval", "Scalability", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [16.528095245361328, -2.558072805404663], "cluster": 6}, {"key": "zhang2021joint", "year": "2021", "citations": "6", "title": "Joint Learning Of Deep Retrieval Model And Product Quantization Based Embedding Index", "abstract": "<p>Embedding index that enables fast approximate nearest neighbor(ANN) search,\nserves as an indispensable component for state-of-the-art deep retrieval\nsystems. Traditional approaches, often separating the two steps of embedding\nlearning and index building, incur additional indexing time and decayed\nretrieval accuracy. In this paper, we propose a novel method called Poeem,\nwhich stands for product quantization based embedding index jointly trained\nwith deep retrieval model, to unify the two separate steps within an end-to-end\ntraining, by utilizing a few techniques including the gradient straight-through\nestimator, warm start strategy, optimal space decomposition and Givens\nrotation. Extensive experimental results show that the proposed method not only\nimproves retrieval accuracy significantly but also reduces the indexing time to\nalmost none. We have open sourced our approach for the sake of comparison and\nreproducibility.</p>\n", "tags": ["SIGIR", "Vector-Indexing", "Evaluation", "Quantization"], "tsne_embedding": [-3.1309149265289307, -28.199785232543945], "cluster": 3}, {"key": "zhang2021moon", "year": "2021", "citations": "10", "title": "MOON: Multi-hash Codes Joint Learning For Cross-media Retrieval", "abstract": "<p>In recent years, cross-media hashing technique has attracted increasing\nattention for its high computation efficiency and low storage cost. However,\nthe existing approaches still have some limitations, which need to be explored.\n1) A fixed hash length (e.g., 16bits or 32bits) is predefined before learning\nthe binary codes. Therefore, these models need to be retrained when the hash\nlength changes, that consumes additional computation power, reducing the\nscalability in practical applications. 2) Existing cross-modal approaches only\nexplore the information in the original multimedia data to perform the hash\nlearning, without exploiting the semantic information contained in the learned\nhash codes. To this end, we develop a novel Multiple hash cOdes jOint learNing\nmethod (MOON) for cross-media retrieval. Specifically, the developed MOON\nsynchronously learns the hash codes with multiple lengths in a unified\nframework. Besides, to enhance the underlying discrimination, we combine the\nclues from the multimodal data, semantic labels and learned hash codes for hash\nlearning. As far as we know, the proposed MOON is the first work to\nsimultaneously learn different length hash codes without retraining in\ncross-media retrieval. Experiments on several databases show that our MOON can\nachieve promising performance, outperforming some recent competitive shallow\nand deep methods.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Scalability", "Memory-Efficiency", "Tools-&-Libraries", "Compact-Codes", "Evaluation"], "tsne_embedding": [13.340540885925293, -3.7428455352783203], "cluster": 6}, {"key": "zhang2021mr", "year": "2021", "citations": "25", "title": "Mr. Tydi: A Multi-lingual Benchmark For Dense Retrieval", "abstract": "<p>We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual\nretrieval in eleven typologically diverse languages, designed to evaluate\nranking with learned dense representations. The goal of this resource is to\nspur research in dense retrieval techniques in non-English languages, motivated\nby recent observations that existing techniques for representation learning\nperform poorly when applied to out-of-distribution data. As a starting point,\nwe provide zero-shot baselines for this new dataset based on a multi-lingual\nadaptation of DPR that we call \u201cmDPR\u201d. Experiments show that although the\neffectiveness of mDPR is much lower than BM25, dense representations\nnevertheless appear to provide valuable relevance signals, improving BM25\nresults in sparse-dense hybrids. In addition to analyses of our results, we\nalso discuss future challenges and present a research agenda in multi-lingual\ndense retrieval. Mr. TyDi can be downloaded at\nhttps://github.com/castorini/mr.tydi.</p>\n", "tags": ["Evaluation", "Few-Shot-&-Zero-Shot", "Datasets"], "tsne_embedding": [-27.68723487854004, 26.82502555847168], "cluster": 8}, {"key": "zhang2021orthonormal", "year": "2023", "citations": "9", "title": "Orthonormal Product Quantization Network For Scalable Face Image Retrieval", "abstract": "<p>Existing deep quantization methods provided an efficient solution for\nlarge-scale image retrieval. However, the significant intra-class variations\nlike pose, illumination, and expressions in face images, still pose a challenge\nfor face image retrieval. In light of this, face image retrieval requires\nsufficiently powerful learning metrics, which are absent in current deep\nquantization works. Moreover, to tackle the growing unseen identities in the\nquery stage, face image retrieval drives more demands regarding model\ngeneralization and system scalability than general image retrieval tasks. This\npaper integrates product quantization with orthonormal constraints into an\nend-to-end deep learning framework to effectively retrieve face images.\nSpecifically, a novel scheme that uses predefined orthonormal vectors as\ncodewords is proposed to enhance the quantization informativeness and reduce\ncodewords\u2019 redundancy. A tailored loss function maximizes discriminability\namong identities in each quantization subspace for both the quantized and\noriginal features. An entropy-based regularization term is imposed to reduce\nthe quantization error. Experiments are conducted on four commonly-used face\ndatasets under both seen and unseen identities retrieval settings. Our method\noutperforms all the compared deep hashing/quantization state-of-the-arts under\nboth settings. Results validate the effectiveness of the proposed orthonormal\ncodewords in improving models\u2019 standard retrieval performance and\ngeneralization ability. Combing with further experiments on two general image\ndatasets, it demonstrates the broad superiority of our method for scalable\nimage retrieval.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Neural-Hashing", "Quantization", "Tools-&-Libraries", "Image-Retrieval", "Hashing-Methods", "Scalability"], "tsne_embedding": [-25.336166381835938, -0.9795999526977539], "cluster": 1}, {"key": "zhang2021visual", "year": "2018", "citations": "58", "title": "Visual Search At Alibaba", "abstract": "<p>This paper introduces the large scale visual search algorithm and system\ninfrastructure at Alibaba. The following challenges are discussed under the\nE-commercial circumstance at Alibaba (a) how to handle heterogeneous image data\nand bridge the gap between real-shot images from user query and the online\nimages. (b) how to deal with large scale indexing for massive updating data.\n(c) how to train deep models for effective feature representation without huge\nhuman annotations. (d) how to improve the user engagement by considering the\nquality of the content. We take advantage of large image collection of Alibaba\nand state-of-the-art deep learning techniques to perform visual search at\nscale. We present solutions and implementation details to overcome those\nproblems and also share our learnings from building such a large scale\ncommercial visual search engine. Specifically, model and search-based fusion\napproach is introduced to effectively predict categories. Also, we propose a\ndeep CNN model for joint detection and feature learning by mining user click\nbehavior. The binary index engine is designed to scale up indexing without\ncompromising recall and precision. Finally, we apply all the stages into an\nend-to-end system architecture, which can simultaneously achieve highly\nefficient and scalable performance adapting to real-shot images. Extensive\nexperiments demonstrate the advancement of each module in our system. We hope\nvisual search at Alibaba becomes more widely incorporated into today\u2019s\ncommercial applications.</p>\n", "tags": ["KDD", "Evaluation", "Image-Retrieval"], "tsne_embedding": [-13.402115821838379, -40.73881912231445], "cluster": 3}, {"key": "zhang2021vldeformer", "year": "2022", "citations": "18", "title": "Vldeformer: Vision-language Decomposed Transformer For Fast Cross-modal Retrieval", "abstract": "<p>Cross-model retrieval has emerged as one of the most important upgrades for\ntext-only search engines (SE). Recently, with powerful representation for\npairwise text-image inputs via early interaction, the accuracy of\nvision-language (VL) transformers has outperformed existing methods for\ntext-image retrieval. However, when the same paradigm is used for inference,\nthe efficiency of the VL transformers is still too low to be applied in a real\ncross-modal SE. Inspired by the mechanism of human learning and using\ncross-modal knowledge, this paper presents a novel Vision-Language Decomposed\nTransformer (VLDeformer), which greatly increases the efficiency of VL\ntransformers while maintaining their outstanding accuracy. By the proposed\nmethod, the cross-model retrieval is separated into two stages: the VL\ntransformer learning stage, and the VL decomposition stage. The latter stage\nplays the role of single modal indexing, which is to some extent like the term\nindexing of a text SE. The model learns cross-modal knowledge from\nearly-interaction pre-training and is then decomposed into an individual\nencoder. The decomposition requires only small target datasets for supervision\nand achieves both \\(1000+\\) times acceleration and less than \\(0.6\\)% average\nrecall drop. VLDeformer also outperforms state-of-the-art visual-semantic\nembedding methods on COCO and Flickr30k.</p>\n", "tags": ["Efficiency", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-32.11014175415039, -14.452215194702148], "cluster": 5}, {"key": "zhang2022discriminative", "year": "2022", "citations": "0", "title": "Discriminative Supervised Subspace Learning For Cross-modal Retrieval", "abstract": "<p>Nowadays the measure between heterogeneous data is still an open problem for\ncross-modal retrieval. The core of cross-modal retrieval is how to measure the\nsimilarity between different types of data. Many approaches have been developed\nto solve the problem. As one of the mainstream, approaches based on subspace\nlearning pay attention to learning a common subspace where the similarity among\nmulti-modal data can be measured directly. However, many of the existing\napproaches only focus on learning a latent subspace. They ignore the full use\nof discriminative information so that the semantically structural information\nis not well preserved. Therefore satisfactory results can not be achieved as\nexpected. We in this paper propose a discriminative supervised subspace\nlearning for cross-modal retrieval(DS2L), to make full use of discriminative\ninformation and better preserve the semantically structural information.\nSpecifically, we first construct a shared semantic graph to preserve the\nsemantic structure within each modality. Subsequently, the Hilbert-Schmidt\nIndependence Criterion(HSIC) is introduced to preserve the consistence between\nfeature-similarity and semantic-similarity of samples. Thirdly, we introduce a\nsimilarity preservation term, thus our model can compensate for the\nshortcomings of insufficient use of discriminative data and better preserve the\nsemantically structural information within each modality. The experimental\nresults obtained on three well-known benchmark datasets demonstrate the\neffectiveness and competitiveness of the proposed method against the compared\nclassic subspace learning approaches.</p>\n", "tags": ["Supervised", "Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [8.61398983001709, -2.822436809539795], "cluster": 6}, {"key": "zhang2022hashing", "year": "2022", "citations": "2", "title": "Hashing Learning With Hyper-class Representation", "abstract": "<p>Existing unsupervised hash learning is a kind of attribute-centered\ncalculation. It may not accurately preserve the similarity between data. This\nleads to low down the performance of hash function learning. In this paper, a\nhash algorithm is proposed with a hyper-class representation. It is a two-steps\napproach. The first step finds potential decision features and establish\nhyper-class. The second step constructs hash learning based on the hyper-class\ninformation in the first step, so that the hash codes of the data within the\nhyper-class are as similar as possible, as well as the hash codes of the data\nbetween the hyper-classes are as different as possible. To evaluate the\nefficiency, a series of experiments are conducted on four public datasets. The\nexperimental results show that the proposed hash algorithm is more efficient\nthan the compared algorithms, in terms of mean average precision (MAP), average\nprecision (AP) and Hamming radius 2 (HAM2)</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [30.87445068359375, -1.7467252016067505], "cluster": 9}, {"key": "zhang2022hybrid", "year": "2023", "citations": "3", "title": "Hybrid Inverted Index Is A Robust Accelerator For Dense Retrieval", "abstract": "<p>Inverted file structure is a common technique for accelerating dense\nretrieval. It clusters documents based on their embeddings; during searching,\nit probes nearby clusters w.r.t. an input query and only evaluates documents\nwithin them by subsequent codecs, thus avoiding the expensive cost of\nexhaustive traversal. However, the clustering is always lossy, which results in\nthe miss of relevant documents in the probed clusters and hence degrades\nretrieval quality. In contrast, lexical matching, such as overlaps of salient\nterms, tends to be strong feature for identifying relevant documents. In this\nwork, we present the Hybrid Inverted Index (HI\\(^2\\)), where the embedding\nclusters and salient terms work collaboratively to accelerate dense retrieval.\nTo make best of both effectiveness and efficiency, we devise a cluster selector\nand a term selector, to construct compact inverted lists and efficiently\nsearching through them. Moreover, we leverage simple unsupervised algorithms as\nwell as end-to-end knowledge distillation to learn these two modules, with the\nlatter further boosting the effectiveness. Based on comprehensive experiments\non popular retrieval benchmarks, we verify that clusters and terms indeed\ncomplement each other, enabling HI\\(^2\\) to achieve lossless retrieval quality\nwith competitive efficiency across various index settings. Our code and\ncheckpoint are publicly available at\nhttps://github.com/namespace-Pt/Adon/tree/HI2.</p>\n", "tags": ["Efficiency", "Vector-Indexing", "EMNLP", "Unsupervised"], "tsne_embedding": [25.98702621459961, 21.531177520751953], "cluster": 2}, {"key": "zhang2022instance", "year": "2023", "citations": "0", "title": "Instance Image Retrieval By Learning Purely From Within The Dataset", "abstract": "<p>Quality feature representation is key to instance image retrieval. To attain\nit, existing methods usually resort to a deep model pre-trained on benchmark\ndatasets or even fine-tune the model with a task-dependent labelled auxiliary\ndataset. Although achieving promising results, this approach is restricted by\ntwo issues: 1) the domain gap between benchmark datasets and the dataset of a\ngiven retrieval task; 2) the required auxiliary dataset cannot be readily\nobtained. In light of this situation, this work looks into a different approach\nwhich has not been well investigated for instance image retrieval previously:\n{can we learn feature representation \\textit{specific to} a given retrieval\ntask in order to achieve excellent retrieval?} Our finding is encouraging. By\nadding an object proposal generator to generate image regions for\nself-supervised learning, the investigated approach can successfully learn\nfeature representation specific to a given dataset for retrieval. This\nrepresentation can be made even more effective by boosting it with image\nsimilarity information mined from the dataset. As experimentally validated,\nsuch a simple ``self-supervised learning + self-boosting\u2019\u2019 approach can well\ncompete with the relevant state-of-the-art retrieval methods. Ablation study is\nconducted to show the appealing properties of this approach and its limitation\non generalisation across datasets.</p>\n", "tags": ["Self-Supervised", "Image-Retrieval", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-27.87969207763672, -23.855775833129883], "cluster": 5}, {"key": "zhang2022lightfr", "year": "2022", "citations": "43", "title": "Lightfr: Lightweight Federated Recommendation With Privacy-preserving Matrix Factorization", "abstract": "<p>Federated recommender system (FRS), which enables many local devices to train\na shared model jointly without transmitting local raw data, has become a\nprevalent recommendation paradigm with privacy-preserving advantages. However,\nprevious work on FRS performs similarity search via inner product in continuous\nembedding space, which causes an efficiency bottleneck when the scale of items\nis extremely large. We argue that such a scheme in federated settings ignores\nthe limited capacities in resource-constrained user devices (i.e., storage\nspace, computational overhead, and communication bandwidth), and makes it\nharder to be deployed in large-scale recommender systems. Besides, it has been\nshown that transmitting local gradients in real-valued form between server and\nclients may leak users\u2019 private information. To this end, we propose a\nlightweight federated recommendation framework with privacy-preserving matrix\nfactorization, LightFR, that is able to generate high-quality binary codes by\nexploiting learning to hash technique under federated settings, and thus enjoys\nboth fast online inference and economic memory consumption. Moreover, we devise\nan efficient federated discrete optimization algorithm to collaboratively train\nmodel parameters between the server and clients, which can effectively prevent\nreal-valued gradient attacks from malicious parties. Through extensive\nexperiments on four real-world datasets, we show that our LightFR model\noutperforms several state-of-the-art FRS methods in terms of recommendation\naccuracy, inference efficiency and data privacy.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "Recommender-Systems", "Scalability", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Compact-Codes"], "tsne_embedding": [37.11762619018555, 3.4493789672851562], "cluster": 9}, {"key": "zhang2022multi", "year": "2022", "citations": "38", "title": "Multi-view Document Representation Learning For Open-domain Dense Retrieval", "abstract": "<p>Dense retrieval has achieved impressive advances in first-stage retrieval\nfrom a large-scale document collection, which is built on bi-encoder\narchitecture to produce single vector representation of query and document.\nHowever, a document can usually answer multiple potential queries from\ndifferent views. So the single vector representation of a document is hard to\nmatch with multi-view queries, and faces a semantic mismatch problem. This\npaper proposes a multi-view document representation learning framework, aiming\nto produce multi-view embeddings to represent documents and enforce them to\nalign with different queries. First, we propose a simple yet effective method\nof generating multiple embeddings through viewers. Second, to prevent\nmulti-view embeddings from collapsing to the same one, we further propose a\nglobal-local loss with annealed temperature to encourage the multiple viewers\nto better align with different potential queries. Experiments show our method\noutperforms recent works and achieves state-of-the-art results.</p>\n", "tags": ["Tools-&-Libraries", "Scalability"], "tsne_embedding": [6.757589817047119, -21.067548751831055], "cluster": 7}, {"key": "zhang2022supervised", "year": "2022", "citations": "0", "title": "Supervised Deep Hashing For High-dimensional And Heterogeneous Case-based Reasoning", "abstract": "<p>Case-based Reasoning (CBR) on high-dimensional and heterogeneous data is a\ntrending yet challenging and computationally expensive task in the real world.\nA promising approach is to obtain low-dimensional hash codes representing cases\nand perform a similarity retrieval of cases in Hamming space. However, previous\nmethods based on data-independent hashing rely on random projections or manual\nconstruction, inapplicable to address specific data issues (e.g.,\nhigh-dimensionality and heterogeneity) due to their insensitivity to data\ncharacteristics. To address these issues, this work introduces a novel deep\nhashing network to learn similarity-preserving compact hash codes for efficient\ncase retrieval and proposes a deep-hashing-enabled CBR model HeCBR.\nSpecifically, we introduce position embedding to represent heterogeneous\nfeatures and utilize a multilinear interaction layer to obtain case embeddings,\nwhich effectively filtrates zero-valued features to tackle high-dimensionality\nand sparsity and captures inter-feature couplings. Then, we feed the case\nembeddings into fully-connected layers, and subsequently a hash layer generates\nhash codes with a quantization regularizer to control the quantization loss\nduring relaxation. To cater to incremental learning of CBR, we further propose\nan adaptive learning strategy to update the hash function. Extensive\nexperiments on public datasets show that HeCBR greatly reduces storage and\nsignificantly accelerates case retrieval. HeCBR achieves desirable performance\ncompared with the state-of-the-art CBR methods and performs significantly\nbetter than hashing-based CBR methods in classification.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Neural-Hashing", "Quantization", "Similarity-Search", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [9.179312705993652, 10.795818328857422], "cluster": 6}, {"key": "zhang2023graph", "year": "2023", "citations": "9", "title": "Graph Convolution Based Efficient Re-ranking For Visual Retrieval", "abstract": "<p>Visual retrieval tasks such as image retrieval and person re-identification\n(Re-ID) aim at effectively and thoroughly searching images with similar content\nor the same identity. After obtaining retrieved examples, re-ranking is a\nwidely adopted post-processing step to reorder and improve the initial\nretrieval results by making use of the contextual information from semantically\nneighboring samples. Prevailing re-ranking approaches update distance metrics\nand mostly rely on inefficient crosscheck set comparison operations while\ncomputing expanded neighbors based distances. In this work, we present an\nefficient re-ranking method which refines initial retrieval results by updating\nfeatures. Specifically, we reformulate re-ranking based on Graph Convolution\nNetworks (GCN) and propose a novel Graph Convolution based Re-ranking (GCR) for\nvisual retrieval tasks via feature propagation. To accelerate computation for\nlarge-scale retrieval, a decentralized and synchronous feature propagation\nalgorithm which supports parallel or distributed computing is introduced. In\nparticular, the plain GCR is extended for cross-camera retrieval and an\nimproved feature propagation formulation is presented to leverage affinity\nrelationships across different cameras. It is also extended for video-based\nretrieval, and Graph Convolution based Re-ranking for Video (GCRV) is proposed\nby mathematically deriving a novel profile vector generation method for the\ntracklet. Without bells and whistles, the proposed approaches achieve\nstate-of-the-art performances on seven benchmark datasets from three different\ntasks, i.e., image retrieval, person Re-ID and video-based person Re-ID.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Scalability", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [60.99776840209961, -3.0557444095611572], "cluster": 9}, {"key": "zhang2023image", "year": "2023", "citations": "3", "title": "Image-text Retrieval Via Preserving Main Semantics Of Vision", "abstract": "<p>Image-text retrieval is one of the major tasks of cross-modal retrieval.\nSeveral approaches for this task map images and texts into a common space to\ncreate correspondences between the two modalities. However, due to the content\n(semantics) richness of an image, redundant secondary information in an image\nmay cause false matches. To address this issue, this paper presents a semantic\noptimization approach, implemented as a Visual Semantic Loss (VSL), to assist\nthe model in focusing on an image\u2019s main content. This approach is inspired by\nhow people typically annotate the content of an image by describing its main\ncontent. Thus, we leverage the annotated texts corresponding to an image to\nassist the model in capturing the main content of the image, reducing the\nnegative impact of secondary content. Extensive experiments on two benchmark\ndatasets (MSCOCO and Flickr30K) demonstrate the superior performance of our\nmethod. The code is available at: https://github.com/ZhangXu0963/VSL.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-29.13155746459961, -27.416913986206055], "cluster": 5}, {"key": "zhang2023model", "year": "2023", "citations": "0", "title": "Model-enhanced Vector Index", "abstract": "<p>Embedding-based retrieval methods construct vector indices to search for\ndocument representations that are most similar to the query representations.\nThey are widely used in document retrieval due to low latency and decent recall\nperformance. Recent research indicates that deep retrieval solutions offer\nbetter model quality, but are hindered by unacceptable serving latency and the\ninability to support document updates. In this paper, we aim to enhance the\nvector index with end-to-end deep generative models, leveraging the\ndifferentiable advantages of deep retrieval models while maintaining desirable\nserving efficiency. We propose Model-enhanced Vector Index (MEVI), a\ndifferentiable model-enhanced index empowered by a twin-tower representation\nmodel. MEVI leverages a Residual Quantization (RQ) codebook to bridge the\nsequence-to-sequence deep retrieval and embedding-based models. To\nsubstantially reduce the inference time, instead of decoding the unique\ndocument ids in long sequential steps, we first generate some semantic virtual\ncluster ids of candidate documents in a small number of steps, and then\nleverage the well-adapted embedding vectors to further perform a fine-grained\nsearch for the relevant documents in the candidate virtual clusters. We\nempirically show that our model achieves better performance on the commonly\nused academic benchmarks MSMARCO Passage and Natural Questions, with comparable\nserving latency to dense retrieval solutions.</p>\n", "tags": ["Text-Retrieval", "Efficiency", "Quantization", "Vector-Indexing", "Evaluation"], "tsne_embedding": [6.196103096008301, -20.890628814697266], "cluster": 7}, {"key": "zhang2023new", "year": "2023", "citations": "1", "title": "A New Fine-grained Alignment Method For Image-text Matching", "abstract": "<p>Image-text retrieval is a widely studied topic in the field of computer\nvision due to the exponential growth of multimedia data, whose core concept is\nto measure the similarity between images and text. However, most existing\nretrieval methods heavily rely on cross-attention mechanisms for cross-modal\nfine-grained alignment, which takes into account excessive irrelevant regions\nand treats prominent and non-significant words equally, thereby limiting\nretrieval accuracy. This paper aims to investigate an alignment approach that\nreduces the involvement of non-significant fragments in images and text while\nenhancing the alignment of prominent segments. For this purpose, we introduce\nthe Cross-Modal Prominent Fragments Enhancement Aligning Network(CPFEAN), which\nachieves improved retrieval accuracy by diminishing the participation of\nirrelevant regions during alignment and relatively increasing the alignment\nsimilarity of prominent words. Additionally, we incorporate prior textual\ninformation into image regions to reduce misalignment occurrences. In practice,\nwe first design a novel intra-modal fragments relationship reasoning method,\nand subsequently employ our proposed alignment mechanism to compute the\nsimilarity between images and text. Extensive quantitative comparative\nexperiments on MS-COCO and Flickr30K datasets demonstrate that our approach\noutperforms state-of-the-art methods by about 5% to 10% in the rSum metric.</p>\n", "tags": ["Text-Retrieval", "Datasets"], "tsne_embedding": [-22.31806755065918, -3.9984517097473145], "cluster": 1}, {"key": "zhang2023retsim", "year": "2023", "citations": "0", "title": "Retsim: Resilient And Efficient Text Similarity", "abstract": "<p>This paper introduces RETSim (Resilient and Efficient Text Similarity), a\nlightweight, multilingual deep learning model trained to produce robust metric\nembeddings for near-duplicate text retrieval, clustering, and dataset\ndeduplication tasks. We demonstrate that RETSim is significantly more robust\nand accurate than MinHash and neural text embeddings, achieving new\nstate-of-the-art performance on dataset deduplication, adversarial text\nretrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D\nbenchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual,\nnear-duplicate text retrieval capabilities under adversarial settings. RETSim\nand the W4NT3D benchmark are open-sourced under the MIT License at\nhttps://github.com/google/unisim.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Text-Retrieval", "Robustness", "Datasets", "Evaluation"], "tsne_embedding": [3.7953832149505615, -32.53529739379883], "cluster": 3}, {"key": "zhang2024demo", "year": "2024", "citations": "0", "title": "DEMO: A Statistical Perspective For Efficient Image-text Matching", "abstract": "<p>Image-text matching has been a long-standing problem, which seeks to connect\nvision and language through semantic understanding. Due to the capability to\nmanage large-scale raw data, unsupervised hashing-based approaches have gained\nprominence recently. They typically construct a semantic similarity structure\nusing the natural distance, which subsequently provides guidance to the model\noptimization process. However, the similarity structure could be biased at the\nboundaries of semantic distributions, causing error accumulation during\nsequential optimization. To tackle this, we introduce a novel hashing approach\ntermed Distribution-based Structure Mining with Consistency Learning (DEMO) for\nefficient image-text matching. From a statistical view, DEMO characterizes each\nimage using multiple augmented views, which are considered as samples drawn\nfrom its intrinsic semantic distribution. Then, we employ a non-parametric\ndistribution divergence to ensure a robust and precise similarity structure. In\naddition, we introduce collaborative consistency learning which not only\npreserves the similarity structure in the Hamming space but also encourages\nconsistency between retrieval distribution from different directions in a\nself-supervised manner. Through extensive experiments on three benchmark\nimage-text matching datasets, we demonstrate that DEMO achieves superior\nperformance compared with many state-of-the-art methods.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Scalability", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [2.8800618648529053, -1.106497049331665], "cluster": 6}, {"key": "zhang2024efficient", "year": "2024", "citations": "0", "title": "Efficient And Effective Retrieval Of Dense-sparse Hybrid Vectors Using Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>ANNS for embedded vector representations of texts is commonly used in\ninformation retrieval, with two important information representations being\nsparse and dense vectors. While it has been shown that combining these\nrepresentations improves accuracy, the current method of conducting sparse and\ndense vector searches separately suffers from low scalability and high system\ncomplexity. Alternatively, building a unified index faces challenges with\naccuracy and efficiency. To address these issues, we propose a graph-based ANNS\nalgorithm for dense-sparse hybrid vectors. Firstly, we propose a distribution\nalignment method to improve accuracy, which pre-samples dense and sparse\nvectors to analyze their distance distribution statistic, resulting in a\n1%\\(\\sim\\)9% increase in accuracy. Secondly, to improve efficiency, we design an\nadaptive two-stage computation strategy that initially computes dense distances\nonly and later computes hybrid distances. Further, we prune the sparse vectors\nto speed up the calculation. Compared to naive implementation, we achieve\n\\(\\sim2.1\\times\\) acceleration. Thorough experiments show that our algorithm\nachieves 8.9x\\(\\sim\\)11.7x throughput at equal accuracy compared to existing\nhybrid vector search algorithms.</p>\n", "tags": ["Efficiency", "Graph-Based-Ann", "Scalability"], "tsne_embedding": [40.60600662231445, 23.06900978088379], "cluster": 2}, {"key": "zhang2024enhanced", "year": "2024", "citations": "0", "title": "An Enhanced Batch Query Architecture In Real-time Recommendation", "abstract": "<p>In industrial recommendation systems on websites and apps, it is essential to\nrecall and predict top-n results relevant to user interests from a content pool\nof billions within milliseconds. To cope with continuous data growth and\nimprove real-time recommendation performance, we have designed and implemented\na high-performance batch query architecture for real-time recommendation\nsystems. Our contributions include optimizing hash structures with a\ncacheline-aware probing method to enhance coalesced hashing, as well as the\nimplementation of a hybrid storage key-value service built upon it. Our\nexperiments indicate this approach significantly surpasses conventional hash\ntables in batch query throughput, achieving up to 90% of the query throughput\nof random memory access when incorporating parallel optimization. The support\nfor NVMe, integrating two-tier storage for hot and cold data, notably reduces\nresource consumption. Additionally, the system facilitates dynamic updates,\nautomated sharding of attributes and feature embedding tables, and introduces\ninnovative protocols for consistency in batch queries, thereby enhancing the\neffectiveness of real-time incremental learning updates. This architecture has\nbeen deployed and in use in the bilibili recommendation system for over a year,\na video content community with hundreds of millions of users, supporting 10x\nincrease in model computation with minimal resource growth, improving outcomes\nwhile preserving the system\u2019s real-time performance.</p>\n", "tags": ["Hashing-Methods", "CIKM", "Efficiency", "Recommender-Systems", "Evaluation"], "tsne_embedding": [31.41153907775879, 9.937618255615234], "cluster": 2}, {"key": "zhang2024kp", "year": "2024", "citations": "1", "title": "KP-RED: Exploiting Semantic Keypoints For Joint 3D Shape Retrieval And Deformation", "abstract": "<p>In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and\nDeformation framework that takes object scans as input and jointly retrieves\nand deforms the most geometrically similar CAD models from a pre-processed\ndatabase to tightly match the target. Unlike existing dense matching based\nmethods that typically struggle with noisy partial scans, we propose to\nleverage category-consistent sparse keypoints to naturally handle both full and\npartial object scans. Specifically, we first employ a lightweight retrieval\nmodule to establish a keypoint-based embedding space, measuring the similarity\namong objects by dynamically aggregating deformation-aware local-global\nfeatures around extracted keypoints. Objects that are close in the embedding\nspace are considered similar in geometry. Then we introduce the neural\ncage-based deformation module that estimates the influence vector of each\nkeypoint upon cage vertices inside its local support region to control the\ndeformation of the retrieved shape. Extensive experiments on the synthetic\ndataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED\nsurpasses existing state-of-the-art approaches by a large margin. Codes and\ntrained models are released on https://github.com/lolrudy/KP-RED.</p>\n", "tags": ["Tools-&-Libraries", "CVPR", "Datasets"], "tsne_embedding": [-37.112937927246094, -14.379559516906738], "cluster": 5}, {"key": "zhang2024learning", "year": "2024", "citations": "0", "title": "Learning Id-free Item Representation With Token Crossing For Multimodal Recommendation", "abstract": "<p>Current multimodal recommendation models have extensively explored the\neffective utilization of multimodal information; however, their reliance on ID\nembeddings remains a performance bottleneck. Even with the assistance of\nmultimodal information, optimizing ID embeddings remains challenging for\nID-based Multimodal Recommender when interaction data is sparse. Furthermore,\nthe unique nature of item-specific ID embeddings hinders the information\nexchange among related items and the spatial requirement of ID embeddings\nincreases with the scale of item. Based on these limitations, we propose an\nID-free MultimOdal TOken Representation scheme named MOTOR that represents each\nitem using learnable multimodal tokens and connects them through shared tokens.\nSpecifically, we first employ product quantization to discretize each item\u2019s\nmultimodal features (e.g., images, text) into discrete token IDs. We then\ninterpret the token embeddings corresponding to these token IDs as implicit\nitem features, introducing a new Token Cross Network to capture the implicit\ninteraction patterns among these tokens. The resulting representations can\nreplace the original ID embeddings and transform the original ID-based\nmultimodal recommender into ID-free system, without introducing any additional\nloss design. MOTOR reduces the overall space requirements of these models,\nfacilitating information interaction among related items, while also\nsignificantly enhancing the model\u2019s recommendation capability. Extensive\nexperiments on nine mainstream models demonstrate the significant performance\nimprovement achieved by MOTOR, highlighting its effectiveness in enhancing\nmultimodal recommendation systems.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Evaluation"], "tsne_embedding": [21.51804542541504, -20.942790985107422], "cluster": 7}, {"key": "zhang2024magiclens", "year": "2024", "citations": "0", "title": "Magiclens: Self-supervised Image Retrieval With Open-ended Instructions", "abstract": "<p>Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent works leverage text\ninstructions to allow users to more freely express their search intents.\nHowever, they primarily focus on image pairs that are visually similar and/or\ncan be characterized by a small set of pre-defined relations. The core thesis\nof this paper is that text instructions can enable retrieving images with\nricher relations beyond visual similarity. To show this, we introduce\nMagicLens, a series of self-supervised image retrieval models that support\nopen-ended instructions. MagicLens is built on a key novel insight: image pairs\nthat naturally occur on the same web pages contain a wide range of implicit\nrelations (e.g., inside view of), and we can bring those implicit relations\nexplicit by synthesizing instructions via foundation models. Trained on 36.7M\n(query image, instruction, target image) triplets with rich semantic relations\nmined from the web, MagicLens achieves results comparable with or better than\nprior best on eight benchmarks of various image retrieval tasks, while\nmaintaining high parameter efficiency with a significantly smaller model size.\nAdditional human analyses on a 1.4M-image unseen corpus further demonstrate the\ndiversity of search intents supported by MagicLens. Code and models are\npublicly available at https://open-vision-language.github.io/MagicLens/.</p>\n", "tags": ["Supervised", "Self-Supervised", "Image-Retrieval", "Efficiency"], "tsne_embedding": [-21.675491333007812, -32.49644470214844], "cluster": 5}, {"key": "zhang2024notellm", "year": "2024", "citations": "6", "title": "Notellm: A Retrievable Large Language Model For Note Recommendation", "abstract": "<p>People enjoy sharing \u201cnotes\u201d including their experiences within online\ncommunities. Therefore, recommending notes aligned with user interests has\nbecome a crucial task. Existing online methods only input notes into BERT-based\nmodels to generate note embeddings for assessing similarity. However, they may\nunderutilize some important cues, e.g., hashtags or categories, which represent\nthe key concepts of notes. Indeed, learning to generate hashtags/categories can\npotentially enhance note embeddings, both of which compress key note\ninformation into limited content. Besides, Large Language Models (LLMs) have\nsignificantly outperformed BERT in understanding natural languages. It is\npromising to introduce LLMs into note recommendation. In this paper, we propose\na novel unified framework called NoteLLM, which leverages LLMs to address the\nitem-to-item (I2I) note recommendation. Specifically, we utilize Note\nCompression Prompt to compress a note into a single special token, and further\nlearn the potentially related notes\u2019 embeddings via a contrastive learning\napproach. Moreover, we use NoteLLM to summarize the note and generate the\nhashtag/category automatically through instruction tuning. Extensive\nvalidations on real scenarios demonstrate the effectiveness of our proposed\nmethod compared with the online baseline and show major improvements in the\nrecommendation system of Xiaohongshu.</p>\n", "tags": ["Self-Supervised", "Tools-&-Libraries", "Recommender-Systems"], "tsne_embedding": [19.288368225097656, -26.010835647583008], "cluster": 7}, {"key": "zhang2025binary", "year": "2013", "citations": "98", "title": "Binary Code Ranking With Weighted Hamming Distance", "abstract": "<p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most\nexisting binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or\nsimilarity of two points are approximated by the Hamming\ndistance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are\noften lots of results sharing the same Hamming distance to\na query, which makes this distance measure ambiguous and\nposes a critical issue for similarity search where ranking is\nimportant. In this paper, we propose a weighted Hamming\ndistance ranking algorithm (WhRank) to rank the binary\ncodes of hashing methods. By assigning different bit-level\nweights to different hash bits, the returned binary codes\nare ranked at a finer-grained binary code level. We give\nan algorithm to learn the data-adaptive and query-sensitive\nweight for each hash bit. Evaluations on two large-scale\nimage data sets demonstrate the efficacy of our weighted\nHamming distance for binary code ranking.</p>\n", "tags": ["Hashing-Methods", "Efficiency", "CVPR", "Scalability", "Similarity-Search", "Compact-Codes"], "tsne_embedding": [-2.404053211212158, 28.576297760009766], "cluster": 8}, {"key": "zhang2025bit", "year": "2015", "citations": "418", "title": "Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification", "abstract": "<p>Extracting informative image features and learning\neffective approximate hashing functions are two crucial steps in\nimage retrieval . Conventional methods often study these two\nsteps separately, e.g., learning hash functions from a predefined\nhand-crafted feature space. Meanwhile, the bit lengths of output\nhashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised\nlearning framework to generate compact and bit-scalable hashing\ncodes directly from raw images. We pose hashing learning as\na problem of regularized similarity learning. Specifically, we\norganize the training images into a batch of triplet samples,\neach sample containing two images with the same label and one\nwith a different label. With these triplet samples, we maximize\nthe margin between matched pairs and mismatched pairs in the\nHamming space. In addition, a regularization term is introduced\nto enforce the adjacency consistency, i.e., images of similar\nappearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end\nfashion, where discriminative image features and hash functions\nare simultaneously optimized. Furthermore, each bit of our\nhashing codes is unequally weighted so that we can manipulate\nthe code lengths by truncating the insignificant bits. Our\nframework outperforms state-of-the-arts on public benchmarks\nof similar image search and also achieves promising results in\nthe application of person re-identification in surveillance. It is\nalso shown that the generated bit-scalable hashing codes well\npreserve the discriminative powers with shorter code lengths.</p>\n", "tags": ["Hashing-Methods", "Image-Retrieval", "Tools-&-Libraries", "Supervised", "Neural-Hashing"], "tsne_embedding": [-7.243420600891113, 2.0529332160949707], "cluster": 1}, {"key": "zhang2025composite", "year": "2011", "citations": "221", "title": "Composite Hashing With Multiple Information Sources", "abstract": "<p>Similarity search applications with a large amount of text\nand image data demands an efficient and effective solution.\nOne useful strategy is to represent the examples in databases\nas compact binary codes through semantic hashing, which\nhas attracted much attention due to its fast query/search\nspeed and drastically reduced storage requirement. All of\nthe current semantic hashing methods only deal with the\ncase when each example is represented by one type of features.\nHowever, examples are often described from several\ndifferent information sources in many real world applications.\nFor example, the characteristics of a webpage can be\nderived from both its content part and its associated links.\nTo address the problem of learning good hashing codes in\nthis scenario, we propose a novel research problem \u2013 Composite\nHashing with Multiple Information Sources (CHMIS).\nThe focus of the new research problem is to design an algorithm\nfor incorporating the features from different information\nsources into the binary hashing codes efficiently and\neffectively. In particular, we propose an algorithm CHMISAW\n(CHMIS with Adjusted Weights) for learning the codes.\nThe proposed algorithm integrates information from several\ndifferent sources into the binary hashing codes by adjusting\nthe weights on each individual source for maximizing\nthe coding performance, and enables fast conversion from\nquery examples to their binary hashing codes. Experimental\nresults on five different datasets demonstrate the superior\nperformance of the proposed method against several other\nstate-of-the-art semantic hashing techniques.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Similarity-Search", "SIGIR", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [13.318289756774902, 12.074524879455566], "cluster": 6}, {"key": "zhang2025deep", "year": "2021", "citations": "21", "title": "Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval", "abstract": "<p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Neural-Hashing", "Tools-&-Libraries", "Compact-Codes", "Efficiency", "Image-Retrieval", "Memory-Efficiency", "Hashing-Methods", "Scalability"], "tsne_embedding": [-5.082067966461182, -1.549883484840393], "cluster": 1}, {"key": "zhang2025efficient", "year": "2016", "citations": "112", "title": "Efficient Training Of Very Deep Neural Networks For Supervised Hashing", "abstract": "<p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively \u201cshallow\u201d networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "CVPR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [31.624061584472656, -5.349379539489746], "cluster": 9}, {"key": "zhang2025fast", "year": "2021", "citations": "9", "title": "Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization", "abstract": "<p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Compact-Codes", "Quantization", "Multimodal-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [18.12259864807129, -10.693266868591309], "cluster": 7}, {"key": "zhang2025hierarchical", "year": "2021", "citations": "2", "title": "Hierarchical Deep Hashing For Fast Large Scale Image Retrieval", "abstract": "<p>Fast image retrieval is of great importance in many computer vision tasks and especially practical applications. Deep hashing, the state-of-the-art fast image retrieval scheme, introduces deep learning to learn the hash functions and generate binary hash codes, and outperforms the other image retrieval methods in terms of accuracy. However, all the existing deep hashing methods could only generate one level hash codes and require a linear traversal of all the hash codes to figure out the closest one when a new query arrives, which is very time-consuming and even intractable for large scale applications. In this work, we propose a Hierarchical Deep Hashing(HDHash) scheme to speed up the state-of-the-art deep hashing methods. More specifically, hierarchical deep hash codes of multiple levels can be generated and indexed with tree structures rather than linear ones, and pruning irrelevant branches can sharply decrease the retrieval time. To our best knowledge, this is the first work to introduce hierarchical indexed deep hashing for fast large scale image retrieval. Extensive experimental results on three benchmark datasets demonstrate that the proposed HDHash scheme achieves better or comparable accuracy with significantly improved efficiency and reduced memory as compared to state-of- the-art fast image retrieval schemes.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [-1.7019833326339722, 19.449649810791016], "cluster": 8}, {"key": "zhang2025high", "year": "2021", "citations": "62", "title": "High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval", "abstract": "<p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods", "Multimodal-Retrieval", "Compact-Codes", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [9.102011680603027, 9.735011100769043], "cluster": 6}, {"key": "zhang2025large", "year": "2014", "citations": "619", "title": "Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for similarity search in multimedia\ndata. In particular, more and more attentions\nhave been payed to multimodal hashing for search in\nmultimedia data with multiple modalities, such as images\nwith tags. Typically, supervised information of semantic\nlabels is also available for the data points in\nmany real applications. Hence, many supervised multimodal\nhashing (SMH) methods have been proposed\nto utilize such semantic labels to further improve the\nsearch accuracy. However, the training time complexity\nof most existing SMH methods is too high, which\nmakes them unscalable to large-scale datasets. In this\npaper, a novel SMH method, called semantic correlation\nmaximization (SCM), is proposed to seamlessly integrate\nsemantic labels into the hashing learning procedure\nfor large-scale data modeling. Experimental results\non two real-world datasets show that SCM can signifi-\ncantly outperform the state-of-the-art SMH methods, in\nterms of both accuracy and scalability.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Similarity-Search", "AAAI", "Memory-Efficiency", "Datasets", "Supervised"], "tsne_embedding": [5.993252277374268, -4.120007038116455], "cluster": 6}, {"key": "zhang2025self", "year": "2010", "citations": "354", "title": "Self-taught Hashing For Fast Similarity Search", "abstract": "<p>The ability of fast similarity search at large scale is of great\nimportance to many Information Retrieval (IR) applications.\nA promising way to accelerate similarity search is semantic\nhashing which designs compact binary codes for a large number\nof documents so that semantically similar documents\nare mapped to similar codes (within a short Hamming distance).\nAlthough some recently proposed techniques are\nable to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents\nremains to be a very challenging problem. In this\npaper, we emphasise this issue and propose a novel SelfTaught\nHashing (STH) approach to semantic hashing: we\nfirst find the optimal l-bit binary codes for all documents in\nthe given corpus via unsupervised learning, and then train\nl classifiers via supervised learning to predict the l-bit code\nfor any query document unseen before. Our experiments on\nthree real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and\nlinear Support Vector Machine (SVM) outperforms stateof-the-art\ntechniques significantly.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Compact-Codes", "Similarity-Search", "SIGIR", "Datasets", "Supervised", "Unsupervised"], "tsne_embedding": [15.052105903625488, -16.675888061523438], "cluster": 7}, {"key": "zhang2025supervised", "year": "2014", "citations": "286", "title": "Supervised Hashing With Latent Factor Models", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for approximate nearest neighbor\nsearch in large-scale datasets. Traditional hashing methods\ntry to learn the hash codes in an unsupervised way where\nthe metric (Euclidean) structure of the training data is preserved.\nVery recently, supervised hashing methods, which\ntry to preserve the semantic structure constructed from the\nsemantic labels of the training points, have exhibited higher\naccuracy than unsupervised methods. In this paper, we\npropose a novel supervised hashing method, called latent\nfactor hashing (LFH), to learn similarity-preserving binary\ncodes based on latent factor models. An algorithm with\nconvergence guarantee is proposed to learn the parameters\nof LFH. Furthermore, a linear-time variant with stochastic\nlearning is proposed for training LFH on large-scale datasets.\nExperimental results on two large datasets with semantic\nlabels show that LFH can achieve superior accuracy than\nstate-of-the-art methods with comparable training time.</p>\n", "tags": ["Hashing-Methods", "Scalability", "SIGIR", "Memory-Efficiency", "Datasets", "Supervised", "Neural-Hashing", "Unsupervised"], "tsne_embedding": [18.72487449645996, 2.6640784740448], "cluster": 6}, {"key": "zhao2015deep", "year": "2015", "citations": "463", "title": "Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval", "abstract": "<p>With the rapid growth of web images, hashing has received\nincreasing interests in large scale image retrieval.\nResearch efforts have been devoted to learning compact binary\ncodes that preserve semantic similarity based on labels.\nHowever, most of these hashing methods are designed\nto handle simple binary similarity. The complex multilevel\nsemantic structure of images associated with multiple labels\nhave not yet been well explored. Here we propose a deep\nsemantic ranking based method for learning hash functions\nthat preserve multilevel semantic similarity between multilabel\nimages. In our approach, deep convolutional neural\nnetwork is incorporated into hash functions to jointly\nlearn feature representations and mappings from them to\nhash codes, which avoids the limitation of semantic representation\npower of hand-crafted features. Meanwhile, a\nranking list that encodes the multilevel similarity information\nis employed to guide the learning of such deep hash\nfunctions. An effective scheme based on surrogate loss is\nused to solve the intractable optimization problem of nonsmooth\nand multivariate ranking measures involved in the\nlearning procedure. Experimental results show the superiority\nof our proposed approach over several state-of-theart\nhashing methods in term of ranking evaluation metrics\nwhen tested on multi-label image datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "CVPR", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [1.445745825767517, -2.465636730194092], "cluster": 6}, {"key": "zhao2017scalable", "year": "2017", "citations": "7", "title": "Scalable Nearest Neighbor Search Based On Knn Graph", "abstract": "<p>Nearest neighbor search is known as a challenging issue that has been studied\nfor several decades. Recently, this issue becomes more and more imminent in\nviewing that the big data problem arises from various fields. In this paper, a\nscalable solution based on hill-climbing strategy with the support of k-nearest\nneighbor graph (kNN) is presented. Two major issues have been considered in the\npaper. Firstly, an efficient kNN graph construction method based on two means\ntree is presented. For the nearest neighbor search, an enhanced hill-climbing\nprocedure is proposed, which sees considerable performance boost over original\nprocedure. Furthermore, with the support of inverted indexing derived from\nresidue vector quantization, our method achieves close to 100% recall with high\nspeed efficiency in two state-of-the-art evaluation benchmarks. In addition, a\ncomparative study on both the compressional and traditional nearest neighbor\nsearch methods is presented. We show that our method achieves the best\ntrade-off between search quality, efficiency and memory complexity.</p>\n", "tags": ["Survey-Paper", "Graph-Based-Ann", "Efficiency", "Quantization", "Evaluation"], "tsne_embedding": [43.588111877441406, 23.899499893188477], "cluster": 2}, {"key": "zhao2018approximate", "year": "2018", "citations": "3", "title": "Approximate K-nn Graph Construction: A Generic Online Approach", "abstract": "<p>Nearest neighbor search and k-nearest neighbor graph construction are two\nfundamental issues arise from many disciplines such as multimedia information\nretrieval, data-mining and machine learning. They become more and more imminent\ngiven the big data emerge in various fields in recent years. In this paper, a\nsimple but effective solution both for approximate k-nearest neighbor search\nand approximate k-nearest neighbor graph construction is presented. These two\nissues are addressed jointly in our solution. On the one hand, the approximate\nk-nearest neighbor graph construction is treated as a search task. Each sample\nalong with its k-nearest neighbors are joined into the k-nearest neighbor graph\nby performing the nearest neighbor search sequentially on the graph under\nconstruction. On the other hand, the built k-nearest neighbor graph is used to\nsupport k-nearest neighbor search. Since the graph is built online, the dynamic\nupdate on the graph, which is not possible from most of the existing solutions,\nis supported. This solution is feasible for various distance measures. Its\neffectiveness both as k-nearest neighbor construction and k-nearest neighbor\nsearch approaches is verified across different types of data in different\nscales, various dimensions and under different metrics.</p>\n", "tags": ["Graph-Based-Ann"], "tsne_embedding": [52.4171028137207, 8.790167808532715], "cluster": 9}, {"key": "zhao2019merge", "year": "2021", "citations": "2", "title": "On The Merge Of K-nn Graph", "abstract": "<p>k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.</p>\n", "tags": ["Evaluation", "Scalability", "Datasets"], "tsne_embedding": [58.2108154296875, 2.096451759338379], "cluster": 9}, {"key": "zhao2019weakly", "year": "2019", "citations": "22", "title": "A Weakly Supervised Adaptive Triplet Loss For Deep Metric Learning", "abstract": "<p>We address the problem of distance metric learning in visual similarity\nsearch, defined as learning an image embedding model which projects images into\nEuclidean space where semantically and visually similar images are closer and\ndissimilar images are further from one another. We present a weakly supervised\nadaptive triplet loss (ATL) capable of capturing fine-grained semantic\nsimilarity that encourages the learned image embedding models to generalize\nwell on cross-domain data. The method uses weakly labeled product description\ndata to implicitly determine fine grained semantic classes, avoiding the need\nto annotate large amounts of training data. We evaluate on the Amazon fashion\nretrieval benchmark and DeepFashion in-shop retrieval data. The method boosts\nthe performance of triplet loss baseline by 10.6% on cross-domain data and\nout-performs the state-of-art model on all evaluation metrics.</p>\n", "tags": ["Supervised", "ICCV", "Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-18.33696746826172, -43.25965118408203], "cluster": 3}, {"key": "zhao2020dime", "year": "2019", "citations": "1", "title": "DIME: An Online Tool For The Visual Comparison Of Cross-modal Retrieval Models", "abstract": "<p>Cross-modal retrieval relies on accurate models to retrieve relevant results\nfor queries across modalities such as image, text, and video. In this paper, we\nbuild upon previous work by tackling the difficulty of evaluating models both\nquantitatively and qualitatively quickly. We present DIME (Dataset, Index,\nModel, Embedding), a modality-agnostic tool that handles multimodal datasets,\ntrained models, and data preprocessors to support straightforward model\ncomparison with a web browser graphical user interface. DIME inherently\nsupports building modality-agnostic queryable indexes and extraction of\nrelevant feature embeddings, and thus effectively doubles as an efficient\ncross-modal tool to explore and search through datasets.</p>\n", "tags": ["Multimodal-Retrieval", "Evaluation", "Datasets"], "tsne_embedding": [7.934139251708984, -32.27263641357422], "cluster": 7}, {"key": "zhao2020stacked", "year": "2020", "citations": "11", "title": "Stacked Convolutional Deep Encoding Network For Video-text Retrieval", "abstract": "<p>Existing dominant approaches for cross-modal video-text retrieval task are to\nlearn a joint embedding space to measure the cross-modal similarity. However,\nthese methods rarely explore long-range dependency inside video frames or\ntextual words leading to insufficient textual and visual details. In this\npaper, we propose a stacked convolutional deep encoding network for video-text\nretrieval task, which considers to simultaneously encode long-range and\nshort-range dependency in the videos and texts. Specifically, a multi-scale\ndilated convolutional (MSDC) block within our approach is able to encode\nshort-range temporal cues between video frames or text words by adopting\ndifferent scales of kernel size and dilation size of convolutional layer. A\nstacked structure is designed to expand the receptive fields by repeatedly\nadopting the MSDC block, which further captures the long-range relations\nbetween these cues. Moreover, to obtain more robust textual representations, we\nfully utilize the powerful language model named Transformer in two stages:\npretraining phrase and fine-tuning phrase. Extensive experiments on two\ndifferent benchmark datasets (MSR-VTT, MSVD) show that our proposed method\noutperforms other state-of-the-art approaches.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [-35.8857536315918, -28.533935546875], "cluster": 5}, {"key": "zhao2021feature", "year": "2022", "citations": "20", "title": "A Feature Consistency Driven Attention Erasing Network For Fine-grained Image Retrieval", "abstract": "<p>Large-scale fine-grained image retrieval has two main problems. First, low\ndimensional feature embedding can fasten the retrieval process but bring\naccuracy reduce due to overlooking the feature of significant attention regions\nof images in fine-grained datasets. Second, fine-grained images lead to the\nsame category query hash codes mapping into the different cluster in database\nhash latent space. To handle these two issues, we propose a feature consistency\ndriven attention erasing network (FCAENet) for fine-grained image retrieval.\nFor the first issue, we propose an adaptive augmentation module in FCAENet,\nwhich is selective region erasing module (SREM). SREM makes the network more\nrobust on subtle differences of fine-grained task by adaptively covering some\nregions of raw images. The feature extractor and hash layer can learn more\nrepresentative hash code for fine-grained images by SREM. With regard to the\nsecond issue, we fully exploit the pair-wise similarity information and add the\nenhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation\nstabler between the query hash code and database hash code. We conduct\nextensive experiments on five fine-grained benchmark datasets (CUB2011,\nAircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash\ncode. The results show that FCAENet achieves the state-of-the-art (SOTA)\nfine-grained retrieval performance compared with other methods.</p>\n", "tags": ["Datasets", "CVPR", "Evaluation", "Image-Retrieval", "Hashing-Methods", "Scalability"], "tsne_embedding": [-23.359127044677734, 4.771461009979248], "cluster": 1}, {"key": "zhao2021large", "year": "2019", "citations": "10", "title": "Large-scale Visual Search With Binary Distributed Graph At Alibaba", "abstract": "<p>Graph-based approximate nearest neighbor search has attracted more and more\nattentions due to its online search advantages. Numbers of methods studying the\nenhancement of speed and recall have been put forward. However, few of them\nfocus on the efficiency and scale of offline graph-construction. For a deployed\nvisual search system with several billions of online images in total, building\na billion-scale offline graph in hours is essential, which is almost\nunachievable by most existing methods. In this paper, we propose a novel\nalgorithm called Binary Distributed Graph to solve this problem. Specifically,\nwe combine binary codes with graph structure to speedup online and offline\nprocedures, and achieve comparable performance with the ones in real-value\nbased scenarios by recalling more binary candidates. Furthermore, the\ngraph-construction is optimized to completely distributed implementation, which\nsignificantly accelerates the offline process and gets rid of the limitation of\nmemory and disk within a single machine. Experimental comparisons on Alibaba\nCommodity Data Set (more than three billion images) show that the proposed\nmethod outperforms the state-of-the-art with respect to the online/offline\ntrade-off.</p>\n", "tags": ["Graph-Based-Ann", "CIKM", "Efficiency", "Scalability", "Image-Retrieval", "Large-Scale-Search", "Compact-Codes", "Evaluation"], "tsne_embedding": [50.22996139526367, 10.366327285766602], "cluster": 9}, {"key": "zhao2021rescuing", "year": "2021", "citations": "3", "title": "Rescuing Deep Hashing From Dead Bits Problem", "abstract": "<p>Deep hashing methods have shown great retrieval accuracy and efficiency in\nlarge-scale image retrieval. How to optimize discrete hash bits is always the\nfocus in deep hashing methods. A common strategy in these methods is to adopt\nan activation function, e.g. \\(\\operatorname{sigmoid}(\\cdot)\\) or\n\\(\\operatorname{tanh}(\\cdot)\\), and minimize a quantization loss to approximate\ndiscrete values. However, this paradigm may make more and more hash bits stuck\ninto the wrong saturated area of the activation functions and never escaped. We\ncall this problem \u201cDead Bits Problem~(DBP)\u201d. Besides, the existing quantization\nloss will aggravate DBP as well. In this paper, we propose a simple but\neffective gradient amplifier which acts before activation functions to\nalleviate DBP. Moreover, we devise an error-aware quantization loss to further\nalleviate DBP. It avoids the negative effect of quantization loss based on the\nsimilarity between two images. The proposed gradient amplifier and error-aware\nquantization loss are compatible with a variety of deep hashing methods.\nExperimental results on three datasets demonstrate the efficiency of the\nproposed gradient amplifier and the error-aware quantization loss.</p>\n", "tags": ["Datasets", "Neural-Hashing", "Quantization", "AAAI", "Efficiency", "Image-Retrieval", "Hashing-Methods", "IJCAI", "Scalability"], "tsne_embedding": [-6.580595970153809, 15.271658897399902], "cluster": 8}, {"key": "zhao2022constrained", "year": "2022", "citations": "1", "title": "Constrained Approximate Similarity Search On Proximity Graph", "abstract": "<p>Search engines and recommendation systems are built to efficiently display\nrelevant information from those massive amounts of candidates. Typically a\nthree-stage mechanism is employed in those systems: (i) a small collection of\nitems are first retrieved by (e.g.,) approximate near neighbor search\nalgorithms; (ii) then a collection of constraints are applied on the retrieved\nitems; (iii) a fine-grained ranking neural network is employed to determine the\nfinal recommendation. We observe a major defect of the original three-stage\npipeline: Although we only target to retrieve \\(k\\) vectors in the final\nrecommendation, we have to preset a sufficiently large \\(s\\) (\\(s &gt; k\\)) for each\nquery, and ``hope\u2019\u2019 the number of survived vectors after the filtering is not\nsmaller than \\(k\\). That is, at least \\(k\\) vectors in the \\(s\\) similar candidates\nsatisfy the query constraints.\n  In this paper, we investigate this constrained similarity search problem and\nattempt to merge the similarity search stage and the filtering stage into one\nsingle search operation. We introduce AIRSHIP, a system that integrates a\nuser-defined function filtering into the similarity search framework. The\nproposed system does not need to build extra indices nor require prior\nknowledge of the query constraints. We propose three optimization strategies:\n(1) starting point selection, (2) multi-direction search, and (3) biased\npriority queue selection. Experimental evaluations on both synthetic and real\ndata confirm the effectiveness of the proposed AIRSHIP algorithm. We focus on\nconstrained graph-based approximate near neighbor (ANN) search in this study,\nin part because graph-based ANN is known to achieve excellent performance. We\nbelieve it is also possible to develop constrained hashing-based ANN or\nconstrained quantization-based ANN.</p>\n", "tags": ["Graph-Based-Ann", "Hashing-Methods", "Quantization", "Recommender-Systems", "Similarity-Search", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [45.92716598510742, -5.392267227172852], "cluster": 9}, {"key": "zhao2022progressive", "year": "2022", "citations": "21", "title": "Progressive Learning For Image Retrieval With Hybrid-modality Queries", "abstract": "<p>Image retrieval with hybrid-modality queries, also known as composing text\nand image for image retrieval (CTI-IR), is a retrieval task where the search\nintention is expressed in a more complex query format, involving both vision\nand text modalities. For example, a target product image is searched using a\nreference product image along with text about changing certain attributes of\nthe reference image as the query. It is a more challenging image retrieval task\nthat requires both semantic space learning and cross-modal fusion. Previous\napproaches that attempt to deal with both aspects achieve unsatisfactory\nperformance. In this paper, we decompose the CTI-IR task into a three-stage\nlearning problem to progressively learn the complex knowledge for image\nretrieval with hybrid-modality queries. We first leverage the semantic\nembedding space for open-domain image-text retrieval, and then transfer the\nlearned knowledge to the fashion-domain with fashion-related pre-training\ntasks. Finally, we enhance the pre-trained model from single-query to\nhybrid-modality query for the CTI-IR task. Furthermore, as the contribution of\nindividual modality in the hybrid-modality query varies for different retrieval\nscenarios, we propose a self-supervised adaptive weighting strategy to\ndynamically determine the importance of image and text in the hybrid-modality\nquery for better retrieval. Extensive experiments show that our proposed model\nsignificantly outperforms state-of-the-art methods in the mean of Recall@K by\n24.9% and 9.5% on the Fashion-IQ and Shoes benchmark datasets respectively.</p>\n", "tags": ["Self-Supervised", "Text-Retrieval", "Image-Retrieval", "SIGIR", "Datasets", "Supervised", "Evaluation"], "tsne_embedding": [-29.989320755004883, -18.292654037475586], "cluster": 5}, {"key": "zhao2023embedding", "year": "2023", "citations": "4", "title": "Embedding In Recommender Systems: A Survey", "abstract": "<p>Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that coverts the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors and can enhance\nthe recommendation performance. Applying embedding techniques captures complex\nentity relationships and has spurred substantial research. In this survey, we\nprovide an overview of the recent literature on embedding techniques in\nrecommender systems. This survey covers embedding methods like collaborative\nfiltering, self-supervised learning, and graph-based techniques. Collaborative\nfiltering generates embeddings capturing user-item preferences, excelling in\nsparse data. Self-supervised methods leverage contrastive or generative\nlearning for various tasks. Graph-based techniques like node2vec exploit\ncomplex relationships in network-rich environments. Addressing the scalability\nchallenges inherent to embedding methods, our survey delves into innovative\ndirections within the field of recommendation systems. These directions aim to\nenhance performance and reduce computational complexity, paving the way for\nimproved recommender systems. Among these innovative approaches, we will\nintroduce Auto Machine Learning (AutoML), hash techniques, and quantization\ntechniques in this survey. We discuss various architectures and techniques and\nhighlight the challenges and future directions in these aspects. This survey\naims to provide a comprehensive overview of the state-of-the-art in this\nrapidly evolving field and serve as a useful resource for researchers and\npractitioners working in the area of recommender systems.</p>\n", "tags": ["Survey-Paper", "Graph-Based-Ann", "Self-Supervised", "Quantization", "Recommender-Systems", "Scalability", "Supervised", "Evaluation"], "tsne_embedding": [54.877891540527344, -3.891657590866089], "cluster": 9}, {"key": "zhao2024kalahash", "year": "2025", "citations": "0", "title": "Kalahash: Knowledge-anchored Low-resource Adaptation For Deep Hashing", "abstract": "<p>Deep hashing has been widely used for large-scale approximate nearest\nneighbor search due to its storage and search efficiency. However, existing\ndeep hashing methods predominantly rely on abundant training data, leaving the\nmore challenging scenario of low-resource adaptation for deep hashing\nrelatively underexplored. This setting involves adapting pre-trained models to\ndownstream tasks with only an extremely small number of training samples\navailable. Our preliminary benchmarks reveal that current methods suffer\nsignificant performance degradation due to the distribution shift caused by\nlimited training samples. To address these challenges, we introduce\nClass-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically\nconstructs low-rank adaptation matrices by leveraging class-level textual\nknowledge embeddings. CLoRA effectively incorporates prior class knowledge as\nanchors, enabling parameter-efficient fine-tuning while maintaining the\noriginal data distribution. Furthermore, we propose Knowledge-Guided Discrete\nOptimization (KIDDO), a framework to utilize class knowledge to compensate for\nthe scarcity of visual information and enhance the discriminability of hash\ncodes. Extensive experiments demonstrate that our proposed method, Knowledge-\nAnchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts\nretrieval performance and achieves a 4x data efficiency in low-resource\nscenarios.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "AAAI", "Tools-&-Libraries", "Evaluation"], "tsne_embedding": [25.113245010375977, -7.502984046936035], "cluster": 6}, {"key": "zhao2025deep", "year": "2015", "citations": "463", "title": "Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval", "abstract": "<p>With the rapid growth of web images, hashing has received\nincreasing interests in large scale image retrieval.\nResearch efforts have been devoted to learning compact binary\ncodes that preserve semantic similarity based on labels.\nHowever, most of these hashing methods are designed\nto handle simple binary similarity. The complex multilevel\nsemantic structure of images associated with multiple labels\nhave not yet been well explored. Here we propose a deep\nsemantic ranking based method for learning hash functions\nthat preserve multilevel semantic similarity between multilabel\nimages. In our approach, deep convolutional neural\nnetwork is incorporated into hash functions to jointly\nlearn feature representations and mappings from them to\nhash codes, which avoids the limitation of semantic representation\npower of hand-crafted features. Meanwhile, a\nranking list that encodes the multilevel similarity information\nis employed to guide the learning of such deep hash\nfunctions. An effective scheme based on surrogate loss is\nused to solve the intractable optimization problem of nonsmooth\nand multivariate ranking measures involved in the\nlearning procedure. Experimental results show the superiority\nof our proposed approach over several state-of-theart\nhashing methods in term of ranking evaluation metrics\nwhen tested on multi-label image datasets.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "CVPR", "Image-Retrieval", "Datasets", "Evaluation"], "tsne_embedding": [1.4457451105117798, -2.4656550884246826], "cluster": 6}, {"key": "zhao2025note", "year": "2025", "citations": "0", "title": "A Note On Efficient Privacy-preserving Similarity Search For Encrypted Vectors", "abstract": "<p>Traditional approaches to vector similarity search over encrypted data rely\non fully homomorphic encryption (FHE) to enable computation without decryption.\nHowever, the substantial computational overhead of FHE makes it impractical for\nlarge-scale real-time applications. This work explores a more efficient\nalternative: using additively homomorphic encryption (AHE) for\nprivacy-preserving similarity search. We consider scenarios where either the\nquery vector or the database vectors remain encrypted, a setting that\nfrequently arises in applications such as confidential recommender systems and\nsecure federated learning. While AHE only supports addition and scalar\nmultiplication, we show that it is sufficient to compute inner product\nsimilarity\u2013one of the most widely used similarity measures in vector\nretrieval. Compared to FHE-based solutions, our approach significantly reduces\ncomputational overhead by avoiding ciphertext-ciphertext multiplications and\nbootstrapping, while still preserving correctness and privacy. We present an\nefficient algorithm for encrypted similarity search under AHE and analyze its\nerror growth and security implications. Our method provides a scalable and\npractical solution for privacy-preserving vector search in real-world machine\nlearning applications.</p>\n", "tags": ["Efficiency", "Recommender-Systems", "Similarity-Search", "Scalability"], "tsne_embedding": [13.192769050598145, 16.658082962036133], "cluster": 6}, {"key": "zhe2018deep", "year": "2019", "citations": "36", "title": "Deep Class-wise Hashing: Semantics-preserving Hashing Via Class-wise Loss", "abstract": "<p>Deep supervised hashing has emerged as an influential solution to large-scale\nsemantic image retrieval problems in computer vision. In the light of recent\nprogress, convolutional neural network based hashing methods typically seek\npair-wise or triplet labels to conduct the similarity preserving learning.\nHowever, complex semantic concepts of visual contents are hard to capture by\nsimilar/dissimilar labels, which limits the retrieval performance. Generally,\npair-wise or triplet losses not only suffer from expensive training costs but\nalso lack in extracting sufficient semantic information. In this regard, we\npropose a novel deep supervised hashing model to learn more compact class-level\nsimilarity preserving binary codes. Our deep learning based model is motivated\nby deep metric learning that directly takes semantic labels as supervised\ninformation in training and generates corresponding discriminant hashing code.\nSpecifically, a novel cubic constraint loss function based on Gaussian\ndistribution is proposed, which preserves semantic variations while penalizes\nthe overlap part of different classes in the embedding space. To address the\ndiscrete optimization problem introduced by binary codes, a two-step\noptimization strategy is proposed to provide efficient training and avoid the\nproblem of gradient vanishing. Extensive experiments on four large-scale\nbenchmark databases show that our model can achieve the state-of-the-art\nretrieval performance. Moreover, when training samples are limited, our method\nsurpasses other supervised deep hashing methods with non-negligible margins.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Compact-Codes", "Neural-Hashing", "Scalability", "Image-Retrieval", "Supervised", "Evaluation"], "tsne_embedding": [10.328238487243652, -2.289017915725708], "cluster": 6}, {"key": "zhe2018directional", "year": "2019", "citations": "72", "title": "Directional Statistics-based Deep Metric Learning For Image Classification And Retrieval", "abstract": "<p>Deep distance metric learning (DDML), which is proposed to learn image\nsimilarity metrics in an end-to-end manner based on the convolution neural\nnetwork, has achieved encouraging results in many computer vision\ntasks.\\(L2\\)-normalization in the embedding space has been used to improve the\nperformance of several DDML methods. However, the commonly used Euclidean\ndistance is no longer an accurate metric for \\(L2\\)-normalized embedding space,\ni.e., a hyper-sphere. Another challenge of current DDML methods is that their\nloss functions are usually based on rigid data formats, such as the triplet\ntuple. Thus, an extra process is needed to prepare data in specific formats. In\naddition, their losses are obtained from a limited number of samples, which\nleads to a lack of the global view of the embedding space. In this paper, we\nreplace the Euclidean distance with the cosine similarity to better utilize the\n\\(L2\\)-normalization, which is able to attenuate the curse of dimensionality.\nMore specifically, a novel loss function based on the von Mises-Fisher\ndistribution is proposed to learn a compact hyper-spherical embedding space.\nMoreover, a new efficient learning algorithm is developed to better capture the\nglobal structure of the embedding space. Experiments for both classification\nand retrieval tasks on several standard datasets show that our method achieves\nstate-of-the-art performance with a simpler training procedure. Furthermore, we\ndemonstrate that, even with a small number of convolutional layers, our model\ncan still obtain significantly better classification performance than the\nwidely used softmax loss.</p>\n", "tags": ["Distance-Metric-Learning", "Datasets", "CVPR", "Evaluation"], "tsne_embedding": [-26.71254539489746, -10.11500358581543], "cluster": 5}, {"key": "zhen2012co", "year": "2012", "citations": "195", "title": "Co-regularized Hashing For Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity\nsearch. To obtain compact hash codes, a recent trend seeks to learn the hash\nfunctions from data automatically. In this paper, we study hash function learning\nin the context of multimodal data. We propose a novel multimodal hash function\nlearning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization\nframework. The hash functions for each bit of the hash codes are\nlearned by solving DC (difference of convex functions) programs, while the learning\nfor multiple bits proceeds via a boosting procedure so that the bias introduced\nby the hash functions can be sequentially minimized. We empirically compare\nCRH with two state-of-the-art multimodal hash function learning methods on two\npublicly available data sets.</p>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods", "Scalability"], "tsne_embedding": [20.575422286987305, -3.9391825199127197], "cluster": 6}, {"key": "zhen2025co", "year": "2012", "citations": "195", "title": "Co-regularized Hashing For Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity\nsearch. To obtain compact hash codes, a recent trend seeks to learn the hash\nfunctions from data automatically. In this paper, we study hash function learning\nin the context of multimodal data. We propose a novel multimodal hash function\nlearning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization\nframework. The hash functions for each bit of the hash codes are\nlearned by solving DC (difference of convex functions) programs, while the learning\nfor multiple bits proceeds via a boosting procedure so that the bias introduced\nby the hash functions can be sequentially minimized. We empirically compare\nCRH with two state-of-the-art multimodal hash function learning methods on two\npublicly available data sets.</p>\n", "tags": ["Tools-&-Libraries", "Hashing-Methods", "Scalability"], "tsne_embedding": [20.575902938842773, -3.9393439292907715], "cluster": 6}, {"key": "zheng2015person", "year": "2015", "citations": "60", "title": "Person Re-identification Meets Image Search", "abstract": "<p>For long time, person re-identification and image search are two separately\nstudied tasks. However, for person re-identification, the effectiveness of\nlocal features and the \u201cquery-search\u201d mode make it well posed for image search\ntechniques.\n  In the light of recent advances in image search, this paper proposes to treat\nperson re-identification as an image search problem. Specifically, this paper\nclaims two major contributions. 1) By designing an unsupervised Bag-of-Words\nrepresentation, we are devoted to bridging the gap between the two tasks by\nintegrating techniques from image search in person re-identification. We show\nthat our system sets up an effective yet efficient baseline that is amenable to\nfurther supervised/unsupervised improvements. 2) We contribute a new high\nquality dataset which uses DPM detector and includes a number of distractor\nimages. Our dataset reaches closer to realistic settings, and new perspectives\nare provided.\n  Compared with approaches that rely on feature-feature match, our method is\nfaster by over two orders of magnitude. Moreover, on three datasets, we report\ncompetitive results compared with the state-of-the-art methods.</p>\n", "tags": ["Supervised", "Unsupervised", "Image-Retrieval", "Datasets"], "tsne_embedding": [-34.60836410522461, 11.234761238098145], "cluster": 0}, {"key": "zheng2016sift", "year": "2017", "citations": "709", "title": "SIFT Meets CNN: A Decade Survey Of Instance Retrieval", "abstract": "<p>In the early days, content-based image retrieval (CBIR) was studied with\nglobal features. Since 2003, image retrieval based on local descriptors (de\nfacto SIFT) has been extensively studied for over a decade due to the advantage\nof SIFT in dealing with image transformations. Recently, image representations\nbased on the convolutional neural network (CNN) have attracted increasing\ninterest in the community and demonstrated impressive performance. Given this\ntime of rapid evolution, this article provides a comprehensive survey of\ninstance retrieval over the last decade. Two broad categories, SIFT-based and\nCNN-based methods, are presented. For the former, according to the codebook\nsize, we organize the literature into using large/medium-sized/small codebooks.\nFor the latter, we discuss three lines of methods, i.e., using pre-trained or\nfine-tuned CNN models, and hybrid methods. The first two perform a single-pass\nof an image to the network, while the last category employs a patch-based\nfeature extraction scheme. This survey presents milestones in modern instance\nretrieval, reviews a broad selection of previous works in different categories,\nand provides insights on the connection between SIFT and CNN-based methods.\nAfter analyzing and comparing retrieval performance of different categories on\nseveral datasets, we discuss promising directions towards generic and\nspecialized instance retrieval.</p>\n", "tags": ["Survey-Paper", "Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-52.774269104003906, -0.15092729032039642], "cluster": 0}, {"key": "zheng2017dual", "year": "2020", "citations": "127", "title": "Dual-path Convolutional Image-text Embeddings With Instance Loss", "abstract": "<p>Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.</p>\n", "tags": ["Unsupervised", "Datasets"], "tsne_embedding": [-26.12744903564453, -18.658771514892578], "cluster": 5}, {"key": "zheng2019visual", "year": "2019", "citations": "2", "title": "Visual Similarity Attention", "abstract": "<p>While there has been substantial progress in learning suitable distance\nmetrics, these techniques in general lack transparency and decision reasoning,\ni.e., explaining why the input set of images is similar or dissimilar. In this\nwork, we solve this key problem by proposing the first method to generate\ngeneric visual similarity explanations with gradient-based attention. We\ndemonstrate that our technique is agnostic to the specific similarity model\ntype, e.g., we show applicability to Siamese, triplet, and quadruplet models.\nFurthermore, we make our proposed similarity attention a principled part of the\nlearning process, resulting in a new paradigm for learning similarity\nfunctions. We demonstrate that our learning mechanism results in more\ngeneralizable, as well as explainable, similarity models. Finally, we\ndemonstrate the generality of our framework by means of experiments on a\nvariety of tasks, including image retrieval, person re-identification, and\nlow-shot semantic segmentation.</p>\n", "tags": ["Tools-&-Libraries", "Image-Retrieval"], "tsne_embedding": [-17.770751953125, -9.091463088989258], "cluster": 1}, {"key": "zheng2020generative", "year": "2020", "citations": "10", "title": "Generative Semantic Hashing Enhanced Via Boltzmann Machines", "abstract": "<p>Generative semantic hashing is a promising technique for large-scale\ninformation retrieval thanks to its fast retrieval speed and small memory\nfootprint. For the tractability of training, existing generative-hashing\nmethods mostly assume a factorized form for the posterior distribution,\nenforcing independence among the bits of hash codes. From the perspectives of\nboth model representation and code space size, independence is always not the\nbest assumption. In this paper, to introduce correlations among the bits of\nhash codes, we propose to employ the distribution of Boltzmann machine as the\nvariational posterior. To address the intractability issue of training, we\nfirst develop an approximate method to reparameterize the distribution of a\nBoltzmann machine by augmenting it as a hierarchical concatenation of a\nGaussian-like distribution and a Bernoulli distribution. Based on that, an\nasymptotically-exact lower bound is further derived for the evidence lower\nbound (ELBO). With these novel techniques, the entire model can be optimized\nefficiently. Extensive experimental results demonstrate that by effectively\nmodeling correlations among different bits within a hash code, our model can\nachieve significant performance gains.</p>\n", "tags": ["Hashing-Methods", "Text-Retrieval", "Efficiency", "Scalability", "Evaluation"], "tsne_embedding": [21.483402252197266, 3.6230592727661133], "cluster": 6}, {"key": "zheng2021deep", "year": "2021", "citations": "36", "title": "Deep Relational Metric Learning", "abstract": "<p>This paper presents a deep relational metric learning (DRML) framework for\nimage clustering and retrieval. Most existing deep metric learning methods\nlearn an embedding space with a general objective of increasing interclass\ndistances and decreasing intraclass distances. However, the conventional losses\nof metric learning usually suppress intraclass variations which might be\nhelpful to identify samples of unseen classes. To address this problem, we\npropose to adaptively learn an ensemble of features that characterizes an image\nfrom different aspects to model both interclass and intraclass distributions.\nWe further employ a relational module to capture the correlations among each\nfeature in the ensemble and construct a graph to represent an image. We then\nperform relational inference on the graph to integrate the ensemble and obtain\na relation-aware embedding to measure the similarities. Extensive experiments\non the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets\ndemonstrate that our framework improves existing deep metric learning methods\nand achieves very competitive results.</p>\n", "tags": ["ICCV", "Tools-&-Libraries", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [57.35887908935547, -7.342021942138672], "cluster": 9}, {"key": "zheng2022introspective", "year": "2022", "citations": "0", "title": "Introspective Deep Metric Learning For Image Retrieval", "abstract": "<p>This paper proposes an introspective deep metric learning (IDML) framework\nfor uncertainty-aware comparisons of images. Conventional deep metric learning\nmethods produce confident semantic distances between images regardless of the\nuncertainty level. However, we argue that a good similarity model should\nconsider the semantic discrepancies with caution to better deal with ambiguous\nimages for more robust training. To achieve this, we propose to represent an\nimage using not only a semantic embedding but also an accompanying uncertainty\nembedding, which describes the semantic characteristics and ambiguity of an\nimage, respectively. We further propose an introspective similarity metric to\nmake similarity judgments between images considering both their semantic\ndifferences and ambiguities. The proposed IDML framework improves the\nperformance of deep metric learning through uncertainty modeling and attains\nstate-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford\nOnline Products datasets for image retrieval and clustering. We further provide\nan in-depth analysis of our framework to demonstrate the effectiveness and\nreliability of IDML. Code is available at: https://github.com/wzzheng/IDML.</p>\n", "tags": ["Distance-Metric-Learning", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Evaluation"], "tsne_embedding": [-23.6629695892334, -9.331714630126953], "cluster": 1}, {"key": "zheng2022spatial", "year": "2022", "citations": "1", "title": "Spatial Autoregressive Coding For Graph Neural Recommendation", "abstract": "<p>Graph embedding methods including traditional shallow models and deep Graph\nNeural Networks (GNNs) have led to promising applications in recommendation.\nNevertheless, shallow models especially random-walk-based algorithms fail to\nadequately exploit neighbor proximity in sampled subgraphs or sequences due to\ntheir optimization paradigm. GNN-based algorithms suffer from the insufficient\nutilization of high-order information and easily cause over-smoothing problems\nwhen stacking too much layers, which may deteriorate the recommendations of\nlow-degree (long-tail) items, limiting the expressiveness and scalability. In\nthis paper, we propose a novel framework SAC, namely Spatial Autoregressive\nCoding, to solve the above problems in a unified way. To adequately leverage\nneighbor proximity and high-order information, we design a novel spatial\nautoregressive paradigm. Specifically, we first randomly mask multi-hop\nneighbors and embed the target node by integrating all other surrounding\nneighbors with an explicit multi-hop attention. Then we reinforce the model to\nlearn a neighbor-predictive coding for the target node by contrasting the\ncoding and the masked neighbors\u2019 embedding, equipped with a new hard negative\nsampling strategy. To learn the minimal sufficient representation for the\ntarget-to-neighbor prediction task and remove the redundancy of neighbors, we\ndevise Neighbor Information Bottleneck by maximizing the mutual information\nbetween target predictive coding and the masked neighbors\u2019 embedding, and\nsimultaneously constraining those between the coding and surrounding neighbors\u2019\nembedding. Experimental results on both public recommendation datasets and a\nreal scenario web-scale dataset Douyin-Friend-Recommendation demonstrate the\nsuperiority of SAC compared with state-of-the-art methods.</p>\n", "tags": ["Recommender-Systems", "Scalability", "Tools-&-Libraries", "Large-Scale-Search", "Datasets"], "tsne_embedding": [47.569061279296875, -0.8102336525917053], "cluster": 9}, {"key": "zheng2024prototypical", "year": "2024", "citations": "0", "title": "Prototypical Hash Encoding For On-the-fly Fine-grained Category Discovery", "abstract": "<p>In this paper, we study a practical yet challenging task, On-the-fly Category\nDiscovery (OCD), aiming to online discover the newly-coming stream data that\nbelong to both known and unknown classes, by leveraging only known category\nknowledge contained in labeled data. Previous OCD methods employ the hash-based\ntechnique to represent old/new categories by hash codes for instance-wise\ninference. However, directly mapping features into low-dimensional hash space\nnot only inevitably damages the ability to distinguish classes and but also\ncauses \u201chigh sensitivity\u201d issue, especially for fine-grained classes, leading\nto inferior performance. To address these issues, we propose a novel\nPrototypical Hash Encoding (PHE) framework consisting of Category-aware\nPrototype Generation (CPG) and Discriminative Category Encoding (DCE) to\nmitigate the sensitivity of hash code while preserving rich discriminative\ninformation contained in high-dimension feature space, in a two-stage\nprojection fashion. CPG enables the model to fully capture the intra-category\ndiversity by representing each category with multiple prototypes. DCE boosts\nthe discrimination ability of hash code with the guidance of the generated\ncategory prototypes and the constraint of minimum separation distance. By\njointly optimizing CPG and DCE, we demonstrate that these two components are\nmutually beneficial towards an effective OCD. Extensive experiments show the\nsignificant superiority of our PHE over previous methods, e.g., obtaining an\nimprovement of +5.3% in ALL ACC averaged on all datasets. Moreover, due to the\nnature of the interpretable prototypes, we visually analyze the underlying\nmechanism of how PHE helps group certain samples into either known or unknown\ncategories. Code is available at https://github.com/HaiyangZheng/PHE.</p>\n", "tags": ["Tools-&-Libraries", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [6.2612223625183105, 15.560279846191406], "cluster": 6}, {"key": "zheng2025enhancing", "year": "2025", "citations": "0", "title": "Enhancing Embedding Representation Stability In Recommendation Systems With Semantic ID", "abstract": "<p>The exponential growth of online content has posed significant challenges to\nID-based models in industrial recommendation systems, ranging from extremely\nhigh cardinality and dynamically growing ID space, to highly skewed engagement\ndistributions, to prediction instability as a result of natural id life cycles\n(e.g, the birth of new IDs and retirement of old IDs). To address these issues,\nmany systems rely on random hashing to handle the id space and control the\ncorresponding model parameters (i.e embedding table). However, this approach\nintroduces data pollution from multiple ids sharing the same embedding, leading\nto degraded model performance and embedding representation instability.\n  This paper examines these challenges and introduces Semantic ID prefix ngram,\na novel token parameterization technique that significantly improves the\nperformance of the original Semantic ID. Semantic ID prefix ngram creates\nsemantically meaningful collisions by hierarchically clustering items based on\ntheir content embeddings, as opposed to random assignments. Through extensive\nexperimentation, we demonstrate that Semantic ID prefix ngram not only\naddresses embedding instability but also significantly improves tail id\nmodeling, reduces overfitting, and mitigates representation shifts. We further\nhighlight the advantages of Semantic ID prefix ngram in attention-based models\nthat contextualize user histories, showing substantial performance\nimprovements. We also report our experience of integrating Semantic ID into\nMeta production Ads Ranking system, leading to notable performance gains and\nenhanced prediction stability in live deployments.</p>\n", "tags": ["Evaluation", "Recommender-Systems", "Hashing-Methods"], "tsne_embedding": [26.2266845703125, -23.9305477142334], "cluster": 7}, {"key": "zhong2017re", "year": "2017", "citations": "1369", "title": "Re-ranking Person Re-identification With K-reciprocal Encoding", "abstract": "<p>When considering person re-identification (re-ID) as a retrieval process,\nre-ranking is a critical step to improve its accuracy. Yet in the re-ID\ncommunity, limited effort has been devoted to re-ranking, especially those\nfully automatic, unsupervised solutions. In this paper, we propose a\nk-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is\nthat if a gallery image is similar to the probe in the k-reciprocal nearest\nneighbors, it is more likely to be a true match. Specifically, given an image,\na k-reciprocal feature is calculated by encoding its k-reciprocal nearest\nneighbors into a single vector, which is used for re-ranking under the Jaccard\ndistance. The final distance is computed as the combination of the original\ndistance and the Jaccard distance. Our re-ranking method does not require any\nhuman interaction or any labeled data, so it is applicable to large-scale\ndatasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW\ndatasets confirm the effectiveness of our method.</p>\n", "tags": ["CVPR", "Scalability", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Unsupervised"], "tsne_embedding": [-32.24412155151367, 8.62570571899414], "cluster": 0}, {"key": "zhong2020compact", "year": "2019", "citations": "8", "title": "Compact Deep Aggregation For Set Retrieval", "abstract": "<p>The objective of this work is to learn a compact embedding of a set of\ndescriptors that is suitable for efficient retrieval and ranking, whilst\nmaintaining discriminability of the individual descriptors. We focus on a\nspecific example of this general problem \u2013 that of retrieving images\ncontaining multiple faces from a large scale dataset of images. Here the set\nconsists of the face descriptors in each image, and given a query for multiple\nidentities, the goal is then to retrieve, in order, images which contain all\nthe identities, all but one, \\etc\n  To this end, we make the following contributions: first, we propose a CNN\narchitecture \u2013 {\\em SetNet} \u2013 to achieve the objective: it learns face\ndescriptors and their aggregation over a set to produce a compact fixed length\ndescriptor designed for set retrieval, and the score of an image is a count of\nthe number of identities that match the query; second, we show that this\ncompact descriptor has minimal loss of discriminability up to two faces per\nimage, and degrades slowly after that \u2013 far exceeding a number of baselines;\nthird, we explore the speed vs.\\ retrieval quality trade-off for set retrieval\nusing this compact descriptor; and, finally, we collect and annotate a large\ndataset of images containing various number of celebrities, which we use for\nevaluation and is publicly released.</p>\n", "tags": ["Evaluation", "Similarity-Search", "Datasets"], "tsne_embedding": [-50.227569580078125, -3.3003499507904053], "cluster": 0}, {"key": "zhong2022evaluating", "year": "2022", "citations": "12", "title": "Evaluating Token-level And Passage-level Dense Retrieval Models For Math Information Retrieval", "abstract": "<p>With the recent success of dense retrieval methods based on bi-encoders,\nstudies have applied this approach to various interesting downstream retrieval\ntasks with good efficiency and in-domain effectiveness. Recently, we have also\nseen the presence of dense retrieval models in Math Information Retrieval (MIR)\ntasks, but the most effective systems remain classic retrieval methods that\nconsider hand-crafted structure features. In this work, we try to combine the\nbest of both worlds:\\ a well-defined structure search method for effective\nformula search and efficient bi-encoder dense retrieval models to capture\ncontextual similarities. Specifically, we have evaluated two representative\nbi-encoder models for token-level and passage-level dense retrieval on recent\nMIR tasks. Our results show that bi-encoder models are highly complementary to\nexisting structure search methods, and we are able to advance the\nstate-of-the-art on MIR datasets.</p>\n", "tags": ["EMNLP", "Efficiency", "Datasets"], "tsne_embedding": [2.184792995452881, -20.543054580688477], "cluster": 7}, {"key": "zhou2016generic", "year": "2016", "citations": "0", "title": "A Generic Inverted Index Framework For Similarity Search On The GPU - Technical Report", "abstract": "<p>We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named \\(\\tau\\)-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source.</p>\n", "tags": ["Hashing-Methods", "Locality-Sensitive-Hashing", "Efficiency", "Quantization", "Similarity-Search", "Tools-&-Libraries", "Datasets"], "tsne_embedding": [13.395737648010254, -31.210542678833008], "cluster": 7}, {"key": "zhou2016learning", "year": "2017", "citations": "139", "title": "Learning Low Dimensional Convolutional Neural Networks For High-resolution Remote Sensing Image Retrieval", "abstract": "<p>Learning powerful feature representations for image retrieval has always been\na challenging task in the field of remote sensing. Traditional methods focus on\nextracting low-level hand-crafted features which are not only time-consuming\nbut also tend to achieve unsatisfactory performance due to the content\ncomplexity of remote sensing images. In this paper, we investigate how to\nextract deep feature representations based on convolutional neural networks\n(CNN) for high-resolution remote sensing image retrieval (HRRSIR). To this end,\ntwo effective schemes are proposed to generate powerful feature representations\nfor HRRSIR. In the first scheme, the deep features are extracted from the\nfully-connected and convolutional layers of the pre-trained CNN models,\nrespectively; in the second scheme, we propose a novel CNN architecture based\non conventional convolution layers and a three-layer perceptron. The novel CNN\nmodel is then trained on a large remote sensing dataset to learn low\ndimensional features. The two schemes are evaluated on several public and\nchallenging datasets, and the results indicate that the proposed schemes and in\nparticular the novel CNN are able to achieve state-of-the-art performance.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-48.26759338378906, -1.470746636390686], "cluster": 0}, {"key": "zhou2016transfer", "year": "2016", "citations": "25", "title": "Transfer Hashing With Privileged Information", "abstract": "<p>Most existing learning to hash methods assume that there are sufficient data,\neither labeled or unlabeled, on the domain of interest (i.e., the target\ndomain) for training. However, this assumption cannot be satisfied in some\nreal-world applications. To address this data sparsity issue in hashing,\ninspired by transfer learning, we propose a new framework named Transfer\nHashing with Privileged Information (THPI). Specifically, we extend the\nstandard learning to hash method, Iterative Quantization (ITQ), in a transfer\nlearning manner, namely ITQ+. In ITQ+, a new slack function is learned from\nauxiliary data to approximate the quantization error in ITQ. We developed an\nalternating optimization approach to solve the resultant optimization problem\nfor ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure\namong the auxiliary data for learning more precise binary codes in the target\ndomain. Extensive experiments on several benchmark datasets verify the\neffectiveness of our proposed approaches through comparisons with several\nstate-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Tools-&-Libraries", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [15.384391784667969, -4.163280963897705], "cluster": 6}, {"key": "zhou2017deep", "year": "2017", "citations": "4", "title": "Deep Hashing With Triplet Quantization Loss", "abstract": "<p>With the explosive growth of image databases, deep hashing, which learns\ncompact binary descriptors for images, has become critical for fast image\nretrieval. Many existing deep hashing methods leverage quantization loss,\ndefined as distance between the features before and after quantization, to\nreduce the error from binarizing features. While minimizing the quantization\nloss guarantees that quantization has minimal effect on retrieval accuracy, it\nunfortunately significantly reduces the expressiveness of features even before\nthe quantization. In this paper, we show that the above definition of\nquantization loss is too restricted and in fact not necessary for maintaining\nhigh retrieval accuracy. We therefore propose a new form of quantization loss\nmeasured in triplets. The core idea of the triplet quantization loss is to\nlearn discriminative real-valued descriptors which lead to minimal loss on\nretrieval accuracy after quantization. Extensive experiments on two widely used\nbenchmark data sets of different scales, CIFAR-10 and In-shop, demonstrate that\nthe proposed method outperforms the state-of-the-art deep hashing methods.\nMoreover, we show that the compact binary descriptors obtained with triplet\nquantization loss lead to very small performance drop after quantization.</p>\n", "tags": ["Quantization", "Evaluation", "Hashing-Methods", "Neural-Hashing"], "tsne_embedding": [-1.8894603252410889, 20.958471298217773], "cluster": 8}, {"key": "zhou2019ladder", "year": "2020", "citations": "27", "title": "Ladder Loss For Coherent Visual-semantic Embedding", "abstract": "<p>For visual-semantic embedding, the existing methods normally treat the\nrelevance between queries and candidates in a bipolar way \u2013 relevant or\nirrelevant, and all \u201cirrelevant\u201d candidates are uniformly pushed away from the\nquery by an equal margin in the embedding space, regardless of their various\nproximity to the query. This practice disregards relatively discriminative\ninformation and could lead to suboptimal ranking in the retrieval results and\npoorer user experience, especially in the long-tail query scenario where a\nmatching candidate may not necessarily exist. In this paper, we introduce a\ncontinuous variable to model the relevance degree between queries and multiple\ncandidates, and propose to learn a coherent embedding space, where candidates\nwith higher relevance degrees are mapped closer to the query than those with\nlower relevance degrees. In particular, the new ladder loss is proposed by\nextending the triplet loss inequality to a more general inequality chain, which\nimplements variable push-away margins according to respective relevance\ndegrees. In addition, a proper Coherent Score metric is proposed to better\nmeasure the ranking results including those \u201cirrelevant\u201d candidates. Extensive\nexperiments on multiple datasets validate the efficacy of our proposed method,\nwhich achieves significant improvement over existing state-of-the-art methods.</p>\n", "tags": ["AAAI", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-26.700881958007812, 27.09345817565918], "cluster": 8}, {"key": "zhou2020it", "year": "2020", "citations": "1", "title": "It's The Best Only When It Fits You Most: Finding Related Models For Serving Based On Dynamic Locality Sensitive Hashing", "abstract": "<p>In recent, deep learning has become the most popular direction in machine\nlearning and artificial intelligence. However, preparation of training data is\noften a bottleneck in the lifecycle of deploying a deep learning model for\nproduction or research. Reusing models for inferencing a dataset can greatly\nsave the human costs required for training data creation. Although there exist\na number of model sharing platform such as TensorFlow Hub, PyTorch Hub, DLHub,\nmost of these systems require model uploaders to manually specify the details\nof each model and model downloaders to screen keyword search results for\nselecting a model. They are in lack of an automatic model searching tool. This\npaper proposes an end-to-end process of searching related models for serving\nbased on the similarity of the target dataset and the training datasets of the\navailable models. While there exist many similarity measurements, we study how\nto efficiently apply these metrics without pair-wise comparison and compare the\neffectiveness of these metrics. We find that our proposed adaptivity\nmeasurement which is based on Jensen-Shannon (JS) divergence, is an effective\nmeasurement, and its computation can be significantly accelerated by using the\ntechnique of locality sensitive hashing.</p>\n", "tags": ["Locality-Sensitive-Hashing", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [23.132665634155273, -11.639402389526367], "cluster": 7}, {"key": "zhou2020ladder", "year": "2020", "citations": "27", "title": "Ladder Loss For Coherent Visual-semantic Embedding", "abstract": "<p>For visual-semantic embedding, the existing methods normally treat the\nrelevance between queries and candidates in a bipolar way \u2013 relevant or\nirrelevant, and all \u201cirrelevant\u201d candidates are uniformly pushed away from the\nquery by an equal margin in the embedding space, regardless of their various\nproximity to the query. This practice disregards relatively discriminative\ninformation and could lead to suboptimal ranking in the retrieval results and\npoorer user experience, especially in the long-tail query scenario where a\nmatching candidate may not necessarily exist. In this paper, we introduce a\ncontinuous variable to model the relevance degree between queries and multiple\ncandidates, and propose to learn a coherent embedding space, where candidates\nwith higher relevance degrees are mapped closer to the query than those with\nlower relevance degrees. In particular, the new ladder loss is proposed by\nextending the triplet loss inequality to a more general inequality chain, which\nimplements variable push-away margins according to respective relevance\ndegrees. In addition, a proper Coherent Score metric is proposed to better\nmeasure the ranking results including those \u201cirrelevant\u201d candidates. Extensive\nexperiments on multiple datasets validate the efficacy of our proposed method,\nwhich achieves significant improvement over existing state-of-the-art methods.</p>\n", "tags": ["AAAI", "Distance-Metric-Learning", "Datasets"], "tsne_embedding": [-26.700881958007812, 27.09345817565918], "cluster": 8}, {"key": "zhou2021moving", "year": "2022", "citations": "8", "title": "Moving Towards Centers: Re-ranking With Attention And Memory For Re-identification", "abstract": "<p>Re-ranking utilizes contextual information to optimize the initial ranking\nlist of person or vehicle re-identification (re-ID), which boosts the retrieval\nperformance at post-processing steps. This paper proposes a re-ranking network\nto predict the correlations between the probe and top-ranked neighbor samples.\nSpecifically, all the feature embeddings of query and gallery images are\nexpanded and enhanced by a linear combination of their neighbors, with the\ncorrelation prediction serving as discriminative combination weights. The\ncombination process is equivalent to moving independent embeddings toward the\nidentity centers, improving cluster compactness. For correlation prediction, we\nfirst aggregate the contextual information for probe\u2019s k-nearest neighbors via\nthe Transformer encoder. Then, we distill and refine the probe-related features\ninto the Contextual Memory cell via attention mechanism. Like humans that\nretrieve images by not only considering probe images but also memorizing the\nretrieved ones, the Contextual Memory produces multi-view descriptions for each\ninstance. Finally, the neighbors are reconstructed with features fetched from\nthe Contextual Memory, and a binary classifier predicts their correlations with\nthe probe. Experiments on six widely-used person and vehicle re-ID benchmarks\ndemonstrate the effectiveness of the proposed method. Especially, our method\nsurpasses the state-of-the-art re-ranking approaches on large-scale datasets by\na significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP\nimprovements on VERI-Wild, MSMT17, and VehicleID datasets.</p>\n", "tags": ["Scalability", "Datasets", "Re-Ranking", "Hybrid-Ann-Methods", "Evaluation"], "tsne_embedding": [-28.929718017578125, 11.99303150177002], "cluster": 0}, {"key": "zhou2023marvel", "year": "2024", "citations": "1", "title": "MARVEL: Unlocking The Multi-modal Capability Of Dense Retrieval Via Visual Module Plugin", "abstract": "<p>This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin\n(MARVEL), which learns an embedding space for queries and multi-modal documents\nto conduct retrieval. MARVEL encodes queries and multi-modal documents with a\nunified encoder model, which helps to alleviate the modality gap between images\nand texts. Specifically, we enable the image understanding ability of the\nwell-trained dense retriever, T5-ANCE, by incorporating the visual module\u2019s\nencoded image features as its inputs. To facilitate the multi-modal retrieval\ntasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which\nregards anchor texts as queries, and extracts the related text and image\ndocuments from anchor-linked web pages. Our experiments show that MARVEL\nsignificantly outperforms the state-of-the-art methods on the multi-modal\nretrieval dataset WebQA and ClueWeb22-MM. MARVEL provides an opportunity to\nbroaden the advantages of text retrieval to the multi-modal scenario. Besides,\nwe also illustrate that the language model has the ability to extract image\nsemantics and partly map the image features to the input word embedding space.\nAll codes are available at https://github.com/OpenMatch/MARVEL.</p>\n", "tags": ["Evaluation", "Text-Retrieval", "Datasets"], "tsne_embedding": [1.9533048868179321, -33.59113311767578], "cluster": 3}, {"key": "zhou2024vista", "year": "2024", "citations": "0", "title": "VISTA: Visualized Text Embedding For Universal Multi-modal Retrieval", "abstract": "<p>Multi-modal retrieval becomes increasingly popular in practice. However, the\nexisting retrievers are mostly text-oriented, which lack the capability to\nprocess visual information. Despite the presence of vision-language models like\nCLIP, the current methods are severely limited in representing the text-only\nand image-only data. In this work, we present a new embedding model VISTA for\nuniversal multi-modal retrieval. Our work brings forth threefold technical\ncontributions. Firstly, we introduce a flexible architecture which extends a\npowerful text encoder with the image understanding capability by introducing\nvisual token embeddings. Secondly, we develop two data generation strategies,\nwhich bring high-quality composed image-text to facilitate the training of the\nembedding model. Thirdly, we introduce a multi-stage training algorithm, which\nfirst aligns the visual token embedding with the text encoder using massive\nweakly labeled data, and then develops multi-modal representation capability\nusing the generated composed image-text data. In our experiments, VISTA\nachieves superior performances across a variety of multi-modal retrieval tasks\nin both zero-shot and supervised settings. Our model, data, and source code are\navailable at https://github.com/FlagOpen/FlagEmbedding.</p>\n", "tags": ["Few-Shot-&-Zero-Shot", "Supervised"], "tsne_embedding": [-22.938684463500977, -18.9552059173584], "cluster": 5}, {"key": "zhu2013linear", "year": "2013", "citations": "278", "title": "Linear Cross-modal Hashing For Efficient Multimedia Search", "abstract": "<p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel \ncross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia \nsearch across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals \ninto consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. \nMore specifically, for each modal, we first partition the training data into \\(k\\) clusters and then represent each training data \npoint with its distances to \\(k\\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce \nthe time complexity of the training phase from traditional O(n2) or higher to O(n), where \\(n\\) is the training data size, leading to \npractical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. \nTo preserve the inter-similarity among data points across different modals, we transform the derived data representations into a \ncommon binary subspace in which binary codes from all the modals are \u201cconsistent\u201d and comparable. The transformation simultaneously \noutputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, \nit is first mapped into the binary codes using the modal\u2019s hash functions, followed by matching the database binary codes of any other \nmodals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in \ncomparison with the state of the art.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [22.136615753173828, -8.457136154174805], "cluster": 7}, {"key": "zhu2014personalized", "year": "2014", "citations": "25", "title": "Personalized Recommendation With Corrected Similarity", "abstract": "<p>Personalized recommendation attracts a surge of interdisciplinary researches.\nEspecially, similarity based methods in applications of real recommendation\nsystems achieve great success. However, the computations of similarities are\noverestimated or underestimated outstandingly due to the defective strategy of\nunidirectional similarity estimation. In this paper, we solve this drawback by\nleveraging mutual correction of forward and backward similarity estimations,\nand propose a new personalized recommendation index, i.e., corrected similarity\nbased inference (CSI). Through extensive experiments on four benchmark\ndatasets, the results show a greater improvement of CSI in comparison with\nthese mainstream baselines. And the detailed analysis is presented to unveil\nand understand the origin of such difference between CSI and mainstream\nindices.</p>\n", "tags": ["Recommender-Systems", "Evaluation", "Datasets"], "tsne_embedding": [-17.137468338012695, -5.69500207901001], "cluster": 1}, {"key": "zhu2016deep", "year": "2016", "citations": "636", "title": "Deep Hashing Network For Efficient Similarity Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.\nHowever, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.\nThe DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Quantization", "Image-Retrieval", "Similarity-Search", "Scalability", "AAAI", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-5.607899188995361, 5.787330627441406], "cluster": 1}, {"key": "zhu2016radon", "year": "2016", "citations": "12", "title": "Radon Features And Barcodes For Medical Image Retrieval Via SVM", "abstract": "<p>For more than two decades, research has been performed on content-based image\nretrieval (CBIR). By combining Radon projections and the support vector\nmachines (SVM), a content-based medical image retrieval method is presented in\nthis work. The proposed approach employs the normalized Radon projections with\ncorresponding image category labels to build an SVM classifier, and the Radon\nbarcode database which encodes every image in a binary format is also generated\nsimultaneously to tag all images. To retrieve similar images when a query image\nis given, Radon projections and the barcode of the query image are generated.\nSubsequently, the k-nearest neighbor search method is applied to find the\nimages with minimum Hamming distance of the Radon barcode within the same class\npredicted by the trained SVM classifier that uses Radon features. The\nperformance of the proposed method is validated by using the IRMA 2009 dataset\nwith 14,410 x-ray images in 57 categories. The results demonstrate that our\nmethod has the capacity to retrieve similar responses for the correctly\nidentified query image and even for those mistakenly classified by SVM. The\napproach further is very fast and has low memory requirement.</p>\n", "tags": ["Evaluation", "Image-Retrieval", "Datasets"], "tsne_embedding": [-49.31610870361328, 12.74000072479248], "cluster": 0}, {"key": "zhu2017deep", "year": "2017", "citations": "81", "title": "Deep Hybrid Similarity Learning For Person Re-identification", "abstract": "<p>Person Re-IDentification (Re-ID) aims to match person images captured from\ntwo non-overlapping cameras. In this paper, a deep hybrid similarity learning\n(DHSL) method for person Re-ID based on a convolution neural network (CNN) is\nproposed. In our approach, a CNN learning feature pair for the input image pair\nis simultaneously extracted. Then, both the element-wise absolute difference\nand multiplication of the CNN learning feature pair are calculated. Finally, a\nhybrid similarity function is designed to measure the similarity between the\nfeature pair, which is realized by learning a group of weight coefficients to\nproject the element-wise absolute difference and multiplication into a\nsimilarity score. Consequently, the proposed DHSL method is able to reasonably\nassign parameters of feature learning and metric learning in a CNN so that the\nperformance of person Re-ID is improved. Experiments on three challenging\nperson Re-ID databases, QMUL GRID, VIPeR and CUHK03, illustrate that the\nproposed DHSL method is superior to multiple state-of-the-art person Re-ID\nmethods.</p>\n", "tags": ["Evaluation", "Distance-Metric-Learning"], "tsne_embedding": [-50.81597137451172, -5.422351837158203], "cluster": 0}, {"key": "zhu2017discrete", "year": "2017", "citations": "9", "title": "Discrete Multi-modal Hashing With Canonical Views For Robust Mobile Landmark Search", "abstract": "<p>Mobile landmark search (MLS) recently receives increasing attention for its\ngreat practical values. However, it still remains unsolved due to two important\nchallenges. One is high bandwidth consumption of query transmission, and the\nother is the huge visual variations of query images sent from mobile devices.\nIn this paper, we propose a novel hashing scheme, named as canonical view based\ndiscrete multi-modal hashing (CV-DMH), to handle these problems via a novel\nthree-stage learning procedure. First, a submodular function is designed to\nmeasure visual representativeness and redundancy of a view set. With it,\ncanonical views, which capture key visual appearances of landmark with limited\nredundancy, are efficiently discovered with an iterative mining strategy.\nSecond, multi-modal sparse coding is applied to transform visual features from\nmultiple modalities into an intermediate representation. It can robustly and\nadaptively characterize visual contents of varied landmark images with certain\ncanonical views. Finally, compact binary codes are learned on intermediate\nrepresentation within a tailored discrete binary embedding model which\npreserves visual relations of images measured with canonical views and removes\nthe involved noises. In this part, we develop a new augmented Lagrangian\nmultiplier (ALM) based optimization method to directly solve the discrete\nbinary codes. We can not only explicitly deal with the discrete constraint, but\nalso consider the bit-uncorrelated constraint and balance constraint together.\nExperiments on real world landmark datasets demonstrate the superior\nperformance of CV-DMH over several state-of-the-art methods.</p>\n", "tags": ["Compact-Codes", "Evaluation", "Hashing-Methods", "Datasets"], "tsne_embedding": [-18.24545669555664, 4.123361587524414], "cluster": 1}, {"key": "zhu2017part", "year": "2017", "citations": "70", "title": "Part-based Deep Hashing For Large-scale Person Re-identification", "abstract": "<p>Large-scale is a trend in person re-identification (re-id). It is important\nthat real-time search be performed in a large gallery. While previous methods\nmostly focus on discriminative learning, this paper makes the attempt in\nintegrating deep learning and hashing into one framework to evaluate the\nefficiency and accuracy for large-scale person re-id. We integrate spatial\ninformation for discriminative visual representation by partitioning the\npedestrian image into horizontal parts. Specifically, Part-based Deep Hashing\n(PDH) is proposed, in which batches of triplet samples are employed as the\ninput of the deep hashing architecture. Each triplet sample contains two\npedestrian images (or parts) with the same identity and one pedestrian image\n(or part) of the different identity. A triplet loss function is employed with a\nconstraint that the Hamming distance of pedestrian images (or parts) with the\nsame identity is smaller than ones with the different identity. In the\nexperiment, we show that the proposed Part-based Deep Hashing method yields\nvery competitive re-id accuracy on the large-scale Market-1501 and\nMarket-1501+500K datasets.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Efficiency", "Scalability", "Tools-&-Libraries", "Datasets", "Neural-Hashing"], "tsne_embedding": [-32.23097610473633, -43.96409606933594], "cluster": 5}, {"key": "zhu2018attention", "year": "2018", "citations": "89", "title": "Attention-based Pyramid Aggregation Network For Visual Place Recognition", "abstract": "<p>Visual place recognition is challenging in the urban environment and is\nusually viewed as a large scale image retrieval task. The intrinsic challenges\nin place recognition exist that the confusing objects such as cars and trees\nfrequently occur in the complex urban scene, and buildings with repetitive\nstructures may cause over-counting and the burstiness problem degrading the\nimage representations. To address these problems, we present an Attention-based\nPyramid Aggregation Network (APANet), which is trained in an end-to-end manner\nfor place recognition. One main component of APANet, the spatial pyramid\npooling, can effectively encode the multi-size buildings containing\ngeo-information. The other one, the attention block, is adopted as a region\nevaluator for suppressing the confusing regional features while highlighting\nthe discriminative ones. When testing, we further propose a simple yet\neffective PCA power whitening strategy, which significantly improves the widely\nused PCA whitening by reasonably limiting the impact of over-counting.\nExperimental evaluations demonstrate that the proposed APANet outperforms the\nstate-of-the-art methods on two place recognition benchmarks, and generalizes\nwell on standard image retrieval datasets.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [-8.374320983886719, -52.15663146972656], "cluster": 3}, {"key": "zhu2019exploring", "year": "2018", "citations": "154", "title": "Exploring Auxiliary Context: Discrete Semantic Transfer Hashing For Scalable Image Retrieval", "abstract": "<p>Unsupervised hashing can desirably support scalable content-based image\nretrieval (SCBIR) for its appealing advantages of semantic label independence,\nmemory and search efficiency. However, the learned hash codes are embedded with\nlimited discriminative semantics due to the intrinsic limitation of image\nrepresentation. To address the problem, in this paper, we propose a novel\nhashing approach, dubbed as <em>Discrete Semantic Transfer Hashing</em> (DSTH).\nThe key idea is to <em>directly</em> augment the semantics of discrete image hash\ncodes by exploring auxiliary contextual modalities. To this end, a unified\nhashing framework is formulated to simultaneously preserve visual similarities\nof images and perform semantic transfer from contextual modalities. Further, to\nguarantee direct semantic transfer and avoid information loss, we explicitly\nimpose the discrete constraint, bit\u2013uncorrelation constraint and bit-balance\nconstraint on hash codes. A novel and effective discrete optimization method\nbased on augmented Lagrangian multiplier is developed to iteratively solve the\noptimization problem. The whole learning process has linear computation\ncomplexity and desirable scalability. Experiments on three benchmark datasets\ndemonstrate the superiority of DSTH compared with several state-of-the-art\napproaches.</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Efficiency", "Scalability", "Image-Retrieval", "Tools-&-Libraries", "Datasets", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-12.046090126037598, 13.515443801879883], "cluster": 8}, {"key": "zhu2020dual", "year": "2020", "citations": "38", "title": "Dual-level Semantic Transfer Deep Hashing For Efficient Social Image Retrieval", "abstract": "<p>Social network stores and disseminates a tremendous amount of user shared\nimages. Deep hashing is an efficient indexing technique to support large-scale\nsocial image retrieval, due to its deep representation capability, fast\nretrieval speed and low storage cost. Particularly, unsupervised deep hashing\nhas well scalability as it does not require any manually labelled data for\ntraining. However, owing to the lacking of label guidance, existing methods\nsuffer from severe semantic shortage when optimizing a large amount of deep\nneural network parameters. Differently, in this paper, we propose a Dual-level\nSemantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a\nunified deep hash learning framework. Our model targets at learning the\nsemantically enhanced deep hash codes by specially exploiting the\nuser-generated tags associated with the social images. Specifically, we design\na complementary dual-level semantic transfer mechanism to efficiently discover\nthe potential semantics of tags and seamlessly transfer them into binary hash\ncodes. On the one hand, instance-level semantics are directly preserved into\nhash codes from the associated tags with adverse noise removing. Besides, an\nimage-concept hypergraph is constructed for indirectly transferring the latent\nhigh-order semantic correlations of images and tags into hash codes. Moreover,\nthe hash codes are obtained simultaneously with the deep representation\nlearning by the discrete hash optimization strategy. Extensive experiments on\ntwo public social image retrieval datasets validate the superior performance of\nour method compared with state-of-the-art hashing methods. The source codes of\nour method can be obtained at https://github.com/research2020-1/DSTDH</p>\n", "tags": ["Hashing-Methods", "Neural-Hashing", "Scalability", "Image-Retrieval", "Memory-Efficiency", "Tools-&-Libraries", "Datasets", "Evaluation", "Unsupervised"], "tsne_embedding": [4.6831560134887695, -6.652617454528809], "cluster": 6}, {"key": "zhu2022lower", "year": "2022", "citations": "2", "title": "A Lower Bound Of Hash Codes' Performance", "abstract": "<p>As a crucial approach for compact representation learning, hashing has\nachieved great success in effectiveness and efficiency. Numerous heuristic\nHamming space metric learning objectives are designed to obtain high-quality\nhash codes. Nevertheless, a theoretical analysis of criteria for learning good\nhash codes remains largely unexploited. In this paper, we prove that\ninter-class distinctiveness and intra-class compactness among hash codes\ndetermine the lower bound of hash codes\u2019 performance. Promoting these two\ncharacteristics could lift the bound and improve hash learning. We then propose\na surrogate model to fully exploit the above objective by estimating the\nposterior of hash codes and controlling it, which results in a low-bias\noptimization. Extensive experiments reveal the effectiveness of the proposed\nmethod. By testing on a series of hash-models, we obtain performance\nimprovements among all of them, with an up to \\(26.5%\\) increase in mean Average\nPrecision and an up to \\(20.5%\\) increase in accuracy. Our code is publicly\navailable at https://github.com/VL-Group/LBHash.</p>\n", "tags": ["Efficiency", "Evaluation", "Hashing-Methods", "Distance-Metric-Learning"], "tsne_embedding": [20.073989868164062, -0.8649188876152039], "cluster": 6}, {"key": "zhu2023adaptive", "year": "2024", "citations": "2", "title": "Adaptive Confidence Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>The multi-view hash method converts heterogeneous data from multiple views\ninto binary hash codes, which is one of the critical technologies in multimedia\nretrieval. However, the current methods mainly explore the complementarity\namong multiple views while lacking confidence learning and fusion. Moreover, in\npractical application scenarios, the single-view data contain redundant noise.\nTo conduct the confidence learning and eliminate unnecessary noise, we propose\na novel Adaptive Confidence Multi-View Hashing (ACMVH) method. First, a\nconfidence network is developed to extract useful information from various\nsingle-view features and remove noise information. Furthermore, an adaptive\nconfidence multi-view network is employed to measure the confidence of each\nview and then fuse multi-view features through a weighted summation. Lastly, a\ndilation network is designed to further enhance the feature representation of\nthe fused features. To the best of our knowledge, we pioneer the application of\nconfidence learning into the field of multimedia retrieval. Extensive\nexperiments on two public datasets show that the proposed ACMVH performs better\nthan state-of-the-art methods (maximum increase of 3.24%). The source code is\navailable at https://github.com/HackerHyper/ACMVH.</p>\n", "tags": ["ICASSP", "Hashing-Methods", "Datasets"], "tsne_embedding": [-10.07287311553955, 10.73100471496582], "cluster": 8}, {"key": "zhu2023central", "year": "2024", "citations": "1", "title": "Central Similarity Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>Hash representation learning of multi-view heterogeneous data is the key to\nimproving the accuracy of multimedia retrieval. However, existing methods\nutilize local similarity and fall short of deeply fusing the multi-view\nfeatures, resulting in poor retrieval accuracy. Current methods only use local\nsimilarity to train their model. These methods ignore global similarity.\nFurthermore, most recent works fuse the multi-view features via a weighted sum\nor concatenation. We contend that these fusion methods are insufficient for\ncapturing the interaction between various views. We present a novel Central\nSimilarity Multi-View Hashing (CSMVH) method to address the mentioned problems.\nCentral similarity learning is used for solving the local similarity problem,\nwhich can utilize the global similarity between the hash center and samples. We\npresent copious empirical data demonstrating the superiority of gate-based\nfusion over conventional approaches. On the MS COCO and NUS-WIDE, the proposed\nCSMVH performs better than the state-of-the-art methods by a large margin (up\nto 11.41% mean Average Precision (mAP) improvement).</p>\n", "tags": ["Evaluation", "Hashing-Methods"], "tsne_embedding": [7.00062894821167, 8.989974021911621], "cluster": 6}, {"key": "zhu2023clip", "year": "2023", "citations": "1", "title": "CLIP Multi-modal Hashing: A New Baseline CLIPMH", "abstract": "<p>The multi-modal hashing method is widely used in multimedia retrieval. It can\nfuse multi-source data to generate binary hash code. However, the current\nmulti-modal methods have the problem of low retrieval accuracy. The reason is\nthat the individual backbone networks have limited feature expression\ncapabilities and are not jointly pre-trained on large-scale unsupervised\nmulti-modal data. To solve this problem, we propose a new baseline CLIP\nMulti-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and\nimage features, and then fuse to generate hash code. CLIP improves the\nexpressiveness of each modal feature. In this way, it can greatly improve the\nretrieval performance of multi-modal hashing methods. In comparison to\nstate-of-the-art unsupervised and supervised multi-modal hashing methods,\nexperiments reveal that the proposed CLIPMH can significantly enhance\nperformance (Maximum increase of 8.38%). CLIP also has great advantages over\nthe text and visual backbone networks commonly used before.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-0.8208141326904297, -6.030392646789551], "cluster": 1}, {"key": "zhu2023coarse", "year": "2023", "citations": "5", "title": "Coarse-to-fine: Learning Compact Discriminative Representation For Single-stage Image Retrieval", "abstract": "<p>Image retrieval targets to find images from a database that are visually\nsimilar to the query image. Two-stage methods following retrieve-and-rerank\nparadigm have achieved excellent performance, but their separate local and\nglobal modules are inefficient to real-world applications. To better trade-off\nretrieval efficiency and accuracy, some approaches fuse global and local\nfeature into a joint representation to perform single-stage image retrieval.\nHowever, they are still challenging due to various situations to tackle,\n\\(e.g.\\), background, occlusion and viewpoint. In this work, we design a\nCoarse-to-Fine framework to learn Compact Discriminative representation (CFCD)\nfor end-to-end single-stage image retrieval-requiring only image-level labels.\nSpecifically, we first design a novel adaptive softmax-based loss which\ndynamically tunes its scale and margin within each mini-batch and increases\nthem progressively to strengthen supervision during training and intra-class\ncompactness. Furthermore, we propose a mechanism which attentively selects\nprominent local descriptors and infuse fine-grained semantic relations into the\nglobal representation by a hard negative sampling strategy to optimize\ninter-class distinctiveness at a global scale. Extensive experimental results\nhave demonstrated the effectiveness of our method, which achieves\nstate-of-the-art single-stage image retrieval performance on benchmarks such as\nRevisited Oxford and Revisited Paris. Code is available at\nhttps://github.com/bassyess/CFCD.</p>\n", "tags": ["ICCV", "Efficiency", "Image-Retrieval", "Tools-&-Libraries", "Re-Ranking", "Evaluation"], "tsne_embedding": [-29.494375228881836, -4.178944110870361], "cluster": 0}, {"key": "zhu2023deep", "year": "2023", "citations": "14", "title": "Deep Metric Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>Learning the hash representation of multi-view heterogeneous data is an\nimportant task in multimedia retrieval. However, existing methods fail to\neffectively fuse the multi-view features and utilize the metric information\nprovided by the dissimilar samples, leading to limited retrieval precision.\nCurrent methods utilize weighted sum or concatenation to fuse the multi-view\nfeatures. We argue that these fusion methods cannot capture the interaction\namong different views. Furthermore, these methods ignored the information\nprovided by the dissimilar samples. We propose a novel deep metric multi-view\nhashing (DMMVH) method to address the mentioned problems. Extensive empirical\nevidence is presented to show that gate-based fusion is better than typical\nmethods. We introduce deep metric learning to the multi-view hashing problems,\nwhich can utilize metric information of dissimilar samples. On the\nMIR-Flickr25K, MS COCO, and NUS-WIDE, our method outperforms the current\nstate-of-the-art methods by a large margin (up to 15.28 mean Average Precision\n(mAP) improvement).</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "Text-Retrieval", "Evaluation"], "tsne_embedding": [6.845503807067871, 8.972822189331055], "cluster": 6}, {"key": "zhu2023fashion", "year": "2023", "citations": "1", "title": "Fashion Image Retrieval With Multi-granular Alignment", "abstract": "<p>Fashion image retrieval task aims to search relevant clothing items of a\nquery image from the gallery. The previous recipes focus on designing different\ndistance-based loss functions, pulling relevant pairs to be close and pushing\nirrelevant images apart. However, these methods ignore fine-grained features\n(e.g. neckband, cuff) of clothing images. In this paper, we propose a novel\nfashion image retrieval method leveraging both global and fine-grained\nfeatures, dubbed Multi-Granular Alignment (MGA). Specifically, we design a\nFine-Granular Aggregator(FGA) to capture and aggregate detailed patterns. Then\nwe propose Attention-based Token Alignment (ATA) to align image features at the\nmulti-granular level in a coarse-to-fine manner. To prove the effectiveness of\nour proposed method, we conduct experiments on two sub-tasks (In-Shop &amp;\nConsumer2Shop) of the public fashion datasets DeepFashion. The experimental\nresults show that our MGA outperforms the state-of-the-art methods by 1.8% and\n0.6% in the two sub-tasks on the R@1 metric, respectively.</p>\n", "tags": ["Image-Retrieval", "Datasets"], "tsne_embedding": [-18.997093200683594, -48.946529388427734], "cluster": 3}, {"key": "zhu2023r", "year": "2023", "citations": "0", "title": "\\(r^{2}\\)former: Unified \\(r\\)etrieval And \\(r\\)eranking Transformer For Place Recognition", "abstract": "<p>Visual Place Recognition (VPR) estimates the location of query images by\nmatching them with images in a reference database. Conventional methods\ngenerally adopt aggregated CNN features for global retrieval and RANSAC-based\ngeometric verification for reranking. However, RANSAC only employs geometric\ninformation but ignores other possible information that could be useful for\nreranking, e.g. local feature correlations, and attention values. In this\npaper, we propose a unified place recognition framework that handles both\nretrieval and reranking with a novel transformer model, named \\(R^{2}\\)Former.\nThe proposed reranking module takes feature correlation, attention value, and\nxy coordinates into account, and learns to determine whether the image pair is\nfrom the same location. The whole pipeline is end-to-end trainable and the\nreranking module alone can also be adopted on other CNN or transformer\nbackbones as a generic component. Remarkably, \\(R^{2}\\)Former significantly\noutperforms state-of-the-art methods on major VPR datasets with much less\ninference time and memory consumption. It also achieves the state-of-the-art on\nthe hold-out MSLS challenge set and could serve as a simple yet strong solution\nfor real-world large-scale applications. Experiments also show vision\ntransformer tokens are comparable and sometimes better than CNN local features\non local matching. The code is released at\nhttps://github.com/Jeff-Zilence/R2Former.</p>\n", "tags": ["Tools-&-Libraries", "Scalability", "Datasets"], "tsne_embedding": [-52.018089294433594, -4.720165729522705], "cluster": 0}, {"key": "zhu2024bringing", "year": "2024", "citations": "1", "title": "Bringing Multimodality To Amazon Visual Search System", "abstract": "<p>Image to image matching has been well studied in the computer vision\ncommunity. Previous studies mainly focus on training a deep metric learning\nmodel matching visual patterns between the query image and gallery images. In\nthis study, we show that pure image-to-image matching suffers from false\npositives caused by matching to local visual patterns. To alleviate this issue,\nwe propose to leverage recent advances in vision-language pretraining research.\nSpecifically, we introduce additional image-text alignment losses into deep\nmetric learning, which serve as constraints to the image-to-image matching\nloss. With additional alignments between the text (e.g., product title) and\nimage pairs, the model can learn concepts from both modalities explicitly,\nwhich avoids matching low-level visual features. We progressively develop two\nvariants, a 3-tower and a 4-tower model, where the latter takes one more short\ntext query input. Through extensive experiments, we show that this change leads\nto a substantial improvement to the image to image matching problem. We further\nleveraged this model for multimodal search, which takes both image and\nreformulation text queries to improve search quality. Both offline and online\nexperiments show strong improvements on the main metrics. Specifically, we see\n4.95% relative improvement on image matching click through rate with the\n3-tower model and 1.13% further improvement from the 4-tower model.</p>\n", "tags": ["KDD", "Distance-Metric-Learning", "Image-Retrieval"], "tsne_embedding": [-25.93494987487793, -17.212871551513672], "cluster": 5}, {"key": "zhu2024clip", "year": "2024", "citations": "2", "title": "CLIP Multi-modal Hashing For Multimedia Retrieval", "abstract": "<p>Multi-modal hashing methods are widely used in multimedia retrieval, which\ncan fuse multi-source data to generate binary hash code. However, the\nindividual backbone networks have limited feature expression capabilities and\nare not jointly pre-trained on large-scale unsupervised multi-modal data,\nresulting in low retrieval accuracy. To address this issue, we propose a novel\nCLIP Multi-modal Hashing (CLIPMH) method. Our method employs the CLIP framework\nto extract both text and vision features and then fuses them to generate hash\ncode. Due to enhancement on each modal feature, our method has great\nimprovement in the retrieval performance of multi-modal hashing methods.\nCompared with state-of-the-art unsupervised and supervised multi-modal hashing\nmethods, experiments reveal that the proposed CLIPMH can significantly improve\nperformance (a maximum increase of 8.38% in mAP).</p>\n", "tags": ["Hashing-Methods", "Scalability", "Tools-&-Libraries", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [-0.7999036312103271, -6.026705265045166], "cluster": 1}, {"key": "zhu2024cost", "year": "2024", "citations": "4", "title": "Cost: Contrastive Quantization Based Semantic Tokenization For Generative Recommendation", "abstract": "<p>Embedding-based retrieval serves as a dominant approach to candidate item\nmatching for industrial recommender systems. With the success of generative AI,\ngenerative retrieval has recently emerged as a new retrieval paradigm for\nrecommendation, which casts item retrieval as a generation problem. Its model\nconsists of two stages: semantic tokenization and autoregressive generation.\nThe first stage involves item tokenization that constructs discrete semantic\ntokens to index items, while the second stage autoregressively generates\nsemantic tokens of candidate items. Therefore, semantic tokenization serves as\na crucial preliminary step for training generative recommendation models.\nExisting research usually employs a vector quantizier with reconstruction loss\n(e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to\ncapture the essential neighborhood relationships that are vital for effective\nitem modeling in recommender systems. In this paper, we propose a contrastive\nquantization-based semantic tokenization approach, named CoST, which harnesses\nboth item relationships and semantic information to learn semantic tokens. Our\nexperimental results highlight the significant impact of semantic tokenization\non generative recommendation performance, with CoST achieving up to a 43%\nimprovement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over\nprevious baselines.</p>\n", "tags": ["Quantization", "Recommender-Systems", "Datasets", "Evaluation"], "tsne_embedding": [23.151025772094727, -22.84825897216797], "cluster": 7}, {"key": "zhu2024fast", "year": "2024", "citations": "0", "title": "Fast Exact Retrieval For Nearest-neighbor Lookup (FERN)", "abstract": "<p>Exact nearest neighbor search is a computationally intensive process, and\neven its simpler sibling \u2013 vector retrieval \u2013 can be computationally complex.\nThis is exacerbated when retrieving vectors which have high-dimension \\(d\\)\nrelative to the number of vectors, \\(N\\), in the database. Exact nearest neighbor\nretrieval has been generally acknowledged to be a \\(O(Nd)\\) problem with no\nsub-linear solutions. Attention has instead shifted towards Approximate\nNearest-Neighbor (ANN) retrieval techniques, many of which have sub-linear or\neven logarithmic time complexities. However, if our intuition from binary\nsearch problems (e.g. \\(d=1\\) vector retrieval) carries, there ought to be a way\nto retrieve an organized representation of vectors without brute-forcing our\nway to a solution. For low dimension (e.g. \\(d=2\\) or \\(d=3\\) cases),\n\\texttt{kd-trees} provide a \\(O(dlog N)\\) algorithm for retrieval. Unfortunately\nthe algorithm deteriorates rapidly to a \\(O(dN)\\) solution at high dimensions\n(e.g. \\(k=128\\)), in practice. We propose a novel algorithm for logarithmic Fast\nExact Retrieval for Nearest-neighbor lookup (FERN), inspired by\n\\texttt{kd-trees}. The algorithm achieves \\(O(dlog N)\\) look-up with 100%\nrecall on 10 million \\(d=128\\) uniformly randomly generated\nvectors.\\footnote{Code available at https://github.com/RichardZhu123/ferns}</p>\n", "tags": ["Tree-Based-Ann", "Evaluation"], "tsne_embedding": [21.04417610168457, 43.446250915527344], "cluster": 4}, {"key": "zhu2025deep", "year": "2016", "citations": "636", "title": "Deep Hashing Network For Efficient Similarity Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.\nHowever, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.\nThe DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods", "Compact-Codes", "Efficiency", "Quantization", "Image-Retrieval", "Similarity-Search", "Scalability", "AAAI", "Datasets", "Supervised", "Neural-Hashing"], "tsne_embedding": [-5.607836723327637, 5.787292003631592], "cluster": 1}, {"key": "zhu2025linear", "year": "2013", "citations": "278", "title": "Linear Cross-modal Hashing For Efficient Multimedia Search", "abstract": "<p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel \ncross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia \nsearch across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals \ninto consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. \nMore specifically, for each modal, we first partition the training data into \\(k\\) clusters and then represent each training data \npoint with its distances to \\(k\\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce \nthe time complexity of the training phase from traditional O(n2) or higher to O(n), where \\(n\\) is the training data size, leading to \npractical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. \nTo preserve the inter-similarity among data points across different modals, we transform the derived data representations into a \ncommon binary subspace in which binary codes from all the modals are \u201cconsistent\u201d and comparable. The transformation simultaneously \noutputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, \nit is first mapped into the binary codes using the modal\u2019s hash functions, followed by matching the database binary codes of any other \nmodals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in \ncomparison with the state of the art.</p>\n", "tags": ["Hashing-Methods", "Scalability", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [22.13715171813965, -8.457079887390137], "cluster": 7}, {"key": "zhuang2016fast", "year": "2016", "citations": "136", "title": "Fast Training Of Triplet-based Deep Binary Embedding Networks", "abstract": "<p>In this paper, we aim to learn a mapping (or embedding) from images to a\ncompact binary space in which Hamming distances correspond to a ranking measure\nfor the image retrieval task.\n  We make use of a triplet loss because this has been shown to be most\neffective for ranking problems.\n  However, training in previous works can be prohibitively expensive due to the\nfact that optimization is directly performed on the triplet space, where the\nnumber of possible triplets for training is cubic in the number of training\nexamples.\n  To address this issue, we propose to formulate high-order binary codes\nlearning as a multi-label classification problem by explicitly separating\nlearning into two interleaved stages.\n  To solve the first stage, we design a large-scale high-order binary codes\ninference algorithm to reduce the high-order objective to a standard binary\nquadratic problem such that graph cuts can be used to efficiently infer the\nbinary code which serve as the label of each training datum.\n  In the second stage we propose to map the original image to compact binary\ncodes via carefully designed deep convolutional neural networks (CNNs) and the\nhashing function fitting can be solved by training binary CNN classifiers.\n  An incremental/interleaved optimization strategy is proffered to ensure that\nthese two steps are interactive with each other during training for better\naccuracy.\n  We conduct experiments on several benchmark datasets, which demonstrate both\nimproved training time (by as much as two orders of magnitude) as well as\nproducing state-of-the-art hashing for various retrieval tasks.</p>\n", "tags": ["Hashing-Methods", "Distance-Metric-Learning", "CVPR", "Scalability", "Image-Retrieval", "Datasets", "Compact-Codes", "Evaluation"], "tsne_embedding": [13.280299186706543, -5.44773006439209], "cluster": 6}, {"key": "zhuang2023towards", "year": "2023", "citations": "4", "title": "Towards Fast And Accurate Image-text Retrieval With Self-supervised Fine-grained Alignment", "abstract": "<p>Image-text retrieval requires the system to bridge the heterogenous gap\nbetween vision and language for accurate retrieval while keeping the network\nlightweight-enough for efficient retrieval. Existing trade-off solutions mainly\nstudy from the view of incorporating cross-modal interactions with the\nindependent-embedding framework or leveraging stronger pretrained encoders,\nwhich still demand time-consuming similarity measurement or heavyweight model\nstructure in the retrieval stage. In this work, we propose an image-text\nalignment module SelfAlign on top of the independent-embedding framework, which\nimproves the retrieval accuracy while maintains the retrieval efficiency\nwithout extra supervision. SelfAlign contains two collaborative sub-modules\nthat force image-text alignment at both concept level and context level by\nself-supervised contrastive learning. It does not require cross-modal embedding\ninteractions during training while maintaining independent image and text\nencoders during retrieval. With comparable time cost, SelfAlign consistently\nboosts the accuracy of state-of-the-art non-pretraining independent-embedding\nmodels respectively by 9.1%, 4.2% and 6.6% in terms of R@sum score on\nFlickr30K, MSCOCO 1K and MS-COCO 5K datasets. The retrieval accuracy also\noutperforms most existing interactive-embedding models with orders of magnitude\ndecrease in retrieval time. The source code is available at:\nhttps://github.com/Zjamie813/SelfAlign.</p>\n", "tags": ["Self-Supervised", "Text-Retrieval", "Efficiency", "Similarity-Search", "Tools-&-Libraries", "Datasets", "Supervised"], "tsne_embedding": [-33.156532287597656, -2.0098860263824463], "cluster": 0}, {"key": "zieba2018bingan", "year": "2018", "citations": "36", "title": "Bingan: Learning Compact Binary Descriptors With A Regularized GAN", "abstract": "<p>In this paper, we propose a novel regularization method for Generative\nAdversarial Networks, which allows the model to learn discriminative yet\ncompact binary representations of image patches (image descriptors). We employ\nthe dimensionality reduction that takes place in the intermediate layers of the\ndiscriminator network and train binarized low-dimensional representation of the\npenultimate layer to mimic the distribution of the higher-dimensional preceding\nlayers. To achieve this, we introduce two loss terms that aim at: (i) reducing\nthe correlation between the dimensions of the binarized low-dimensional\nrepresentation of the penultimate layer i. e. maximizing joint entropy) and\n(ii) propagating the relations between the dimensions in the high-dimensional\nspace to the low-dimensional space. We evaluate the resulting binary image\ndescriptors on two challenging applications, image matching and retrieval, and\nachieve state-of-the-art results.</p>\n", "tags": ["Hashing-Methods", "Robustness"], "tsne_embedding": [-22.39118003845215, 12.254781723022461], "cluster": 8}, {"key": "zolfaghari2021crossclr", "year": "2021", "citations": "95", "title": "Crossclr: Cross-modal Contrastive Learning For Multi-modal Video Representations", "abstract": "<p>Contrastive learning allows us to flexibly define powerful losses by\ncontrasting positive pairs from sets of negative samples. Recently, the\nprinciple has also been used to learn cross-modal embeddings for video and\ntext, yet without exploiting its full potential. In particular, previous losses\ndo not take the intra-modality similarities into account, which leads to\ninefficient embeddings, as the same content is mapped to multiple points in the\nembedding space. With CrossCLR, we present a contrastive loss that fixes this\nissue. Moreover, we define sets of highly related samples in terms of their\ninput embeddings and exclude them from the negative samples to avoid issues\nwith false negatives. We show that these principles consistently improve the\nquality of the learned embeddings. The joint embeddings learned with CrossCLR\nextend the state of the art in video-text retrieval on Youcook2 and LSMDC\ndatasets and in video captioning on Youcook2 dataset by a large margin. We also\ndemonstrate the generality of the concept by learning improved joint embeddings\nfor other pairs of modalities.</p>\n", "tags": ["Self-Supervised", "ICCV", "Distance-Metric-Learning", "Text-Retrieval", "Datasets"], "tsne_embedding": [-33.514862060546875, -16.994014739990234], "cluster": 5}, {"key": "zoran2017learning", "year": "2017", "citations": "9", "title": "Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees", "abstract": "<p>Nearest neighbor (kNN) methods have been gaining popularity in recent years\nin light of advances in hardware and efficiency of algorithms. There is a\nplethora of methods to choose from today, each with their own advantages and\ndisadvantages. One requirement shared between all kNN based methods is the need\nfor a good representation and distance measure between samples.\n  We introduce a new method called differentiable boundary tree which allows\nfor learning deep kNN representations. We build on the recently proposed\nboundary tree algorithm which allows for efficient nearest neighbor\nclassification, regression and retrieval. By modelling traversals in the tree\nas stochastic events, we are able to form a differentiable cost function which\nis associated with the tree\u2019s predictions. Using a deep neural network to\ntransform the data and back-propagating through the tree allows us to learn\ngood representations for kNN methods.\n  We demonstrate that our method is able to learn suitable representations\nallowing for very efficient trees with a clearly interpretable structure.</p>\n", "tags": ["Efficiency"], "tsne_embedding": [37.391258239746094, -6.179842948913574], "cluster": 9}, {"key": "zou2019transductive", "year": "2020", "citations": "14", "title": "Transductive Zero-shot Hashing For Multilabel Image Retrieval", "abstract": "<p>Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.</p>\n", "tags": ["Hashing-Methods", "Quantization", "Few-Shot-&-Zero-Shot", "Scalability", "Image-Retrieval", "Datasets", "Compact-Codes"], "tsne_embedding": [1.76913321018219, -4.435481548309326], "cluster": 6}, {"key": "zou2023interfacing", "year": "2023", "citations": "0", "title": "Interfacing Foundation Models' Embeddings", "abstract": "<p>Foundation models possess strong capabilities in reasoning and memorizing\nacross modalities. To further unleash the power of foundation models, we\npresent FIND, a generalized interface for aligning foundation models\u2019\nembeddings with unified image and dataset-level understanding spanning modality\nand granularity. As shown in the teaser figure, a lightweight transformer\ninterface without tuning any foundation model weights is enough for\nsegmentation, grounding, and retrieval in an interleaved manner. The proposed\ninterface has the following favorable attributes: (1) Generalizable. It applies\nto various tasks spanning retrieval, segmentation, etc., under the same\narchitecture and weights. (2) Interleavable. With the benefit of multi-task\nmulti-modal training, the proposed interface creates an interleaved shared\nembedding space. (3) Extendable. The proposed interface is adaptive to new\ntasks, and new models. In light of the interleaved embedding space, we\nintroduce FIND-Bench, which introduces new training and evaluation annotations\nto the COCO dataset for interleaved segmentation and retrieval. We are the\nfirst work aligning foundations models\u2019 embeddings for interleave\nunderstanding. Meanwhile, our approach achieves state-of-the-art performance on\nFIND-Bench and competitive performance on standard retrieval and segmentation\nsettings.</p>\n", "tags": ["Evaluation", "Datasets"], "tsne_embedding": [-42.421260833740234, 7.638751983642578], "cluster": 0}, {"key": "zvedeniouk2010angle", "year": "2010", "citations": "0", "title": "Angle Tree: Nearest Neighbor Search In High Dimensions With Low Intrinsic Dimensionality", "abstract": "<p>We propose an extension of tree-based space-partitioning indexing structures\nfor data with low intrinsic dimensionality embedded in a high dimensional\nspace. We call this extension an Angle Tree. Our extension can be applied to\nboth classical kd-trees as well as the more recent rp-trees. The key idea of\nour approach is to store the angle (the \u201cdihedral angle\u201d) between the data\nregion (which is a low dimensional manifold) and the random hyperplane that\nsplits the region (the \u201csplitter\u201d). We show that the dihedral angle can be used\nto obtain a tight lower bound on the distance between the query point and any\npoint on the opposite side of the splitter. This in turn can be used to\nefficiently prune the search space. We introduce a novel randomized strategy to\nefficiently calculate the dihedral angle with a high degree of accuracy.\nExperiments and analysis on real and synthetic data sets shows that the Angle\nTree is the most efficient known indexing structure for nearest neighbor\nqueries in terms of preprocessing and space usage while achieving high accuracy\nand fast search time.</p>\n", "tags": ["Tree-Based-Ann"], "tsne_embedding": [16.215625762939453, 40.803199768066406], "cluster": 4}, {"key": "\u00f1anculef2020self", "year": "2021", "citations": "5", "title": "Self-supervised Bernoulli Autoencoders For Semi-supervised Hashing", "abstract": "<p>Semantic hashing is an emerging technique for large-scale similarity search\nbased on representing high-dimensional data using similarity-preserving binary\ncodes used for efficient indexing and search. It has recently been shown that\nvariational autoencoders, with Bernoulli latent representations parametrized by\nneural nets, can be successfully trained to learn such codes in supervised and\nunsupervised scenarios, improving on more traditional methods thanks to their\nability to handle the binary constraints architecturally. However, the scenario\nwhere labels are scarce has not been studied yet.\n  This paper investigates the robustness of hashing methods based on\nvariational autoencoders to the lack of supervision, focusing on two\nsemi-supervised approaches currently in use. The first augments the variational\nautoencoder\u2019s training objective to jointly model the distribution over the\ndata and the class labels. The second approach exploits the annotations to\ndefine an additional pairwise loss that enforces consistency between the\nsimilarity in the code (Hamming) space and the similarity in the label space.\nOur experiments show that both methods can significantly increase the hash\ncodes\u2019 quality. The pairwise approach can exhibit an advantage when the number\nof labelled points is large. However, we found that this method degrades\nquickly and loses its advantage when labelled samples decrease. To circumvent\nthis problem, we propose a novel supervision method in which the model uses its\nlabel distribution predictions to implement the pairwise objective. Compared to\nthe best baseline, this procedure yields similar performance in fully\nsupervised settings but improves the results significantly when labelled data\nis scarce. Our code is made publicly available at\nhttps://github.com/amacaluso/SSB-VAE.</p>\n", "tags": ["Self-Supervised", "Hashing-Methods", "Neural-Hashing", "Text-Retrieval", "Scalability", "Similarity-Search", "Robustness", "Supervised", "Evaluation", "Unsupervised"], "tsne_embedding": [9.470677375793457, -0.3778521716594696], "cluster": 6}]