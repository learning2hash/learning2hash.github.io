<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Resources | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Resources" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran." />
<meta property="og:description" content="A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran." />
<link rel="canonical" href="https://learning2hash.github.io/resources.html" />
<meta property="og:url" content="https://learning2hash.github.io/resources.html" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Resources" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran.","headline":"Resources","url":"https://learning2hash.github.io/resources.html"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item active" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <h1 id="resources-on-machine-learning-for-hashing">Resources on Machine Learning for Hashing</h1>

<h3 id="-introductory-video-material">üé•üìò Introductory Video Material</h3>

<ul>
  <li>
    <p><strong><a href="https://cs.nju.edu.cn/lwj/slides/L2H.pdf">Dr. Wu-Jun Li‚Äôs tutorial slides</a></strong>: These tutorial slides by Dr. Wu-Jun Li offer a comprehensive introduction to learning to hash (L2H) techniques. It‚Äôs an excellent resource for anyone seeking a deep understanding of hashing from a technical perspective.</p>
  </li>
  <li>
    <p><strong><a href="https://www.youtube.com/watch?v=Arni-zkqMBA">Intro to LSH - Part 1</a></strong>: In this video, Dr. Victor Lavrenko provides an introduction to Locality-Sensitive Hashing (LSH). Part 1 covers the basic concepts and intuition behind LSH, making it accessible for beginners.</p>
  </li>
  <li>
    <p><strong><a href="https://www.youtube.com/watch?v=dgH0NP8Qxa8">Intro to LSH - Part 2</a></strong>: Part 2 of Dr. Lavrenko‚Äôs LSH series dives deeper into the mathematics and mechanics of how LSH works.</p>
  </li>
  <li>
    <p><strong><a href="https://www.youtube.com/embed/tQ0OJXowLJA">Hashing Algorithms for Large-Scale Machine Learning - 2017 Rice Machine Learning Workshop</a></strong>: This video is a recording of a presentation from the 2017 Rice Machine Learning Workshop. It offers a detailed overview of various hashing algorithms used for large-scale machine learning.</p>
  </li>
</ul>

<h3 id="conferences-and-workshops">üé§üßë‚Äçüî¨Conferences and Workshops</h3>

<ul>
  <li>
    <p><strong><a href="https://www.ijcnn.org/">IJCNN 2025: Scalable and Deep Graph Learning and Mining</a></strong>: Workshop including hashing methods applied to graph structures for retrieval and similarity.</p>
  </li>
  <li>
    <p><strong><a href="https://big-ann-benchmarks.com/neurips23.html">Practical Vector Search Challenge 2023</a></strong>: This challenge aims to push the boundaries of approximate nearest neighbor (ANN) search techniques and offers a platform for researchers and developers to benchmark their solutions on billion-scale datasets.</p>
  </li>
  <li>
    <p><strong><a href="http://big-ann-benchmarks.com/index.html#call">Billion-Scale Approximate Nearest Neighbor Search Challenge: NeurIPS‚Äô21 Competition Track</a></strong>: Competitors must improve search accuracy and speed on extremely large datasets, providing valuable insights into the performance of state-of-the-art methods for nearest neighbor search.</p>
  </li>
  <li>
    <p><strong><a href="http://www.ee.oulu.fi/~lili/CEFRLatICCV2019.html">Compact and Efficient Feature Representation and Learning in Computer Vision, ICCV 2019</a></strong>: This workshop at ICCV 2019 focuses on efficient learning techniques for compact feature representations, including binary hashing methods.</p>
  </li>
  <li>
    <p><strong><a href="http://www.sisap.org/2020/">International Conference on Similarity Search and Applications</a></strong>: SISAP is an annual conference dedicated to the study of similarity search techniques.</p>
  </li>
  <li>
    <p><strong><a href="https://workshop-edlcv.github.io/">Joint Workshop on Efficient Deep Learning in Computer Vision</a></strong>: This workshop, co-located with CVPR 2020, focused on the intersection of deep learning and efficient computing techniques for computer vision tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://icde.org/">IEEE International Conference on Data Engineering (ICDE)</a></strong>: ICDE is one of the leading conferences on data engineering, where researchers present advances in data management, indexing, and search.</p>
  </li>
  <li>
    <p><strong><a href="https://www.kdd.org/kdd2021/">ACM International Conference on Knowledge Discovery and Data Mining (KDD)</a></strong>: KDD is a premier conference on data mining and machine learning.</p>
  </li>
  <li>
    <p><strong><a href="https://www.siam.org/conferences/cm/conference/sdm22">SIAM International Conference on Data Mining (SDM)</a></strong>: SDM is an important conference for researchers in data mining, focusing on the latest developments in algorithms, data analysis, and big data applications.</p>
  </li>
</ul>

<h3 id="-survey-papers">üìÑüî¨ Survey Papers</h3>

<p>For a deeper dive, these survey papers are excellent resources:</p>

<ul>
  <li>
    <p><strong><a href="https://arxiv.org/abs/2412.03875">Learning to Hash for Recommendation: A Survey</a></strong> (2024): A dedicated overview of hashing-based methods used in recommender systems, from binary encodings to retrieval-aware deep architectures.</p>
  </li>
  <li>
    <p><strong><a href="https://arxiv.org/pdf/1509.05472.pdf">Learning to Hash for Indexing Big Data - A Survey</a></strong>: This comprehensive survey explores the evolution of hashing techniques for indexing and retrieving big data.</p>
  </li>
  <li>
    <p><strong><a href="https://arxiv.org/pdf/1606.00185.pdf">A Survey on Learning to Hash</a></strong>: This survey provides a detailed overview of different learning-to-hash algorithms, categorized into unsupervised, semi-supervised, and supervised methods.</p>
  </li>
  <li>
    <p><strong><a href="http://www.cs.utexas.edu/~grauman/temp/GraumanFergus_Hashing_chapterdraft.pdf">Learning Binary Hash Codes for Large-Scale Image Search</a></strong>: This paper focuses on learning binary hash codes for efficient large-scale image search. The paper is particularly useful for researchers working on image retrieval and large-scale computer vision tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf">Locality-Sensitive Hashing for Finding Nearest Neighbors</a></strong>: This tutorial-style survey introduces Locality-Sensitive Hashing (LSH) as a method for efficient nearest neighbor search. It explains the principles behind LSH and demonstrates how it can be applied to large-scale datasets.</p>
  </li>
  <li>
    <p><strong><a href="https://arxiv.org/pdf/1909.06046.pdf">Deep Learning for Hashing: A Survey</a></strong>: This survey provides an in-depth overview of deep learning-based hashing techniques, which have become increasingly popular for large-scale retrieval tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://www.sciencedirect.com/science/article/abs/pii/S016786552030208X">Learning to Hash With Binary Deep Neural Networks: A Survey</a></strong>: This survey focuses on binary deep neural networks and their use in learning to hash. It explores how these networks are trained to produce compact binary codes that can be used for efficient data retrieval in large-scale datasets.</p>
  </li>
</ul>

<h3 id="-courses">üéìüìö Courses</h3>

<p>Some university courses cover topics related to machine learning and efficient computing, with publicly available materials:</p>

<ul>
  <li>
    <p><strong><a href="http://www.inf.ed.ac.uk/teaching/courses/exc/index_17-18.html">Extreme Computing</a></strong> (University of Edinburgh): Focuses on the challenges and techniques involved in building and scaling systems for processing massive datasets.</p>
  </li>
  <li>
    <p><strong><a href="https://www.inf.ed.ac.uk/teaching/courses/tts/">Text Technologies for Data Science</a></strong> (University of Edinburgh): Covers processing, analysis, and modeling of textual data. Includes topics in text mining, NLP, and information retrieval ‚Äî with relevance to similarity search and hashing.</p>
  </li>
  <li>
    <p><strong><a href="https://web.stanford.edu/class/cs276/">CS276: Information Retrieval</a></strong> (Stanford University): A comprehensive, foundational course covering algorithms for vector similarity search, ranking, indexing, and hashing.</p>
  </li>
</ul>

<h4 id="-deeplearningai-short-courses-on-vector-search--ann">üß† DeepLearning.AI Short Courses on Vector Search &amp; ANN</h4>

<ul>
  <li>
    <p><strong><a href="https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/?utm_source=chatgpt.com">Vector Databases: from Embeddings to Applications</a></strong>: Learn how vector databases work (dense vs sparse search, multilingual embeddings, hybrid search) with real-world applications using Weaviate. <em>(~55 min)</em></p>
  </li>
  <li>
    <p><strong><a href="https://learn.deeplearning.ai/courses/retrieval-optimization-from-tokenization-to-vector-quantization/lesson/lpcaz/introduction?utm_source=chatgpt.com">Retrieval Optimization: from Tokenization to Vector Quantization</a></strong>: Deep dive into ANN performance tuning ‚Äî covering HNSW, product/scalar/binary quantization, and index compression techniques. Created with Qdrant.</p>
  </li>
  <li>
    <p><strong><a href="https://www.deeplearning.ai/short-courses/building-applications-vector-databases/?utm_source=chatgpt.com">Building Applications with Vector Databases</a></strong>: Hands-on course for building RAG, semantic search, hybrid retrieval, and anomaly detection apps using Pinecone.</p>
  </li>
  <li>
    <p><strong><a href="https://www.deeplearning.ai/courses/retrieval-augmented-generation-rag/?utm_source=chatgpt.com">Retrieval Augmented Generation (RAG)</a></strong>: Explore architectures and implementation of RAG pipelines using vector indices, chunking, retrieval filtering, and prompt design.</p>
  </li>
  <li>
    <p><strong><a href="https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/?utm_source=chatgpt.com">Knowledge Graphs for RAG</a></strong>: Learn how to connect vector embeddings with structured data using Neo4j to improve retrieval in multimodal and structured RAG systems.</p>
  </li>
  <li>
    <p><strong><a href="https://www.deeplearning.ai/short-courses/prompt-compression-and-query-optimization/?utm_source=chatgpt.com">Prompt Compression and Query Optimization</a></strong>: Covers retrieval latency reduction via query filtering, projection, re-ranking, and prompt shortening ‚Äî with examples using MongoDB Atlas Vector Search.</p>
  </li>
</ul>

<h3 id="--blog-posts">üìùüì∞  Blog Posts</h3>

<p>Blog posts are a great way to keep up with cutting-edge research. Here are some of our favorites:</p>

<ul>
  <li>
    <p><strong><a href="https://ann-benchmarks.com/index.html">ANN-Benchmarks</a></strong>: A standard benchmarking platform for evaluating the performance of Approximate Nearest Neighbor (ANN) algorithms on a range of real-world and synthetic datasets. Continuously updated and widely cited, it provides reproducible results for comparison across indexing methods and libraries.</p>
  </li>
  <li>
    <p><strong><a href="https://medium.com/@sean.j.moran/learning-to-hash-finding-the-needle-in-the-haystack-with-ai-24a15f85de0e">Learning to Hash ‚Äî Finding the Needle in the Haystack with AI</a></strong>: This blog post, authored by Sean Moran, provides a beginner-friendly introduction to the concept of learning to hash, focusing on how AI techniques like deep learning can improve approximate nearest neighbor search.</p>
  </li>
  <li>
    <p><strong><a href="https://towardsdatascience.com/fast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb">Fast Near-Duplicate Image Search Using Locality-Sensitive Hashing</a></strong>: This post explains how Locality-Sensitive Hashing (LSH) can be applied to find near-duplicate images efficiently.</p>
  </li>
  <li>
    <p><strong><a href="https://blog.bradfieldcs.com/an-introduction-to-hashing-in-the-era-of-machine-learning-6039394549b0">An Introduction to Hashing in the Era of Machine Learning</a></strong>: This blog post gives an overview of hashing techniques, specifically in the context of modern machine learning applications.</p>
  </li>
  <li>
    <p><strong><a href="https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134">Locality-Sensitive Hashing: Reducing Data Dimensionality</a></strong>: This article introduces Locality-Sensitive Hashing (LSH) as a method for reducing the dimensionality of high-dimensional data while preserving similarity.</p>
  </li>
  <li>
    <p><strong><a href="https://engineering.fb.com/2020/11/23/ai-research/faiss/">Efficient Similarity Search with Faiss</a></strong>: This blog post, from Facebook AI Research (FAIR), provides an in-depth explanation of Faiss, an open-source library for efficient similarity search.</p>
  </li>
  <li>
    <p><strong><a href="https://www.wikiwand.com/en/Johnson%E2%80%93Lindenstrauss_lemma">Johnson‚ÄìLindenstrauss Lemma</a></strong>: This resource describes the Johnson-Lindenstrauss Lemma, a mathematical result that provides a way to reduce the dimensionality of data while approximately preserving distances between points.</p>
  </li>
  <li>
    <p><strong><a href="http://rakaposhi.eas.asu.edu/s01-cse494-mailarchive/msg00054.html">LSH Ideas</a></strong>: This article offers ideas and insights about Locality-Sensitive Hashing (LSH), focusing on its conceptual foundation and potential applications.</p>
  </li>
  <li>
    <p><strong><a href="http://tylerneylon.com/a/lsh1/">Introduction to Locality-Sensitive Hashing (Great Visualizations)</a></strong>: This tutorial, rich with visual aids, provides an easy-to-follow introduction to Locality-Sensitive Hashing (LSH).</p>
  </li>
  <li>
    <p><strong><a href="https://www.quora.com/What-is-locality-sensitive-hashing">What is Locality-Sensitive Hashing?</a></strong>: This Quora discussion explains LSH in simple terms. It covers the core principles of how LSH works and why it is useful for approximate nearest neighbor search.</p>
  </li>
</ul>

<h3 id="-hashing-software-packages">üß©üíæ Hashing Software Packages</h3>

<h4 id="-hashing-algorithms">üì¶ Hashing Algorithms</h4>

<ul>
  <li>
    <p><strong><a href="https://github.com/thulab/DeepHash">Deep Hashing Toolbox</a></strong>: An open-source implementation designed for learning to hash with deep neural networks. Useful for deep similarity search research.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/beowolx/rensa">Rensa (beowolx) ‚Äì High‚Äëperformance MinHash</a></strong>: A Rust‚Äëbased MinHash implementation with Python bindings. Fast and memory-efficient for deduplication tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/yxtay/Deep-Supervised-Hashing">Deep Supervised Hashing (DSH)</a></strong>: A PyTorch implementation of Deep Supervised Hashing, which learns compact binary codes using supervision for high retrieval performance.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/thuml/HashNet">HashNet</a></strong>: Implements HashNet, a deep hashing method that handles imbalanced data distributions and learns binary hash codes end-to-end.</p>
  </li>
</ul>

<h4 id="Ô∏è-indexing--ann-libraries">üèóÔ∏è Indexing / ANN Libraries</h4>

<ul>
  <li>
    <p><strong><a href="https://github.com/facebookresearch/faiss">Faiss (Facebook AI Similarity Search)</a></strong>: A powerful library by Facebook AI Research for efficient similarity search of dense vectors. Supports PQ, IVF, HNSW, and more.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/spotify/annoy">Annoy (Approximate Nearest Neighbors Oh Yeah)</a></strong>: A C++/Python library from Spotify for fast approximate nearest neighbor search. Optimized for read-heavy workloads.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/nmslib/nmslib">NMSLIB</a></strong>: A cross-platform library for similarity search in non-metric spaces. Frequently used in search and recommendation systems.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/nmslib/hnswlib">HNSWlib</a></strong>: Implements Hierarchical Navigable Small World (HNSW) graphs for fast and accurate ANN search.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/google-research/google-research/tree/master/scann">ScaNN (Scalable Nearest Neighbors)</a></strong>: Developed by Google Research, ScaNN is optimized for vector similarity search at production scale using quantization and reordering.</p>
  </li>
</ul>

<h4 id="Ô∏è-vector-databases">üõ†Ô∏è Vector Databases</h4>

<ul>
  <li>
    <p><strong><a href="https://milvus.io/">Milvus</a></strong>: A production-ready open-source vector database for similarity search. Supports multiple ANN algorithms and distributed deployments.</p>
  </li>
  <li>
    <p><strong><a href="https://weaviate.io/">Weaviate</a></strong>: An open-source vector database with semantic search capabilities, supporting hybrid search, classification, and modules like CLIP and OpenAI.</p>
  </li>
  <li>
    <p><strong><a href="https://qdrant.tech/">Qdrant</a></strong>: A fast and scalable vector database written in Rust. Provides gRPC and REST APIs and supports filtering and payload-based search.</p>
  </li>
</ul>

<h3 id="-benchmarking-tools-and-leaderboards">üß™üìä Benchmarking Tools and Leaderboards</h3>

<h4 id="-ann-benchmarks-comparing-nearest-neighbor-libraries">üß™ ANN-Benchmarks: Comparing Nearest Neighbor Libraries</h4>

<p><strong><a href="https://github.com/erikbern/ann-benchmarks">ANN-Benchmarks</a></strong> is the standard benchmarking framework for evaluating Approximate Nearest Neighbor (ANN) algorithms on a wide range of datasets and distance metrics.</p>

<p>It includes:</p>
<ul>
  <li>Dockerized runners for 30+ ANN libraries including FAISS, HNSWlib, NMSLIB, Annoy, ScaNN, Milvus, and more.</li>
  <li>Scripts to run and visualize benchmarking results.</li>
  <li>Precomputed datasets in HDF5 format for fair and reproducible evaluation.</li>
</ul>

<p>üìÑ Related Paper: <a href="https://arxiv.org/abs/1807.05614">Aum√ºller et al. (2019)</a></p>

<h4 id="Ô∏è-evaluated-libraries-on-ann-benchmarks">üóÉÔ∏è Evaluated Libraries on ANN-Benchmarks</h4>
<p>Some key evaluated libraries:</p>
<ul>
  <li><a href="https://github.com/facebookresearch/faiss">FAISS</a></li>
  <li><a href="https://github.com/nmslib/hnswlib">HNSWlib</a></li>
  <li><a href="https://github.com/spotify/annoy">Annoy</a></li>
  <li><a href="https://github.com/google-research/google-research/tree/master/scann">ScaNN</a></li>
  <li><a href="https://github.com/nmslib/nmslib">NMSLIB</a></li>
  <li><a href="https://github.com/weaviate/weaviate">Weaviate</a></li>
  <li><a href="https://github.com/milvus-io/milvus">Milvus</a></li>
  <li><a href="https://github.com/qdrant/qdrant">Qdrant</a></li>
  <li><a href="https://github.com/alexklibisz/elastiknn">Elastiknn</a></li>
  <li><a href="https://github.com/microsoft/SPTAG">SPTAG (Microsoft)</a></li>
  <li><a href="https://github.com/microsoft/diskann">DiskANN (Microsoft)</a></li>
  <li><a href="https://github.com/lmcinnes/pynndescent">PyNNDescent</a></li>
  <li><a href="https://github.com/flann-lib/flann">FLANN</a></li>
</ul>

<p>Full list: <a href="https://github.com/erikbern/ann-benchmarks#evaluated">github.com/erikbern/ann-benchmarks#evaluated</a></p>

<h4 id="-precomputed-benchmark-datasets">üì• Precomputed Benchmark Datasets</h4>
<p>All datasets are split into train/test sets with ground truth for top-100 neighbors:</p>

<table>
  <thead>
    <tr>
      <th>Dataset</th>
      <th>Dim</th>
      <th>Train/Test</th>
      <th>Distance</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DEEP1B</td>
      <td>96</td>
      <td>9.9M / 10k</td>
      <td>Angular</td>
      <td><a href="http://ann-benchmarks.com/deep-image-96-angular.hdf5">HDF5</a></td>
    </tr>
    <tr>
      <td>Fashion-MNIST</td>
      <td>784</td>
      <td>60k / 10k</td>
      <td>Euclidean</td>
      <td><a href="http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5">HDF5</a></td>
    </tr>
    <tr>
      <td>SIFT</td>
      <td>128</td>
      <td>1M / 10k</td>
      <td>Euclidean</td>
      <td><a href="http://ann-benchmarks.com/sift-128-euclidean.hdf5">HDF5</a></td>
    </tr>
    <tr>
      <td>GIST</td>
      <td>960</td>
      <td>1M / 1k</td>
      <td>Euclidean</td>
      <td><a href="http://ann-benchmarks.com/gist-960-euclidean.hdf5">HDF5</a></td>
    </tr>
    <tr>
      <td>NYTimes</td>
      <td>256</td>
      <td>290k / 10k</td>
      <td>Angular</td>
      <td><a href="http://ann-benchmarks.com/nytimes-256-angular.hdf5">HDF5</a></td>
    </tr>
    <tr>
      <td>GloVe (25‚Äì200d)</td>
      <td>‚Äî</td>
      <td>1.18M / 10k</td>
      <td>Angular</td>
      <td><a href="https://github.com/erikbern/ann-benchmarks#datasets">Link</a></td>
    </tr>
    <tr>
      <td>Last.fm</td>
      <td>65</td>
      <td>292k / 50k</td>
      <td>Angular</td>
      <td><a href="http://ann-benchmarks.com/lastfm-64-dot.hdf5">HDF5</a></td>
    </tr>
    <tr>
      <td>COCO-I2I</td>
      <td>512</td>
      <td>113k / 10k</td>
      <td>Angular</td>
      <td><a href="https://github.com/fabiocarrara/str-encoders/releases/download/v0.1.3/coco-i2i-512-angular.hdf5">HDF5</a></td>
    </tr>
    <tr>
      <td>COCO-T2I</td>
      <td>512</td>
      <td>113k / 10k</td>
      <td>Angular</td>
      <td><a href="https://github.com/fabiocarrara/str-encoders/releases/download/v0.1.3/coco-t2i-512-angular.hdf5">HDF5</a></td>
    </tr>
  </tbody>
</table>

<p>More: <a href="http://ann-benchmarks.com">ann-benchmarks.com</a></p>

<h4 id="-related-projects">üß† Related Projects</h4>

<ul>
  <li><strong><a href="https://big-ann-benchmarks.com/neurips23.html">Billion-Scale ANN Leaderboard</a></strong>: Continuously updated leaderboard comparing the performance of various billion-scale approximate nearest neighbor methods across recall, latency, and memory tradeoffs.</li>
</ul>

<h3 id="-books">üìöüìñ Books</h3>

<p>Here are a few recommended books on large-scale machine learning:</p>

<ul>
  <li>
    <p><strong><a href="http://www.mmds.org/">Mining of Massive Datasets</a></strong>: This classic book explores large-scale data mining techniques, including graph processing, clustering, recommendation, and Locality-Sensitive Hashing (LSH). It‚Äôs a core resource for anyone working on scalable algorithms for big data.</p>
  </li>
  <li>
    <p><strong><a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html">Introduction to Information Retrieval</a></strong>: Authored by Manning, Raghavan, and Sch√ºtze, this book is essential reading for understanding search engines, indexing, relevance, and vector space models ‚Äî including chapters on hashing for text retrieval.</p>
  </li>
  <li>
    <p><strong><a href="https://link.springer.com/book/10.1007/978-3-031-01766-7">Efficient Processing of Deep Neural Networks</a></strong>: A practical and theoretical guide to optimizing deep neural networks for deployment. It covers model compression, quantization, and hashing, making it highly relevant for efficient deep hashing research.</p>
  </li>
  <li>
    <p><strong><a href="http://work.caltech.edu/telecourse.html">Learning from Data</a></strong> by Yaser S. Abu-Mostafa et al.: A concise, intuitive introduction to the principles of supervised learning and generalization theory ‚Äî foundational for understanding supervised hashing methods.</p>
  </li>
  <li>
    <p><strong><a href="https://link.springer.com/book/10.1007/3-540-44520-1">Similarity Search: The Metric Space Approach</a></strong> by Zezula et al.: A foundational text on similarity search in metric spaces, offering deep insight into indexing and retrieval techniques that predate modern hashing but remain highly relevant.</p>
  </li>
  <li>
    <p><strong><a href="https://www.cs.cornell.edu/jeh/book.pdf">Foundations of Data Science</a></strong> by Blum, Hopcroft, and Kannan: A mathematically rigorous treatment of data science topics, including high-dimensional geometry, random projections, and algorithms that underlie LSH and related hashing techniques.</p>
  </li>
  <li>
    <p><strong><a href="https://www.deeplearningbook.org/">Deep Learning</a></strong> by Goodfellow, Bengio, and Courville: The definitive book on deep learning. While not specific to hashing, it provides the theoretical backbone for understanding the neural network architectures used in deep supervised hashing models.</p>
  </li>
</ul>

<h3 id="Ô∏è-pre-processed-datasets-for-download">üóÉÔ∏èüì• Pre-Processed Datasets for Download</h3>

<ul>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/875u1rkva9iffpj/Gist512CIFAR10.mat?dl=0">CIFAR-10 Gist Features (.mat)</a></strong>: This dataset contains GIST features extracted from the CIFAR-10 dataset, a popular image classification benchmark.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/dwixb9ry4zwvcp9/LabelMe_gist.mat?dl=0">LabelMe Gist Features (.mat)</a></strong>: A set of GIST features extracted from the LabelMe dataset, which includes a large collection of labeled images.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/x3j6ik6kvnw95h3/MNIST_gnd_release.mat?dl=0">MNIST Pixel Features (.mat)</a></strong>: This dataset contains pixel-level features extracted from the MNIST dataset, a benchmark for handwritten digit recognition.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/29f6r7pqevfy2ck/sift1m.mat?dl=0">SIFT 1M Features (.mat)</a></strong>: This dataset consists of SIFT (Scale-Invariant Feature Transform) descriptors for one million image patches, commonly used in image matching and retrieval tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/wql7m8wuvn9efhj/20Newsgroups.mat?dl=0">20 Newsgroups (.mat)</a></strong>: This dataset contains features extracted from the 20 Newsgroups text dataset, a collection of approximately 20,000 documents categorized into 20 different newsgroups.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/qasz8z3sr1pjqog/TDT2.mat?dl=0">TDT2 (.mat)</a></strong>: This dataset includes features from the Topic Detection and Tracking (TDT2) dataset, designed for research on text retrieval and clustering.</p>
  </li>
  <li>
    <p><strong><a href="http://corpus-texmex.irisa.fr/">BIGANN Dataset</a></strong>: The BIGANN dataset includes SIFT descriptors extracted from a large collection of images. It is widely used for benchmarking large-scale approximate nearest neighbor (ANN) search algorithms.</p>
  </li>
  <li>
    <p><strong><a href="https://dl.fbaipublicfiles.com/billion-scale-ann-benchmarks/FB_ssnpp_database.u8bin">Facebook SimSearchNet++</a></strong>: A large-scale dataset developed by Facebook for the SimSearchNet++ model, which is used to benchmark billion-scale similarity search algorithms in the context of AI and machine learning applications.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/microsoft/SPTAG/tree/master/datasets/SPACEV1B">Microsoft SPACEV-1B</a></strong>: This dataset from Microsoft includes one billion vectors for testing large-scale similarity search algorithms. It is a benchmark for efficient vector retrieval systems and helps evaluate ANN algorithms‚Äô performance.</p>
  </li>
  <li>
    <p><strong><a href="https://research.yandex.com/datasets/biganns">Yandex DEEP-1B</a></strong>: The DEEP-1B dataset from Yandex consists of one billion deep image descriptors for benchmarking approximate nearest neighbor search algorithms. It provides a challenging, large-scale benchmark for evaluating hashing and ANN methods.</p>
  </li>
  <li>
    <p><strong><a href="https://research.yandex.com/datasets/biganns">Yandex Text-to-Image-1B</a></strong>: A dataset that includes one billion text-to-image matching features, useful for evaluating and benchmarking similarity search techniques that bridge the gap between text and image modalities.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/facebookresearch/faiss/wiki/Indexing-1B-vectors">Deep1B Dataset</a></strong>: The Deep1B dataset contains one billion deep representations of images, widely used in large-scale similarity search benchmarks.</p>
  </li>
  <li>
    <p><strong><a href="https://research.yandex.com/datasets">DEEP-10M</a></strong>: A smaller variant of the DEEP-1B dataset, containing 10 million deep image descriptors.</p>
  </li>
  <li>
    <p><strong><a href="https://gluebenchmark.com/">GLUE Benchmark</a></strong>: The General Language Understanding Evaluation (GLUE) benchmark consists of a variety of natural language processing tasks that test a model‚Äôs understanding of language. While not traditionally used for image hashing, it provides valuable challenges for text-based hashing techniques.</p>
  </li>
</ul>


    </div>

  </body>
</html>
