<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Resources | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Resources" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran." />
<meta property="og:description" content="A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran." />
<link rel="canonical" href="https://learning2hash.github.io/resources.html" />
<meta property="og:url" content="https://learning2hash.github.io/resources.html" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Resources" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A Webpage dedicated to the latest research on Hash Function Learning. Maintained by Sean Moran.","headline":"Resources","url":"https://learning2hash.github.io/resources.html"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item active" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <h1 id="resources-on-machine-learning-for-hashing">Resources on Machine Learning for Hashing</h1>

<h3 id="conferences-and-workshops">Conferences and Workshops</h3>

<ul>
  <li>
    <p><strong><a href="https://big-ann-benchmarks.com/neurips23.html">Practical Vector Search Challenge 2023</a></strong>: This competition is part of the NeurIPS 2023 event, focusing on developing advanced indexing data structures and algorithms for vector search problems. The challenge aims to push the boundaries of approximate nearest neighbor (ANN) search techniques and offers a platform for researchers and developers to benchmark their solutions on billion-scale datasets.</p>
  </li>
  <li>
    <p><strong><a href="http://big-ann-benchmarks.com/index.html#call">Billion-Scale Approximate Nearest Neighbor Search Challenge: NeurIPS’21 Competition Track</a></strong>: Held at NeurIPS 2021, this challenge invites participants to develop efficient and scalable ANN search algorithms. Competitors must improve search accuracy and speed on extremely large datasets, providing valuable insights into the performance of state-of-the-art methods for nearest neighbor search.</p>
  </li>
  <li>
    <p><strong><a href="http://www.ee.oulu.fi/~lili/CEFRLatICCV2019.html">Compact and Efficient Feature Representation and Learning in Computer Vision, ICCV 2019</a></strong>: This workshop at ICCV 2019 focuses on efficient learning techniques for compact feature representations, including binary hashing methods. It highlights the growing importance of resource-efficient models in computer vision, especially in applications requiring fast retrieval or low-memory footprint solutions.</p>
  </li>
  <li>
    <p><strong><a href="http://www.sisap.org/2020/">International Conference on Similarity Search and Applications</a></strong>: SISAP is an annual conference dedicated to the study of similarity search techniques. It provides a platform for discussing algorithms and applications related to similarity-based search, including learning to hash, ANN, and metric space indexing. The conference covers both theoretical and practical aspects of similarity search across various domains.</p>
  </li>
  <li>
    <p><strong><a href="https://workshop-edlcv.github.io/">Joint Workshop on Efficient Deep Learning in Computer Vision</a></strong>: This workshop, co-located with CVPR 2020, focuses on the intersection of deep learning and efficient computing techniques for computer vision tasks. The topics include model compression, efficient training, and binary hashing methods for large-scale image retrieval, offering researchers insights into reducing the computational cost of deep learning models.</p>
  </li>
  <li>
    <p><strong><a href="https://icde.org/">IEEE International Conference on Data Engineering (ICDE)</a></strong>: ICDE is one of the leading conferences on data engineering, where researchers present advances in data management, indexing, and search. It includes topics like efficient data indexing and retrieval using hashing techniques, making it a significant venue for discussing large-scale data handling solutions.</p>
  </li>
  <li>
    <p><strong><a href="https://www.kdd.org/kdd2021/">ACM International Conference on Knowledge Discovery and Data Mining (KDD)</a></strong>: KDD is a premier conference on data mining and machine learning. The conference features sessions on learning to hash, ANN search, and large-scale data retrieval methods. It is a key platform for researchers exploring machine learning techniques for data-driven decision-making and efficient search algorithms.</p>
  </li>
  <li>
    <p><strong><a href="https://www.siam.org/conferences/cm/conference/sdm22">SIAM International Conference on Data Mining (SDM)</a></strong>: SDM is an important conference for researchers in data mining, focusing on the latest developments in algorithms, data analysis, and big data applications. It includes discussions on hashing algorithms, nearest neighbor search, and related topics, making it relevant for those interested in efficient data processing techniques.</p>
  </li>
</ul>

<h3 id="introductory-video-material">Introductory Video Material</h3>

<ul>
  <li>
    <p><strong><a href="https://cs.nju.edu.cn/lwj/slides/L2H.pdf">Dr. Wu-Jun Li’s tutorial slides</a></strong>: These tutorial slides by Dr. Wu-Jun Li offer a comprehensive introduction to learning to hash (L2H) techniques. They cover the theoretical foundations, different hashing methods, and their applications in machine learning and large-scale data retrieval. It’s an excellent resource for anyone seeking a deep understanding of hashing from a technical perspective.</p>
  </li>
  <li>
    <p><strong><a href="https://www.youtube.com/watch?v=Arni-zkqMBA">Intro to LSH - Part 1</a></strong>: In this video, Dr. Victor Lavrenko provides an introduction to Locality-Sensitive Hashing (LSH), focusing on how it helps to reduce the dimensionality of data and improve the efficiency of nearest neighbor search. Part 1 covers the basic concepts and intuition behind LSH, making it accessible for beginners.</p>
  </li>
  <li>
    <p><strong><a href="https://www.youtube.com/watch?v=dgH0NP8Qxa8">Intro to LSH - Part 2</a></strong>: Part 2 of Dr. Lavrenko’s LSH series dives deeper into the mathematics and mechanics of how LSH works. It explains how to implement LSH for high-dimensional data and demonstrates its practical applications in machine learning, particularly in similarity search and large-scale data retrieval.</p>
  </li>
  <li>
    <p><strong><a href="https://www.youtube.com/embed/tQ0OJXowLJA">Hashing Algorithms for Large-Scale Machine Learning - 2017 Rice Machine Learning Workshop</a></strong>: This video is a recording of a presentation from the 2017 Rice Machine Learning Workshop. It offers a detailed overview of various hashing algorithms used for large-scale machine learning. The presentation includes discussions on how hashing can improve efficiency in tasks like data indexing, image retrieval, and nearest neighbor search, making it a valuable resource for researchers and practitioners working on large datasets.</p>
  </li>
</ul>

<h3 id="survey-papers">Survey Papers</h3>

<p>For a deeper dive, these survey papers are excellent resources:</p>

<ul>
  <li>
    <p><strong><a href="https://arxiv.org/pdf/1509.05472.pdf">Learning to Hash for Indexing Big Data - A Survey</a></strong>: This comprehensive survey explores the evolution of hashing techniques for indexing and retrieving big data. It covers various approaches to learning to hash, including data-dependent and data-independent methods, and evaluates their effectiveness in handling large-scale datasets. The survey highlights the importance of hashing in accelerating data retrieval for applications like search engines, recommendation systems, and large-scale machine learning.</p>
  </li>
  <li>
    <p><strong><a href="https://arxiv.org/pdf/1606.00185.pdf">A Survey on Learning to Hash</a></strong>: This survey provides a detailed overview of different learning-to-hash algorithms, categorized into unsupervised, semi-supervised, and supervised methods. It discusses their applications in tasks like image retrieval, nearest neighbor search, and recommendation systems. The paper offers a comparative analysis of the performance of various hashing methods, making it a valuable resource for researchers looking to understand the current state of the field.</p>
  </li>
  <li>
    <p><strong><a href="http://www.cs.utexas.edu/~grauman/temp/GraumanFergus_Hashing_chapterdraft.pdf">Learning Binary Hash Codes for Large-Scale Image Search</a></strong>: This paper focuses on learning binary hash codes for efficient large-scale image search. It provides insights into different hashing techniques used to compress high-dimensional image features into binary codes while preserving their visual similarity. The paper is particularly useful for researchers working on image retrieval and large-scale computer vision tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf">Locality-Sensitive Hashing for Finding Nearest Neighbors</a></strong>: This tutorial-style survey introduces Locality-Sensitive Hashing (LSH) as a method for efficient nearest neighbor search. It explains the principles behind LSH and demonstrates how it can be applied to large-scale datasets, reducing the computational cost of finding nearest neighbors in high-dimensional spaces. The paper includes practical examples and is a great introduction to LSH for researchers and practitioners.</p>
  </li>
  <li>
    <p><strong><a href="https://arxiv.org/pdf/1909.06046.pdf">Deep Learning for Hashing: A Survey</a></strong>: This survey provides an in-depth overview of deep learning-based hashing techniques, which have become increasingly popular for large-scale retrieval tasks. It discusses various deep learning models for generating hash codes and compares their performance with traditional hashing methods. The paper also highlights emerging trends and challenges in the field, making it a must-read for anyone interested in deep learning applications in hashing.</p>
  </li>
  <li>
    <p><strong><a href="https://www.sciencedirect.com/science/article/abs/pii/S016786552030208X">Learning to Hash With Binary Deep Neural Networks: A Survey</a></strong>: This survey focuses on binary deep neural networks and their use in learning to hash. It explores how these networks are trained to produce compact binary codes that can be used for efficient data retrieval in large-scale datasets. The paper offers a detailed comparison of binary neural networks with other learning-to-hash methods, making it a valuable resource for those interested in the intersection of deep learning and hashing.</p>
  </li>
</ul>

<h3 id="pre-processed-datasets-for-download">Pre-Processed Datasets for Download</h3>

<ul>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/875u1rkva9iffpj/Gist512CIFAR10.mat?dl=0">CIFAR-10 Gist Features (.mat)</a></strong>: This dataset contains Gist features extracted from the CIFAR-10 dataset, a popular image classification benchmark. Gist features are low-dimensional representations of images used for image retrieval and similarity search tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/dwixb9ry4zwvcp9/LabelMe_gist.mat?dl=0">LabelMe Gist Features (.mat)</a></strong>: A set of Gist features extracted from the LabelMe dataset, which includes a large collection of labeled images. The Gist features provide a compact representation of the image content, making them useful for hashing-based image retrieval.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/x3j6ik6kvnw95h3/MNIST_gnd_release.mat?dl=0">MNIST Pixel Features (.mat)</a></strong>: This dataset contains pixel-level features extracted from the MNIST dataset, a benchmark for handwritten digit recognition. The features can be used in machine learning models and hashing techniques for classification and retrieval tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/29f6r7pqevfy2ck/sift1m.mat?dl=0">SIFT 1M Features (.mat)</a></strong>: This dataset consists of SIFT (Scale-Invariant Feature Transform) descriptors for one million image patches, commonly used in image matching and retrieval tasks. SIFT descriptors are often used in similarity search and hashing algorithms for visual search applications.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/wql7m8wuvn9efhj/20Newsgroups.mat?dl=0">20 Newsgroups (.mat)</a></strong>: This dataset contains features extracted from the 20 Newsgroups text dataset, a collection of approximately 20,000 documents categorized into 20 different newsgroups. It is frequently used for text classification and document retrieval experiments.</p>
  </li>
  <li>
    <p><strong><a href="https://www.dropbox.com/s/qasz8z3sr1pjqog/TDT2.mat?dl=0">TDT2 (.mat)</a></strong>: This dataset includes features from the Topic Detection and Tracking (TDT2) dataset, designed for research on text retrieval and clustering. It contains documents labeled with different categories, ideal for testing hashing-based text retrieval methods.</p>
  </li>
  <li>
    <p><strong><a href="http://corpus-texmex.irisa.fr/">BIGANN Dataset</a></strong>: The BIGANN dataset includes SIFT descriptors extracted from a large collection of images. It is widely used for benchmarking large-scale approximate nearest neighbor (ANN) search algorithms, making it an essential dataset for similarity search research.</p>
  </li>
  <li>
    <p><strong><a href="https://dl.fbaipublicfiles.com/billion-scale-ann-benchmarks/FB_ssnpp_database.u8bin">Facebook SimSearchNet++</a></strong>: A large-scale dataset developed by Facebook for the SimSearchNet++ model, which is used to benchmark billion-scale similarity search algorithms in the context of AI and machine learning applications.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/microsoft/SPTAG/tree/master/datasets/SPACEV1B">Microsoft SPACEV-1B</a></strong>: This dataset from Microsoft includes one billion vectors for testing large-scale similarity search algorithms. It is a benchmark for efficient vector retrieval systems and helps evaluate ANN algorithms’ performance.</p>
  </li>
  <li>
    <p><strong><a href="https://research.yandex.com/datasets/biganns">Yandex DEEP-1B</a></strong>: The DEEP-1B dataset from Yandex consists of one billion deep image descriptors for benchmarking approximate nearest neighbor search algorithms. It provides a challenging, large-scale benchmark for evaluating hashing and ANN methods.</p>
  </li>
  <li>
    <p><strong><a href="https://research.yandex.com/datasets/biganns">Yandex Text-to-Image-1B</a></strong>: A dataset that includes one billion text-to-image matching features, useful for evaluating and benchmarking similarity search techniques that bridge the gap between text and image modalities.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/facebookresearch/faiss/wiki/Indexing-1B-vectors">Deep1B Dataset</a></strong>: The Deep1B dataset contains one billion deep representations of images, widely used in large-scale similarity search benchmarks, especially for evaluating the performance of hashing and indexing algorithms in high-dimensional spaces.</p>
  </li>
  <li>
    <p><strong><a href="https://research.yandex.com/datasets">DEEP-10M</a></strong>: A smaller variant of the DEEP-1B dataset, containing 10 million deep image descriptors. It is used for testing similarity search and hashing algorithms on smaller but still large-scale datasets.</p>
  </li>
  <li>
    <p><strong><a href="https://gluebenchmark.com/">GLUE Benchmark</a></strong>: The General Language Understanding Evaluation (GLUE) benchmark consists of a variety of natural language processing tasks that test a model’s understanding of language. While not traditionally used for image hashing, it provides valuable challenges for text-based hashing techniques.</p>
  </li>
</ul>

<h3 id="courses">Courses</h3>

<p>Some university courses cover topics related to machine learning and efficient computing, with publicly available materials:</p>

<ul>
  <li>
    <p><strong><a href="http://www.inf.ed.ac.uk/teaching/courses/exc/index_17-18.html">Extreme Computing</a> at the University of Edinburgh</strong>: This course focuses on the challenges and techniques involved in building and scaling systems for processing massive datasets. Topics include distributed computing, parallelism, and efficient algorithms for large-scale data processing, with applications in machine learning, data mining, and search systems. Hashing techniques for data indexing and retrieval are also covered.</p>
  </li>
  <li>
    <p><strong><a href="https://www.inf.ed.ac.uk/teaching/courses/tts/">Text Technologies for Data Science</a> at the University of Edinburgh</strong>: This course covers the processing, analysis, and modeling of textual data in data science applications. It includes topics such as text mining, natural language processing, and information retrieval, with a focus on how these techniques can be used to extract insights from large text corpora. Hashing techniques for efficient text retrieval are part of the syllabus.</p>
  </li>
  <li>
    <p><strong><a href="https://web.stanford.edu/class/cs276/">CS276: Information Retrieval</a> at Stanford University</strong>: A comprehensive course that covers the fundamental principles of information retrieval, including algorithms for vector similarity search and hashing. It introduces students to key concepts in text search engines, web crawling, and indexing techniques, making it an essential resource for understanding the role of hashing in efficient search and retrieval systems.</p>
  </li>
</ul>

<h3 id="blog-posts">Blog Posts</h3>

<p>Blog posts are a great way to keep up with cutting-edge research. Here are some of our favorites:</p>

<ul>
  <li>
    <p><strong><a href="https://medium.com/@sean.j.moran/learning-to-hash-finding-the-needle-in-the-haystack-with-ai-24a15f85de0e">Learning to Hash — Finding the Needle in the Haystack with AI</a></strong>: This blog post, authored by Sean Moran, provides a beginner-friendly introduction to the concept of learning to hash, focusing on how AI techniques like deep learning can improve approximate nearest neighbor search. It explores how learning-based hashing is applied to efficiently retrieve relevant information from massive datasets, serving as a foundation for modern search engines and recommendation systems.</p>
  </li>
  <li>
    <p><strong><a href="https://towardsdatascience.com/fast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb">Fast Near-Duplicate Image Search Using Locality-Sensitive Hashing</a></strong>: This post explains how Locality-Sensitive Hashing (LSH) can be applied to find near-duplicate images efficiently. It breaks down the key steps of the process, such as using LSH to map high-dimensional image vectors to hash codes that allow for quick comparison. It is a practical guide to LSH, making it highly relevant for developers working on image retrieval problems.</p>
  </li>
  <li>
    <p><strong><a href="https://blog.bradfieldcs.com/an-introduction-to-hashing-in-the-era-of-machine-learning-6039394549b0">An Introduction to Hashing in the Era of Machine Learning</a></strong>: This blog post gives an overview of hashing techniques, specifically in the context of modern machine learning applications. It introduces readers to hashing’s key role in indexing, data storage, and efficient retrieval, and explores how these techniques are evolving with the rise of big data and AI-driven algorithms.</p>
  </li>
  <li>
    <p><strong><a href="https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134">Locality-Sensitive Hashing: Reducing Data Dimensionality</a></strong>: This article introduces Locality-Sensitive Hashing (LSH) as a method for reducing the dimensionality of high-dimensional data while preserving similarity. It explains how LSH can be used for approximate nearest neighbor search and gives an intuitive breakdown of the technique, making it accessible to those new to the topic.</p>
  </li>
  <li>
    <p><strong><a href="https://engineering.fb.com/2020/11/23/ai-research/faiss/">Efficient Similarity Search with Faiss</a></strong>: This blog post, from Facebook AI Research (FAIR), provides an in-depth explanation of Faiss, an open-source library for efficient similarity search. It covers the library’s internal mechanics, its use cases in large-scale similarity search tasks, and how Faiss handles billion-scale datasets, offering practical insights for developers and researchers working on AI-driven search systems.</p>
  </li>
  <li>
    <p><strong><a href="https://www.wikiwand.com/en/Johnson%E2%80%93Lindenstrauss_lemma">Johnson–Lindenstrauss Lemma</a></strong>: This resource describes the Johnson-Lindenstrauss Lemma, a mathematical result that provides a way to reduce the dimensionality of data while approximately preserving distances between points. The lemma is foundational in the field of dimensionality reduction and has applications in hashing, especially in scenarios involving high-dimensional data.</p>
  </li>
  <li>
    <p><strong><a href="http://rakaposhi.eas.asu.edu/s01-cse494-mailarchive/msg00054.html">LSH Ideas</a></strong>: This article offers ideas and insights about Locality-Sensitive Hashing (LSH), focusing on its conceptual foundation and potential applications. It provides a more academic look at the LSH method, aimed at readers looking to deepen their understanding of its theoretical underpinnings.</p>
  </li>
  <li>
    <p><strong><a href="http://tylerneylon.com/a/lsh1/">Introduction to Locality-Sensitive Hashing (Great Visualizations)</a></strong>: This tutorial, rich with visual aids, provides an easy-to-follow introduction to Locality-Sensitive Hashing (LSH). It explains the LSH technique step-by-step, making it accessible to beginners and those who appreciate learning through visualizations.</p>
  </li>
  <li>
    <p><strong><a href="https://www.quora.com/What-is-locality-sensitive-hashing">What is Locality-Sensitive Hashing?</a></strong>: This Quora discussion explains LSH in simple terms. It covers the core principles of how LSH works and why it is useful for approximate nearest neighbor search, providing a concise yet effective overview for those new to the concept.</p>
  </li>
</ul>

<h3 id="hashing-software-packages">Hashing Software Packages</h3>

<ul>
  <li>
    <p><strong><a href="https://github.com/facebookresearch/faiss">Faiss (Facebook AI Similarity Search)</a></strong>: Faiss is a high-performance library developed by Facebook AI Research (FAIR) for efficient similarity search and clustering of dense vectors. It is optimized for handling large-scale datasets and provides a range of indexing algorithms, including brute-force, approximate nearest neighbor search, and quantization techniques, making it widely used in machine learning and AI applications.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/spotify/annoy">Annoy (Approximate Nearest Neighbors Oh Yeah)</a></strong>: Annoy is a fast and lightweight C++ library developed by Spotify for performing approximate nearest neighbor search in high-dimensional spaces. It is particularly well-suited for applications requiring quick retrieval of similar vectors, such as music recommendation systems, and is optimized for read-heavy tasks with low memory overhead.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/thulab/DeepHash">Deep Hashing Toolbox</a></strong>: Deep Hashing Toolbox is an open-source implementation designed for learning to hash with deep neural networks. It includes a variety of deep learning-based hashing models aimed at generating compact binary codes for efficient image and data retrieval. This toolbox is a valuable resource for researchers working on deep learning-driven similarity search tasks.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/nmslib/nmslib">NMSLIB</a></strong>: NMSLIB (Non-Metric Space Library) is a cross-platform library designed for approximate nearest neighbor search in non-metric spaces. It supports a wide variety of search methods, including those for metric and non-metric spaces, making it a highly flexible tool for large-scale search applications. NMSLIB is commonly used in search engines, recommendation systems, and machine learning pipelines.</p>
  </li>
  <li>
    <p><strong><a href="https://github.com/nmslib/hnswlib">HNSWlib</a></strong>: HNSWlib is an efficient implementation of Hierarchical Navigable Small World (HNSW) graphs, which provide fast approximate nearest neighbor search. This library is widely used for large-scale search tasks, offering low memory usage and high accuracy, making it suitable for high-dimensional data retrieval tasks in machine learning and AI projects.</p>
  </li>
</ul>

<h3 id="books">Books</h3>

<p>Here are a few recommended books on large-scale machine learning:</p>

<ul>
  <li>
    <p><strong><a href="http://www.mmds.org/">Mining of Massive Datasets</a></strong>: This book covers a wide range of large-scale data mining topics, including graph processing, machine learning algorithms, and large-scale search. It features a detailed section on Locality-Sensitive Hashing (LSH), explaining how LSH can be used to efficiently index and retrieve data in large datasets. This is a fundamental resource for understanding data mining techniques at scale.</p>
  </li>
  <li>
    <p><strong><a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html">Introduction to Information Retrieval</a></strong>: A classic textbook in the field of information retrieval, authored by leading experts, it covers the full spectrum of data indexing and retrieval techniques. The book explores topics such as vector space models, relevance ranking, and search algorithms, with sections dedicated to hashing techniques for efficient data retrieval.</p>
  </li>
  <li>
    <p><strong><a href="https://link.springer.com/book/10.1007/978-3-031-01766-7">Efficient Processing of Deep Neural Networks</a></strong>: This book provides a thorough exploration of various techniques for optimizing and processing deep neural networks. It covers both the theoretical foundations and practical implementations for improving the efficiency of DNNs, including model compression, quantization, and hashing for efficient data storage and retrieval. It is essential for researchers focused on the intersection of deep learning and efficient computation.</p>
  </li>
  <li>
    <p><strong><a href="https://deepai.org/machine-learning-glossary-and-terms/deep-learning">Hashing and Deep Learning in Nearest Neighbor Search</a></strong>: This article discusses the intersection of hashing techniques and deep learning for large-scale search tasks, such as approximate nearest neighbor (ANN) search. It explores how deep learning models can be combined with hashing methods to create more efficient and scalable search algorithms. The resource is particularly useful for those interested in deep learning’s application to big data search problems.</p>
  </li>
</ul>


    </div>

  </body>
</html>
