[
{"key": "adir2022privacy", "year": "2022", "citations": "5", "title":"Privacy-preserving record linkage using local sensitive hash and private set intersection", "abstract": "<p>The amount of data stored in data repositories increases every year. This\nmakes it challenging to link records between different datasets across\ncompanies and even internally, while adhering to privacy regulations. Address\nor name changes, and even different spelling used for entity data, can prevent\ncompanies from using private deduplication or record-linking solutions such as\nprivate set intersection (PSI). To this end, we propose a new and efficient\nprivacy-preserving record linkage (PPRL) protocol that combines PSI and local\nsensitive hash (LSH) functions, and runs in linear time. We explain the privacy\nguarantees that our protocol provides and demonstrate its practicality by\nexecuting the protocol over two datasets with \\(2^{20}\\) records each, in \\(11-45\\)\nminutes, depending on network settings.</p>\n", "tags": ["Datasets","Locality-Sensitive-Hashing"] },
{"key": "aggarwal2018learning", "year": "2019", "citations": "12", "title":"Learning Style Compatibility for Furniture", "abstract": "<p>When judging style, a key question that often arises is whether or not a pair\nof objects are compatible with each other. In this paper we investigate how\nSiamese networks can be used efficiently for assessing the style compatibility\nbetween images of furniture items. We show that the middle layers of pretrained\nCNNs can capture essential information about furniture style, which allows for\nefficient applications of such networks for this task. We also use a joint\nimage-text embedding method that allows for the querying of stylistically\ncompatible furniture items, along with additional attribute constraints based\non text. To evaluate our methods, we collect and present a large scale dataset\nof images of furniture of different style categories accompanied by text\nattributes.</p>\n", "tags": ["Datasets"] },
{"key": "agrawal2020tag", "year": "2021", "citations": "21", "title":"Tag Embedding Based Personalized Point Of Interest Recommendation System", "abstract": "<p>Personalized Point of Interest recommendation is very helpful for satisfying\nusers’ needs at new places. In this article, we propose a tag embedding based\nmethod for Personalized Recommendation of Point Of Interest. We model the\nrelationship between tags corresponding to Point Of Interest. The model\nprovides representative embedding corresponds to a tag in a way that related\ntags will be closer. We model Point of Interest-based on tag embedding and also\nmodel the users (user profile) based on the Point Of Interest rated by them.\nfinally, we rank the user’s candidate Point Of Interest based on cosine\nsimilarity between user’s embedding and Point of Interest’s embedding. Further,\nwe find the parameters required to model user by discrete optimizing over\ndifferent measures (like ndcg@5, MRR, …). We also analyze the result while\nconsidering the same parameters for all users and individual parameters for\neach user. Along with it we also analyze the effect on the result while\nchanging the dataset to model the relationship between tags. Our method also\nminimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase\n2 dataset and have significant improvement over all the measures on the state\nof the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,\nwhich shows the effectiveness of the method.</p>\n", "tags": ["Datasets","Recommender-Systems"] },
{"key": "aguerrebere2023similarity", "year": "2023", "citations": "15", "title":"Similarity search in the blink of an eye with compressed indices", "abstract": "<p>Nowadays, data is represented by vectors. Retrieving those vectors, among\nmillions and billions, that are similar to a given query is a ubiquitous\nproblem, known as similarity search, of relevance for a wide range of\napplications. Graph-based indices are currently the best performing techniques\nfor billion-scale similarity search. However, their random-access memory\npattern presents challenges to realize their full potential. In this work, we\npresent new techniques and systems for creating faster and smaller graph-based\nindices. To this end, we introduce a novel vector compression method,\nLocally-adaptive Vector Quantization (LVQ), that uses per-vector scaling and\nscalar quantization to improve search performance with fast similarity\ncomputations and a reduced effective bandwidth, while decreasing memory\nfootprint and barely impacting accuracy. LVQ, when combined with a new\nhigh-performance computing system for graph-based similarity search,\nestablishes the new state of the art in terms of performance and memory\nfootprint. For billions of vectors, LVQ outcompetes the second-best\nalternatives: (1) in the low-memory regime, by up to 20.7x in throughput with\nup to a 3x memory footprint reduction, and (2) in the high-throughput regime by\n5.8x with 1.4x less memory.</p>\n", "tags": ["Similarity-Search","Graph-Based-ANN","Quantization","Memory-Efficiency","Scalability","Large-Scale-Search","Evaluation"] },
{"key": "ahle2016parameter", "year": "2017", "citations": "19", "title":"Parameter-free Locality Sensitive Hashing for Spherical Range Reporting", "abstract": "<p>We present a data structure for <em>spherical range reporting</em> on a point set\n\\(S\\), i.e., reporting all points in \\(S\\) that lie within radius \\(r\\) of a given\nquery point \\(q\\). Our solution builds upon the Locality-Sensitive Hashing (LSH)\nframework of Indyk and Motwani, which represents the asymptotically best\nsolutions to near neighbor problems in high dimensions. While traditional LSH\ndata structures have several parameters whose optimal values depend on the\ndistance distribution from \\(q\\) to the points of \\(S\\), our data structure is\nparameter-free, except for the space usage, which is configurable by the user.\nNevertheless, its expected query time basically matches that of an LSH data\nstructure whose parameters have been <em>optimally chosen for the data and query</em>\nin question under the given space constraints. In particular, our data\nstructure provides a smooth trade-off between hard queries (typically addressed\nby standard LSH) and easy queries such as those where the number of points to\nreport is a constant fraction of \\(S\\), or where almost all points in \\(S\\) are far\naway from the query point. In contrast, known data structures fix LSH\nparameters based on certain parameters of the input alone.\n  The algorithm has expected query time bounded by \\(O(t (n/t)^\\rho)\\), where \\(t\\)\nis the number of points to report and \\(\\rho\\in (0,1)\\) depends on the data\ndistribution and the strength of the LSH family used. We further present a\nparameter-free way of using multi-probing, for LSH families that support it,\nand show that for many such families this approach allows us to get expected\nquery time close to \\(O(n^\\rho+t)\\), which is the best we can hope to achieve\nusing LSH. The previously best running time in high dimensions was \\(Ω(t\nn^\\rho)\\). For many data distributions where the intrinsic dimensionality of the\npoint set close to \\(q\\) is low, we can give improved upper bounds on the\nexpected query time.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Tools-&-Libraries","Efficiency"] },
{"key": "ahle2017optimal", "year": "2017", "citations": "13", "title":"Optimal Las Vegas Locality Sensitive Data Structures", "abstract": "<p>We show that approximate similarity (near neighbour) search can be solved in\nhigh dimensions with performance matching state of the art (data independent)\nLocality Sensitive Hashing, but with a guarantee of no false negatives.\n  Specifically, we give two data structures for common problems.\n  For \\(c\\)-approximate near neighbour in Hamming space we get query time\n\\(dn^{1/c+o(1)}\\) and space \\(dn^{1+1/c+o(1)}\\) matching that of\n\\cite{indyk1998approximate} and answering a long standing open question\nfrom~\\cite{indyk2000dimensionality} and~\\cite{pagh2016locality} in the\naffirmative.\n  By means of a new deterministic reduction from \\(\\ell_1\\) to Hamming we also\nsolve \\(\\ell_1\\) and \\(ℓ₂\\) with query time \\(d^2n^{1/c+o(1)}\\) and space \\(d^2\nn^{1+1/c+o(1)}\\).\n  For \\((s_1,s_2)\\)-approximate Jaccard similarity we get query time\n\\(dn^{\\rho+o(1)}\\) and space \\(dn^{1+\\rho+o(1)}\\),\n\\(\\rho=log\\frac{1+s_1}{2s_1}\\big/log\\frac{1+s_2}{2s_2}\\), when sets have equal\nsize, matching the performance of~\\cite{tobias2016}.\n  The algorithms are based on space partitions, as with classic LSH, but we\nconstruct these using a combination of brute force, tensoring, perfect hashing\nand splitter functions `a la~\\cite{naor1995splitters}. We also show a new\ndimensionality reduction lemma with 1-sided error.</p>\n", "tags": ["Hashing-Methods","Evaluation","Locality-Sensitive-Hashing","Efficiency"] },
{"key": "aksoy2022satellite", "year": "2022", "citations": "8", "title":"Satellite Image Search in AgoraEO", "abstract": "<p>The growing operational capability of global Earth Observation (EO) creates\nnew opportunities for data-driven approaches to understand and protect our\nplanet. However, the current use of EO archives is very restricted due to the\nhuge archive sizes and the limited exploration capabilities provided by EO\nplatforms. To address this limitation, we have recently proposed MiLaN, a\ncontent-based image retrieval approach for fast similarity search in satellite\nimage archives. MiLaN is a deep hashing network based on metric learning that\nencodes high-dimensional image features into compact binary hash codes. We use\nthese codes as keys in a hash table to enable real-time nearest neighbor search\nand highly accurate retrieval. In this demonstration, we showcase the\nefficiency of MiLaN by integrating it with EarthQube, a browser and search\nengine within AgoraEO. EarthQube supports interactive visual exploration and\nQuery-by-Example over satellite image repositories. Demo visitors will interact\nwith EarthQube playing the role of different users that search images in a\nlarge-scale remote sensing archive by their semantic content and apply other\nfilters.</p>\n", "tags": ["Similarity-Search","Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Neural-Hashing","Scalability","Efficiency"] },
{"key": "alemu2018multi", "year": "2019", "citations": "23", "title":"Multi-feature Fusion for Image Retrieval Using Constrained Dominant Sets", "abstract": "<p>Aggregating different image features for image retrieval has recently shown\nits effectiveness. While highly effective, though, the question of how to\nuplift the impact of the best features for a specific query image persists as\nan open computer vision problem. In this paper, we propose a computationally\nefficient approach to fuse several hand-crafted and deep features, based on the\nprobabilistic distribution of a given membership score of a constrained cluster\nin an unsupervised manner. First, we introduce an incremental nearest neighbor\n(NN) selection method, whereby we dynamically select k-NN to the query. We then\nbuild several graphs from the obtained NN sets and employ constrained dominant\nsets (CDS) on each graph G to assign edge weights which consider the intrinsic\nmanifold structure of the graph, and detect false matches to the query.\nFinally, we elaborate the computation of feature positive-impact weight (PIW)\nbased on the dispersive degree of the characteristics vector. To this end, we\nexploit the entropy of a cluster membership-score distribution. In addition,\nthe final NN set bypasses a heuristic voting scheme. Experiments on several\nretrieval benchmark datasets show that our method can improve the\nstate-of-the-art result.</p>\n", "tags": ["Unsupervised","Datasets","Evaluation","Image-Retrieval"] },
{"key": "amato2016aggregating", "year": "2017", "citations": "11", "title":"Aggregating Binary Local Descriptors for Image Retrieval", "abstract": "<p>Content-Based Image Retrieval based on local features is computationally\nexpensive because of the complexity of both extraction and matching of local\nfeature. On one hand, the cost for extracting, representing, and comparing\nlocal visual descriptors has been dramatically reduced by recently proposed\nbinary local features. On the other hand, aggregation techniques provide a\nmeaningful summarization of all the extracted feature of an image into a single\ndescriptor, allowing us to speed up and scale up the image search. Only a few\nworks have recently mixed together these two research directions, defining\naggregation methods for binary local features, in order to leverage on the\nadvantage of both approaches. In this paper, we report an extensive comparison\namong state-of-the-art aggregation methods applied to binary features. Then, we\nmathematically formalize the application of Fisher Kernels to Bernoulli Mixture\nModels. Finally, we investigate the combination of the aggregated binary\nfeatures with the emerging Convolutional Neural Network (CNN) features. Our\nresults show that aggregation methods on binary features are effective and\nrepresent a worthwhile alternative to the direct matching. Moreover, the\ncombination of the CNN with the Fisher Vector (FV) built upon binary features\nallowed us to obtain a relative improvement over the CNN results that is in\nline with that recently obtained using the combination of the CNN with the FV\nbuilt upon SIFTs. The advantage of using the FV built upon binary features is\nthat the extraction process of binary features is about two order of magnitude\nfaster than SIFTs.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "amrouche2021hashing", "year": "2021", "citations": "5", "title":"Hashing and metric learning for charged particle tracking", "abstract": "<p>We propose a novel approach to charged particle tracking at high intensity\nparticle colliders based on Approximate Nearest Neighbors search. With hundreds\nof thousands of measurements per collision to be reconstructed e.g. at the High\nLuminosity Large Hadron Collider, the currently employed combinatorial track\nfinding approaches become inadequate. Here, we use hashing techniques to\nseparate measurements into buckets of 20-50 hits and increase their purity\nusing metric learning. Two different approaches are studied to further resolve\ntracks inside buckets: Local Fisher Discriminant Analysis and Neural Networks\nfor triplet similarity learning. We demonstrate the proposed approach on\nsimulated collisions and show significant speed improvement with bucket\ntracking efficiency of 96% and a fake rate of 8% on unseen particle events.</p>\n", "tags": ["Hashing-Methods","Distance-Metric-Learning","Efficiency"] },
{"key": "an2020fast", "year": "2022", "citations": "44", "title":"Fast and Incremental Loop Closure Detection with Deep Features and Proximity Graphs", "abstract": "<p>In recent years, the robotics community has extensively examined methods\nconcerning the place recognition task within the scope of simultaneous\nlocalization and mapping applications.This article proposes an appearance-based\nloop closure detection pipeline named ``FILD++” (Fast and Incremental Loop\nclosure Detection).First, the system is fed by consecutive images and, via\npassing them twice through a single convolutional neural network, global and\nlocal deep features are extracted.Subsequently, a hierarchical navigable\nsmall-world graph incrementally constructs a visual database representing the\nrobot’s traversed path based on the computed global features.Finally, a query\nimage, grabbed each time step, is set to retrieve similar locations on the\ntraversed route.An image-to-image pairing follows, which exploits local\nfeatures to evaluate the spatial information. Thus, in the proposed article, we\npropose a single network for global and local feature extraction in contrast to\nour previous work (FILD), while an exhaustive search for the verification\nprocess is adopted over the generated deep local features avoiding the\nutilization of hash codes. Exhaustive experiments on eleven publicly available\ndatasets exhibit the system’s high performance (achieving the highest recall\nscore on eight of them) and low execution times (22.05 ms on average in New\nCollege, which is the largest one containing 52480 images) compared to other\nstate-of-the-art approaches.</p>\n", "tags": ["Hashing-Methods","Datasets","Evaluation","Graph-Based-ANN"] },
{"key": "andoni2008near", "year": "2008", "citations": "1420", "title":"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", "abstract": "<p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>\n", "tags": ["Hashing-Methods","Efficiency"] },
{"key": "andoni2015practical", "year": "2015", "citations": "239", "title":"Practical and Optimal LSH for Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular\ndistance that yields an approximate Near Neighbor Search algorithm with the\nasymptotically optimal running time exponent. Unlike earlier algorithms with this\nproperty (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe\nversion of this algorithm and conduct an experimental evaluation on real\nand synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the\nquality of any LSH family for angular distance. Our lower bound implies that the\nabove LSH family exhibits a trade-off between evaluation time and quality that is\nclose to optimal for a natural class of LSH functions.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Evaluation"] },
{"key": "andoni2016optimal", "year": "2017", "citations": "66", "title":"Optimal Hashing-based Time-Space Trade-offs for Approximate Near Neighbors", "abstract": "<p>[See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n\\(c\\)-Approximate Near Neighbor Search problem. For the \\(d\\)-dimensional Euclidean\nspace and \\(n\\)-point datasets, we develop a data structure with space \\(n^{1 +\n\\rho_u + o(1)} + O(dn)\\) and query time \\(n^{\\rho_q + o(1)} + d n^{o(1)}\\) for\nevery \\(\\rho_u, \\rho_q \\geq 0\\) such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor \\(c &gt; 1\\), improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for \\(\\rho_q = 0\\), improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes.</p>\n", "tags": ["Hashing-Methods","Datasets","Efficiency"] },
{"key": "andoni2018approximate", "year": "2019", "citations": "49", "title":"Approximate Nearest Neighbor Search in High Dimensions", "abstract": "<p>The nearest neighbor problem is defined as follows: Given a set \\(P\\) of \\(n\\)\npoints in some metric space \\((X,D)\\), build a data structure that, given any\npoint \\(q\\), returns a point in \\(P\\) that is closest to \\(q\\) (its “nearest\nneighbor” in \\(P\\)). The data structure stores additional information about the\nset \\(P\\), which is then used to find the nearest neighbor without computing all\ndistances between \\(q\\) and \\(P\\). The problem has a wide range of applications in\nmachine learning, computer vision, databases and other fields.\n  To reduce the time needed to find nearest neighbors and the amount of memory\nused by the data structure, one can formulate the {\\em approximate} nearest\nneighbor problem, where the the goal is to return any point \\(p’ \\in P\\) such\nthat the distance from \\(q\\) to \\(p’\\) is at most \\(c \\cdot \\min_{p \\in P} D(q,p)\\),\nfor some \\(c \\geq 1\\). Over the last two decades, many efficient solutions to\nthis problem were developed. In this article we survey these developments, as\nwell as their connections to questions in geometric functional analysis and\ncombinatorial geometry.</p>\n", "tags": ["Survey-Paper"] },
{"key": "andoni2025near", "year": "2008", "citations": "1420", "title":"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", "abstract": "<p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>\n", "tags": ["Hashing-Methods","Efficiency"] },
{"key": "andoni2025practical", "year": "2015", "citations": "239", "title":"Practical and Optimal LSH for Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular\ndistance that yields an approximate Near Neighbor Search algorithm with the\nasymptotically optimal running time exponent. Unlike earlier algorithms with this\nproperty (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe\nversion of this algorithm and conduct an experimental evaluation on real\nand synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the\nquality of any LSH family for angular distance. Our lower bound implies that the\nabove LSH family exhibits a trade-off between evaluation time and quality that is\nclose to optimal for a natural class of LSH functions.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Evaluation"] },
{"key": "andré2017accelerated", "year": "2017", "citations": "14", "title":"Accelerated Nearest Neighbor Search with Quick ADC", "abstract": "<p>Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).</p>\n", "tags": ["Quantization","Evaluation","Efficiency","Compact-Codes","Multimodal-Retrieval"] },
{"key": "appalaraju2017image", "year": "2017", "citations": "58", "title":"Image similarity using Deep CNN and Curriculum Learning", "abstract": "<p>Image similarity involves fetching similar looking images given a reference\nimage. Our solution called SimNet, is a deep siamese network which is trained\non pairs of positive and negative images using a novel online pair mining\nstrategy inspired by Curriculum learning. We also created a multi-scale CNN,\nwhere the final image embedding is a joint representation of top as well as\nlower layer embedding’s. We go on to show that this multi-scale siamese network\nis better at capturing fine grained image similarities than traditional CNN’s.</p>\n", "tags": ["Uncategorized"] },
{"key": "artetxe2018margin", "year": "2019", "citations": "163", "title":"Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings", "abstract": "<p>Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version.</p>\n", "tags": ["Distance-Metric-Learning","Evaluation"] },
{"key": "artetxe2018massively", "year": "2019", "citations": "747", "title":"Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond", "abstract": "<p>We introduce an architecture to learn joint multilingual sentence\nrepresentations for 93 languages, belonging to more than 30 different families\nand written in 28 different scripts. Our system uses a single BiLSTM encoder\nwith a shared BPE vocabulary for all languages, which is coupled with an\nauxiliary decoder and trained on publicly available parallel corpora. This\nenables us to learn a classifier on top of the resulting embeddings using\nEnglish annotated data only, and transfer it to any of the 93 languages without\nany modification. Our experiments in cross-lingual natural language inference\n(XNLI dataset), cross-lingual document classification (MLDoc dataset) and\nparallel corpus mining (BUCC dataset) show the effectiveness of our approach.\nWe also introduce a new test set of aligned sentences in 112 languages, and\nshow that our sentence embeddings obtain strong results in multilingual\nsimilarity search even for low-resource languages. Our implementation, the\npre-trained encoder and the multilingual test set are available at\nhttps://github.com/facebookresearch/LASER</p>\n", "tags": ["Similarity-Search","Few-Shot-&-Zero-Shot","Datasets"] },
{"key": "artetxe2019bilingual", "year": "2019", "citations": "53", "title":"Bilingual Lexicon Induction through Unsupervised Machine Translation", "abstract": "<p>A recent research line has obtained strong results on bilingual lexicon\ninduction by aligning independently trained word embeddings in two languages\nand using the resulting cross-lingual embeddings to induce word translation\npairs through nearest neighbor or related retrieval methods. In this paper, we\npropose an alternative approach to this problem that builds on the recent work\non unsupervised machine translation. This way, instead of directly inducing a\nbilingual lexicon from cross-lingual embeddings, we use them to build a\nphrase-table, combine it with a language model, and use the resulting machine\ntranslation system to generate a synthetic parallel corpus, from which we\nextract the bilingual lexicon using statistical word alignment techniques. As\nsuch, our method can work with any word embedding and cross-lingual mapping\ntechnique, and it does not require any additional resource besides the\nmonolingual corpus used to train the embeddings. When evaluated on the exact\nsame cross-lingual embeddings, our proposed method obtains an average\nimprovement of 6 accuracy points over nearest neighbor and 4 points over CSLS\nretrieval, establishing a new state-of-the-art in the standard MUSE dataset.</p>\n", "tags": ["Unsupervised","Datasets"] },
{"key": "artetxe2019margin", "year": "2019", "citations": "163", "title":"Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings", "abstract": "<p>Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version.</p>\n", "tags": ["Distance-Metric-Learning","Evaluation"] },
{"key": "atighehchi2019cryptanalysis", "year": "2020", "citations": "56", "title":"A Cryptanalysis of Two Cancelable Biometric Schemes based on Index-of-Max Hashing", "abstract": "<p>Cancelable biometric schemes generate secure biometric templates by combining\nuser specific tokens and biometric data. The main objective is to create\nirreversible, unlinkable, and revocable templates, with high accuracy in\nmatching. In this paper, we cryptanalyze two recent cancelable biometric\nschemes based on a particular locality sensitive hashing function, index-of-max\n(IoM): Gaussian Random Projection-IoM (GRP-IoM) and Uniformly Random\nPermutation-IoM (URP-IoM). As originally proposed, these schemes were claimed\nto be resistant against reversibility, authentication, and linkability attacks\nunder the stolen token scenario. We propose several attacks against GRP-IoM and\nURP-IoM, and argue that both schemes are severely vulnerable against\nauthentication and linkability attacks. We also propose better, but not yet\npractical, reversibility attacks against GRP-IoM. The correctness and practical\nimpact of our attacks are verified over the same dataset provided by the\nauthors of these two schemes.</p>\n", "tags": ["Hashing-Methods","Datasets","Locality-Sensitive-Hashing"] },
{"key": "aumüller2017distance", "year": "2018", "citations": "18", "title":"Distance-Sensitive hashing", "abstract": "<p>Locality-sensitive hashing (LSH) is an important tool for managing\nhigh-dimensional noisy or uncertain data, for example in connection with data\ncleaning (similarity join) and noise-robust search (similarity search).\nHowever, for a number of problems the LSH framework is not known to yield good\nsolutions, and instead ad hoc solutions have been designed for particular\nsimilarity and distance measures. For example, this is true for\noutput-sensitive similarity search/join, and for indexes supporting annulus\nqueries that aim to report a point close to a certain given distance from the\nquery point.\n  In this paper we initiate the study of distance-sensitive hashing (DSH), a\ngeneralization of LSH that seeks a family of hash functions such that the\nprobability of two points having the same hash value is a given function of the\ndistance between them. More precisely, given a distance space \\((X,\n\\text{dist})\\) and a “collision probability function” (CPF) \\(f\\colon\n\\mathbb{R}\\rightarrow [0,1]\\) we seek a distribution over pairs of functions\n\\((h,g)\\) such that for every pair of points \\(x, y \\in X\\) the collision\nprobability is \\(\\Pr[h(x)=g(y)] = f(\\text{dist}(x,y))\\). Locality-sensitive\nhashing is the study of how fast a CPF can decrease as the distance grows. For\nmany spaces, \\(f\\) can be made exponentially decreasing even if we restrict\nattention to the symmetric case where \\(g=h\\). We show that the asymmetry\nachieved by having a pair of functions makes it possible to achieve CPFs that\nare, for example, increasing or unimodal, and show how this leads to principled\nsolutions to problems not addressed by the LSH framework. This includes a novel\napplication to privacy-preserving distance estimation. We believe that the DSH\nframework will find further applications in high-dimensional data management.</p>\n", "tags": ["Hashing-Methods","Similarity-Search","Locality-Sensitive-Hashing","Tools-&-Libraries"] },
{"key": "aumüller2020differentially", "year": "2020", "citations": "5", "title":"Differentially Private Sketches for Jaccard Similarity Estimation", "abstract": "<p>This paper describes two locally-differential private algorithms for\nreleasing user vectors such that the Jaccard similarity between these vectors\ncan be efficiently estimated. The basic building block is the well known\nMinHash method. To achieve a privacy-utility trade-off, MinHash is extended in\ntwo ways using variants of Generalized Randomized Response and the Laplace\nMechanism. A theoretical analysis provides bounds on the absolute error and\nexperiments show the utility-privacy trade-off on synthetic and real-world\ndata. The paper ends with a critical discussion of related work.</p>\n", "tags": ["Locality-Sensitive-Hashing"] },
{"key": "avgoustinakis2020audio", "year": "2021", "citations": "8", "title":"Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning", "abstract": "<p>In this work, we address the problem of audio-based near-duplicate video\nretrieval. We propose the Audio Similarity Learning (AuSiL) approach that\neffectively captures temporal patterns of audio similarity between video pairs.\nFor the robust similarity calculation between two videos, we first extract\nrepresentative audio-based video descriptors by leveraging transfer learning\nbased on a Convolutional Neural Network (CNN) trained on a large scale dataset\nof audio events, and then we calculate the similarity matrix derived from the\npairwise similarity of these descriptors. The similarity matrix is subsequently\nfed to a CNN network that captures the temporal structures existing within its\ncontent. We train our network following a triplet generation process and\noptimizing the triplet loss function. To evaluate the effectiveness of the\nproposed approach, we have manually annotated two publicly available video\ndatasets based on the audio duplicity between their videos. The proposed\napproach achieves very competitive results compared to three state-of-the-art\nmethods. Also, unlike the competing methods, it is very robust to the retrieval\nof audio duplicates generated with speed transformations.</p>\n", "tags": ["Distance-Metric-Learning","Video-Retrieval","Datasets"] },
{"key": "avgoustinakis2021audio", "year": "2021", "citations": "8", "title":"Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning", "abstract": "<p>In this work, we address the problem of audio-based near-duplicate video\nretrieval. We propose the Audio Similarity Learning (AuSiL) approach that\neffectively captures temporal patterns of audio similarity between video pairs.\nFor the robust similarity calculation between two videos, we first extract\nrepresentative audio-based video descriptors by leveraging transfer learning\nbased on a Convolutional Neural Network (CNN) trained on a large scale dataset\nof audio events, and then we calculate the similarity matrix derived from the\npairwise similarity of these descriptors. The similarity matrix is subsequently\nfed to a CNN network that captures the temporal structures existing within its\ncontent. We train our network following a triplet generation process and\noptimizing the triplet loss function. To evaluate the effectiveness of the\nproposed approach, we have manually annotated two publicly available video\ndatasets based on the audio duplicity between their videos. The proposed\napproach achieves very competitive results compared to three state-of-the-art\nmethods. Also, unlike the competing methods, it is very robust to the retrieval\nof audio duplicates generated with speed transformations.</p>\n", "tags": ["Distance-Metric-Learning","Video-Retrieval","Datasets"] },
{"key": "bai2018learning", "year": "2020", "citations": "64", "title":"Learning-based Efficient Graph Similarity Computation via Multi-Scale Convolutional Set Matching", "abstract": "<p>Graph similarity computation is one of the core operations in many\ngraph-based applications, such as graph similarity search, graph database\nanalysis, graph clustering, etc. Since computing the exact distance/similarity\nbetween two graphs is typically NP-hard, a series of approximate methods have\nbeen proposed with a trade-off between accuracy and speed. Recently, several\ndata-driven approaches based on neural networks have been proposed, most of\nwhich model the graph-graph similarity as the inner product of their\ngraph-level representations, with different techniques proposed for generating\none embedding per graph. However, using one fixed-dimensional embedding per\ngraph may fail to fully capture graphs in varying sizes and link structures, a\nlimitation that is especially problematic for the task of graph similarity\ncomputation, where the goal is to find the fine-grained difference between two\ngraphs. In this paper, we address the problem of graph similarity computation\nfrom another perspective, by directly matching two sets of node embeddings\nwithout the need to use fixed-dimensional vectors to represent whole graphs for\ntheir similarity computation. The model, GraphSim, achieves the\nstate-of-the-art performance on four real-world graph datasets under six out of\neight settings (here we count a specific dataset and metric combination as one\nsetting), compared to existing popular methods for approximate Graph Edit\nDistance (GED) and Maximum Common Subgraph (MCS) computation.</p>\n", "tags": ["Similarity-Search","Graph-Based-ANN","Datasets","AAAI","Evaluation"] },
{"key": "bai2020targeted", "year": "2020", "citations": "68", "title":"Targeted Attack for Deep Hashing based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the ℓ∞ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>\n", "tags": ["Video-Retrieval","Image-Retrieval","Scalability","Neural-Hashing","Hashing-Methods","Evaluation","Robustness"] },
{"key": "bai2025targeted", "year": "2020", "citations": "68", "title":"Targeted Attack for Deep Hashing based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the ℓ∞ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>\n", "tags": ["Video-Retrieval","Image-Retrieval","Scalability","Neural-Hashing","Hashing-Methods","Evaluation","Robustness"] },
{"key": "baldrati2023composed", "year": "2023", "citations": "13", "title":"Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features", "abstract": "<p>Given a query composed of a reference image and a relative caption, the\nComposed Image Retrieval goal is to retrieve images visually similar to the\nreference one that integrates the modifications expressed by the caption. Given\nthat recent research has demonstrated the efficacy of large-scale vision and\nlanguage pre-trained (VLP) models in various tasks, we rely on features from\nthe OpenAI CLIP model to tackle the considered task. We initially perform a\ntask-oriented fine-tuning of both CLIP encoders using the element-wise sum of\nvisual and textual features. Then, in the second stage, we train a Combiner\nnetwork that learns to combine the image-text features integrating the bimodal\ninformation and providing combined features used to perform the retrieval. We\nuse contrastive learning in both stages of training. Starting from the bare\nCLIP features as a baseline, experimental results show that the task-oriented\nfine-tuning and the carefully crafted Combiner network are highly effective and\noutperform more complex state-of-the-art approaches on FashionIQ and CIRR, two\npopular and challenging datasets for composed image retrieval. Code and\npre-trained models are available at https://github.com/ABaldrati/CLIP4Cir</p>\n", "tags": ["Self-Supervised","Scalability","Datasets","Image-Retrieval"] },
{"key": "baranchuk2018revisiting", "year": "2018", "citations": "67", "title":"Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors", "abstract": "<p>This work addresses the problem of billion-scale nearest neighbor search. The\nstate-of-the-art retrieval systems for billion-scale databases are currently\nbased on the inverted multi-index, the recently proposed generalization of the\ninverted index structure. The multi-index provides a very fine-grained\npartition of the feature space that allows extracting concise and accurate\nshort-lists of candidates for the search queries. In this paper, we argue that\nthe potential of the simple inverted index was not fully exploited in previous\nworks and advocate its usage both for the highly-entangled deep descriptors and\nrelatively disentangled SIFT descriptors. We introduce a new retrieval system\nthat is based on the inverted index and outperforms the multi-index by a large\nmargin for the same memory consumption and construction complexity. For\nexample, our system achieves the state-of-the-art recall rates several times\nfaster on the dataset of one billion deep descriptors compared to the efficient\nimplementation of the inverted multi-index from the FAISS library.</p>\n", "tags": ["Vector-Indexing","Tools-&-Libraries","Datasets","Scalability","Large-Scale-Search","Evaluation"] },
{"key": "baranchuk2019learning", "year": "2019", "citations": "8", "title":"Learning to Route in Similarity Graphs", "abstract": "<p>Recently similarity graphs became the leading paradigm for efficient nearest\nneighbor search, outperforming traditional tree-based and LSH-based methods.\nSimilarity graphs perform the search via greedy routing: a query traverses the\ngraph and in each vertex moves to the adjacent vertex that is the closest to\nthis query. In practice, similarity graphs are often susceptible to local\nminima, when queries do not reach its nearest neighbors, getting stuck in\nsuboptimal vertices. In this paper we propose to learn the routing function\nthat overcomes local minima via incorporating information about the graph\nglobal structure. In particular, we augment the vertices of a given graph with\nadditional representations that are learned to provide the optimal routing from\nthe start vertex to the query nearest neighbor. By thorough experiments, we\ndemonstrate that the proposed learnable routing successfully diminishes the\nlocal minima problem and significantly improves the overall search performance.</p>\n", "tags": ["Tree-Based-ANN","Evaluation","Locality-Sensitive-Hashing"] },
{"key": "barz2018hierarchy", "year": "2019", "citations": "52", "title":"Hierarchy-based Image Embeddings for Semantic Image Retrieval", "abstract": "<p>Deep neural networks trained for classification have been found to learn\npowerful image representations, which are also often used for other tasks such\nas comparing images w.r.t. their visual similarity. However, visual similarity\ndoes not imply semantic similarity. In order to learn semantically\ndiscriminative features, we propose to map images onto class embeddings whose\npair-wise dot products correspond to a measure of semantic similarity between\nclasses. Such an embedding does not only improve image retrieval results, but\ncould also facilitate integrating semantics for other tasks, e.g., novelty\ndetection or few-shot learning. We introduce a deterministic algorithm for\ncomputing the class centroids directly based on prior world-knowledge encoded\nin a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,\nand ImageNet show that our learned semantic image embeddings improve the\nsemantic consistency of image retrieval results by a large margin.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Evaluation","Image-Retrieval"] },
{"key": "beck2019distributed", "year": "2019", "citations": "29", "title":"A Distributed and Approximated Nearest Neighbors Algorithm for an Efficient Large Scale Mean Shift Clustering", "abstract": "<p>In this paper we target the class of modal clustering methods where clusters\nare defined in terms of the local modes of the probability density function\nwhich generates the data. The most well-known modal clustering method is the\nk-means clustering. Mean Shift clustering is a generalization of the k-means\nclustering which computes arbitrarily shaped clusters as defined as the basins\nof attraction to the local modes created by the density gradient ascent paths.\nDespite its potential, the Mean Shift approach is a computationally expensive\nmethod for unsupervised learning. Thus, we introduce two contributions aiming\nto provide clustering algorithms with a linear time complexity, as opposed to\nthe quadratic time complexity for the exact Mean Shift clustering. Firstly we\npropose a scalable procedure to approximate the density gradient ascent.\nSecond, our proposed scalable cluster labeling technique is presented. Both\npropositions are based on Locality Sensitive Hashing (LSH) to approximate\nnearest neighbors. These two techniques may be used for moderate sized\ndatasets. Furthermore, we show that using our proposed approximations of the\ndensity gradient ascent as a pre-processing step in other clustering methods\ncan also improve dedicated classification metrics. For the latter, a\ndistributed implementation, written for the Spark/Scala ecosystem is proposed.\nFor all these considered clustering methods, we present experimental results\nillustrating their labeling accuracy and their potential to solve concrete\nproblems.</p>\n", "tags": ["Hashing-Methods","Unsupervised","Datasets","Locality-Sensitive-Hashing"] },
{"key": "bencohen2021semantic", "year": "2021", "citations": "29", "title":"Semantic Diversity Learning for Zero-Shot Multi-label Classification", "abstract": "<p>Training a neural network model for recognizing multiple labels associated\nwith an image, including identifying unseen labels, is challenging, especially\nfor images that portray numerous semantically diverse labels. As challenging as\nthis task is, it is an essential task to tackle since it represents many\nreal-world cases, such as image retrieval of natural images. We argue that\nusing a single embedding vector to represent an image, as commonly practiced,\nis not sufficient to rank both relevant seen and unseen labels accurately. This\nstudy introduces an end-to-end model training for multi-label zero-shot\nlearning that supports semantic diversity of the images and labels. We propose\nto use an embedding matrix having principal embedding vectors trained using a\ntailored loss function. In addition, during training, we suggest up-weighting\nin the loss function image samples presenting higher semantic diversity to\nencourage the diversity of the embedding matrix. Extensive experiments show\nthat our proposed method improves the zero-shot model’s quality in tag-based\nimage retrieval achieving SoTA results on several common datasets (NUS-Wide,\nCOCO, Open Images).</p>\n", "tags": ["ICCV","Datasets","Few-Shot-&-Zero-Shot","Image-Retrieval"] },
{"key": "bhunia2018texture", "year": "2019", "citations": "17", "title":"Texture Synthesis Guided Deep Hashing for Texture Image Retrieval", "abstract": "<p>With the large-scale explosion of images and videos over the internet,\nefficient hashing methods have been developed to facilitate memory and time\nefficient retrieval of similar images. However, none of the existing works uses\nhashing to address texture image retrieval mostly because of the lack of\nsufficiently large texture image databases. Our work addresses this problem by\ndeveloping a novel deep learning architecture that generates binary hash codes\nfor input texture images. For this, we first pre-train a Texture Synthesis\nNetwork (TSN) which takes a texture patch as input and outputs an enlarged view\nof the texture by injecting newer texture content. Thus it signifies that the\nTSN encodes the learnt texture specific information in its intermediate layers.\nIn the next stage, a second network gathers the multi-scale feature\nrepresentations from the TSN’s intermediate layers using channel-wise\nattention, combines them in a progressive manner to a dense continuous\nrepresentation which is finally converted into a binary hash code with the help\nof individual and pairwise label information. The new enlarged texture patches\nalso help in data augmentation to alleviate the problem of insufficient texture\ndata and are used to train the second stage of the network. Experiments on\nthree public texture image retrieval datasets indicate the superiority of our\ntexture synthesis guided hashing approach over current state-of-the-art\nmethods.</p>\n", "tags": ["Similarity-Search","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Scalability"] },
{"key": "bhunia2022adaptive", "year": "2022", "citations": "20", "title":"Adaptive Fine-Grained Sketch-Based Image Retrieval", "abstract": "<p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has\nshifted towards generalising a model to new categories without any training\ndata from them. In real-world applications, however, a trained FG-SBIR model is\noften applied to both new categories and different human sketchers, i.e.,\ndifferent drawing styles. Although this complicates the generalisation problem,\nfortunately, a handful of examples are typically available, enabling the model\nto adapt to the new category/style. In this paper, we offer a novel perspective\n– instead of asking for a model that generalises, we advocate for one that\nquickly adapts, with just very few samples during testing (in a few-shot\nmanner). To solve this new problem, we introduce a novel model-agnostic\nmeta-learning (MAML) based framework with several key modifications: (1) As a\nretrieval task with a margin-based contrastive loss, we simplify the MAML\ntraining in the inner loop to make it more stable and tractable. (2) The margin\nin our contrastive loss is also meta-learned with the rest of the model. (3)\nThree additional regularisation losses are introduced in the outer loop, to\nmake the meta-learned FG-SBIR model more effective for category/style\nadaptation. Extensive experiments on public datasets suggest a large gain over\ngeneralisation and zero-shot based approaches, and a few strong few-shot\nbaselines.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Tools-&-Libraries","Image-Retrieval","Distance-Metric-Learning","Datasets"] },
{"key": "bianchi2021contrastive", "year": "2021", "citations": "19", "title":"Contrastive Language-Image Pre-training for the Italian Language", "abstract": "<p>CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal\nmodel that jointly learns representations of images and texts. The model is\ntrained on a massive amount of English data and shows impressive performance on\nzero-shot classification tasks. Training the same model on a different language\nis not trivial, since data in other languages might be not enough and the model\nneeds high-quality translations of the texts to guarantee a good performance.\nIn this paper, we present the first CLIP model for the Italian Language\n(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show\nthat CLIP-Italian outperforms the multilingual CLIP model on the tasks of image\nretrieval and zero-shot classification.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Evaluation"] },
{"key": "bianchi2021query2prod2vec", "year": "2021", "citations": "11", "title":"Query2Prod2Vec Grounded Word Embeddings for eCommerce", "abstract": "<p>We present Query2Prod2Vec, a model that grounds lexical representations for\nproduct search in product embeddings: in our model, meaning is a mapping\nbetween words and a latent space of products in a digital shop. We leverage\nshopping sessions to learn the underlying space and use merchandising\nannotations to build lexical analogies for evaluation: our experiments show\nthat our model is more accurate than known techniques from the NLP and IR\nliterature. Finally, we stress the importance of data efficiency for product\nsearch outside of retail giants, and highlight how Query2Prod2Vec fits with\npractical constraints faced by most practitioners.</p>\n", "tags": ["Evaluation","Efficiency"] },
{"key": "bin2023unifying", "year": "2023", "citations": "17", "title":"Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval", "abstract": "<p>Most existing cross-modal retrieval methods employ two-stream encoders with\ndifferent architectures for images and texts, \\textit{e.g.}, CNN for images and\nRNN/Transformer for texts. Such discrepancy in architectures may induce\ndifferent semantic distribution spaces and limit the interactions between\nimages and texts, and further result in inferior alignment between images and\ntexts. To fill this research gap, inspired by recent advances of Transformers\nin vision tasks, we propose to unify the encoder architectures with\nTransformers for both modalities. Specifically, we design a cross-modal\nretrieval framework purely based on two-stream Transformers, dubbed\n\\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image\nTransformer, a text Transformer, and a hierarchical alignment module. With such\nidentical architectures, the encoders could produce representations with more\nsimilar characteristics for images and texts, and make the interactions and\nalignments between them much easier. Besides, to leverage the rich semantics,\nwe devise a hierarchical alignment scheme to explore multi-level\ncorrespondences of different layers between images and texts. To evaluate the\neffectiveness of the proposed HAT, we conduct extensive experiments on two\nbenchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that\nHAT outperforms SOTA baselines by a large margin. Specifically, on two key\ntasks, \\textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves\n7.6% and 16.7% relative score improvement of Recall@1 on MSCOCO, and 4.4%\nand 11.6% on Flickr30k respectively. The code is available at\nhttps://github.com/LuminosityX/HAT.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Datasets","Evaluation","Multimodal-Retrieval"] },
{"key": "biten2021is", "year": "2022", "citations": "17", "title":"Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching", "abstract": "<p>The task of image-text matching aims to map representations from different\nmodalities into a common joint visual-textual embedding. However, the most\nwidely used datasets for this task, MSCOCO and Flickr30K, are actually image\ncaptioning datasets that offer a very limited set of relationships between\nimages and sentences in their ground-truth annotations. This limited ground\ntruth information forces us to use evaluation metrics based on binary\nrelevance: given a sentence query we consider only one image as relevant.\nHowever, many other relevant images or captions may be present in the dataset.\nIn this work, we propose two metrics that evaluate the degree of semantic\nrelevance of retrieved items, independently of their annotated binary\nrelevance. Additionally, we incorporate a novel strategy that uses an image\ncaptioning metric, CIDEr, to define a Semantic Adaptive Margin (SAM) to be\noptimized in a standard triplet loss. By incorporating our formulation to\nexisting models, a <em>large</em> improvement is obtained in scenarios where\navailable training data is limited. We also demonstrate that the performance on\nthe annotated image-caption pairs is maintained while improving on other\nnon-annotated relevant items when employing the full training set. Code with\nour metrics and adaptive margin formulation will be made public.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "bogolin2021cross", "year": "2022", "citations": "58", "title":"Cross Modal Retrieval with Querybank Normalisation", "abstract": "<p>Profiting from large-scale training datasets, advances in neural architecture\ndesign and efficient inference, joint embeddings have become the dominant\napproach for tackling cross-modal retrieval. In this work we first show that,\ndespite their effectiveness, state-of-the-art joint embeddings suffer\nsignificantly from the longstanding “hubness problem” in which a small number\nof gallery embeddings form the nearest neighbours of many queries. Drawing\ninspiration from the NLP literature, we formulate a simple but effective\nframework called Querybank Normalisation (QB-Norm) that re-normalises query\nsimilarities to account for hubs in the embedding space. QB-Norm improves\nretrieval performance without requiring retraining. Differently from prior\nwork, we show that QB-Norm works effectively without concurrent access to any\ntest set queries. Within the QB-Norm framework, we also propose a novel\nsimilarity normalisation method, the Dynamic Inverted Softmax, that is\nsignificantly more robust than existing approaches. We showcase QB-Norm across\na range of cross modal retrieval models and benchmarks where it consistently\nenhances strong baselines beyond the state of the art. Code is available at\nhttps://vladbogo.github.io/QB-Norm/.</p>\n", "tags": ["Tools-&-Libraries","Datasets","CVPR","Scalability","Evaluation","Multimodal-Retrieval"] },
{"key": "bouma2019individual", "year": "2018", "citations": "36", "title":"Individual common dolphin identification via metric embedding learning", "abstract": "<p>Photo-identification (photo-id) of dolphin individuals is a commonly used\ntechnique in ecological sciences to monitor state and health of individuals, as\nwell as to study the social structure and distribution of a population.\nTraditional photo-id involves a laborious manual process of matching each\ndolphin fin photograph captured in the field to a catalogue of known\nindividuals.\n  We examine this problem in the context of open-set recognition and utilise a\ntriplet loss function to learn a compact representation of fin images in a\nEuclidean embedding, where the Euclidean distance metric represents fin\nsimilarity. We show that this compact representation can be successfully learnt\nfrom a fairly small (in deep learning context) training set and still\ngeneralise well to out-of-sample identities (completely new dolphin\nindividuals), with top-1 and top-5 test set (37 individuals) accuracy of\n\\(90.5\\pm2\\) and \\(93.6\\pm1\\) percent. In the presence of 1200 distractors, top-1\naccuracy dropped by \\(12%\\); however, top-5 accuracy saw only a \\(2.8%\\) drop</p>\n", "tags": ["Distance-Metric-Learning"] },
{"key": "bruch2023approximate", "year": "2023", "citations": "6", "title":"An Approximate Algorithm for Maximum Inner Product Search over Streaming Sparse Vectors", "abstract": "<p>Maximum Inner Product Search or top-k retrieval on sparse vectors is\nwell-understood in information retrieval, with a number of mature algorithms\nthat solve it exactly. However, all existing algorithms are tailored to text\nand frequency-based similarity measures. To achieve optimal memory footprint\nand query latency, they rely on the near stationarity of documents and on laws\ngoverning natural languages. We consider, instead, a setup in which collections\nare streaming – necessitating dynamic indexing – and where indexing and\nretrieval must work with arbitrarily distributed real-valued vectors. As we\nshow, existing algorithms are no longer competitive in this setup, even against\nnaive solutions. We investigate this gap and present a novel approximate\nsolution, called Sinnamon, that can efficiently retrieve the top-k results for\nsparse real valued vectors drawn from arbitrary distributions. Notably,\nSinnamon offers levers to trade-off memory consumption, latency, and accuracy,\nmaking the algorithm suitable for constrained applications and systems. We give\ntheoretical results on the error introduced by the approximate nature of the\nalgorithm, and present an empirical evaluation of its performance on two\nhardware platforms and synthetic and real-valued datasets. We conclude by\nlaying out concrete directions for future research on this general top-k\nretrieval problem over sparse vectors.</p>\n", "tags": ["Memory-Efficiency","Datasets","Evaluation"] },
{"key": "bruch2024efficient", "year": "2024", "citations": "14", "title":"Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations", "abstract": "<p>Learned sparse representations form an attractive class of contextual\nembeddings for text retrieval. That is so because they are effective models of\nrelevance and are interpretable by design. Despite their apparent compatibility\nwith inverted indexes, however, retrieval over sparse embeddings remains\nchallenging. That is due to the distributional differences between learned\nembeddings and term frequency-based lexical models of relevance such as BM25.\nRecognizing this challenge, a great deal of research has gone into, among other\nthings, designing retrieval algorithms tailored to the properties of learned\nsparse representations, including approximate retrieval systems. In fact, this\ntask featured prominently in the latest BigANN Challenge at NeurIPS 2023, where\napproximate algorithms were evaluated on a large benchmark dataset by\nthroughput and recall. In this work, we propose a novel organization of the\ninverted index that enables fast yet effective approximate retrieval over\nlearned sparse embeddings. Our approach organizes inverted lists into\ngeometrically-cohesive blocks, each equipped with a summary vector. During\nquery processing, we quickly determine if a block must be evaluated using the\nsummaries. As we show experimentally, single-threaded query processing using\nour method, Seismic, reaches sub-millisecond per-query latency on various\nsparse embeddings of the MS MARCO dataset while maintaining high recall. Our\nresults indicate that Seismic is one to two orders of magnitude faster than\nstate-of-the-art inverted index-based solutions and further outperforms the\nwinning (graph-based) submissions to the BigANN Challenge by a significant\nmargin.</p>\n", "tags": ["Text-Retrieval","Graph-Based-ANN","Datasets","SIGIR","Evaluation"] },
{"key": "cai2016revisit", "year": "2019", "citations": "30", "title":"A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many\nareas of machine learning and data mining. During the past decade, numerous\nhashing algorithms are proposed to solve this problem. Every proposed algorithm\nclaims outperform other state-of-the-art hashing methods. However, the\nevaluation of these hashing papers was not thorough enough, and those claims\nshould be re-examined. The ultimate goal of an ANNS method is returning the\nmost accurate answers (nearest neighbors) in the shortest time. If implemented\ncorrectly, almost all the hashing methods will have their performance improved\nas the code length increases. However, many existing hashing papers only report\nthe performance with the code length shorter than 128. In this paper, we\ncarefully revisit the problem of search with a hash index, and analyze the pros\nand cons of two popular hash index search procedures. Then we proposed a very\nsimple but effective two level index structures and make a thorough comparison\nof eleven popular hashing algorithms. Surprisingly, the random-projection-based\nLocality Sensitive Hashing (LSH) is the best performed algorithm, which is in\ncontradiction to the claims in all the other ten hashing papers. Despite the\nextreme simplicity of random-projection-based LSH, our results show that the\ncapability of this algorithm has been far underestimated. For the sake of\nreproducibility, all the codes used in the paper are released on GitHub, which\ncan be used as a testing platform for a fair comparison between various hashing\nalgorithms.</p>\n", "tags": ["Hashing-Methods","Vector-Indexing","Evaluation","Locality-Sensitive-Hashing"] },
{"key": "cai2017revisit", "year": "2017", "citations": "13", "title":"A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval", "abstract": "<p>There is a growing trend in studying deep hashing methods for content-based\nimage retrieval (CBIR), where hash functions and binary codes are learnt using\ndeep convolutional neural networks and then the binary codes can be used to do\napproximate nearest neighbor (ANN) search. All the existing deep hashing papers\nreport their methods’ superior performance over the traditional hashing methods\naccording to their experimental results. However, there are serious flaws in\nthe evaluations of existing deep hashing papers: (1) The datasets they used are\ntoo small and simple to simulate the real CBIR situation. (2) They did not\ncorrectly include the search time in their evaluation criteria, while the\nsearch time is crucial in real CBIR systems. (3) The performance of some\nunsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses\nmultiple hash tables, which is an important factor should be considered in the\nevaluation while most of the deep hashing papers failed to do so.\n  We re-evaluate several state-of-the-art deep hashing methods with a carefully\ndesigned experimental setting. Empirical results reveal that the performance of\nthese deep hashing methods are inferior to multi-table IsoH, a very simple\nunsupervised hashing method. Thus, the conclusions in all the deep hashing\npapers should be carefully re-examined.</p>\n", "tags": ["Supervised","Locality-Sensitive-Hashing","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Unsupervised","Scalability","Evaluation"] },
{"key": "cakaloglu2018text", "year": "2018", "citations": "6", "title":"Text Embeddings for Retrieval From a Large Knowledge Base", "abstract": "<p>Text embedding representing natural language documents in a semantic vector\nspace can be used for document retrieval using nearest neighbor lookup. In\norder to study the feasibility of neural models specialized for retrieval in a\nsemantically meaningful way, we suggest the use of the Stanford Question\nAnswering Dataset (SQuAD) in an open-domain question answering context, where\nthe first task is to find paragraphs useful for answering a given question.\nFirst, we compare the quality of various text-embedding methods on the\nperformance of retrieval and give an extensive empirical comparison on the\nperformance of various non-augmented base embedding with, and without IDF\nweighting. Our main results are that by training deep residual neural models,\nspecifically for retrieval purposes, can yield significant gains when it is\nused to augment existing embeddings. We also establish that deeper models are\nsuperior to this task. The best base baseline embeddings augmented by our\nlearned neural approach improves the top-1 paragraph recall of the system by\n14%.</p>\n", "tags": ["Datasets","Text-Retrieval","Evaluation"] },
{"key": "cakir2015adaptive", "year": "2015", "citations": "88", "title":"Adaptive Hashing for Fast Similarity Search", "abstract": "<p>With the staggering growth in image and video datasets,\nalgorithms that provide fast similarity search and compact\nstorage are crucial. Hashing methods that map the\ndata into Hamming space have shown promise; however,\nmany of these methods employ a batch-learning strategy\nin which the computational cost and memory requirements\nmay become intractable and infeasible with larger and\nlarger datasets. To overcome these challenges, we propose\nan online learning algorithm based on stochastic gradient\ndescent in which the hash functions are updated iteratively\nwith streaming data. In experiments with three image retrieval\nbenchmarks, our online algorithm attains retrieval\naccuracy that is comparable to competing state-of-the-art\nbatch-learning solutions, while our formulation is orders\nof magnitude faster and being online it is adaptable to the\nvariations of the data. Moreover, our formulation yields improved\nretrieval performance over a recently reported online\nhashing technique, Online Kernel Hashing.</p>\n", "tags": ["Image-Retrieval","ICCV","Datasets","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "cakir2018hashing", "year": "2019", "citations": "91", "title":"Hashing with Mutual Information", "abstract": "<p>Binary vector embeddings enable fast nearest neighbor retrieval in large\ndatabases of high-dimensional objects, and play an important role in many\npractical applications, such as image and video retrieval. We study the problem\nof learning binary vector embeddings under a supervised setting, also known as\nhashing. We propose a novel supervised hashing method based on optimizing an\ninformation-theoretic quantity: mutual information. We show that optimizing\nmutual information can reduce ambiguity in the induced neighborhood structure\nin the learned Hamming space, which is essential in obtaining high retrieval\nperformance. To this end, we optimize mutual information in deep neural\nnetworks with minibatch stochastic gradient descent, with a formulation that\nmaximally and efficiently utilizes available supervision. Experiments on four\nimage retrieval benchmarks, including ImageNet, confirm the effectiveness of\nour method in learning high-quality binary embeddings for nearest neighbor\nretrieval.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Neural-Hashing","Video-Retrieval","Evaluation"] },
{"key": "cakir2019hashing", "year": "2019", "citations": "91", "title":"Hashing with Binary Matrix Pursuit", "abstract": "<p>We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that, under mild assumptions, a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly, we show that with high-capacity hash functions such as CNNs, binary code inference can be greatly simplified for many standard neighborhood definitions, yielding smaller optimization problems and more robust codes. Incorporating our findings, we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval","Compact-Codes"] },
{"key": "cakir2025adaptive", "year": "2015", "citations": "88", "title":"Adaptive Hashing for Fast Similarity Search", "abstract": "<p>With the staggering growth in image and video datasets,\nalgorithms that provide fast similarity search and compact\nstorage are crucial. Hashing methods that map the\ndata into Hamming space have shown promise; however,\nmany of these methods employ a batch-learning strategy\nin which the computational cost and memory requirements\nmay become intractable and infeasible with larger and\nlarger datasets. To overcome these challenges, we propose\nan online learning algorithm based on stochastic gradient\ndescent in which the hash functions are updated iteratively\nwith streaming data. In experiments with three image retrieval\nbenchmarks, our online algorithm attains retrieval\naccuracy that is comparable to competing state-of-the-art\nbatch-learning solutions, while our formulation is orders\nof magnitude faster and being online it is adaptable to the\nvariations of the data. Moreover, our formulation yields improved\nretrieval performance over a recently reported online\nhashing technique, Online Kernel Hashing.</p>\n", "tags": ["Image-Retrieval","ICCV","Datasets","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "cakir2025hashing", "year": "2019", "citations": "91", "title":"Hashing with Mutual Information", "abstract": "<p>Binary vector embeddings enable fast nearest neighbor retrieval in large databases of high-dimensional objects, and play an important role in many practical applications, such as image and video retrieval. We study the problem of learning binary vector embeddings under a supervised setting, also known as hashing. We propose a novel supervised hashing method based on optimizing an information-theoretic quantity: mutual information. We show that optimizing mutual information can reduce ambiguity in the induced neighborhood structure in the learned Hamming space, which is essential in obtaining high retrieval performance. To this end, we optimize mutual information in deep neural networks with minibatch stochastic gradient descent, with a formulation that maximally and efficiently utilizes available supervision. Experiments on four image retrieval benchmarks, including ImageNet, confirm the effectiveness of our method in learning high-quality binary embeddings for nearest neighbor retrieval.</p>\n", "tags": ["Video-Retrieval","Image-Retrieval","Neural-Hashing","Hashing-Methods","Evaluation","Supervised"] },
{"key": "calixto2017multilingual", "year": "2017", "citations": "15", "title":"Multilingual Multi-modal Embeddings for Natural Language Processing", "abstract": "<p>We propose a novel discriminative model that learns embeddings from\nmultilingual and multi-modal data, meaning that our model can take advantage of\nimages and descriptions in multiple languages to improve embedding quality. To\nthat end, we introduce a modification of a pairwise contrastive estimation\noptimisation function as our training objective. We evaluate our embeddings on\nan image-sentence ranking (ISR), a semantic textual similarity (STS), and a\nneural machine translation (NMT) task. We find that the additional multilingual\nsignals lead to improvements on both the ISR and STS tasks, and the\ndiscriminative cost can also be used in re-ranking \\(n\\)-best lists produced by\nNMT models, yielding strong improvements.</p>\n", "tags": ["Hybrid-ANN-Methods","Re-Ranking"] },
{"key": "camara2019spatio", "year": "2019", "citations": "25", "title":"Spatio-Semantic ConvNet-Based Visual Place Recognition", "abstract": "<p>We present a Visual Place Recognition system that follows the two-stage\nformat common to image retrieval pipelines. The system encodes images of places\nby employing the activations of different layers of a pre-trained,\noff-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. In the\nfirst stage of our method and given a query image of a place, a number of top\ncandidate images is retrieved from a previously stored database of places. In\nthe second stage, we propose an exhaustive comparison of the query image\nagainst these candidates by encoding semantic and spatial information in the\nform of CNN features. Results from our approach outperform by a large margin\nstate-of-the-art visual place recognition methods on five of the most commonly\nused benchmark datasets. The performance gain is especially remarkable on the\nmost challenging datasets, with more than a twofold recognition improvement\nwith respect to the latest published work.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "cao2016correlation", "year": "2016", "citations": "98", "title":"Correlation Autoencoder Hashing for Supervised Cross-Modal Search", "abstract": "<p>Due to its storage and query efficiency, hashing has been widely\napplied to approximate nearest neighbor search from large-scale\ndatasets. While there is increasing interest in cross-modal hashing\nwhich facilitates cross-media retrieval by embedding data from different modalities into a common Hamming space, how to distill the\ncross-modal correlation structure effectively remains a challenging\nproblem. In this paper, we propose a novel supervised cross-modal\nhashing method, Correlation Autoencoder Hashing (CAH), to learn\ndiscriminative and compact binary codes based on deep autoencoders. Specifically, CAH jointly maximizes the feature correlation\nrevealed by bimodal data and the semantic correlation conveyed in\nsimilarity labels, while embeds them into hash codes by nonlinear\ndeep autoencoders. Extensive experiments clearly show the superior effectiveness and efficiency of CAH against the state-of-the-art\nhashing methods on standard cross-modal retrieval benchmarks.</p>\n", "tags": ["Scalability","Efficiency","Datasets","Multimodal-Retrieval","Compact-Codes","Hashing-Methods","Supervised"] },
{"key": "cao2016deep", "year": "2016", "citations": "264", "title":"Deep Cauchy Hashing for Hamming Space Retrieval", "abstract": "<p>Due to its computation efficiency and retrieval quality,\nhashing has been widely applied to approximate nearest\nneighbor search for large-scale image retrieval, while deep\nhashing further improves the retrieval quality by end-toend representation learning and hash coding. With compact\nhash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a\ngiven Hamming radius to each query, by hash table lookups\ninstead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small\nHamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming\nspace retrieval.  This work presents Deep Cauchy Hashing\n(DCH), a novel deep hashing model that generates compact\nand concentrated binary hash codes to enable efficient and\neffective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs\nwith Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate\nthat DCH can generate highly concentrated hash codes and\nyield state-of-the-art Hamming space retrieval performance\non three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","Datasets","Neural-Hashing","KDD","Hashing-Methods","Evaluation"] },
{"key": "cao2016transitive", "year": "2017", "citations": "25", "title":"Transitive Hashing Network for Heterogeneous Multimedia Retrieval", "abstract": "<p>Hashing has been widely applied to large-scale multimedia retrieval due to\nthe storage and retrieval efficiency. Cross-modal hashing enables efficient\nretrieval from database of one modality in response to a query of another\nmodality. Existing work on cross-modal hashing assumes heterogeneous\nrelationship across modalities for hash function learning. In this paper, we\nrelax the strong assumption by only requiring such heterogeneous relationship\nin an auxiliary dataset different from the query/database domain. We craft a\nhybrid deep architecture to simultaneously learn the cross-modal correlation\nfrom the auxiliary dataset, and align the dataset distributions between the\nauxiliary dataset and the query/database domain, which generates transitive\nhash codes for heterogeneous multimedia retrieval. Extensive experiments\nexhibit that the proposed approach yields state of the art multimedia retrieval\nperformance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.</p>\n", "tags": ["Hashing-Methods","Datasets","AAAI","Scalability","Evaluation","Efficiency"] },
{"key": "cao2017collective", "year": "2017", "citations": "97", "title":"Collective Deep Quantization for Efficient Cross-Modal Retrieval", "abstract": "<p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across\ncontent modalities, e.g., using an image to retrieve for\ntexts. This paper presents a compact coding solution for\nefficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior\nperformance over the hashing solutions in single-modal\nsimilarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to\nintroduce quantization in end-to-end deep architecture\nfor cross-modal retrieval. The major contribution lies in\njointly learning deep representations and the quantizers\nfor both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our\napproach simultaneously learns the common quantizer\ncodebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ\nenables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art\ncross-modal retrieval results on standard benchmarks.</p>\n", "tags": ["Multimodal-Retrieval","Similarity-Search","Hashing-Methods","AAAI","Evaluation","Quantization"] },
{"key": "cao2017transfer", "year": "2018", "citations": "15", "title":"Transfer Adversarial Hashing for Hamming Space Retrieval", "abstract": "<p>Hashing is widely applied to large-scale image retrieval due to the storage\nand retrieval efficiency. Existing work on deep hashing assumes that the\ndatabase in the target domain is identically distributed with the training set\nin the source domain. This paper relaxes this assumption to a transfer\nretrieval setting, which allows the database and the training set to come from\ndifferent but relevant domains. However, the transfer retrieval setting will\nintroduce two technical difficulties: first, the hash model trained on the\nsource domain cannot work well on the target domain due to the large\ndistribution gap; second, the domain gap makes it difficult to concentrate the\ndatabase points to be within a small Hamming ball. As a consequence, transfer\nretrieval performance within Hamming Radius 2 degrades significantly in\nexisting hashing methods. This paper presents Transfer Adversarial Hashing\n(TAH), a new hybrid deep architecture that incorporates a pairwise\n\\(t\\)-distribution cross-entropy loss to learn concentrated hash codes and an\nadversarial network to align the data distributions between the source and\ntarget domains. TAH can generate compact transfer hash codes for efficient\nimage retrieval on both source and target domains. Comprehensive experiments\nvalidate that TAH yields state of the art Hamming space retrieval performance\non standard datasets.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Efficiency","AAAI","Scalability","Evaluation","Robustness"] },
{"key": "cao2018deep", "year": "2018", "citations": "33", "title":"Deep Priority Hashing", "abstract": "<p>Deep hashing enables image retrieval by end-to-end learning of deep\nrepresentations and hash codes from training data with pairwise similarity\ninformation. Subject to the distribution skewness underlying the similarity\ninformation, most existing deep hashing methods may underperform for imbalanced\ndata due to misspecified loss functions. This paper presents Deep Priority\nHashing (DPH), an end-to-end architecture that generates compact and balanced\nhash codes in a Bayesian learning framework. The main idea is to reshape the\nstandard cross-entropy loss for similarity-preserving learning such that it\ndown-weighs the loss associated to highly-confident pairs. This idea leads to a\nnovel priority cross-entropy loss, which prioritizes the training on uncertain\npairs over confident pairs. Also, we propose another priority quantization\nloss, which prioritizes hard-to-quantize examples for generation of nearly\nlossless hash codes. Extensive experiments demonstrate that DPH can generate\nhigh-quality hash codes and yield state-of-the-art image retrieval results on\nthree datasets, ImageNet, NUS-WIDE, and MS-COCO.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Quantization"] },
{"key": "cao2018end", "year": "2019", "citations": "65", "title":"End-to-End Latent Fingerprint Search", "abstract": "<p>Latent fingerprints are one of the most important and widely used sources of\nevidence in law enforcement and forensic agencies. Yet the performance of the\nstate-of-the-art latent recognition systems is far from satisfactory, and they\noften require manual markups to boost the latent search performance. Further,\nthe COTS systems are proprietary and do not output the true comparison scores\nbetween a latent and reference prints to conduct quantitative evidential\nanalysis. We present an end-to-end latent fingerprint search system, including\nautomated region of interest (ROI) cropping, latent image preprocessing,\nfeature extraction, feature comparison , and outputs a candidate list. Two\nseparate minutiae extraction models provide complementary minutiae templates.\nTo compensate for the small number of minutiae in small area and poor quality\nlatents, a virtual minutiae set is generated to construct a texture template. A\n96-dimensional descriptor is extracted for each minutia from its neighborhood.\nFor computational efficiency, the descriptor length for virtual minutiae is\nfurther reduced to 16 using product quantization. Our end-to-end system is\nevaluated on three latent databases: NIST SD27 (258 latents); MSP (1,200\nlatents), WVU (449 latents) and N2N (10,000 latents) against a background set\nof 100K rolled prints, which includes the true rolled mates of the latents with\nrank-1 retrieval rates of 65.7%, 69.4%, 65.5%, and 7.6% respectively. A\nmulti-core solution implemented on 24 cores obtains 1ms per latent to rolled\ncomparison.</p>\n", "tags": ["Quantization","Evaluation","Efficiency"] },
{"key": "cao2019enhancing", "year": "2019", "citations": "94", "title":"Enhancing Remote Sensing Image Retrieval with Triplet Deep Metric Learning Network", "abstract": "<p>With the rapid growing of remotely sensed imagery data, there is a high\ndemand for effective and efficient image retrieval tools to manage and exploit\nsuch data. In this letter, we present a novel content-based remote sensing\nimage retrieval method based on Triplet deep metric learning convolutional\nneural network (CNN). By constructing a Triplet network with metric learning\nobjective function, we extract the representative features of the images in a\nsemantic space in which images from the same class are close to each other\nwhile those from different classes are far apart. In such a semantic space,\nsimple metric measures such as Euclidean distance can be used directly to\ncompare the similarity of images and effectively retrieve images of the same\nclass. We also investigate a supervised and an unsupervised learning methods\nfor reducing the dimensionality of the learned semantic features. We present\ncomprehensive experimental results on two publicly available remote sensing\nimage retrieval datasets and show that our method significantly outperforms\nstate-of-the-art.</p>\n", "tags": ["Supervised","Image-Retrieval","Distance-Metric-Learning","Datasets","Unsupervised"] },
{"key": "cao2019unsupervised", "year": "2019", "citations": "12", "title":"Unsupervised Deep Metric Learning via Auxiliary Rotation Loss", "abstract": "<p>Deep metric learning is an important area due to its applicability to many\ndomains such as image retrieval and person re-identification. The main drawback\nof such models is the necessity for labeled data. In this work, we propose to\ngenerate pseudo-labels for deep metric learning directly from clustering\nassignment and we introduce unsupervised deep metric learning (UDML)\nregularized by a self-supervision (SS) task. In particular, we propose to\nregularize the training process by predicting image rotations. Our method\n(UDML-SS) jointly learns discriminative embeddings, unsupervised clustering\nassignments of the embeddings, as well as a self-supervised pretext task.\nUDML-SS iteratively cluster embeddings using traditional clustering algorithm\n(e.g., k-means), and sampling training pairs based on the cluster assignment\nfor metric learning, while optimizing self-supervised pretext task in a\nmulti-task fashion. The role of self-supervision is to stabilize the training\nprocess and encourages the model to learn meaningful feature representations\nthat are not distorted due to unreliable clustering assignments. The proposed\nmethod performs well on standard benchmarks for metric learning, where it\noutperforms current state-of-the-art approaches by a large margin and it also\nshows competitive performance with various metric learning loss functions.</p>\n", "tags": ["Unsupervised","Supervised","Image-Retrieval","Distance-Metric-Learning","Self-Supervised","Evaluation"] },
{"key": "cao2020unifying", "year": "2020", "citations": "293", "title":"Unifying Deep Local and Global Features for Image Search", "abstract": "<p>Image retrieval is the problem of searching an image database for items that\nare similar to a query image. To address this task, two main types of image\nrepresentations have been studied: global and local image features. In this\nwork, our key contribution is to unify global and local features into a single\ndeep model, enabling accurate retrieval with efficient feature extraction. We\nrefer to the new model as DELG, standing for DEep Local and Global features. We\nleverage lessons from recent feature learning work and propose a model that\ncombines generalized mean pooling for global features and attentive selection\nfor local features. The entire network can be learned end-to-end by carefully\nbalancing the gradient flow between two heads – requiring only image-level\nlabels. We also introduce an autoencoder-based dimensionality reduction\ntechnique for local features, which is integrated into the model, improving\ntraining efficiency and matching performance. Comprehensive experiments show\nthat our model achieves state-of-the-art image retrieval on the Revisited\nOxford and Paris datasets, and state-of-the-art single-model instance-level\nrecognition on the Google Landmarks dataset v2. Code and models are available\nat https://github.com/tensorflow/models/tree/master/research/delf .</p>\n", "tags": ["Datasets","Image-Retrieval","Evaluation","Efficiency"] },
{"key": "cao2025collective", "year": "2017", "citations": "97", "title":"Collective Deep Quantization for Efficient Cross-Modal Retrieval", "abstract": "<p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across\ncontent modalities, e.g., using an image to retrieve for\ntexts. This paper presents a compact coding solution for\nefficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior\nperformance over the hashing solutions in single-modal\nsimilarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to\nintroduce quantization in end-to-end deep architecture\nfor cross-modal retrieval. The major contribution lies in\njointly learning deep representations and the quantizers\nfor both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our\napproach simultaneously learns the common quantizer\ncodebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ\nenables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art\ncross-modal retrieval results on standard benchmarks.</p>\n", "tags": ["Multimodal-Retrieval","Similarity-Search","Hashing-Methods","AAAI","Evaluation","Quantization"] },
{"key": "cao2025correlation", "year": "2016", "citations": "98", "title":"Correlation Autoencoder Hashing for Supervised Cross-Modal Search", "abstract": "<p>Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors, and then generate binary codes by another separate quantization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes.\nThis paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perception for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.</p>\n", "tags": ["Scalability","Efficiency","Multimodal-Retrieval","Compact-Codes","Evaluation","Quantization","Hashing-Methods","Supervised"] },
{"key": "cao2025deep", "year": "2016", "citations": "264", "title":"Deep Visual-Semantic Hashing for Cross-Modal Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been\nwidely applied to approximate nearest neighbor search for\nlarge-scale multimedia retrieval. Cross-modal hashing, which\nenables efficient retrieval of images in response to text queries\nor vice versa, has received increasing attention recently. Most\nexisting work on cross-modal hashing does not capture the\nspatial dependency of images and temporal dynamics of text\nsentences for learning powerful feature representations and\ncross-modal embeddings that mitigate the heterogeneity of\ndifferent modalities. This paper presents a new Deep Visual Semantic Hashing (DVSH) model that generates compact\nhash codes of images and sentences in an end-to-end deep\nlearning architecture, which capture the intrinsic cross-modal\ncorrespondences between visual data and natural language.\nDVSH is a hybrid deep architecture that constitutes a visual semantic fusion network for learning joint embedding space\nof images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact\nbinary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based\non a novel combination of Convolutional Neural Networks\nover images, Recurrent Neural Networks over sentences, and\na structured max-margin objective that integrates all things\ntogether to enable learning of similarity-preserving and highquality hash codes. Extensive empirical evidence shows that\nour DVSH approach yields state of the art results in crossmodal retrieval experiments on image-sentences datasets,\ni.e. standard IAPR TC-12 and large-scale Microsoft COCO.</p>\n", "tags": ["Scalability","Efficiency","Datasets","Text-Retrieval","Multimodal-Retrieval","KDD","Compact-Codes","Similarity-Search","Hashing-Methods"] },
{"key": "carreiraperpinan2015hashing", "year": "2015", "citations": "142", "title":"Hashing with Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image\ndatabases is binary hashing, where each high-dimensional,\nreal-valued image is mapped onto a low-dimensional, binary\nvector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves\nbinary constraints, and most approaches approximate\nthe optimization by relaxing the constraints and then\nbinarizing the result. Here, we focus on the binary autoencoder\nmodel, which seeks to reconstruct an image from the\nbinary code produced by the hash function. We show that\nthe optimization can be simplified with the method of auxiliary\ncoordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder\nand decoder separately, and one that optimizes the code for\neach image. Image retrieval experiments show the resulting\nhash function outperforms or is competitive with state-ofthe-art\nmethods for binary hashing.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval","CVPR","Compact-Codes"] },
{"key": "carreiraperpinan2025hashing", "year": "2015", "citations": "142", "title":"Hashing with Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image\ndatabases is binary hashing, where each high-dimensional,\nreal-valued image is mapped onto a low-dimensional, binary\nvector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves\nbinary constraints, and most approaches approximate\nthe optimization by relaxing the constraints and then\nbinarizing the result. Here, we focus on the binary autoencoder\nmodel, which seeks to reconstruct an image from the\nbinary code produced by the hash function. We show that\nthe optimization can be simplified with the method of auxiliary\ncoordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder\nand decoder separately, and one that optimizes the code for\neach image. Image retrieval experiments show the resulting\nhash function outperforms or is competitive with state-ofthe-art\nmethods for binary hashing.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval","CVPR","Compact-Codes"] },
{"key": "carreiraperpiñán2016ensemble", "year": "2016", "citations": "8", "title":"An ensemble diversity approach to supervised binary hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Compact-Codes","Evaluation"] },
{"key": "castrejon2016learning", "year": "2016", "citations": "199", "title":"Learning Aligned Cross-Modal Representations from Weakly Aligned Data", "abstract": "<p>People can recognize scenes across many different modalities beyond natural\nimages. In this paper, we investigate how to learn cross-modal scene\nrepresentations that transfer across modalities. To study this problem, we\nintroduce a new cross-modal scene dataset. While convolutional neural networks\ncan categorize cross-modal scenes well, they also learn an intermediate\nrepresentation not aligned across modalities, which is undesirable for\ncross-modal transfer applications. We present methods to regularize cross-modal\nconvolutional neural networks so that they have a shared representation that is\nagnostic of the modality. Our experiments suggest that our scene representation\ncan help transfer representations across modalities for retrieval. Moreover,\nour visualizations suggest that units emerge in the shared representation that\ntend to activate on consistent concepts independently of the modality.</p>\n", "tags": ["Datasets","CVPR"] },
{"key": "chaidaroon2017variational", "year": "2017", "citations": "71", "title":"Variational Deep Semantic Hashing for Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over\nthe past decade, efficient similarity search methods have become\na crucial component of large-scale information retrieval systems.\nA popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The\nrecent advances of deep learning in a wide range of applications\nhas demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative\nmodels naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,\nwhich is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["Scalability","Text-Retrieval","Tools-&-Libraries","SIGIR","Supervised","Compact-Codes","Similarity-Search","Hashing-Methods","Unsupervised"] },
{"key": "chaidaroon2018deep", "year": "2018", "citations": "29", "title":"Deep Semantic Text Hashing with Weak Supervision", "abstract": "<p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>\n", "tags": ["Scalability","Datasets","Text-Retrieval","SIGIR","Compact-Codes","Similarity-Search","Hashing-Methods","Unsupervised"] },
{"key": "chaidaroon2025deep", "year": "2018", "citations": "29", "title":"Deep Semantic Text Hashing with Weak Supervision", "abstract": "<p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>\n", "tags": ["Scalability","Datasets","Text-Retrieval","SIGIR","Compact-Codes","Similarity-Search","Hashing-Methods","Unsupervised"] },
{"key": "chaidaroon2025variational", "year": "2017", "citations": "71", "title":"Variational Deep Semantic Hashing for Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over\nthe past decade, efficient similarity search methods have become\na crucial component of large-scale information retrieval systems.\nA popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The\nrecent advances of deep learning in a wide range of applications\nhas demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative\nmodels naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,\nwhich is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["Scalability","Text-Retrieval","Tools-&-Libraries","SIGIR","Supervised","Compact-Codes","Similarity-Search","Hashing-Methods","Unsupervised"] },
{"key": "chandrasekhar2017compression", "year": "2017", "citations": "21", "title":"Compression of Deep Neural Networks for Image Instance Retrieval", "abstract": "<p>Image instance retrieval is the problem of retrieving images from a database\nwhich contain the same object. Convolutional Neural Network (CNN) based\ndescriptors are becoming the dominant approach for generating {\\it global image\ndescriptors} for the instance retrieval problem. One major drawback of\nCNN-based {\\it global descriptors} is that uncompressed deep neural network\nmodels require hundreds of megabytes of storage making them inconvenient to\ndeploy in mobile applications or in custom hardware. In this work, we study the\nproblem of neural network model compression focusing on the image instance\nretrieval task. We study quantization, coding, pruning and weight sharing\ntechniques for reducing model size for the instance retrieval problem. We\nprovide extensive experimental results on the trade-off between retrieval\nperformance and model size for different types of networks on several data sets\nproviding the most comprehensive study on this topic. We compress models to the\norder of a few MBs: two orders of magnitude smaller than the uncompressed\nmodels while achieving negligible loss in retrieval performance.</p>\n", "tags": ["Quantization","Evaluation"] },
{"key": "charikar2018hashing", "year": "2017", "citations": "52", "title":"Hashing-Based-Estimators for Kernel Density in High Dimensions", "abstract": "<p>Given a set of points \\(P\\subset \\mathbb{R}^{d}\\) and a kernel \\(k\\), the Kernel\nDensity Estimate at a point \\(x\\in\\mathbb{R}^{d}\\) is defined as\n\\(\\mathrm{KDE}<em>{P}(x)=\\frac{1}{|P|}\\sum</em>{y\\in P} k(x,y)\\). We study the problem\nof designing a data structure that given a data set \\(P\\) and a kernel function,\nreturns <em>approximations to the kernel density</em> of a query point in <em>sublinear\ntime</em>. We introduce a class of unbiased estimators for kernel density\nimplemented through locality-sensitive hashing, and give general theorems\nbounding the variance of such estimators. These estimators give rise to\nefficient data structures for estimating the kernel density in high dimensions\nfor a variety of commonly used kernels. Our work is the first to provide\ndata-structures with theoretical guarantees that improve upon simple random\nsampling in high dimensions.</p>\n", "tags": ["Hashing-Methods"] },
{"key": "charikar2020kernel", "year": "2020", "citations": "8", "title":"Kernel Density Estimation through Density Constrained Near Neighbor Search", "abstract": "<p>In this paper we revisit the kernel density estimation problem: given a\nkernel \\(K(x, y)\\) and a dataset of \\(n\\) points in high dimensional Euclidean\nspace, prepare a data structure that can quickly output, given a query \\(q\\), a\n\\((1+\\epsilon)\\)-approximation to \\(\\mu:=\\frac1{|P|}\\sum_{p\\in P} K(p, q)\\). First,\nwe give a single data structure based on classical near neighbor search\ntechniques that improves upon or essentially matches the query time and space\ncomplexity for all radial kernels considered in the literature so far. We then\nshow how to improve both the query complexity and runtime by using recent\nadvances in data-dependent near neighbor search.\n  We achieve our results by giving a new implementation of the natural\nimportance sampling scheme. Unlike previous approaches, our algorithm first\nsamples the dataset uniformly (considering a geometric sequence of sampling\nrates), and then uses existing approximate near neighbor search techniques on\nthe resulting smaller dataset to retrieve the sampled points that lie at an\nappropriate distance from the query. We show that the resulting sampled dataset\nhas strong geometric structure, making approximate near neighbor search return\nthe required samples much more efficiently than for worst case datasets of the\nsame size. As an example application, we show that this approach yields a data\nstructure that achieves query time \\(\\mu^{-(1+o(1))/4}\\) and space complexity\n\\(\\mu^{-(1+o(1))}\\) for the Gaussian kernel. Our data dependent approach achieves\nquery time \\(\\mu^{-0.173-o(1)}\\) and space \\(\\mu^{-(1+o(1))}\\) for the Gaussian\nkernel. The data dependent analysis relies on new techniques for tracking the\ngeometric structure of the input datasets in a recursive hashing process that\nwe hope will be of interest in other applications in near neighbor search.</p>\n", "tags": ["Hashing-Methods","Datasets","Efficiency"] },
{"key": "chaudhuri2021crossatnet", "year": "2020", "citations": "28", "title":"CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based Image Retrieval", "abstract": "<p>We propose a novel framework for cross-modal zero-shot learning (ZSL) in the\ncontext of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema\nmainly considers simultaneous mappings among the two image views and the\nsemantic side information. Therefore, it is desirable to consider fine-grained\nclasses mainly in the sketch domain using highly discriminative and\nsemantically rich feature space. However, the existing deep generative\nmodeling-based SBIR approaches majorly focus on bridging the gaps between the\nseen and unseen classes by generating pseudo-unseen-class samples. Besides,\nviolating the ZSL protocol by not utilizing any unseen-class information during\ntraining, such techniques do not pay explicit attention to modeling the\ndiscriminative nature of the shared space. Also, we note that learning a\nunified feature space for both the multi-view visual data is a tedious task\nconsidering the significant domain difference between sketches and color\nimages. In this respect, as a remedy, we introduce a novel framework for\nzero-shot SBIR. While we define a cross-modal triplet loss to ensure the\ndiscriminative nature of the shared space, an innovative cross-modal attention\nlearning strategy is also proposed to guide feature extraction from the image\ndomain exploiting information from the respective sketch counterpart. In order\nto preserve the semantic consistency of the shared space, we consider a graph\nCNN-based module that propagates the semantic class topology to the shared\nspace. To ensure an improved response time during inference, we further explore\nthe possibility of representing the shared space in terms of hash codes.\nExperimental results obtained on the benchmark TU-Berlin and the Sketchy\ndatasets confirm the superiority of CrossATNet in yielding state-of-the-art\nresults.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "chen2018blazingly", "year": "2018", "citations": "283", "title":"Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning", "abstract": "<p>This paper tackles the problem of video object segmentation, given some user\nannotation which indicates the object of interest. The problem is formulated as\npixel-wise retrieval in a learned embedding space: we embed pixels of the same\nobject instance into the vicinity of each other, using a fully convolutional\nnetwork trained by a modified triplet loss as the embedding model. Then the\nannotated pixels are set as reference and the rest of the pixels are classified\nusing a nearest-neighbor approach. The proposed method supports different kinds\nof user input such as segmentation mask in the first frame (semi-supervised\nscenario), or a sparse set of clicked points (interactive scenario). In the\nsemi-supervised scenario, we achieve results competitive with the state of the\nart but at a fraction of computation cost (275 milliseconds per frame). In the\ninteractive scenario where the user is able to refine their input iteratively,\nthe proposed method provides instant response to each input, and reaches\ncomparable quality to competing methods with much less interaction.</p>\n", "tags": ["Distance-Metric-Learning","CVPR","Supervised"] },
{"key": "chen2018deep", "year": "2018", "citations": "60", "title":"Deep Supervised Hashing With Anchor Graph", "abstract": "<p>Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware’s memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.</p>\n", "tags": ["Scalability","Datasets","CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "chen2018distributed", "year": "2018", "citations": "7", "title":"Distributed Collaborative Hashing and Its Applications in Ant Financial", "abstract": "<p>Collaborative filtering, especially latent factor model, has been popularly\nused in personalized recommendation. Latent factor model aims to learn user and\nitem latent factors from user-item historic behaviors. To apply it into real\nbig data scenarios, efficiency becomes the first concern, including offline\nmodel training efficiency and online recommendation efficiency. In this paper,\nwe propose a Distributed Collaborative Hashing (DCH) model which can\nsignificantly improve both efficiencies. Specifically, we first propose a\ndistributed learning framework, following the state-of-the-art parameter server\nparadigm, to learn the offline collaborative model. Our model can be learnt\nefficiently by distributedly computing subgradients in minibatches on workers\nand updating model parameters on servers asynchronously. We then adopt hashing\ntechnique to speedup the online recommendation procedure. Recommendation can be\nquickly made through exploiting lookup hash tables. We conduct thorough\nexperiments on two real large-scale datasets. The experimental results\ndemonstrate that, comparing with the classic and state-of-the-art (distributed)\nlatent factor models, DCH has comparable performance in terms of recommendation\naccuracy but has both fast convergence speed in offline model training\nprocedure and realtime efficiency in online recommendation procedure.\nFurthermore, the encouraging performance of DCH is also shown for several\nreal-world applications in Ant Financial.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Recommender-Systems","KDD","Scalability","Evaluation","Efficiency"] },
{"key": "chen2018improving", "year": "2019", "citations": "12", "title":"Improving Deep Binary Embedding Networks by Order-aware Reweighting of Triplets", "abstract": "<p>In this paper, we focus on triplet-based deep binary embedding networks for\nimage retrieval task. The triplet loss has been shown to be most effective for\nthe ranking problem. However, most of the previous works treat the triplets\nequally or select the hard triplets based on the loss. Such strategies do not\nconsider the order relations, which is important for retrieval task. To this\nend, we propose an order-aware reweighting method to effectively train the\ntriplet-based deep networks, which up-weights the important triplets and\ndown-weights the uninformative triplets. First, we present the order-aware\nweighting factors to indicate the importance of the triplets, which depend on\nthe rank order of binary codes. Then, we reshape the triplet loss to the\nsquared triplet loss such that the loss function will put more weights on the\nimportant triplets. Extensive evaluations on four benchmark datasets show that\nthe proposed method achieves significant performance compared with the\nstate-of-the-art baselines.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Datasets","Compact-Codes","Evaluation"] },
{"key": "chen2019differentiable", "year": "2019", "citations": "13", "title":"Differentiable Product Quantization for End-to-End Embedding Compression", "abstract": "<p>Embedding layers are commonly used to map discrete symbols into continuous\nembedding vectors that reflect their semantic meanings. Despite their\neffectiveness, the number of parameters in an embedding layer increases\nlinearly with the number of symbols and poses a critical challenge on memory\nand storage constraints. In this work, we propose a generic and end-to-end\nlearnable compression framework termed differentiable product quantization\n(DPQ). We present two instantiations of DPQ that leverage different\napproximation techniques to enable differentiability in end-to-end learning.\nOur method can readily serve as a drop-in alternative for any existing\nembedding layer. Empirically, DPQ offers significant compression ratios\n(14-238\\(\\times\\)) at negligible or no performance cost on 10 datasets across\nthree different language tasks.</p>\n", "tags": ["Datasets","Quantization","Evaluation","Tools-&-Libraries"] },
{"key": "chen2019efficient", "year": "2021", "citations": "12", "title":"Efficient Object Embedding for Spliced Image Retrieval", "abstract": "<p>Detecting spliced images is one of the emerging challenges in computer\nvision. Unlike prior methods that focus on detecting low-level artifacts\ngenerated during the manipulation process, we use an image retrieval approach\nto tackle this problem. When given a spliced query image, our goal is to\nretrieve the original image from a database of authentic images. To achieve\nthis goal, we propose representing an image by its constituent objects based on\nthe intuition that the finest granularity of manipulations is oftentimes at the\nobject-level. We introduce a framework, object embeddings for spliced image\nretrieval (OE-SIR), that utilizes modern object detectors to localize object\nregions. Each region is then embedded and collectively used to represent the\nimage. Further, we propose a student-teacher training paradigm for learning\ndiscriminative embeddings within object regions to avoid expensive multiple\nforward passes. Detailed analysis of the efficacy of different feature\nembedding models is also provided in this study. Extensive experimental results\nshow that the OE-SIR achieves state-of-the-art performance in spliced image\nretrieval.</p>\n", "tags": ["CVPR","Evaluation","Tools-&-Libraries","Image-Retrieval"] },
{"key": "chen2019hybrid", "year": "2019", "citations": "66", "title":"Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval", "abstract": "<p>In zero-shot image retrieval (ZSIR) task, embedding learning becomes more\nattractive, however, many methods follow the traditional metric learning idea\nand omit the problems behind zero-shot settings. In this paper, we first\nemphasize the importance of learning visual discriminative metric and\npreventing the partial/selective learning behavior of learner in ZSIR, and then\npropose the Decoupled Metric Learning (DeML) framework to achieve these\nindividually. Instead of coarsely optimizing an unified metric, we decouple it\ninto multiple attention-specific parts so as to recurrently induce the\ndiscrimination and explicitly enhance the generalization. And they are mainly\nachieved by our object-attention module based on random walk graph propagation\nand the channel-attention module based on the adversary constraint,\nrespectively. We demonstrate the necessity of addressing the vital problems in\nZSIR on the popular benchmarks, outperforming the state-of-theart methods by a\nsignificant margin. Code is available at http://www.bhchen.cn</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Tools-&-Libraries","Image-Retrieval","Distance-Metric-Learning","CVPR"] },
{"key": "chen2019two", "year": "2019", "citations": "45", "title":"A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps", "abstract": "<p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Hashing-Methods","Multimodal-Retrieval","Compact-Codes"] },
{"key": "chen2019vector", "year": "2019", "citations": "7", "title":"Vector and Line Quantization for Billion-scale Similarity Search on GPUs", "abstract": "<p>Billion-scale high-dimensional approximate nearest neighbour (ANN) search has\nbecome an important problem for searching similar objects among the vast amount\nof images and videos available online. The existing ANN methods are usually\ncharacterized by their specific indexing structures, including the inverted\nindex and the inverted multi-index structure. The inverted index structure is\namenable to GPU-based implementations, and the state-of-the-art systems such as\nFaiss are able to exploit the massive parallelism offered by GPUs. However, the\ninverted index requires high memory overhead to index the dataset effectively.\nThe inverted multi-index structure is difficult to implement for GPUs, and also\nineffective in dealing with database with different data distributions. In this\npaper we propose a novel hierarchical inverted index structure generated by\nvector and line quantization methods. Our quantization method improves both\nsearch efficiency and accuracy, while maintaining comparable memory\nconsumption. This is achieved by reducing search space and increasing the\nnumber of indexed regions. We introduce a new ANN search system, VLQ-ADC, that\nis based on the proposed inverted index, and perform extensive evaluation on\ntwo public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation\nshows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and\nCPU-based systems in terms of both accuracy and search speed. The source code\nof VLQ-ADC is available at\nhttps://github.com/zjuchenwei/vector-line-quantization.</p>\n", "tags": ["Similarity-Search","Vector-Indexing","Tools-&-Libraries","Datasets","Quantization","Scalability","Large-Scale-Search","Evaluation","Efficiency"] },
{"key": "chen2020fine", "year": "2020", "citations": "272", "title":"Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning", "abstract": "<p>Cross-modal retrieval between videos and texts has attracted growing\nattentions due to the rapid emergence of videos on the web. The current\ndominant approach for this problem is to learn a joint embedding space to\nmeasure cross-modal similarities. However, simple joint embeddings are\ninsufficient to represent complicated visual and textual details, such as\nscenes, objects, actions and their compositions. To improve fine-grained\nvideo-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model,\nwhich decomposes video-text matching into global-to-local levels. To be\nspecific, the model disentangles texts into hierarchical semantic graph\nincluding three levels of events, actions, entities and relationships across\nlevels. Attention-based graph reasoning is utilized to generate hierarchical\ntextual embeddings, which can guide the learning of diverse and hierarchical\nvideo representations. The HGR model aggregates matchings from different\nvideo-text levels to capture both global and local details. Experimental\nresults on three video-text datasets demonstrate the advantages of our model.\nSuch hierarchical decomposition also enables better generalization across\ndatasets and improves the ability to distinguish fine-grained semantic\ndifferences.</p>\n", "tags": ["Datasets","CVPR","Text-Retrieval","Multimodal-Retrieval"] },
{"key": "chen2020making", "year": "2019", "citations": "21", "title":"Making Online Sketching Hashing Even Faster", "abstract": "<p>Data-dependent hashing methods have demonstrated good performance in various\nmachine learning applications to learn a low-dimensional representation from\nthe original data. However, they still suffer from several obstacles: First,\nmost of existing hashing methods are trained in a batch mode, yielding\ninefficiency for training streaming data. Second, the computational cost and\nthe memory consumption increase extraordinarily in the big data setting, which\nperplexes the training procedure. Third, the lack of labeled data hinders the\nimprovement of the model performance. To address these difficulties, we utilize\nonline sketching hashing (OSH) and present a FasteR Online Sketching Hashing\n(FROSH) algorithm to sketch the data in a more compact form via an independent\ntransformation. We provide theoretical justification to guarantee that our\nproposed FROSH consumes less time and achieves a comparable sketching precision\nunder the same memory cost of OSH. We also extend FROSH to its distributed\nimplementation, namely DFROSH, to further reduce the training time cost of\nFROSH while deriving the theoretical bound of the sketching precision. Finally,\nwe conduct extensive experiments on both synthetic and real datasets to\ndemonstrate the attractive merits of FROSH and DFROSH.</p>\n", "tags": ["Hashing-Methods","Datasets","Evaluation"] },
{"key": "chen2020strongly", "year": "2020", "citations": "50", "title":"Strongly Constrained Discrete Hashing", "abstract": "<p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","Quantization","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "chen2021long", "year": "2021", "citations": "9", "title":"Long-Tail Hashing", "abstract": "<p>Hashing, which represents data items as compact binary codes, has\nbeen becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well\nas its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially\nbalanced datasets which are not commonly available in real-world\nscenarios. In this paper, we propose Long-Tail Hashing Network\n(LTHNet), a novel two-stage deep hashing approach that addresses\nthe problem of learning to hash for more realistic datasets where\nthe data labels roughly exhibit a long-tail distribution. Specifically,\nthe first stage is to learn relaxed embeddings of the given dataset\nwith its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those\nobtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual\nknowledge transfer between head and tail classes, and thus enrich\nimage representations for hashing. Our experiments have shown\nthat LTHNet achieves dramatic performance improvements over all\nstate-of-the-art competitors on long-tail datasets, with no or little\nsacrifice on balanced datasets. Further analyses reveal that while to\nour surprise directly manipulating class weights in the loss function\nhas little effect, the extended dynamic meta-embedding module, the\nusage of cross-entropy loss instead of square loss, and the relatively\nsmall batch-size for training all contribute to LTHNet’s success.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","SIGIR","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "chen2021multimodal", "year": "2021", "citations": "52", "title":"Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos", "abstract": "<p>Multimodal self-supervised learning is getting more and more attention as it\nallows not only to train large networks without human supervision but also to\nsearch and retrieve data across various modalities. In this context, this paper\nproposes a self-supervised training framework that learns a common multimodal\nembedding space that, in addition to sharing representations across different\nmodalities, enforces a grouping of semantically similar instances. To this end,\nwe extend the concept of instance-level contrastive learning with a multimodal\nclustering step in the training pipeline to capture semantic similarities\nacross modalities. The resulting embedding space enables retrieval of samples\nacross all modalities, even from unseen datasets and different domains. To\nevaluate our approach, we train our model on the HowTo100M dataset and evaluate\nits zero-shot retrieval capabilities in two challenging domains, namely\ntext-to-video retrieval, and temporal action localization, showing\nstate-of-the-art results on four different datasets.</p>\n", "tags": ["ICCV","Few-Shot-&-Zero-Shot","Supervised","Tools-&-Libraries","Datasets","Video-Retrieval","Self-Supervised"] },
{"key": "chen2022approximate", "year": "2022", "citations": "12", "title":"Approximate Nearest Neighbor Search under Neural Similarity Metric for Large-Scale Recommendation", "abstract": "<p>Model-based methods for recommender systems have been studied extensively for\nyears. Modern recommender systems usually resort to 1) representation learning\nmodels which define user-item preference as the distance between their\nembedding representations, and 2) embedding-based Approximate Nearest Neighbor\n(ANN) search to tackle the efficiency problem introduced by large-scale corpus.\nWhile providing efficient retrieval, the embedding-based retrieval pattern also\nlimits the model capacity since the form of user-item preference measure is\nrestricted to the distance between their embedding representations. However,\nfor other more precise user-item preference measures, e.g., preference scores\ndirectly derived from a deep neural network, they are computationally\nintractable because of the lack of an efficient retrieval method, and an\nexhaustive search for all user-item pairs is impractical. In this paper, we\npropose a novel method to extend ANN search to arbitrary matching functions,\ne.g., a deep neural network. Our main idea is to perform a greedy walk with a\nmatching function in a similarity graph constructed from all items. To solve\nthe problem that the similarity measures of graph construction and user-item\nmatching function are heterogeneous, we propose a pluggable adversarial\ntraining task to ensure the graph search with arbitrary matching function can\nachieve fairly high precision. Experimental results in both open source and\nindustry datasets demonstrate the effectiveness of our method. The proposed\nmethod has been fully deployed in the Taobao display advertising platform and\nbrings a considerable advertising revenue increase. We also summarize our\ndetailed experiences in deployment in this paper.</p>\n", "tags": ["Similarity-Search","Scalability","Efficiency","Evaluation","Distance-Metric-Learning","Datasets","CIKM","Recommender-Systems","Graph-Based-ANN","Robustness"] },
{"key": "chen2022intra", "year": "2022", "citations": "6", "title":"Intra-Modal Constraint Loss For Image-Text Retrieval", "abstract": "<p>Cross-modal retrieval has drawn much attention in both computer vision and\nnatural language processing domains. With the development of convolutional and\nrecurrent neural networks, the bottleneck of retrieval across image-text\nmodalities is no longer the extraction of image and text features but an\nefficient loss function learning in embedding space. Many loss functions try to\ncloser pairwise features from heterogeneous modalities. This paper proposes a\nmethod for learning joint embedding of images and texts using an intra-modal\nconstraint loss function to reduce the violation of negative pairs from the\nsame homogeneous modality. Experimental results show that our approach\noutperforms state-of-the-art bi-directional image-text retrieval methods on\nFlickr30K and Microsoft COCO datasets. Our code is publicly available:\nhttps://github.com/CanonChen/IMC.</p>\n", "tags": ["Datasets","Text-Retrieval","Multimodal-Retrieval"] },
{"key": "chen2022learning", "year": "2022", "citations": "24", "title":"Learning Binarized Graph Representations with Multi-faceted Quantization Reinforcement for Top-K Recommendation", "abstract": "<p>Learning vectorized embeddings is at the core of various recommender systems\nfor user-item matching. To perform efficient online inference, representation\nquantization, aiming to embed the latent features by a compact sequence of\ndiscrete numbers, recently shows the promising potentiality in optimizing both\nmemory and computation overheads. However, existing work merely focuses on\nnumerical quantization whilst ignoring the concomitant information loss issue,\nwhich, consequently, leads to conspicuous performance degradation. In this\npaper, we propose a novel quantization framework to learn Binarized Graph\nRepresentations for Top-K Recommendation (BiGeaR). BiGeaR introduces\nmulti-faceted quantization reinforcement at the pre-, mid-, and post-stage of\nbinarized representation learning, which substantially retains the\nrepresentation informativeness against embedding binarization. In addition to\nsaving the memory footprint, BiGeaR further develops solid online inference\nacceleration with bitwise operations, providing alternative flexibility for the\nrealistic deployment. The empirical results over five large real-world\nbenchmarks show that BiGeaR achieves about 22%~40% performance improvement over\nthe state-of-the-art quantization-based recommender system, and recovers about\n95%~102% of the performance capability of the best full-precision counterpart\nwith over 8x time and space reduction.</p>\n", "tags": ["Tools-&-Libraries","Recommender-Systems","KDD","Memory-Efficiency","Quantization","Evaluation"] },
{"key": "chen2022multi", "year": "2022", "citations": "19", "title":"Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos", "abstract": "<p>Geo-tagged photo based tourist attraction recommendation can discover users’\ntravel preferences from their taken photos, so as to recommend suitable tourist\nattractions to them. However, existing visual content based methods cannot\nfully exploit the user and tourist attraction information of photos to extract\nvisual features, and do not differentiate the significances of different\nphotos. In this paper, we propose multi-level visual similarity based\npersonalized tourist attraction recommendation using geo-tagged photos (MEAL).\nMEAL utilizes the visual contents of photos and interaction behavior data to\nobtain the final embeddings of users and tourist attractions, which are then\nused to predict the visit probabilities. Specifically, by crossing the user and\ntourist attraction information of photos, we define four visual similarity\nlevels and introduce a corresponding quintuplet loss to embed the visual\ncontents of photos. In addition, to capture the significances of different\nphotos, we exploit the self-attention mechanism to obtain the visual\nrepresentations of users and tourist attractions. We conducted experiments on a\ndataset crawled from Flickr, and the experimental results proved the advantage\nof this method.</p>\n", "tags": ["Datasets","Recommender-Systems"] },
{"key": "chen2023bipartite", "year": "2023", "citations": "15", "title":"Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space", "abstract": "<p>Searching on bipartite graphs is basal and versatile to many real-world Web\napplications, e.g., online recommendation, database retrieval, and\nquery-document searching. Given a query node, the conventional approaches rely\non the similarity matching with the vectorized node embeddings in the\ncontinuous Euclidean space. To efficiently manage intensive similarity\ncomputation, developing hashing techniques for graph structured data has\nrecently become an emerging research direction. Despite the retrieval\nefficiency in Hamming space, prior work is however confronted with catastrophic\nperformance decay. In this work, we investigate the problem of hashing with\nGraph Convolutional Network on bipartite graphs for effective Top-N search. We\npropose an end-to-end Bipartite Graph Convolutional Hashing approach, namely\nBGCH, which consists of three novel and effective modules: (1) adaptive graph\nconvolutional hashing, (2) latent feature dispersion, and (3) Fourier\nserialized gradient estimation. Specifically, the former two modules achieve\nthe substantial retention of the structural information against the inevitable\ninformation loss in hash encoding; the last module develops Fourier Series\ndecomposition to the hashing function in the frequency domain mainly for more\naccurate gradient estimation. The extensive experiments on six real-world\ndatasets not only show the performance superiority over the competing\nhashing-based counterparts, but also demonstrate the effectiveness of all\nproposed model components contained therein.</p>\n", "tags": ["Hashing-Methods","Datasets","Recommender-Systems","Evaluation","Efficiency"] },
{"key": "chen2025deep", "year": "2018", "citations": "60", "title":"Deep Hashing via Discrepancy Minimization", "abstract": "<p>This paper presents a discrepancy minimizing model to\naddress the discrete optimization problem in hashing learning. The discrete optimization introduced by binary constraint is an NP-hard mixed integer programming problem.\nIt is usually addressed by relaxing the binary variables into\ncontinuous variables to adapt to the gradient based learning of hashing functions, especially the training of deep\nneural networks. To deal with the objective discrepancy\ncaused by relaxation, we transform the original binary optimization into differentiable optimization problem over hash\nfunctions through series expansion. This transformation decouples the binary constraint and the similarity preserving\nhashing function optimization. The transformed objective\nis optimized in a tractable alternating optimization framework with gradual discrepancy minimization. Extensive experimental results on three benchmark datasets validate the\nefficacy of the proposed discrepancy minimizing hashing.</p>\n", "tags": ["Datasets","CVPR","Neural-Hashing","Tools-&-Libraries","Hashing-Methods","Evaluation"] },
{"key": "chen2025long", "year": "2021", "citations": "9", "title":"Long-Tail Hashing", "abstract": "<p>Hashing, which represents data items as compact binary codes, has\nbeen becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well\nas its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially\nbalanced datasets which are not commonly available in real-world\nscenarios. In this paper, we propose Long-Tail Hashing Network\n(LTHNet), a novel two-stage deep hashing approach that addresses\nthe problem of learning to hash for more realistic datasets where\nthe data labels roughly exhibit a long-tail distribution. Specifically,\nthe first stage is to learn relaxed embeddings of the given dataset\nwith its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those\nobtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual\nknowledge transfer between head and tail classes, and thus enrich\nimage representations for hashing. Our experiments have shown\nthat LTHNet achieves dramatic performance improvements over all\nstate-of-the-art competitors on long-tail datasets, with no or little\nsacrifice on balanced datasets. Further analyses reveal that while to\nour surprise directly manipulating class weights in the loss function\nhas little effect, the extended dynamic meta-embedding module, the\nusage of cross-entropy loss instead of square loss, and the relatively\nsmall batch-size for training all contribute to LTHNet’s success.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","SIGIR","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "chen2025strongly", "year": "2020", "citations": "50", "title":"Strongly Constrained Discrete Hashing", "abstract": "<p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","Quantization","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "chen2025two", "year": "2019", "citations": "45", "title":"A Two-step Cross-modal Hashing by Exploiting Label Correlations and Preserving Similarity in Both Steps", "abstract": "<p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Hashing-Methods","Multimodal-Retrieval","Compact-Codes"] },
{"key": "cheng2020robust", "year": "2020", "citations": "48", "title":"Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval", "abstract": "<p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>\n", "tags": ["Scalability","Datasets","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "cheng2025robust", "year": "2020", "citations": "48", "title":"Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval", "abstract": "<p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>\n", "tags": ["Scalability","Datasets","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "chiu2018learning", "year": "2019", "citations": "24", "title":"Learning to Index for Nearest Neighbor Search", "abstract": "<p>In this study, we present a novel ranking model based on learning\nneighborhood relationships embedded in the index space. Given a query point,\nconventional approximate nearest neighbor search calculates the distances to\nthe cluster centroids, before ranking the clusters from near to far based on\nthe distances. The data indexed in the top-ranked clusters are retrieved and\ntreated as the nearest neighbor candidates for the query. However, the loss of\nquantization between the data and cluster centroids will inevitably harm the\nsearch accuracy. To address this problem, the proposed model ranks clusters\nbased on their nearest neighbor probabilities rather than the query-centroid\ndistances. The nearest neighbor probabilities are estimated by employing neural\nnetworks to characterize the neighborhood relationships, i.e., the density\nfunction of nearest neighbors with respect to the query. The proposed\nprobability-based ranking can replace the conventional distance-based ranking\nfor finding candidate clusters, and the predicted probability can be used to\ndetermine the data quantity to be retrieved from the candidate cluster. Our\nexperimental results demonstrated that the proposed ranking model could boost\nthe search performance effectively in billion-scale datasets.</p>\n", "tags": ["Datasets","Scalability","Quantization","Large-Scale-Search","Evaluation"] },
{"key": "chowdhury2018instance", "year": "2019", "citations": "5", "title":"Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing", "abstract": "<p>Supervised learning models are typically trained on a single dataset and the\nperformance of these models rely heavily on the size of the dataset, i.e.,\namount of data available with the ground truth. Learning algorithms try to\ngeneralize solely based on the data that is presented with during the training.\nIn this work, we propose an inductive transfer learning method that can augment\nlearning models by infusing similar instances from different learning tasks in\nthe Natural Language Processing (NLP) domain. We propose to use instance\nrepresentations from a source dataset, \\textit{without inheriting anything}\nfrom the source learning model. Representations of the instances of\n\\textit{source} \\&amp; \\textit{target} datasets are learned, retrieval of relevant\nsource instances is performed using soft-attention mechanism and\n\\textit{locality sensitive hashing}, and then, augmented into the model during\ntraining on the target dataset. Our approach simultaneously exploits the local\n\\textit{instance level information} as well as the macro statistical viewpoint\nof the dataset. Using this approach we have shown significant improvements for\nthree major news classification datasets over the baseline. Experimental\nevaluations also show that the proposed approach reduces dependency on labeled\ndata by a significant margin for comparable performance. With our proposed\ncross dataset learning procedure we show that one can achieve\ncompetitive/better performance than learning from a single dataset.</p>\n", "tags": ["Supervised","Locality-Sensitive-Hashing","Hashing-Methods","Datasets","Evaluation"] },
{"key": "christiani2016framework", "year": "2017", "citations": "21", "title":"A Framework for Similarity Search with Space-Time Tradeoffs using Locality-Sensitive Filtering", "abstract": "<p>We present a framework for similarity search based on Locality-Sensitive\nFiltering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive\nHashing (LSH) framework to support space-time tradeoffs. Given a family of\nfilters, defined as a distribution over pairs of subsets of space with certain\nlocality-sensitivity properties, we can solve the approximate near neighbor\nproblem in \\(d\\)-dimensional space for an \\(n\\)-point data set with query time\n\\(dn^{\\rho_q+o(1)}\\), update time \\(dn^{\\rho_u+o(1)}\\), and space usage \\(dn + n^{1</p>\n<ul>\n  <li>\\rho_u + o(1)}\\). The space-time tradeoff is tied to the tradeoff between\nquery time and update time, controlled by the exponents \\(\\rho_q, \\rho_u\\) that\nare determined by the filter family. Locality-sensitive filtering was\nintroduced by Becker et al. (SODA 2016) together with a framework yielding a\nsingle, balanced, tradeoff between query time and space, further relying on the\nassumption of an efficient oracle for the filter evaluation algorithm. We\nextend the LSF framework to support space-time tradeoffs and through a\ncombination of existing techniques we remove the oracle assumption.\nBuilding on a filter family for the unit sphere by Laarhoven (arXiv 2015) we\nuse a kernel embedding technique by Rahimi &amp; Recht (NIPS 2007) to show a\nsolution to the \\((r,cr)\\)-near neighbor problem in \\(\\ell_s^d\\)-space for \\(0 &lt; s\n\\leq 2\\) with query and update exponents\n\\(\\rho_q=\\frac{c^s(1+\\lambda)^2}{(c^s+\\lambda)^2}\\) and\n\\(\\rho_u=\\frac{c^s(1-\\lambda)^2}{(c^s+\\lambda)^2}\\) where \\(\\lambda\\in[-1,1]\\) is a\ntradeoff parameter. This result improves upon the space-time tradeoff of\nKapralov (PODS 2015) and is shown to be optimal in the case of a balanced\ntradeoff. Finally, we show a lower bound for the space-time tradeoff on the\nunit sphere that matches Laarhoven’s and our own upper bound in the case of\nrandom data.</li>\n</ul>\n", "tags": ["Similarity-Search","Tools-&-Libraries","Locality-Sensitive-Hashing","Hashing-Methods","Evaluation","Efficiency"] },
{"key": "christiani2016set", "year": "2017", "citations": "45", "title":"Set Similarity Search Beyond MinHash", "abstract": "<p>We consider the problem of approximate set similarity search under\nBraun-Blanquet similarity \\(B(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} \\cap\n\\mathbf{y}| / \\max(|\\mathbf{x}|, |\\mathbf{y}|)\\). The \\((b_2, b_2)\\)-approximate\nBraun-Blanquet similarity search problem is to preprocess a collection of sets\n\\(P\\) such that, given a query set \\(\\mathbf{q}\\), if there exists \\(\\mathbf{x} \\in\nP\\) with \\(B(\\mathbf{q}, \\mathbf{x}) \\geq b_1\\), then we can efficiently return\n\\(\\mathbf{x}’ \\in P\\) with \\(B(\\mathbf{q}, \\mathbf{x}’) &gt; b_2\\).\n  We present a simple data structure that solves this problem with space usage\n\\(O(n^{1+\\rho}log n + \\sum_{\\mathbf{x} \\in P}|\\mathbf{x}|)\\) and query time\n\\(O(|\\mathbf{q}|n^{\\rho} log n)\\) where \\(n = |P|\\) and \\(\\rho =\nlog(1/b_1)/log(1/b_2)\\). Making use of existing lower bounds for\nlocality-sensitive hashing by O’Donnell et al. (TOCT 2014) we show that this\nvalue of \\(\\rho\\) is tight across the parameter space, i.e., for every choice of\nconstants \\(0 &lt; b_2 &lt; b_1 &lt; 1\\).\n  In the case where all sets have the same size our solution strictly improves\nupon the value of \\(\\rho\\) that can be obtained through the use of\nstate-of-the-art data-independent techniques in the Indyk-Motwani\nlocality-sensitive hashing framework (STOC 1998) such as Broder’s MinHash (CCS\n1997) for Jaccard similarity and Andoni et al.’s cross-polytope LSH (NIPS 2015)\nfor cosine similarity. Surprisingly, even though our solution is\ndata-independent, for a large part of the parameter space we outperform the\ncurrently best data-dependent method by Andoni and Razenshteyn (STOC 2015).</p>\n", "tags": ["Similarity-Search","Tools-&-Libraries","Locality-Sensitive-Hashing","Hashing-Methods","Distance-Metric-Learning","Efficiency"] },
{"key": "christiani2017fast", "year": "2019", "citations": "14", "title":"Fast Locality-Sensitive Hashing Frameworks for Approximate Near Neighbor Search", "abstract": "<p>The Indyk-Motwani Locality-Sensitive Hashing (LSH) framework (STOC 1998) is a\ngeneral technique for constructing a data structure to answer approximate near\nneighbor queries by using a distribution \\(\\mathcal{H}\\) over locality-sensitive\nhash functions that partition space. For a collection of \\(n\\) points, after\npreprocessing, the query time is dominated by \\(O(n^{\\rho} log n)\\) evaluations\nof hash functions from \\(\\mathcal{H}\\) and \\(O(n^{\\rho})\\) hash table lookups and\ndistance computations where \\(\\rho \\in (0,1)\\) is determined by the\nlocality-sensitivity properties of \\(\\mathcal{H}\\). It follows from a recent\nresult by Dahlgaard et al. (FOCS 2017) that the number of locality-sensitive\nhash functions can be reduced to \\(O(log^2 n)\\), leaving the query time to be\ndominated by \\(O(n^{\\rho})\\) distance computations and \\(O(n^{\\rho} log n)\\)\nadditional word-RAM operations. We state this result as a general framework and\nprovide a simpler analysis showing that the number of lookups and distance\ncomputations closely match the Indyk-Motwani framework, making it a viable\nreplacement in practice. Using ideas from another locality-sensitive hashing\nframework by Andoni and Indyk (SODA 2006) we are able to reduce the number of\nadditional word-RAM operations to \\(O(n^\\rho)\\).</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Tools-&-Libraries","Efficiency"] },
{"key": "christiani2018confirmation", "year": "2018", "citations": "6", "title":"Confirmation Sampling for Exact Nearest Neighbor Search", "abstract": "<p>Locality-sensitive hashing (LSH), introduced by Indyk and Motwani in STOC\n‘98, has been an extremely influential framework for nearest neighbor search in\nhigh-dimensional data sets. While theoretical work has focused on the\napproximate nearest neighbor problems, in practice LSH data structures with\nsuitably chosen parameters are used to solve the exact nearest neighbor problem\n(with some error probability). Sublinear query time is often possible in\npractice even for exact nearest neighbor search, intuitively because the\nnearest neighbor tends to be significantly closer than other data points.\nHowever, theory offers little advice on how to choose LSH parameters outside of\npre-specified worst-case settings.\n  We introduce the technique of confirmation sampling for solving the exact\nnearest neighbor problem using LSH. First, we give a general reduction that\ntransforms a sequence of data structures that each find the nearest neighbor\nwith a small, unknown probability, into a data structure that returns the\nnearest neighbor with probability \\(1-\\delta\\), using as few queries as possible.\nSecond, we present a new query algorithm for the LSH Forest data structure with\n\\(L\\) trees that is able to return the exact nearest neighbor of a query point\nwithin the same time bound as an LSH Forest of \\(Ω(L)\\) trees with internal\nparameters specifically tuned to the query and data.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Tools-&-Libraries","Efficiency"] },
{"key": "chun2021probabilistic", "year": "2021", "citations": "169", "title":"Probabilistic Embeddings for Cross-Modal Retrieval", "abstract": "<p>Cross-modal retrieval methods build a common representation space for samples\nfrom multiple modalities, typically from the vision and the language domains.\nFor images and their captions, the multiplicity of the correspondences makes\nthe task particularly challenging. Given an image (respectively a caption),\nthere are multiple captions (respectively images) that equally make sense. In\nthis paper, we argue that deterministic functions are not sufficiently powerful\nto capture such one-to-many correspondences. Instead, we propose to use\nProbabilistic Cross-Modal Embedding (PCME), where samples from the different\nmodalities are represented as probabilistic distributions in the common\nembedding space. Since common benchmarks such as COCO suffer from\nnon-exhaustive annotations for cross-modal matches, we propose to additionally\nevaluate retrieval on the CUB dataset, a smaller yet clean database where all\npossible image-caption pairs are annotated. We extensively ablate PCME and\ndemonstrate that it not only improves the retrieval performance over its\ndeterministic counterpart but also provides uncertainty estimates that render\nthe embeddings more interpretable. Code is available at\nhttps://github.com/naver-ai/pcme</p>\n", "tags": ["Datasets","CVPR","Evaluation","Multimodal-Retrieval"] },
{"key": "chung2017learning", "year": "2017", "citations": "63", "title":"Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval", "abstract": "<p>Deep neural networks have been investigated in learning latent\nrepresentations of medical images, yet most of the studies limit their approach\nin a single supervised convolutional neural network (CNN), which usually rely\nheavily on a large scale annotated dataset for training. To learn image\nrepresentations with less supervision involved, we propose a deep Siamese CNN\n(SCNN) architecture that can be trained with only binary image pair\ninformation. We evaluated the learned image representations on a task of\ncontent-based medical image retrieval using a publicly available multiclass\ndiabetic retinopathy fundus image dataset. The experimental results show that\nour proposed deep SCNN is comparable to the state-of-the-art single supervised\nCNN, and requires much less supervision for training.</p>\n", "tags": ["Datasets","Supervised","Image-Retrieval"] },
{"key": "coleman2020similarity", "year": "2022", "citations": "16", "title":"Similarity Search for Efficient Active Learning and Search of Rare Concepts", "abstract": "<p>Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.</p>\n", "tags": ["Similarity-Search","Datasets","AAAI","Scalability","Large-Scale-Search","Evaluation","Efficiency"] },
{"key": "coleman2021graph", "year": "2021", "citations": "5", "title":"Graph Reordering for Cache-Efficient Near Neighbor Search", "abstract": "<p>Graph search is one of the most successful algorithmic trends in near\nneighbor search. Several of the most popular and empirically successful\nalgorithms are, at their core, a simple walk along a pruned near neighbor\ngraph. Such algorithms consistently perform at the top of industrial speed\nbenchmarks for applications such as embedding search. However, graph traversal\napplications often suffer from poor memory access patterns, and near neighbor\nsearch is no exception to this rule. Our measurements show that popular search\nindices such as the hierarchical navigable small-world graph (HNSW) can have\npoor cache miss performance. To address this problem, we apply graph reordering\nalgorithms to near neighbor graphs. Graph reordering is a memory layout\noptimization that groups commonly-accessed nodes together in memory. We present\nexhaustive experiments applying several reordering algorithms to a leading\ngraph-based near neighbor method based on the HNSW index. We find that\nreordering improves the query time by up to 40%, and we demonstrate that the\ntime needed to reorder the graph is negligible compared to the time required to\nconstruct the index.</p>\n", "tags": ["Graph-Based-ANN","Evaluation","Efficiency"] },
{"key": "connor2017high", "year": "2017", "citations": "19", "title":"High-Dimensional Simplexes for Supermetric Search", "abstract": "<p>In 1953, Blumenthal showed that every semi-metric space that is isometrically\nembeddable in a Hilbert space has the n-point property; we have previously\ncalled such spaces supermetric spaces. Although this is a strictly stronger\nproperty than triangle inequality, it is nonetheless closely related and many\nuseful metric spaces possess it. These include Euclidean, Cosine and\nJensen-Shannon spaces of any dimension. A simple corollary of the n-point\nproperty is that, for any (n+1) objects sampled from the space, there exists an\nn-dimensional simplex in Euclidean space whose edge lengths correspond to the\ndistances among the objects. We show how the construction of such simplexes in\nhigher dimensions can be used to give arbitrarily tight lower and upper bounds\non distances within the original space. This allows the construction of an\nn-dimensional Euclidean space, from which lower and upper bounds of the\noriginal space can be calculated, and which is itself an indexable space with\nthe n-point property. For similarity search, the engineering tradeoffs are\ngood: we show significant reductions in data size and metric cost with little\nloss of accuracy, leading to a significant overall improvement in search\nperformance.</p>\n", "tags": ["Similarity-Search","Evaluation"] },
{"key": "corbière2017leveraging", "year": "2017", "citations": "59", "title":"Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction", "abstract": "<p>In this paper, we present a method to learn a visual representation adapted\nfor e-commerce products. Based on weakly supervised learning, our model learns\nfrom noisy datasets crawled on e-commerce website catalogs and does not require\nany manual labeling. We show that our representation can be used for downward\nclassification tasks over clothing categories with different levels of\ngranularity. We also demonstrate that the learnt representation is suitable for\nimage retrieval. We achieve nearly state-of-art results on the DeepFashion\nIn-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without\nusing the provided training set.</p>\n", "tags": ["ICCV","Datasets","Supervised","Image-Retrieval"] },
{"key": "couairon2021embedding", "year": "2022", "citations": "13", "title":"Embedding Arithmetic of Multimodal Queries for Image Retrieval", "abstract": "<p>Latent text representations exhibit geometric regularities, such as the\nfamous analogy: queen is to king what woman is to man. Such structured semantic\nrelations were not demonstrated on image representations. Recent works aiming\nat bridging this semantic gap embed images and text into a multimodal space,\nenabling the transfer of text-defined transformations to the image modality. We\nintroduce the SIMAT dataset to evaluate the task of Image Retrieval with\nMultimodal queries. SIMAT contains 6k images and 18k textual transformation\nqueries that aim at either replacing scene elements or changing pairwise\nrelationships between scene elements. The goal is to retrieve an image\nconsistent with the (source image, text transformation) query. We use an\nimage/text matching oracle (OSCAR) to assess whether the image transformation\nis successful. The SIMAT dataset will be publicly available. We use SIMAT to\nevaluate the geometric properties of multimodal embedding spaces trained with\nan image/text matching objective, like CLIP. We show that vanilla CLIP\nembeddings are not very well suited to transform images with delta vectors, but\nthat a simple finetuning on the COCO dataset can bring dramatic improvements.\nWe also study whether it is beneficial to leverage pretrained universal\nsentence encoders (FastText, LASER and LaBSE).</p>\n", "tags": ["Datasets","CVPR","Image-Retrieval"] },
{"key": "csurka2014unsupervised", "year": "2014", "citations": "6", "title":"Unsupervised Visual and Textual Information Fusion in Multimedia Retrieval - A Graph-based Point of View", "abstract": "<p>Multimedia collections are more than ever growing in size and diversity.\nEffective multimedia retrieval systems are thus critical to access these\ndatasets from the end-user perspective and in a scalable way. We are interested\nin repositories of image/text multimedia objects and we study multimodal\ninformation fusion techniques in the context of content based multimedia\ninformation retrieval. We focus on graph based methods which have proven to\nprovide state-of-the-art performances. We particularly examine two of such\nmethods : cross-media similarities and random walk based scores. From a\ntheoretical viewpoint, we propose a unifying graph based framework which\nencompasses the two aforementioned approaches. Our proposal allows us to\nhighlight the core features one should consider when using a graph based\ntechnique for the combination of visual and textual information. We compare\ncross-media and random walk based results using three different real-world\ndatasets. From a practical standpoint, our extended empirical analysis allow us\nto provide insights and guidelines about the use of graph based methods for\nmultimodal information fusion in content based multimedia information\nretrieval.</p>\n", "tags": ["Unsupervised","Datasets","Tools-&-Libraries","Graph-Based-ANN"] },
{"key": "dadaneh2020pairwise", "year": "2020", "citations": "11", "title":"Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and Self-Control Gradient Estimator", "abstract": "<p>Semantic hashing has become a crucial component of fast similarity search in\nmany large-scale information retrieval systems, in particular, for text data.\nVariational auto-encoders (VAEs) with binary latent variables as hashing codes\nprovide state-of-the-art performance in terms of precision for document\nretrieval. We propose a pairwise loss function with discrete latent VAE to\nreward within-class similarity and between-class dissimilarity for supervised\nhashing. Instead of solving the optimization relying on existing biased\ngradient estimators, an unbiased low-variance gradient estimator is adopted to\noptimize the hashing function by evaluating the non-differentiable loss\nfunction over two correlated sets of binary hashing codes to control the\nvariance of gradient estimates. This new semantic hashing framework achieves\nsuperior performance compared to the state-of-the-arts, as demonstrated by our\ncomprehensive experiments.</p>\n", "tags": ["Similarity-Search","Supervised","Text-Retrieval","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","Scalability","Evaluation"] },
{"key": "dahlgaard2017fast", "year": "2017", "citations": "30", "title":"Fast Similarity Sketching", "abstract": "<p>We consider the \\(\\textit{Similarity Sketching}\\) problem: Given a universe\n\\([u] = \\{0,\\ldots, u-1\\}\\) we want a random function \\(S\\) mapping subsets\n\\(A\\subseteq [u]\\) into vectors \\(S(A)\\) of size \\(t\\), such that the Jaccard\nsimilarity \\(J(A,B) = |A\\cap B|/|A\\cup B|\\) between sets \\(A\\) and \\(B\\) is\npreserved. More precisely, define \\(X_i = [S(A)[i] =\n  S(B)[i]]\\) and \\(X = \\sum_{i\\in [t]} X_i\\). We want \\(E[X_i]=J(A,B)\\), and we want\n\\(X\\) to be strongly concentrated around \\(E[X] = t \\cdot J(A,B)\\) (i.e.\nChernoff-style bounds). This is a fundamental problem which has found numerous\napplications in data mining, large-scale classification, computer vision,\nsimilarity search, etc. via the classic MinHash algorithm. The vectors \\(S(A)\\)\nare also called \\(\\textit{sketches}\\). Strong concentration is critical, for\noften we want to sketch many sets \\(B_1,\\ldots,B_n\\) so that we later, for a\nquery set \\(A\\), can find (one of) the most similar \\(B_i\\). It is then critical\nthat no \\(B_i\\) looks much more similar to \\(A\\) due to errors in the sketch.\n  The seminal \\(t\\times\\textit{MinHash}\\) algorithm uses \\(t\\) random hash\nfunctions \\(h_1,\\ldots, h_t\\), and stores \\(\\left ( \\min_{a\\in A} h_1(A),\\ldots,\n\\min_{a\\in A} h_t(A) \\right )\\) as the sketch of \\(A\\). The main drawback of\nMinHash is, however, its \\(O(t\\cdot |A|)\\) running time, and finding a sketch\nwith similar properties and faster running time has been the subject of several\npapers. (continued…)</p>\n", "tags": ["Similarity-Search","Scalability","Locality-Sensitive-Hashing"] },
{"key": "dahlgaard2017practical", "year": "2017", "citations": "16", "title":"Practical Hash Functions for Similarity Estimation and Dimensionality Reduction", "abstract": "<p>Hashing is a basic tool for dimensionality reduction employed in several\naspects of machine learning. However, the perfomance analysis is often carried\nout under the abstract assumption that a truly random unit cost hash function\nis used, without concern for which concrete hash function is employed. The\nconcrete hash function may work fine on sufficiently random input. The question\nis if it can be trusted in the real world when faced with more structured\ninput.\n  In this paper we focus on two prominent applications of hashing, namely\nsimilarity estimation with the one permutation hashing (OPH) scheme of Li et\nal. [NIPS’12] and feature hashing (FH) of Weinberger et al. [ICML’09], both of\nwhich have found numerous applications, i.e. in approximate near-neighbour\nsearch with LSH and large-scale classification with SVM.\n  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS’15] which was\nproved to perform like a truly random hash function in many applications,\nincluding OPH. Here we first show improved concentration bounds for FH with\ntruly random hashing and then argue that mixed tabulation performs similar for\nsparse input. Our main contribution, however, is an experimental comparison of\ndifferent hashing schemes when used inside FH, OPH, and LSH.\n  We find that mixed tabulation hashing is almost as fast as the\nmultiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work\nwell on sufficiently random data, but we demonstrate that in the above\napplications, it can lead to bias and poor concentration on both real-world and\nsynthetic data. We also compare with the popular MurmurHash3, which has no\nproven guarantees. Mixed tabulation and MurmurHash3 both perform similar to\ntruly random hashing in our experiments. However, mixed tabulation is 40%\nfaster than MurmurHash3, and it has the proven guarantee of good performance on\nall possible input.</p>\n", "tags": ["Hashing-Methods","Scalability","Evaluation","Locality-Sensitive-Hashing"] },
{"key": "dai2017stochastic", "year": "2017", "citations": "73", "title":"Stochastic Generative Hashing", "abstract": "<p>Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.</p>\n", "tags": ["Hashing-Methods","Datasets","Scalability"] },
{"key": "dai2020convolutional", "year": "2020", "citations": "7", "title":"Convolutional Embedding for Edit Distance", "abstract": "<p>Edit-distance-based string similarity search has many applications such as\nspell correction, data de-duplication, and sequence alignment. However,\ncomputing edit distance is known to have high complexity, which makes string\nsimilarity search challenging for large datasets. In this paper, we propose a\ndeep learning pipeline (called CNN-ED) that embeds edit distance into Euclidean\ndistance for fast approximate similarity search. A convolutional neural network\n(CNN) is used to generate fixed-length vector embeddings for a dataset of\nstrings and the loss function is a combination of the triplet loss and the\napproximation error. To justify our choice of using CNN instead of other\nstructures (e.g., RNN) as the model, theoretical analysis is conducted to show\nthat some basic operations in our CNN model preserve edit distance.\nExperimental results show that CNN-ED outperforms data-independent CGK\nembedding and RNN-based GRU embedding in terms of both accuracy and efficiency\nby a large margin. We also show that string similarity search can be\nsignificantly accelerated using CNN-based embeddings, sometimes by orders of\nmagnitude.</p>\n", "tags": ["Similarity-Search","Distance-Metric-Learning","Datasets","SIGIR","Efficiency"] },
{"key": "dalins2019pdq", "year": "2019", "citations": "5", "title":"PDQ & TMK + PDQF -- A Test Drive of Facebook's Perceptual Hashing Algorithms", "abstract": "<p>Efficient and reliable automated detection of modified image and multimedia\nfiles has long been a challenge for law enforcement, compounded by the harm\ncaused by repeated exposure to psychologically harmful materials. In August\n2019 Facebook open-sourced their PDQ and TMK + PDQF algorithms for image and\nvideo similarity measurement, respectively. In this report, we review the\nalgorithms’ performance on detecting commonly encountered transformations on\nreal-world case data, sourced from contemporary investigations. We also provide\na reference implementation to demonstrate the potential application and\nintegration of such algorithms within existing law enforcement systems.</p>\n", "tags": ["Hashing-Methods","Survey-Paper","Evaluation"] },
{"key": "dash2020open", "year": "2021", "citations": "12", "title":"Open Knowledge Graphs Canonicalization using Variational Autoencoders", "abstract": "<p>Noun phrases and Relation phrases in open knowledge graphs are not\ncanonicalized, leading to an explosion of redundant and ambiguous\nsubject-relation-object triples. Existing approaches to solve this problem take\na two-step approach. First, they generate embedding representations for both\nnoun and relation phrases, then a clustering algorithm is used to group them\nusing the embeddings as features. In this work, we propose Canonicalizing Using\nVariational Autoencoders (CUVA), a joint model to learn both embeddings and\ncluster assignments in an end-to-end approach, which leads to a better vector\nrepresentation for the noun and relation phrases. Our evaluation over multiple\nbenchmarks shows that CUVA outperforms the existing state-of-the-art\napproaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate\nentity canonicalization systems.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "datar2004locality", "year": "2004", "citations": "2887", "title":"Locality-sensitive hashing scheme based on p-stable distributions", "abstract": "<p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain “bounded growth” condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Tree-Based-ANN","Efficiency"] },
{"key": "datar2025locality", "year": "2004", "citations": "2887", "title":"Locality-sensitive hashing scheme based on p-stable distributions", "abstract": "<p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain “bounded growth” condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Tree-Based-ANN","Efficiency"] },
{"key": "deng2017learning", "year": "2017", "citations": "37", "title":"Learning Deep Similarity Models with Focus Ranking for Fabric Image Retrieval", "abstract": "<p>Fabric image retrieval is beneficial to many applications including clothing\nsearching, online shopping and cloth modeling. Learning pairwise image\nsimilarity is of great importance to an image retrieval task. With the\nresurgence of Convolutional Neural Networks (CNNs), recent works have achieved\nsignificant progresses via deep representation learning with metric embedding,\nwhich drives similar examples close to each other in a feature space, and\ndissimilar ones apart from each other. In this paper, we propose a novel\nembedding method termed focus ranking that can be easily unified into a CNN for\njointly learning image representations and metrics in the context of\nfine-grained fabric image retrieval. Focus ranking aims to rank similar\nexamples higher than all dissimilar ones by penalizing ranking disorders via\nthe minimization of the overall cost attributed to similar samples being ranked\nbelow dissimilar ones. At the training stage, training samples are organized\ninto focus ranking units for efficient optimization. We build a large-scale\nfabric image retrieval dataset (FIRD) with about 25,000 images of 4,300\nfabrics, and test the proposed model on the FIRD dataset. Experimental results\nshow the superiority of the proposed model over existing metric embedding\nmodels.</p>\n", "tags": ["Datasets","Scalability","Image-Retrieval"] },
{"key": "deng2019triplet", "year": "2018", "citations": "378", "title":"Triplet-Based Deep Hashing Network for Cross-Modal Retrieval", "abstract": "<p>Given the benefits of its low storage requirements and high retrieval\nefficiency, hashing has recently received increasing attention. In\nparticular,cross-modal hashing has been widely and successfully used in\nmultimedia similarity search applications. However, almost all existing methods\nemploying cross-modal hashing cannot obtain powerful hash codes due to their\nignoring the relative similarity between heterogeneous data that contains\nricher semantic information, leading to unsatisfactory retrieval performance.\nIn this paper, we propose a triplet-based deep hashing (TDH) network for\ncross-modal retrieval. First, we utilize the triplet labels, which describes\nthe relative relationships among three instances as supervision in order to\ncapture more general semantic correlations between cross-modal instances. We\nthen establish a loss function from the inter-modal view and the intra-modal\nview to boost the discriminative abilities of the hash codes. Finally, graph\nregularization is introduced into our proposed TDH method to preserve the\noriginal semantic similarity between hash codes in Hamming space. Experimental\nresults show that our proposed method outperforms several state-of-the-art\napproaches on two popular cross-modal datasets.</p>\n", "tags": ["Similarity-Search","Hashing-Methods","Datasets","Neural-Hashing","Efficiency","Evaluation","Multimodal-Retrieval"] },
{"key": "deng2019two", "year": "2019", "citations": "75", "title":"Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","Neural-Hashing","Tools-&-Libraries","Hashing-Methods","Supervised"] },
{"key": "deng2025two", "year": "2019", "citations": "75", "title":"Two-Stream Deep Hashing With Class-Specific Centers for Supervised Image Search", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","Neural-Hashing","Tools-&-Libraries","Hashing-Methods","Supervised"] },
{"key": "dey2018learning", "year": "2018", "citations": "24", "title":"Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch", "abstract": "<p>In this work we introduce a cross modal image retrieval system that allows\nboth text and sketch as input modalities for the query. A cross-modal deep\nnetwork architecture is formulated to jointly model the sketch and text input\nmodalities as well as the the image output modality, learning a common\nembedding between text and images and between sketches and images. In addition,\nan attention model is used to selectively focus the attention on the different\nobjects of the image, allowing for retrieval with multiple objects in the\nquery. Experiments show that the proposed method performs the best in both\nsingle and multiple object image retrieval in standard datasets.</p>\n", "tags": ["Datasets","Image-Retrieval"] },
{"key": "ding2014collective", "year": "2014", "citations": "641", "title":"Collective Matrix Factorization Hashing for Multimodal data", "abstract": "<p>Nearest neighbor search methods based on hashing have\nattracted considerable attention for effective and efficient\nlarge-scale similarity search in computer vision and information\nretrieval community. In this paper, we study the\nproblems of learning hash functions in the context of multimodal\ndata for cross-view similarity search. We put forward\na novel hashing method, which is referred to Collective\nMatrix Factorization Hashing (CMFH). CMFH learns unified\nhash codes by collective matrix factorization with latent\nfactor model from different modalities of one instance,\nwhich can not only supports cross-view search but also increases\nthe search accuracy by merging multiple view information\nsources. We also prove that CMFH, a similaritypreserving\nhashing learning method, has upper and lower\nboundaries. Extensive experiments verify that CMFH significantly\noutperforms several state-of-the-art methods on\nthree different datasets.</p>\n", "tags": ["Scalability","Datasets","CVPR","Similarity-Search","Hashing-Methods"] },
{"key": "ding2015knn", "year": "2015", "citations": "14", "title":"kNN Hashing with Factorized Neighborhood Representation", "abstract": "<p>Hashing is very effective for many tasks in reducing the\nprocessing time and in compressing massive databases. Although lots of approaches have been developed to learn\ndata-dependent hash functions in recent years, how to learn\nhash functions to yield good performance with acceptable\ncomputational and memory cost is still a challenging problem. Based on the observation that retrieval precision is\nhighly related to the kNN classification accuracy, this paper\nproposes a novel kNN-based supervised hashing method,\nwhich learns hash functions by directly maximizing the kNN\naccuracy of the Hamming-embedded training data. To make\nit scalable well to large problem, we propose a factorized\nneighborhood representation to parsimoniously model the\nneighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,\nwe further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn\naccurate hashing functions with tolerable computation and\nstorage cost. Experiments on four benchmarks demonstrate\nthat our method outperforms the state-of-the-arts.</p>\n", "tags": ["ICCV","Neural-Hashing","Memory-Efficiency","Hashing-Methods","Evaluation","Supervised"] },
{"key": "ding2019bilinear", "year": "2019", "citations": "14", "title":"Bilinear Supervised Hashing Based on 2D Image Features", "abstract": "<p>Hashing has been recognized as an efficient representation learning method to\neffectively handle big data due to its low computational complexity and memory\ncost. Most of the existing hashing methods focus on learning the\nlow-dimensional vectorized binary features based on the high-dimensional raw\nvectorized features. However, studies on how to obtain preferable binary codes\nfrom the original 2D image features for retrieval is very limited. This paper\nproposes a bilinear supervised discrete hashing (BSDH) method based on 2D image\nfeatures which utilizes bilinear projections to binarize the image matrix\nfeatures such that the intrinsic characteristics in the 2D image space are\npreserved in the learned binary codes. Meanwhile, the bilinear projection\napproximation and vectorization binary codes regression are seamlessly\nintegrated together to formulate the final robust learning framework.\nFurthermore, a discrete optimization strategy is developed to alternatively\nupdate each variable for obtaining the high-quality binary codes. In addition,\ntwo 2D image features, traditional SURF-based FVLAD feature and CNN-based\nAlexConv5 feature are designed for further improving the performance of the\nproposed BSDH method. Results of extensive experiments conducted on four\nbenchmark datasets show that the proposed BSDH method almost outperforms all\ncompeting hashing methods with different input features by different evaluation\nprotocols.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Evaluation"] },
{"key": "ding2025collective", "year": "2014", "citations": "641", "title":"Collective Matrix Factorization Hashing for Multimodal data", "abstract": "<p>Nearest neighbor search methods based on hashing have\nattracted considerable attention for effective and efficient\nlarge-scale similarity search in computer vision and information\nretrieval community. In this paper, we study the\nproblems of learning hash functions in the context of multimodal\ndata for cross-view similarity search. We put forward\na novel hashing method, which is referred to Collective\nMatrix Factorization Hashing (CMFH). CMFH learns unified\nhash codes by collective matrix factorization with latent\nfactor model from different modalities of one instance,\nwhich can not only supports cross-view search but also increases\nthe search accuracy by merging multiple view information\nsources. We also prove that CMFH, a similaritypreserving\nhashing learning method, has upper and lower\nboundaries. Extensive experiments verify that CMFH significantly\noutperforms several state-of-the-art methods on\nthree different datasets.</p>\n", "tags": ["Scalability","Datasets","CVPR","Similarity-Search","Hashing-Methods"] },
{"key": "ding2025knn", "year": "2015", "citations": "14", "title":"kNN Hashing with Factorized Neighborhood Representation", "abstract": "<p>Hashing is very effective for many tasks in reducing the\nprocessing time and in compressing massive databases. Although lots of approaches have been developed to learn\ndata-dependent hash functions in recent years, how to learn\nhash functions to yield good performance with acceptable\ncomputational and memory cost is still a challenging problem. Based on the observation that retrieval precision is\nhighly related to the kNN classification accuracy, this paper\nproposes a novel kNN-based supervised hashing method,\nwhich learns hash functions by directly maximizing the kNN\naccuracy of the Hamming-embedded training data. To make\nit scalable well to large problem, we propose a factorized\nneighborhood representation to parsimoniously model the\nneighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,\nwe further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn\naccurate hashing functions with tolerable computation and\nstorage cost. Experiments on four benchmarks demonstrate\nthat our method outperforms the state-of-the-arts.</p>\n", "tags": ["ICCV","Neural-Hashing","Memory-Efficiency","Hashing-Methods","Evaluation","Supervised"] },
{"key": "do2016binary", "year": "2016", "citations": "20", "title":"Binary Hashing with Semidefinite Relaxation and Augmented Lagrangian", "abstract": "<p>This paper proposes two approaches for inferencing binary codes in two-step\n(supervised, unsupervised) hashing. We first introduce an unified formulation\nfor both supervised and unsupervised hashing. Then, we cast the learning of one\nbit as a Binary Quadratic Problem (BQP). We propose two approaches to solve\nBQP. In the first approach, we relax BQP as a semidefinite programming problem\nwhich its global optimum can be achieved. We theoretically prove that the\nobjective value of the binary solution achieved by this approach is well\nbounded. In the second approach, we propose an augmented Lagrangian based\napproach to solve BQP directly without relaxing the binary constraint.\nExperimental results on three benchmark datasets show that our proposed methods\ncompare favorably with the state of the art.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Unsupervised","Evaluation"] },
{"key": "do2016embedding", "year": "2017", "citations": "37", "title":"Embedding based on function approximation for large scale image search", "abstract": "<p>The objective of this paper is to design an embedding method that maps local\nfeatures describing an image (e.g. SIFT) to a higher dimensional representation\nuseful for the image retrieval problem. First, motivated by the relationship\nbetween the linear approximation of a nonlinear function in high dimensional\nspace and the stateof-the-art feature representation used in image retrieval,\ni.e., VLAD, we propose a new approach for the approximation. The embedded\nvectors resulted by the function approximation process are then aggregated to\nform a single representation for image retrieval. Second, in order to make the\nproposed embedding method applicable to large scale problem, we further derive\nits fast version in which the embedded vectors can be efficiently computed,\ni.e., in the closed-form. We compare the proposed embedding methods with the\nstate of the art in the context of image search under various settings: when\nthe images are represented by medium length vectors, short vectors, or binary\nvectors. The experimental results show that the proposed embedding methods\noutperform existing the state of the art on the standard public image retrieval\nbenchmarks.</p>\n", "tags": ["Image-Retrieval"] },
{"key": "do2016learning", "year": "2016", "citations": "169", "title":"Learning to Hash with Binary Deep Neural Network", "abstract": "<p>This work proposes deep network models and learning algorithms for\nunsupervised and supervised binary hashing. Our novel network design constrains\none hidden layer to directly output the binary codes. This addresses a\nchallenging issue in some previous works: optimizing non-smooth objective\nfunctions due to binarization. Moreover, we incorporate independence and\nbalance properties in the direct and strict forms in the learning. Furthermore,\nwe include similarity preserving property in our objective function. Our\nresulting optimization with these binary, independence, and balance constraints\nis difficult to solve. We propose to attack it with alternating optimization\nand careful relaxation. Experimental results on three benchmark datasets show\nthat our proposed methods compare favorably with the state of the art.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Compact-Codes","Unsupervised","Evaluation"] },
{"key": "do2017compact", "year": "2019", "citations": "23", "title":"Compact Hash Code Learning with Binary Deep Neural Network", "abstract": "<p>Learning compact binary codes for image retrieval problem using deep neural\nnetworks has recently attracted increasing attention. However, training deep\nhashing networks is challenging due to the binary constraints on the hash\ncodes. In this paper, we propose deep network models and learning algorithms\nfor learning binary hash codes given image representations under both\nunsupervised and supervised manners. The novelty of our network design is that\nwe constrain one hidden layer to directly output the binary codes. This design\nhas overcome a challenging problem in some previous works: optimizing\nnon-smooth objective functions because of binarization. In addition, we propose\nto incorporate independence and balance properties in the direct and strict\nforms into the learning schemes. We also include a similarity preserving\nproperty in our objective functions. The resulting optimizations involving\nthese binary, independence, and balance constraints are difficult to solve. To\ntackle this difficulty, we propose to learn the networks with alternating\noptimization and careful relaxation. Furthermore, by leveraging the powerful\ncapacity of convolutional neural networks, we propose an end-to-end\narchitecture that jointly learns to extract visual features and produce binary\nhash codes. Experimental results for the benchmark datasets show that the\nproposed methods compare favorably or outperform the state of the art.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Compact-Codes","Unsupervised","Evaluation"] },
{"key": "do2017simultaneous", "year": "2017", "citations": "38", "title":"Simultaneous Feature Aggregating and Hashing for Large-scale Image Search", "abstract": "<p>In most state-of-the-art hashing-based visual search systems, local image\ndescriptors of an image are first aggregated as a single feature vector. This\nfeature vector is then subjected to a hashing function that produces a binary\nhash code. In previous work, the aggregating and the hashing processes are\ndesigned independently. In this paper, we propose a novel framework where\nfeature aggregating and hashing are designed simultaneously and optimized\njointly. Specifically, our joint optimization produces aggregated\nrepresentations that can be better reconstructed by some binary codes. This\nleads to more discriminative binary hash codes and improved retrieval accuracy.\nIn addition, we also propose a fast version of the recently-proposed Binary\nAutoencoder to be used in our proposed framework. We perform extensive\nretrieval experiments on several benchmark datasets with both SIFT and\nconvolutional features. Our results suggest that the proposed framework\nachieves significant improvements over the state of the art.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Compact-Codes","CVPR","Scalability","Evaluation"] },
{"key": "do2018binary", "year": "2019", "citations": "9", "title":"Binary Constrained Deep Hashing Network for Image Retrieval without Manual Annotation", "abstract": "<p>Learning compact binary codes for image retrieval task using deep neural\nnetworks has attracted increasing attention recently. However, training deep\nhashing networks for the task is challenging due to the binary constraints on\nthe hash codes, the similarity preserving property, and the requirement for a\nvast amount of labelled images. To the best of our knowledge, none of the\nexisting methods has tackled all of these challenges completely in a unified\nframework. In this work, we propose a novel end-to-end deep learning approach\nfor the task, in which the network is trained to produce binary codes directly\nfrom image pixels without the need of manual annotation. In particular, to deal\nwith the non-smoothness of binary constraints, we propose a novel pairwise\nconstrained loss function, which simultaneously encodes the distances between\npairs of hash codes, and the binary quantization error. In order to train the\nnetwork with the proposed loss function, we propose an efficient parameter\nlearning algorithm. In addition, to provide similar / dissimilar training\nimages to train the network, we exploit 3D models reconstructed from unlabelled\nimages for automatic generation of enormous training image pairs. The extensive\nexperiments on image retrieval benchmark datasets demonstrate the improvements\nof the proposed method over the state-of-the-art compact representation methods\non the image retrieval problem.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Quantization","Evaluation"] },
{"key": "do2018selective", "year": "2019", "citations": "30", "title":"From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval", "abstract": "<p>In the large-scale image retrieval task, the two most important requirements\nare the discriminability of image representations and the efficiency in\ncomputation and storage of representations. Regarding the former requirement,\nConvolutional Neural Network (CNN) is proven to be a very powerful tool to\nextract highly discriminative local descriptors for effective image search.\nAdditionally, in order to further improve the discriminative power of the\ndescriptors, recent works adopt fine-tuned strategies. In this paper, taking a\ndifferent approach, we propose a novel, computationally efficient, and\ncompetitive framework. Specifically, we firstly propose various strategies to\ncompute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and eliminate redundant\nfeatures. Our in-depth analyses demonstrate that proposed masking schemes are\neffective to address the burstiness drawback and improve retrieval accuracy.\nSecondly, we propose to employ recent embedding and aggregating methods which\ncan significantly boost the feature discriminability. Regarding the computation\nand storage efficiency, we include a hashing module to produce very compact\nbinary image representations. Extensive experiments on six image retrieval\nbenchmarks demonstrate that our proposed framework achieves the\nstate-of-the-art retrieval performances.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Scalability","Efficiency"] },
{"key": "do2019simultaneous", "year": "2019", "citations": "14", "title":"Simultaneous Feature Aggregating and Hashing for Compact Binary Code Learning", "abstract": "<p>Representing images by compact hash codes is an attractive approach for\nlarge-scale content-based image retrieval. In most state-of-the-art\nhashing-based image retrieval systems, for each image, local descriptors are\nfirst aggregated as a global representation vector. This global vector is then\nsubjected to a hashing function to generate a binary hash code. In previous\nworks, the aggregating and the hashing processes are designed independently.\nHence these frameworks may generate suboptimal hash codes. In this paper, we\nfirst propose a novel unsupervised hashing framework in which feature\naggregating and hashing are designed simultaneously and optimized jointly.\nSpecifically, our joint optimization generates aggregated representations that\ncan be better reconstructed by some binary codes. This leads to more\ndiscriminative binary hash codes and improved retrieval accuracy. In addition,\nthe proposed method is flexible. It can be extended for supervised hashing.\nWhen the data label is available, the framework can be adapted to learn binary\ncodes which minimize the reconstruction loss w.r.t. label vectors. Furthermore,\nwe also propose a fast version of the state-of-the-art hashing method Binary\nAutoencoder to be used in our proposed frameworks. Extensive experiments on\nbenchmark datasets under various settings show that the proposed methods\noutperform state-of-the-art unsupervised and supervised hashing methods.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Unsupervised","Scalability","Evaluation"] },
{"key": "dolhansky2020adversarial", "year": "2020", "citations": "9", "title":"Adversarial collision attacks on image hashing functions", "abstract": "<p>Hashing images with a perceptual algorithm is a common approach to solving\nduplicate image detection problems. However, perceptual image hashing\nalgorithms are differentiable, and are thus vulnerable to gradient-based\nadversarial attacks. We demonstrate that not only is it possible to modify an\nimage to produce an unrelated hash, but an exact image hash collision between a\nsource and target image can be produced via minuscule adversarial\nperturbations. In a white box setting, these collisions can be replicated\nacross nearly every image pair and hash type (including both deep and\nnon-learned hashes). Furthermore, by attacking points other than the output of\na hashing function, an attacker can avoid having to know the details of a\nparticular algorithm, resulting in collisions that transfer across different\nhash sizes or model architectures. Using these techniques, an adversary can\npoison the image lookup table of a duplicate image detection service, resulting\nin undefined or unwanted behavior. Finally, we offer several potential\nmitigations to gradient-based image hash attacks.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval","Robustness"] },
{"key": "dong2017video", "year": "2018", "citations": "17", "title":"Video retrieval based on deep convolutional neural network", "abstract": "<p>Recently, with the enormous growth of online videos, fast video retrieval\nresearch has received increasing attention. As an extension of image hashing\ntechniques, traditional video hashing methods mainly depend on hand-crafted\nfeatures and transform the real-valued features into binary hash codes. As\nvideos provide far more diverse and complex visual information than images,\nextracting features from videos is much more challenging than that from images.\nTherefore, high-level semantic features to represent videos are needed rather\nthan low-level hand-crafted methods. In this paper, a deep convolutional neural\nnetwork is proposed to extract high-level semantic features and a binary hash\nfunction is then integrated into this framework to achieve an end-to-end\noptimization. Particularly, our approach also combines triplet loss function\nwhich preserves the relative similarity and difference of videos and\nclassification loss function as the optimization objective. Experiments have\nbeen performed on two public datasets and the results demonstrate the\nsuperiority of our proposed method compared with other state-of-the-art video\nretrieval methods.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Datasets","Video-Retrieval"] },
{"key": "dong2019document", "year": "2019", "citations": "19", "title":"Document Hashing with Mixture-Prior Generative Models", "abstract": "<p>Hashing is promising for large-scale information retrieval tasks thanks to\nthe efficiency of distance evaluation between binary codes. Generative hashing\nis often used to generate hashing codes in an unsupervised way. However,\nexisting generative hashing methods only considered the use of simple priors,\nlike Gaussian and Bernoulli priors, which limits these methods to further\nimprove their performance. In this paper, two mixture-prior generative models\nare proposed, under the objective to produce high-quality hashing codes for\ndocuments. Specifically, a Gaussian mixture prior is first imposed onto the\nvariational auto-encoder (VAE), followed by a separate step to cast the\ncontinuous latent representation of VAE into binary code. To avoid the\nperformance loss caused by the separate casting, a model using a Bernoulli\nmixture prior is further developed, in which an end-to-end training is admitted\nby resorting to the straight-through (ST) discrete gradient estimator.\nExperimental results on several benchmark datasets demonstrate that the\nproposed methods, especially the one using Bernoulli mixture priors,\nconsistently outperform existing ones by a substantial margin.</p>\n", "tags": ["Hashing-Methods","Datasets","Compact-Codes","EMNLP","Unsupervised","Scalability","Evaluation","Efficiency"] },
{"key": "dong2019learning", "year": "2020", "citations": "26", "title":"Learning Space Partitions for Nearest Neighbor Search", "abstract": "<p>Space partitions of \\(\\mathbb{R}^d\\) underlie a vast and important class of\nfast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for building space\npartitions reducing the problem to balanced graph partitioning followed by\nsupervised classification. We instantiate this general approach with the KaHIP\ngraph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,\nto obtain a new partitioning procedure called Neural Locality-Sensitive Hashing\n(Neural LSH). On several standard benchmarks for NNS, our experiments show that\nthe partitions obtained by Neural LSH consistently outperform partitions found\nby quantization-based and tree-based methods as well as classic, data-oblivious\nLSH.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Locality-Sensitive-Hashing","Hashing-Methods","Quantization","Tree-Based-ANN"] },
{"key": "dong2020learning", "year": "2020", "citations": "26", "title":"Learning Space Partitions for Nearest Neighbor Search", "abstract": "<p>Space partitions of underlie a vast and important\nclass of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.\nWe instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>\n", "tags": ["Tree-Based-ANN","Locality-Sensitive-Hashing","Tools-&-Libraries","Quantization","Hashing-Methods","Supervised"] },
{"key": "dong2020using", "year": "2021", "citations": "5", "title":"Using Text to Teach Image Retrieval", "abstract": "<p>Image retrieval relies heavily on the quality of the data modeling and the\ndistance measurement in the feature space. Building on the concept of image\nmanifold, we first propose to represent the feature space of images, learned\nvia neural networks, as a graph. Neighborhoods in the feature space are now\ndefined by the geodesic distance between images, represented as graph vertices\nor manifold samples. When limited images are available, this manifold is\nsparsely sampled, making the geodesic computation and the corresponding\nretrieval harder. To address this, we augment the manifold samples with\ngeometrically aligned text, thereby using a plethora of sentences to teach us\nabout images. In addition to extensive results on standard datasets\nillustrating the power of text to help in image retrieval, a new public dataset\nbased on CLEVR is introduced to quantify the semantic similarity between visual\ndata and text data. The experimental results show that the joint embedding\nmanifold is a robust representation, allowing it to be a better basis to\nperform image retrieval given only an image and a textual instruction on the\ndesired modifications over the image</p>\n", "tags": ["Datasets","CVPR","Image-Retrieval"] },
{"key": "dong2025learning", "year": "2020", "citations": "26", "title":"Learning Space Partitions for Nearest Neighbor Search", "abstract": "<p>Space partitions of underlie a vast and important\nclass of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.\nWe instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>\n", "tags": ["Tree-Based-ANN","Locality-Sensitive-Hashing","Tools-&-Libraries","Quantization","Hashing-Methods","Supervised"] },
{"key": "doras2019cover", "year": "2019", "citations": "22", "title":"Cover Detection using Dominant Melody Embeddings", "abstract": "<p>Automatic cover detection – the task of finding in an audio database all the\ncovers of one or several query tracks – has long been seen as a challenging\ntheoretical problem in the MIR community and as an acute practical problem for\nauthors and composers societies. Original algorithms proposed for this task\nhave proven their accuracy on small datasets, but are unable to scale up to\nmodern real-life audio corpora. On the other hand, faster approaches designed\nto process thousands of pairwise comparisons resulted in lower accuracy, making\nthem unsuitable for practical use.\n  In this work, we propose a neural network architecture that is trained to\nrepresent each track as a single embedding vector. The computation burden is\ntherefore left to the embedding extraction – that can be conducted offline and\nstored, while the pairwise comparison task reduces to a simple Euclidean\ndistance computation. We further propose to extract each track’s embedding out\nof its dominant melody representation, obtained by another neural network\ntrained for this task. We then show that this architecture improves\nstate-of-the-art accuracy both on small and large datasets, and is able to\nscale to query databases of thousands of tracks in a few seconds.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "dorfer2017end", "year": "2018", "citations": "32", "title":"End-to-End Cross-Modality Retrieval with CCA Projections and Pairwise Ranking Loss", "abstract": "<p>Cross-modality retrieval encompasses retrieval tasks where the fetched items\nare of a different type than the search query, e.g., retrieving pictures\nrelevant to a given text query. The state-of-the-art approach to cross-modality\nretrieval relies on learning a joint embedding space of the two modalities,\nwhere items from either modality are retrieved using nearest-neighbor search.\nIn this work, we introduce a neural network layer based on Canonical\nCorrelation Analysis (CCA) that learns better embedding spaces by analytically\ncomputing projections that maximize correlation. In contrast to previous\napproaches, the CCA Layer (CCAL) allows us to combine existing objectives for\nembedding space learning, such as pairwise ranking losses, with the optimal\nprojections of CCA. We show the effectiveness of our approach for\ncross-modality retrieval on three different scenarios (text-to-image,\naudio-sheet-music and zero-shot retrieval), surpassing both Deep CCA and a\nmulti-view network using freely learned projections optimized by a pairwise\nranking loss, especially when little training data is available (the code for\nall three methods is released at: https://github.com/CPJKU/cca_layer).</p>\n", "tags": ["Text-Retrieval","Few-Shot-&-Zero-Shot"] },
{"key": "dou2020learning", "year": "2020", "citations": "7", "title":"Learning Global and Local Consistent Representations for Unsupervised Image Retrieval via Deep Graph Diffusion Networks", "abstract": "<p>Diffusion has shown great success in improving accuracy of unsupervised image\nretrieval systems by utilizing high-order structures of image manifold.\nHowever, existing diffusion methods suffer from three major limitations: 1)\nthey usually rely on local structures without considering global manifold\ninformation; 2) they focus on improving pair-wise similarities within existing\nimages input output transductively while lacking flexibility to learn\nrepresentations for novel unseen instances inductively; 3) they fail to scale\nto large datasets due to prohibitive memory consumption and computational\nburden due to intrinsic high-order operations on the whole graph. In this\npaper, to address these limitations, we propose a novel method, Graph Diffusion\nNetworks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant\nof deep learning algorithms on irregular graphs. GRAD-Net learns semantic\nrepresentations by exploiting both local and global structures of image\nmanifold in an unsupervised fashion. By utilizing sparse coding techniques,\nGRAD-Net not only preserves global information on the image manifold, but also\nenables scalable training and efficient querying. Experiments on several large\nbenchmark datasets demonstrate effectiveness of our method over\nstate-of-the-art diffusion algorithms for unsupervised image retrieval.</p>\n", "tags": ["Unsupervised","Datasets","Evaluation","Image-Retrieval"] },
{"key": "douze2016polysemous", "year": "2016", "citations": "45", "title":"Polysemous codes", "abstract": "<p>This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90’s to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine.</p>\n", "tags": ["Vector-Indexing","Datasets","Compact-Codes","Quantization","Evaluation","Efficiency"] },
{"key": "douze2024faiss", "year": "2024", "citations": "17", "title":"The Faiss library", "abstract": "<p>Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.</p>\n", "tags": ["Similarity-Search","Evaluation","Tools-&-Libraries"] },
{"key": "driemel2017locality", "year": "2017", "citations": "31", "title":"Locality-sensitive hashing of curves", "abstract": "<p>We study data structures for storing a set of polygonal curves in \\({\\rm R}^d\\)\nsuch that, given a query curve, we can efficiently retrieve similar curves from\nthe set, where similarity is measured using the discrete Fr'echet distance or\nthe dynamic time warping distance. To this end we devise the first\nlocality-sensitive hashing schemes for these distance measures. A major\nchallenge is posed by the fact that these distance measures internally optimize\nthe alignment between the curves. We give solutions for different types of\nalignments including constrained and unconstrained versions. For unconstrained\nalignments, we improve over a result by Indyk from 2002 for short curves. Let\n\\(n\\) be the number of input curves and let \\(m\\) be the maximum complexity of a\ncurve in the input. In the particular case where \\(m \\leq \\frac{\\alpha}{4d} log\nn\\), for some fixed \\(\\alpha&gt;0\\), our solutions imply an approximate near-neighbor\ndata structure for the discrete Fr'echet distance that uses space in\n\\(O(n^{1+\\alpha}log n)\\) and achieves query time in \\(O(n^{\\alpha}log^2 n)\\) and\nconstant approximation factor. Furthermore, our solutions provide a trade-off\nbetween approximation quality and computational performance: for any parameter\n\\(k \\in [m]\\), we can give a data structure that uses space in \\(O(2^{2k}m^{k-1} n\nlog n + nm)\\), answers queries in \\(O( 2^{2k} m^{k}log n)\\) time and achieves\napproximation factor in \\(O(m/k)\\).</p>\n", "tags": ["Hashing-Methods","Evaluation","Efficiency"] },
{"key": "dubey2020decade", "year": "2021", "citations": "242", "title":"A Decade Survey of Content Based Image Retrieval using Deep Learning", "abstract": "<p>The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.</p>\n", "tags": ["Datasets","Survey-Paper","Evaluation","Image-Retrieval"] },
{"key": "dubey2021vision", "year": "2022", "citations": "47", "title":"Vision Transformer Hashing for Image Retrieval", "abstract": "<p>Deep learning has shown a tremendous growth in hashing techniques for image\nretrieval. Recently, Transformer has emerged as a new architecture by utilizing\nself-attention without convolution. Transformer is also extended to Vision\nTransformer (ViT) for the visual recognition with a promising performance on\nImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)\nfor image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone\nnetwork and add the hashing head. The proposed VTS model is fine tuned for\nhashing under six different image retrieval frameworks, including Deep\nSupervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network\n(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)\nwith their objective functions. We perform the extensive experiments on\nCIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image\nretrieval outperforms the recent state-of-the-art hashing techniques with a\ngreat margin. We also find the proposed VTS model as the backbone network is\nbetter than the existing networks, such as AlexNet and ResNet. The code is\nreleased at https://github.com/shivram1987/VisionTransformerHashing.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Quantization","Evaluation"] },
{"key": "ducau2019automatic", "year": "2019", "citations": "11", "title":"Automatic Malware Description via Attribute Tagging and Similarity Embedding", "abstract": "<p>With the rapid proliferation and increased sophistication of malicious\nsoftware (malware), detection methods no longer rely only on manually generated\nsignatures but have also incorporated more general approaches like machine\nlearning detection. Although powerful for conviction of malicious artifacts,\nthese methods do not produce any further information about the type of threat\nthat has been detected neither allows for identifying relationships between\nmalware samples. In this work, we address the information gap between machine\nlearning and signature-based detection methods by learning a representation\nspace for malware samples in which files with similar malicious behaviors\nappear close to each other. We do so by introducing a deep learning based\ntagging model trained to generate human-interpretable semantic descriptions of\nmalicious software, which, at the same time provides potentially more useful\nand flexible information than malware family names.\n  We show that the malware descriptions generated with the proposed approach\ncorrectly identify more than 95% of eleven possible tag descriptions for a\ngiven sample, at a deployable false positive rate of 1% per tag. Furthermore,\nwe use the learned representation space to introduce a similarity index between\nmalware files, and empirically demonstrate using dynamic traces from files’\nexecution, that is not only more effective at identifying samples from the same\nfamilies, but also 32 times smaller than those based on raw feature vectors.</p>\n", "tags": ["Uncategorized"] },
{"key": "dutta2017stochastic", "year": "2018", "citations": "19", "title":"Stochastic Graphlet Embedding", "abstract": "<p>Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of –\nexplicit/implicit – graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.</p>\n", "tags": ["Hashing-Methods","Evaluation","Graph-Based-ANN"] },
{"key": "eghbali2016fast", "year": "2018", "citations": "15", "title":"Fast Cosine Similarity Search in Binary Space with Angular Multi-index Hashing", "abstract": "<p>Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find \\(K\\) codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular \\(K\\) nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.</p>\n", "tags": ["Similarity-Search","Vector-Indexing","Hashing-Methods","Distance-Metric-Learning","Datasets","Compact-Codes"] },
{"key": "eghbali2019deep", "year": "2019", "citations": "27", "title":"Deep Spherical Quantization for Image Search", "abstract": "<p>Hashing methods, which encode high-dimensional images with compact discrete\ncodes, have been widely applied to enhance large-scale image retrieval. In this\npaper, we put forward Deep Spherical Quantization (DSQ), a novel method to make\ndeep convolutional neural networks generate supervised and compact binary codes\nfor efficient image search. Our approach simultaneously learns a mapping that\ntransforms the input images into a low-dimensional discriminative space, and\nquantizes the transformed data points using multi-codebook quantization. To\neliminate the negative effect of norm variance on codebook learning, we force\nthe network to L_2 normalize the extracted features and then quantize the\nresulting vectors using a new supervised quantization technique specifically\ndesigned for points lying on a unit hypersphere. Furthermore, we introduce an\neasy-to-implement extension of our quantization technique that enforces\nsparsity on the codebooks. Extensive experiments demonstrate that DSQ and its\nsparse variant can generate semantically separable compact binary codes\noutperforming many state-of-the-art image retrieval methods on three\nbenchmarks.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Quantization","Compact-Codes","CVPR","Scalability"] },
{"key": "elezi2019group", "year": "2020", "citations": "48", "title":"The Group Loss for Deep Metric Learning", "abstract": "<p>Deep metric learning has yielded impressive results in tasks such as\nclustering and image retrieval by leveraging neural networks to obtain highly\ndiscriminative feature embeddings, which can be used to group samples into\ndifferent classes. Much research has been devoted to the design of smart loss\nfunctions or data mining strategies for training such networks. Most methods\nconsider only pairs or triplets of samples within a mini-batch to compute the\nloss function, which is commonly based on the distance between embeddings. We\npropose Group Loss, a loss function based on a differentiable label-propagation\nmethod that enforces embedding similarity across all samples of a group while\npromoting, at the same time, low-density regions amongst data points belonging\nto different groups. Guided by the smoothness assumption that “similar objects\nshould belong to the same group”, the proposed loss trains the neural network\nfor a classification task, enforcing a consistent labelling amongst samples\nwithin a class. We show state-of-the-art results on clustering and image\nretrieval on several datasets, and show the potential of our method when\ncombined with other techniques such as ensembles</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Image-Retrieval"] },
{"key": "elnouby2021training", "year": "2021", "citations": "120", "title":"Training Vision Transformers for Image Retrieval", "abstract": "<p>Transformers have shown outstanding results for natural language\nunderstanding and, more recently, for image classification. We here extend this\nwork and propose a transformer-based approach for image retrieval: we adopt\nvision transformers for generating image descriptors and train the resulting\nmodel with a metric learning objective, which combines a contrastive loss with\na differential entropy regularizer. Our results show consistent and significant\nimprovements of transformers over convolution-based approaches. In particular,\nour method outperforms the state of the art on several public benchmarks for\ncategory-level retrieval, namely Stanford Online Product, In-Shop and CUB-200.\nFurthermore, our experiments on ROxford and RParis also show that, in\ncomparable settings, transformers are competitive for particular object\nretrieval, especially in the regime of short vector representations and\nlow-resolution images.</p>\n", "tags": ["Distance-Metric-Learning","Image-Retrieval"] },
{"key": "ercoli2016compact", "year": "2017", "citations": "42", "title":"Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases", "abstract": "<p>In this paper we present an efficient method for visual descriptors retrieval\nbased on compact hash codes computed using a multiple k-means assignment. The\nmethod has been applied to the problem of approximate nearest neighbor (ANN)\nsearch of local and global visual content descriptors, and it has been tested\non different datasets: three large scale public datasets of up to one billion\ndescriptors (BIGANN) and, supported by recent progress in convolutional neural\nnetworks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental results\nshow that, despite its simplicity, the proposed method obtains a very high\nperformance that makes it superior to more complex state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods","Datasets","Evaluation"] },
{"key": "ertl2017superminhash", "year": "2017", "citations": "13", "title":"SuperMinHash - A New Minwise Hashing Algorithm for Jaccard Similarity Estimation", "abstract": "<p>This paper presents a new algorithm for calculating hash signatures of sets\nwhich can be directly used for Jaccard similarity estimation. The new approach\nis an improvement over the MinHash algorithm, because it has a better runtime\nbehavior and the resulting signatures allow a more precise estimation of the\nJaccard index.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing"] },
{"key": "ertl2018bagminhash", "year": "2018", "citations": "15", "title":"BagMinHash - Minwise Hashing Algorithm for Weighted Sets", "abstract": "<p>Minwise hashing has become a standard tool to calculate signatures which\nallow direct estimation of Jaccard similarities. While very efficient\nalgorithms already exist for the unweighted case, the calculation of signatures\nfor weighted sets is still a time consuming task. BagMinHash is a new algorithm\nthat can be orders of magnitude faster than current state of the art without\nany particular restrictions or assumptions on weights or data dimensionality.\nApplied to the special case of unweighted sets, it represents the first\nefficient algorithm producing independent signature components. A series of\ntests finally verifies the new algorithm and also reveals limitations of other\napproaches published in the recent past.</p>\n", "tags": ["KDD","Hashing-Methods"] },
{"key": "ertl2019probminhash", "year": "2020", "citations": "23", "title":"ProbMinHash -- A Class of Locality-Sensitive Hash Algorithms for the (Probability) Jaccard Similarity", "abstract": "<p>The probability Jaccard similarity was recently proposed as a natural\ngeneralization of the Jaccard similarity to measure the proximity of sets whose\nelements are associated with relative frequencies or probabilities. In\ncombination with a hash algorithm that maps those weighted sets to compact\nsignatures which allow fast estimation of pairwise similarities, it constitutes\na valuable method for big data applications such as near-duplicate detection,\nnearest neighbor search, or clustering. This paper introduces a class of\none-pass locality-sensitive hash algorithms that are orders of magnitude faster\nthan the original approach. The performance gain is achieved by calculating\nsignature components not independently, but collectively. Four different\nalgorithms are proposed based on this idea. Two of them are statistically\nequivalent to the original approach and can be used as drop-in replacements.\nThe other two may even improve the estimation error by introducing statistical\ndependence between signature components. Moreover, the presented techniques can\nbe specialized for the conventional Jaccard similarity, resulting in highly\nefficient algorithms that outperform traditional minwise hashing and that are\nable to compete with the state of the art.</p>\n", "tags": ["Hashing-Methods","Evaluation"] },
{"key": "fan2013supervised", "year": "2013", "citations": "15", "title":"Supervised binary hash code learning with jensen shannon divergence", "abstract": "<p>This paper proposes to learn binary hash codes within\na statistical learning framework, in which an upper bound\nof the probability of Bayes decision errors is derived for\ndifferent forms of hash functions and a rigorous proof of\nthe convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent\nperformance improvements of existing hash code learning\nalgorithms, regardless of whether original algorithms are\nunsupervised or supervised. This paper also illustrates a\nfast hash coding method that exploits simple binary tests to\nachieve orders of magnitude improvement in coding speed\nas compared to projection based methods.</p>\n", "tags": ["ICCV","Unsupervised","Tools-&-Libraries","Hashing-Methods","Evaluation","Supervised"] },
{"key": "fan2020deep", "year": "2020", "citations": "81", "title":"Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes", "abstract": "<p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away\nfrom zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,\n2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as\ndisclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from\nthe target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple — even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","AAAI","Compact-Codes","Tools-&-Libraries","Hashing-Methods","IJCAI","Supervised"] },
{"key": "fan2025deep", "year": "2020", "citations": "81", "title":"Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes", "abstract": "<p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away\nfrom zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,\n2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as\ndisclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from\nthe target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple — even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","AAAI","Compact-Codes","Tools-&-Libraries","Hashing-Methods","IJCAI","Supervised"] },
{"key": "fan2025supervised", "year": "2013", "citations": "15", "title":"Supervised binary hash code learning with jensen shannon divergence", "abstract": "<p>This paper proposes to learn binary hash codes within\na statistical learning framework, in which an upper bound\nof the probability of Bayes decision errors is derived for\ndifferent forms of hash functions and a rigorous proof of\nthe convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent\nperformance improvements of existing hash code learning\nalgorithms, regardless of whether original algorithms are\nunsupervised or supervised. This paper also illustrates a\nfast hash coding method that exploits simple binary tests to\nachieve orders of magnitude improvement in coding speed\nas compared to projection based methods.</p>\n", "tags": ["ICCV","Unsupervised","Tools-&-Libraries","Hashing-Methods","Evaluation","Supervised"] },
{"key": "fang2020attention", "year": "2020", "citations": "12", "title":"Attention-based Saliency Hashing for Ophthalmic Image Retrieval", "abstract": "<p>Deep hashing methods have been proved to be effective for the large-scale\nmedical image search assisting reference-based diagnosis for clinicians.\nHowever, when the salient region plays a maximal discriminative role in\nophthalmic image, existing deep hashing methods do not fully exploit the\nlearning ability of the deep network to capture the features of salient regions\npointedly. The different grades or classes of ophthalmic images may be share\nsimilar overall performance but have subtle differences that can be\ndifferentiated by mining salient regions. To address this issue, we propose a\nnovel end-to-end network, named Attention-based Saliency Hashing (ASH), for\nlearning compact hash-code to represent ophthalmic images. ASH embeds a\nspatial-attention module to focus more on the representation of salient regions\nand highlights their essential role in differentiating ophthalmic images.\nBenefiting from the spatial-attention module, the information of salient\nregions can be mapped into the hash-code for similarity calculation. In the\ntraining stage, we input the image pairs to share the weights of the network,\nand a pairwise loss is designed to maximize the discriminability of the\nhash-code. In the retrieval stage, ASH obtains the hash-code by inputting an\nimage with an end-to-end manner, then the hash-code is used to similarity\ncalculation to return the most similar images. Extensive experiments on two\ndifferent modalities of ophthalmic image datasets demonstrate that the proposed\nASH can further improve the retrieval performance compared to the\nstate-of-the-art deep hashing methods due to the huge contributions of the\nspatial-attention module.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Scalability","Evaluation"] },
{"key": "fang2021combating", "year": "2021", "citations": "10", "title":"Combating Ambiguity for Hash-code Learning in Medical Instance Retrieval", "abstract": "<p>When encountering a dubious diagnostic case, medical instance retrieval can\nhelp radiologists make evidence-based diagnoses by finding images containing\ninstances similar to a query case from a large image database. The similarity\nbetween the query case and retrieved similar cases is determined by visual\nfeatures extracted from pathologically abnormal regions. However, the\nmanifestation of these regions often lacks specificity, i.e., different\ndiseases can have the same manifestation, and different manifestations may\noccur at different stages of the same disease. To combat the manifestation\nambiguity in medical instance retrieval, we propose a novel deep framework\ncalled Y-Net, encoding images into compact hash-codes generated from\nconvolutional features by feature aggregation. Y-Net can learn highly\ndiscriminative convolutional features by unifying the pixel-wise segmentation\nloss and classification loss. The segmentation loss allows exploring subtle\nspatial differences for good spatial-discriminability while the classification\nloss utilizes class-aware semantic information for good semantic-separability.\nAs a result, Y-Net can enhance the visual features in pathologically abnormal\nregions and suppress the disturbing of the background during model training,\nwhich could effectively embed discriminative features into the hash-codes in\nthe retrieval stage. Extensive experiments on two medical image datasets\ndemonstrate that Y-Net can alleviate the ambiguity of pathologically abnormal\nregions and its retrieval performance outperforms the state-of-the-art method\nby an average of 9.27% on the returned list of 10.</p>\n", "tags": ["Evaluation","Datasets","Tools-&-Libraries"] },
{"key": "fang2021deep", "year": "2021", "citations": "52", "title":"Deep Triplet Hashing Network for Case-based Medical Image Retrieval", "abstract": "<p>Deep hashing methods have been shown to be the most efficient approximate\nnearest neighbor search techniques for large-scale image retrieval. However,\nexisting deep hashing methods have a poor small-sample ranking performance for\ncase-based medical image retrieval. The top-ranked images in the returned query\nresults may be as a different class than the query image. This ranking problem\nis caused by classification, regions of interest (ROI), and small-sample\ninformation loss in the hashing space. To address the ranking problem, we\npropose an end-to-end framework, called Attention-based Triplet Hashing (ATH)\nnetwork, to learn low-dimensional hash codes that preserve the classification,\nROI, and small-sample information. We embed a spatial-attention module into the\nnetwork structure of our ATH to focus on ROI information. The spatial-attention\nmodule aggregates the spatial information of feature maps by utilizing\nmax-pooling, element-wise maximum, and element-wise mean operations jointly\nalong the channel axis. The triplet cross-entropy loss can help to map the\nclassification information of images and similarity between images into the\nhash codes. Extensive experiments on two case-based medical datasets\ndemonstrate that our proposed ATH can further improve the retrieval performance\ncompared to the state-of-the-art deep hashing methods and boost the ranking\nperformance for small samples. Compared to the other loss methods, the triplet\ncross-entropy loss can enhance the classification performance and hash\ncode-discriminability</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Scalability","Evaluation"] },
{"key": "feng2016deep", "year": "2017", "citations": "12", "title":"Deep Image Set Hashing", "abstract": "<p>In applications involving matching of image sets, the information from\nmultiple images must be effectively exploited to represent each set.\nState-of-the-art methods use probabilistic distribution or subspace to model a\nset and use specific distance measure to compare two sets. These methods are\nslow to compute and not compact to use in a large scale scenario.\nLearning-based hashing is often used in large scale image retrieval as they\nprovide a compact representation of each sample and the Hamming distance can be\nused to efficiently compare two samples. However, most hashing methods encode\neach image separately and discard knowledge that multiple images in the same\nset represent the same object or person. We investigate the set hashing problem\nby combining both set representation and hashing in a single deep neural\nnetwork. An image set is first passed to a CNN module to extract image\nfeatures, then these features are aggregated using two types of set feature to\ncapture both set specific and database-wide distribution information. The\ncomputed set feature is then fed into a multilayer perceptron to learn a\ncompact binary embedding. Triplet loss is used to train the network by forming\nset similarity relations using class labels. We extensively evaluate our\napproach on datasets used for image matching and show highly competitive\nperformance compared to state-of-the-art methods.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "feng2020adversarial", "year": "2020", "citations": "43", "title":"Adversarial Attack on Deep Product Quantization Network for Image Retrieval", "abstract": "<p>Deep product quantization network (DPQN) has recently received much attention\nin fast image retrieval tasks due to its efficiency of encoding\nhigh-dimensional visual features especially when dealing with large-scale\ndatasets. Recent studies show that deep neural networks (DNNs) are vulnerable\nto input with small and maliciously designed perturbations (a.k.a., adversarial\nexamples). This phenomenon raises the concern of security issues for DPQN in\nthe testing/deploying stage as well. However, little effort has been devoted to\ninvestigating how adversarial examples affect DPQN. To this end, we propose\nproduct quantization adversarial generation (PQ-AG), a simple yet effective\nmethod to generate adversarial examples for product quantization based\nretrieval systems. PQ-AG aims to generate imperceptible adversarial\nperturbations for query images to form adversarial queries, whose nearest\nneighbors from a targeted product quantizaiton model are not semantically\nrelated to those from the original queries. Extensive experiments show that our\nPQ-AQ successfully creates adversarial examples to mislead targeted product\nquantization retrieval models. Besides, we found that our PQ-AG significantly\ndegrades retrieval performance in both white-box and black-box settings.</p>\n", "tags": ["Image-Retrieval","Datasets","Efficiency","AAAI","Scalability","Quantization","Evaluation","Robustness"] },
{"key": "feng2020unifying", "year": "2020", "citations": "5", "title":"Unifying Specialist Image Embedding into Universal Image Embedding", "abstract": "<p>Deep image embedding provides a way to measure the semantic similarity of two\nimages. It plays a central role in many applications such as image search, face\nverification, and zero-shot learning. It is desirable to have a universal deep\nembedding model applicable to various domains of images. However, existing\nmethods mainly rely on training specialist embedding models each of which is\napplicable to images from a single domain. In this paper, we study an important\nbut unexplored task: how to train a single universal image embedding model to\nmatch the performance of several specialists on each specialist’s domain.\nSimply fusing the training data from multiple domains cannot solve this problem\nbecause some domains become overfitted sooner when trained together using\nexisting methods. Therefore, we propose to distill the knowledge in multiple\nspecialists into a universal embedding to solve this problem. In contrast to\nexisting embedding distillation methods that distill the absolute distances\nbetween images, we transform the absolute distances between images into a\nprobabilistic distribution and minimize the KL-divergence between the\ndistributions of the specialists and the universal embedding. Using several\npublic datasets, we validate that our proposed method accomplishes the goal of\nuniversal image embedding.</p>\n", "tags": ["Datasets","Few-Shot-&-Zero-Shot","Evaluation","Image-Retrieval"] },
{"key": "ferdowsi2017sparse", "year": "2017", "citations": "12", "title":"Sparse Ternary Codes for similarity search have higher coding gain than dense binary codes", "abstract": "<p>This paper addresses the problem of Approximate Nearest Neighbor (ANN) search\nin pattern recognition where feature vectors in a database are encoded as\ncompact codes in order to speed-up the similarity search in large-scale\ndatabases. Considering the ANN problem from an information-theoretic\nperspective, we interpret it as an encoding, which maps the original feature\nvectors to a less entropic sparse representation while requiring them to be as\ninformative as possible. We then define the coding gain for ANN search using\ninformation-theoretic measures. We next show that the classical approach to\nthis problem, which consists of binarization of the projected vectors is\nsub-optimal. Instead, a properly designed ternary encoding achieves higher\ncoding gains and lower complexity.</p>\n", "tags": ["Hashing-Methods","Scalability","Compact-Codes","Similarity-Search"] },
{"key": "fernandes2020locality", "year": "2021", "citations": "7", "title":"Locality Sensitive Hashing with Extended Differential Privacy", "abstract": "<p>Extended differential privacy, a generalization of standard differential\nprivacy (DP) using a general metric, has been widely studied to provide\nrigorous privacy guarantees while keeping high utility. However, existing works\non extended DP are limited to few metrics, such as the Euclidean metric.\nConsequently, they have only a small number of applications, such as\nlocation-based services and document processing. In this paper, we propose a\ncouple of mechanisms providing extended DP with a different metric: angular\ndistance (or cosine distance). Our mechanisms are based on locality sensitive\nhashing (LSH), which can be applied to the angular distance and work well for\npersonal data in a high-dimensional space. We theoretically analyze the privacy\nproperties of our mechanisms, and prove extended DP for input data by taking\ninto account that LSH preserves the original metric only approximately. We\napply our mechanisms to friend matching based on high-dimensional personal data\nwith angular distance in the local model, and evaluate our mechanisms using two\nreal datasets. We show that LDP requires a very large privacy budget and that\nRAPPOR does not work in this application. Then we show that our mechanisms\nenable friend matching with high utility and rigorous privacy guarantees based\non extended DP.</p>\n", "tags": ["Hashing-Methods","Datasets","Locality-Sensitive-Hashing"] },
{"key": "forcen2020co", "year": "2020", "citations": "19", "title":"Co-occurrence of deep convolutional features for image search", "abstract": "<p>Image search can be tackled using deep features from pre-trained\nConvolutional Neural Networks (CNN). The feature map from the last\nconvolutional layer of a CNN encodes descriptive information from which a\ndiscriminative global descriptor can be obtained. We propose a new\nrepresentation of co-occurrences from deep convolutional features to extract\nadditional relevant information from this last convolutional layer. Combining\nthis co-occurrence map with the feature map, we achieve an improved image\nrepresentation. We present two different methods to get the co-occurrence\nrepresentation, the first one based on direct aggregation of activations, and\nthe second one, based on a trainable co-occurrence representation. The image\ndescriptors derived from our methodology improve the performance in very\nwell-known image retrieval datasets as we prove in the experiments.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "frady2020neuromorphic", "year": "2020", "citations": "47", "title":"Neuromorphic Nearest-Neighbor Search Using Intel's Pohoiki Springs", "abstract": "<p>Neuromorphic computing applies insights from neuroscience to uncover\ninnovations in computing technology. In the brain, billions of interconnected\nneurons perform rapid computations at extremely low energy levels by leveraging\nproperties that are foreign to conventional computing systems, such as temporal\nspiking codes and finely parallelized processing units integrating both memory\nand computation. Here, we showcase the Pohoiki Springs neuromorphic system, a\nmesh of 768 interconnected Loihi chips that collectively implement 100 million\nspiking neurons in silicon. We demonstrate a scalable approximate k-nearest\nneighbor (k-NN) algorithm for searching large databases that exploits\nneuromorphic principles. Compared to state-of-the-art conventional CPU-based\nimplementations, we achieve superior latency, index build time, and energy\nefficiency when evaluated on several standard datasets containing over 1\nmillion high-dimensional patterns. Further, the system supports adding new data\npoints to the indexed database online in O(1) time unlike all but brute force\nconventional k-NN implementations.</p>\n", "tags": ["Datasets","Efficiency"] },
{"key": "fredriksson2016geometric", "year": "2016", "citations": "6", "title":"Geometric Near-neighbor Access Tree (GNAT) revisited", "abstract": "<p>Geometric Near-neighbor Access Tree (GNAT) is a metric space indexing method\nbased on hierarchical hyperplane partitioning of the space. While GNAT is very\nefficient in proximity searching, it has a bad reputation of being a memory\nhog. We show that this is partially based on too coarse analysis, and that the\nmemory requirements can be lowered while at the same time improving the search\nefficiency. We also show how to make GNAT memory adaptive in a smooth way, and\nthat the hyperplane partitioning can be replaced with ball partitioning, which\ncan further improve the search performance. We conclude with experimental\nresults showing the new methods can give significant performance boost.</p>\n", "tags": ["Evaluation","Efficiency"] },
{"key": "fu2016improved", "year": "2015", "citations": "5", "title":"An Improved System for Sentence-level Novelty Detection in Textual Streams", "abstract": "<p>Novelty detection in news events has long been a difficult problem. A number\nof models performed well on specific data streams but certain issues are far\nfrom being solved, particularly in large data streams from the WWW where\nunpredictability of new terms requires adaptation in the vector space model. We\npresent a novel event detection system based on the Incremental Term\nFrequency-Inverse Document Frequency (TF-IDF) weighting incorporated with\nLocality Sensitive Hashing (LSH). Our system could efficiently and effectively\nadapt to the changes within the data streams of any new terms with continual\nupdates to the vector space model. Regarding miss probability, our proposed\nnovelty detection framework outperforms a recognised baseline system by\napproximately 16% when evaluating a benchmark dataset from Google News.</p>\n", "tags": ["Locality-Sensitive-Hashing","Tools-&-Libraries","Hashing-Methods","Datasets","Evaluation"] },
{"key": "fu2017fast", "year": "2019", "citations": "219", "title":"Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph", "abstract": "<p>Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial search scenario of Taobao (Alibaba Group) and has been integrated into their search engine at billion-node scale.</p>\n", "tags": ["Graph-Based-ANN","Datasets","Scalability","Evaluation","Efficiency"] },
{"key": "fu2020deep", "year": "2021", "citations": "15", "title":"Deep Momentum Uncertainty Hashing", "abstract": "<p>Combinatorial optimization (CO) has been a hot research topic because of its\ntheoretic and practical importance. As a classic CO problem, deep hashing aims\nto find an optimal code for each data from finite discrete possibilities, while\nthe discrete nature brings a big challenge to the optimization process.\nPrevious methods usually mitigate this challenge by binary approximation,\nsubstituting binary codes for real-values via activation functions or\nregularizations. However, such approximation leads to uncertainty between\nreal-values and binary ones, degrading retrieval performance. In this paper, we\npropose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly\nestimates the uncertainty during training and leverages the uncertainty\ninformation to guide the approximation process. Specifically, we model\nbit-level uncertainty via measuring the discrepancy between the output of a\nhashing network and that of a momentum-updated network. The discrepancy of each\nbit indicates the uncertainty of the hashing network to the approximate output\nof that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can\nbe regarded as image-level uncertainty. It embodies the uncertainty of the\nhashing network to the corresponding input image. The hashing bit and image\nwith higher uncertainty are paid more attention during optimization. To the\nbest of our knowledge, this is the first work to study the uncertainty in\nhashing bits. Extensive experiments are conducted on four datasets to verify\nthe superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a\nmillion-scale dataset Clothing1M. Our method achieves the best performance on\nall of the datasets and surpasses existing state-of-the-art methods by a large\nmargin.</p>\n", "tags": ["Evaluation","Datasets","Compact-Codes","Hashing-Methods","Neural-Hashing","CVPR"] },
{"key": "fu2020hard", "year": "2020", "citations": "6", "title":"Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning", "abstract": "<p>Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape\nof a given 2D image from a large 3D shape database. The common routine is to\nmap 2D images and 3D shapes into an embedding space and define (or learn) a\nshape similarity measure. While metric learning with some adaptation techniques\nseems to be a natural solution to shape similarity learning, the performance is\noften unsatisfactory for fine-grained shape retrieval. In the paper, we\nidentify the source of the poor performance and propose a practical solution to\nthis problem. We find that the shape difference between a negative pair is\nentangled with the texture gap, making metric learning ineffective in pushing\naway negative pairs. To tackle this issue, we develop a geometry-focused\nmulti-view metric learning framework empowered by texture synthesis. The\nsynthesis of textures for 3D shape models creates hard triplets, which suppress\nthe adverse effects of rich texture in 2D images, thereby push the network to\nfocus more on discovering geometric characteristics. Our approach shows\nstate-of-the-art performance on a recently released large-scale 3D-FUTURE[1]\nrepository, as well as three widely studied benchmarks, including Pix3D[2],\nStanford Cars[3], and Comp Cars[4]. Codes will be made publicly available at:\nhttps://github.com/3D-FRONT-FUTURE/IBSR-texture</p>\n", "tags": ["Distance-Metric-Learning","Scalability","Evaluation","Tools-&-Libraries"] },
{"key": "galanopoulos2022are", "year": "2023", "citations": "11", "title":"Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval", "abstract": "<p>In this paper we tackle the cross-modal video retrieval problem and, more\nspecifically, we focus on text-to-video retrieval. We investigate how to\noptimally combine multiple diverse textual and visual features into feature\npairs that lead to generating multiple joint feature spaces, which encode\ntext-video pairs into comparable representations. To learn these\nrepresentations our proposed network architecture is trained by following a\nmultiple space learning procedure. Moreover, at the retrieval stage, we\nintroduce additional softmax operations for revising the inferred query-video\nsimilarities. Extensive experiments in several setups based on three\nlarge-scale datasets (IACC.3, V3C1, and MSR-VTT) lead to conclusions on how to\nbest combine text-visual features and document the performance of the proposed\nnetwork. Source code is made publicly available at:\nhttps://github.com/bmezaris/TextToVideoRetrieval-TtimesV</p>\n", "tags": ["Datasets","Scalability","Video-Retrieval","Evaluation"] },
{"key": "galanopoulos2023are", "year": "2023", "citations": "11", "title":"Are All Combinations Equal? Combining Textual and Visual Features with Multiple Space Learning for Text-Based Video Retrieval", "abstract": "<p>In this paper we tackle the cross-modal video retrieval problem and, more\nspecifically, we focus on text-to-video retrieval. We investigate how to\noptimally combine multiple diverse textual and visual features into feature\npairs that lead to generating multiple joint feature spaces, which encode\ntext-video pairs into comparable representations. To learn these\nrepresentations our proposed network architecture is trained by following a\nmultiple space learning procedure. Moreover, at the retrieval stage, we\nintroduce additional softmax operations for revising the inferred query-video\nsimilarities. Extensive experiments in several setups based on three\nlarge-scale datasets (IACC.3, V3C1, and MSR-VTT) lead to conclusions on how to\nbest combine text-visual features and document the performance of the proposed\nnetwork. Source code is made publicly available at:\nhttps://github.com/bmezaris/TextToVideoRetrieval-TtimesV</p>\n", "tags": ["Datasets","Scalability","Video-Retrieval","Evaluation"] },
{"key": "gan2023binary", "year": "2023", "citations": "7", "title":"Binary Embedding-based Retrieval at Tencent", "abstract": "<p>Large-scale embedding-based retrieval (EBR) is the cornerstone of\nsearch-related industrial applications. Given a user query, the system of EBR\naims to identify relevant information from a large corpus of documents that may\nbe tens or hundreds of billions in size. The storage and computation turn out\nto be expensive and inefficient with massive documents and high concurrent\nqueries, making it difficult to further scale up. To tackle the challenge, we\npropose a binary embedding-based retrieval (BEBR) engine equipped with a\nrecurrent binarization algorithm that enables customized bits per dimension.\nSpecifically, we compress the full-precision query and document embeddings,\nformulated as float vectors in general, into a composition of multiple binary\nvectors using a lightweight transformation model with residual multilayer\nperception (MLP) blocks. We can therefore tailor the number of bits for\ndifferent applications to trade off accuracy loss and cost savings.\nImportantly, we enable task-agnostic efficient training of the binarization\nmodel using a new embedding-to-embedding strategy. We also exploit the\ncompatible training of binary embeddings so that the BEBR engine can support\nindexing among multiple embedding versions within a unified system. To further\nrealize efficient search, we propose Symmetric Distance Calculation (SDC) to\nachieve lower response time than Hamming codes. We successfully employed the\nintroduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World,\netc. The binarization algorithm can be seamlessly generalized to various tasks\nwith multiple modalities. Extensive experiments on offline benchmarks and\nonline A/B tests demonstrate the efficiency and effectiveness of our method,\nsignificantly saving 30%~50% index costs with almost no loss of accuracy at the\nsystem level.</p>\n", "tags": ["Hashing-Methods","KDD","Scalability","Evaluation","Efficiency"] },
{"key": "ganea2021incremental", "year": "2021", "citations": "55", "title":"Incremental Few-Shot Instance Segmentation", "abstract": "<p>Few-shot instance segmentation methods are promising when labeled training\ndata for novel classes is scarce. However, current approaches do not facilitate\nflexible addition of novel classes. They also require that examples of each\nclass are provided at train and test time, which is memory intensive. In this\npaper, we address these limitations by presenting the first incremental\napproach to few-shot instance segmentation: iMTFA. We learn discriminative\nembeddings for object instances that are merged into class representatives.\nStoring embedding vectors rather than images effectively solves the memory\noverhead problem. We match these class embeddings at the RoI-level using cosine\nsimilarity. This allows us to add new classes without the need for further\ntraining or access to previous training data. In a series of experiments, we\nconsistently outperform the current state-of-the-art. Moreover, the reduced\nmemory requirements allow us to evaluate, for the first time, few-shot instance\nsegmentation performance on all classes in COCO jointly.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","CVPR","Evaluation"] },
{"key": "gao2020complementing", "year": "2020", "citations": "61", "title":"Complementing Lexical Retrieval with Semantic Residual Embedding", "abstract": "<p>This paper presents CLEAR, a retrieval model that seeks to complement\nclassical lexical exact-match models such as BM25 with semantic matching\nsignals from a neural embedding matching model. CLEAR explicitly trains the\nneural embedding to encode language structures and semantics that lexical\nretrieval fails to capture with a novel residual-based embedding learning\nmethod. Empirical evaluations demonstrate the advantages of CLEAR over\nstate-of-the-art retrieval models, and that it can substantially improve the\nend-to-end accuracy and efficiency of reranking pipelines.</p>\n", "tags": ["Efficiency"] },
{"key": "gao2022long", "year": "2023", "citations": "5", "title":"Long-tail Cross Modal Hashing", "abstract": "<p>Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced\ndata, while imbalanced data with long-tail distribution is more general in\nreal-world. Several long-tail hashing methods have been proposed but they can\nnot adapt for multi-modal data, due to the complex interplay between labels and\nindividuality and commonality information of multi-modal data. Furthermore, CMH\nmethods mostly mine the commonality of multi-modal data to learn hash codes,\nwhich may override tail labels encoded by the individuality of respective\nmodalities. In this paper, we propose LtCMH (Long-tail CMH) to handle\nimbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the\nindividuality and commonality of different modalities by minimizing the\ndependency between the individuality of respective modalities and by enhancing\nthe commonality of these modalities. Then it dynamically combines the\nindividuality and commonality with direct features extracted from respective\nmodalities to create meta features that enrich the representation of tail\nlabels, and binaries meta features to generate hash codes. LtCMH significantly\noutperforms state-of-the-art baselines on long-tail datasets and holds a better\n(or comparable) performance on datasets with balanced labels.</p>\n", "tags": ["AAAI","Datasets","Hashing-Methods","Evaluation"] },
{"key": "gao2022precise", "year": "2023", "citations": "81", "title":"Precise Zero-Shot Dense Retrieval without Relevance Labels", "abstract": "<p>While dense retrieval has been shown effective and efficient across tasks and\nlanguages, it remains difficult to create effective fully zero-shot dense\nretrieval systems when no relevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning and encoding relevance. Instead,\nwe propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a\nquery, HyDE first zero-shot instructs an instruction-following language model\n(e.g. InstructGPT) to generate a hypothetical document. The document captures\nrelevance patterns but is unreal and may contain false details. Then, an\nunsupervised contrastively learned encoder~(e.g. Contriever) encodes the\ndocument into an embedding vector. This vector identifies a neighborhood in the\ncorpus embedding space, where similar real documents are retrieved based on\nvector similarity. This second step ground the generated document to the actual\ncorpus, with the encoder’s dense bottleneck filtering out the incorrect\ndetails. Our experiments show that HyDE significantly outperforms the\nstate-of-the-art unsupervised dense retriever Contriever and shows strong\nperformance comparable to fine-tuned retrievers, across various tasks (e.g. web\nsearch, QA, fact verification) and languages~(e.g. sw, ko, ja).</p>\n", "tags": ["Unsupervised","Few-Shot-&-Zero-Shot","Evaluation"] },
{"key": "garcia2017learning", "year": "2019", "citations": "40", "title":"Learning Non-Metric Visual Similarity for Image Retrieval", "abstract": "<p>Measuring visual similarity between two or more instances within a data\ndistribution is a fundamental task in image retrieval. Theoretically,\nnon-metric distances are able to generate a more complex and accurate\nsimilarity model than metric distances, provided that the non-linear data\ndistribution is precisely captured by the system. In this work, we explore\nneural networks models for learning a non-metric similarity function for\ninstance search. We argue that non-metric similarity functions based on neural\nnetworks can build a better model of human visual perception than standard\nmetric distances. As our proposed similarity function is differentiable, we\nexplore a real end-to-end trainable approach for image retrieval, i.e. we learn\nthe weights from the input image pixels to the final similarity score.\nExperimental evaluation shows that non-metric similarity networks are able to\nlearn visual similarities between images and improve performance on top of\nstate-of-the-art image representations, boosting results in standard image\nretrieval datasets with respect standard metric distances.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "garcia2019context", "year": "2019", "citations": "26", "title":"Context-Aware Embeddings for Automatic Art Analysis", "abstract": "<p>Automatic art analysis aims to classify and retrieve artistic representations\nfrom a collection of images by using computer vision and machine learning\ntechniques. In this work, we propose to enhance visual representations from\nneural networks with contextual artistic information. Whereas visual\nrepresentations are able to capture information about the content and the style\nof an artwork, our proposed context-aware embeddings additionally encode\nrelationships between different artistic attributes, such as author, school, or\nhistorical period. We design two different approaches for using context in\nautomatic art analysis. In the first one, contextual data is obtained through a\nmulti-task learning model, in which several attributes are trained together to\nfind visual relationships between elements. In the second approach, context is\nobtained through an art-specific knowledge graph, which encodes relationships\nbetween artistic attributes. An exhaustive evaluation of both of our models in\nseveral art analysis problems, such as author identification, type\nclassification, or cross-modal retrieval, show that performance is improved by\nup to 7.3% in art classification and 37.24% in retrieval when context-aware\nembeddings are used.</p>\n", "tags": ["Evaluation","Multimodal-Retrieval"] },
{"key": "gattupalli2018weakly", "year": "2019", "citations": "42", "title":"Weakly Supervised Deep Image Hashing through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised\nlearning problems that utilize images and label information to learn the binary\nhash codes. However, large-scale labeled image data is expensive to obtain,\nthus imposing a restriction on the usage of such algorithms. On the other hand,\nunlabelled image data is abundant due to the existence of many Web image\nrepositories. Such Web images may often come with images tags that contain\nuseful information, although raw tags, in general, do not readily lead to\nsemantic labels. Motivated by this scenario, we formulate the problem of\nsemantic image hashing as a weakly-supervised learning problem. We utilize the\ninformation contained in the user-generated tags associated with the images to\nlearn the hash codes. More specifically, we extract the word2vec semantic\nembeddings of the tags and use the information contained in them for\nconstraining the learning. Accordingly, we name our model Weakly Supervised\nDeep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of\nsemantic image retrieval and is compared against several state-of-art models.\nResults show that our approach sets a new state-of-art in the area of weekly\nsupervised image hashing.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Neural-Hashing","CVPR","Scalability"] },
{"key": "gattupalli2019weakly", "year": "2019", "citations": "42", "title":"Weakly Supervised Deep Image Hashing through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.\nMotivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.\nAccordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>\n", "tags": ["Image-Retrieval","Scalability","CVPR","Neural-Hashing","Hashing-Methods","Supervised"] },
{"key": "gattupalli2025weakly", "year": "2019", "citations": "42", "title":"Weakly Supervised Deep Image Hashing through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.\nMotivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.\nAccordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>\n", "tags": ["Image-Retrieval","Scalability","CVPR","Neural-Hashing","Hashing-Methods","Supervised"] },
{"key": "ge2014graph", "year": "2014", "citations": "69", "title":"Graph Cuts for Supervised Binary Coding", "abstract": "<p>Learning short binary codes is challenged by the inherent discrete\nnature of the problem. The graph cuts algorithm is a well-studied\ndiscrete label assignment solution in computer vision, but has not yet\nbeen applied to solve the binary coding problems. This is partially because\nit was unclear how to use it to learn the encoding (hashing) functions\nfor out-of-sample generalization. In this paper, we formulate supervised\nbinary coding as a single optimization problem that involves both\nthe encoding functions and the binary label assignment. Then we apply\nthe graph cuts algorithm to address the discrete optimization problem\ninvolved, with no continuous relaxation. This method, named as Graph\nCuts Coding (GCC), shows competitive results in various datasets.</p>\n", "tags": ["Hashing-Methods","Compact-Codes","Supervised","Datasets"] },
{"key": "ge2020self", "year": "2020", "citations": "107", "title":"Self-supervising Fine-grained Region Similarities for Large-scale Image Localization", "abstract": "<p>The task of large-scale retrieval-based image localization is to estimate the\ngeographical location of a query image by recognizing its nearest reference\nimages from a city-scale dataset. However, the general public benchmarks only\nprovide noisy GPS labels associated with the training images, which act as weak\nsupervisions for learning image-to-image similarities. Such label noise\nprevents deep neural networks from learning discriminative features for\naccurate localization. To tackle this challenge, we propose to self-supervise\nimage-to-region similarities in order to fully explore the potential of\ndifficult positive images alongside their sub-regions. The estimated\nimage-to-region similarities can serve as extra training supervision for\nimproving the network in generations, which could in turn gradually refine the\nfine-grained similarities to achieve optimal performance. Our proposed\nself-enhanced image-to-region similarity labels effectively deal with the\ntraining bottleneck in the state-of-the-art pipelines without any additional\nparameters or manual annotations in both training and inference. Our method\noutperforms state-of-the-arts on the standard localization benchmarks by\nnoticeable margins and shows excellent generalization capability on multiple\nimage retrieval datasets.</p>\n", "tags": ["Datasets","Scalability","Evaluation","Image-Retrieval"] },
{"key": "ge2021structured", "year": "2021", "citations": "43", "title":"Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval", "abstract": "<p>The current state-of-the-art image-sentence retrieval methods implicitly\nalign the visual-textual fragments, like regions in images and words in\nsentences, and adopt attention modules to highlight the relevance of\ncross-modal semantic correspondences. However, the retrieval performance\nremains unsatisfactory due to a lack of consistent representation in both\nsemantics and structural spaces. In this work, we propose to address the above\nissue from two aspects: (i) constructing intrinsic structure (along with\nrelations) among the fragments of respective modalities, e.g., “dog \\(\\to\\) play\n\\(\\to\\) ball” in semantic structure for an image, and (ii) seeking explicit\ninter-modal structural and semantic correspondence between the visual and\ntextual modalities. In this paper, we propose a novel Structured Multi-modal\nFeature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In\norder to jointly and explicitly learn the visual-textual embedding and the\ncross-modal alignment, SMFEA creates a novel multi-modal structured module with\na shared context-aware referral tree. In particular, the relations of the\nvisual and textual fragments are modeled by constructing Visual Context-aware\nStructured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree\nencoder (TCS-Tree) with shared labels, from which visual and textual features\ncan be jointly learned and optimized. We utilize the multi-modal tree structure\nto explicitly align the heterogeneous image-sentence data by maximizing the\nsemantic and structural similarity between corresponding inter-modal tree\nnodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks\ndemonstrate the superiority of the proposed model in comparison to the\nstate-of-the-art methods.</p>\n", "tags": ["Evaluation"] },
{"key": "ge2022cross", "year": "2023", "citations": "29", "title":"Cross-modal Semantic Enhanced Interaction for Image-Sentence Retrieval", "abstract": "<p>Image-sentence retrieval has attracted extensive research attention in\nmultimedia and computer vision due to its promising application. The key issue\nlies in jointly learning the visual and textual representation to accurately\nestimate their similarity. To this end, the mainstream schema adopts an\nobject-word based attention to calculate their relevance scores and refine\ntheir interactive representations with the attention features, which, however,\nneglects the context of the object representation on the inter-object\nrelationship that matches the predicates in sentences. In this paper, we\npropose a Cross-modal Semantic Enhanced Interaction method, termed CMSEI for\nimage-sentence retrieval, which correlates the intra- and inter-modal semantics\nbetween objects and words. In particular, we first design the intra-modal\nspatial and semantic graphs based reasoning to enhance the semantic\nrepresentations of objects guided by the explicit relationships of the objects’\nspatial positions and their scene graph. Then the visual and textual semantic\nrepresentations are refined jointly via the inter-modal interactive attention\nand the cross-modal alignment. To correlate the context of objects with the\ntextual context, we further refine the visual semantic representation via the\ncross-level object-sentence and word-image based interactive attention.\nExperimental results on seven standard evaluation metrics show that the\nproposed CMSEI outperforms the state-of-the-art and the alternative approaches\non MS-COCO and Flickr30K benchmarks.</p>\n", "tags": ["Evaluation"] },
{"key": "ge2025graph", "year": "2014", "citations": "69", "title":"Graph Cuts for Supervised Binary Coding", "abstract": "<p>Learning short binary codes is challenged by the inherent discrete\nnature of the problem. The graph cuts algorithm is a well-studied\ndiscrete label assignment solution in computer vision, but has not yet\nbeen applied to solve the binary coding problems. This is partially because\nit was unclear how to use it to learn the encoding (hashing) functions\nfor out-of-sample generalization. In this paper, we formulate supervised\nbinary coding as a single optimization problem that involves both\nthe encoding functions and the binary label assignment. Then we apply\nthe graph cuts algorithm to address the discrete optimization problem\ninvolved, with no continuous relaxation. This method, named as Graph\nCuts Coding (GCC), shows competitive results in various datasets.</p>\n", "tags": ["Hashing-Methods","Compact-Codes","Supervised","Datasets"] },
{"key": "gella2017image", "year": "2017", "citations": "72", "title":"Image Pivoting for Learning Multilingual Multimodal Representations", "abstract": "<p>In this paper we propose a model to learn multimodal multilingual\nrepresentations for matching images and sentences in different languages, with\nthe aim of advancing multilingual versions of image search and image\nunderstanding. Our model learns a common representation for images and their\ndescriptions in two different languages (which need not be parallel) by\nconsidering the image as a pivot between two languages. We introduce a new\npairwise ranking loss function which can handle both symmetric and asymmetric\nsimilarity between the two modalities. We evaluate our models on\nimage-description ranking for German and English, and on semantic textual\nsimilarity of image descriptions in English. In both cases we achieve\nstate-of-the-art performance.</p>\n", "tags": ["Evaluation","Image-Retrieval","EMNLP"] },
{"key": "gerritse2020graph", "year": "2020", "citations": "25", "title":"Graph-Embedding Empowered Entity Retrieval", "abstract": "<p>In this research, we improve upon the current state of the art in entity\nretrieval by re-ranking the result list using graph embeddings. The paper shows\nthat graph embeddings are useful for entity-oriented search tasks. We\ndemonstrate empirically that encoding information from the knowledge graph into\n(graph) embeddings contributes to a higher increase in effectiveness of entity\nretrieval results than using plain word embeddings. We analyze the impact of\nthe accuracy of the entity linker on the overall retrieval effectiveness. Our\nanalysis further deploys the cluster hypothesis to explain the observed\nadvantages of graph embeddings over the more widely used word embeddings, for\nuser tasks involving ranking entities.</p>\n", "tags": ["Hybrid-ANN-Methods","Re-Ranking"] },
{"key": "gerritse2025graph", "year": "2020", "citations": "25", "title":"Graph-Embedding Empowered Entity Retrieval", "abstract": "<p>In this research, we investigate methods for entity retrieval using graph embeddings. While various methods have been proposed over the years, most utilize a single graph embedding and entity linking approach. This hinders our understanding of how different graph embedding and entity linking methods impact entity retrieval. To address this gap, we investigate the effects of three different categories of graph embedding techniques and five different entity linking methods. We perform a reranking of entities using the distance between the embeddings of annotated entities and the entities we wish to rerank. We conclude that the selection of both graph embeddings and entity linkers significantly impacts the effectiveness of entity retrieval. For graph embeddings, methods that incorporate both graph structure and textual descriptions of entities are the most effective. For entity linking, both precision and recall concerning concepts are important for optimal retrieval performance. Additionally, it is essential for the graph to encompass as many entities as possible.</p>\n", "tags": ["Evaluation","Re-Ranking"] },
{"key": "gildenblat2019self", "year": "2019", "citations": "36", "title":"Self-Supervised Similarity Learning for Digital Pathology", "abstract": "<p>Using features extracted from networks pretrained on ImageNet is a common\npractice in applications of deep learning for digital pathology. However it\npresents the downside of missing domain specific image information. In digital\npathology, supervised training data is expensive and difficult to collect. We\npropose a self-supervised method for feature extraction by similarity learning\non whole slide images (WSI) that is simple to implement and allows creation of\nrobust and compact image descriptors. We train a siamese network, exploiting\nimage spatial continuity and assuming spatially adjacent tiles in the image are\nmore similar to each other than distant tiles. Our network outputs feature\nvectors of length 128, which allows dramatically lower memory storage and\nfaster processing than networks pretrained on ImageNet. We apply the method on\ndigital pathology WSIs from the Camelyon16 train set and assess and compare our\nmethod by measuring image retrieval of tumor tiles and descriptor pair distance\nratio for distant/near tiles in the Camelyon16 test set. We show that our\nmethod yields better retrieval task results than existing ImageNet based and\ngeneric self-supervised feature extraction methods. To the best of our\nknowledge, this is also the first published method for self-supervised learning\ntailored for digital pathology.</p>\n", "tags": ["Self-Supervised","Supervised","Image-Retrieval"] },
{"key": "gillick2018end", "year": "2018", "citations": "81", "title":"End-to-End Retrieval in Continuous Space", "abstract": "<p>Most text-based information retrieval (IR) systems index objects by words or\nphrases. These discrete systems have been augmented by models that use\nembeddings to measure similarity in continuous space. But continuous-space\nmodels are typically used just to re-rank the top candidates. We consider the\nproblem of end-to-end continuous retrieval, where standard approximate nearest\nneighbor (ANN) search replaces the usual discrete inverted index, and rely\nentirely on distances between learned embeddings. By training simple models\nspecifically for retrieval, with an appropriate model architecture, we improve\non a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval\ntasks. We also discuss the problem of evaluation for retrieval systems, and\nshow how to modify existing pairwise similarity datasets for this purpose.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "gillick2019learning", "year": "2019", "citations": "189", "title":"Learning Dense Representations for Entity Retrieval", "abstract": "<p>We show that it is feasible to perform entity linking by training a dual\nencoder (two-tower) model that encodes mentions and entities in the same dense\nvector space, where candidate entities are retrieved by approximate nearest\nneighbor search. Unlike prior work, this setup does not rely on an alias table\nfollowed by a re-ranker, and is thus the first fully learned entity retrieval\nmodel. We show that our dual encoder, trained using only anchor-text links in\nWikipedia, outperforms discrete alias table and BM25 baselines, and is\ncompetitive with the best comparable results on the standard TACKBP-2010\ndataset. In addition, it can retrieve candidates extremely fast, and\ngeneralizes well to a new dataset derived from Wikinews. On the modeling side,\nwe demonstrate the dramatic value of an unsupervised negative mining algorithm\nfor this task.</p>\n", "tags": ["Unsupervised","Datasets"] },
{"key": "gionis1999similarity", "year": "1999", "citations": "3205", "title":"Similarity Search in High Dimensions via Hashing", "abstract": "<p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,\nall known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;\nin fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our\nmethod gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.\nExperimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>\n", "tags": ["Distance-Metric-Learning","Tree-Based-ANN","Similarity-Search","Hashing-Methods","Vector-Indexing"] },
{"key": "gionis2025similarity", "year": "1999", "citations": "3205", "title":"Similarity Search in High Dimensions via Hashing", "abstract": "<p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,\nall known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;\nin fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our\nmethod gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.\nExperimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>\n", "tags": ["Distance-Metric-Learning","Tree-Based-ANN","Similarity-Search","Hashing-Methods","Vector-Indexing"] },
{"key": "giraud2019superpixel", "year": "2017", "citations": "6", "title":"Superpixel-based Color Transfer", "abstract": "<p>In this work, we propose a fast superpixel-based color transfer method (SCT)\nbetween two images. Superpixels enable to decrease the image dimension and to\nextract a reduced set of color candidates. We propose to use a fast approximate\nnearest neighbor matching algorithm in which we enforce the match diversity by\nlimiting the selection of the same superpixels. A fusion framework is designed\nto transfer the matched colors, and we demonstrate the improvement obtained\nover exact matching results. Finally, we show that SCT is visually competitive\ncompared to state-of-the-art methods.</p>\n", "tags": ["Tools-&-Libraries"] },
{"key": "gkelios2021investigating", "year": "2021", "citations": "30", "title":"Investigating the Vision Transformer Model for Image Retrieval Tasks", "abstract": "<p>This paper introduces a plug-and-play descriptor that can be effectively\nadopted for image retrieval tasks without prior initialization or preparation.\nThe description method utilizes the recently proposed Vision Transformer\nnetwork while it does not require any training data to adjust parameters. In\nimage retrieval tasks, the use of Handcrafted global and local descriptors has\nbeen very successfully replaced, over the last years, by the Convolutional\nNeural Networks (CNN)-based methods. However, the experimental evaluation\nconducted in this paper on several benchmarking datasets against 36\nstate-of-the-art descriptors from the literature demonstrates that a neural\nnetwork that contains no convolutional layer, such as Vision Transformer, can\nshape a global descriptor and achieve competitive results. As fine-tuning is\nnot required, the presented methodology’s low complexity encourages adoption of\nthe architecture as an image retrieval baseline model, replacing the\ntraditional and well adopted CNN-based approaches and inaugurating a new era in\nimage retrieval approaches.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "godil2011retrieval", "year": "2006", "citations": "19", "title":"Retrieval and Clustering from a 3D Human Database based on Body and Head Shape", "abstract": "<p>In this paper, we describe a framework for similarity based retrieval and\nclustering from a 3D human database. Our technique is based on both body and\nhead shape representation and the retrieval is based on similarity of both of\nthem. The 3D human database used in our study is the CAESAR anthropometric\ndatabase which contains approximately 5000 bodies. We have developed a\nweb-based interface for specifying the queries to interact with the retrieval\nsystem. Our approach performs the similarity based retrieval in a reasonable\namount of time and is a practical approach.</p>\n", "tags": ["Tools-&-Libraries"] },
{"key": "gomez2018learning", "year": "2019", "citations": "23", "title":"Learning to Learn from Web Data through Deep Semantic Embeddings", "abstract": "<p>In this paper we propose to learn a multimodal image and text embedding from\nWeb and Social Media data, aiming to leverage the semantic knowledge learnt in\nthe text domain and transfer it to a visual model for semantic image retrieval.\nWe demonstrate that the pipeline can learn from images with associated text\nwithout supervision and perform a thourough analysis of five different text\nembeddings in three different benchmarks. We show that the embeddings learnt\nwith Web and Social Media data have competitive performances over supervised\nmethods in the text based image retrieval task, and we clearly outperform state\nof the art in the MIRFlickr dataset when training in the target data. Further\nwe demonstrate how semantic multimodal image retrieval can be performed using\nthe learnt embeddings, going beyond classical instance-level retrieval\nproblems. Finally, we present a new dataset, InstaCities1M, composed by\nInstagram images and their associated texts that can be used for fair\ncomparison of image-text embeddings.</p>\n", "tags": ["Datasets","Supervised","Evaluation","Image-Retrieval"] },
{"key": "gomez2019self", "year": "2019", "citations": "13", "title":"Self-Supervised Learning from Web Data for Multimodal Retrieval", "abstract": "<p>Self-Supervised learning from multimodal image and text data allows deep\nneural networks to learn powerful features with no need of human annotated\ndata. Web and Social Media platforms provide a virtually unlimited amount of\nthis multimodal data. In this work we propose to exploit this free available\ndata to learn a multimodal image and text embedding, aiming to leverage the\nsemantic knowledge learnt in the text domain and transfer it to a visual model\nfor semantic image retrieval. We demonstrate that the proposed pipeline can\nlearn from images with associated textwithout supervision and analyze the\nsemantic structure of the learnt joint image and text embedding space. We\nperform a thorough analysis and performance comparison of five different state\nof the art text embeddings in three different benchmarks. We show that the\nembeddings learnt with Web and Social Media data have competitive performances\nover supervised methods in the text based image retrieval task, and we clearly\noutperform state of the art in the MIRFlickr dataset when training in the\ntarget data. Further, we demonstrate how semantic multimodal image retrieval\ncan be performed using the learnt embeddings, going beyond classical\ninstance-level retrieval problems. Finally, we present a new dataset,\nInstaCities1M, composed by Instagram images and their associated texts that can\nbe used for fair comparison of image-text embeddings.</p>\n", "tags": ["Supervised","Image-Retrieval","Datasets","Self-Supervised","Evaluation","Multimodal-Retrieval"] },
{"key": "gominski2019challenging", "year": "2019", "citations": "7", "title":"Challenging deep image descriptors for retrieval in heterogeneous iconographic collections", "abstract": "<p>This article proposes to study the behavior of recent and efficient\nstate-of-the-art deep-learning based image descriptors for content-based image\nretrieval, facing a panel of complex variations appearing in heterogeneous\nimage datasets, in particular in cultural collections that may involve\nmulti-source, multi-date and multi-view Permission to make digital</p>\n", "tags": ["Datasets"] },
{"key": "gong2013learning", "year": "2013", "citations": "183", "title":"Learning Binary Codes for High-Dimensional Data Using Bilinear Projections", "abstract": "<p>Recent advances in visual recognition indicate that to\nachieve good retrieval and classification accuracy on largescale\ndatasets like ImageNet, extremely high-dimensional\nvisual descriptors, e.g., Fisher Vectors, are needed. We\npresent a novel method for converting such descriptors to\ncompact similarity-preserving binary codes that exploits\ntheir natural matrix structure to reduce their dimensionality\nusing compact bilinear projections instead of a single\nlarge projection matrix. This method achieves comparable\nretrieval and classification accuracy to the original descriptors\nand to the state-of-the-art Product Quantization\napproach while having orders of magnitude faster code generation\ntime and smaller memory footprint.</p>\n", "tags": ["Datasets","CVPR","Memory-Efficiency","Compact-Codes","Quantization"] },
{"key": "gong2022improving", "year": "2022", "citations": "13", "title":"Improving Visual-Semantic Embeddings by Learning Semantically-Enhanced Hard Negatives for Cross-modal Information Retrieval", "abstract": "<p>Visual Semantic Embedding (VSE) aims to extract the semantics of images and\ntheir descriptions, and embed them into the same latent space for cross-modal\ninformation retrieval. Most existing VSE networks are trained by adopting a\nhard negatives loss function which learns an objective margin between the\nsimilarity of relevant and irrelevant image-description embedding pairs.\nHowever, the objective margin in the hard negatives loss function is set as a\nfixed hyperparameter that ignores the semantic differences of the irrelevant\nimage-description pairs. To address the challenge of measuring the optimal\nsimilarities between image-description pairs before obtaining the trained VSE\nnetworks, this paper presents a novel approach that comprises two main parts:\n(1) finds the underlying semantics of image descriptions; and (2) proposes a\nnovel semantically enhanced hard negatives loss function, where the learning\nobjective is dynamically determined based on the optimal similarity scores\nbetween irrelevant image-description pairs. Extensive experiments were carried\nout by integrating the proposed methods into five state-of-the-art VSE networks\nthat were applied to three benchmark datasets for cross-modal information\nretrieval tasks. The results revealed that the proposed methods achieved the\nbest performance and can also be adopted by existing and future VSE networks.</p>\n", "tags": ["Evaluation","Datasets","CVPR"] },
{"key": "gong2025learning", "year": "2013", "citations": "183", "title":"Learning Binary Codes for High-Dimensional Data Using Bilinear Projections", "abstract": "<p>Recent advances in visual recognition indicate that to\nachieve good retrieval and classification accuracy on largescale\ndatasets like ImageNet, extremely high-dimensional\nvisual descriptors, e.g., Fisher Vectors, are needed. We\npresent a novel method for converting such descriptors to\ncompact similarity-preserving binary codes that exploits\ntheir natural matrix structure to reduce their dimensionality\nusing compact bilinear projections instead of a single\nlarge projection matrix. This method achieves comparable\nretrieval and classification accuracy to the original descriptors\nand to the state-of-the-art Product Quantization\napproach while having orders of magnitude faster code generation\ntime and smaller memory footprint.</p>\n", "tags": ["Datasets","CVPR","Memory-Efficiency","Compact-Codes","Quantization"] },
{"key": "gordo2016end", "year": "2017", "citations": "532", "title":"End-to-end Learning of Deep Visual Representations for Image Retrieval", "abstract": "<p>While deep learning has become a key ingredient in the top performing methods\nfor many computer vision tasks, it has failed so far to bring similar\nimprovements to instance-level image retrieval. In this article, we argue that\nreasons for the underwhelming results of deep methods on image retrieval are\nthreefold: i) noisy training data, ii) inappropriate deep architecture, and\niii) suboptimal training procedure. We address all three issues.\n  First, we leverage a large-scale but noisy landmark dataset and develop an\nautomatic cleaning method that produces a suitable training set for deep\nretrieval. Second, we build on the recent R-MAC descriptor, show that it can be\ninterpreted as a deep and differentiable architecture, and present improvements\nto enhance it. Last, we train this network with a siamese architecture that\ncombines three streams with a triplet loss. At the end of the training process,\nthe proposed architecture produces a global image representation in a single\nforward pass that is well suited for image retrieval. Extensive experiments\nshow that our approach significantly outperforms previous retrieval approaches,\nincluding state-of-the-art methods based on costly local descriptor indexing\nand spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively\nreport 94.7, 96.6, and 94.8 mean average precision. Our representations can\nalso be heavily compressed using product quantization with little loss in\naccuracy. For additional material, please see\nwww.xrce.xerox.com/Deep-Image-Retrieval.</p>\n", "tags": ["Image-Retrieval","Distance-Metric-Learning","Datasets","Scalability","Quantization","Evaluation"] },
{"key": "grauman2012learning", "year": "2012", "citations": "102", "title":"Learning Binary Hash Codes for Large-Scale Image Search", "abstract": "<p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>\n\n<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>\n", "tags": ["Image-Retrieval","Scalability","Distance-Metric-Learning","Survey-Paper","Large-Scale-Search","Locality-Sensitive-Hashing","Supervised","Compact-Codes","Hashing-Methods","Unsupervised"] },
{"key": "grauman2025learning", "year": "2012", "citations": "102", "title":"Learning Binary Hash Codes for Large-Scale Image Search", "abstract": "<p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>\n\n<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>\n", "tags": ["Image-Retrieval","Scalability","Distance-Metric-Learning","Survey-Paper","Large-Scale-Search","Locality-Sensitive-Hashing","Supervised","Compact-Codes","Hashing-Methods","Unsupervised"] },
{"key": "gripon2016associative", "year": "2018", "citations": "9", "title":"Associative Memories to Accelerate Approximate Nearest Neighbor Search", "abstract": "<p>Nearest neighbor search is a very active field in machine learning for it\nappears in many application cases, including classification and object\nretrieval. In its canonical version, the complexity of the search is linear\nwith both the dimension and the cardinal of the collection of vectors the\nsearch is performed in. Recently many works have focused on reducing the\ndimension of vectors using quantization techniques or hashing, while providing\nan approximate result. In this paper we focus instead on tackling the cardinal\nof the collection of vectors. Namely, we introduce a technique that partitions\nthe collection of vectors and stores each part in its own associative memory.\nWhen a query vector is given to the system, associative memories are polled to\nidentify which one contain the closest match. Then an exhaustive search is\nconducted only on the part of vectors stored in the selected associative\nmemory. We study the effectiveness of the system when messages to store are\ngenerated from i.i.d. uniform \\(\\pm\\)1 random variables or 0-1 sparse i.i.d.\nrandom variables. We also conduct experiment on both synthetic data and real\ndata and show it is possible to achieve interesting trade-offs between\ncomplexity and accuracy.</p>\n", "tags": ["Hashing-Methods","Quantization"] },
{"key": "grzegorczyk2016binary", "year": "2017", "citations": "5", "title":"Binary Paragraph Vectors", "abstract": "<p>Recently Le &amp; Mikolov described two log-linear models, called Paragraph\nVector, that can be used to learn state-of-the-art distributed representations\nof documents. Inspired by this work, we present Binary Paragraph Vector models:\nsimple neural networks that learn short binary codes for fast information\nretrieval. We show that binary paragraph vectors outperform autoencoder-based\nbinary codes, despite using fewer bits. We also evaluate their precision in\ntransfer learning settings, where binary codes are inferred for documents\nunrelated to the training corpus. Results from these experiments indicate that\nbinary paragraph vectors can capture semantics relevant for various\ndomain-specific documents. Finally, we present a model that simultaneously\nlearns short binary codes and longer, real-valued representations. This model\ncan be used to rapidly retrieve a short list of highly relevant documents from\na large document collection.</p>\n", "tags": ["Compact-Codes","Evaluation"] },
{"key": "gu2018attention", "year": "2018", "citations": "28", "title":"Attention-Aware Generalized Mean Pooling for Image Retrieval", "abstract": "<p>It has been shown that image descriptors extracted by convolutional neural\nnetworks (CNNs) achieve remarkable results for retrieval problems. In this\npaper, we apply attention mechanism to CNN, which aims at enhancing more\nrelevant features that correspond to important keypoints in the input image.\nThe generated attention-aware features are then aggregated by the previous\nstate-of-the-art generalized mean (GeM) pooling followed by normalization to\nproduce a compact global descriptor, which can be efficiently compared to other\nimage descriptors by the dot product. An extensive comparison of our proposed\napproach with state-of-the-art methods is performed on the new challenging\nROxford5k and RParis6k retrieval benchmarks. Results indicate significant\nimprovement over previous work. In particular, our attention-aware GeM (AGeM)\ndescriptor outperforms state-of-the-art method on ROxford5k under the `Hard’\nevaluation protocal.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "gu2020symmetrical", "year": "2020", "citations": "21", "title":"Symmetrical Synthesis for Deep Metric Learning", "abstract": "<p>Deep metric learning aims to learn embeddings that contain semantic\nsimilarity information among data points. To learn better embeddings, methods\nto generate synthetic hard samples have been proposed. Existing methods of\nsynthetic hard sample generation are adopting autoencoders or generative\nadversarial networks, but this leads to more hyper-parameters, harder\noptimization, and slower training speed. In this paper, we address these\nproblems by proposing a novel method of synthetic hard sample generation called\nsymmetrical synthesis. Given two original feature points from the same class,\nthe proposed method firstly generates synthetic points with each other as an\naxis of symmetry. Secondly, it performs hard negative pair mining within the\noriginal and synthetic points to select a more informative negative pair for\ncomputing the metric learning loss. Our proposed method is hyper-parameter free\nand plug-and-play for existing metric learning losses without network\nmodification. We demonstrate the superiority of our proposed method over\nexisting methods for a variety of loss functions on clustering and image\nretrieval tasks. Our implementations is publicly available.</p>\n", "tags": ["AAAI","Distance-Metric-Learning","Robustness"] },
{"key": "gu2021cross", "year": "2022", "citations": "13", "title":"Cross-modal Image Retrieval with Deep Mutual Information Maximization", "abstract": "<p>In this paper, we study the cross-modal image retrieval, where the inputs\ncontain a source image plus some text that describes certain modifications to\nthis image and the desired image. Prior work usually uses a three-stage\nstrategy to tackle this task: 1) extract the features of the inputs; 2) fuse\nthe feature of the source image and its modified text to obtain fusion feature;\n3) learn a similarity metric between the desired image and the source image +\nmodified text by using deep metric learning. Since classical image/text\nencoders can learn the useful representation and common pair-based loss\nfunctions of distance metric learning are enough for cross-modal retrieval,\npeople usually improve retrieval accuracy by designing new fusion networks.\nHowever, these methods do not successfully handle the modality gap caused by\nthe inconsistent distribution and representation of the features of different\nmodalities, which greatly influences the feature fusion and similarity\nlearning. To alleviate this problem, we adopt the contrastive self-supervised\nlearning method Deep InforMax (DIM) to our approach to bridge this gap by\nenhancing the dependence between the text, the image, and their fusion.\nSpecifically, our method narrows the modality gap between the text modality and\nthe image modality by maximizing mutual information between their not exactly\nsemantically identical representation. Moreover, we seek an effective common\nsubspace for the semantically same fusion feature and desired image’s feature\nby utilizing Deep InforMax between the low-level layer of the image encoder and\nthe high-level layer of the fusion network. Extensive experiments on three\nlarge-scale benchmark datasets show that we have bridged the modality gap\nbetween different modalities and achieve state-of-the-art retrieval\nperformance.</p>\n", "tags": ["Supervised","Image-Retrieval","Distance-Metric-Learning","Datasets","Self-Supervised","Scalability","Evaluation","Multimodal-Retrieval"] },
{"key": "gu2021local", "year": "2022", "citations": "14", "title":"Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking", "abstract": "<p>The goal of local citation recommendation is to recommend a missing reference\nfrom the local citation context and optionally also from the global context. To\nbalance the tradeoff between speed and accuracy of citation recommendation in\nthe context of a large-scale paper database, a viable approach is to first\nprefetch a limited number of relevant documents using efficient ranking methods\nand then to perform a fine-grained reranking using more sophisticated models.\nIn that vein, BM25 has been found to be a tough-to-beat approach to\nprefetching, which is why recent work has focused mainly on the reranking step.\nEven so, we explore prefetching with nearest neighbor search among text\nembeddings constructed by a hierarchical attention network. When coupled with a\nSciBERT reranker fine-tuned on local citation recommendation tasks, our\nhierarchical Attention encoder (HAtten) achieves high prefetch recall for a\ngiven number of candidates to be reranked. Consequently, our reranker requires\nfewer prefetch candidates to rerank, yet still achieves state-of-the-art\nperformance on various local citation recommendation datasets such as ACL-200,\nFullTextPeerRead, RefSeer, and arXiv.</p>\n", "tags": ["Datasets","Recommender-Systems","Re-Ranking","Scalability","Evaluation"] },
{"key": "gu2021multimodal", "year": "2021", "citations": "32", "title":"Multimodal Representation for Neural Code Search", "abstract": "<p>Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of code\nsearch. Last, we define intuitive quantification metrics oriented to the\ncompleteness of semantic and syntactic information of the code data, to help\nunderstand the experimental findings.</p>\n", "tags": ["Scalability","Evaluation"] },
{"key": "gu2022accelerating", "year": "2022", "citations": "10", "title":"Accelerating Code Search with Deep Hashing and Code Classification", "abstract": "<p>Code search is to search reusable code snippets from source code corpus based\non natural languages queries. Deep learning-based methods of code search have\nshown promising results. However, previous methods focus on retrieval accuracy\nbut lacked attention to the efficiency of the retrieval process. We propose a\nnovel method CoSHC to accelerate code search with deep hashing and code\nclassification, aiming to perform an efficient code search without sacrificing\ntoo much accuracy. To evaluate the effectiveness of CoSHC, we apply our method\nto five code search models. Extensive experimental results indicate that\ncompared with previous code search baselines, CoSHC can save more than 90% of\nretrieval time meanwhile preserving at least 99% of retrieval accuracy.</p>\n", "tags": ["Hashing-Methods","Neural-Hashing","Efficiency"] },
{"key": "guan2019post", "year": "2019", "citations": "16", "title":"Post-Training 4-bit Quantization on Embedding Tables", "abstract": "<p>Continuous representations have been widely adopted in recommender systems\nwhere a large number of entities are represented using embedding vectors. As\nthe cardinality of the entities increases, the embedding components can easily\ncontain millions of parameters and become the bottleneck in both storage and\ninference due to large memory consumption. This work focuses on post-training\n4-bit quantization on the continuous embeddings. We propose row-wise uniform\nquantization with greedy search and codebook-based quantization that\nconsistently outperforms state-of-the-art quantization approaches on reducing\naccuracy degradation. We deploy our uniform quantization technique on a\nproduction model in Facebook and demonstrate that it can reduce the model size\nto only 13.89% of the single-precision version while the model quality stays\nneutral.</p>\n", "tags": ["Recommender-Systems","Quantization","Evaluation"] },
{"key": "gui2019fast", "year": "2017", "citations": "265", "title":"Fast Supervised Discrete Hashing", "abstract": "<p>Learning-based hashing algorithms are <code class=\"language-plaintext highlighter-rouge\">hot topics\" because they can greatly\nincrease the scale at which existing methods operate. In this paper, we propose\na new learning-based hashing method called</code>fast supervised discrete hashing”\n(FSDH) based on ``supervised discrete hashing” (SDH). Regressing the training\nexamples (or hash code) to the corresponding class labels is widely used in\nordinary least squares regression. Rather than adopting this method, FSDH uses\na very simple yet effective regression of the class labels of training examples\nto the corresponding hash code to accelerate the algorithm. To the best of our\nknowledge, this strategy has not previously been used for hashing. Traditional\nSDH decomposes the optimization into three sub-problems, with the most critical\nsub-problem - discrete optimization for binary hash codes - solved using\niterative discrete cyclic coordinate descent (DCC), which is time-consuming.\nHowever, FSDH has a closed-form solution and only requires a single rather than\niterative hash code-solving step, which is highly efficient. Furthermore, FSDH\nis usually faster than SDH for solving the projection matrix for least squares\nregression, making FSDH generally faster than SDH. For example, our results\nshow that FSDH is about 12-times faster than SDH when the number of hashing\nbits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than\nFastHash when the number of hashing bits is 64 on the MNIST data-base. Our\nexperimental results show that FSDH is not only fast, but also outperforms\nother comparative methods.</p>\n", "tags": ["Hashing-Methods","Supervised"] },
{"key": "gui2019supervised", "year": "2016", "citations": "92", "title":"Supervised Discrete Hashing with Relaxation", "abstract": "<p>Data-dependent hashing has recently attracted attention due to being able to\nsupport efficient retrieval and storage of high-dimensional data such as\ndocuments, images, and videos. In this paper, we propose a novel learning-based\nhashing method called “Supervised Discrete Hashing with Relaxation” (SDHR)\nbased on “Supervised Discrete Hashing” (SDH). SDH uses ordinary least squares\nregression and traditional zero-one matrix encoding of class label information\nas the regression target (code words), thus fixing the regression target. In\nSDHR, the regression target is instead optimized. The optimized regression\ntarget matrix satisfies a large margin constraint for correct classification of\neach example. Compared with SDH, which uses the traditional zero-one matrix,\nSDHR utilizes the learned regression target matrix and, therefore, more\naccurately measures the classification error of the regression model and is\nmore flexible. As expected, SDHR generally outperforms SDH. Experimental\nresults on two large-scale image datasets (CIFAR-10 and MNIST) and a\nlarge-scale and challenging face dataset (FRGC) demonstrate the effectiveness\nand efficiency of SDHR.</p>\n", "tags": ["Similarity-Search","Supervised","Hashing-Methods","Datasets","Scalability","Efficiency"] },
{"key": "gui2022cross", "year": "2022", "citations": "21", "title":"Cross-Language Binary-Source Code Matching with Intermediate Representations", "abstract": "<p>Binary-source code matching plays an important role in many security and\nsoftware engineering related tasks such as malware detection, reverse\nengineering and vulnerability assessment. Currently, several approaches have\nbeen proposed for binary-source code matching by jointly learning the\nembeddings of binary code and source code in a common vector space. Despite\nmuch effort, existing approaches target on matching the binary code and source\ncode written in a single programming language. However, in practice, software\napplications are often written in different programming languages to cater for\ndifferent requirements and computing platforms. Matching binary and source code\nacross programming languages introduces additional challenges when maintaining\nmulti-language and multi-platform applications. To this end, this paper\nformulates the problem of cross-language binary-source code matching, and\ndevelops a new dataset for this new problem. We present a novel approach XLIR,\nwhich is a Transformer-based neural network by learning the intermediate\nrepresentations for both binary and source code. To validate the effectiveness\nof XLIR, comprehensive experiments are conducted on two tasks of cross-language\nbinary-source code matching, and cross-language source-source code matching, on\ntop of our curated dataset. Experimental results and analysis show that our\nproposed XLIR with intermediate representations significantly outperforms other\nstate-of-the-art models in both of the two tasks.</p>\n", "tags": ["Datasets","Compact-Codes"] },
{"key": "guo2019accelerating", "year": "2019", "citations": "91", "title":"Accelerating Large-Scale Inference with Anisotropic Vector Quantization", "abstract": "<p>Quantization based techniques are the current state-of-the-art for scaling\nmaximum inner product search to massive databases. Traditional approaches to\nquantization aim to minimize the reconstruction error of the database points.\nBased on the observation that for a given query, the database points that have\nthe largest inner products are more relevant, we develop a family of\nanisotropic quantization loss functions. Under natural statistical assumptions,\nwe show that quantization with these loss functions leads to a new variant of\nvector quantization that more greatly penalizes the parallel component of a\ndatapoint’s residual relative to its orthogonal component. The proposed\napproach achieves state-of-the-art results on the public benchmarks available\nat \\url{ann-benchmarks.com}.</p>\n", "tags": ["Scalability","Quantization"] },
{"key": "guo2019hierarchical", "year": "2019", "citations": "22", "title":"Hierarchical Document Encoder for Parallel Corpus Mining", "abstract": "<p>We explore using multilingual document embeddings for nearest neighbor mining\nof parallel data. Three document-level representations are investigated: (i)\ndocument embeddings generated by simply averaging multilingual sentence\nembeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a\nhierarchical multilingual document encoder (HiDE) that builds on our\nsentence-level model. The results show document embeddings derived from\nsentence-level averaging are surprisingly effective for clean datasets, but\nsuggest models trained hierarchically at the document-level are more effective\non noisy data. Analysis experiments demonstrate our hierarchical models are\nvery robust to variations in the underlying sentence embedding quality. Using\ndocument embeddings trained with HiDE achieves state-of-the-art performance on\nUnited Nations (UN) parallel document mining, 94.9% P@1 for en-fr and 97.3% P@1\nfor en-es.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "guo2020deep", "year": "2021", "citations": "5", "title":"Deep Kernel Supervised Hashing for Node Classification in Structural Networks", "abstract": "<p>Node classification in structural networks has been proven to be useful in\nmany real world applications. With the development of network embedding, the\nperformance of node classification has been greatly improved. However, nearly\nall the existing network embedding based methods are hard to capture the actual\ncategory features of a node because of the linearly inseparable problem in\nlow-dimensional space; meanwhile they cannot incorporate simultaneously network\nstructure information and node label information into network embedding. To\naddress the above problems, in this paper, we propose a novel Deep Kernel\nSupervised Hashing (DKSH) method to learn the hashing representations of nodes\nfor node classification. Specifically, a deep multiple kernel learning is first\nproposed to map nodes into suitable Hilbert space to deal with linearly\ninseparable problem. Then, instead of only considering structural similarity\nbetween two nodes, a novel similarity matrix is designed to merge both network\nstructure information and node label information. Supervised by the similarity\nmatrix, the learned hashing representations of nodes simultaneously preserve\nthe two kinds of information well from the learned Hilbert space. Extensive\nexperiments show that the proposed method significantly outperforms the\nstate-of-the-art baselines over three real world benchmark datasets.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Neural-Hashing","Evaluation"] },
{"key": "gupta2022medical", "year": "2023", "citations": "7", "title":"Medical Image Retrieval via Nearest Neighbor Search on Pre-trained Image Features", "abstract": "<p>Nearest neighbor search (NNS) aims to locate the points in high-dimensional\nspace that is closest to the query point. The brute-force approach for finding\nthe nearest neighbor becomes computationally infeasible when the number of\npoints is large. The NNS has multiple applications in medicine, such as\nsearching large medical imaging databases, disease classification, diagnosis,\netc. With a focus on medical imaging, this paper proposes DenseLinkSearch an\neffective and efficient algorithm that searches and retrieves the relevant\nimages from heterogeneous sources of medical images. Towards this, given a\nmedical database, the proposed algorithm builds the index that consists of\npre-computed links of each point in the database. The search algorithm utilizes\nthe index to efficiently traverse the database in search of the nearest\nneighbor. We extensively tested the proposed NNS approach and compared the\nperformance with state-of-the-art NNS approaches on benchmark datasets and our\ncreated medical image datasets. The proposed approach outperformed the existing\napproach in terms of retrieving accurate neighbors and retrieval speed. We also\nexplore the role of medical image feature representation in content-based\nmedical image retrieval tasks. We propose a Transformer-based feature\nrepresentation technique that outperformed the existing pre-trained Transformer\napproach on CLEF 2011 medical image retrieval task. The source code of our\nexperiments are available at https://github.com/deepaknlp/DLS.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "gómez2018single", "year": "2018", "citations": "54", "title":"Single Shot Scene Text Retrieval", "abstract": "<p>Textual information found in scene images provides high level semantic\ninformation about the image and its context and it can be leveraged for better\nscene understanding. In this paper we address the problem of scene text\nretrieval: given a text query, the system must return all images containing the\nqueried text. The novelty of the proposed model consists in the usage of a\nsingle shot CNN architecture that predicts at the same time bounding boxes and\na compact text representation of the words in them. In this way, the text based\nimage retrieval task can be casted as a simple nearest neighbor search of the\nquery text representation over the outputs of the CNN over the entire image\ndatabase. Our experiments demonstrate that the proposed architecture\noutperforms previous state-of-the-art while it offers a significant increase in\nprocessing speed.</p>\n", "tags": ["Text-Retrieval","Image-Retrieval"] },
{"key": "han2017beyond", "year": "2017", "citations": "10", "title":"Beyond SIFT using Binary features for Loop Closure Detection", "abstract": "<p>In this paper a binary feature based Loop Closure Detection (LCD) method is\nproposed, which for the first time achieves higher precision-recall (PR)\nperformance compared with state-of-the-art SIFT feature based approaches. The\nproposed system originates from our previous work Multi-Index hashing for Loop\nclosure Detection (MILD), which employs Multi-Index Hashing\n(MIH)~\\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of\nbinary features. As the accuracy of MILD is limited by repeating textures and\ninaccurate image similarity measurement, burstiness handling is introduced to\nsolve this problem and achieves considerable accuracy improvement.\nAdditionally, a comprehensive theoretical analysis on MIH used in MILD is\nconducted to further explore the potentials of hashing methods for ANN search\nof binary features from probabilistic perspective. This analysis provides more\nfreedom on best parameter choosing in MIH for different application scenarios.\nExperiments on popular public datasets show that the proposed approach achieved\nthe highest accuracy compared with state-of-the-art while running at 30Hz for\ndatabases containing thousands of images.</p>\n", "tags": ["Similarity-Search","Vector-Indexing","Hashing-Methods","Datasets","Evaluation"] },
{"key": "han2021text", "year": "2021", "citations": "36", "title":"Text-Based Person Search with Limited Data", "abstract": "<p>Text-based person search (TBPS) aims at retrieving a target person from an\nimage gallery with a descriptive text query. Solving such a fine-grained\ncross-modal retrieval task is challenging, which is further hampered by the\nlack of large-scale datasets. In this paper, we present a framework with two\nnovel components to handle the problems brought by limited data. Firstly, to\nfully utilize the existing small-scale benchmarking datasets for more\ndiscriminative feature learning, we introduce a cross-modal momentum\ncontrastive learning framework to enrich the training data for a given\nmini-batch. Secondly, we propose to transfer knowledge learned from existing\ncoarse-grained large-scale datasets containing image-text pairs from\ndrastically different problem domains to compensate for the lack of TBPS\ntraining data. A transfer learning method is designed so that useful\ninformation can be transferred despite the large domain gap. Armed with these\ncomponents, our method achieves new state of the art on the CUHK-PEDES dataset\nwith significant improvements over the prior art in terms of Rank-1 and mAP.\nOur code is available at https://github.com/BrandonHanx/TextReID.</p>\n", "tags": ["Tools-&-Libraries","Datasets","Self-Supervised","Scalability","Evaluation","Multimodal-Retrieval"] },
{"key": "hansen2019unsupervised", "year": "2019", "citations": "25", "title":"Unsupervised Semantic Hashing with Pairwise Reconstruction", "abstract": "<p>Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing, documents are encoded as short binary vectors (i.e., hash codes), such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this, we present Semantic Hashing with Pairwise Reconstruction (PairRec), which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes, and then learns to reconstruct the same query document from both of these hash codes (i.e., pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches, and obtain significant performance improvements in the task of document similarity search.</p>\n", "tags": ["Scalability","Datasets","Text-Retrieval","SIGIR","Supervised","Similarity-Search","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "hansen2020content", "year": "2020", "citations": "36", "title":"Content-aware Neural Hashing for Cold-start Recommendation", "abstract": "<p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item’s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>\n", "tags": ["Recommender-Systems","Neural-Hashing","Text-Retrieval","SIGIR","Hashing-Methods","Evaluation"] },
{"key": "hansen2020unsupervised", "year": "2020", "citations": "22", "title":"Unsupervised Semantic Hashing with Pairwise Reconstruction", "abstract": "<p>Semantic Hashing is a popular family of methods for efficient similarity\nsearch in large-scale datasets. In Semantic Hashing, documents are encoded as\nshort binary vectors (i.e., hash codes), such that semantic similarity can be\nefficiently computed using the Hamming distance. Recent state-of-the-art\napproaches have utilized weak supervision to train better performing hashing\nmodels. Inspired by this, we present Semantic Hashing with Pairwise\nReconstruction (PairRec), which is a discrete variational autoencoder based\nhashing model. PairRec first encodes weakly supervised training pairs (a query\ndocument and a semantically similar document) into two hash codes, and then\nlearns to reconstruct the same query document from both of these hash codes\n(i.e., pairwise reconstruction). This pairwise reconstruction enables our model\nto encode local neighbourhood structures within the hash code directly through\nthe decoder. We experimentally compare PairRec to traditional and\nstate-of-the-art approaches, and obtain significant performance improvements in\nthe task of document similarity search.</p>\n", "tags": ["Similarity-Search","Supervised","Text-Retrieval","Hashing-Methods","Datasets","SIGIR","Unsupervised","Scalability","Evaluation"] },
{"key": "hansen2021unsupervised", "year": "2021", "citations": "8", "title":"Unsupervised Multi-Index Semantic Hashing", "abstract": "<p>Semantic hashing represents documents as compact binary vectors (hash codes)\nand allows both efficient and effective similarity search in large-scale\ninformation retrieval. The state of the art has primarily focused on learning\nhash codes that improve similarity search effectiveness, while assuming a\nbrute-force linear scan strategy for searching over all the hash codes, even\nthough much faster alternatives exist. One such alternative is multi-index\nhashing, an approach that constructs a smaller candidate set to search over,\nwhich depending on the distribution of the hash codes can lead to sub-linear\nsearch time. In this work, we propose Multi-Index Semantic Hashing (MISH), an\nunsupervised hashing model that learns hash codes that are both effective and\nhighly efficient by being optimized for multi-index hashing. We derive novel\ntraining objectives, which enable to learn hash codes that reduce the candidate\nsets produced by multi-index hashing, while being end-to-end trainable. In\nfact, our proposed training objectives are model agnostic, i.e., not tied to\nhow the hash codes are generated specifically in MISH, and are straight-forward\nto include in existing and future semantic hashing models. We experimentally\ncompare MISH to state-of-the-art semantic hashing baselines in the task of\ndocument similarity search. We find that even though multi-index hashing also\nimproves the efficiency of the baselines compared to a linear scan, they are\nstill upwards of 33% slower than MISH, while MISH is still able to obtain\nstate-of-the-art effectiveness.</p>\n", "tags": ["Similarity-Search","Vector-Indexing","Supervised","Text-Retrieval","Hashing-Methods","Neural-Hashing","Unsupervised","Scalability","Efficiency"] },
{"key": "hansen2025content", "year": "2020", "citations": "36", "title":"Content-aware Neural Hashing for Cold-start Recommendation", "abstract": "<p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item’s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>\n", "tags": ["Recommender-Systems","Neural-Hashing","Text-Retrieval","SIGIR","Hashing-Methods","Evaluation"] },
{"key": "hansen2025unsupervised", "year": "2019", "citations": "25", "title":"Unsupervised Neural Generative Semantic Hashing", "abstract": "<p>Fast similarity search is a key component in large-scale information retrieval, where semantic hashing has become a popular strategy for representing documents as binary hash codes. Recent advances in this area have been obtained through neural network based models: generative models trained by learning to reconstruct the original documents. We present a novel unsupervised generative semantic hashing approach, \\textit{Ranking based Semantic Hashing} (RBSH) that consists of both a variational and a ranking based component. Similarly to variational autoencoders, the variational component is trained to reconstruct the original document conditioned on its generated hash code, and as in prior work, it only considers documents individually. The ranking component solves this limitation by incorporating inter-document similarity into the hash code generation, modelling document ranking through a hinge loss. To circumvent the need for labelled data to compute the hinge loss, we use a weak labeller and thus keep the approach fully unsupervised.\nExtensive experimental evaluation on four publicly available datasets against traditional baselines and recent state-of-the-art methods for semantic hashing shows that RBSH significantly outperforms all other methods across all evaluated hash code lengths. In fact, RBSH hash codes are able to perform similarly to state-of-the-art hash codes while using 2-4x fewer bits.</p>\n", "tags": ["Scalability","Datasets","Text-Retrieval","SIGIR","Similarity-Search","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "hao2016what", "year": "2016", "citations": "9", "title":"What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "abstract": "<p>Previous work has shown that feature maps of deep convolutional neural\nnetworks (CNNs) can be interpreted as feature representation of a particular\nimage region. Features aggregated from these feature maps have been exploited\nfor image retrieval tasks and achieved state-of-the-art performances in recent\nyears. The key to the success of such methods is the feature representation.\nHowever, the different factors that impact the effectiveness of features are\nstill not explored thoroughly. There are much less discussion about the best\ncombination of them.\n  The main contribution of our paper is the thorough evaluations of the various\nfactors that affect the discriminative ability of the features extracted from\nCNNs. Based on the evaluation results, we also identify the best choices for\ndifferent factors and propose a new multi-scale image feature representation\nmethod to encode the image effectively. Finally, we show that the proposed\nmethod generalises well and outperforms the state-of-the-art methods on four\ntypical datasets used for visual instance retrieval.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "hashimoto2021case", "year": "2023", "citations": "20", "title":"Case-based Similar Image Retrieval for Weakly Annotated Large Histopathological Images of Malignant Lymphoma Using Deep Metric Learning", "abstract": "<p>In the present study, we propose a novel case-based similar image retrieval\n(SIR) method for hematoxylin and eosin (H&amp;E)-stained histopathological images\nof malignant lymphoma. When a whole slide image (WSI) is used as an input\nquery, it is desirable to be able to retrieve similar cases by focusing on\nimage patches in pathologically important regions such as tumor cells. To\naddress this problem, we employ attention-based multiple instance learning,\nwhich enables us to focus on tumor-specific regions when the similarity between\ncases is computed. Moreover, we employ contrastive distance metric learning to\nincorporate immunohistochemical (IHC) staining patterns as useful supervised\ninformation for defining appropriate similarity between heterogeneous malignant\nlymphoma cases. In the experiment with 249 malignant lymphoma patients, we\nconfirmed that the proposed method exhibited higher evaluation measures than\nthe baseline case-based SIR methods. Furthermore, the subjective evaluation by\npathologists revealed that our similarity measure using IHC staining patterns\nis appropriate for representing the similarity of H&amp;E-stained tissue images for\nmalignant lymphoma.</p>\n", "tags": ["Distance-Metric-Learning","Supervised","Evaluation","Image-Retrieval"] },
{"key": "he2017hashing", "year": "2018", "citations": "89", "title":"Hashing as Tie-Aware Learning to Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest\nneighbor retrieval. In this paper, we develop learning to rank formulations for\nhashing, aimed at directly optimizing ranking-based evaluation metrics such as\nAverage Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We\nfirst observe that the integer-valued Hamming distance often leads to tied\nrankings, and propose to use tie-aware versions of AP and NDCG to evaluate\nhashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive\ntheir continuous relaxations, and perform gradient-based optimization with deep\nneural networks. Our results establish the new state-of-the-art for image\nretrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["Hashing-Methods","CVPR","Evaluation"] },
{"key": "he2018hashing", "year": "2018", "citations": "89", "title":"Hashing as Tie-Aware Learning to Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval","Evaluation","CVPR"] },
{"key": "he2018local", "year": "2018", "citations": "209", "title":"Local Descriptors Optimized for Average Precision", "abstract": "<p>Extraction of local feature descriptors is a vital stage in the solution\npipelines for numerous computer vision tasks. Learning-based approaches improve\nperformance in certain tasks, but still cannot replace handcrafted features in\ngeneral. In this paper, we improve the learning of local feature descriptors by\noptimizing the performance of descriptor matching, which is a common stage that\nfollows descriptor extraction in local feature based pipelines, and can be\nformulated as nearest neighbor retrieval. Specifically, we directly optimize a\nranking-based retrieval performance metric, Average Precision, using deep\nneural networks. This general-purpose solution can also be viewed as a listwise\nlearning to rank approach, which is advantageous compared to recent local\nranking approaches. On standard benchmarks, descriptors learned with our\nformulation achieve state-of-the-art results in patch verification, patch\nretrieval, and image matching.</p>\n", "tags": ["CVPR","Evaluation"] },
{"key": "he2019k", "year": "2019", "citations": "31", "title":"K-Nearest Neighbors Hashing", "abstract": "<p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which\nenables efficient similarity search and storage. However,\nthe non-isometry sign(·) function makes it hard to project\nthe nearest neighbors in continuous data space into the\nclosest codewords in discrete Hamming space. In this work,\nwe revisit the sign(·) function from the perspective of space partitioning.\nIn specific, we bridge the gap between\nk-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(·).\nTheoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>\n", "tags": ["Datasets","CVPR","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "he2019view", "year": "2019", "citations": "57", "title":"View N-gram Network for 3D Object Retrieval", "abstract": "<p>How to aggregate multi-view representations of a 3D object into an\ninformative and discriminative one remains a key challenge for multi-view 3D\nobject retrieval. Existing methods either use view-wise pooling strategies\nwhich neglect the spatial information across different views or employ\nrecurrent neural networks which may face the efficiency problem. To address\nthese issues, we propose an effective and efficient framework called View\nN-gram Network (VNN). Inspired by n-gram models in natural language processing,\nVNN divides the view sequence into a set of visual n-grams, which involve\noverlapping consecutive view sub-sequences. By doing so, spatial information\nacross multiple views is captured, which helps to learn a discriminative global\nembedding for each 3D object. Experiments on 3D shape retrieval benchmarks,\nincluding ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the\nsuperiority of our proposed method.</p>\n", "tags": ["ICCV","Datasets","Tools-&-Libraries","Efficiency"] },
{"key": "he2025hashing", "year": "2018", "citations": "89", "title":"Hashing as Tie-Aware Learning to Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval","Evaluation","CVPR"] },
{"key": "he2025k", "year": "2019", "citations": "31", "title":"K-Nearest Neighbors Hashing", "abstract": "<p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which\nenables efficient similarity search and storage. However,\nthe non-isometry sign(·) function makes it hard to project\nthe nearest neighbors in continuous data space into the\nclosest codewords in discrete Hamming space. In this work,\nwe revisit the sign(·) function from the perspective of space partitioning.\nIn specific, we bridge the gap between\nk-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(·).\nTheoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>\n", "tags": ["Datasets","CVPR","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "hekmatfar2020embedding", "year": "2021", "citations": "8", "title":"Embedding Ranking-Oriented Recommender System Graphs", "abstract": "<p>Graph-based recommender systems (GRSs) analyze the structural information in\nthe graphical representation of data to make better recommendations, especially\nwhen the direct user-item relation data is sparse. Ranking-oriented GRSs that\nform a major class of recommendation systems, mostly use the graphical\nrepresentation of preference (or rank) data for measuring node similarities,\nfrom which they can infer a recommendation list using a neighborhood-based\nmechanism. In this paper, we propose PGRec, a novel graph-based\nranking-oriented recommendation framework. PGRec models the preferences of the\nusers over items, by a novel graph structure called PrefGraph. This graph is\nthen exploited by an improved embedding approach, taking advantage of both\nfactorization and deep learning methods, to extract vectors representing users,\nitems, and preferences. The resulting embedding are then used for predicting\nusers’ unknown pairwise preferences from which the final recommendation lists\nare inferred. We have evaluated the performance of the proposed method against\nthe state of the art model-based and neighborhood-based recommendation methods,\nand our experiments show that PGRec outperforms the baseline algorithms up to\n3.2% in terms of NDCG@10 in different MovieLens datasets.</p>\n", "tags": ["Tools-&-Libraries","Graph-Based-ANN","Datasets","Recommender-Systems","Evaluation"] },
{"key": "hemati2022learning", "year": "2023", "citations": "9", "title":"Learning Binary and Sparse Permutation-Invariant Representations for Fast and Memory Efficient Whole Slide Image Search", "abstract": "<p>Learning suitable Whole slide images (WSIs) representations for efficient\nretrieval systems is a non-trivial task. The WSI embeddings obtained from\ncurrent methods are in Euclidean space not ideal for efficient WSI retrieval.\nFurthermore, most of the current methods require high GPU memory due to the\nsimultaneous processing of multiple sets of patches. To address these\nchallenges, we propose a novel framework for learning binary and sparse WSI\nrepresentations utilizing a deep generative modelling and the Fisher Vector. We\nintroduce new loss functions for learning sparse and binary\npermutation-invariant WSI representations that employ instance-based training\nachieving better memory efficiency. The learned WSI representations are\nvalidated on The Cancer Genomic Atlas (TCGA) and Liver-Kidney-Stomach (LKS)\ndatasets. The proposed method outperforms Yottixel (a recent search engine for\nhistopathology images) both in terms of retrieval accuracy and speed. Further,\nwe achieve competitive performance against SOTA on the public benchmark LKS\ndataset for WSI classification.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Datasets","Memory-Efficiency","Evaluation","Efficiency"] },
{"key": "hendriksen2021extending", "year": "2022", "citations": "14", "title":"Extending CLIP for Category-to-image Retrieval in E-commerce", "abstract": "<p>E-commerce provides rich multimodal data that is barely leveraged in\npractice. One aspect of this data is a category tree that is being used in\nsearch and recommendation. However, in practice, during a user’s session there\nis often a mismatch between a textual and a visual representation of a given\ncategory. Motivated by the problem, we introduce the task of category-to-image\nretrieval in e-commerce and propose a model for the task, CLIP-ITA. The model\nleverages information from multiple modalities (textual, visual, and attribute\nmodality) to create product representations. We explore how adding information\nfrom multiple modalities (textual, visual, and attribute modality) impacts the\nmodel’s performance. In particular, we observe that CLIP-ITA significantly\noutperforms a comparable model that leverages only the visual modality and a\ncomparable model that leverages the visual and attribute modality.</p>\n", "tags": ["Recommender-Systems","Evaluation","Image-Retrieval"] },
{"key": "henkel2020supporting", "year": "2020", "citations": "5", "title":"Supporting large-scale image recognition with out-of-domain samples", "abstract": "<p>This article presents an efficient end-to-end method to perform\ninstance-level recognition employed to the task of labeling and ranking\nlandmark images. In a first step, we embed images in a high dimensional feature\nspace using convolutional neural networks trained with an additive angular\nmargin loss and classify images using visual similarity. We then efficiently\nre-rank predictions and filter noise utilizing similarity to out-of-domain\nimages. Using this approach we achieved the 1st place in the 2020 edition of\nthe Google Landmark Recognition challenge.</p>\n", "tags": ["Scalability"] },
{"key": "heo2012spherical", "year": "2012", "citations": "380", "title":"Spherical Hashing", "abstract": "<p>Many binary code encoding schemes based on hashing\nhave been actively studied recently, since they can provide\nefficient similarity search, especially nearest neighbor\nsearch, and compact data representations suitable for handling\nlarge scale image databases in many computer vision\nproblems. Existing hashing techniques encode highdimensional\ndata points by using hyperplane-based hashing\nfunctions. In this paper we propose a novel hyperspherebased\nhashing function, spherical hashing, to map more\nspatially coherent data points into a binary code compared\nto hyperplane-based hashing functions. Furthermore, we\npropose a new binary code distance function, spherical\nHamming distance, that is tailored to our hyperspherebased\nbinary coding scheme, and design an efficient iterative\noptimization process to achieve balanced partitioning\nof data points for each hash function and independence between\nhashing functions. Our extensive experiments show\nthat our spherical hashing technique significantly outperforms\nsix state-of-the-art hashing techniques based on hyperplanes\nacross various image benchmarks of sizes ranging\nfrom one to 75 million of GIST descriptors. The performance\ngains are consistent and large, up to 100% improvements.\nThe excellent results confirm the unique merits of\nthe proposed idea in using hyperspheres to encode proximity\nregions in high-dimensional spaces. Finally, our method\nis intuitive and easy to implement.</p>\n", "tags": ["CVPR","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "heo2025spherical", "year": "2012", "citations": "380", "title":"Spherical Hashing", "abstract": "<p>Many binary code encoding schemes based on hashing\nhave been actively studied recently, since they can provide\nefficient similarity search, especially nearest neighbor\nsearch, and compact data representations suitable for handling\nlarge scale image databases in many computer vision\nproblems. Existing hashing techniques encode highdimensional\ndata points by using hyperplane-based hashing\nfunctions. In this paper we propose a novel hyperspherebased\nhashing function, spherical hashing, to map more\nspatially coherent data points into a binary code compared\nto hyperplane-based hashing functions. Furthermore, we\npropose a new binary code distance function, spherical\nHamming distance, that is tailored to our hyperspherebased\nbinary coding scheme, and design an efficient iterative\noptimization process to achieve balanced partitioning\nof data points for each hash function and independence between\nhashing functions. Our extensive experiments show\nthat our spherical hashing technique significantly outperforms\nsix state-of-the-art hashing techniques based on hyperplanes\nacross various image benchmarks of sizes ranging\nfrom one to 75 million of GIST descriptors. The performance\ngains are consistent and large, up to 100% improvements.\nThe excellent results confirm the unique merits of\nthe proposed idea in using hyperspheres to encode proximity\nregions in high-dimensional spaces. Finally, our method\nis intuitive and easy to implement.</p>\n", "tags": ["CVPR","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "hoang2017selective", "year": "2017", "citations": "58", "title":"Selective Deep Convolutional Features for Image Retrieval", "abstract": "<p>Convolutional Neural Network (CNN) is a very powerful approach to extract\ndiscriminative local descriptors for effective image search. Recent work adopts\nfine-tuned strategies to further improve the discriminative power of the\ndescriptors. Taking a different approach, in this paper, we propose a novel\nframework to achieve competitive retrieval performance. Firstly, we propose\nvarious masking schemes, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and remove a large number\nof redundant features. We demonstrate that this can effectively address the\nburstiness issue and improve retrieval accuracy. Secondly, we propose to employ\nrecent embedding and aggregating methods to further enhance feature\ndiscriminability. Extensive experiments demonstrate that our proposed framework\nachieves state-of-the-art retrieval accuracy.</p>\n", "tags": ["Evaluation","Tools-&-Libraries","Image-Retrieval"] },
{"key": "hoang2020unsupervised", "year": "2020", "citations": "28", "title":"Unsupervised Deep Cross-modality Spectral Hashing", "abstract": "<p>This paper presents a novel framework, namely Deep Cross-modality Spectral\nHashing (DCSH), to tackle the unsupervised learning problem of binary hash\ncodes for efficient cross-modal retrieval. The framework is a two-step hashing\napproach which decouples the optimization into (1) binary optimization and (2)\nhashing function learning. In the first step, we propose a novel spectral\nembedding-based algorithm to simultaneously learn single-modality and binary\ncross-modality representations. While the former is capable of well preserving\nthe local structure of each modality, the latter reveals the hidden patterns\nfrom all modalities. In the second step, to learn mapping functions from\ninformative data inputs (images and word embeddings) to binary codes obtained\nfrom the first step, we leverage the powerful CNN for images and propose a\nCNN-based deep architecture to learn text modality. Quantitative evaluations on\nthree standard benchmark datasets demonstrate that the proposed DCSH method\nconsistently outperforms other state-of-the-art methods.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Compact-Codes","Unsupervised","Evaluation","Multimodal-Retrieval"] },
{"key": "hoffer2016semi", "year": "2016", "citations": "28", "title":"Semi-supervised deep learning by metric embedding", "abstract": "<p>Deep networks are successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are usually much less suited for semi-supervised\nproblems because of their tendency to overfit easily when trained on small\namounts of data. In this work we will explore a new training objective that is\ntargeting a semi-supervised regime with only a small subset of labeled data.\nThis criterion is based on a deep metric embedding over distance relations\nwithin the set of labeled samples, together with constraints over the\nembeddings of the unlabeled set. The final learned representations are\ndiscriminative in euclidean space, and hence can be used with subsequent\nnearest-neighbor classification using the labeled samples.</p>\n", "tags": ["Supervised"] },
{"key": "hong2017content", "year": "2017", "citations": "10", "title":"Content-Based Video-Music Retrieval Using Soft Intra-Modal Structure Constraint", "abstract": "<p>Up to now, only limited research has been conducted on cross-modal retrieval\nof suitable music for a specified video or vice versa. Moreover, much of the\nexisting research relies on metadata such as keywords, tags, or associated\ndescription that must be individually produced and attached posterior. This\npaper introduces a new content-based, cross-modal retrieval method for video\nand music that is implemented through deep neural networks. We train the\nnetwork via inter-modal ranking loss such that videos and music with similar\nsemantics end up close together in the embedding space. However, if only the\ninter-modal ranking constraint is used for embedding, modality-specific\ncharacteristics can be lost. To address this problem, we propose a novel soft\nintra-modal structure loss that leverages the relative distance relationship\nbetween intra-modal samples before embedding. We also introduce reasonable\nquantitative and qualitative experimental protocols to solve the lack of\nstandard protocols for less-mature video-music related tasks. Finally, we\nconstruct a large-scale 200K video-music pair benchmark. All the datasets and\nsource code can be found in our online repository\n(https://github.com/csehong/VM-NET).</p>\n", "tags": ["Datasets","Scalability","Evaluation","Multimodal-Retrieval"] },
{"key": "hou2024bridging", "year": "2024", "citations": "11", "title":"Bridging Language and Items for Retrieval and Recommendation", "abstract": "<p>This paper introduces BLaIR, a series of pretrained sentence embedding models\nspecialized for recommendation scenarios. BLaIR is trained to learn\ncorrelations between item metadata and potential natural language context,\nwhich is useful for retrieving and recommending items. To pretrain BLaIR, we\ncollect Amazon Reviews 2023, a new dataset comprising over 570 million reviews\nand 48 million items from 33 categories, significantly expanding beyond the\nscope of previous versions. We evaluate the generalization ability of BLaIR\nacross multiple domains and tasks, including a new task named complex product\nsearch, referring to retrieving relevant items given long, complex natural\nlanguage contexts. Leveraging large language models like ChatGPT, we\ncorrespondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical\nresults on the new task, as well as conventional retrieval and recommendation\ntasks, demonstrate that BLaIR exhibit strong text and item representation\ncapacity. Our datasets, code, and checkpoints are available at:\nhttps://github.com/hyp1231/AmazonReviews2023.</p>\n", "tags": ["Datasets","Recommender-Systems","Evaluation"] },
{"key": "hsu2018unsupervised", "year": "2018", "citations": "26", "title":"Unsupervised Multimodal Representation Learning across Medical Images and Reports", "abstract": "<p>Joint embeddings between medical imaging modalities and associated radiology\nreports have the potential to offer significant benefits to the clinical\ncommunity, ranging from cross-domain retrieval to conditional generation of\nreports to the broader goals of multimodal representation learning. In this\nwork, we establish baseline joint embedding results measured via both local and\nglobal retrieval methods on the soon to be released MIMIC-CXR dataset\nconsisting of both chest X-ray images and the associated radiology reports. We\nexamine both supervised and unsupervised methods on this task and show that for\ndocument retrieval tasks with the learned representations, only a limited\namount of supervision is needed to yield results comparable to those of\nfully-supervised methods.</p>\n", "tags": ["Unsupervised","Datasets","Supervised","Text-Retrieval"] },
{"key": "hu2017learning", "year": "2017", "citations": "206", "title":"Learning Discrete Representations via Information Maximizing Self-Augmented Training", "abstract": "<p>Learning discrete representations of data is a central machine learning task\nbecause of the compactness of the representations and ease of interpretation.\nThe task includes clustering and hash learning as special cases. Deep neural\nnetworks are promising to be used because they can model the non-linearity of\ndata and scale to large datasets. However, their model complexity is huge, and\ntherefore, we need to carefully regularize the networks in order to learn\nuseful representations that exhibit intended invariance for applications of\ninterest. To this end, we propose a method called Information Maximizing\nSelf-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose\nthe invariance on discrete representations. More specifically, we encourage the\npredicted representations of augmented data points to be close to those of the\noriginal data points in an end-to-end fashion. At the same time, we maximize\nthe information-theoretic dependency between data and their predicted discrete\nrepresentations. Extensive experiments on benchmark datasets show that IMSAT\nproduces state-of-the-art results for both clustering and unsupervised hash\nlearning.</p>\n", "tags": ["Hashing-Methods","Unsupervised","Datasets","Evaluation"] },
{"key": "hu2018web", "year": "2018", "citations": "49", "title":"Web-Scale Responsive Visual Search at Bing", "abstract": "<p>In this paper, we introduce a web-scale general visual search system deployed\nin Microsoft Bing. The system accommodates tens of billions of images in the\nindex, with thousands of features for each image, and can respond in less than\n200 ms. In order to overcome the challenges in relevance, latency, and\nscalability in such large scale of data, we employ a cascaded learning-to-rank\nframework based on various latest deep learning visual features, and deploy in\na distributed heterogeneous computing platform. Quantitative and qualitative\nexperiments show that our system is able to support various applications on\nBing website and apps.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","KDD","Scalability","Large-Scale-Search"] },
{"key": "hu2019separated", "year": "2019", "citations": "30", "title":"Separated Variational Hashing Networks for Cross-Modal Retrieval", "abstract": "<p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>\n", "tags": ["Efficiency","Multimodal-Retrieval","Memory-Efficiency","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "hu2022feature", "year": "2022", "citations": "12", "title":"Feature Representation Learning for Unsupervised Cross-domain Image Retrieval", "abstract": "<p>Current supervised cross-domain image retrieval methods can achieve excellent\nperformance. However, the cost of data collection and labeling imposes an\nintractable barrier to practical deployment in real applications. In this\npaper, we investigate the unsupervised cross-domain image retrieval task, where\nclass labels and pairing annotations are no longer a prerequisite for training.\nThis is an extremely challenging task because there is no supervision for both\nin-domain feature representation learning and cross-domain alignment. We\naddress both challenges by introducing: 1) a new cluster-wise contrastive\nlearning mechanism to help extract class semantic-aware features, and 2) a\nnovel distance-of-distance loss to effectively measure and minimize the domain\ndiscrepancy without any external supervision. Experiments on the Office-Home\nand DomainNet datasets consistently show the superior image retrieval\naccuracies of our framework over state-of-the-art approaches. Our source code\ncan be found at https://github.com/conghuihu/UCDIR.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Image-Retrieval","Datasets","Unsupervised","Evaluation"] },
{"key": "hu2025separated", "year": "2019", "citations": "30", "title":"Separated Variational Hashing Networks for Cross-Modal Retrieval", "abstract": "<p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>\n", "tags": ["Efficiency","Multimodal-Retrieval","Memory-Efficiency","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "huang2016local", "year": "2016", "citations": "124", "title":"Local Similarity-Aware Deep Feature Embedding", "abstract": "<p>Existing deep embedding methods in vision tasks are capable of learning a\ncompact Euclidean space from images, where Euclidean distances correspond to a\nsimilarity metric. To make learning more effective and efficient, hard sample\nmining is usually employed, with samples identified through computing the\nEuclidean feature distance. However, the global Euclidean distance cannot\nfaithfully characterize the true feature similarity in a complex visual feature\nspace, where the intraclass distance in a high-density region may be larger\nthan the interclass distance in low-density regions. In this paper, we\nintroduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of\nlearning a similarity metric adaptive to local feature structure. The metric\ncan be used to select genuinely hard samples in a local neighborhood to guide\nthe deep embedding learning in an online and robust manner. The new layer is\nappealing in that it is pluggable to any convolutional networks and is trained\nend-to-end. Our local similarity-aware feature embedding not only demonstrates\nfaster convergence and boosted performance on two complex image retrieval\ndatasets, its large margin nature also leads to superior generalization results\nunder the large and open set scenarios of transfer learning and zero-shot\nlearning on ImageNet 2010 and ImageNet-10K datasets.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Image-Retrieval","Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "huang2017cross", "year": "2017", "citations": "15", "title":"Cross-modal Deep Metric Learning with Multi-task Regularization", "abstract": "<p>DNN-based cross-modal retrieval has become a research hotspot, by which users\ncan search results across various modalities like image and text. However,\nexisting methods mainly focus on the pairwise correlation and reconstruction\nerror of labeled data. They ignore the semantically similar and dissimilar\nconstraints between different modalities, and cannot take advantage of\nunlabeled data. This paper proposes Cross-modal Deep Metric Learning with\nMulti-task Regularization (CDMLMR), which integrates quadruplet ranking loss\nand semi-supervised contrastive loss for modeling cross-modal semantic\nsimilarity in a unified multi-task learning architecture. The quadruplet\nranking loss can model the semantically similar and dissimilar constraints to\npreserve cross-modal relative similarity ranking information. The\nsemi-supervised contrastive loss is able to maximize the semantic similarity on\nboth labeled and unlabeled data. Compared to the existing methods, CDMLMR\nexploits not only the similarity ranking information but also unlabeled\ncross-modal data, and thus boosts cross-modal retrieval accuracy.</p>\n", "tags": ["Distance-Metric-Learning","Supervised","Multimodal-Retrieval"] },
{"key": "huang2017online", "year": "2013", "citations": "51", "title":"Online Hashing", "abstract": "<p>Although hash function learning algorithms have achieved great success in\nrecent years, most existing hash models are off-line, which are not suitable\nfor processing sequential or online data. To address this problem, this work\nproposes an online hash model to accommodate data coming in stream for online\nlearning. Specifically, a new loss function is proposed to measure the\nsimilarity loss between a pair of data samples in hamming space. Then, a\nstructured hash model is derived and optimized in a passive-aggressive way.\nTheoretical analysis on the upper bound of the cumulative loss for the proposed\nonline hash model is provided. Furthermore, we extend our online hashing from a\nsingle-model to a multi-model online hashing that trains multiple models so as\nto retain diverse online hashing models in order to avoid biased update. The\ncompetitive efficiency and effectiveness of the proposed online hash models are\nverified through extensive experiments on several large-scale datasets as\ncompared to related hashing methods.</p>\n", "tags": ["Hashing-Methods","Datasets","Scalability","Efficiency"] },
{"key": "huang2017unsupervised", "year": "2017", "citations": "54", "title":"Unsupervised Triplet Hashing for Fast Image Retrieval", "abstract": "<p>Hashing has played a pivotal role in large-scale image retrieval. With the\ndevelopment of Convolutional Neural Network (CNN), hashing learning has shown\ngreat promise. But existing methods are mostly tuned for classification, which\nare not optimized for retrieval tasks, especially for instance-level retrieval.\nIn this study, we propose a novel hashing method for large-scale image\nretrieval. Considering the difficulty in obtaining labeled datasets for image\nretrieval task in large scale, we propose a novel CNN-based unsupervised\nhashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised\nhashing network is designed under the following three principles: 1) more\ndiscriminative representations for image retrieval; 2) minimum quantization\nloss between the original real-valued feature descriptors and the learned hash\ncodes; 3) maximum information entropy for the learned hash codes. Extensive\nexperiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH\noutperforms several state-of-the-art unsupervised hashing methods in terms of\nretrieval accuracy.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Quantization","Neural-Hashing","Unsupervised","Scalability"] },
{"key": "huang2019accelerate", "year": "2019", "citations": "23", "title":"Accelerate Learning of Deep Hashing With Gradient Attention", "abstract": "<p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>\n", "tags": ["Image-Retrieval","Scalability","ICCV","Datasets","Neural-Hashing","Hashing-Methods","Evaluation"] },
{"key": "huang2024cross", "year": "2024", "citations": "9", "title":"Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval", "abstract": "<p>Current image-text retrieval methods have demonstrated impressive performance\nin recent years. However, they still face two problems: the inter-modal\nmatching missing problem and the intra-modal semantic loss problem. These\nproblems can significantly affect the accuracy of image-text retrieval. To\naddress these challenges, we propose a novel method called Cross-modal and\nUni-modal Soft-label Alignment (CUSA). Our method leverages the power of\nuni-modal pre-trained models to provide soft-label supervision signals for the\nimage-text retrieval model. Additionally, we introduce two alignment\ntechniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label\nAlignment (USA), to overcome false negatives and enhance similarity recognition\nbetween uni-modal samples. Our method is designed to be plug-and-play, meaning\nit can be easily applied to existing image-text retrieval models without\nchanging their original architectures. Extensive experiments on various\nimage-text retrieval models and datasets, we demonstrate that our method can\nconsistently improve the performance of image-text retrieval and achieve new\nstate-of-the-art results. Furthermore, our method can also boost the uni-modal\nretrieval performance of image-text retrieval models, enabling it to achieve\nuniversal retrieval. The code and supplementary files can be found at\nhttps://github.com/lerogo/aaai24_itr_cusa.</p>\n", "tags": ["AAAI","Datasets","Text-Retrieval","Evaluation"] },
{"key": "huang2025accelerate", "year": "2019", "citations": "23", "title":"Accelerate Learning of Deep Hashing With Gradient Attention", "abstract": "<p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>\n", "tags": ["Image-Retrieval","Scalability","ICCV","Datasets","Neural-Hashing","Hashing-Methods","Evaluation"] },
{"key": "hui2021efficient", "year": "2022", "citations": "15", "title":"Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition", "abstract": "<p>Point cloud based retrieval for place recognition is still a challenging\nproblem due to drastic appearance and illumination changes of scenes in\nchanging environments. Existing deep learning based global descriptors for the\nretrieval task usually consume a large amount of computation resources (e.g.,\nmemory), which may not be suitable for the cases of limited hardware resources.\nIn this paper, we develop an efficient point cloud learning network (EPC-Net)\nto form a global descriptor for visual place recognition, which can obtain good\nperformance and reduce computation memory and inference time. First, we propose\na lightweight but effective neural network module, called ProxyConv, to\naggregate the local geometric features of point clouds. We leverage the spatial\nadjacent matrix and proxy points to simplify the original edge convolution for\nlower memory consumption. Then, we design a lightweight grouped VLAD network\n(G-VLAD) to form global descriptors for retrieval. Compared with the original\nVLAD network, we propose a grouped fully connected (GFC) layer to decompose the\nhigh-dimensional vectors into a group of low-dimensional vectors, which can\nreduce the number of parameters of the network and maintain the discrimination\nof the feature vector. Finally, to further reduce the inference time, we\ndevelop a simple version of EPC-Net, called EPC-Net-L, which consists of two\nProxyConv modules and one max pooling layer to aggregate global descriptors. By\ndistilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative\nglobal descriptors for retrieval. Extensive experiments on the Oxford dataset\nand three in-house datasets demonstrate that our proposed method can achieve\nstate-of-the-art performance with lower parameters, FLOPs, and runtime per\nframe.</p>\n", "tags": ["Datasets","Scalability","Evaluation"] },
{"key": "hui2022efficient", "year": "2022", "citations": "15", "title":"Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition", "abstract": "<p>Point cloud based retrieval for place recognition is still a challenging\nproblem due to drastic appearance and illumination changes of scenes in\nchanging environments. Existing deep learning based global descriptors for the\nretrieval task usually consume a large amount of computation resources (e.g.,\nmemory), which may not be suitable for the cases of limited hardware resources.\nIn this paper, we develop an efficient point cloud learning network (EPC-Net)\nto form a global descriptor for visual place recognition, which can obtain good\nperformance and reduce computation memory and inference time. First, we propose\na lightweight but effective neural network module, called ProxyConv, to\naggregate the local geometric features of point clouds. We leverage the spatial\nadjacent matrix and proxy points to simplify the original edge convolution for\nlower memory consumption. Then, we design a lightweight grouped VLAD network\n(G-VLAD) to form global descriptors for retrieval. Compared with the original\nVLAD network, we propose a grouped fully connected (GFC) layer to decompose the\nhigh-dimensional vectors into a group of low-dimensional vectors, which can\nreduce the number of parameters of the network and maintain the discrimination\nof the feature vector. Finally, to further reduce the inference time, we\ndevelop a simple version of EPC-Net, called EPC-Net-L, which consists of two\nProxyConv modules and one max pooling layer to aggregate global descriptors. By\ndistilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative\nglobal descriptors for retrieval. Extensive experiments on the Oxford dataset\nand three in-house datasets demonstrate that our proposed method can achieve\nstate-of-the-art performance with lower parameters, FLOPs, and runtime per\nframe.</p>\n", "tags": ["Datasets","Scalability","Evaluation"] },
{"key": "hussein2017unified", "year": "2017", "citations": "10", "title":"Unified Embedding and Metric Learning for Zero-Exemplar Event Detection", "abstract": "<p>Event detection in unconstrained videos is conceived as a content-based video\nretrieval with two modalities: textual and visual. Given a text describing a\nnovel event, the goal is to rank related videos accordingly. This task is\nzero-exemplar, no video examples are given to the novel event.\n  Related works train a bank of concept detectors on external data sources.\nThese detectors predict confidence scores for test videos, which are ranked and\nretrieved accordingly. In contrast, we learn a joint space in which the visual\nand textual representations are embedded. The space casts a novel event as a\nprobability of pre-defined events. Also, it learns to measure the distance\nbetween an event and its related videos.\n  Our model is trained end-to-end on publicly available EventNet. When applied\nto TRECVID Multimedia Event Detection dataset, it outperforms the\nstate-of-the-art by a considerable margin.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","CVPR"] },
{"key": "imbriaco2019aggregated", "year": "2019", "citations": "73", "title":"Aggregated Deep Local Features for Remote Sensing Image Retrieval", "abstract": "<p>Remote Sensing Image Retrieval remains a challenging topic due to the special\nnature of Remote Sensing Imagery. Such images contain various different\nsemantic objects, which clearly complicates the retrieval task. In this paper,\nwe present an image retrieval pipeline that uses attentive, local convolutional\nfeatures and aggregates them using the Vector of Locally Aggregated Descriptors\n(VLAD) to produce a global descriptor. We study various system parameters such\nas the multiplicative and additive attention mechanisms and descriptor\ndimensionality. We propose a query expansion method that requires no external\ninputs. Experiments demonstrate that even without training, the local\nconvolutional features and global representation outperform other systems.\nAfter system tuning, we can achieve state-of-the-art or competitive results.\nFurthermore, we observe that our query expansion method increases overall\nsystem performance by about 3%, using only the top-three retrieved images.\nFinally, we show how dimensionality reduction produces compact descriptors with\nincreased retrieval performance and fast retrieval computation times, e.g. 50%\nfaster than the current systems.</p>\n", "tags": ["Image-Retrieval","Evaluation","Efficiency"] },
{"key": "irie2014locally", "year": "2014", "citations": "94", "title":"Locally Linear Hashing for Extracting Non-Linear Manifolds", "abstract": "<p>Previous efforts in hashing intend to preserve data variance\nor pairwise affinity, but neither is adequate in capturing\nthe manifold structures hidden in most visual data. In\nthis paper, we tackle this problem by reconstructing the locally\nlinear structures of manifolds in the binary Hamming\nspace, which can be learned by locality-sensitive sparse\ncoding. We cast the problem as a joint minimization of\nreconstruction error and quantization loss, and show that,\ndespite its NP-hardness, a local optimum can be obtained\nefficiently via alternative optimization. Our method distinguishes\nitself from existing methods in its remarkable ability\nto extract the nearest neighbors of the query from the\nsame manifold, instead of from the ambient space. On extensive\nexperiments on various image benchmarks, our results\nimprove previous state-of-the-art by 28-74% typically,\nand 627% on the Yale face data.</p>\n", "tags": ["Hashing-Methods","Quantization","CVPR"] },
{"key": "irie2025locally", "year": "2014", "citations": "94", "title":"Locally Linear Hashing for Extracting Non-Linear Manifolds", "abstract": "<p>Previous efforts in hashing intend to preserve data variance\nor pairwise affinity, but neither is adequate in capturing\nthe manifold structures hidden in most visual data. In\nthis paper, we tackle this problem by reconstructing the locally\nlinear structures of manifolds in the binary Hamming\nspace, which can be learned by locality-sensitive sparse\ncoding. We cast the problem as a joint minimization of\nreconstruction error and quantization loss, and show that,\ndespite its NP-hardness, a local optimum can be obtained\nefficiently via alternative optimization. Our method distinguishes\nitself from existing methods in its remarkable ability\nto extract the nearest neighbors of the query from the\nsame manifold, instead of from the ambient space. On extensive\nexperiments on various image benchmarks, our results\nimprove previous state-of-the-art by 28-74% typically,\nand 627% on the Yale face data.</p>\n", "tags": ["Hashing-Methods","Quantization","CVPR"] },
{"key": "iscen2017fast", "year": "2018", "citations": "37", "title":"Fast Spectral Ranking for Similarity Search", "abstract": "<p>Despite the success of deep learning on representing images for particular\nobject retrieval, recent studies show that the learned representations still\nlie on manifolds in a high dimensional space. This makes the Euclidean nearest\nneighbor search biased for this task. Exploring the manifolds online remains\nexpensive even if a nearest neighbor graph has been computed offline. This work\nintroduces an explicit embedding reducing manifold search to Euclidean search\nfollowed by dot product similarity search. This is equivalent to linear graph\nfiltering of a sparse signal in the frequency domain. To speed up online\nsearch, we compute an approximate Fourier basis of the graph offline. We\nimprove the state of art on particular object retrieval datasets including the\nchallenging Instre dataset containing small objects. At a scale of 10^5 images,\nthe offline cost is only a few hours, while query time is comparable to\nstandard similarity search.</p>\n", "tags": ["Similarity-Search","Datasets","CVPR","Efficiency"] },
{"key": "ishaq2019clustered", "year": "2019", "citations": "5", "title":"Clustered Hierarchical Entropy-Scaling Search of Astronomical and Biological Data", "abstract": "<p>Both astronomy and biology are experiencing explosive growth of data,\nresulting in a “big data” problem that stands in the way of a “big data”\nopportunity for discovery. One common question asked of such data is that of\napproximate search (\\(\\rho-\\)nearest neighbors search). We present a hierarchical\nsearch algorithm for such data sets that takes advantage of particular\ngeometric properties apparent in both astronomical and biological data sets,\nnamely the metric entropy and fractal dimensionality of the data. We present\nCHESS (Clustered Hierarchical Entropy-Scaling Search), a search tool with\nvirtually no loss in specificity or sensitivity, demonstrating a \\(13.6\\times\\)\nspeedup over linear search on the Sloan Digital Sky Survey’s APOGEE data set\nand a \\(68\\times\\) speedup on the GreenGenes 16S metagenomic data set, as well as\nasymptotically fewer distance comparisons on APOGEE when compared to the\nFALCONN locality-sensitive hashing library. CHESS demonstrates an asymptotic\ncomplexity not directly dependent on data set size, and is in practice at least\nan order of magnitude faster than linear search by performing fewer distance\ncomparisons. Unlike locality-sensitive hashing approaches, CHESS can work with\nany user-defined distance function. CHESS also allows for implicit data\ncompression, which we demonstrate on the APOGEE data set. We also discuss an\nextension allowing for efficient k-nearest neighbors search.</p>\n", "tags": ["Hashing-Methods","Survey-Paper","Tools-&-Libraries","Efficiency"] },
{"key": "iwasaki2018optimization", "year": "2018", "citations": "39", "title":"Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity Search in High-dimensional Data", "abstract": "<p>Searching for high-dimensional vector data with high accuracy is an\ninevitable search technology for various types of data. Graph-based indexes are\nknown to reduce the query time for high-dimensional data. To further improve\nthe query time by using graphs, we focused on the indegrees and outdegrees of\ngraphs. While a sufficient number of incoming edges (indegrees) are\nindispensable for increasing search accuracy, an excessive number of outgoing\nedges (outdegrees) should be suppressed so as to not increase the query time.\nTherefore, we propose three degree-adjustment methods: static degree adjustment\nof not only outdegrees but also indegrees, dynamic degree adjustment with which\noutdegrees are determined by the search accuracy users require, and path\nadjustment to remove edges that have alternative search paths to reduce\noutdegrees. We also show how to obtain optimal degree-adjustment parameters and\nthat our methods outperformed previous methods for image and textual data.</p>\n", "tags": ["Graph-Based-ANN","Efficiency"] },
{"key": "jain2009fast", "year": "2009", "citations": "265", "title":"Fast Similarity Search for Learned Metrics", "abstract": "<p>We propose a method to efficiently index into a large database of examples according to a learned metric.\nGiven a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric\nlearning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity\nconstraints. To enable sub-linear time similarity search under the learned metric, we show how\nto encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We\nfurther formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces\nwhose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.\nWe demonstrate the approach applied to systems and image datasets, and show that our learned metrics\nimprove accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-\ncient indexing with a learned distance and very large databases.</p>\n", "tags": ["Hashing-Methods","Distance-Metric-Learning","Datasets","Similarity-Search"] },
{"key": "jain2010hashing", "year": "2010", "citations": "69", "title":"Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning", "abstract": "<p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the \ndatabase. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method’s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions \nof unlabeled points.</p>\n", "tags": ["Hashing-Methods","Scalability"] },
{"key": "jain2016approximate", "year": "2016", "citations": "17", "title":"Approximate search with quantized sparse representations", "abstract": "<p>This paper tackles the task of storing a large collection of vectors, such as\nvisual descriptors, and of searching in it. To this end, we propose to\napproximate database vectors by constrained sparse coding, where possible atom\nweights are restricted to belong to a finite subset. This formulation\nencompasses, as particular cases, previous state-of-the-art methods such as\nproduct or residual quantization. As opposed to traditional sparse coding\nmethods, quantized sparse coding includes memory usage as a design constraint,\nthereby allowing us to index a large collection such as the BIGANN\nbillion-sized benchmark. Our experiments, carried out on standard benchmarks,\nshow that our formulation leads to competitive solutions when considering\ndifferent trade-offs between learning/coding time, index size and search\nquality.</p>\n", "tags": ["Memory-Efficiency","Quantization","Evaluation"] },
{"key": "jain2017compact", "year": "2017", "citations": "6", "title":"Compact Environment-Invariant Codes for Robust Visual Place Recognition", "abstract": "<p>Robust visual place recognition (VPR) requires scene representations that are\ninvariant to various environmental challenges such as seasonal changes and\nvariations due to ambient lighting conditions during day and night. Moreover, a\npractical VPR system necessitates compact representations of environmental\nfeatures. To satisfy these requirements, in this paper we suggest a\nmodification to the existing pipeline of VPR systems to incorporate supervised\nhashing. The modified system learns (in a supervised setting) compact binary\ncodes from image feature descriptors. These binary codes imbibe robustness to\nthe visual variations exposed to it during the training phase, thereby, making\nthe system adaptive to severe environmental changes. Also, incorporating\nsupervised hashing makes VPR computationally more efficient and easy to\nimplement on simple hardware. This is because binary embeddings can be learned\nover simple-to-compute features and the distance computation is also in the\nlow-dimensional hamming space of binary codes. We have performed experiments on\nseveral challenging data sets covering seasonal, illumination and viewpoint\nvariations. We also compare two widely used supervised hashing methods of\nCCAITQ and MLH and show that this new pipeline out-performs or closely matches\nthe state-of-the-art deep learning VPR methods that are based on\nhigh-dimensional features extracted from pre-trained deep convolutional neural\nnetworks.</p>\n", "tags": ["Supervised","Hashing-Methods","Neural-Hashing","Compact-Codes","Robustness"] },
{"key": "jain2017learning", "year": "2018", "citations": "14", "title":"Learning a Complete Image Indexing Pipeline", "abstract": "<p>To work at scale, a complete image indexing system comprises two components:\nAn inverted file index to restrict the actual search to only a subset that\nshould contain most of the items relevant to the query; An approximate distance\ncomputation mechanism to rapidly scan these lists. While supervised deep\nlearning has recently enabled improvements to the latter, the former continues\nto be based on unsupervised clustering in the literature. In this work, we\npropose a first system that learns both components within a unifying neural\nframework of structured binary encoding.</p>\n", "tags": ["Vector-Indexing","Supervised","Tools-&-Libraries","Hashing-Methods","CVPR","Unsupervised"] },
{"key": "jain2018learning", "year": "2018", "citations": "14", "title":"Learning a Complete Image Indexing Pipeline", "abstract": "<p>To work at scale, a complete image indexing system comprises two components:\nAn inverted file index to restrict the actual search to only a subset that\nshould contain most of the items relevant to the query; An approximate distance\ncomputation mechanism to rapidly scan these lists. While supervised deep\nlearning has recently enabled improvements to the latter, the former continues\nto be based on unsupervised clustering in the literature. In this work, we\npropose a first system that learns both components within a unifying neural\nframework of structured binary encoding.</p>\n", "tags": ["Vector-Indexing","Supervised","Tools-&-Libraries","Hashing-Methods","CVPR","Unsupervised"] },
{"key": "jain2025fast", "year": "2009", "citations": "265", "title":"Fast Similarity Search for Learned Metrics", "abstract": "<p>We propose a method to efficiently index into a large database of examples according to a learned metric.\nGiven a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric\nlearning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity\nconstraints. To enable sub-linear time similarity search under the learned metric, we show how\nto encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We\nfurther formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces\nwhose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.\nWe demonstrate the approach applied to systems and image datasets, and show that our learned metrics\nimprove accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-\ncient indexing with a learned distance and very large databases.</p>\n", "tags": ["Hashing-Methods","Distance-Metric-Learning","Datasets","Similarity-Search"] },
{"key": "jain2025hashing", "year": "2010", "citations": "69", "title":"Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning", "abstract": "<p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the \ndatabase. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method’s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions \nof unlabeled points.</p>\n", "tags": ["Hashing-Methods","Scalability"] },
{"key": "jang2020generalized", "year": "2020", "citations": "41", "title":"Generalized Product Quantization Network for Semi-supervised Image Retrieval", "abstract": "<p>Image retrieval methods that employ hashing or vector quantization have\nachieved great success by taking advantage of deep learning. However, these\napproaches do not meet expectations unless expensive label information is\nsufficient. To resolve this issue, we propose the first quantization-based\nsemi-supervised image retrieval scheme: Generalized Product Quantization (GPQ)\nnetwork. We design a novel metric learning strategy that preserves semantic\nsimilarity between labeled data, and employ entropy regularization term to\nfully exploit inherent potentials of unlabeled data. Our solution increases the\ngeneralization capacity of the quantization network, which allows overcoming\nprevious limitations in the retrieval community. Extensive experimental results\ndemonstrate that GPQ yields state-of-the-art performance on large-scale real\nimage benchmark datasets.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Datasets","Quantization","CVPR","Scalability","Evaluation"] },
{"key": "jang2021deep", "year": "2022", "citations": "23", "title":"Deep Hash Distillation for Image Retrieval", "abstract": "<p>In hash-based image retrieval systems, degraded or transformed inputs usually\ngenerate different codes from the original, deteriorating the retrieval\naccuracy. To mitigate this issue, data augmentation can be applied during\ntraining. However, even if augmented samples of an image are similar in real\nfeature space, the quantization can scatter them far away in Hamming space.\nThis results in representation discrepancies that can impede training and\ndegrade performance. In this work, we propose a novel self-distilled hashing\nscheme to minimize the discrepancy while exploiting the potential of augmented\ndata. By transferring the hash knowledge of the weakly-transformed samples to\nthe strong ones, we make the hash code insensitive to various transformations.\nWe also introduce hash proxy-based similarity learning and binary cross\nentropy-based quantization loss to provide fine quality hash codes. Ultimately,\nwe construct a deep hashing framework that not only improves the existing deep\nhashing approaches, but also achieves the state-of-the-art retrieval results.\nExtensive experiments are conducted and confirm the effectiveness of our work.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Neural-Hashing","Quantization","Evaluation"] },
{"key": "jang2021self", "year": "2021", "citations": "52", "title":"Self-supervised Product Quantization for Deep Unsupervised Image Retrieval", "abstract": "<p>Supervised deep learning-based hash and vector quantization are enabling fast\nand large-scale image retrieval systems. By fully exploiting label annotations,\nthey are achieving outstanding retrieval performances compared to the\nconventional methods. However, it is painstaking to assign labels precisely for\na vast amount of training data, and also, the annotation process is\nerror-prone. To tackle these issues, we propose the first deep unsupervised\nimage retrieval method dubbed Self-supervised Product Quantization (SPQ)\nnetwork, which is label-free and trained in a self-supervised manner. We design\na Cross Quantized Contrastive learning strategy that jointly learns codewords\nand deep visual descriptors by comparing individually transformed images\n(views). Our method analyzes the image contents to extract descriptive\nfeatures, allowing us to understand image representations for accurate\nretrieval. By conducting extensive experiments on benchmarks, we demonstrate\nthat the proposed method yields state-of-the-art results even without\nsupervised pretraining.</p>\n", "tags": ["ICCV","Unsupervised","Supervised","Image-Retrieval","Quantization","Self-Supervised","Scalability"] },
{"key": "jang2021ultra", "year": "2021", "citations": "9", "title":"Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval", "abstract": "<p>The semantic matching capabilities of neural information retrieval can\nameliorate synonymy and polysemy problems of symbolic approaches. However,\nneural models’ dense representations are more suitable for re-ranking, due to\ntheir inefficiency. Sparse representations, either in symbolic or latent form,\nare more efficient with an inverted index. Taking the merits of the sparse and\ndense representations, we propose an ultra-high dimensional (UHD)\nrepresentation scheme equipped with directly controllable sparsity. UHD’s large\ncapacity and minimal noise and interference among the dimensions allow for\nbinarized representations, which are highly efficient for storage and search.\nAlso proposed is a bucketing method, where the embeddings from multiple layers\nof BERT are selected/merged to represent diverse linguistic aspects. We test\nour models with MS MARCO and TREC CAR, showing that our models outperforms\nother sparse models</p>\n", "tags": ["EMNLP","Hybrid-ANN-Methods","Re-Ranking","Text-Retrieval"] },
{"key": "jeong2018efficient", "year": "2018", "citations": "9", "title":"Efficient end-to-end learning for quantizable representations", "abstract": "<p>Embedding representation learning via neural networks is at the core\nfoundation of modern similarity based search. While much effort has been put in\ndeveloping algorithms for learning binary hamming code representations for\nsearch efficiency, this still requires a linear scan of the entire dataset per\neach query and trades off the search accuracy through binarization. To this\nend, we consider the problem of directly learning a quantizable embedding\nrepresentation and the sparse binary hash code end-to-end which can be used to\nconstruct an efficient hash table not only providing significant search\nreduction in the number of data but also achieving the state of the art search\naccuracy outperforming previous state of the art deep metric learning methods.\nWe also show that finding the optimal sparse binary hash code in a mini-batch\ncan be computed exactly in polynomial time by solving a minimum cost flow\nproblem. Our results on Cifar-100 and on ImageNet datasets show the state of\nthe art search accuracy in precision@k and NMI metrics while providing up to\n98X and 478X search speedup respectively over exhaustive linear search. The\nsource code is available at\nhttps://github.com/maestrojeong/Deep-Hash-Table-ICML18</p>\n", "tags": ["Hashing-Methods","Distance-Metric-Learning","Datasets","Evaluation","Efficiency"] },
{"key": "jeong2022augmenting", "year": "2022", "citations": "7", "title":"Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation", "abstract": "<p>Dense retrieval models, which aim at retrieving the most relevant document\nfor an input query on a dense representation space, have gained considerable\nattention for their remarkable success. Yet, dense models require a vast amount\nof labeled training data for notable performance, whereas it is often\nchallenging to acquire query-document pairs annotated by humans. To tackle this\nproblem, we propose a simple but effective Document Augmentation for dense\nRetrieval (DAR) framework, which augments the representations of documents with\ntheir interpolation and perturbation. We validate the performance of DAR on\nretrieval tasks with two benchmark datasets, showing that the proposed DAR\nsignificantly outperforms relevant baselines on the dense retrieval of both the\nlabeled and unlabeled documents.</p>\n", "tags": ["Datasets","Evaluation","Tools-&-Libraries"] },
{"key": "ji2017cross", "year": "2017", "citations": "79", "title":"Cross-Domain Image Retrieval with Attention Modeling", "abstract": "<p>With the proliferation of e-commerce websites and the ubiquitousness of smart\nphones, cross-domain image retrieval using images taken by smart phones as\nqueries to search products on e-commerce websites is emerging as a popular\napplication. One challenge of this task is to locate the attention of both the\nquery and database images. In particular, database images, e.g. of fashion\nproducts, on e-commerce websites are typically displayed with other\naccessories, and the images taken by users contain noisy background and large\nvariations in orientation and lighting. Consequently, their attention is\ndifficult to locate. In this paper, we exploit the rich tag information\navailable on the e-commerce websites to locate the attention of database\nimages. For query images, we use each candidate image in the database as the\ncontext to locate the query attention. Novel deep convolutional neural network\narchitectures, namely TagYNet and CtxYNet, are proposed to learn the attention\nweights and then extract effective representations of the images. Experimental\nresults on public datasets confirm that our approaches have significant\nimprovement over the existing methods in terms of the retrieval accuracy and\nefficiency.</p>\n", "tags": ["Datasets","Image-Retrieval","Efficiency"] },
{"key": "ji2018attribute", "year": "2020", "citations": "83", "title":"Attribute-Guided Network for Cross-Modal Zero-Shot Hashing", "abstract": "<p>Zero-Shot Hashing aims at learning a hashing model that is trained only by\ninstances from seen categories but can generate well to those of unseen\ncategories. Typically, it is achieved by utilizing a semantic embedding space\nto transfer knowledge from seen domain to unseen domain. Existing efforts\nmainly focus on single-modal retrieval task, especially Image-Based Image\nRetrieval (IBIR). However, as a highlighted research topic in the field of\nhashing, cross-modal retrieval is more common in real world applications. To\naddress the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task, we propose a\nnovel Attribute-Guided Network (AgNet), which can perform not only IBIR, but\nalso Text-Based Image Retrieval (TBIR). In particular, AgNet aligns different\nmodal data into a semantically rich attribute space, which bridges the gap\ncaused by modality heterogeneity and zero-shot setting. We also design an\neffective strategy that exploits the attribute to guide the generation of hash\ncodes for image and text within the same network. Extensive experimental\nresults on three benchmark datasets (AwA, SUN, and ImageNet) demonstrate the\nsuperiority of AgNet on both cross-modal and single-modal zero-shot image\nretrieval tasks.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Image-Retrieval","Hashing-Methods","Datasets","Evaluation","Multimodal-Retrieval"] },
{"key": "ji2023hierarchical", "year": "2024", "citations": "8", "title":"Hierarchical Matching and Reasoning for Multi-Query Image Retrieval", "abstract": "<p>As a promising field, Multi-Query Image Retrieval (MQIR) aims at searching\nfor the semantically relevant image given multiple region-specific text\nqueries. Existing works mainly focus on a single-level similarity between image\nregions and text queries, which neglects the hierarchical guidance of\nmulti-level similarities and results in incomplete alignments. Besides, the\nhigh-level semantic correlations that intrinsically connect different\nregion-query pairs are rarely considered. To address above limitations, we\npropose a novel Hierarchical Matching and Reasoning Network (HMRN) for MQIR. It\ndisentangles MQIR into three hierarchical semantic representations, which is\nresponsible to capture fine-grained local details, contextual global scopes,\nand high-level inherent correlations. HMRN comprises two modules: Scalar-based\nMatching (SM) module and Vector-based Reasoning (VR) module. Specifically, the\nSM module characterizes the multi-level alignment similarity, which consists of\na fine-grained local-level similarity and a context-aware global-level\nsimilarity. Afterwards, the VR module is developed to excavate the potential\nsemantic correlations among multiple region-query pairs, which further explores\nthe high-level reasoning similarity. Finally, these three-level similarities\nare aggregated into a joint similarity space to form the ultimate similarity.\nExtensive experiments on the benchmark dataset demonstrate that our HMRN\nsubstantially surpasses the current state-of-the-art methods. For instance,\ncompared with the existing best method Drill-down, the metric R@1 in the last\nround is improved by 23.4%. Our source codes will be released at\nhttps://github.com/LZH-053/HMRN.</p>\n", "tags": ["Evaluation","Datasets","Image-Retrieval"] },
{"key": "jia2019efficient", "year": "2019", "citations": "129", "title":"Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms", "abstract": "<p>Given a data set \\(\\mathcal{D}\\) containing millions of data points and a data\nconsumer who is willing to pay for $\\(X\\) to train a machine learning (ML) model\nover \\(\\mathcal{D}\\), how should we distribute this $\\(X\\) to each data point to\nreflect its “value”? In this paper, we define the “relative value of data” via\nthe Shapley value, as it uniquely possesses properties with appealing\nreal-world interpretations, such as fairness, rationality and\ndecentralizability. For general, bounded utility functions, the Shapley value\nis known to be challenging to compute: to get Shapley values for all \\(N\\) data\npoints, it requires \\(O(2^N)\\) model evaluations for exact computation and\n\\(O(Nlog N)\\) for \\((\\epsilon, \\delta)\\)-approximation. In this paper, we focus on\none popular family of ML models relying on \\(K\\)-nearest neighbors (\\(K\\)NN). The\nmost surprising result is that for unweighted \\(K\\)NN classifiers and regressors,\nthe Shapley value of all \\(N\\) data points can be computed, exactly, in \\(O(Nlog\nN)\\) time – an exponential improvement on computational complexity! Moreover,\nfor \\((\\epsilon, \\delta)\\)-approximation, we are able to develop an algorithm\nbased on Locality Sensitive Hashing (LSH) with only sublinear complexity\n\\(O(N^{h(\\epsilon,K)}log N)\\) when \\(\\epsilon\\) is not too small and \\(K\\) is not\ntoo large. We empirically evaluate our algorithms on up to \\(10\\) million data\npoints and even our exact algorithm is up to three orders of magnitude faster\nthan the baseline approximation algorithm. The LSH-based approximation\nalgorithm can accelerate the value calculation process even further. We then\nextend our algorithms to other scenarios such as (1) weighed \\(K\\)NN classifiers,\n(2) different data points are clustered by different data curators, and (3)\nthere are data analysts providing computation who also requires proper\nvaluation.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing"] },
{"key": "jiang2015scalable", "year": "2015", "citations": "208", "title":"Scalable Graph Hashing with Feature Transformation", "abstract": "<p>Hashing has been widely used for approximate nearest\nneighbor (ANN) search in big data applications\nbecause of its low storage cost and fast retrieval\nspeed. The goal of hashing is to map the data\npoints from the original space into a binary-code\nspace where the similarity (neighborhood structure)\nin the original space is preserved. By directly\nexploiting the similarity to guide the hashing\ncode learning procedure, graph hashing has attracted\nmuch attention. However, most existing graph\nhashing methods cannot achieve satisfactory performance\nin real applications due to the high complexity\nfor graph modeling. In this paper, we propose\na novel method, called scalable graph hashing\nwith feature transformation (SGH), for large-scale\ngraph hashing. Through feature transformation, we\ncan effectively approximate the whole graph without\nexplicitly computing the similarity graph matrix,\nbased on which a sequential learning method\nis proposed to learn the hash functions in a bit-wise\nmanner. Experiments on two datasets with one million\ndata points show that our SGH method can\noutperform the state-of-the-art methods in terms of\nboth accuracy and scalability.</p>\n", "tags": ["Scalability","Efficiency","Datasets","Memory-Efficiency","Hashing-Methods","Evaluation"] },
{"key": "jiang2016deep", "year": "2017", "citations": "752", "title":"Deep Cross-Modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, cross-modal hashing (CMH)\nhas been widely used for similarity search in multimedia retrieval\napplications. However, almost all existing CMH methods are based on\nhand-crafted features which might not be optimally compatible with the\nhash-code learning procedure. As a result, existing CMH methods with\nhandcrafted features may not achieve satisfactory performance. In this paper,\nwe propose a novel cross-modal hashing method, called deep crossmodal hashing\n(DCMH), by integrating feature learning and hash-code learning into the same\nframework. DCMH is an end-to-end learning framework with deep neural networks,\none for each modality, to perform feature learning from scratch. Experiments on\ntwo real datasets with text-image modalities show that DCMH can outperform\nother baselines to achieve the state-of-the-art performance in cross-modal\nretrieval applications.</p>\n", "tags": ["Similarity-Search","Tools-&-Libraries","Hashing-Methods","Datasets","Memory-Efficiency","CVPR","Evaluation"] },
{"key": "jiang2017asymmetric", "year": "2018", "citations": "250", "title":"Asymmetric Deep Supervised Hashing", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.</p>\n", "tags": ["Supervised","Hashing-Methods","Neural-Hashing","AAAI","Scalability","Evaluation","Efficiency"] },
{"key": "jiang2017deep", "year": "2017", "citations": "752", "title":"Deep Cross-Modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity\nsearch in multimedia retrieval applications. However, most\nexisting CMH methods are based on hand-crafted features\nwhich might not be optimally compatible with the hash-code\nlearning procedure. As a result, existing CMH methods\nwith hand-crafted features may not achieve satisfactory\nperformance. In this paper, we propose a novel CMH\nmethod, called deep cross-modal hashing (DCMH), by\nintegrating feature learning and hash-code learning into\nthe same framework. DCMH is an end-to-end learning\nframework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments\non three real datasets with image-text modalities show\nthat DCMH can outperform other baselines to achieve\nthe state-of-the-art performance in cross-modal retrieval\napplications.</p>\n", "tags": ["Datasets","CVPR","Memory-Efficiency","Multimodal-Retrieval","Tools-&-Libraries","Hashing-Methods","Evaluation"] },
{"key": "jiang2017discrete", "year": "2019", "citations": "143", "title":"Discrete Latent Factor Model for Cross-Modal Hashing", "abstract": "<p>Due to its storage and retrieval efficiency, cross-modal hashing~(CMH) has\nbeen widely used for cross-modal similarity search in multimedia applications.\nAccording to the training strategy, existing CMH methods can be mainly divided\ninto two categories: relaxation-based continuous methods and discrete methods.\nIn general, the training of relaxation-based continuous methods is faster than\ndiscrete methods, but the accuracy of relaxation-based continuous methods is\nnot satisfactory. On the contrary, the accuracy of discrete methods is\ntypically better than relaxation-based continuous methods, but the training of\ndiscrete methods is time-consuming. In this paper, we propose a novel CMH\nmethod, called discrete latent factor model based cross-modal hashing~(DLFH),\nfor cross modal similarity search. DLFH is a discrete method which can directly\nlearn the binary hash codes for CMH. At the same time, the training of DLFH is\nefficient. Experiments on real datasets show that DLFH can achieve\nsignificantly better accuracy than existing methods, and the training time of\nDLFH is comparable to that of relaxation-based continuous methods which are\nmuch faster than existing discrete methods.</p>\n", "tags": ["Hashing-Methods","Datasets","Similarity-Search","Efficiency"] },
{"key": "jiang2019graph", "year": "2020", "citations": "28", "title":"Graph-based Multi-view Binary Learning for Image Clustering", "abstract": "<p>Hashing techniques, also known as binary code learning, have recently gained\nincreasing attention in large-scale data analysis and storage. Generally, most\nexisting hash clustering methods are single-view ones, which lack complete\nstructure or complementary information from multiple views. For cluster tasks,\nabundant prior researches mainly focus on learning discrete hash code while few\nworks take original data structure into consideration. To address these\nproblems, we propose a novel binary code algorithm for clustering, which adopts\ngraph embedding to preserve the original data structure, called (Graph-based\nMulti-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding\nthe information of multiple views into a compact binary code, which explores\ncomplementary information from multiple views. In particular, in order to\nmaintain the graph-based structure of the original data, we adopt a Laplacian\nmatrix to preserve the local linear relationship of the data and map it to the\nHamming space. Considering different views have distinctive contributions to\nthe final clustering results, GMBL adopts a strategy of automatically assign\nweights for each view to better guide the clustering. Finally, An alternating\niterative optimization method is adopted to optimize discrete binary codes\ndirectly instead of relaxing the binary constraint in two steps. Experiments on\nfive public datasets demonstrate the superiority of our proposed method\ncompared with previous approaches in terms of clustering performance.</p>\n", "tags": ["Graph-Based-ANN","Hashing-Methods","Datasets","Compact-Codes","Scalability","Evaluation"] },
{"key": "jiang2025deep", "year": "2017", "citations": "752", "title":"Deep Cross-Modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity\nsearch in multimedia retrieval applications. However, most\nexisting CMH methods are based on hand-crafted features\nwhich might not be optimally compatible with the hash-code\nlearning procedure. As a result, existing CMH methods\nwith hand-crafted features may not achieve satisfactory\nperformance. In this paper, we propose a novel CMH\nmethod, called deep cross-modal hashing (DCMH), by\nintegrating feature learning and hash-code learning into\nthe same framework. DCMH is an end-to-end learning\nframework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments\non three real datasets with image-text modalities show\nthat DCMH can outperform other baselines to achieve\nthe state-of-the-art performance in cross-modal retrieval\napplications.</p>\n", "tags": ["Datasets","CVPR","Memory-Efficiency","Multimodal-Retrieval","Tools-&-Libraries","Hashing-Methods","Evaluation"] },
{"key": "jiang2025scalable", "year": "2015", "citations": "208", "title":"Scalable Graph Hashing with Feature Transformation", "abstract": "<p>Hashing has been widely used for approximate nearest\nneighbor (ANN) search in big data applications\nbecause of its low storage cost and fast retrieval\nspeed. The goal of hashing is to map the data\npoints from the original space into a binary-code\nspace where the similarity (neighborhood structure)\nin the original space is preserved. By directly\nexploiting the similarity to guide the hashing\ncode learning procedure, graph hashing has attracted\nmuch attention. However, most existing graph\nhashing methods cannot achieve satisfactory performance\nin real applications due to the high complexity\nfor graph modeling. In this paper, we propose\na novel method, called scalable graph hashing\nwith feature transformation (SGH), for large-scale\ngraph hashing. Through feature transformation, we\ncan effectively approximate the whole graph without\nexplicitly computing the similarity graph matrix,\nbased on which a sequential learning method\nis proposed to learn the hash functions in a bit-wise\nmanner. Experiments on two datasets with one million\ndata points show that our SGH method can\noutperform the state-of-the-art methods in terms of\nboth accuracy and scalability.</p>\n", "tags": ["Scalability","Efficiency","Datasets","Memory-Efficiency","Hashing-Methods","Evaluation"] },
{"key": "jimenez2017class", "year": "2017", "citations": "71", "title":"Class-Weighted Convolutional Features for Visual Instance Search", "abstract": "<p>Image retrieval in realistic scenarios targets large dynamic datasets of\nunlabeled images. In these cases, training or fine-tuning a model every time\nnew images are added to the database is neither efficient nor scalable.\nConvolutional neural networks trained for image classification over large\ndatasets have been proven effective feature extractors for image retrieval. The\nmost successful approaches are based on encoding the activations of\nconvolutional layers, as they convey the image spatial information. In this\npaper, we go beyond this spatial information and propose a local-aware encoding\nof convolutional features based on semantic information predicted in the target\nimage. To this end, we obtain the most discriminative regions of an image using\nClass Activation Maps (CAMs). CAMs are based on the knowledge contained in the\nnetwork and therefore, our approach, has the additional advantage of not\nrequiring external information. In addition, we use CAMs to generate object\nproposals during an unsupervised re-ranking stage after a first fast search.\nOur experiments on two public available datasets for instance retrieval,\nOxford5k and Paris6k, demonstrate the competitiveness of our approach\noutperforming the current state-of-the-art when using off-the-shelf models\ntrained on ImageNet. The source code and model used in this paper are publicly\navailable at http://imatge-upc.github.io/retrieval-2017-cam/.</p>\n", "tags": ["Image-Retrieval","Datasets","Hybrid-ANN-Methods","Re-Ranking","Unsupervised"] },
{"key": "jin2013complementary", "year": "2013", "citations": "59", "title":"Complementary Projection Hashing", "abstract": "<p>Recently, hashing techniques have been widely applied\nto solve the approximate nearest neighbors search problem\nin many vision applications. Generally, these hashing\napproaches generate 2^c buckets, where c is the length\nof the hash code. A good hashing method should satisfy\nthe following two requirements: 1) mapping the nearby\ndata points into the same bucket or nearby (measured by\nthe Hamming distance) buckets. 2) all the data points are\nevenly distributed among all the buckets. In this paper,\nwe propose a novel algorithm named Complementary Projection\nHashing (CPH) to find the optimal hashing functions\nwhich explicitly considers the above two requirements.\nSpecifically, CPH aims at sequentially finding a series of hyperplanes\n(hashing functions) which cross the sparse region\nof the data. At the same time, the data points are evenly distributed\nin the hypercubes generated by these hyperplanes.\nThe experiments comparing with the state-of-the-art hashing\nmethods demonstrate the effectiveness of the proposed\nmethod.</p>\n", "tags": ["Hashing-Methods","ICCV"] },
{"key": "jin2018deep", "year": "2018", "citations": "38", "title":"Deep Saliency Hashing", "abstract": "<p>In recent years, hashing methods have been proved to be effective and\nefficient for the large-scale Web media search. However, the existing general\nhashing methods have limited discriminative power for describing fine-grained\nobjects that share similar overall appearance but have subtle difference. To\nsolve this problem, we for the first time introduce the attention mechanism to\nthe learning of fine-grained hashing codes. Specifically, we propose a novel\ndeep hashing model, named deep saliency hashing (DSaH), which automatically\nmines salient regions and learns semantic-preserving hashing codes\nsimultaneously. DSaH is a two-step end-to-end model consisting of an attention\nnetwork and a hashing network. Our loss function contains three basic\ncomponents, including the semantic loss, the saliency loss, and the\nquantization loss. As the core of DSaH, the saliency loss guides the attention\nnetwork to mine discriminative regions from pairs of images. We conduct\nextensive experiments on both fine-grained and general retrieval datasets for\nperformance evaluation. Experimental results on fine-grained datasets,\nincluding Oxford Flowers-17, Stanford Dogs-120, and CUB Bird demonstrate that\nour DSaH performs the best for fine-grained retrieval task and beats the\nstrongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and\nCUB Bird. DSaH is also comparable to several state-of-the-art hashing methods\non general datasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["Hashing-Methods","Datasets","Neural-Hashing","Scalability","Quantization","Evaluation"] },
{"key": "jin2018unsupervised", "year": "2019", "citations": "25", "title":"Unsupervised Semantic Deep Hashing", "abstract": "<p>In recent years, deep hashing methods have been proved to be efficient since\nit employs convolutional neural network to learn features and hashing codes\nsimultaneously. However, these methods are mostly supervised. In real-world\napplication, it is a time-consuming and overloaded task for annotating a large\nnumber of images. In this paper, we propose a novel unsupervised deep hashing\nmethod for large-scale image retrieval. Our method, namely unsupervised\nsemantic deep hashing (\\textbf{USDH}), uses semantic information preserved in\nthe CNN feature layer to guide the training of network. We enforce four\ncriteria on hashing codes learning based on VGG-19 model: 1) preserving\nrelevant information of feature space in hashing space; 2) minimizing\nquantization loss between binary-like codes and hashing codes; 3) improving the\nusage of each bit in hashing codes by using maximum information entropy, and 4)\ninvariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have\ndemonstrated that \\textbf{USDH} outperforms several state-of-the-art\nunsupervised hashing methods for image retrieval. We also conduct experiments\non Oxford 17 datasets for fine-grained classification to verify its efficiency\nfor other computer vision tasks.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Quantization","Neural-Hashing","Unsupervised","Scalability","Efficiency"] },
{"key": "jin2019deep", "year": "2020", "citations": "91", "title":"Deep Semantic Multimodal Hashing Network for Scalable Image-Text and Video-Text Retrievals", "abstract": "<p>Hashing has been widely applied to multimodal retrieval on large-scale\nmultimedia data due to its efficiency in computation and storage. In this\narticle, we propose a novel deep semantic multimodal hashing network (DSMHN)\nfor scalable image-text and video-text retrieval. The proposed deep hashing\nframework leverages 2-D convolutional neural networks (CNN) as the backbone\nnetwork to capture the spatial information for image-text retrieval, while the\n3-D CNN as the backbone network to capture the spatial and temporal information\nfor video-text retrieval. In the DSMHN, two sets of modality-specific hash\nfunctions are jointly learned by explicitly preserving both intermodality\nsimilarities and intramodality semantic labels. Specifically, with the\nassumption that the learned hash codes should be optimal for the classification\ntask, two stream networks are jointly trained to learn the hash functions by\nembedding the semantic labels on the resultant hash codes. Moreover, a unified\ndeep multimodal hashing framework is proposed to learn compact and high-quality\nhash codes by exploiting the feature representation learning, intermodality\nsimilarity-preserving learning, semantic label-preserving learning, and hash\nfunction learning with different types of loss functions simultaneously. The\nproposed DSMHN method is a generic and scalable deep hashing framework for both\nimage-text and video-text retrievals, which can be flexibly integrated with\ndifferent types of loss functions. We conduct extensive experiments for both\nsingle modal- and cross-modal-retrieval tasks on four widely used\nmultimodal-retrieval data sets. Experimental results on both image-text- and\nvideo-text-retrieval tasks demonstrate that the DSMHN significantly outperforms\nthe state-of-the-art methods.</p>\n", "tags": ["Text-Retrieval","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","Efficiency","Scalability","Multimodal-Retrieval"] },
{"key": "jin2020deep", "year": "2020", "citations": "59", "title":"Deep Saliency Hashing for Fine-grained Retrieval", "abstract": "<p>In recent years, hashing methods have been proved to be\neffective and efficient for the large-scale Web media search.\nHowever, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle\ndifference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained\nhashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which\nautomatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network\nand a hashing network. Our loss function contains three\nbasic components, including the semantic loss, the saliency\nloss, and the quantization loss. As the core of DSaH, the\nsaliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval\ndatasets for performance evaluation. Experimental results\non fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH\nperforms the best for fine-grained retrieval task and beats\nstrongest competitor (DTQ) by approximately 10% on both\nStanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general\ndatasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["Scalability","Datasets","Neural-Hashing","Hashing-Methods","Evaluation","Quantization"] },
{"key": "jin2021unsupervised", "year": "2021", "citations": "14", "title":"Unsupervised Discrete Hashing with Affinity Similarity", "abstract": "<p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>\n", "tags": ["Image-Retrieval","Neural-Hashing","Self-Supervised","Tools-&-Libraries","Supervised","Compact-Codes","Quantization","Hashing-Methods","Evaluation","Unsupervised","Graph-Based-ANN"] },
{"key": "jin2025complementary", "year": "2013", "citations": "59", "title":"Complementary Projection Hashing", "abstract": "<p>Recently, hashing techniques have been widely applied\nto solve the approximate nearest neighbors search problem\nin many vision applications. Generally, these hashing\napproaches generate 2^c buckets, where c is the length\nof the hash code. A good hashing method should satisfy\nthe following two requirements: 1) mapping the nearby\ndata points into the same bucket or nearby (measured by\nthe Hamming distance) buckets. 2) all the data points are\nevenly distributed among all the buckets. In this paper,\nwe propose a novel algorithm named Complementary Projection\nHashing (CPH) to find the optimal hashing functions\nwhich explicitly considers the above two requirements.\nSpecifically, CPH aims at sequentially finding a series of hyperplanes\n(hashing functions) which cross the sparse region\nof the data. At the same time, the data points are evenly distributed\nin the hypercubes generated by these hyperplanes.\nThe experiments comparing with the state-of-the-art hashing\nmethods demonstrate the effectiveness of the proposed\nmethod.</p>\n", "tags": ["Hashing-Methods","ICCV"] },
{"key": "jin2025deep", "year": "2020", "citations": "59", "title":"Deep Saliency Hashing for Fine-grained Retrieval", "abstract": "<p>In recent years, hashing methods have been proved to be\neffective and efficient for the large-scale Web media search.\nHowever, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle\ndifference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained\nhashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which\nautomatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network\nand a hashing network. Our loss function contains three\nbasic components, including the semantic loss, the saliency\nloss, and the quantization loss. As the core of DSaH, the\nsaliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval\ndatasets for performance evaluation. Experimental results\non fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH\nperforms the best for fine-grained retrieval task and beats\nstrongest competitor (DTQ) by approximately 10% on both\nStanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general\ndatasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["Scalability","Datasets","Neural-Hashing","Hashing-Methods","Evaluation","Quantization"] },
{"key": "jin2025unsupervised", "year": "2021", "citations": "14", "title":"Unsupervised Discrete Hashing with Affinity Similarity", "abstract": "<p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>\n", "tags": ["Image-Retrieval","Neural-Hashing","Self-Supervised","Tools-&-Libraries","Supervised","Compact-Codes","Quantization","Hashing-Methods","Evaluation","Unsupervised","Graph-Based-ANN"] },
{"key": "jing2015visual", "year": "2015", "citations": "144", "title":"Visual Search at Pinterest", "abstract": "<p>We demonstrate that, with the availability of distributed computation\nplatforms such as Amazon Web Services and open-source tools, it is possible for\na small engineering team to build, launch and maintain a cost-effective,\nlarge-scale visual search system with widely available tools. We also\ndemonstrate, through a comprehensive set of live experiments at Pinterest, that\ncontent recommendation powered by visual search improve user engagement. By\nsharing our implementation details and the experiences learned from launching a\ncommercial visual search engines from scratch, we hope visual search are more\nwidely incorporated into today’s commercial applications.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Recommender-Systems","KDD","Scalability"] },
{"key": "johnson2017billion", "year": "2019", "citations": "2024", "title":"Billion-scale similarity search with GPUs", "abstract": "<p>Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.</p>\n", "tags": ["Similarity-Search","Datasets","Scalability","Quantization","Large-Scale-Search","Evaluation"] },
{"key": "joly2011random", "year": "2011", "citations": "151", "title":"Random Maximum Margin Hashing", "abstract": "<p>Following the success of hashing methods for multidimensional\nindexing, more and more works are interested\nin embedding visual feature space in compact hash codes.\nSuch approaches are not an alternative to using index structures\nbut a complementary way to reduce both the memory\nusage and the distance computation cost. Several data\ndependent hash functions have notably been proposed to\nclosely fit data distribution and provide better selectivity\nthan usual random projections such as LSH. However, improvements\noccur only for relatively small hash code sizes\nup to 64 or 128 bits. As discussed in the paper, this is mainly\ndue to the lack of independence between the produced hash\nfunctions. We introduce a new hash function family that\nattempts to solve this issue in any kernel space. Rather\nthan boosting the collision probability of close points, our\nmethod focus on data scattering. By training purely random\nsplits of the data, regardless the closeness of the training\nsamples, it is indeed possible to generate consistently\nmore independent hash functions. On the other side, the\nuse of large margin classifiers allows to maintain good generalization\nperformances. Experiments show that our new\nRandom Maximum Margin Hashing scheme (RMMH) outperforms\nfour state-of-the-art hashing methods, notably in\nkernel spaces.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","CVPR","Vector-Indexing"] },
{"key": "joly2025random", "year": "2011", "citations": "151", "title":"Random Maximum Margin Hashing", "abstract": "<p>Following the success of hashing methods for multidimensional\nindexing, more and more works are interested\nin embedding visual feature space in compact hash codes.\nSuch approaches are not an alternative to using index structures\nbut a complementary way to reduce both the memory\nusage and the distance computation cost. Several data\ndependent hash functions have notably been proposed to\nclosely fit data distribution and provide better selectivity\nthan usual random projections such as LSH. However, improvements\noccur only for relatively small hash code sizes\nup to 64 or 128 bits. As discussed in the paper, this is mainly\ndue to the lack of independence between the produced hash\nfunctions. We introduce a new hash function family that\nattempts to solve this issue in any kernel space. Rather\nthan boosting the collision probability of close points, our\nmethod focus on data scattering. By training purely random\nsplits of the data, regardless the closeness of the training\nsamples, it is indeed possible to generate consistently\nmore independent hash functions. On the other side, the\nuse of large margin classifiers allows to maintain good generalization\nperformances. Experiments show that our new\nRandom Maximum Margin Hashing scheme (RMMH) outperforms\nfour state-of-the-art hashing methods, notably in\nkernel spaces.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","CVPR","Vector-Indexing"] },
{"key": "jose2020optimized", "year": "2021", "citations": "7", "title":"Optimized Feature Space Learning for Generating Efficient Binary Codes for Image Retrieval", "abstract": "<p>In this paper we propose an approach for learning low dimensional optimized\nfeature space with minimum intra-class variance and maximum inter-class\nvariance. We address the problem of high-dimensionality of feature vectors\nextracted from neural networks by taking care of the global statistics of\nfeature space. Classical approach of Linear Discriminant Analysis (LDA) is\ngenerally used for generating an optimized low dimensional feature space for\nsingle-labeled images. Since, image retrieval involves both multi-labeled and\nsingle-labeled images, we utilize the equivalence between LDA and Canonical\nCorrelation Analysis (CCA) to generate an optimized feature space for\nsingle-labeled images and use CCA to generate an optimized feature space for\nmulti-labeled images. Our approach correlates the projections of feature\nvectors with label vectors in our CCA based network architecture. The neural\nnetwork minimize a loss function which maximizes the correlation coefficients.\nWe binarize our generated feature vectors with the popular Iterative\nQuantization (ITQ) approach and also propose an ensemble network to generate\nbinary codes of desired bit length for image retrieval. Our measurement of mean\naverage precision shows competitive results on other state-of-the-art\nsingle-labeled and multi-labeled image retrieval datasets.</p>\n", "tags": ["Image-Retrieval","Datasets","Compact-Codes","Quantization","Evaluation"] },
{"key": "jun2019combination", "year": "2019", "citations": "41", "title":"Combination of Multiple Global Descriptors for Image Retrieval", "abstract": "<p>Recent studies in image retrieval task have shown that ensembling different\nmodels and combining multiple global descriptors lead to performance\nimprovement. However, training different models for the ensemble is not only\ndifficult but also inefficient with respect to time and memory. In this paper,\nwe propose a novel framework that exploits multiple global descriptors to get\nan ensemble effect while it can be trained in an end-to-end manner. The\nproposed framework is flexible and expandable by the global descriptor, CNN\nbackbone, loss, and dataset. Moreover, we investigate the effectiveness of\ncombining multiple global descriptors with quantitative and qualitative\nanalysis. Our extensive experiments show that the combined descriptor\noutperforms a single global descriptor, as it can utilize different types of\nfeature properties. In the benchmark evaluation, the proposed framework\nachieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop\nClothes, and Stanford Online Products on image retrieval tasks. Our model\nimplementations and pretrained models are publicly available.</p>\n", "tags": ["Datasets","Evaluation","Tools-&-Libraries","Image-Retrieval"] },
{"key": "jush2023medical", "year": "2024", "citations": "8", "title":"Medical Image Retrieval Using Pretrained Embeddings", "abstract": "<p>A wide range of imaging techniques and data formats available for medical\nimages make accurate retrieval from image databases challenging.\n  Efficient retrieval systems are crucial in advancing medical research,\nenabling large-scale studies and innovative diagnostic tools. Thus, addressing\nthe challenges of medical image retrieval is essential for the continued\nenhancement of healthcare and research.\n  In this study, we evaluated the feasibility of employing four\nstate-of-the-art pretrained models for medical image retrieval at modality,\nbody region, and organ levels and compared the results of two similarity\nindexing approaches. Since the employed networks take 2D images, we analyzed\nthe impacts of weighting and sampling strategies to incorporate 3D information\nduring retrieval of 3D volumes. We showed that medical image retrieval is\nfeasible using pretrained networks without any additional training or\nfine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for\nvarious tasks at modality, body region, and organ level.</p>\n", "tags": ["Similarity-Search","Scalability","Evaluation","Image-Retrieval"] },
{"key": "jääsaari2018efficient", "year": "2019", "citations": "12", "title":"Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor algorithms are used to speed up nearest neighbor\nsearch in a wide array of applications. However, current indexing methods\nfeature several hyperparameters that need to be tuned to reach an acceptable\naccuracy–speed trade-off. A grid search in the parameter space is often\nimpractically slow due to a time-consuming index-building procedure. Therefore,\nwe propose an algorithm for automatically tuning the hyperparameters of\nindexing methods based on randomized space-partitioning trees. In particular,\nwe present results using randomized k-d trees, random projection trees and\nrandomized PCA trees. The tuning algorithm adds minimal overhead to the\nindex-building process but is able to find the optimal hyperparameters\naccurately. We demonstrate that the algorithm is significantly faster than\nexisting approaches, and that the indexing methods used are competitive with\nthe state-of-the-art methods in query time while being faster to build.</p>\n", "tags": ["Tree-Based-ANN","Locality-Sensitive-Hashing","Efficiency"] },
{"key": "kalantidis2015cross", "year": "2016", "citations": "418", "title":"Cross-dimensional Weighting for Aggregated Deep Convolutional Features", "abstract": "<p>We propose a simple and straightforward way of creating powerful image\nrepresentations via cross-dimensional weighting and aggregation of deep\nconvolutional neural network layer outputs. We first present a generalized\nframework that encompasses a broad family of approaches and includes\ncross-dimensional pooling and weighting steps. We then propose specific\nnon-parametric schemes for both spatial- and channel-wise weighting that boost\nthe effect of highly active spatial responses and at the same time regulate\nburstiness effects. We experiment on different public datasets for image search\nand show that our approach outperforms the current state-of-the-art for\napproaches based on pre-trained networks. We also provide an easy-to-use, open\nsource implementation that reproduces our results.</p>\n", "tags": ["Datasets","Tools-&-Libraries","Image-Retrieval"] },
{"key": "kanda2020succinct", "year": "2020", "citations": "11", "title":"Succinct Trit-array Trie for Scalable Trajectory Similarity Search", "abstract": "<p>Massive datasets of spatial trajectories representing the mobility of a\ndiversity of moving objects are ubiquitous in research and industry. Similarity\nsearch of a large collection of trajectories is indispensable for turning these\ndatasets into knowledge. Locality sensitive hashing (LSH) is a powerful\ntechnique for fast similarity searches. Recent methods employ LSH and attempt\nto realize an efficient similarity search of trajectories; however, those\nmethods are inefficient in terms of search time and memory when applied to\nmassive datasets. To address this problem, we present the trajectory-indexing\nsuccinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for\ntrajectory similarity searches. tSTAT quickly performs the search on a tree\ndata structure called trie. We also present two novel techniques that enable to\ndramatically enhance the memory efficiency of tSTAT. One is a node reduction\ntechnique that substantially omits redundant trie nodes while maintaining the\ntime performance. The other is a space-efficient representation that leverages\nthe idea behind succinct data structures (i.e., a compressed data structure\nsupporting fast data operations). We experimentally test tSTAT on its ability\nto retrieve similar trajectories for a query from large collections of\ntrajectories and show that tSTAT performs superiorly in comparison to\nstate-of-the-art similarity search methods.</p>\n", "tags": ["Similarity-Search","Locality-Sensitive-Hashing","Hashing-Methods","Datasets","Memory-Efficiency","Evaluation","Efficiency"] },
{"key": "kang2016column", "year": "2016", "citations": "299", "title":"Column Sampling Based Discrete Supervised Hashing", "abstract": "<p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.\nHowever, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.\nCOSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>\n", "tags": ["Image-Retrieval","Datasets","Neural-Hashing","Supervised","Hashing-Methods","AAAI","Evaluation","Unsupervised"] },
{"key": "kang2019candidate", "year": "2019", "citations": "53", "title":"Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation", "abstract": "<p>Generating the Top-N recommendations from a large corpus is computationally\nexpensive to perform at scale. Candidate generation and re-ranking based\napproaches are often adopted in industrial settings to alleviate efficiency\nproblems. However it remains to be fully studied how well such schemes\napproximate complete rankings (or how many candidates are required to achieve a\ngood approximation), or to develop systematic approaches to generate\nhigh-quality candidates efficiently. In this paper, we seek to investigate\nthese questions via proposing a candidate generation and re-ranking based\nframework (CIGAR), which first learns a preference-preserving binary embedding\nfor building a hash table to retrieve candidates, and then learns to re-rank\nthe candidates using real-valued ranking models with a candidate-oriented\nobjective. We perform a comprehensive study on several large-scale real-world\ndatasets consisting of millions of users/items and hundreds of millions of\ninteractions. Our results show that CIGAR significantly boosts the Top-N\naccuracy against state-of-the-art recommendation models, while reducing the\nquery time by orders of magnitude. We hope that this work could draw more\nattention to the candidate generation problem in recommender systems.</p>\n", "tags": ["CIKM","Tools-&-Libraries","Hashing-Methods","Datasets","Recommender-Systems","Compact-Codes","Hybrid-ANN-Methods","Re-Ranking","Scalability","Efficiency"] },
{"key": "kang2019maximum", "year": "2019", "citations": "37", "title":"Maximum-Margin Hamming Hashing", "abstract": "<p>Deep hashing enables computation and memory efficient\nimage search through end-to-end learning of feature representations and binary codes. While linear scan over binary\nhash codes is more efficient than over the high-dimensional\nrepresentations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for\neach query, there is a Hamming ball centered at the query\nand the data points within the ball are returned as relevant.\nSince inside the Hamming ball implies retrievable while\noutside irretrievable, it is crucial to explicitly characterize\nthe Hamming ball. The main idea of this work is to directly\nembody the Hamming radius into the loss functions, leading\nto Maximum-Margin Hamming Hashing (MMHH), a new\nmodel specifically optimized for Hamming space retrieval.\nWe introduce a max-margin t-distribution loss, where the\nt-distribution concentrates more similar data points to be\nwithin the Hamming ball, and the margin characterizes the\nHamming radius such that less penalization is applied to\nsimilar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.\nThe model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.\nOur method yields state-of-the-art results on four datasets\nand shows superior performance on noisy data.</p>\n", "tags": ["Image-Retrieval","ICCV","Datasets","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Robustness"] },
{"key": "kang2020learning", "year": "2020", "citations": "32", "title":"Learning to Embed Categorical Features without Embedding Tables for Recommendation", "abstract": "<p>Embedding learning of categorical features (e.g. user/item IDs) is at the\ncore of various recommendation models including matrix factorization and neural\ncollaborative filtering. The standard approach creates an embedding table where\neach row represents a dedicated embedding vector for every unique feature\nvalue. However, this method fails to efficiently handle high-cardinality\nfeatures and unseen feature values (e.g. new video ID) that are prevalent in\nreal-world recommendation systems. In this paper, we propose an alternative\nembedding framework Deep Hash Embedding (DHE), replacing embedding tables by a\ndeep embedding network to compute embeddings on the fly. DHE first encodes the\nfeature value to a unique identifier vector with multiple hashing functions and\ntransformations, and then applies a DNN to convert the identifier vector to an\nembedding. The encoding module is deterministic, non-learnable, and free of\nstorage, while the embedding network is updated during the training time to\nlearn embedding generation. Empirical results show that DHE achieves comparable\nAUC against the standard one-hot full embedding, with smaller model sizes. Our\nwork sheds light on the design of DNN-based alternative embedding schemes for\ncategorical features without using embedding table lookup.</p>\n", "tags": ["Hashing-Methods","Recommender-Systems","Neural-Hashing","Tools-&-Libraries"] },
{"key": "kang2025column", "year": "2016", "citations": "299", "title":"Column Sampling Based Discrete Supervised Hashing", "abstract": "<p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.\nHowever, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.\nCOSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>\n", "tags": ["Image-Retrieval","Datasets","Neural-Hashing","Supervised","Hashing-Methods","AAAI","Evaluation","Unsupervised"] },
{"key": "kang2025maximum", "year": "2019", "citations": "37", "title":"Maximum-Margin Hamming Hashing", "abstract": "<p>Deep hashing enables computation and memory efficient\nimage search through end-to-end learning of feature representations and binary codes. While linear scan over binary\nhash codes is more efficient than over the high-dimensional\nrepresentations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for\neach query, there is a Hamming ball centered at the query\nand the data points within the ball are returned as relevant.\nSince inside the Hamming ball implies retrievable while\noutside irretrievable, it is crucial to explicitly characterize\nthe Hamming ball. The main idea of this work is to directly\nembody the Hamming radius into the loss functions, leading\nto Maximum-Margin Hamming Hashing (MMHH), a new\nmodel specifically optimized for Hamming space retrieval.\nWe introduce a max-margin t-distribution loss, where the\nt-distribution concentrates more similar data points to be\nwithin the Hamming ball, and the margin characterizes the\nHamming radius such that less penalization is applied to\nsimilar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.\nThe model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.\nOur method yields state-of-the-art results on four datasets\nand shows superior performance on noisy data.</p>\n", "tags": ["Image-Retrieval","ICCV","Datasets","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Robustness"] },
{"key": "karaman2019unsupervised", "year": "2019", "citations": "14", "title":"Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval", "abstract": "<p>We propose an unsupervised hashing method which aims to produce binary codes\nthat preserve the ranking induced by a real-valued representation. Such compact\nhash codes enable the complete elimination of real-valued feature storage and\nallow for significant reduction of the computation complexity and storage cost\nof large-scale image retrieval applications. Specifically, we learn a neural\nnetwork-based model, which transforms the input representation into a binary\nrepresentation. We formalize the training objective of the network in an\nintuitive and effective way, considering each training sample as a query and\naiming to obtain the same retrieval results using the produced hash codes as\nthose obtained with the original features. This training formulation directly\noptimizes the hashing model for the target usage of the hash codes it produces.\nWe further explore the addition of a decoder trained to obtain an approximated\nreconstruction of the original features. At test time, we retrieved the most\npromising database samples with an efficient graph-based search procedure using\nonly our hash codes and perform re-ranking using the reconstructed features,\nthus without needing to access the original features at all. Experiments\nconducted on multiple publicly available large-scale datasets show that our\nmethod consistently outperforms all compared state-of-the-art unsupervised\nhashing methods and that the reconstruction procedure can effectively boost the\nsearch accuracy with a minimal constant additional cost.</p>\n", "tags": ["Scalability","Hybrid-ANN-Methods","Datasets","Unsupervised","Graph-Based-ANN","Compact-Codes","Hashing-Methods","Multimodal-Retrieval","Neural-Hashing","Memory-Efficiency","Image-Retrieval","Supervised","Re-Ranking"] },
{"key": "kato2009solving", "year": "2010", "citations": "37", "title":"Solving $k$-Nearest Neighbor Problem on Multiple Graphics Processors", "abstract": "<p>The recommendation system is a software system to predict customers’ unknown\npreferences from known preferences. In the recommendation system, customers’\npreferences are encoded into vectors, and finding the nearest vectors to each\nvector is an essential part. This vector-searching part of the problem is\ncalled a \\(k\\)-nearest neighbor problem. We give an effective algorithm to solve\nthis problem on multiple graphics processor units (GPUs).\n  Our algorithm consists of two parts: an \\(N\\)-body problem and a partial sort.\nFor a algorithm of the \\(N\\)-body problem, we applied the idea of a known\nalgorithm for the \\(N\\)-body problem in physics, although another trick is need\nto overcome the problem of small sized shared memory. For the partial sort, we\ngive a novel GPU algorithm which is effective for small \\(k\\). In our partial\nsort algorithm, a heap is accessed in parallel by threads with a low cost of\nsynchronization. Both of these two parts of our algorithm utilize maximal power\nof coalesced memory access, so that a full bandwidth is achieved.\n  By an experiment, we show that when the size of the problem is large, an\nimplementation of the algorithm on two GPUs runs more than 330 times faster\nthan a single core implementation on a latest CPU. We also show that our\nalgorithm scales well with respect to the number of GPUs.</p>\n", "tags": ["Recommender-Systems"] },
{"key": "keisler2020visual", "year": "2019", "citations": "21", "title":"Visual search over billions of aerial and satellite images", "abstract": "<p>We present a system for performing visual search over billions of aerial and\nsatellite images. The purpose of visual search is to find images that are\nvisually similar to a query image. We define visual similarity using 512\nabstract visual features generated by a convolutional neural network that has\nbeen trained on aerial and satellite imagery. The features are converted to\nbinary values to reduce data and compute requirements. We employ a hash-based\nsearch using Bigtable, a scalable database service from Google Cloud. Searching\nthe continental United States at 1-meter pixel resolution, corresponding to\napproximately 2 billion images, takes approximately 0.1 seconds. This system\nenables real-time visual search over the surface of the earth, and an\ninteractive demo is available at https://search.descarteslabs.com.</p>\n", "tags": ["Image-Retrieval","Efficiency"] },
{"key": "kennedy2016fast", "year": "2016", "citations": "9", "title":"Fast Cross-Polytope Locality-Sensitive Hashing", "abstract": "<p>We provide a variant of cross-polytope locality sensitive hashing with\nrespect to angular distance which is provably optimal in asymptotic sensitivity\nand enjoys \\(\\mathcal{O}(d \\ln d )\\) hash computation time. Building on a recent\nresult (by Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015), we show that\noptimal asymptotic sensitivity for cross-polytope LSH is retained even when the\ndense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform\nfollowed by discrete pseudo-rotation, reducing the hash computation time from\n\\(\\mathcal{O}(d^2)\\) to \\(\\mathcal{O}(d \\ln d )\\). Moreover, our scheme achieves\nthe optimal rate of convergence for sensitivity. By incorporating a\nlow-randomness Johnson-Lindenstrauss transform, our scheme can be modified to\nrequire only \\(\\mathcal{O}(\\ln^9(d))\\) random bits</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing"] },
{"key": "khandelwal2020nearest", "year": "2020", "citations": "130", "title":"Nearest Neighbor Machine Translation", "abstract": "<p>We introduce \\(k\\)-nearest-neighbor machine translation (\\(k\\)NN-MT), which\npredicts tokens with a nearest neighbor classifier over a large datastore of\ncached examples, using representations from a neural translation model for\nsimilarity search. This approach requires no additional training and scales to\ngive the decoder direct access to billions of examples at test time, resulting\nin a highly expressive model that consistently improves performance across many\nsettings. Simply adding nearest neighbor search improves a state-of-the-art\nGerman-English translation model by 1.5 BLEU. \\(k\\)NN-MT allows a single model to\nbe adapted to diverse domains by using a domain-specific datastore, improving\nresults by an average of 9.2 BLEU over zero-shot transfer, and achieving new\nstate-of-the-art results – without training on these domains. A massively\nmultilingual model can also be specialized for particular language pairs, with\nimprovements of 3 BLEU for translating from English into German and Chinese.\nQualitatively, \\(k\\)NN-MT is easily interpretable; it combines source and target\ncontext to retrieve highly relevant examples.</p>\n", "tags": ["Similarity-Search","Few-Shot-&-Zero-Shot","Evaluation"] },
{"key": "khasanova2016multi", "year": "2016", "citations": "7", "title":"Multi-modal image retrieval with random walk on multi-layer graphs", "abstract": "<p>The analysis of large collections of image data is still a challenging\nproblem due to the difficulty of capturing the true concepts in visual data.\nThe similarity between images could be computed using different and possibly\nmultimodal features such as color or edge information or even text labels. This\nmotivates the design of image analysis solutions that are able to effectively\nintegrate the multi-view information provided by different feature sets. We\ntherefore propose a new image retrieval solution that is able to sort images\nthrough a random walk on a multi-layer graph, where each layer corresponds to a\ndifferent type of information about the image data. We study in depth the\ndesign of the image graph and propose in particular an effective method to\nselect the edge weights for the multi-layer graph, such that the image ranking\nscores are optimised. We then provide extensive experiments in different\nreal-world photo collections, which confirm the high performance of our new\nimage retrieval algorithm that generally surpasses state-of-the-art solutions\ndue to a more meaningful image similarity computation.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "khurshid2020cross", "year": "2019", "citations": "5", "title":"Cross-View Image Retrieval -- Ground to Aerial Image Retrieval through Deep Learning", "abstract": "<p>Cross-modal retrieval aims to measure the content similarity between\ndifferent types of data. The idea has been previously applied to visual, text,\nand speech data. In this paper, we present a novel cross-modal retrieval method\nspecifically for multi-view images, called Cross-view Image Retrieval CVIR. Our\napproach aims to find a feature space as well as an embedding space in which\nsamples from street-view images are compared directly to satellite-view images\n(and vice-versa). For this comparison, a novel deep metric learning based\nsolution “DeepCVIR” has been proposed. Previous cross-view image datasets are\ndeficient in that they (1) lack class information; (2) were originally\ncollected for cross-view image geolocalization task with coupled images; (3) do\nnot include any images from off-street locations. To train, compare, and\nevaluate the performance of cross-view image retrieval, we present a new 6\nclass cross-view image dataset termed as CrossViewRet which comprises of images\nincluding freeway, mountain, palace, river, ship, and stadium with 700\nhigh-resolution dual-view images for each class. Results show that the proposed\nDeepCVIR outperforms conventional matching approaches on the CVIR task for the\ngiven dataset and would also serve as the baseline for future research.</p>\n", "tags": ["Image-Retrieval","Distance-Metric-Learning","Datasets","Evaluation","Multimodal-Retrieval"] },
{"key": "kim2018attention", "year": "2018", "citations": "239", "title":"Attention-based Ensemble for Deep Metric Learning", "abstract": "<p>Deep metric learning aims to learn an embedding function, modeled as deep\nneural network. This embedding function usually puts semantically similar\nimages close while dissimilar images far from each other in the learned\nembedding space. Recently, ensemble has been applied to deep metric learning to\nyield state-of-the-art results. As one important aspect of ensemble, the\nlearners should be diverse in their feature embeddings. To this end, we propose\nan attention-based ensemble, which uses multiple attention masks, so that each\nlearner can attend to different parts of the object. We also propose a\ndivergence loss, which encourages diversity among the learners. The proposed\nmethod is applied to the standard benchmarks of deep metric learning and\nexperimental results show that it outperforms the state-of-the-art methods by a\nsignificant margin on image retrieval tasks.</p>\n", "tags": ["Distance-Metric-Learning","Image-Retrieval"] },
{"key": "kim2019deep", "year": "2019", "citations": "88", "title":"Deep Metric Learning Beyond Binary Supervision", "abstract": "<p>Metric Learning for visual similarity has mostly adopted binary supervision\nindicating whether a pair of images are of the same class or not. Such a binary\nindicator covers only a limited subset of image relations, and is not\nsufficient to represent semantic similarity between images described by\ncontinuous and/or structured labels such as object poses, image captions, and\nscene graphs. Motivated by this, we present a novel method for deep metric\nlearning using continuous labels. First, we propose a new triplet loss that\nallows distance ratios in the label space to be preserved in the learned metric\nspace. The proposed loss thus enables our model to learn the degree of\nsimilarity rather than just the order. Furthermore, we design a triplet mining\nstrategy adapted to metric learning with continuous labels. We address three\ndifferent image retrieval tasks with continuous labels in terms of human poses,\nroom layouts and image captions, and demonstrate the superior performance of\nour approach compared to previous methods.</p>\n", "tags": ["Distance-Metric-Learning","CVPR","Evaluation","Image-Retrieval"] },
{"key": "kim2021multi", "year": "2021", "citations": "11", "title":"Multi-level Distance Regularization for Deep Metric Learning", "abstract": "<p>We propose a novel distance-based regularization method for deep metric\nlearning called Multi-level Distance Regularization (MDR). MDR explicitly\ndisturbs a learning procedure by regularizing pairwise distances between\nembedding vectors into multiple levels that represents a degree of similarity\nbetween a pair. In the training stage, the model is trained with both MDR and\nan existing loss function of deep metric learning, simultaneously; the two\nlosses interfere with the objective of each other, and it makes the learning\nprocess difficult. Moreover, MDR prevents some examples from being ignored or\noverly influenced in the learning process. These allow the parameters of the\nembedding network to be settle on a local optima with better generalization.\nWithout bells and whistles, MDR with simple Triplet loss achieves\nthe-state-of-the-art performance in various benchmark datasets: CUB-200-2011,\nCars-196, Stanford Online Products, and In-Shop Clothes Retrieval. We\nextensively perform ablation studies on its behaviors to show the effectiveness\nof MDR. By easily adopting our MDR, the previous approaches can be improved in\nperformance and generalization ability.</p>\n", "tags": ["AAAI","Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "kim2022accelerating", "year": "2022", "citations": "20", "title":"Accelerating Large-Scale Graph-based Nearest Neighbor Search on a Computational Storage Platform", "abstract": "<p>K-nearest neighbor search is one of the fundamental tasks in various\napplications and the hierarchical navigable small world (HNSW) has recently\ndrawn attention in large-scale cloud services, as it easily scales up the\ndatabase while offering fast search. On the other hand, a computational storage\ndevice (CSD) that combines programmable logic and storage modules on a single\nboard becomes popular to address the data bandwidth bottleneck of modern\ncomputing systems. In this paper, we propose a computational storage platform\nthat can accelerate a large-scale graph-based nearest neighbor search algorithm\nbased on SmartSSD CSD. To this end, we modify the algorithm more amenable on\nthe hardware and implement two types of accelerators using HLS- and RTL-based\nmethodology with various optimization methods. In addition, we scale up the\nproposed platform to have 4 SmartSSDs and apply graph parallelism to boost the\nsystem performance further. As a result, the proposed computational storage\nplatform achieves 75.59 query per second throughput for the SIFT1B dataset at\n258.66W power dissipation, which is 12.83x and 17.91x faster and 10.43x and\n24.33x more energy efficient than the conventional CPU-based and GPU-based\nserver platform, respectively. With multi-terabyte storage and custom\nacceleration capability, we believe that the proposed computational storage\nplatform is a promising solution for cost-sensitive cloud datacenters.</p>\n", "tags": ["Datasets","Scalability","Evaluation","Graph-Based-ANN"] },
{"key": "kim2022improving", "year": "2023", "citations": "28", "title":"Improving Cross-Modal Retrieval with Set of Diverse Embeddings", "abstract": "<p>Cross-modal retrieval across image and text modalities is a challenging task\ndue to its inherent ambiguity: An image often exhibits various situations, and\na caption can be coupled with diverse images. Set-based embedding has been\nstudied as a solution to this problem. It seeks to encode a sample into a set\nof different embedding vectors that capture different semantics of the sample.\nIn this paper, we present a novel set-based embedding method, which is distinct\nfrom previous work in two aspects. First, we present a new similarity function\ncalled smooth-Chamfer similarity, which is designed to alleviate the side\neffects of existing similarity functions for set-based embedding. Second, we\npropose a novel set prediction module to produce a set of embedding vectors\nthat effectively captures diverse semantics of input by the slot attention\nmechanism. Our method is evaluated on the COCO and Flickr30K datasets across\ndifferent visual backbones, where it outperforms existing methods including\nones that demand substantially larger computation at inference.</p>\n", "tags": ["Datasets","CVPR","Multimodal-Retrieval"] },
{"key": "kim2023exposing", "year": "2023", "citations": "15", "title":"Exposing and Mitigating Spurious Correlations for Cross-Modal Retrieval", "abstract": "<p>Cross-modal retrieval methods are the preferred tool to search databases for\nthe text that best matches a query image and vice versa. However, image-text\nretrieval models commonly learn to memorize spurious correlations in the\ntraining data, such as frequent object co-occurrence, instead of looking at the\nactual underlying reasons for the prediction in the image. For image-text\nretrieval, this manifests in retrieved sentences that mention objects that are\nnot present in the query image. In this work, we introduce ODmAP@k, an object\ndecorrelation metric that measures a model’s robustness to spurious\ncorrelations in the training data. We use automatic image and text\nmanipulations to control the presence of such object correlations in designated\ntest data. Additionally, our data synthesis technique is used to tackle model\nbiases due to spurious correlations of semantically unrelated objects in the\ntraining data. We apply our proposed pipeline, which involves the finetuning of\nimage-text retrieval frameworks on carefully designed synthetic data, to three\nstate-of-the-art models for image-text retrieval. This results in significant\nimprovements for all three models, both in terms of the standard retrieval\nperformance and in terms of our object decorrelation metric. The code is\navailable at https://github.com/ExplainableML/Spurious_CM_Retrieval.</p>\n", "tags": ["Multimodal-Retrieval","Text-Retrieval","CVPR","Evaluation","Robustness"] },
{"key": "klein2017end", "year": "2019", "citations": "59", "title":"End-to-End Supervised Product Quantization for Image Search and Retrieval", "abstract": "<p>Product Quantization, a dictionary based hashing method, is one of the\nleading unsupervised hashing techniques. While it ignores the labels, it\nharnesses the features to construct look up tables that can approximate the\nfeature space. In recent years, several works have achieved state of the art\nresults on hashing benchmarks by learning binary representations in a\nsupervised manner. This work presents Deep Product Quantization (DPQ), a\ntechnique that leads to more accurate retrieval and classification than the\nlatest state of the art methods, while having similar computational complexity\nand memory footprint as the Product Quantization method. To our knowledge, this\nis the first work to introduce a dictionary-based representation that is\ninspired by Product Quantization and which is learned end-to-end, and thus\nbenefits from the supervised signal. DPQ explicitly learns soft and hard\nrepresentations to enable an efficient and accurate asymmetric search, by using\na straight-through estimator. Our method obtains state of the art results on an\nextensive array of retrieval and classification experiments.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Neural-Hashing","Memory-Efficiency","CVPR","Unsupervised","Quantization"] },
{"key": "kobayashi2020decomposing", "year": "2021", "citations": "31", "title":"Decomposing Normal and Abnormal Features of Medical Images for Content-based Image Retrieval", "abstract": "<p>Medical images can be decomposed into normal and abnormal features, which is\nconsidered as the compositionality. Based on this idea, we propose an\nencoder-decoder network to decompose a medical image into two discrete latent\ncodes: a normal anatomy code and an abnormal anatomy code. Using these latent\ncodes, we demonstrate a similarity retrieval by focusing on either normal or\nabnormal features of medical images.</p>\n", "tags": ["Similarity-Search","Image-Retrieval"] },
{"key": "kobayashi2023sketch", "year": "2023", "citations": "6", "title":"Sketch-based Medical Image Retrieval", "abstract": "<p>The amount of medical images stored in hospitals is increasing faster than\never; however, utilizing the accumulated medical images has been limited. This\nis because existing content-based medical image retrieval (CBMIR) systems\nusually require example images to construct query vectors; nevertheless,\nexample images cannot always be prepared. Besides, there can be images with\nrare characteristics that make it difficult to find similar example images,\nwhich we call isolated samples. Here, we introduce a novel sketch-based medical\nimage retrieval (SBMIR) system that enables users to find images of interest\nwithout example images. The key idea lies in feature decomposition of medical\nimages, whereby the entire feature of a medical image can be decomposed into\nand reconstructed from normal and abnormal features. By extending this idea,\nour SBMIR system provides an easy-to-use two-step graphical user interface:\nusers first select a template image to specify a normal feature and then draw a\nsemantic sketch of the disease on the template image to represent an abnormal\nfeature. Subsequently, it integrates the two kinds of input to construct a\nquery vector and retrieves reference images with the closest reference vectors.\nUsing two datasets, ten healthcare professionals with various clinical\nbackgrounds participated in the user test for evaluation. As a result, our\nSBMIR system enabled users to overcome previous challenges, including image\nretrieval based on fine-grained image characteristics, image retrieval without\nexample images, and image retrieval for isolated samples. Our SBMIR system\nachieves flexible medical image retrieval on demand, thereby expanding the\nutility of medical image databases.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "komorowski2017random", "year": "2019", "citations": "9", "title":"Random Binary Trees for Approximate Nearest Neighbour Search in Binary Space", "abstract": "<p>Approximate nearest neighbour (ANN) search is one of the most important\nproblems in computer science fields such as data mining or computer vision. In\nthis paper, we focus on ANN for high-dimensional binary vectors and we propose\na simple yet powerful search method that uses Random Binary Search Trees\n(RBST). We apply our method to a dataset of 1.25M binary local feature\ndescriptors obtained from a real-life image-based localisation system provided\nby Google as a part of Project Tango. An extensive evaluation of our method\nagainst the state-of-the-art variations of Locality Sensitive Hashing (LSH),\nnamely Uniform LSH and Multi-probe LSH, shows the superiority of our method in\nterms of retrieval precision with performance boost of over 20%</p>\n", "tags": ["Similarity-Search","Locality-Sensitive-Hashing","Hashing-Methods","Datasets","Evaluation"] },
{"key": "kong2012isotropic", "year": "2012", "citations": "260", "title":"Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": ["Hashing-Methods"] },
{"key": "kong2012manhattan", "year": "2012", "citations": "105", "title":"Manhattan Hashing for Large-Scale Image Retrieval", "abstract": "<p>Hashing is used to learn binary-code representation for data with\nexpectation of preserving the neighborhood structure in the original\nfeature space. Due to its fast query speed and reduced storage\ncost, hashing has been widely used for efficient nearest neighbor\nsearch in a large variety of applications like text and image retrieval.\nMost existing hashing methods adopt Hamming distance to\nmeasure the similarity (neighborhood) between points in the hashcode\nspace. However, one problem with Hamming distance is that\nit may destroy the neighborhood structure in the original feature\nspace, which violates the essential goal of hashing. In this paper,\nManhattan hashing (MH), which is based on Manhattan distance, is\nproposed to solve the problem of Hamming distance based hashing.\nThe basic idea of MH is to encode each projected dimension with\nmultiple bits of natural binary code (NBC), based on which the\nManhattan distance between points in the hashcode space is calculated\nfor nearest neighbor search. MH can effectively preserve the\nneighborhood structure in the data to achieve the goal of hashing.\nTo the best of our knowledge, this is the first work to adopt Manhattan\ndistance with NBC for hashing. Experiments on several largescale\nimage data sets containing up to one million points show that\nour MH method can significantly outperform other state-of-the-art\nmethods.</p>\n", "tags": ["Image-Retrieval","Scalability","SIGIR","Compact-Codes","Hashing-Methods"] },
{"key": "kong2025isotropic", "year": "2012", "citations": "260", "title":"Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": ["Hashing-Methods"] },
{"key": "kong2025manhattan", "year": "2012", "citations": "105", "title":"Manhattan Hashing for Large-Scale Image Retrieval", "abstract": "<p>Hashing is used to learn binary-code representation for data with\nexpectation of preserving the neighborhood structure in the original\nfeature space. Due to its fast query speed and reduced storage\ncost, hashing has been widely used for efficient nearest neighbor\nsearch in a large variety of applications like text and image retrieval.\nMost existing hashing methods adopt Hamming distance to\nmeasure the similarity (neighborhood) between points in the hashcode\nspace. However, one problem with Hamming distance is that\nit may destroy the neighborhood structure in the original feature\nspace, which violates the essential goal of hashing. In this paper,\nManhattan hashing (MH), which is based on Manhattan distance, is\nproposed to solve the problem of Hamming distance based hashing.\nThe basic idea of MH is to encode each projected dimension with\nmultiple bits of natural binary code (NBC), based on which the\nManhattan distance between points in the hashcode space is calculated\nfor nearest neighbor search. MH can effectively preserve the\nneighborhood structure in the data to achieve the goal of hashing.\nTo the best of our knowledge, this is the first work to adopt Manhattan\ndistance with NBC for hashing. Experiments on several largescale\nimage data sets containing up to one million points show that\nour MH method can significantly outperform other state-of-the-art\nmethods.</p>\n", "tags": ["Image-Retrieval","Scalability","SIGIR","Compact-Codes","Hashing-Methods"] },
{"key": "kordopatiszilos2021leveraging", "year": "2021", "citations": "15", "title":"Leveraging EfficientNet and Contrastive Learning for Accurate Global-scale Location Estimation", "abstract": "<p>In this paper, we address the problem of global-scale image geolocation,\nproposing a mixed classification-retrieval scheme. Unlike other methods that\nstrictly tackle the problem as a classification or retrieval task, we combine\nthe two practices in a unified solution leveraging the advantages of each\napproach with two different modules. The first leverages the EfficientNet\narchitecture to assign images to a specific geographic cell in a robust way.\nThe second introduces a new residual architecture that is trained with\ncontrastive learning to map input images to an embedding space that minimizes\nthe pairwise geodesic distance of same-location images. For the final location\nestimation, the two modules are combined with a search-within-cell scheme,\nwhere the locations of most similar images from the predicted geographic cell\nare aggregated based on a spatial clustering scheme. Our approach demonstrates\nvery competitive performance on four public datasets, achieving new\nstate-of-the-art performance in fine granularity scales, i.e., 15.0% at 1km\nrange on Im2GPS3k.</p>\n", "tags": ["Evaluation","Multimodal-Retrieval","Self-Supervised","Datasets"] },
{"key": "kostić2021multi", "year": "2021", "citations": "12", "title":"Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models", "abstract": "<p>Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.</p>\n", "tags": ["Evaluation","Graph-Based-ANN","Datasets"] },
{"key": "koutaki2016fast", "year": "2016", "citations": "7", "title":"Fast Supervised Discrete Hashing and its Analysis", "abstract": "<p>In this paper, we propose a learning-based supervised discrete hashing\nmethod. Binary hashing is widely used for large-scale image retrieval as well\nas video and document searches because the compact representation of binary\ncode is essential for data storage and reasonable for query searches using\nbit-operations. The recently proposed Supervised Discrete Hashing (SDH)\nefficiently solves mixed-integer programming problems by alternating\noptimization and the Discrete Cyclic Coordinate descent (DCC) method. We show\nthat the SDH model can be simplified without performance degradation based on\nsome preliminary experiments; we call the approximate model for this the “Fast\nSDH” (FSDH) model. We analyze the FSDH model and provide a mathematically exact\nsolution for it. In contrast to SDH, our model does not require an alternating\noptimization algorithm and does not depend on initial values. FSDH is also\neasier to implement than Iterative Quantization (ITQ). Experimental results\ninvolving a large-scale database showed that FSDH outperforms conventional SDH\nin terms of precision, recall, and computation time.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Quantization","Scalability","Evaluation"] },
{"key": "krishna2021evaluating", "year": "2021", "citations": "23", "title":"Evaluating Contrastive Models for Instance-based Image Retrieval", "abstract": "<p>In this work, we evaluate contrastive models for the task of image retrieval.\nWe hypothesise that models that are learned to encode semantic similarity among\ninstances via discriminative learning should perform well on the task of image\nretrieval, where relevancy is defined in terms of instances of the same object.\nThrough our extensive evaluation, we find that representations from models\ntrained using contrastive methods perform on-par with (and outperforms) a\npre-trained supervised baseline trained on the ImageNet labels in retrieval\ntasks under various configurations. This is remarkable given that the\ncontrastive models require no explicit supervision. Thus, we conclude that\nthese models can be used to bootstrap base models to build more robust image\nretrieval engines.</p>\n", "tags": ["Evaluation","Image-Retrieval","Supervised","Multimodal-Retrieval"] },
{"key": "kuang2019fashion", "year": "2019", "citations": "87", "title":"Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid", "abstract": "<p>Matching clothing images from customers and online shopping stores has rich\napplications in E-commerce. Existing algorithms encoded an image as a global\nfeature vector and performed retrieval with the global representation. However,\ndiscriminative local information on clothes are submerged in this global\nrepresentation, resulting in sub-optimal performance. To address this issue, we\npropose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which\nlearns similarities between a query and a gallery cloth by using both global\nand local representations in multiple scales. The similarity pyramid is\nrepresented by a Graph of similarity, where nodes represent similarities\nbetween clothing components at different scales, and the final matching score\nis obtained by message passing along edges. In GRNet, graph reasoning is solved\nby training a graph convolutional network, enabling to align salient clothing\ncomponents to improve clothing retrieval. To facilitate future researches, we\nintroduce a new benchmark FindFashion, containing rich annotations of bounding\nboxes, views, occlusions, and cropping. Extensive experiments show that GRNet\nobtains new state-of-the-art results on two challenging benchmarks, e.g.,\npushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%,\nand 75% (i.e., 4%, 10%, and 10% absolute improvements), outperforming\ncompetitors with large margins. On FindFashion, GRNet achieves considerable\nimprovements on all empirical settings.</p>\n", "tags": ["ICCV","Evaluation"] },
{"key": "kulis2009kernelized", "year": "2009", "citations": "908", "title":"Kernelized Locality-Sensitive Hashing for Scalable Image Search", "abstract": "<p>Fast retrieval methods are critical for large-scale and\ndata-driven vision applications. Recent work has explored\nways to embed high-dimensional features or complex distance\nfunctions into a low-dimensional Hamming space\nwhere items can be efficiently searched. However, existing\nmethods do not apply for high-dimensional kernelized\ndata when the underlying feature embedding for the kernel\nis unknown. We show how to generalize locality-sensitive\nhashing to accommodate arbitrary kernel functions, making\nit possible to preserve the algorithm’s sub-linear time similarity\nsearch guarantees for a wide class of useful similarity\nfunctions. Since a number of successful image-based kernels\nhave unknown or incomputable embeddings, this is especially\nvaluable for image retrieval tasks. We validate our\ntechnique on several large-scale datasets, and show that it\nenables accurate and fast performance for example-based\nobject classification, feature matching, and content-based\nretrieval.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","ICCV","Datasets","Hashing-Methods","Evaluation"] },
{"key": "kulis2009learning", "year": "2009", "citations": "841", "title":"Learning to Hash with Binary Reconstructive Embeddings", "abstract": "<p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been\nseveral recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.\nIn this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the\nreconstruction error between the original distances and the Hamming distances of the corresponding binary\nembeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that\nis able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic\nhashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions\nabout the underlying distribution of the data. We present results over several domains to demonstrate that\nour method outperforms existing state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods","Scalability","Efficiency"] },
{"key": "kulis2025kernelized", "year": "2009", "citations": "908", "title":"Kernelized Locality-Sensitive Hashing for Scalable Image Search", "abstract": "<p>Fast retrieval methods are critical for large-scale and\ndata-driven vision applications. Recent work has explored\nways to embed high-dimensional features or complex distance\nfunctions into a low-dimensional Hamming space\nwhere items can be efficiently searched. However, existing\nmethods do not apply for high-dimensional kernelized\ndata when the underlying feature embedding for the kernel\nis unknown. We show how to generalize locality-sensitive\nhashing to accommodate arbitrary kernel functions, making\nit possible to preserve the algorithm’s sub-linear time similarity\nsearch guarantees for a wide class of useful similarity\nfunctions. Since a number of successful image-based kernels\nhave unknown or incomputable embeddings, this is especially\nvaluable for image retrieval tasks. We validate our\ntechnique on several large-scale datasets, and show that it\nenables accurate and fast performance for example-based\nobject classification, feature matching, and content-based\nretrieval.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","ICCV","Datasets","Hashing-Methods","Evaluation"] },
{"key": "kulis2025learning", "year": "2009", "citations": "841", "title":"Learning to Hash with Binary Reconstructive Embeddings", "abstract": "<p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been\nseveral recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.\nIn this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the\nreconstruction error between the original distances and the Hamming distances of the corresponding binary\nembeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that\nis able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic\nhashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions\nabout the underlying distribution of the data. We present results over several domains to demonstrate that\nour method outperforms existing state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods","Scalability","Efficiency"] },
{"key": "kulkarni2023lexically", "year": "2023", "citations": "19", "title":"Lexically-Accelerated Dense Retrieval", "abstract": "<p>Retrieval approaches that score documents based on learned dense vectors\n(i.e., dense retrieval) rather than lexical signals (i.e., conventional\nretrieval) are increasingly popular. Their ability to identify related\ndocuments that do not necessarily contain the same terms as those appearing in\nthe user’s query (thereby improving recall) is one of their key advantages.\nHowever, to actually achieve these gains, dense retrieval approaches typically\nrequire an exhaustive search over the document collection, making them\nconsiderably more expensive at query-time than conventional lexical approaches.\nSeveral techniques aim to reduce this computational overhead by approximating\nthe results of a full dense retriever. Although these approaches reasonably\napproximate the top results, they suffer in terms of recall – one of the key\nadvantages of dense retrieval. We introduce ‘LADR’ (Lexically-Accelerated Dense\nRetrieval), a simple-yet-effective approach that improves the efficiency of\nexisting dense retrieval models without compromising on retrieval\neffectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval\nexploration that uses a document proximity graph. We explore two variants of\nLADR: a proactive approach that expands the search space to the neighbors of\nall seed documents, and an adaptive approach that selectively searches the\ndocuments with the highest estimated relevance in an iterative fashion. Through\nextensive experiments across a variety of dense retrieval models, we find that\nLADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier\namong approximate k nearest neighbor techniques. Further, we find that when\ntuned to take around 8ms per query in retrieval latency on our hardware, LADR\nconsistently achieves both precision and recall that are on par with an\nexhaustive search on standard benchmarks.</p>\n", "tags": ["Graph-Based-ANN","SIGIR","Evaluation","Efficiency"] },
{"key": "kumar2011learning", "year": "2011", "citations": "438", "title":"Learning hash functions for cross-view similarity search", "abstract": "<p>Many applications in Multilingual and Multimodal\nInformation Access involve searching large\ndatabases of high dimensional data objects with\nmultiple (conditionally independent) views. In this\nwork we consider the problem of learning hash\nfunctions for similarity search across the views\nfor such applications. We propose a principled\nmethod for learning a hash function for each view\ngiven a set of multiview training data objects. The\nhash functions map similar objects to similar codes\nacross the views thus enabling cross-view similarity\nsearch. We present results from an extensive\nempirical study of the proposed approach\nwhich demonstrate its effectiveness on Japanese\nlanguage People Search and Multilingual People\nSearch problems.</p>\n", "tags": ["Hashing-Methods","Evaluation","Similarity-Search"] },
{"key": "kumar2025learning", "year": "2011", "citations": "438", "title":"Learning hash functions for cross-view similarity search", "abstract": "<p>Many applications in Multilingual and Multimodal\nInformation Access involve searching large\ndatabases of high dimensional data objects with\nmultiple (conditionally independent) views. In this\nwork we consider the problem of learning hash\nfunctions for similarity search across the views\nfor such applications. We propose a principled\nmethod for learning a hash function for each view\ngiven a set of multiview training data objects. The\nhash functions map similar objects to similar codes\nacross the views thus enabling cross-view similarity\nsearch. We present results from an extensive\nempirical study of the proposed approach\nwhich demonstrate its effectiveness on Japanese\nlanguage People Search and Multilingual People\nSearch problems.</p>\n", "tags": ["Hashing-Methods","Evaluation","Similarity-Search"] },
{"key": "kwok2020learning", "year": "2020", "citations": "10", "title":"Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval", "abstract": "<p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","Compact-Codes","Hashing-Methods","Evaluation","Quantization"] },
{"key": "kwok2025learning", "year": "2020", "citations": "10", "title":"Learning to Hash with a Dimension Analysis-based Quantizer for Image Retrieval", "abstract": "<p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","Compact-Codes","Hashing-Methods","Evaluation","Quantization"] },
{"key": "laarhoven2017graph", "year": "2017", "citations": "5", "title":"Graph-based time-space trade-offs for approximate near neighbors", "abstract": "<p>We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size \\(n = 2^{o(d)}\\) on\nthe \\(d\\)-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor \\(c &gt; 1\\) in query time \\(n^{\\rho_q + o(1)}\\) and space \\(n^{1 + \\rho_s +\no(1)}\\), for arbitrary \\(\\rho_q, \\rho_s \\geq 0\\) satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small \\(c\\) and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA’17]. We further\nstudy how the trade-offs scale when the data set is of size \\(n =\n2^{\\Theta(d)}\\), and analyze asymptotic complexities when applying these results\nto lattice sieving.</p>\n", "tags": ["Graph-Based-ANN","Efficiency"] },
{"key": "lai2015simultaneous", "year": "2015", "citations": "916", "title":"Simultaneous Feature Learning and Hash Coding with Deep Neural Networks", "abstract": "<p>Similarity-preserving hashing is a widely-used method\nfor nearest neighbour search in large-scale image retrieval\ntasks. For most existing hashing methods, an image is\nfirst encoded as a vector of hand-engineering visual features,\nfollowed by another separate projection or quantization\nstep that generates binary codes. However, such visual\nfeature vectors may not be optimally compatible with the\ncoding process, thus producing sub-optimal hashing codes.\nIn this paper, we propose a deep architecture for supervised\nhashing, in which images are mapped into binary codes via\ncarefully designed deep neural networks. The pipeline of\nthe proposed deep architecture consists of three building\nblocks: 1) a sub-network with a stack of convolution layers\nto produce the effective intermediate image features; 2)\na divide-and-encode module to divide the intermediate image\nfeatures into multiple branches, each encoded into one\nhash bit; and 3) a triplet ranking loss designed to characterize\nthat one image is more similar to the second image than\nto the third one. Extensive evaluations on several benchmark\nimage datasets show that the proposed simultaneous\nfeature learning and hash coding pipeline brings substantial\nimprovements over other state-of-the-art supervised or\nunsupervised hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","CVPR","Neural-Hashing","Supervised","Compact-Codes","Similarity-Search","Hashing-Methods","Quantization","Evaluation","Unsupervised"] },
{"key": "lai2016instance", "year": "2016", "citations": "78", "title":"Instance-Aware Hashing for Multi-Label Image Retrieval", "abstract": "<p>Similarity-preserving hashing is a commonly used method for nearest neighbour\nsearch in large-scale image retrieval. For image retrieval, deep-networks-based\nhashing methods are appealing since they can simultaneously learn effective\nimage representations and compact hash codes. This paper focuses on\ndeep-networks-based hashing for multi-label images, each of which may contain\nobjects of multiple categories. In most existing hashing methods, each image is\nrepresented by one piece of hash code, which is referred to as semantic\nhashing. This setting may be suboptimal for multi-label image retrieval. To\nsolve this problem, we propose a deep architecture that learns\n\\textbf{instance-aware} image representations for multi-label image data, which\nare organized in multiple groups, with each group containing the features for\none category. The instance-aware representations not only bring advantages to\nsemantic hashing, but also can be used in category-aware hashing, in which an\nimage is represented by multiple pieces of hash codes and each piece of code\ncorresponds to a category. Extensive evaluations conducted on several benchmark\ndatasets demonstrate that, for both semantic hashing and category-aware\nhashing, the proposed method shows substantial improvement over the\nstate-of-the-art supervised and unsupervised hashing methods.</p>\n", "tags": ["Supervised","Text-Retrieval","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Unsupervised","Scalability","Evaluation"] },
{"key": "lai2017improved", "year": "2018", "citations": "12", "title":"Improved Search in Hamming Space using Deep Multi-Index Hashing", "abstract": "<p>Similarity-preserving hashing is a widely-used method for nearest neighbour\nsearch in large-scale image retrieval tasks. There has been considerable\nresearch on generating efficient image representation via the\ndeep-network-based hashing methods. However, the issue of efficient searching\nin the deep representation space remains largely unsolved. To this end, we\npropose a simple yet efficient deep-network-based multi-index hashing method\nfor simultaneously learning the powerful image representation and the efficient\nsearching. To achieve these two goals, we introduce the multi-index hashing\n(MIH) mechanism into the proposed deep architecture, which divides the binary\ncodes into multiple substrings. Due to the non-uniformly distributed codes will\nresult in inefficiency searching, we add the two balanced constraints at\nfeature-level and instance-level, respectively. Extensive evaluations on\nseveral benchmark image retrieval datasets show that the learned balanced\nbinary codes bring dramatic speedups and achieve comparable performance over\nthe existing baselines.</p>\n", "tags": ["Vector-Indexing","Image-Retrieval","Hashing-Methods","Datasets","Compact-Codes","Scalability","Evaluation"] },
{"key": "lai2017transductive", "year": "2018", "citations": "9", "title":"Transductive Zero-Shot Hashing via Coarse-to-Fine Similarity Mining", "abstract": "<p>Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes\nwithout training data, which is an important and challenging problem. Most\nexisting ZSH approaches exploit transfer learning via an intermediate shared\nsemantic representations between the seen/source classes and novel/target\nclasses. However, due to having disjoint, the hash functions learned from the\nsource dataset are biased when applied directly to the target classes. In this\npaper, we study the transductive ZSH, i.e., we have unlabeled data for novel\nclasses. We put forward a simple yet efficient joint learning approach via\ncoarse-to-fine similarity mining which transfers knowledges from source data to\ntarget data. It mainly consists of two building blocks in the proposed deep\narchitecture: 1) a shared two-streams network, which the first stream operates\non the source data and the second stream operates on the unlabeled data, to\nlearn the effective common image representations, and 2) a coarse-to-fine\nmodule, which begins with finding the most representative images from target\nclasses and then further detect similarities among these images, to transfer\nthe similarities of the source data to the target data in a greedy fashion.\nExtensive evaluation results on several benchmark datasets demonstrate that the\nproposed hashing method achieves significant improvement over the\nstate-of-the-art methods.</p>\n", "tags": ["Evaluation","Datasets","Few-Shot-&-Zero-Shot","Hashing-Methods","Multimodal-Retrieval"] },
{"key": "lai2025simultaneous", "year": "2015", "citations": "916", "title":"Simultaneous Feature Learning and Hash Coding with Deep Neural Networks", "abstract": "<p>Similarity-preserving hashing is a widely-used method\nfor nearest neighbour search in large-scale image retrieval\ntasks. For most existing hashing methods, an image is\nfirst encoded as a vector of hand-engineering visual features,\nfollowed by another separate projection or quantization\nstep that generates binary codes. However, such visual\nfeature vectors may not be optimally compatible with the\ncoding process, thus producing sub-optimal hashing codes.\nIn this paper, we propose a deep architecture for supervised\nhashing, in which images are mapped into binary codes via\ncarefully designed deep neural networks. The pipeline of\nthe proposed deep architecture consists of three building\nblocks: 1) a sub-network with a stack of convolution layers\nto produce the effective intermediate image features; 2)\na divide-and-encode module to divide the intermediate image\nfeatures into multiple branches, each encoded into one\nhash bit; and 3) a triplet ranking loss designed to characterize\nthat one image is more similar to the second image than\nto the third one. Extensive evaluations on several benchmark\nimage datasets show that the proposed simultaneous\nfeature learning and hash coding pipeline brings substantial\nimprovements over other state-of-the-art supervised or\nunsupervised hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","CVPR","Neural-Hashing","Supervised","Compact-Codes","Similarity-Search","Hashing-Methods","Quantization","Evaluation","Unsupervised"] },
{"key": "lam2018word2bits", "year": "2018", "citations": "27", "title":"Word2Bits - Quantized Word Vectors", "abstract": "<p>Word vectors require significant amounts of memory and storage, posing issues\nto resource limited devices like mobile phones and GPUs. We show that high\nquality quantized word vectors using 1-2 bits per parameter can be learned by\nintroducing a quantization function into Word2Vec. We furthermore show that\ntraining with the quantization function acts as a regularizer. We train word\nvectors on English Wikipedia (2017) and evaluate them on standard word\nsimilarity and analogy tasks and on question answering (SQuAD). Our quantized\nword vectors not only take 8-16x less space than full precision (32 bit) word\nvectors but also outperform them on word similarity tasks and question\nanswering.</p>\n", "tags": ["Quantization","Evaluation"] },
{"key": "laskar2017context", "year": "2017", "citations": "42", "title":"Context Aware Query Image Representation for Particular Object Retrieval", "abstract": "<p>The current models of image representation based on Convolutional Neural\nNetworks (CNN) have shown tremendous performance in image retrieval. Such\nmodels are inspired by the information flow along the visual pathway in the\nhuman visual cortex. We propose that in the field of particular object\nretrieval, the process of extracting CNN representations from query images with\na given region of interest (ROI) can also be modelled by taking inspiration\nfrom human vision. Particularly, we show that by making the CNN pay attention\non the ROI while extracting query image representation leads to significant\nimprovement over the baseline methods on challenging Oxford5k and Paris6k\ndatasets. Furthermore, we propose an extension to a recently introduced\nencoding method for CNN representations, regional maximum activations of\nconvolutions (R-MAC). The proposed extension weights the regional\nrepresentations using a novel saliency measure prior to aggregation. This leads\nto further improvement in retrieval accuracy.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "laskar2019geometric", "year": "2020", "citations": "12", "title":"Geometric Image Correspondence Verification by Dense Pixel Matching", "abstract": "<p>This paper addresses the problem of determining dense pixel correspondences\nbetween two images and its application to geometric correspondence verification\nin image retrieval. The main contribution is a geometric correspondence\nverification approach for re-ranking a shortlist of retrieved database images\nbased on their dense pair-wise matching with the query image at a pixel level.\nWe determine a set of cyclically consistent dense pixel matches between the\npair of images and evaluate local similarity of matched pixels using neural\nnetwork based image descriptors. Final re-ranking is based on a novel\nsimilarity function, which fuses the local similarity metric with a global\nsimilarity metric and a geometric consistency measure computed for the matched\npixels. For dense matching our approach utilizes a modified version of a\nrecently proposed dense geometric correspondence network (DGC-Net), which we\nalso improve by optimizing the architecture. The proposed model and similarity\nmetric compare favourably to the state-of-the-art image retrieval methods. In\naddition, we apply our method to the problem of long-term visual localization\ndemonstrating promising results and generalization across datasets.</p>\n", "tags": ["Image-Retrieval","Distance-Metric-Learning","Datasets","Hybrid-ANN-Methods","Re-Ranking"] },
{"key": "lassance2022composite", "year": "2021", "citations": "5", "title":"Composite Code Sparse Autoencoders for first stage retrieval", "abstract": "<p>We propose a Composite Code Sparse Autoencoder (CCSA) approach for\nApproximate Nearest Neighbor (ANN) search of document representations based on\nSiamese-BERT models. In Information Retrieval (IR), the ranking pipeline is\ngenerally decomposed in two stages: the first stage focus on retrieving a\ncandidate set from the whole collection. The second stage re-ranks the\ncandidate set by relying on more complex models. Recently, Siamese-BERT models\nhave been used as first stage ranker to replace or complement the traditional\nbag-of-word models. However, indexing and searching a large document collection\nrequire efficient similarity search on dense vectors and this is why ANN\ntechniques come into play. Since composite codes are naturally sparse, we first\nshow how CCSA can learn efficient parallel inverted index thanks to an\nuniformity regularizer. Second, CCSA can be used as a binary quantization\nmethod and we propose to combine it with the recent graph based ANN techniques.\nOur experiments on MSMARCO dataset reveal that CCSA outperforms IVF with\nproduct quantization. Furthermore, CCSA binary quantization is beneficial for\nthe index size, and memory usage for the graph-based HNSW method, while\nmaintaining a good level of recall and MRR. Third, we compare with recent\nsupervised quantization methods for image retrieval and find that CCSA is able\nto outperform them.</p>\n", "tags": ["Similarity-Search","Vector-Indexing","Supervised","Image-Retrieval","Graph-Based-ANN","Datasets","SIGIR","Memory-Efficiency","Quantization","Evaluation"] },
{"key": "lee2018stacked", "year": "2018", "citations": "1151", "title":"Stacked Cross Attention for Image-Text Matching", "abstract": "<p>In this paper, we study the problem of image-text matching. Inferring the\nlatent semantic alignment between objects or other salient stuff (e.g. snow,\nsky, lawn) and the corresponding words in sentences allows to capture\nfine-grained interplay between vision and language, and makes image-text\nmatching more interpretable. Prior work either simply aggregates the similarity\nof all possible pairs of regions and words without attending differentially to\nmore and less important words or regions, or uses a multi-step attentional\nprocess to capture limited number of semantic alignments which is less\ninterpretable. In this paper, we present Stacked Cross Attention to discover\nthe full latent alignments using both image regions and words in a sentence as\ncontext and infer image-text similarity. Our approach achieves the\nstate-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,\nour approach outperforms the current best methods by 22.1% relatively in text\nretrieval from image query, and 18.2% relatively in image retrieval with text\nquery (based on Recall@1). On MS-COCO, our approach improves sentence retrieval\nby 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1\nusing the 5K test set). Code has been made available at:\nhttps://github.com/kuanghuei/SCAN.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "lee2019contextualized", "year": "2020", "citations": "36", "title":"Contextualized Sparse Representations for Real-Time Open-Domain Question Answering", "abstract": "<p>Open-domain question answering can be formulated as a phrase retrieval\nproblem, in which we can expect huge scalability and speed benefit but often\nsuffer from low accuracy due to the limitation of existing phrase\nrepresentation models. In this paper, we aim to improve the quality of each\nphrase embedding by augmenting it with a contextualized sparse representation\n(Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g.,\ntf-idf) or directly learned (only few thousand dimensions), we leverage\nrectified self-attention to indirectly learn sparse vectors in n-gram\nvocabulary space. By augmenting the previous phrase retrieval model (Seo et\nal., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.\nOur CuratedTREC score is even better than the best known retrieve &amp; read model\nwith at least 45x faster inference speed.</p>\n", "tags": ["Scalability","Efficiency"] },
{"key": "lee2020metric", "year": "2020", "citations": "11", "title":"Metric Learning vs Classification for Disentangled Music Representation Learning", "abstract": "<p>Deep representation learning offers a powerful paradigm for mapping input\ndata onto an organized embedding space and is useful for many music information\nretrieval tasks. Two central methods for representation learning include deep\nmetric learning and classification, both having the same goal of learning a\nrepresentation that can generalize well across tasks. Along with\ngeneralization, the emerging concept of disentangled representations is also of\ngreat interest, where multiple semantic concepts (e.g., genre, mood,\ninstrumentation) are learned jointly but remain separable in the learned\nrepresentation space. In this paper we present a single representation learning\nframework that elucidates the relationship between metric learning,\nclassification, and disentanglement in a holistic manner. For this, we (1)\noutline past work on the relationship between metric learning and\nclassification, (2) extend this relationship to multi-label data by exploring\nthree different learning approaches and their disentangled versions, and (3)\nevaluate all models on four tasks (training time, similarity retrieval,\nauto-tagging, and triplet prediction). We find that classification-based models\nare generally advantageous for training time, similarity retrieval, and\nauto-tagging, while deep metric learning exhibits better performance for\ntriplet-prediction. Finally, we show that our proposed approach yields\nstate-of-the-art results for music auto-tagging.</p>\n", "tags": ["Similarity-Search","Distance-Metric-Learning","Evaluation","Tools-&-Libraries"] },
{"key": "lee2022correlation", "year": "2022", "citations": "59", "title":"Correlation Verification for Image Retrieval", "abstract": "<p>Geometric verification is considered a de facto solution for the re-ranking\ntask in image retrieval. In this study, we propose a novel image retrieval\nre-ranking network named Correlation Verification Networks (CVNet). Our\nproposed network, comprising deeply stacked 4D convolutional layers, gradually\ncompresses dense feature correlation into image similarity while learning\ndiverse geometric matching patterns from various image pairs. To enable\ncross-scale matching, it builds feature pyramids and constructs cross-scale\nfeature correlations within a single inference, replacing costly multi-scale\ninferences. In addition, we use curriculum learning with the hard negative\nmining and Hide-and-Seek strategy to handle hard samples without losing\ngenerality. Our proposed re-ranking network shows state-of-the-art performance\non several retrieval benchmarks with a significant margin (+12.6% in mAP on\nROxford-Hard+1M set) over state-of-the-art methods. The source code and models\nare available online: https://github.com/sungonce/CVNet.</p>\n", "tags": ["Image-Retrieval","CVPR","Hybrid-ANN-Methods","Re-Ranking","Evaluation"] },
{"key": "lei2019semi", "year": "2019", "citations": "49", "title":"Semi-Heterogeneous Three-Way Joint Embedding Network for Sketch-Based Image Retrieval", "abstract": "<p>Sketch-based image retrieval (SBIR) is a challenging task due to the large\ncross-domain gap between sketches and natural images. How to align abstract\nsketches and natural images into a common high-level semantic space remains a\nkey problem in SBIR. In this paper, we propose a novel semi-heterogeneous\nthree-way joint embedding network (Semi3-Net), which integrates three branches\n(a sketch branch, a natural image branch, and an edgemap branch) to learn more\ndiscriminative cross-domain feature representations for the SBIR task. The key\ninsight lies with how we cultivate the mutual and subtle relationships amongst\nthe sketches, natural images, and edgemaps. A semi-heterogeneous feature\nmapping is designed to extract bottom features from each domain, where the\nsketch and edgemap branches are shared while the natural image branch is\nheterogeneous to the other branches. In addition, a joint semantic embedding is\nintroduced to embed the features from different domains into a common\nhigh-level semantic space, where all of the three branches are shared. To\nfurther capture informative features common to both natural images and the\ncorresponding edgemaps, a co-attention model is introduced to conduct common\nchannel-wise feature recalibration between different domains. A hybrid-loss\nmechanism is designed to align the three branches, where an alignment loss and\na sketch-edgemap contrastive loss are presented to encourage the network to\nlearn invariant cross-domain representations. Experimental results on two\nwidely used category-level datasets (Sketchy and TU-Berlin Extension)\ndemonstrate that the proposed method outperforms state-of-the-art methods.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Image-Retrieval"] },
{"key": "lei2020locality", "year": "2020", "citations": "24", "title":"Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring", "abstract": "<p>Locality-Sensitive Hashing (LSH) is one of the most popular methods for\n\\(c\\)-Approximate Nearest Neighbor Search (\\(c\\)-ANNS) in high-dimensional spaces.\nIn this paper, we propose a novel LSH scheme based on the Longest Circular\nCo-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.\nWe introduce a novel concept of LCCS and a new data structure named Circular\nShift Array (CSA) for \\(k\\)-LCCS search. The insight of LCCS search framework is\nthat close data objects will have a longer LCCS than the far-apart ones with\nhigh probability. LCCS-LSH is <em>LSH-family-independent</em>, and it supports\n\\(c\\)-ANNS with different kinds of distance metrics. We also introduce a\nmulti-probe version of LCCS-LSH and conduct extensive experiments over five\nreal-life datasets. The experimental results demonstrate that LCCS-LSH\noutperforms state-of-the-art LSH schemes.</p>\n", "tags": ["Locality-Sensitive-Hashing","Tools-&-Libraries","Hashing-Methods","Distance-Metric-Learning","Datasets"] },
{"key": "lejeune2019adaptive", "year": "2019", "citations": "8", "title":"Adaptive Estimation for Approximate k-Nearest-Neighbor Computations", "abstract": "<p>Algorithms often carry out equally many computations for “easy” and “hard”\nproblem instances. In particular, algorithms for finding nearest neighbors\ntypically have the same running time regardless of the particular problem\ninstance. In this paper, we consider the approximate k-nearest-neighbor\nproblem, which is the problem of finding a subset of O(k) points in a given set\nof points that contains the set of k nearest neighbors of a given query point.\nWe propose an algorithm based on adaptively estimating the distances, and show\nthat it is essentially optimal out of algorithms that are only allowed to\nadaptively estimate distances. We then demonstrate both theoretically and\nexperimentally that the algorithm can achieve significant speedups relative to\nthe naive method.</p>\n", "tags": ["Uncategorized"] },
{"key": "leng2015hashing", "year": "2015", "citations": "30", "title":"Hashing for Distributed Data", "abstract": "<p>Recently, hashing based approximate nearest\nneighbors search has attracted much attention.\nExtensive centralized hashing algorithms have\nbeen proposed and achieved promising performance. However, due to the large scale of many\napplications, the data is often stored or even collected in a distributed manner. Learning hash\nfunctions by aggregating all the data into a fusion\ncenter is infeasible because of the prohibitively\nexpensive communication and computation overhead.\nIn this paper, we develop a novel hashing\nmodel to learn hash functions in a distributed setting. We cast a centralized hashing model as a\nset of subproblems with consensus constraints.\nWe find these subproblems can be analytically\nsolved in parallel on the distributed compute nodes. Since no training data is transmitted across\nthe nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large\nscale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>\n", "tags": ["Hashing-Methods","Evaluation","Datasets"] },
{"key": "leng2025hashing", "year": "2015", "citations": "30", "title":"Hashing for Distributed Data", "abstract": "<p>Recently, hashing based approximate nearest\nneighbors search has attracted much attention.\nExtensive centralized hashing algorithms have\nbeen proposed and achieved promising performance. However, due to the large scale of many\napplications, the data is often stored or even collected in a distributed manner. Learning hash\nfunctions by aggregating all the data into a fusion\ncenter is infeasible because of the prohibitively\nexpensive communication and computation overhead.\nIn this paper, we develop a novel hashing\nmodel to learn hash functions in a distributed setting. We cast a centralized hashing model as a\nset of subproblems with consensus constraints.\nWe find these subproblems can be analytically\nsolved in parallel on the distributed compute nodes. Since no training data is transmitted across\nthe nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large\nscale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>\n", "tags": ["Hashing-Methods","Evaluation","Datasets"] },
{"key": "leonhardt2021efficient", "year": "2022", "citations": "12", "title":"Efficient Neural Ranking using Forward Indexes", "abstract": "<p>Neural document ranking approaches, specifically transformer models, have\nachieved impressive gains in ranking performance. However, query processing\nusing such over-parameterized models is both resource and time intensive. In\nthis paper, we propose the Fast-Forward index – a simple vector forward index\nthat facilitates ranking documents using interpolation of lexical and semantic\nscores – as a replacement for contextual re-rankers and dense indexes based on\nnearest neighbor search. Fast-Forward indexes rely on efficient sparse models\nfor retrieval and merely look up pre-computed dense transformer-based vector\nrepresentations of documents and passages in constant time for fast CPU-based\nsemantic similarity computation during query processing. We propose index\npruning and theoretically grounded early stopping techniques to improve the\nquery processing throughput. We conduct extensive large-scale experiments on\nTREC-DL datasets and show improvements over hybrid indexes in performance and\nquery processing efficiency using only CPUs. Fast-Forward indexes can provide\nsuperior ranking performance using interpolation due to the complementary\nbenefits of lexical and semantic similarities.</p>\n", "tags": ["Datasets","Hybrid-ANN-Methods","Scalability","Evaluation","Efficiency"] },
{"key": "levi2020rethinking", "year": "2021", "citations": "15", "title":"Rethinking preventing class-collapsing in metric learning with margin-based losses", "abstract": "<p>Metric learning seeks perceptual embeddings where visually similar instances\nare close and dissimilar instances are apart, but learned representations can\nbe sub-optimal when the distribution of intra-class samples is diverse and\ndistinct sub-clusters are present. Although theoretically with optimal\nassumptions, margin-based losses such as the triplet loss and margin loss have\na diverse family of solutions. We theoretically prove and empirically show that\nunder reasonable noise assumptions, margin-based losses tend to project all\nsamples of a class with various modes onto a single point in the embedding\nspace, resulting in a class collapse that usually renders the space ill-sorted\nfor classification or retrieval. To address this problem, we propose a simple\nmodification to the embedding losses such that each sample selects its nearest\nsame-class counterpart in a batch as the positive element in the tuple. This\nallows for the presence of multiple sub-clusters within each class. The\nadaptation can be integrated into a wide range of metric learning losses. The\nproposed sampling method demonstrates clear benefits on various fine-grained\nimage retrieval datasets over a variety of existing losses; qualitative\nretrieval results show that samples with similar visual patterns are indeed\ncloser in the embedding space.</p>\n", "tags": ["ICCV","Distance-Metric-Learning","Datasets","Image-Retrieval"] },
{"key": "leyvavallina2021generalized", "year": "2021", "citations": "13", "title":"Generalized Contrastive Optimization of Siamese Networks for Place Recognition", "abstract": "<p>Visual place recognition is a challenging task in computer vision and a key\ncomponent of camera-based localization and navigation systems. Recently,\nConvolutional Neural Networks (CNNs) achieved high results and good\ngeneralization capabilities. They are usually trained using pairs or triplets\nof images labeled as either similar or dissimilar, in a binary fashion. In\npractice, the similarity between two images is not binary, but continuous.\nFurthermore, training these CNNs is computationally complex and involves costly\npair and triplet mining strategies. We propose a Generalized Contrastive loss\n(GCL) function that relies on image similarity as a continuous measure, and use\nit to train a siamese CNN. Furthermore, we present three techniques for\nautomatic annotation of image pairs with labels indicating their degree of\nsimilarity, and deploy them to re-annotate the MSLS, TB-Places, and 7Scenes\ndatasets. We demonstrate that siamese CNNs trained using the GCL function and\nthe improved annotations consistently outperform their binary counterparts. Our\nmodels trained on MSLS outperform the state-of-the-art methods, including\nNetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the\nPittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons\ndatasets. Furthermore, training a siamese network using the GCL function does\nnot require complex pair mining. We release the source code at\nhttps://github.com/marialeyvallina/generalized_contrastive_loss.</p>\n", "tags": ["Distance-Metric-Learning","Datasets"] },
{"key": "leyvavallina2023data", "year": "2023", "citations": "24", "title":"Data-efficient Large Scale Place Recognition with Graded Similarity Supervision", "abstract": "<p>Visual place recognition (VPR) is a fundamental task of computer vision for\nvisual localization. Existing methods are trained using image pairs that either\ndepict the same place or not. Such a binary indication does not consider\ncontinuous relations of similarity between images of the same place taken from\ndifferent positions, determined by the continuous nature of camera pose. The\nbinary similarity induces a noisy supervision signal into the training of VPR\nmethods, which stall in local minima and require expensive hard mining\nalgorithms to guarantee convergence. Motivated by the fact that two images of\nthe same place only partially share visual cues due to camera pose differences,\nwe deploy an automatic re-annotation strategy to re-label VPR datasets. We\ncompute graded similarity labels for image pairs based on available\nlocalization metadata. Furthermore, we propose a new Generalized Contrastive\nLoss (GCL) that uses graded similarity labels for training contrastive\nnetworks. We demonstrate that the use of the new labels and GCL allow to\ndispense from hard-pair mining, and to train image descriptors that perform\nbetter in VPR by nearest neighbor search, obtaining superior or comparable\nresults than methods that require expensive hard-pair mining and re-ranking\ntechniques. Code and models available at:\nhttps://github.com/marialeyvallina/generalized_contrastive_loss</p>\n", "tags": ["Datasets","CVPR","Hybrid-ANN-Methods","Re-Ranking"] },
{"key": "li2006very", "year": "2006", "citations": "634", "title":"Very Sparse Random Projections", "abstract": "<p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.</p>\n", "tags": ["Locality-Sensitive-Hashing","Efficiency","KDD"] },
{"key": "li2013learning", "year": "2013", "citations": "93", "title":"Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming\nan increasingly important tool in solving\nmany large-scale problems. Recently\na number of approaches to learning datadependent\nhash functions have been developed.\nIn this work, we propose a column\ngeneration based method for learning datadependent\nhash functions on the basis of\nproximity comparison information. Given a\nset of triplets that encode the pairwise proximity\ncomparison information, our method\nlearns hash functions that preserve the relative\ncomparison relationships in the data\nas well as possible within the large-margin\nlearning framework. The learning procedure\nis implemented using column generation and\nhence is named CGHash. At each iteration\nof the column generation procedure, the best\nhash function is selected. Unlike most other\nhashing methods, our method generalizes to\nnew data points naturally; and has a training\nobjective which is convex, thus ensuring\nthat the global optimum can be identi-\nfied. Experiments demonstrate that the proposed\nmethod learns compact binary codes\nand that its retrieval performance compares\nfavorably with state-of-the-art methods when\ntested on a few benchmark datasets.</p>\n", "tags": ["Scalability","Datasets","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "li2015feature", "year": "2015", "citations": "510", "title":"Feature Learning based Deep Supervised Hashing with Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of\nhashing for large-scale image retrieval. However,\nmost existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep\nhashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown\nbetter performance than traditional hashing methods with hand-crafted features. Most of these deep\nhashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this\npaper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH),\nto perform simultaneous feature learning and hashcode learning for applications with pairwise labels.\nExperiments on real datasets show that our DPSH\nmethod can outperform other methods to achieve\nthe state-of-the-art performance in image retrieval\napplications.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","Hashing-Methods","Evaluation","Supervised"] },
{"key": "li2017deep", "year": "2017", "citations": "171", "title":"Deep Unsupervised Image Hashing by Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy, we do not add a term to the loss function as this is difficult to optimize and tune. Instead, we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>\n", "tags": ["Image-Retrieval","Datasets","Neural-Hashing","Supervised","Compact-Codes","Hashing-Methods","Unsupervised"] },
{"key": "li2017fast", "year": "2017", "citations": "14", "title":"Fast k-Nearest Neighbour Search via Prioritized DCI", "abstract": "<p>Most exact methods for k-nearest neighbour search suffer from the curse of\ndimensionality; that is, their query times exhibit exponential dependence on\neither the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing\n(DCI) offers a promising way of circumventing the curse and successfully\nreduces the dependence of query time on intrinsic dimensionality from\nexponential to sublinear. In this paper, we propose a variant of DCI, which we\ncall Prioritized DCI, and show a remarkable improvement in the dependence of\nquery time on intrinsic dimensionality. In particular, a linear increase in\nintrinsic dimensionality, or equivalently, an exponential increase in the\nnumber of points near a query, can be mostly counteracted with just a linear\nincrease in space. We also demonstrate empirically that Prioritized DCI\nsignificantly outperforms prior methods. In particular, relative to\nLocality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of\ndistance evaluations by a factor of 14 to 116 and the memory consumption by a\nfactor of 21.</p>\n", "tags": ["Hashing-Methods","Similarity-Search","Locality-Sensitive-Hashing","Efficiency"] },
{"key": "li2017image", "year": "2018", "citations": "16", "title":"Image Super-resolution via Feature-augmented Random Forest", "abstract": "<p>Recent random-forest (RF)-based image super-resolution approaches inherit\nsome properties from dictionary-learning-based algorithms, but the\neffectiveness of the properties in RF is overlooked in the literature. In this\npaper, we present a novel feature-augmented random forest (FARF) for image\nsuper-resolution, where the conventional gradient-based features are augmented\nwith gradient magnitudes and different feature recipes are formulated on\ndifferent stages in an RF. The advantages of our method are that, firstly, the\ndictionary-learning-based features are enhanced by adding gradient magnitudes,\nbased on the observation that the non-linear gradient magnitude are with highly\ndiscriminative property. Secondly, generalized locality-sensitive hashing (LSH)\nis used to replace principal component analysis (PCA) for feature\ndimensionality reduction and original high-dimensional features are employed,\ninstead of the compressed ones, for the leaf-nodes’ regressors, since\nregressors can benefit from higher dimensional features. This\noriginal-compressed coupled feature sets scheme unifies the unsupervised LSH\nevaluation on both image super-resolution and content-based image retrieval\n(CBIR). Finally, we present a generalized weighted ridge regression (GWRR)\nmodel for the leaf-nodes’ regressors. Experiment results on several public\nbenchmark datasets show that our FARF method can achieve an average gain of\nabout 0.3 dB, compared to traditional RF-based methods. Furthermore, a\nfine-tuned FARF model can compare to or (in many cases) outperform some recent\nstateof-the-art deep-learning-based algorithms.</p>\n", "tags": ["Locality-Sensitive-Hashing","Image-Retrieval","Hashing-Methods","Datasets","Unsupervised","Evaluation"] },
{"key": "li2018discriminative", "year": "2018", "citations": "162", "title":"Discriminative multi-view Privileged Information learning for image re-ranking", "abstract": "<p>Conventional multi-view re-ranking methods usually perform asymmetrical\nmatching between the region of interest (ROI) in the query image and the whole\ntarget image for similarity computation. Due to the inconsistency in the visual\nappearance, this practice tends to degrade the retrieval accuracy particularly\nwhen the image ROI, which is usually interpreted as the image objectness,\naccounts for a smaller region in the image. Since Privileged Information (PI),\nwhich can be viewed as the image prior, enables well characterizing the image\nobjectness, we are aiming at leveraging PI for further improving the\nperformance of the multi-view re-ranking accuracy in this paper. Towards this\nend, we propose a discriminative multi-view re-ranking approach in which both\nthe original global image visual contents and the local auxiliary PI features\nare simultaneously integrated into a unified training framework for generating\nthe latent subspaces with sufficient discriminating power. For the on-the-fly\nre-ranking, since the multi-view PI features are unavailable, we only project\nthe original multi-view image representations onto the latent subspace, and\nthus the re-ranking can be achieved by computing and sorting the distances from\nthe multi-view embeddings to the separating hyperplane. Extensive experimental\nevaluations on the two public benchmarks Oxford5k and Paris6k reveal our\napproach provides further performance boost for accurate image re-ranking,\nwhilst the comparative study demonstrates the advantage of our method against\nother multi-view re-ranking methods.</p>\n", "tags": ["Tools-&-Libraries","CVPR","Hybrid-ANN-Methods","Re-Ranking","Survey-Paper","Evaluation"] },
{"key": "li2018dual", "year": "2019", "citations": "16", "title":"Dual Asymmetric Deep Hashing Learning", "abstract": "<p>Due to the impressive learning power, deep learning has achieved a remarkable\nperformance in supervised hash function learning. In this paper, we propose a\nnovel asymmetric supervised deep hashing method to preserve the semantic\nstructure among different categories and generate the binary codes\nsimultaneously. Specifically, two asymmetric deep networks are constructed to\nreveal the similarity between each pair of images according to their semantic\nlabels. The deep hash functions are then learned through two networks by\nminimizing the gap between the learned features and discrete codes.\nFurthermore, since the binary codes in the Hamming space also should keep the\nsemantic affinity existing in the original space, another asymmetric pairwise\nloss is introduced to capture the similarity between the binary codes and\nreal-value features. This asymmetric loss not only improves the retrieval\nperformance, but also contributes to a quick convergence at the training phase.\nBy taking advantage of the two-stream deep structures and two types of\nasymmetric pairwise functions, an alternating algorithm is designed to optimize\nthe deep features and high-quality binary codes efficiently. Experimental\nresults on three real-world datasets substantiate the effectiveness and\nsuperiority of our approach as compared with state-of-the-art.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Evaluation"] },
{"key": "li2018self", "year": "2018", "citations": "426", "title":"Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval", "abstract": "<p>Thanks to the success of deep learning, cross-modal retrieval has made\nsignificant progress recently. However, there still remains a crucial\nbottleneck: how to bridge the modality gap to further enhance the retrieval\naccuracy. In this paper, we propose a self-supervised adversarial hashing\n(\\textbf{SSAH}) approach, which lies among the early attempts to incorporate\nadversarial learning into cross-modal hashing in a self-supervised fashion. The\nprimary contribution of this work is that two adversarial networks are\nleveraged to maximize the semantic correlation and consistency of the\nrepresentations between different modalities. In addition, we harness a\nself-supervised semantic network to discover high-level semantic information in\nthe form of multi-label annotations. Such information guides the feature\nlearning process and preserves the modality relationships in both the common\nsemantic space and the Hamming space. Extensive experiments carried out on\nthree benchmark datasets validate that the proposed SSAH surpasses the\nstate-of-the-art methods.</p>\n", "tags": ["Supervised","Multimodal-Retrieval","Hashing-Methods","Datasets","CVPR","Self-Supervised","Evaluation","Robustness"] },
{"key": "li2018sign", "year": "2019", "citations": "8", "title":"Sign-Full Random Projections", "abstract": "<p>The method of 1-bit (“sign-sign”) random projections has been a popular tool\nfor efficient search and machine learning on large datasets. Given two \\(D\\)-dim\ndata vectors \\(u\\), \\(v\\in\\mathbb{R}^D\\), one can generate \\(x = \\sum_{i=1}^D u_i\nr_i\\), and \\(y = \\sum_{i=1}^D v_i r_i\\), where \\(r_i\\sim N(0,1)\\) iid. The\n“collision probability” is \\({Pr}\\left(sgn(x)=sgn(y)\\right) =\n1-\\frac{\\cos^{-1}\\rho}{\\pi}\\), where \\(\\rho = \\rho(u,v)\\) is the cosine\nsimilarity.\n  We develop “sign-full” random projections by estimating \\(\\rho\\) from (e.g.,)\nthe expectation \\(E(sgn(x)y)=\\sqrt{\\frac{2}{\\pi}} \\rho\\), which can be further\nsubstantially improved by normalizing \\(y\\). For nonnegative data, we recommend\nan interesting estimator based on \\(E\\left(y_- 1<em>{x\\geq 0} + y</em>+ 1_{x&lt;0}\\right)\\)\nand its normalized version. The recommended estimator almost matches the\naccuracy of the (computationally expensive) maximum likelihood estimator. At\nhigh similarity (\\(\\rho\\rightarrow1\\)), the asymptotic variance of recommended\nestimator is only \\(\\frac{4}{3\\pi} \\approx 0.4\\) of the estimator for sign-sign\nprojections. At small \\(k\\) and high similarity, the improvement would be even\nmuch more substantial.</p>\n", "tags": ["AAAI","Datasets","Locality-Sensitive-Hashing"] },
{"key": "li2019design", "year": "2018", "citations": "36", "title":"The Design and Implementation of a Real Time Visual Search System on JD E-commerce Platform", "abstract": "<p>We present the design and implementation of a visual search system for real\ntime image retrieval on JD.com, the world’s third largest and China’s largest\ne-commerce site. We demonstrate that our system can support real time visual\nsearch with hundreds of billions of product images at sub-second timescales and\nhandle frequent image updates through distributed hierarchical architecture and\nefficient indexing methods. We hope that sharing our practice with our real\nproduction system will inspire the middleware community’s interest and\nappreciation for building practical large scale systems for emerging\napplications, such as ecommerce visual search.</p>\n", "tags": ["Image-Retrieval"] },
{"key": "li2019memory", "year": "2019", "citations": "44", "title":"Memory-Based Neighbourhood Embedding for Visual Recognition", "abstract": "<p>Learning discriminative image feature embeddings is of great importance to\nvisual recognition. To achieve better feature embeddings, most current methods\nfocus on designing different network structures or loss functions, and the\nestimated feature embeddings are usually only related to the input images. In\nthis paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a\ngeneral CNN feature by considering its neighbourhood. The method aims to solve\ntwo critical problems, i.e., how to acquire more relevant neighbours in the\nnetwork training and how to aggregate the neighbourhood information for a more\ndiscriminative embedding. We first augment an episodic memory module into the\nnetwork, which can provide more relevant neighbours for both training and\ntesting. Then the neighbours are organized in a tree graph with the target\ninstance as the root node. The neighbourhood information is gradually\naggregated to the root node in a bottom-up manner, and aggregation weights are\nsupervised by the class relationships between the nodes. We apply MNE on image\nsearch and few shot learning tasks. Extensive ablation studies demonstrate the\neffectiveness of each component, and our method significantly outperforms the\nstate-of-the-art approaches.</p>\n", "tags": ["ICCV","Supervised"] },
{"key": "li2019neighborhood", "year": "2019", "citations": "40", "title":"Neighborhood Preserving Hashing for Scalable Video Retrieval", "abstract": "<p>In this paper, we propose a Neighborhood Preserving\nHashing (NPH) method for scalable video retrieval in an\nunsupervised manner. Unlike most existing deep video\nhashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal\nneighborhood information into the encoding network such\nthat the neighborhood-relevant visual content of a video can\nbe preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses\non partial useful content of each input frame conditioned\non the neighborhood information. We then integrate the\nneighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the\nlearned hashing functions can map similar videos to similar\nbinary codes. Extensive experiments on three widely-used\nbenchmark datasets validate the effectiveness of our proposed approach.</p>\n", "tags": ["Video-Retrieval","ICCV","Datasets","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "li2020deep", "year": "2020", "citations": "8", "title":"Deep Unsupervised Image Hashing by Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video\ncollections without having expensive annotations available. Hashing aims to\nlearn short binary codes for compact storage and efficient semantic retrieval.\nWe propose an unsupervised deep hashing layer called Bi-half Net that maximizes\nentropy of the binary codes. Entropy is maximal when both possible values of\nthe bit are uniformly (half-half) distributed. To maximize bit entropy, we do\nnot add a term to the loss function as this is difficult to optimize and tune.\nInstead, we design a new parameter-free network layer to explicitly force\ncontinuous image features to approximate the optimal half-half bit\ndistribution. This layer is shown to minimize a penalized term of the\nWasserstein distance between the learned continuous image features and the\noptimal half-half bit distribution. Experimental results on the image datasets\nFlickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and\nHmdb-51 show that our approach leads to compact codes and compares favorably to\nthe current state-of-the-art.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Unsupervised"] },
{"key": "li2020task", "year": "2021", "citations": "17", "title":"Task-adaptive Asymmetric Deep Cross-modal Hashing", "abstract": "<p>Supervised cross-modal hashing aims to embed the semantic correlations of\nheterogeneous modality data into the binary hash codes with discriminative\nsemantic labels. Because of its advantages on retrieval and storage efficiency,\nit is widely used for solving efficient cross-modal retrieval. However,\nexisting researches equally handle the different tasks of cross-modal\nretrieval, and simply learn the same couple of hash functions in a symmetric\nway for them. Under such circumstance, the uniqueness of different cross-modal\nretrieval tasks are ignored and sub-optimal performance may be brought.\nMotivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal\nHashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash\nfunctions for two sub-retrieval tasks via simultaneous modality representation\nand asymmetric hash learning. Unlike previous cross-modal hashing approaches,\nour learning framework jointly optimizes semantic preserving that transforms\ndeep features of multimedia data into binary hash codes, and the semantic\nregression which directly regresses query modality representation to explicit\nlabel. With our model, the binary codes can effectively preserve semantic\ncorrelations across different modalities, meanwhile, adaptively capture the\nquery semantics. The superiority of TA-ADCMH is proved on two standard datasets\nfrom many aspects.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Compact-Codes","Efficiency","Evaluation","Multimodal-Retrieval"] },
{"key": "li2021learning", "year": "2022", "citations": "47", "title":"Learning Semantic-Aligned Feature Representation for Text-based Person Search", "abstract": "<p>Text-based person search aims to retrieve images of a certain pedestrian by a\ntextual description. The key challenge of this task is to eliminate the\ninter-modality gap and achieve the feature alignment across modalities. In this\npaper, we propose a semantic-aligned embedding method for text-based person\nsearch, in which the feature alignment across modalities is achieved by\nautomatically learning the semantic-aligned visual features and textual\nfeatures. First, we introduce two Transformer-based backbones to encode robust\nfeature representations of the images and texts. Second, we design a\nsemantic-aligned feature aggregation network to adaptively select and aggregate\nfeatures with the same semantics into part-aware features, which is achieved by\na multi-head attention module constrained by a cross-modality part alignment\nloss and a diversity loss. Experimental results on the CUHK-PEDES and Flickr30K\ndatasets show that our method achieves state-of-the-art performances.</p>\n", "tags": ["Datasets","ICASSP"] },
{"key": "li2021more", "year": "2021", "citations": "23", "title":"More Robust Dense Retrieval with Contrastive Dual Learning", "abstract": "<p>Dense retrieval conducts text retrieval in the embedding space and has shown\nmany advantages compared to sparse retrieval. Existing dense retrievers\noptimize representations of queries and documents with contrastive training and\nmap them to the embedding space. The embedding space is optimized by aligning\nthe matched query-document pairs and pushing the negative documents away from\nthe query. However, in such training paradigm, the queries are only optimized\nto align to the documents and are coarsely positioned, leading to an\nanisotropic query embedding space. In this paper, we analyze the embedding\nspace distributions and propose an effective training paradigm, Contrastive\nDual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained\nquery representations for dense retrieval. DANCE incorporates an additional\ndual training object of query retrieval, inspired by the classic information\nretrieval training axiom, query likelihood. With contrastive learning, the dual\ntraining object of DANCE learns more tailored representations for queries and\ndocuments to keep the embedding space smooth and uniform, thriving on the\nranking performance of DANCE on the MS MARCO document retrieval task. Different\nfrom ANCE that only optimized with the document retrieval task, DANCE\nconcentrates the query embeddings closer to document representations while\nmaking the document distribution more discriminative. Such concentrated query\nembedding distribution assigns more uniform negative sampling probabilities to\nqueries and helps to sufficiently optimize query representations in the query\nretrieval task. Our codes are released at https://github.com/thunlp/DANCE.</p>\n", "tags": ["Self-Supervised","SIGIR","Text-Retrieval","Evaluation"] },
{"key": "li2021self", "year": "2021", "citations": "38", "title":"Self-Supervised Video Hashing via Bidirectional Transformers", "abstract": "<p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>\n", "tags": ["Video-Retrieval","Datasets","CVPR","Self-Supervised","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "li2022adaptive", "year": "2022", "citations": "23", "title":"Adaptive Structural Similarity Preserving for Unsupervised Cross Modal Hashing", "abstract": "<p>Cross-modal hashing is an important approach for multimodal data management\nand application. Existing unsupervised cross-modal hashing algorithms mainly\nrely on data features in pre-trained models to mine their similarity\nrelationships. However, their optimization objectives are based on the static\nmetric between the original uni-modal features, without further exploring data\ncorrelations during the training. In addition, most of them mainly focus on\nassociation mining and alignment among pairwise instances in continuous space\nbut ignore the latent structural correlations contained in the semantic hashing\nspace. In this paper, we propose an unsupervised hash learning framework,\nnamely Adaptive Structural Similarity Preservation Hashing (ASSPH), to solve\nthe above problems. Firstly, we propose an adaptive learning scheme, with\nlimited data and training batches, to enrich semantic correlations of unlabeled\ninstances during the training process and meanwhile to ensure a smooth\nconvergence of the training process. Secondly, we present an asymmetric\nstructural semantic representation learning scheme. We introduce structural\nsemantic metrics based on graph adjacency relations during the semantic\nreconstruction and correlation mining stage and meanwhile align the structure\nsemantics in the hash space with an asymmetric binary optimization process.\nFinally, we conduct extensive experiments to validate the enhancements of our\nwork in comparison with existing works.</p>\n", "tags": ["Text-Retrieval","Tools-&-Libraries","Hashing-Methods","Unsupervised","Evaluation"] },
{"key": "li2023constructing", "year": "2023", "citations": "10", "title":"Constructing Tree-based Index for Efficient and Effective Dense Retrieval", "abstract": "<p>Recent studies have shown that Dense Retrieval (DR) techniques can\nsignificantly improve the performance of first-stage retrieval in IR systems.\nDespite its empirical effectiveness, the application of DR is still limited. In\ncontrast to statistic retrieval models that rely on highly efficient inverted\nindex solutions, DR models build dense embeddings that are difficult to be\npre-processed with most existing search indexing systems. To avoid the\nexpensive cost of brute-force search, the Approximate Nearest Neighbor (ANN)\nalgorithm and corresponding indexes are widely applied to speed up the\ninference process of DR models. Unfortunately, while ANN can improve the\nefficiency of DR models, it usually comes with a significant price on retrieval\nperformance.\n  To solve this issue, we propose JTR, which stands for Joint optimization of\nTRee-based index and query encoding. Specifically, we design a new unified\ncontrastive learning loss to train tree-based index and query encoder in an\nend-to-end manner. The tree-based negative sampling strategy is applied to make\nthe tree have the maximum heap property, which supports the effectiveness of\nbeam search well. Moreover, we treat the cluster assignment as an optimization\nproblem to update the tree-based index that allows overlapped clustering. We\nevaluate JTR on numerous popular retrieval benchmarks. Experimental results\nshow that JTR achieves better retrieval performance while retaining high system\nefficiency compared with widely-adopted baselines. It provides a potential\nsolution to balance efficiency and effectiveness in neural retrieval system\ndesigns.</p>\n", "tags": ["Evaluation","SIGIR","Self-Supervised","Tree-Based-ANN","Efficiency"] },
{"key": "li2023dual", "year": "2022", "citations": "16", "title":"Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval", "abstract": "<p>Unsupervised video hashing usually optimizes binary codes by learning to\nreconstruct input videos. Such reconstruction constraint spends much effort on\nframe-level temporal context changes without focusing on video-level global\nsemantics that are more useful for retrieval. Hence, we address this problem by\ndecomposing video information into reconstruction-dependent and\nsemantic-dependent information, which disentangles the semantic extraction from\nreconstruction constraint. Specifically, we first design a simple dual-stream\nstructure, including a temporal layer and a hash layer. Then, with the help of\nsemantic similarity knowledge obtained from self-supervision, the hash layer\nlearns to capture information for semantic retrieval, while the temporal layer\nlearns to capture the information for reconstruction. In this way, the model\nnaturally preserves the disentangled semantics into binary codes. Validated by\ncomprehensive experiments, our method consistently outperforms the\nstate-of-the-arts on three video benchmarks.</p>\n", "tags": ["Hashing-Methods","Unsupervised","Compact-Codes"] },
{"key": "li2023style", "year": "2023", "citations": "5", "title":"The style transformer with common knowledge optimization for image-text retrieval", "abstract": "<p>Image-text retrieval which associates different modalities has drawn broad\nattention due to its excellent research value and broad real-world application.\nHowever, most of the existing methods haven’t taken the high-level semantic\nrelationships (“style embedding”) and common knowledge from multi-modalities\ninto full consideration. To this end, we introduce a novel style transformer\nnetwork with common knowledge optimization (CKSTN) for image-text retrieval.\nThe main module is the common knowledge adaptor (CKA) with both the style\nembedding extractor (SEE) and the common knowledge optimization (CKO) modules.\nSpecifically, the SEE uses the sequential update strategy to effectively\nconnect the features of different stages in SEE. The CKO module is introduced\nto dynamically capture the latent concepts of common knowledge from different\nmodalities. Besides, to get generalized temporal common knowledge, we propose a\nsequential update strategy to effectively integrate the features of different\nlayers in SEE with previous common feature units. CKSTN demonstrates the\nsuperiorities of the state-of-the-art methods in image-text retrieval on MSCOCO\nand Flickr30K datasets. Moreover, CKSTN is constructed based on the lightweight\ntransformer which is more convenient and practical for the application of real\nscenes, due to the better performance and lower parameters.</p>\n", "tags": ["Datasets","Text-Retrieval","Evaluation"] },
{"key": "li2025deep", "year": "2017", "citations": "171", "title":"Deep Supervised Discrete Hashing", "abstract": "<p>With the rapid growth of image and video data on the web, hashing has been\nextensively studied for image or video search in recent years. Benefiting from\nrecent advances in deep learning, deep hashing methods have achieved promising\nresults for image retrieval. However, there are some limitations of previous deep\nhashing methods (e.g., the semantic information is not fully exploited). In this\npaper, we develop a deep supervised discrete hashing algorithm based on the\nassumption that the learned binary codes should be ideal for classification. Both the\npairwise label information and the classification information are used to learn the\nhash codes within one stream framework. We constrain the outputs of the last layer\nto be binary codes directly, which is rarely investigated in deep hashing algorithm.\nBecause of the discrete nature of hash codes, an alternating minimization method\nis used to optimize the objective function. Experimental results have shown that\nour method outperforms current state-of-the-art methods on benchmark datasets.</p>\n", "tags": ["Video-Retrieval","Image-Retrieval","Datasets","Neural-Hashing","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "li2025feature", "year": "2015", "citations": "510", "title":"Feature Learning based Deep Supervised Hashing with Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of\nhashing for large-scale image retrieval. However,\nmost existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep\nhashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown\nbetter performance than traditional hashing methods with hand-crafted features. Most of these deep\nhashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this\npaper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH),\nto perform simultaneous feature learning and hashcode learning for applications with pairwise labels.\nExperiments on real datasets show that our DPSH\nmethod can outperform other methods to achieve\nthe state-of-the-art performance in image retrieval\napplications.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","Hashing-Methods","Evaluation","Supervised"] },
{"key": "li2025learning", "year": "2013", "citations": "93", "title":"Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming\nan increasingly important tool in solving\nmany large-scale problems. Recently\na number of approaches to learning datadependent\nhash functions have been developed.\nIn this work, we propose a column\ngeneration based method for learning datadependent\nhash functions on the basis of\nproximity comparison information. Given a\nset of triplets that encode the pairwise proximity\ncomparison information, our method\nlearns hash functions that preserve the relative\ncomparison relationships in the data\nas well as possible within the large-margin\nlearning framework. The learning procedure\nis implemented using column generation and\nhence is named CGHash. At each iteration\nof the column generation procedure, the best\nhash function is selected. Unlike most other\nhashing methods, our method generalizes to\nnew data points naturally; and has a training\nobjective which is convex, thus ensuring\nthat the global optimum can be identi-\nfied. Experiments demonstrate that the proposed\nmethod learns compact binary codes\nand that its retrieval performance compares\nfavorably with state-of-the-art methods when\ntested on a few benchmark datasets.</p>\n", "tags": ["Scalability","Datasets","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "li2025neighborhood", "year": "2019", "citations": "40", "title":"Neighborhood Preserving Hashing for Scalable Video Retrieval", "abstract": "<p>In this paper, we propose a Neighborhood Preserving\nHashing (NPH) method for scalable video retrieval in an\nunsupervised manner. Unlike most existing deep video\nhashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal\nneighborhood information into the encoding network such\nthat the neighborhood-relevant visual content of a video can\nbe preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses\non partial useful content of each input frame conditioned\non the neighborhood information. We then integrate the\nneighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the\nlearned hashing functions can map similar videos to similar\nbinary codes. Extensive experiments on three widely-used\nbenchmark datasets validate the effectiveness of our proposed approach.</p>\n", "tags": ["Video-Retrieval","ICCV","Datasets","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "li2025self", "year": "2021", "citations": "38", "title":"Self-Supervised Video Hashing via Bidirectional Transformers", "abstract": "<p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>\n", "tags": ["Video-Retrieval","Datasets","CVPR","Self-Supervised","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "li2025very", "year": "2006", "citations": "634", "title":"Very Sparse Random Projections", "abstract": "<p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.</p>\n", "tags": ["Locality-Sensitive-Hashing","Efficiency","KDD"] },
{"key": "liang2016optimizing", "year": "2016", "citations": "52", "title":"Optimizing Top Precision Performance Measure of Content-Based Image Retrieval by Learning Similarity Function", "abstract": "<p>In this paper we study the problem of content-based image retrieval. In this\nproblem, the most popular performance measure is the top precision measure, and\nthe most important component of a retrieval system is the similarity function\nused to compare a query image against a database image. However, up to now,\nthere is no existing similarity learning method proposed to optimize the top\nprecision measure. To fill this gap, in this paper, we propose a novel\nsimilarity learning method to maximize the top precision measure. We model this\nproblem as a minimization problem with an objective function as the combination\nof the losses of the relevant images ranked behind the top-ranked irrelevant\nimage, and the squared Frobenius norm of the similarity function parameter.\nThis minimization problem is solved as a quadratic programming problem. The\nexperiments over two benchmark data sets show the advantages of the proposed\nmethod over other similarity learning methods when the top precision is used as\nthe performance measure.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "liang2020dynamic", "year": "2021", "citations": "5", "title":"Dynamic Sampling for Deep Metric Learning", "abstract": "<p>Deep metric learning maps visually similar images onto nearby locations and\nvisually dissimilar images apart from each other in an embedding manifold. The\nlearning process is mainly based on the supplied image negative and positive\ntraining pairs. In this paper, a dynamic sampling strategy is proposed to\norganize the training pairs in an easy-to-hard order to feed into the network.\nIt allows the network to learn general boundaries between categories from the\neasy training pairs at its early stages and finalize the details of the model\nmainly relying on the hard training samples in the later. Compared to the\nexisting training sample mining approaches, the hard samples are mined with\nlittle harm to the learned general model. This dynamic sampling strategy is\nformularized as two simple terms that are compatible with various loss\nfunctions. Consistent performance boost is observed when it is integrated with\nseveral popular loss functions on fashion search, fine-grained classification,\nand person re-identification tasks.</p>\n", "tags": ["Distance-Metric-Learning","Evaluation"] },
{"key": "liang2020embedding", "year": "2020", "citations": "23", "title":"Embedding-based Zero-shot Retrieval through Query Generation", "abstract": "<p>Passage retrieval addresses the problem of locating relevant passages,\nusually from a large corpus, given a query. In practice, lexical term-matching\nalgorithms like BM25 are popular choices for retrieval owing to their\nefficiency. However, term-based matching algorithms often miss relevant\npassages that have no lexical overlap with the query and cannot be finetuned to\ndownstream datasets. In this work, we consider the embedding-based two-tower\narchitecture as our neural retrieval model. Since labeled data can be scarce\nand because neural retrieval models require vast amounts of data to train, we\npropose a novel method for generating synthetic training data for retrieval.\nOur system produces remarkable results, significantly outperforming BM25 on 5\nout of 6 datasets tested, by an average of 2.45 points for Recall@1. In some\ncases, our model trained on synthetic data can even outperform the same model\ntrained on real data</p>\n", "tags": ["Datasets","Few-Shot-&-Zero-Shot","Evaluation","Efficiency"] },
{"key": "liang2021dynamic", "year": "2021", "citations": "5", "title":"Dynamic Sampling for Deep Metric Learning", "abstract": "<p>Deep metric learning maps visually similar images onto nearby locations and\nvisually dissimilar images apart from each other in an embedding manifold. The\nlearning process is mainly based on the supplied image negative and positive\ntraining pairs. In this paper, a dynamic sampling strategy is proposed to\norganize the training pairs in an easy-to-hard order to feed into the network.\nIt allows the network to learn general boundaries between categories from the\neasy training pairs at its early stages and finalize the details of the model\nmainly relying on the hard training samples in the later. Compared to the\nexisting training sample mining approaches, the hard samples are mined with\nlittle harm to the learned general model. This dynamic sampling strategy is\nformularized as two simple terms that are compatible with various loss\nfunctions. Consistent performance boost is observed when it is integrated with\nseveral popular loss functions on fashion search, fine-grained classification,\nand person re-identification tasks.</p>\n", "tags": ["Distance-Metric-Learning","Evaluation"] },
{"key": "liao2018triplet", "year": "2017", "citations": "32", "title":"Triplet-based Deep Similarity Learning for Person Re-Identification", "abstract": "<p>In recent years, person re-identification (re-id) catches great attention in\nboth computer vision community and industry. In this paper, we propose a new\nframework for person re-identification with a triplet-based deep similarity\nlearning using convolutional neural networks (CNNs). The network is trained\nwith triplet input: two of them have the same class labels and the other one is\ndifferent. It aims to learn the deep feature representation, with which the\ndistance within the same class is decreased, while the distance between the\ndifferent classes is increased as much as possible. Moreover, we trained the\nmodel jointly on six different datasets, which differs from common practice -\none model is just trained on one dataset and tested also on the same one.\nHowever, the enormous number of possible triplet data among the large number of\ntraining samples makes the training impossible. To address this challenge, a\ndouble-sampling scheme is proposed to generate triplets of images as effective\nas possible. The proposed framework is evaluated on several benchmark datasets.\nThe experimental results show that, our method is effective for the task of\nperson re-identification and it is comparable or even outperforms the\nstate-of-the-art methods.</p>\n", "tags": ["ICCV","Datasets","Evaluation","Tools-&-Libraries"] },
{"key": "lin2013general", "year": "2013", "citations": "199", "title":"A General Two-Step Approach to Learning-Based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which\nis typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to\nrespond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose\na flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.\nThis framework allows a number of existing approaches to hashing to be placed in context, and simplifies the\ndevelopment of new problem-specific hashing methods. Our framework decomposes hashing learning problem\ninto two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically\nbe formulated as binary quadratic problems, and the second step can be accomplished by training standard binary\nclassifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate\nthat the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>\n", "tags": ["Hashing-Methods","Tools-&-Libraries","ICCV"] },
{"key": "lin2014fast", "year": "2014", "citations": "453", "title":"Fast supervised hashing with decision trees for high-dimensional data", "abstract": "<p>Supervised hashing aims to map the original features to\ncompact binary codes that are able to preserve label based\nsimilarity in the Hamming space. Non-linear hash functions\nhave demonstrated their advantage over linear ones due to\ntheir powerful generalization capability. In the literature,\nkernel functions are typically used to achieve non-linearity\nin hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.\nHere we propose to use boosted decision trees for achieving\nnon-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional\ndata. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem\nand an efficient GraphCut based block search method for\nsolving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the binary\ncodes. Experiments demonstrate that our proposed method\nsignificantly outperforms most state-of-the-art methods in\nretrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster\nthan many methods in terms of training time.</p>\n", "tags": ["Scalability","CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "lin2014optimizing", "year": "2014", "citations": "23", "title":"Optimizing Ranking Measures for Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest—multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.\nThe resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "lin2015deep", "year": "2015", "citations": "626", "title":"Deep learning of binary hash codes for fast image retrieval", "abstract": "<p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.\nhe utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","CVPR","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Supervised"] },
{"key": "lin2015semantics", "year": "2015", "citations": "540", "title":"Semantics-Preserving Hashing for Cross-View Retrieval", "abstract": "<p>With benefits of low storage costs and high query speeds,\nhashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.\nIn this paper, we study the problem of cross-view retrieval\nand propose an effective Semantics-Preserving Hashing\nmethod, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them\ninto a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the\nKullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt\nhash codes. And for any unseen instance, predicted hash\ncodes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,\nusing a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>\n", "tags": ["Scalability","Datasets","CVPR","Memory-Efficiency","Hashing-Methods","Evaluation","Supervised"] },
{"key": "lin2018learning", "year": "2018", "citations": "7", "title":"Learning a Disentangled Embedding for Monocular 3D Shape Retrieval and Pose Estimation", "abstract": "<p>We propose a novel approach to jointly perform 3D shape retrieval and pose\nestimation from monocular images.In order to make the method robust to\nreal-world image variations, e.g. complex textures and backgrounds, we learn an\nembedding space from 3D data that only includes the relevant information,\nnamely the shape and pose. Our approach explicitly disentangles a shape vector\nand a pose vector, which alleviates both pose bias for 3D shape retrieval and\ncategorical bias for pose estimation. We then train a CNN to map the images to\nthis embedding space, and then retrieve the closest 3D shape from the database\nand estimate the 6D pose of the object. Our method achieves 10.3 median error\nfor pose estimation and 0.592 top-1-accuracy for category agnostic 3D object\nretrieval on the Pascal3D+ dataset, outperforming the previous state-of-the-art\nmethods on both tasks.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "lin2018supervised", "year": "2018", "citations": "64", "title":"Supervised Online Hashing via Similarity Distribution Learning", "abstract": "<p>Online hashing has attracted extensive research attention when facing\nstreaming data. Most online hashing methods, learning binary codes based on\npairwise similarities of training instances, fail to capture the semantic\nrelationship, and suffer from a poor generalization in large-scale applications\ndue to large variations. In this paper, we propose to model the similarity\ndistributions between the input data and the hashing codes, upon which a novel\nsupervised online hashing method, dubbed as Similarity Distribution based\nOnline Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship\nin the produced Hamming space. Specifically, we first transform the discrete\nsimilarity matrix into a probability matrix via a Gaussian-based normalization\nto address the extremely imbalanced distribution issue. And then, we introduce\na scaling Student t-distribution to solve the challenging initialization\nproblem, and efficiently bridge the gap between the known and unknown\ndistributions. Lastly, we align the two distributions via minimizing the\nKullback-Leibler divergence (KL-diverence) with stochastic gradient descent\n(SGD), by which an intuitive similarity constraint is imposed to update hashing\nmodel on the new streaming data with a powerful generalizing ability to the\npast data. Extensive experiments on three widely-used benchmarks validate the\nsuperiority of the proposed SDOH over the state-of-the-art methods in the\nonline retrieval task.</p>\n", "tags": ["Hashing-Methods","Scalability","Supervised","Compact-Codes"] },
{"key": "lin2019fashion", "year": "2020", "citations": "81", "title":"Fashion Outfit Complementary Item Retrieval", "abstract": "<p>Complementary fashion item recommendation is critical for fashion outfit\ncompletion. Existing methods mainly focus on outfit compatibility prediction\nbut not in a retrieval setting. We propose a new framework for outfit\ncomplementary item retrieval. Specifically, a category-based subspace attention\nnetwork is presented, which is a scalable approach for learning the subspace\nattentions. In addition, we introduce an outfit ranking loss that better models\nthe item relationships of an entire outfit. We evaluate our method on the\noutfit compatibility, FITB and new retrieval tasks. Experimental results\ndemonstrate that our approach outperforms state-of-the-art methods in both\ncompatibility prediction and complementary item retrieval</p>\n", "tags": ["Recommender-Systems","CVPR","Tools-&-Libraries"] },
{"key": "lin2019hadamard", "year": "2020", "citations": "41", "title":"Hadamard Matrix Guided Online Hashing", "abstract": "<p>Online image hashing has attracted increasing research attention recently,\nwhich receives large-scale data in a streaming manner to update the hash\nfunctions on-the-fly. Its key challenge lies in the difficulty of balancing the\nlearning timeliness and model accuracy. To this end, most works follow a\nsupervised setting, i.e., using class labels to boost the hashing performance,\nwhich defects in two aspects: First, strong constraints, e.g., orthogonal or\nsimilarity preserving, are used, which however are typically relaxed and lead\nto large accuracy drop. Second, large amounts of training batches are required\nto learn the up-to-date hash functions, which largely increase the learning\ncomplexity. To handle the above challenges, a novel supervised online hashing\nscheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this\npaper. Our key innovation lies in introducing Hadamard matrix, which is an\northogonal binary matrix built via Sylvester method. In particular, to release\nthe need of strong constraints, we regard each column of Hadamard matrix as the\ntarget code for each class label, which by nature satisfies several desired\nproperties of hashing codes. To accelerate the online training, LSH is first\nadopted to align the lengths of target code and to-be-learned binary code. We\nthen treat the learning of hash functions as a set of binary classification\nproblems to fit the assigned target code. Finally, extensive experiments\ndemonstrate the superior accuracy and efficiency of the proposed method over\nvarious state-of-the-art methods. Codes are available at\nhttps://github.com/lmbxmu/mycode.</p>\n", "tags": ["Supervised","Locality-Sensitive-Hashing","Image-Retrieval","Hashing-Methods","Compact-Codes","Scalability","Evaluation","Efficiency"] },
{"key": "lin2019supervised", "year": "2018", "citations": "64", "title":"Supervised Online Hashing via Hadamard Codebook Learning", "abstract": "<p>In recent years, binary code learning, a.k.a hashing, has received extensive\nattention in large-scale multimedia retrieval. It aims to encode\nhigh-dimensional data points to binary codes, hence the original\nhigh-dimensional metric space can be efficiently approximated via Hamming\nspace. However, most existing hashing methods adopted offline batch learning,\nwhich is not suitable to handle incremental datasets with streaming data or new\ninstances. In contrast, the robustness of the existing online hashing remains\nas an open problem, while the embedding of supervised/semantic information\nhardly boosts the performance of the online hashing, mainly due to the defect\nof unknown category numbers in supervised learning. In this paper, we proposed\nan online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH),\nwhich aims to solve the above problems towards robust and supervised online\nhashing. In particular, we first assign an appropriate high-dimensional binary\ncodes to each class label, which is generated randomly by Hadamard codes to\neach class label, which is generated randomly by Hadamard codes. Subsequently,\nLSH is adopted to reduce the length of such Hadamard codes in accordance with\nthe hash bits, which can adapt the predefined binary codes online, and\ntheoretically guarantee the semantic similarity. Finally, we consider the\nsetting of stochastic data acquisition, which facilitates our method to\nefficiently learn the corresponding hashing functions via stochastic gradient\ndescend (SGD) online. Notably, the proposed HCOH can be embedded with\nsupervised labels and it not limited to a predefined category number. Extensive\nexperiments on three widely-used benchmarks demonstrate the merits of the\nproposed scheme over the state-of-the-art methods. The code is available at\nhttps://github.com/lmbxmu/mycode/tree/master/2018ACMMM_HCOH.</p>\n", "tags": ["Supervised","Locality-Sensitive-Hashing","Hashing-Methods","Datasets","Compact-Codes","Scalability","Evaluation","Robustness"] },
{"key": "lin2019towards", "year": "2019", "citations": "50", "title":"Towards Optimal Discrete Online Hashing with Balanced Similarity", "abstract": "<p>When facing large-scale image datasets, online hashing serves as a promising\nsolution for online retrieval and prediction tasks. It encodes the online\nstreaming data into compact binary codes, and simultaneously updates the hash\nfunctions to renew codes of the existing dataset. To this end, the existing\nmethods update hash functions solely based on the new data batch, without\ninvestigating the correlation between such new data and the existing dataset.\nIn addition, existing works update the hash functions using a relaxation\nprocess in its corresponding approximated continuous space. And it remains as\nan open problem to directly apply discrete optimizations in online hashing. In\nthis paper, we propose a novel supervised online hashing method, termed\nBalanced Similarity for Online Discrete Hashing (BSODH), to solve the above\nproblems in a unified framework. BSODH employs a well-designed hashing\nalgorithm to preserve the similarity between the streaming data and the\nexisting dataset via an asymmetric graph regularization. We further identify\nthe “data-imbalance” problem brought by the constructed asymmetric graph, which\nrestricts the application of discrete optimization in our problem. Therefore, a\nnovel balanced similarity is further proposed, which uses two equilibrium\nfactors to balance the similar and dissimilar weights and eventually enables\nthe usage of discrete optimizations. Extensive experiments conducted on three\nwidely-used benchmarks demonstrate the advantages of the proposed method over\nthe state-of-the-art methods.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Compact-Codes","AAAI","Scalability"] },
{"key": "lin2020fast", "year": "2020", "citations": "22", "title":"Fast Class-wise Updating for Online Hashing", "abstract": "<p>Online image hashing has received increasing research attention recently,\nwhich processes large-scale data in a streaming fashion to update the hash\nfunctions on-the-fly. To this end, most existing works exploit this problem\nunder a supervised setting, i.e., using class labels to boost the hashing\nperformance, which suffers from the defects in both adaptivity and efficiency:\nFirst, large amounts of training batches are required to learn up-to-date hash\nfunctions, which leads to poor online adaptivity. Second, the training is\ntime-consuming, which contradicts with the core need of online learning. In\nthis paper, a novel supervised online hashing scheme, termed Fast Class-wise\nUpdating for Online Hashing (FCOH), is proposed to address the above two\nchallenges by introducing a novel and efficient inner product operation. To\nachieve fast online adaptivity, a class-wise updating method is developed to\ndecompose the binary code learning and alternatively renew the hash functions\nin a class-wise fashion, which well addresses the burden on large amounts of\ntraining batches. Quantitatively, such a decomposition further leads to at\nleast 75% storage saving. To further achieve online efficiency, we propose a\nsemi-relaxation optimization, which accelerates the online training by treating\ndifferent binary constraints independently. Without additional constraints and\nvariables, the time complexity is significantly reduced. Such a scheme is also\nquantitatively shown to well preserve past information during updating hashing\nfunctions. We have quantitatively demonstrated that the collective effort of\nclass-wise updating and semi-relaxation optimization provides a superior\nperformance comparing to various state-of-the-art methods, which is verified\nthrough extensive experiments on three widely-used datasets.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Compact-Codes","Scalability","Evaluation","Efficiency"] },
{"key": "lin2021deep", "year": "2021", "citations": "8", "title":"Deep Self-Adaptive Hashing for Image Retrieval", "abstract": "<p>Hashing technology has been widely used in image retrieval due to its\ncomputational and storage efficiency. Recently, deep unsupervised hashing\nmethods have attracted increasing attention due to the high cost of human\nannotations in the real world and the superiority of deep learning technology.\nHowever, most deep unsupervised hashing methods usually pre-compute a\nsimilarity matrix to model the pairwise relationship in the pre-trained feature\nspace. Then this similarity matrix would be used to guide hash learning, in\nwhich most of the data pairs are treated equivalently. The above process is\nconfronted with the following defects: 1) The pre-computed similarity matrix is\ninalterable and disconnected from the hash learning process, which cannot\nexplore the underlying semantic information. 2) The informative data pairs may\nbe buried by the large number of less-informative data pairs. To solve the\naforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model\nto adaptively capture the semantic information with two special designs:\nAdaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC).\nFirstly, we adopt the AND to initially construct a neighborhood-based\nsimilarity matrix, and then refine this initial similarity matrix with a novel\nupdate strategy to further investigate the semantic structure behind the\nlearned representation. Secondly, we measure the priorities of data pairs with\nPIC and assign adaptive weights to them, which is relies on the assumption that\nmore dissimilar data pairs contain more discriminative information for hash\nlearning. Extensive experiments on several datasets demonstrate that the above\ntwo technologies facilitate the deep hashing model to achieve superior\nperformance.</p>\n", "tags": ["Evaluation","Efficiency","Datasets","Unsupervised","CIKM","Hashing-Methods","Neural-Hashing","Image-Retrieval","Supervised"] },
{"key": "lin2021large", "year": "2021", "citations": "14", "title":"Large-Scale Network Embedding in Apache Spark", "abstract": "<p>Network embedding has been widely used in social recommendation and network\nanalysis, such as recommendation systems and anomaly detection with graphs.\nHowever, most of previous approaches cannot handle large graphs efficiently,\ndue to that (i) computation on graphs is often costly and (ii) the size of\ngraph or the intermediate results of vectors could be prohibitively large,\nrendering it difficult to be processed on a single machine. In this paper, we\npropose an efficient and effective distributed algorithm for network embedding\non large graphs using Apache Spark, which recursively partitions a graph into\nseveral small-sized subgraphs to capture the internal and external structural\ninformation of nodes, and then computes the network embedding for each subgraph\nin parallel. Finally, by aggregating the outputs on all subgraphs, we obtain\nthe embeddings of nodes in a linear cost. After that, we demonstrate in various\nexperiments that our proposed approach is able to handle graphs with billions\nof edges within a few hours and is at least 4 times faster than the\nstate-of-the-art approaches. Besides, it achieves up to \\(4.25%\\) and \\(4.27%\\)\nimprovements on link prediction and node classification tasks respectively. In\nthe end, we deploy the proposed algorithms in two online games of Tencent with\nthe applications of friend recommendation and item recommendation, which\nimprove the competitors by up to \\(91.11%\\) in running time and up to \\(12.80%\\)\nin the corresponding evaluation metrics.</p>\n", "tags": ["KDD","Scalability","Recommender-Systems","Evaluation"] },
{"key": "lin2022deep", "year": "2022", "citations": "17", "title":"Deep Unsupervised Hashing with Latent Semantic Components", "abstract": "<p>Deep unsupervised hashing has been appreciated in the regime of image\nretrieval. However, most prior arts failed to detect the semantic components\nand their relationships behind the images, which makes them lack discriminative\npower. To make up the defect, we propose a novel Deep Semantic Components\nHashing (DSCH), which involves a common sense that an image normally contains a\nbunch of semantic components with homology and co-occurrence relationships.\nBased on this prior, DSCH regards the semantic components as latent variables\nunder the Expectation-Maximization framework and designs a two-step iterative\nalgorithm with the objective of maximum likelihood of training data. Firstly,\nDSCH constructs a semantic component structure by uncovering the fine-grained\nsemantics components of images with a Gaussian Mixture Modal~(GMM), where an\nimage is represented as a mixture of multiple components, and the semantics\nco-occurrence are exploited. Besides, coarse-grained semantics components, are\ndiscovered by considering the homology relationships between fine-grained\ncomponents, and the hierarchy organization is then constructed. Secondly, DSCH\nmakes the images close to their semantic component centers at both fine-grained\nand coarse-grained levels, and also makes the images share similar semantic\ncomponents close to each other. Extensive experiments on three benchmark\ndatasets demonstrate that the proposed hierarchical semantic components indeed\nfacilitate the hashing model to achieve superior performance.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","AAAI","Unsupervised","Evaluation"] },
{"key": "lin2022dense", "year": "2023", "citations": "8", "title":"A Dense Representation Framework for Lexical and Semantic Matching", "abstract": "<p>Lexical and semantic matching capture different successful approaches to text\nretrieval and the fusion of their results has proven to be more effective and\nrobust than either alone. Prior work performs hybrid retrieval by conducting\nlexical and semantic matching using different systems (e.g., Lucene and Faiss,\nrespectively) and then fusing their model outputs. In contrast, our work\nintegrates lexical representations with dense semantic representations by\ndensifying high-dimensional lexical representations into what we call\nlow-dimensional dense lexical representations (DLRs). Our experiments show that\nDLRs can effectively approximate the original lexical representations,\npreserving effectiveness while improving query latency. Furthermore, we can\ncombine dense lexical and semantic representations to generate dense hybrid\nrepresentations (DHRs) that are more flexible and yield faster retrieval\ncompared to existing hybrid techniques. In addition, we explore it jointly\ntraining lexical and semantic representations in a single model and empirically\nshow that the resulting DHRs are able to combine the advantages of the\nindividual components. Our best DHR model is competitive with state-of-the-art\nsingle-vector and multi-vector dense retrievers in both in-domain and zero-shot\nevaluation settings. Furthermore, our model is both faster and requires smaller\nindexes, making our dense representation framework an attractive approach to\ntext retrieval. Our code is available at https://github.com/castorini/dhr.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Evaluation","Text-Retrieval","Tools-&-Libraries"] },
{"key": "lin2025deep", "year": "2015", "citations": "626", "title":"Deep learning of binary hash codes for fast image retrieval", "abstract": "<p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.\nhe utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","CVPR","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Supervised"] },
{"key": "lin2025fast", "year": "2014", "citations": "453", "title":"Fast supervised hashing with decision trees for high-dimensional data", "abstract": "<p>Supervised hashing aims to map the original features to\ncompact binary codes that are able to preserve label based\nsimilarity in the Hamming space. Non-linear hash functions\nhave demonstrated their advantage over linear ones due to\ntheir powerful generalization capability. In the literature,\nkernel functions are typically used to achieve non-linearity\nin hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.\nHere we propose to use boosted decision trees for achieving\nnon-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional\ndata. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem\nand an efficient GraphCut based block search method for\nsolving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the binary\ncodes. Experiments demonstrate that our proposed method\nsignificantly outperforms most state-of-the-art methods in\nretrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster\nthan many methods in terms of training time.</p>\n", "tags": ["Scalability","CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "lin2025general", "year": "2013", "citations": "199", "title":"A General Two-Step Approach to Learning-Based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which\nis typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to\nrespond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose\na flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.\nThis framework allows a number of existing approaches to hashing to be placed in context, and simplifies the\ndevelopment of new problem-specific hashing methods. Our framework decomposes hashing learning problem\ninto two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically\nbe formulated as binary quadratic problems, and the second step can be accomplished by training standard binary\nclassifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate\nthat the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>\n", "tags": ["Hashing-Methods","Tools-&-Libraries","ICCV"] },
{"key": "lin2025optimizing", "year": "2014", "citations": "23", "title":"Optimizing Ranking Measures for Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest—multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.\nThe resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "lin2025semantics", "year": "2015", "citations": "540", "title":"Semantics-Preserving Hashing for Cross-View Retrieval", "abstract": "<p>With benefits of low storage costs and high query speeds,\nhashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.\nIn this paper, we study the problem of cross-view retrieval\nand propose an effective Semantics-Preserving Hashing\nmethod, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them\ninto a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the\nKullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt\nhash codes. And for any unseen instance, predicted hash\ncodes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,\nusing a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>\n", "tags": ["Scalability","Datasets","CVPR","Memory-Efficiency","Hashing-Methods","Evaluation","Supervised"] },
{"key": "lindgren2017leveraging", "year": "2016", "citations": "12", "title":"Leveraging Sparsity for Efficient Submodular Data Summarization", "abstract": "<p>The facility location problem is widely used for summarizing large datasets\nand has additional applications in sensor placement, image retrieval, and\nclustering. One difficulty of this problem is that submodular optimization\nalgorithms require the calculation of pairwise benefits for all items in the\ndataset. This is infeasible for large problems, so recent work proposed to only\ncalculate nearest neighbor benefits. One limitation is that several strong\nassumptions were invoked to obtain provable approximation guarantees. In this\npaper we establish that these extra assumptions are not necessary—solving the\nsparsified problem will be almost optimal under the standard assumptions of the\nproblem. We then analyze a different method of sparsification that is a better\nmodel for methods such as Locality Sensitive Hashing to accelerate the nearest\nneighbor computations and extend the use of the problem to a broader family of\nsimilarities. We validate our approach by demonstrating that it rapidly\ngenerates interpretable summaries.</p>\n", "tags": ["Hashing-Methods","Datasets","Locality-Sensitive-Hashing","Image-Retrieval"] },
{"key": "liong2015deep", "year": "2015", "citations": "579", "title":"Deep Variational and Structural Hashing", "abstract": "<p>In this paper, we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features, we develop a probabilistic framework to infer latent feature representation inside the network. Then, we design a struct layer rather than a bottleneck hash layer, to obtain binary codes through a simple encoding procedure. By doing these, we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval, we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically, we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.</p>\n", "tags": ["Datasets","CVPR","Neural-Hashing","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "liong2017cross", "year": "2017", "citations": "98", "title":"Cross-Modal Deep Variational Hashing", "abstract": "<p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods\nwhich learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural\nnetwork to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent\nvariable as close as possible from the inferred binary codes,\nwhich is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>\n", "tags": ["ICCV","Datasets","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "liong2025cross", "year": "2017", "citations": "98", "title":"Cross-Modal Deep Variational Hashing", "abstract": "<p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods\nwhich learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural\nnetwork to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent\nvariable as close as possible from the inferred binary codes,\nwhich is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>\n", "tags": ["ICCV","Datasets","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "liong2025deep", "year": "2015", "citations": "579", "title":"Deep Hashing for Compact Binary Codes Learning", "abstract": "<p>In this paper, we propose a new deep hashing (DH) approach\nto learn compact binary codes for large scale visual\nsearch. Unlike most existing binary codes learning methods\nwhich seek a single linear projection to map each sample\ninto a binary vector, we develop a deep neural network\nto seek multiple hierarchical non-linear transformations to\nlearn these binary codes, so that the nonlinear relationship\nof samples can be well exploited. Our model is learned under\nthree constraints at the top layer of the deep network:\n1) the loss between the original real-valued feature descriptor\nand the learned binary vector is minimized, 2) the binary\ncodes distribute evenly on each bit, and 3) different bits\nare as independent as possible. To further improve the discriminative\npower of the learned binary codes, we extend\nDH into supervised DH (SDH) by including one discriminative\nterm into the objective function of DH which simultaneously\nmaximizes the inter-class variations and minimizes\nthe intra-class variations of the learned binary codes. Experimental\nresults show the superiority of the proposed approach\nover the state-of-the-arts.</p>\n", "tags": ["CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "liu2011hashing", "year": "2011", "citations": "861", "title":"Hashing with Graphs", "abstract": "<p>Hashing is becoming increasingly popular for\nefficient nearest neighbor search in massive\ndatabases. However, learning short codes\nthat yield good search performance is still\na challenge. Moreover, in many cases realworld\ndata lives on a low-dimensional manifold,\nwhich should be taken into account\nto capture meaningful nearest neighbors. In\nthis paper, we propose a novel graph-based\nhashing method which automatically discovers\nthe neighborhood structure inherent in\nthe data to learn appropriate compact codes.\nTo make such an approach computationally\nfeasible, we utilize Anchor Graphs to obtain\ntractable low-rank adjacency matrices. Our\nformulation allows constant time hashing of a\nnew data point by extrapolating graph Laplacian\neigenvectors to eigenfunctions. Finally,\nwe describe a hierarchical threshold learning\nprocedure in which each eigenfunction yields\nmultiple bits, leading to higher search accuracy.\nExperimental comparison with the\nother state-of-the-art methods on two large\ndatasets demonstrates the efficacy of the proposed\nmethod.</p>\n", "tags": ["Datasets","Compact-Codes","Hashing-Methods","Evaluation","Graph-Based-ANN"] },
{"key": "liu2012supervised", "year": "2012", "citations": "1447", "title":"Supervised Hashing with Kernels", "abstract": "<p>Recent years have witnessed the growing popularity of\nhashing in large-scale vision problems. It has been shown\nthat the hashing quality could be boosted by leveraging supervised\ninformation into hash function learning. However,\nthe existing supervised methods either lack adequate performance\nor often incur cumbersome model training. In this\npaper, we propose a novel kernel-based supervised hashing\nmodel which requires a limited amount of supervised information,\ni.e., similar and dissimilar data pairs, and a feasible\ntraining cost in achieving high quality hashing. The idea\nis to map the data to compact binary codes whose Hamming\ndistances are minimized on similar pairs and simultaneously\nmaximized on dissimilar pairs. Our approach is\ndistinct from prior works by utilizing the equivalence between\noptimizing the code inner products and the Hamming\ndistances. This enables us to sequentially and efficiently\ntrain the hash functions one bit at a time, yielding very\nshort yet discriminative codes. We carry out extensive experiments\non two image benchmarks with up to one million\nsamples, demonstrating that our approach significantly outperforms\nthe state-of-the-arts in searching both metric distance\nneighbors and semantically similar neighbors, with\naccuracy gains ranging from 13% to 46%.</p>\n", "tags": ["Scalability","CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "liu2014collaborative", "year": "2014", "citations": "130", "title":"Collaborative Hashing", "abstract": "<p>Hashing technique has become a promising approach for\nfast similarity search. Most of existing hashing research\npursue the binary codes for the same type of entities by\npreserving their similarities. In practice, there are many\nscenarios involving nearest neighbor search on the data\ngiven in matrix form, where two different types of, yet\nnaturally associated entities respectively correspond to its\ntwo dimensions or views. To fully explore the duality\nbetween the two views, we propose a collaborative hashing\nscheme for the data in matrix form to enable fast search\nin various applications such as image search using bag of\nwords and recommendation using user-item ratings. By\nsimultaneously preserving both the entity similarities in\neach view and the interrelationship between views, our\ncollaborative hashing effectively learns the compact binary\ncodes and the explicit hash functions for out-of-sample\nextension in an alternating optimization way. Extensive\nevaluations are conducted on three well-known datasets\nfor search inside a single view and search across different\nviews, demonstrating that our proposed method outperforms\nstate-of-the-art baselines, with significant accuracy\ngains ranging from 7.67% to 45.87% relatively.</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Compact-Codes","Similarity-Search","Hashing-Methods","Recommender-Systems"] },
{"key": "liu2014discrete", "year": "2014", "citations": "496", "title":"Discrete Graph Hashing", "abstract": "<p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic\ndatabases. In particular, learning based hashing has received considerable\nattention due to its appealing storage and search efficiency. However, the performance\nof most unsupervised learning based hashing methods deteriorates rapidly\nas the hash code length increases. We argue that the degraded performance is due\nto inferior optimization procedures used to achieve discrete binary codes. This\npaper presents a graph-based unsupervised hashing model to preserve the neighborhood\nstructure of massive data in a discrete code space. We cast the graph\nhashing problem into a discrete optimization framework which directly learns the\nbinary codes. A tractable alternating maximization algorithm is then proposed to\nexplicitly deal with the discrete constraints, yielding high-quality codes to well\ncapture the local neighborhoods. Extensive experiments performed on four large\ndatasets with up to one million samples show that our discrete optimization based\ngraph hashing method obtains superior search accuracy over state-of-the-art unsupervised\nhashing methods, especially for longer codes.</p>\n", "tags": ["Efficiency","Datasets","Neural-Hashing","Tools-&-Libraries","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised","Graph-Based-ANN"] },
{"key": "liu2015multi", "year": "2015", "citations": "57", "title":"Multi-View Complementary Hash Tables for Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many\napplications (e.g., visual search, object detection, image\nmatching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.\nHowever, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build\nmultiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.\nIn\nthis paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,\nand learn discriminative hash functions in an efficient way.\nTo build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars\nshared by mis-separated neighbors. Extensive experiments\non three large-scale image datasets demonstrate that the\nproposed method significantly outperforms various naive\nsolutions and state-of-the-art multi-table methods.</p>\n", "tags": ["Image-Retrieval","Scalability","ICCV","Datasets","Tools-&-Libraries","Hashing-Methods"] },
{"key": "liu2016generalized", "year": "2016", "citations": "5", "title":"Generalized residual vector quantization for large scale data", "abstract": "<p>Vector quantization is an essential tool for tasks involving large scale\ndata, for example, large scale similarity search, which is crucial for\ncontent-based information retrieval and analysis. In this paper, we propose a\nnovel vector quantization framework that iteratively minimizes quantization\nerror. First, we provide a detailed review on a relevant vector quantization\nmethod named \\textit{residual vector quantization} (RVQ). Next, we propose\n\\textit{generalized residual vector quantization} (GRVQ) to further improve\nover RVQ. Many vector quantization methods can be viewed as the special cases\nof our proposed framework. We evaluate GRVQ on several large scale benchmark\ndatasets for large scale search, classification and object retrieval. We\ncompared GRVQ with existing methods in detail. Extensive experiments\ndemonstrate our GRVQ framework substantially outperforms existing methods in\nterm of quantization accuracy and computation efficiency.</p>\n", "tags": ["Similarity-Search","Tools-&-Libraries","Datasets","Quantization","Survey-Paper","Evaluation","Efficiency"] },
{"key": "liu2016generating", "year": "2016", "citations": "62", "title":"Generating Binary Tags for Fast Medical Image Retrieval Based on Convolutional Nets and Radon Transform", "abstract": "<p>Content-based image retrieval (CBIR) in large medical image archives is a\nchallenging and necessary task. Generally, different feature extraction methods\nare used to assign expressive and invariant features to each image such that\nthe search for similar images comes down to feature classification and/or\nmatching. The present work introduces a new image retrieval method for medical\napplications that employs a convolutional neural network (CNN) with recently\nintroduced Radon barcodes. We combine neural codes for global classification\nwith Radon barcodes for the final retrieval. We also examine image search based\non regions of interest (ROI) matching after image retrieval. The IRMA dataset\nwith more than 14,000 x-rays images is used to evaluate the performance of our\nmethod. Experimental results show that our approach is superior to many\npublished works.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "liu2016ordinal", "year": "2017", "citations": "23", "title":"Ordinal Constrained Binary Code Learning for Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed extensive attention in binary code learning,\na.k.a. hashing, for nearest neighbor search problems. It has been seen that\nhigh-dimensional data points can be quantized into binary codes to give an\nefficient similarity approximation via Hamming distance. Among existing\nschemes, ranking-based hashing is recent promising that targets at preserving\nordinal relations of ranking in the Hamming space to minimize retrieval loss.\nHowever, the size of the ranking tuples, which shows the ordinal relations, is\nquadratic or cubic to the size of training samples. By given a large-scale\ntraining data set, it is very expensive to embed such ranking tuples in binary\ncode learning. Besides, it remains a dificulty to build ranking tuples\nefficiently for most ranking-preserving hashing, which are deployed over an\nordinal graph-based setting. To handle these problems, we propose a novel\nranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH),\nwhich efficiently learns the optimal hashing functions with a graph-based\napproximation to embed the ordinal relations. The core idea is to reduce the\nsize of ordinal graph with ordinal constraint projection, which preserves the\nordinal relations through a small data set (such as clusters or random\nsamples). In particular, to learn such hash functions effectively, we further\nrelax the discrete constraints and design a specific stochastic gradient decent\nalgorithm for optimization. Experimental results on three large-scale visual\nsearch benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the\nproposed OCH method can achieve superior performance over the state-of-the-arts\napproaches.</p>\n", "tags": ["Graph-Based-ANN","Hashing-Methods","Datasets","Compact-Codes","AAAI","Scalability","Evaluation"] },
{"key": "liu2016supervised", "year": "2016", "citations": "65", "title":"Supervised Matrix Factorization for Cross-Modality Hashing", "abstract": "<p>Matrix factorization has been recently utilized for the task of multi-modal\nhashing for cross-modality visual search, where basis functions are learned to\nmap data from different modalities to the same Hamming embedding. In this\npaper, we propose a novel cross-modality hashing algorithm termed Supervised\nMatrix Factorization Hashing (SMFH) which tackles the multi-modal hashing\nproblem with a collective non-matrix factorization across the different\nmodalities. In particular, SMFH employs a well-designed binary code learning\nalgorithm to preserve the similarities among multi-modal original features\nthrough a graph regularization. At the same time, semantic labels, when\navailable, are incorporated into the learning procedure. We conjecture that all\nthese would facilitate to preserve the most relevant information during the\nbinary quantization process, and hence improve the retrieval accuracy. We\ndemonstrate the superior performance of SMFH on three cross-modality visual\nsearch benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with\nquantitative comparison to various state-of-the-art methods</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Compact-Codes","Quantization","Evaluation"] },
{"key": "liu2017discretely", "year": "2017", "citations": "49", "title":"Discretely Coding Semantic Rank Orders for Supervised Image Hashing", "abstract": "<p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Supervised"] },
{"key": "liu2017end", "year": "2017", "citations": "7", "title":"End-to-end Binary Representation Learning via Direct Binary Embedding", "abstract": "<p>Learning binary representation is essential to large-scale computer vision\ntasks. Most existing algorithms require a separate quantization constraint to\nlearn effective hashing functions. In this work, we present Direct Binary\nEmbedding (DBE), a simple yet very effective algorithm to learn binary\nrepresentation in an end-to-end fashion. By appending an ingeniously designed\nDBE layer to the deep convolutional neural network (DCNN), DBE learns binary\ncode directly from the continuous DBE layer activation without quantization\nerror. By employing the deep residual network (ResNet) as DCNN component, DBE\ncaptures rich semantics from images. Furthermore, in the effort of handling\nmultilabel images, we design a joint cross entropy loss that includes both\nsoftmax cross entropy and weighted binary cross entropy in consideration of the\ncorrelation and independence of labels, respectively. Extensive experiments\ndemonstrate the significant superiority of DBE over state-of-the-art methods on\ntasks of natural object recognition, image retrieval and image annotation.</p>\n", "tags": ["Hashing-Methods","Scalability","Quantization","Image-Retrieval"] },
{"key": "liu2018adversarial", "year": "2019", "citations": "36", "title":"Adversarial Binary Coding for Efficient Person Re-identification", "abstract": "<p>Person re-identification (ReID) aims at matching persons across different\nviews/scenes. In addition to accuracy, the matching efficiency has received\nmore and more attention because of demanding applications using large-scale\ndata. Several binary coding based methods have been proposed for efficient\nReID, which either learn projections to map high-dimensional features to\ncompact binary codes, or directly adopt deep neural networks by simply\ninserting an additional fully-connected layer with tanh-like activations.\nHowever, the former approach requires time-consuming hand-crafted feature\nextraction and complicated (discrete) optimizations; the latter lacks the\nnecessary discriminative information greatly due to the straightforward\nactivation functions. In this paper, we propose a simple yet effective\nframework for efficient ReID inspired by the recent advances in adversarial\nlearning. Specifically, instead of learning explicit projections or adding\nfully-connected mapping layers, the proposed Adversarial Binary Coding (ABC)\nframework guides the extraction of binary codes implicitly and effectively. The\ndiscriminability of the extracted codes is further enhanced by equipping the\nABC with a deep triplet network for the ReID task. More importantly, the ABC\nand triplet network are simultaneously optimized in an end-to-end manner.\nExtensive experiments on three large-scale ReID benchmarks demonstrate the\nsuperiority of our approach over the state-of-the-art methods.</p>\n", "tags": ["Tools-&-Libraries","Compact-Codes","Robustness","Scalability","Evaluation","Efficiency"] },
{"key": "liu2018discriminative", "year": "2018", "citations": "5", "title":"Discriminative Cross-View Binary Representation Learning", "abstract": "<p>Learning compact representation is vital and challenging for large scale\nmultimedia data. Cross-view/cross-modal hashing for effective binary\nrepresentation learning has received significant attention with exponentially\ngrowing availability of multimedia content. Most existing cross-view hashing\nalgorithms emphasize the similarities in individual views, which are then\nconnected via cross-view similarities. In this work, we focus on the\nexploitation of the discriminative information from different views, and\npropose an end-to-end method to learn semantic-preserving and discriminative\nbinary representation, dubbed Discriminative Cross-View Hashing (DCVH), in\nlight of learning multitasking binary representation for various tasks\nincluding cross-view retrieval, image-to-image retrieval, and image\nannotation/tagging. The proposed DCVH has the following key components. First,\nit uses convolutional neural network (CNN) based nonlinear hashing functions\nand multilabel classification for both images and texts simultaneously. Such\nhashing functions achieve effective continuous relaxation during training\nwithout explicit quantization loss by using Direct Binary Embedding (DBE)\nlayers. Second, we propose an effective view alignment via Hamming distance\nminimization, which is efficiently accomplished by bit-wise XOR operation.\nExtensive experiments on two image-text benchmark datasets demonstrate that\nDCVH outperforms state-of-the-art cross-view hashing algorithms as well as\nsingle-view image hashing algorithms. In addition, DCVH can provide competitive\nperformance for image annotation/tagging.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Quantization","Evaluation"] },
{"key": "liu2018stochastic", "year": "2019", "citations": "96", "title":"Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization", "abstract": "<p>This paper tackles the problem of large-scale image-based localization (IBL)\nwhere the spatial location of a query image is determined by finding out the\nmost similar reference images in a large database. For solving this problem, a\ncritical task is to learn discriminative image representation that captures\ninformative information relevant for localization. We propose a novel\nrepresentation learning method having higher location-discriminating power. It\nprovides the following contributions: 1) we represent a place (location) as a\nset of exemplar images depicting the same landmarks and aim to maximize\nsimilarities among intra-place images while minimizing similarities among\ninter-place images; 2) we model a similarity measure as a probability\ndistribution on L_2-metric distances between intra-place and inter-place image\nrepresentations; 3) we propose a new Stochastic Attraction and Repulsion\nEmbedding (SARE) loss function minimizing the KL divergence between the learned\nand the actual probability distributions; 4) we give theoretical comparisons\nbetween SARE, triplet ranking and contrastive losses. It provides insights into\nwhy SARE is better by analyzing gradients. Our SARE loss is easy to implement\nand pluggable to any CNN. Experiments show that our proposed method improves\nthe localization performance on standard benchmarks by a large margin.\nDemonstrating the broad applicability of our method, we obtained the third\nplace out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our\ncode and model are available at https://github.com/Liumouliu/deepIBL.</p>\n", "tags": ["ICCV","Distance-Metric-Learning","Scalability","Evaluation"] },
{"key": "liu2019compositional", "year": "2019", "citations": "15", "title":"Compositional Coding for Collaborative Filtering", "abstract": "<p>Efficiency is crucial to the online recommender systems. Representing users\nand items as binary vectors for Collaborative Filtering (CF) can achieve fast\nuser-item affinity computation in the Hamming space, in recent years, we have\nwitnessed an emerging research effort in exploiting binary hashing techniques\nfor CF methods. However, CF with binary codes naturally suffers from low\naccuracy due to limited representation capability in each bit, which impedes it\nfrom modeling complex structure of the data.\n  In this work, we attempt to improve the efficiency without hurting the model\nperformance by utilizing both the accuracy of real-valued vectors and the\nefficiency of binary codes to represent users/items. In particular, we propose\nthe Compositional Coding for Collaborative Filtering (CCCF) framework, which\nnot only gains better recommendation efficiency than the state-of-the-art\nbinarized CF approaches but also achieves even higher accuracy than the\nreal-valued CF method. Specifically, CCCF innovatively represents each\nuser/item with a set of binary vectors, which are associated with a sparse\nreal-value weight vector. Each value of the weight vector encodes the\nimportance of the corresponding binary vector to the user/item. The continuous\nweight vectors greatly enhances the representation capability of binary codes,\nand its sparsity guarantees the processing speed. Furthermore, an integer\nweight approximation scheme is proposed to further accelerate the speed. Based\non the CCCF framework, we design an efficient discrete optimization algorithm\nto learn its parameters. Extensive experiments on three real-world datasets\nshow that our method outperforms the state-of-the-art binarized CF methods\n(even achieves better performance than the real-valued CF method) by a large\nmargin in terms of both recommendation accuracy and efficiency.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Recommender-Systems","SIGIR","Compact-Codes","Evaluation","Efficiency"] },
{"key": "liu2019cross", "year": "2019", "citations": "16", "title":"Cross-modal Zero-shot Hashing", "abstract": "<p>Hashing has been widely studied for big data retrieval due to its low storage\ncost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing\nmodel that is trained using only samples from seen categories, but can\ngeneralize well to samples of unseen categories. ZSH generally uses category\nattributes to seek a semantic embedding space to transfer knowledge from seen\ncategories to unseen ones. As a result, it may perform poorly when labeled data\nare insufficient. ZSH methods are mainly designed for single-modality data,\nwhich prevents their application to the widely spread multi-modal data. On the\nother hand, existing cross-modal hashing solutions assume that all the\nmodalities share the same category labels, while in practice the labels of\ndifferent data modalities may be different. To address these issues, we propose\na general Cross-modal Zero-shot Hashing (CZHash) solution to effectively\nleverage unlabeled and labeled multi-modality data with different label spaces.\nCZHash first quantifies the composite similarity between instances using label\nand feature information. It then defines an objective function to achieve deep\nfeature learning compatible with the composite similarity preserving, category\nattribute space learning, and hashing coding function learning. CZHash further\nintroduces an alternative optimization procedure to jointly optimize these\nlearning objectives. Experiments on benchmark multi-modal datasets show that\nCZHash significantly outperforms related representative hashing approaches both\non effectiveness and adaptability.</p>\n", "tags": ["Hashing-Methods","Datasets","Few-Shot-&-Zero-Shot","Evaluation"] },
{"key": "liu2019deep", "year": "2018", "citations": "106", "title":"Deep Triplet Quantization", "abstract": "<p>Deep hashing establishes efficient and effective image retrieval by\nend-to-end learning of deep representations and hash codes from similarity\ndata. We present a compact coding solution, focusing on deep learning to\nquantization approach that has shown superior performance over hashing\nsolutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ),\na novel approach to learning deep quantization models from the similarity\ntriplets. To enable more effective triplet training, we design a new triplet\nselection approach, Group Hard, that randomly selects hard triplets in each\nimage group. To generate compact binary codes, we further apply a triplet\nquantization with weak orthogonality during triplet training. The quantization\nloss reduces the codebook redundancy and enhances the quantizability of deep\nrepresentations through back-propagation. Extensive experiments demonstrate\nthat DTQ can generate high-quality and compact binary codes, which yields\nstate-of-the-art image retrieval performance on three benchmark datasets,\nNUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["Similarity-Search","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Quantization","Evaluation"] },
{"key": "liu2019optimal", "year": "2019", "citations": "31", "title":"Optimal Projection Guided Transfer Hashing for Image Retrieval", "abstract": "<p>Recently, learning to hash has been widely studied for image retrieval thanks\nto the computation and storage efficiency of binary codes. For most existing\nlearning to hash methods, sufficient training images are required and used to\nlearn precise hashing codes. However, in some real-world applications, there\nare not always sufficient training images in the domain of interest. In\naddition, some existing supervised approaches need a amount of labeled data,\nwhich is an expensive process in term of time, label and human expertise. To\nhandle such problems, inspired by transfer learning, we propose a simple yet\neffective unsupervised hashing method named Optimal Projection Guided Transfer\nHashing (GTH) where we borrow the images of other different but related domain\ni.e., source domain to help learn precise hashing codes for the domain of\ninterest i.e., target domain. Besides, we propose to seek for the maximum\nlikelihood estimation (MLE) solution of the hashing functions of target and\nsource domains due to the domain gap. Furthermore,an alternating optimization\nmethod is adopted to obtain the two projections of target and source domains\nsuch that the domain hashing disparity is reduced gradually. Extensive\nexperiments on various benchmark databases verify that our method outperforms\nmany state-of-the-art learning to hash methods. The implementation details are\navailable at https://github.com/liuji93/GTH.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Neural-Hashing","Compact-Codes","Unsupervised","Evaluation","Efficiency"] },
{"key": "liu2019query", "year": "2016", "citations": "74", "title":"Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual Search", "abstract": "<p>Hash based nearest neighbor search has become attractive in many\napplications. However, the quantization in hashing usually degenerates the\ndiscriminative power when using Hamming distance ranking. Besides, for\nlarge-scale visual search, existing hashing methods cannot directly support the\nefficient search over the data with multiple sources, and while the literature\nhas shown that adaptively incorporating complementary information from diverse\nsources or views can significantly boost the search performance. To address the\nproblems, this paper proposes a novel and generic approach to building multiple\nhash tables with multiple views and generating fine-grained ranking results at\nbitwise and tablewise levels. For each hash table, a query-adaptive bitwise\nweighting is introduced to alleviate the quantization loss by simultaneously\nexploiting the quality of hash functions and their complement for nearest\nneighbor search. From the tablewise aspect, multiple hash tables are built for\ndifferent data views as a joint index, over which a query-specific rank fusion\nis proposed to rerank all results from the bitwise ranking by diffusing in a\ngraph. Comprehensive experiments on image search over three well-known\nbenchmarks show that the proposed method achieves up to 17.11% and 20.28%\nperformance gains on single and multiple table search over state-of-the-art\nmethods.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Quantization","Re-Ranking","Scalability","Evaluation"] },
{"key": "liu2019ranking", "year": "2019", "citations": "59", "title":"Ranking-based Deep Cross-modal Hashing", "abstract": "<p>Cross-modal hashing has been receiving increasing interests for its low\nstorage cost and fast query speed in multi-modal data retrievals. However, most\nexisting hashing methods are based on hand-crafted or raw level features of\nobjects, which may not be optimally compatible with the coding process.\nBesides, these hashing methods are mainly designed to handle simple pairwise\nsimilarity. The complex multilevel ranking semantic structure of instances\nassociated with multiple labels has not been well explored yet. In this paper,\nwe propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH\nfirstly uses the feature and label information of data to derive a\nsemi-supervised semantic ranking list. Next, to expand the semantic\nrepresentation power of hand-crafted features, RDCMH integrates the semantic\nranking information into deep cross-modal hashing and jointly optimizes the\ncompatible parameters of deep feature representations and of hashing functions.\nExperiments on real multi-modal datasets show that RDCMH outperforms other\ncompetitive baselines and achieves the state-of-the-art performance in\ncross-modal retrieval applications.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Memory-Efficiency","AAAI","Evaluation","Multimodal-Retrieval"] },
{"key": "liu2019strong", "year": "2019", "citations": "13", "title":"A Strong and Robust Baseline for Text-Image Matching", "abstract": "<p>We review the current schemes of text-image matching models and propose\nimprovements for both training and inference. First, we empirically show\nlimitations of two popular loss (sum and max-margin loss) widely used in\ntraining text-image embeddings and propose a trade-off: a kNN-margin loss which\n1) utilizes information from hard negatives and 2) is robust to noise as all\n\\(K\\)-most hardest samples are taken into account, tolerating <em>pseudo</em>\nnegatives and outliers. Second, we advocate the use of Inverted Softmax\n(\\textsc{Is}) and Cross-modal Local Scaling (\\textsc{Csls}) during inference to\nmitigate the so-called hubness problem in high-dimensional embedding space,\nenhancing scores of all metrics by a large margin.</p>\n", "tags": ["Survey-Paper"] },
{"key": "liu2020joint", "year": "2020", "citations": "148", "title":"Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval", "abstract": "<p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>\n", "tags": ["Scalability","Efficiency","Datasets","Multimodal-Retrieval","SIGIR","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "liu2020model", "year": "2020", "citations": "23", "title":"Model Optimization Boosting Framework for Linear Model Hash Learning", "abstract": "<p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>\n", "tags": ["Efficiency","Datasets","Tools-&-Libraries","Hashing-Methods","Evaluation"] },
{"key": "liu2020neuromorphic", "year": "2022", "citations": "15", "title":"Neuromorphic Computing for Content-based Image Retrieval", "abstract": "<p>Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in content-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and\n12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a\nlightweight convolutional neural network without batching while maintaining the\nsame level of matching accuracy. The study validates the potential of\nneuromorphic computing in low-power image retrieval, as a complementary\nparadigm to the existing von Neumann architectures.</p>\n", "tags": ["Image-Retrieval","Recommender-Systems","Evaluation","Efficiency"] },
{"key": "liu2020reinforcing", "year": "2020", "citations": "28", "title":"Reinforcing Short-Length Hashing", "abstract": "<p>Due to the compelling efficiency in retrieval and storage,\nsimilarity-preserving hashing has been widely applied to approximate nearest\nneighbor search in large-scale image retrieval. However, existing methods have\npoor performance in retrieval using an extremely short-length hash code due to\nweak ability of classification and poor distribution of hash bit. To address\nthis issue, in this study, we propose a novel reinforcing short-length hashing\n(RSLH). In this proposed RSLH, mutual reconstruction between the hash\nrepresentation and semantic labels is performed to preserve the semantic\ninformation. Furthermore, to enhance the accuracy of hash representation, a\npairwise similarity matrix is designed to make a balance between accuracy and\ntraining expenditure on memory. In addition, a parameter boosting strategy is\nintegrated to reinforce the precision with hash bits fusion. Extensive\nexperiments on three large-scale image benchmarks demonstrate the superior\nperformance of RSLH under various short-length hashing scenarios.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Scalability","Evaluation","Efficiency"] },
{"key": "liu2021image", "year": "2021", "citations": "108", "title":"Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models", "abstract": "<p>We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&amp;L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.</p>\n", "tags": ["ICCV","Datasets","Image-Retrieval"] },
{"key": "liu2022towards", "year": "2022", "citations": "7", "title":"Towards Fast and Accurate Federated Learning with non-IID Data for Cloud-Based IoT Applications", "abstract": "<p>As a promising method of central model training on decentralized device data\nwhile securing user privacy, Federated Learning (FL)is becoming popular in\nInternet of Things (IoT) design. However, when the data collected by IoT\ndevices are highly skewed in a non-independent and identically distributed\n(non-IID) manner, the accuracy of vanilla FL method cannot be guaranteed.\nAlthough there exist various solutions that try to address the bottleneck of FL\nwith non-IID data, most of them suffer from extra intolerable communication\noverhead and low model accuracy. To enable fast and accurate FL, this paper\nproposes a novel data-based device grouping approach that can effectively\nreduce the disadvantages of weight divergence during the training of non-IID\ndata. However, since our grouping method is based on the similarity of\nextracted feature maps from IoT devices, it may incur additional risks of\nprivacy exposure. To solve this problem, we propose an improved version by\nexploiting similarity information using the Locality-Sensitive Hashing (LSH)\nalgorithm without exposing extracted feature maps. Comprehensive experimental\nresults on well-known benchmarks show that our approach can not only accelerate\nthe convergence rate, but also improve the prediction accuracy for FL with\nnon-IID data.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing"] },
{"key": "liu2023bi", "year": "2024", "citations": "13", "title":"Bi-directional Training for Composed Image Retrieval via Text Prompt Learning", "abstract": "<p>Composed image retrieval searches for a target image based on a multi-modal\nuser query comprised of a reference image and modification text describing the\ndesired changes. Existing approaches to solving this challenging task learn a\nmapping from the (reference image, modification text)-pair to an image\nembedding that is then matched against a large image corpus. One area that has\nnot yet been explored is the reverse direction, which asks the question, what\nreference image when modified as described by the text would produce the given\ntarget image? In this work we propose a bi-directional training scheme that\nleverages such reversed queries and can be applied to existing composed image\nretrieval architectures with minimum changes, which improves the performance of\nthe model. To encode the bi-directional query we prepend a learnable token to\nthe modification text that designates the direction of the query and then\nfinetune the parameters of the text embedding module. We make no other changes\nto the network architecture. Experiments on two standard datasets show that our\nnovel approach achieves improved performance over a baseline BLIP-based model\nthat itself already achieves competitive performance. Our code is released at\nhttps://github.com/Cuberick-Orion/Bi-Blip4CIR.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "liu2025collaborative", "year": "2014", "citations": "130", "title":"Collaborative Hashing", "abstract": "<p>Hashing technique has become a promising approach for\nfast similarity search. Most of existing hashing research\npursue the binary codes for the same type of entities by\npreserving their similarities. In practice, there are many\nscenarios involving nearest neighbor search on the data\ngiven in matrix form, where two different types of, yet\nnaturally associated entities respectively correspond to its\ntwo dimensions or views. To fully explore the duality\nbetween the two views, we propose a collaborative hashing\nscheme for the data in matrix form to enable fast search\nin various applications such as image search using bag of\nwords and recommendation using user-item ratings. By\nsimultaneously preserving both the entity similarities in\neach view and the interrelationship between views, our\ncollaborative hashing effectively learns the compact binary\ncodes and the explicit hash functions for out-of-sample\nextension in an alternating optimization way. Extensive\nevaluations are conducted on three well-known datasets\nfor search inside a single view and search across different\nviews, demonstrating that our proposed method outperforms\nstate-of-the-art baselines, with significant accuracy\ngains ranging from 7.67% to 45.87% relatively.</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Compact-Codes","Similarity-Search","Hashing-Methods","Recommender-Systems"] },
{"key": "liu2025discrete", "year": "2014", "citations": "496", "title":"Discrete Graph Hashing", "abstract": "<p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic\ndatabases. In particular, learning based hashing has received considerable\nattention due to its appealing storage and search efficiency. However, the performance\nof most unsupervised learning based hashing methods deteriorates rapidly\nas the hash code length increases. We argue that the degraded performance is due\nto inferior optimization procedures used to achieve discrete binary codes. This\npaper presents a graph-based unsupervised hashing model to preserve the neighborhood\nstructure of massive data in a discrete code space. We cast the graph\nhashing problem into a discrete optimization framework which directly learns the\nbinary codes. A tractable alternating maximization algorithm is then proposed to\nexplicitly deal with the discrete constraints, yielding high-quality codes to well\ncapture the local neighborhoods. Extensive experiments performed on four large\ndatasets with up to one million samples show that our discrete optimization based\ngraph hashing method obtains superior search accuracy over state-of-the-art unsupervised\nhashing methods, especially for longer codes.</p>\n", "tags": ["Efficiency","Datasets","Neural-Hashing","Tools-&-Libraries","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised","Graph-Based-ANN"] },
{"key": "liu2025discretely", "year": "2017", "citations": "49", "title":"Discretely Coding Semantic Rank Orders for Supervised Image Hashing", "abstract": "<p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Supervised"] },
{"key": "liu2025hashing", "year": "2011", "citations": "861", "title":"Hashing with Graphs", "abstract": "<p>Hashing is becoming increasingly popular for\nefficient nearest neighbor search in massive\ndatabases. However, learning short codes\nthat yield good search performance is still\na challenge. Moreover, in many cases realworld\ndata lives on a low-dimensional manifold,\nwhich should be taken into account\nto capture meaningful nearest neighbors. In\nthis paper, we propose a novel graph-based\nhashing method which automatically discovers\nthe neighborhood structure inherent in\nthe data to learn appropriate compact codes.\nTo make such an approach computationally\nfeasible, we utilize Anchor Graphs to obtain\ntractable low-rank adjacency matrices. Our\nformulation allows constant time hashing of a\nnew data point by extrapolating graph Laplacian\neigenvectors to eigenfunctions. Finally,\nwe describe a hierarchical threshold learning\nprocedure in which each eigenfunction yields\nmultiple bits, leading to higher search accuracy.\nExperimental comparison with the\nother state-of-the-art methods on two large\ndatasets demonstrates the efficacy of the proposed\nmethod.</p>\n", "tags": ["Datasets","Compact-Codes","Hashing-Methods","Evaluation","Graph-Based-ANN"] },
{"key": "liu2025joint", "year": "2020", "citations": "148", "title":"Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval", "abstract": "<p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>\n", "tags": ["Scalability","Efficiency","Datasets","Multimodal-Retrieval","SIGIR","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "liu2025model", "year": "2020", "citations": "23", "title":"Model Optimization Boosting Framework for Linear Model Hash Learning", "abstract": "<p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>\n", "tags": ["Efficiency","Datasets","Tools-&-Libraries","Hashing-Methods","Evaluation"] },
{"key": "liu2025multi", "year": "2015", "citations": "57", "title":"Multi-View Complementary Hash Tables for Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many\napplications (e.g., visual search, object detection, image\nmatching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.\nHowever, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build\nmultiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.\nIn\nthis paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,\nand learn discriminative hash functions in an efficient way.\nTo build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars\nshared by mis-separated neighbors. Extensive experiments\non three large-scale image datasets demonstrate that the\nproposed method significantly outperforms various naive\nsolutions and state-of-the-art multi-table methods.</p>\n", "tags": ["Image-Retrieval","Scalability","ICCV","Datasets","Tools-&-Libraries","Hashing-Methods"] },
{"key": "liu2025supervised", "year": "2012", "citations": "1447", "title":"Supervised Hashing with Kernels", "abstract": "<p>Recent years have witnessed the growing popularity of\nhashing in large-scale vision problems. It has been shown\nthat the hashing quality could be boosted by leveraging supervised\ninformation into hash function learning. However,\nthe existing supervised methods either lack adequate performance\nor often incur cumbersome model training. In this\npaper, we propose a novel kernel-based supervised hashing\nmodel which requires a limited amount of supervised information,\ni.e., similar and dissimilar data pairs, and a feasible\ntraining cost in achieving high quality hashing. The idea\nis to map the data to compact binary codes whose Hamming\ndistances are minimized on similar pairs and simultaneously\nmaximized on dissimilar pairs. Our approach is\ndistinct from prior works by utilizing the equivalence between\noptimizing the code inner products and the Hamming\ndistances. This enables us to sequentially and efficiently\ntrain the hash functions one bit at a time, yielding very\nshort yet discriminative codes. We carry out extensive experiments\non two image benchmarks with up to one million\nsamples, demonstrating that our approach significantly outperforms\nthe state-of-the-arts in searching both metric distance\nneighbors and semantically similar neighbors, with\naccuracy gains ranging from 13% to 46%.</p>\n", "tags": ["Scalability","CVPR","Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "long2018deep", "year": "2018", "citations": "22", "title":"Deep Domain Adaptation Hashing with Adversarial Learning", "abstract": "<p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>\n", "tags": ["Efficiency","Tools-&-Libraries","SIGIR","Hashing-Methods","Robustness"] },
{"key": "long2025deep", "year": "2018", "citations": "22", "title":"Deep Domain Adaptation Hashing with Adversarial Learning", "abstract": "<p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>\n", "tags": ["Efficiency","Tools-&-Libraries","SIGIR","Hashing-Methods","Robustness"] },
{"key": "lu2018domain", "year": "2021", "citations": "12", "title":"Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax", "abstract": "<p>This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),\nfor which the key is to bridge the gap between sketches and photos in terms of\nthe data representation. Inspired by channel-wise attention explored in recent\nyears, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which\nseamlessly incorporates the prior knowledge of sample sketch or photo into SE\nmodule and make the SE module capable of emphasizing appropriate channels\naccording to domain signal. Accordingly, the proposed network can switch its\nmode to achieve a better domain feature with lower intra-class discrepancy.\nMoreover, while previous works simply focus on minimizing intra-class distance\nand maximizing inter-class distance, we introduce a loss function, named\nMultiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative\nEuclidean margin into feature space and ensure that the maximum intra-class\ndistance is smaller than the minimum inter-class distance. This facilitates\nlearning a highly discriminative feature space and ensures a more accurate\nimage retrieval result. Extensive experiments are conducted on two widely used\nSBIR benchmark datasets. Our approach achieves better results on both datasets,\nsurpassing the state-of-the-art methods by a large margin.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "lu2019online", "year": "2019", "citations": "128", "title":"Online Multi-modal Hashing with Dynamic Query-adaption", "abstract": "<p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>\n", "tags": ["Scalability","Memory-Efficiency","Tools-&-Libraries","SIGIR","Supervised","Compact-Codes","Quantization","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "lu2021learnable", "year": "2022", "citations": "23", "title":"Learnable Locality-Sensitive Hashing for Video Anomaly Detection", "abstract": "<p>Video anomaly detection (VAD) mainly refers to identifying anomalous events\nthat have not occurred in the training set where only normal samples are\navailable. Existing works usually formulate VAD as a reconstruction or\nprediction problem. However, the adaptability and scalability of these methods\nare limited. In this paper, we propose a novel distance-based VAD method to\ntake advantage of all the available normal data efficiently and flexibly. In\nour method, the smaller the distance between a testing sample and normal\nsamples, the higher the probability that the testing sample is normal.\nSpecifically, we propose to use locality-sensitive hashing (LSH) to map samples\nwhose similarity exceeds a certain threshold into the same bucket in advance.\nIn this manner, the complexity of near neighbor search is cut down\nsignificantly. To make the samples that are semantically similar get closer and\nsamples not similar get further apart, we propose a novel learnable version of\nLSH that embeds LSH into a neural network and optimizes the hash functions with\ncontrastive learning strategy. The proposed method is robust to data imbalance\nand can handle the large intra-class variations in normal data flexibly.\nBesides, it has a good ability of scalability. Extensive experiments\ndemonstrate the superiority of our method, which achieves new state-of-the-art\nresults on VAD benchmarks.</p>\n", "tags": ["Locality-Sensitive-Hashing","Hashing-Methods","Self-Supervised","Scalability","Evaluation"] },
{"key": "lu2022asymmetric", "year": "2023", "citations": "8", "title":"Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning", "abstract": "<p>Thanks to the efficient retrieval speed and low storage consumption, learning\nto hash has been widely used in visual retrieval tasks. However, existing\nhashing methods assume that the query and retrieval samples lie in homogeneous\nfeature space within the same domain. As a result, they cannot be directly\napplied to heterogeneous cross-domain retrieval. In this paper, we propose a\nGeneralized Image Transfer Retrieval (GITR) problem, which encounters two\ncrucial bottlenecks: 1) the query and retrieval samples may come from different\ndomains, leading to an inevitable {domain distribution gap}; 2) the features of\nthe two domains may be heterogeneous or misaligned, bringing up an additional\n{feature gap}. To address the GITR problem, we propose an Asymmetric Transfer\nHashing (ATH) framework with its unsupervised/semi-supervised/supervised\nrealizations. Specifically, ATH characterizes the domain distribution gap by\nthe discrepancy between two asymmetric hash functions, and minimizes the\nfeature gap with the help of a novel adaptive bipartite graph constructed on\ncross-domain data. By jointly optimizing asymmetric hash functions and the\nbipartite graph, not only can knowledge transfer be achieved but information\nloss caused by feature alignment can also be avoided. Meanwhile, to alleviate\nnegative transfer, the intrinsic geometrical structure of single-domain data is\npreserved by involving a domain affinity graph. Extensive experiments on both\nsingle-domain and cross-domain benchmarks under different GITR subtasks\nindicate the superiority of our ATH method in comparison with the\nstate-of-the-art hashing methods.</p>\n", "tags": ["Similarity-Search","Supervised","Tools-&-Libraries","Hashing-Methods","Unsupervised","Evaluation"] },
{"key": "lu2023attributes", "year": "2023", "citations": "10", "title":"Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval", "abstract": "<p>In recent years, hashing methods have been popular in the large-scale media\nsearch for low storage and strong representation capabilities. To describe\nobjects with similar overall appearance but subtle differences, more and more\nstudies focus on hashing-based fine-grained image retrieval. Existing hashing\nnetworks usually generate both local and global features through attention\nguidance on the same deep activation tensor, which limits the diversity of\nfeature representations. To handle this limitation, we substitute convolutional\ndescriptors for attention-guided features and propose an Attributes Grouping\nand Mining Hashing (AGMH), which groups and embeds the category-specific visual\nattributes in multiple descriptors to generate a comprehensive feature\nrepresentation for efficient fine-grained image retrieval. Specifically, an\nAttention Dispersion Loss (ADL) is designed to force the descriptors to attend\nto various local regions and capture diverse subtle details. Moreover, we\npropose a Stepwise Interactive External Attention (SIEA) to mine critical\nattributes in each descriptor and construct correlations between fine-grained\nattributes and objects. The attention mechanism is dedicated to learning\ndiscrete attributes, which will not cost additional computations in hash codes\ngeneration. Finally, the compact binary codes are learned by preserving\npairwise similarities. Experimental results demonstrate that AGMH consistently\nyields the best performance against state-of-the-art methods on fine-grained\nbenchmark datasets.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Compact-Codes","Scalability","Evaluation"] },
{"key": "lu2025online", "year": "2019", "citations": "128", "title":"Online Multi-modal Hashing with Dynamic Query-adaption", "abstract": "<p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>\n", "tags": ["Scalability","Memory-Efficiency","Tools-&-Libraries","SIGIR","Supervised","Compact-Codes","Quantization","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "luo2018collaborative", "year": "2020", "citations": "14", "title":"Collaborative Learning for Extremely Low Bit Asymmetric Hashing", "abstract": "<p>Hashing techniques are in great demand for a wide range of real-world\napplications such as image retrieval and network compression. Nevertheless,\nexisting approaches could hardly guarantee a satisfactory performance with the\nextremely low-bit (e.g., 4-bit) hash codes due to the severe information loss\nand the shrink of the discrete solution space. In this paper, we propose a\nnovel \\textit{Collaborative Learning} strategy that is tailored for generating\nhigh-quality low-bit hash codes. The core idea is to jointly distill\nbit-specific and informative representations for a group of pre-defined code\nlengths. The learning of short hash codes among the group can benefit from the\nmanifold shared with other long codes, where multiple views from different hash\ncodes provide the supplementary guidance and regularization, making the\nconvergence faster and more stable. To achieve that, an asymmetric hashing\nframework with two variants of multi-head embedding structures is derived,\ntermed as Multi-head Asymmetric Hashing (MAH), leading to great efficiency of\ntraining and querying. Extensive experiments on three benchmark datasets have\nbeen conducted to verify the superiority of the proposed MAH, and have shown\nthat the 8-bit hash codes generated by MAH achieve \\(94.3%\\) of the MAP (Mean\nAverage Precision (MAP)) score on the CIFAR-10 dataset, which significantly\nsurpasses the performance of the 48-bit codes by the state-of-the-arts in image\nretrieval tasks.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Evaluation","Efficiency"] },
{"key": "luo2018fast", "year": "2018", "citations": "89", "title":"Fast Scalable Supervised Hashing", "abstract": "<p>Despite significant progress in supervised hashing, there are three\ncommon limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning\nprocedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply\nsampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods\ntry to replace the large matrix with a smaller one, but the size is\nstill large. Third, among the methods that leverage the pairwise\nsimilarity matrix, most of them only encode the semantic label\ninformation in learning the hash codes, failing to fully capture\nthe characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing\n(FSSH), which circumvents the use of the large similarity matrix by\nintroducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn\nthe hash codes with not only the semantic information but also\nthe features of data. Extensive experiments on three widely used\ndatasets demonstrate its superiority over several state-of-the-art\nmethods in both accuracy and scalability. Our experiment codes\nare available at: https://lcbwlx.wixsite.com/fssh.</p>\n", "tags": ["Scalability","Datasets","Neural-Hashing","SIGIR","Hashing-Methods","Evaluation","Supervised"] },
{"key": "luo2020survey", "year": "2022", "citations": "123", "title":"A Survey on Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims to obtain the samples in the database with the\nsmallest distances from them to the queries, which is a basic task in a range\nof fields, including computer vision and data mining. Hashing is one of the\nmost widely used methods for its computational and storage efficiency. With the\ndevelopment of deep learning, deep hashing methods show more advantages than\ntraditional methods. In this survey, we detailedly investigate current deep\nhashing algorithms including deep supervised hashing and deep unsupervised\nhashing. Specifically, we categorize deep supervised hashing methods into\npairwise methods, ranking-based methods, pointwise methods as well as\nquantization according to how measuring the similarities of the learned hash\ncodes. Moreover, deep unsupervised hashing is categorized into similarity\nreconstruction-based methods, pseudo-label-based methods and prediction-free\nself-supervised learning-based methods based on their semantic learning\nmanners. We also introduce three related important topics including\nsemi-supervised deep hashing, domain adaption deep hashing and multi-modal deep\nhashing. Meanwhile, we present some commonly used public datasets and the\nscheme to measure the performance of deep hashing algorithms. Finally, we\ndiscuss some potential research directions in conclusion.</p>\n", "tags": ["Unsupervised","Supervised","Hashing-Methods","Datasets","Neural-Hashing","Self-Supervised","Quantization","Survey-Paper","Evaluation","Efficiency"] },
{"key": "luo2022survey", "year": "2022", "citations": "123", "title":"A Survey on Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>\n", "tags": ["Efficiency","Unsupervised","Survey-Paper","Datasets","Neural-Hashing","Self-Supervised","Evaluation","Quantization","Hashing-Methods","Supervised"] },
{"key": "luo2025fast", "year": "2018", "citations": "89", "title":"Fast Scalable Supervised Hashing", "abstract": "<p>Despite significant progress in supervised hashing, there are three\ncommon limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning\nprocedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply\nsampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods\ntry to replace the large matrix with a smaller one, but the size is\nstill large. Third, among the methods that leverage the pairwise\nsimilarity matrix, most of them only encode the semantic label\ninformation in learning the hash codes, failing to fully capture\nthe characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing\n(FSSH), which circumvents the use of the large similarity matrix by\nintroducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn\nthe hash codes with not only the semantic information but also\nthe features of data. Extensive experiments on three widely used\ndatasets demonstrate its superiority over several state-of-the-art\nmethods in both accuracy and scalability. Our experiment codes\nare available at: https://lcbwlx.wixsite.com/fssh.</p>\n", "tags": ["Scalability","Datasets","Neural-Hashing","SIGIR","Hashing-Methods","Evaluation","Supervised"] },
{"key": "luo2025survey", "year": "2022", "citations": "123", "title":"A Survey on Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>\n", "tags": ["Efficiency","Unsupervised","Survey-Paper","Datasets","Neural-Hashing","Self-Supervised","Evaluation","Quantization","Hashing-Methods","Supervised"] },
{"key": "ma2018progressive", "year": "2018", "citations": "18", "title":"Progressive Generative Hashing for Image Retrieval", "abstract": "<p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image\nretrieval. Owing to the great learning capacity,\ndeep hashing has become one of the most promising solutions, and achieved attractive performance\nin practice. However, without semantic label information, the unsupervised deep hashing still remains\nan open question. In this paper, we propose a novel\nprogressive generative hashing (PGH) framework\nto help learn a discriminative hashing network in an\nunsupervised way. Different from existing studies,\nit first treats the hash codes as a kind of semantic\ncondition for the similar image generation, and simultaneously feeds the original image and its codes\ninto the generative adversarial networks (GANs).\nThe real images together with the synthetic ones\ncan further help train a discriminative hashing network based on a triplet loss. By iteratively inputting\nthe learnt codes into the hash conditioned GANs, we can progressively enable the hashing network\nto discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>\n", "tags": ["Scalability","Evaluation","Distance-Metric-Learning","Datasets","Unsupervised","AAAI","Robustness","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","IJCAI","Image-Retrieval","Supervised"] },
{"key": "ma2022pre", "year": "2022", "citations": "30", "title":"Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction", "abstract": "<p>Dense retrieval has shown promising results in many information retrieval\n(IR) related tasks, whose foundation is high-quality text representation\nlearning for effective search. Some recent studies have shown that\nautoencoder-based language models are able to boost the dense retrieval\nperformance using a weak decoder. However, we argue that 1) it is not\ndiscriminative to decode all the input texts and, 2) even a weak decoder has\nthe bypass effect on the encoder. Therefore, in this work, we introduce a novel\ncontrastive span prediction task to pre-train the encoder alone, but still\nretain the bottleneck ability of the autoencoder. % Therefore, in this work, we\npropose to drop out the decoder and introduce a novel contrastive span\nprediction task to pre-train the encoder alone. The key idea is to force the\nencoder to generate the text representation close to its own random spans while\nfar away from others using a group-wise contrastive loss. In this way, we can\n1) learn discriminative text representations efficiently with the group-wise\ncontrastive learning over spans and, 2) avoid the bypass effect of the decoder\nthoroughly. Comprehensive experiments over publicly available retrieval\nbenchmark datasets show that our approach can outperform existing pre-training\nmethods for dense retrieval significantly.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","SIGIR","Self-Supervised","Evaluation"] },
{"key": "ma2023direction", "year": "2024", "citations": "11", "title":"Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval", "abstract": "<p>Image-text retrieval has developed rapidly in recent years. However, it is\nstill a challenge in remote sensing due to visual-semantic imbalance, which\nleads to incorrect matching of non-semantic visual and textual features. To\nsolve this problem, we propose a novel Direction-Oriented Visual-semantic\nEmbedding Model (DOVE) to mine the relationship between vision and language.\nOur highlight is to conduct visual and textual representations in latent space,\ndirecting them as close as possible to a redundancy-free regional visual\nrepresentation. Concretely, a Regional-Oriented Attention Module (ROAM)\nadaptively adjusts the distance between the final visual and textual embeddings\nin the latent semantic space, oriented by regional visual features. Meanwhile,\na lightweight Digging Text Genome Assistant (DTGA) is designed to expand the\nrange of tractable textual representation and enhance global word-level\nsemantic connections using less attention operations. Ultimately, we exploit a\nglobal visual-semantic constraint to reduce single visual dependency and serve\nas an external constraint for the final visual and textual representations. The\neffectiveness and superiority of our method are verified by extensive\nexperiments including parameter evaluation, quantitative comparison, ablation\nstudies and visual analysis, on two benchmark datasets, RSICD and RSITMD.</p>\n", "tags": ["Datasets","Text-Retrieval","Evaluation"] },
{"key": "ma2025progressive", "year": "2018", "citations": "18", "title":"Progressive Generative Hashing for Image Retrieval", "abstract": "<p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image\nretrieval. Owing to the great learning capacity,\ndeep hashing has become one of the most promising solutions, and achieved attractive performance\nin practice. However, without semantic label information, the unsupervised deep hashing still remains\nan open question. In this paper, we propose a novel\nprogressive generative hashing (PGH) framework\nto help learn a discriminative hashing network in an\nunsupervised way. Different from existing studies,\nit first treats the hash codes as a kind of semantic\ncondition for the similar image generation, and simultaneously feeds the original image and its codes\ninto the generative adversarial networks (GANs).\nThe real images together with the synthetic ones\ncan further help train a discriminative hashing network based on a triplet loss. By iteratively inputting\nthe learnt codes into the hash conditioned GANs, we can progressively enable the hashing network\nto discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>\n", "tags": ["Scalability","Evaluation","Distance-Metric-Learning","Datasets","Unsupervised","AAAI","Robustness","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","IJCAI","Image-Retrieval","Supervised"] },
{"key": "macdonald2021approximate", "year": "2021", "citations": "14", "title":"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval", "abstract": "<p>Dense retrieval, which describes the use of contextualised language models\nsuch as BERT to identify documents from a collection by leveraging approximate\nnearest neighbour (ANN) techniques, has been increasing in popularity. Two\nfamilies of approaches have emerged, depending on whether documents and queries\nare represented by single or multiple embeddings. ColBERT, the exemplar of the\nlatter, uses an ANN index and approximate scores to identify a set of candidate\ndocuments for each query embedding, which are then re-ranked using accurate\ndocument representations. In this manner, a large number of documents can be\nretrieved for each query, hindering the efficiency of the approach. In this\nwork, we investigate the use of ANN scores for ranking the candidate documents,\nin order to decrease the number of candidate documents being fully scored.\nExperiments conducted on the MSMARCO passage ranking corpus demonstrate that,\nby cutting of the candidate set by using the approximate scores to only 200\ndocuments, we can still obtain an effective ranking without statistically\nsignificant differences in effectiveness, and resulting in a 2x speedup in\nefficiency.</p>\n", "tags": ["Efficiency","Similarity-Search","Vector-Indexing","CIKM"] },
{"key": "mafla2020fine", "year": "2020", "citations": "30", "title":"Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features", "abstract": "<p>Text contained in an image carries high-level semantics that can be exploited\nto achieve richer image understanding. In particular, the mere presence of text\nprovides strong guiding content that should be employed to tackle a diversity\nof computer vision tasks such as image retrieval, fine-grained classification,\nand visual question answering. In this paper, we address the problem of\nfine-grained classification and image retrieval by leveraging textual\ninformation along with visual cues to comprehend the existing intrinsic\nrelation between the two modalities. The novelty of the proposed model consists\nof the usage of a PHOC descriptor to construct a bag of textual words along\nwith a Fisher Vector Encoding that captures the morphology of text. This\napproach provides a stronger multimodal representation for this task and as our\nexperiments demonstrate, it achieves state-of-the-art results on two different\ntasks, fine-grained classification and image retrieval.</p>\n", "tags": ["Image-Retrieval"] },
{"key": "magliani2018efficient", "year": "2018", "citations": "8", "title":"Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition", "abstract": "<p>The problem of landmark recognition has achieved excellent results in\nsmall-scale datasets. When dealing with large-scale retrieval, issues that were\nirrelevant with small amount of data, quickly become fundamental for an\nefficient retrieval phase. In particular, computational time needs to be kept\nas low as possible, whilst the retrieval accuracy has to be preserved as much\nas possible. In this paper we propose a novel multi-index hashing method called\nBag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows\nto drastically reduce the query time and outperforms the accuracy results\ncompared to the state-of-the-art methods for large-scale landmark recognition.\nIt has been demonstrated that this family of algorithms can be applied on\ndifferent embedding techniques like VLAD and R-MAC obtaining excellent results\nin very short times on different public datasets: Holidays+Flickr1M, Oxford105k\nand Paris106k.</p>\n", "tags": ["Similarity-Search","Vector-Indexing","Hashing-Methods","Datasets","Scalability","Efficiency"] },
{"key": "magliani2019efficient", "year": "2019", "citations": "8", "title":"An Efficient Approximate kNN Graph Method for Diffusion on Image Retrieval", "abstract": "<p>The application of the diffusion in many computer vision and artificial\nintelligence projects has been shown to give excellent improvements in\nperformance. One of the main bottlenecks of this technique is the quadratic\ngrowth of the kNN graph size due to the high-quantity of new connections\nbetween nodes in the graph, resulting in long computation times. Several\nstrategies have been proposed to address this, but none are effective and\nefficient. Our novel technique, based on LSH projections, obtains the same\nperformance as the exact kNN graph after diffusion, but in less time\n(approximately 18 times faster on a dataset of a hundred thousand images). The\nproposed method was validated and compared with other state-of-the-art on\nseveral public image datasets, including Oxford5k, Paris6k, and Oxford105k.</p>\n", "tags": ["Datasets","Evaluation","Locality-Sensitive-Hashing"] },
{"key": "mahajan2019joint", "year": "2019", "citations": "8", "title":"Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings", "abstract": "<p>One of the key challenges in learning joint embeddings of multiple\nmodalities, e.g. of images and text, is to ensure coherent cross-modal\nsemantics that generalize across datasets. We propose to address this through\njoint Gaussian regularization of the latent representations. Building on\nWasserstein autoencoders (WAEs) to encode the input in each domain, we enforce\nthe latent embeddings to be similar to a Gaussian prior that is shared across\nthe two domains, ensuring compatible continuity of the encoded semantic\nrepresentations of images and texts. Semantic alignment is achieved through\nsupervision from matching image-text pairs. To show the benefits of our\nsemi-supervised representation, we apply it to cross-modal retrieval and phrase\nlocalization. We not only achieve state-of-the-art accuracy, but significantly\nbetter generalization across datasets, owing to the semantic continuity of the\nlatent space.</p>\n", "tags": ["ICCV","Datasets","Supervised","Multimodal-Retrieval"] },
{"key": "maheshwari2020learning", "year": "2020", "citations": "5", "title":"Learning Colour Representations of Search Queries", "abstract": "<p>Image search engines rely on appropriately designed ranking features that\ncapture various aspects of the content semantics as well as the historic\npopularity. In this work, we consider the role of colour in this relevance\nmatching process. Our work is motivated by the observation that a significant\nfraction of user queries have an inherent colour associated with them. While\nsome queries contain explicit colour mentions (such as ‘black car’ and ‘yellow\ndaisies’), other queries have implicit notions of colour (such as ‘sky’ and\n‘grass’). Furthermore, grounding queries in colour is not a mapping to a single\ncolour, but a distribution in colour space. For instance, a search for ‘trees’\ntends to have a bimodal distribution around the colours green and brown. We\nleverage historical clickthrough data to produce a colour representation for\nsearch queries and propose a recurrent neural network architecture to encode\nunseen queries into colour space. We also show how this embedding can be learnt\nalongside a cross-modal relevance ranker from impression logs where a subset of\nthe result images were clicked. We demonstrate that the use of a query-image\ncolour distance feature leads to an improvement in the ranker performance as\nmeasured by users’ preferences of clicked versus skipped images.</p>\n", "tags": ["SIGIR","Evaluation","Image-Retrieval"] },
{"key": "maheshwari2021scene", "year": "2021", "citations": "10", "title":"Scene Graph Embeddings Using Relative Similarity Supervision", "abstract": "<p>Scene graphs are a powerful structured representation of the underlying\ncontent of images, and embeddings derived from them have been shown to be\nuseful in multiple downstream tasks. In this work, we employ a graph\nconvolutional network to exploit structure in scene graphs and produce image\nembeddings useful for semantic image retrieval. Different from\nclassification-centric supervision traditionally available for learning image\nrepresentations, we address the task of learning from relative similarity\nlabels in a ranking context. Rooted within the contrastive learning paradigm,\nwe propose a novel loss function that operates on pairs of similar and\ndissimilar images and imposes relative ordering between them in embedding\nspace. We demonstrate that this Ranking loss, coupled with an intuitive triple\nsampling strategy, leads to robust representations that outperform well-known\ncontrastive losses on the retrieval task. In addition, we provide qualitative\nevidence of how retrieved results that utilize structured scene information\ncapture the global context of the scene, different from visual similarity\nsearch.</p>\n", "tags": ["AAAI","Distance-Metric-Learning","Self-Supervised","Image-Retrieval"] },
{"key": "maji2020cbir", "year": "2021", "citations": "10", "title":"CBIR using features derived by Deep Learning", "abstract": "<p>In a Content Based Image Retrieval (CBIR) System, the task is to retrieve\nsimilar images from a large database given a query image. The usual procedure\nis to extract some useful features from the query image, and retrieve images\nwhich have similar set of features. For this purpose, a suitable similarity\nmeasure is chosen, and images with high similarity scores are retrieved.\nNaturally the choice of these features play a very important role in the\nsuccess of this system, and high level features are required to reduce the\nsemantic gap.\n  In this paper, we propose to use features derived from pre-trained network\nmodels from a deep-learning convolution network trained for a large image\nclassification problem. This approach appears to produce vastly superior\nresults for a variety of databases, and it outperforms many contemporary CBIR\nsystems. We analyse the retrieval time of the method, and also propose a\npre-clustering of the database based on the above-mentioned features which\nyields comparable results in a much shorter time in most of the cases.</p>\n", "tags": ["Image-Retrieval"] },
{"key": "malali2022learning", "year": "2021", "citations": "8", "title":"Learning to embed semantic similarity for joint image-text retrieval", "abstract": "<p>We present a deep learning approach for learning the joint semantic\nembeddings of images and captions in a Euclidean space, such that the semantic\nsimilarity is approximated by the L2 distances in the embedding space. For\nthat, we introduce a metric learning scheme that utilizes multitask learning to\nlearn the embedding of identical semantic concepts using a center loss. By\nintroducing a differentiable quantization scheme into the end-to-end trainable\nnetwork, we derive a semantic embedding of semantically similar concepts in\nEuclidean space. We also propose a novel metric learning formulation using an\nadaptive margin hinge loss, that is refined during the training phase. The\nproposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets,\nand was shown to compare favorably with contemporary state-of-the-art\napproaches.</p>\n", "tags": ["Distance-Metric-Learning","Quantization","Datasets","Text-Retrieval"] },
{"key": "malkov2016efficient", "year": "2018", "citations": "1063", "title":"Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs", "abstract": "<p>We present a new approach for the approximate K-nearest neighbor search based\non navigable small world graphs with controllable hierarchy (Hierarchical NSW,\nHNSW). The proposed solution is fully graph-based, without any need for\nadditional search structures, which are typically used at the coarse search\nstage of the most proximity graph techniques. Hierarchical NSW incrementally\nbuilds a multi-layer structure consisting from hierarchical set of proximity\ngraphs (layers) for nested subsets of the stored elements. The maximum layer in\nwhich an element is present is selected randomly with an exponentially decaying\nprobability distribution. This allows producing graphs similar to the\npreviously studied Navigable Small World (NSW) structures while additionally\nhaving the links separated by their characteristic distance scales. Starting\nsearch from the upper layer together with utilizing the scale separation boosts\nthe performance compared to NSW and allows a logarithmic complexity scaling.\nAdditional employment of a heuristic for selecting proximity graph neighbors\nsignificantly increases performance at high recall and in case of highly\nclustered data. Performance evaluation has demonstrated that the proposed\ngeneral metric space search index is able to strongly outperform previous\nopensource state-of-the-art vector-only approaches. Similarity of the algorithm\nto the skip list structure allows straightforward balanced distributed\nimplementation.</p>\n", "tags": ["Evaluation","Graph-Based-ANN"] },
{"key": "manandhar2019semantic", "year": "2020", "citations": "10", "title":"Semantic Granularity Metric Learning for Visual Search", "abstract": "<p>Deep metric learning applied to various applications has shown promising\nresults in identification, retrieval and recognition. Existing methods often do\nnot consider different granularity in visual similarity. However, in many\ndomain applications, images exhibit similarity at multiple granularities with\nvisual semantic concepts, e.g. fashion demonstrates similarity ranging from\nclothing of the exact same instance to similar looks/design or a common\ncategory. Therefore, training image triplets/pairs used for metric learning\ninherently possess different degree of information. However, the existing\nmethods often treats them with equal importance during training. This hinders\ncapturing the underlying granularities in feature similarity required for\neffective visual search.\n  In view of this, we propose a new deep semantic granularity metric learning\n(SGML) that develops a novel idea of leveraging attribute semantic space to\ncapture different granularity of similarity, and then integrate this\ninformation into deep metric learning. The proposed method simultaneously\nlearns image attributes and embeddings using multitask CNNs. The two tasks are\nnot only jointly optimized but are further linked by the semantic granularity\nsimilarity mappings to leverage the correlations between the tasks. To this\nend, we propose a new soft-binomial deviance loss that effectively integrates\nthe degree of information in training samples, which helps to capture visual\nsimilarity at multiple granularities. Compared to recent ensemble-based\nmethods, our framework is conceptually elegant, computationally simple and\nprovides better performance. We perform extensive experiments on benchmark\nmetric learning datasets and demonstrate that our method outperforms recent\nstate-of-the-art methods, e.g., 1-4.5% improvement in Recall@1 over the\nprevious state-of-the-arts [1],[2] on DeepFashion In-Shop dataset.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "manocha2017content", "year": "2018", "citations": "38", "title":"Content-based Representations of audio using Siamese neural networks", "abstract": "<p>In this paper, we focus on the problem of content-based retrieval for audio,\nwhich aims to retrieve all semantically similar audio recordings for a given\naudio clip query. This problem is similar to the problem of query by example of\naudio, which aims to retrieve media samples from a database, which are similar\nto the user-provided example. We propose a novel approach which encodes the\naudio into a vector representation using Siamese Neural Networks. The goal is\nto obtain an encoding similar for files belonging to the same audio class, thus\nallowing retrieval of semantically similar audio. Using simple similarity\nmeasures such as those based on simple euclidean distance and cosine similarity\nwe show that these representations can be very effectively used for retrieving\nrecordings similar in audio content.</p>\n", "tags": ["Distance-Metric-Learning","ICASSP"] },
{"key": "markchit2019effective", "year": "2019", "citations": "6", "title":"Effective and Efficient Indexing in Cross-Modal Hashing-Based Datasets", "abstract": "<p>To overcome the barrier of storage and computation, the hashing technique has\nbeen widely used for nearest neighbor search in multimedia retrieval\napplications recently. Particularly, cross-modal retrieval that searches across\ndifferent modalities becomes an active but challenging problem. Although dozens\nof cross-modal hashing algorithms are proposed to yield compact binary codes,\nthe exhaustive search is impractical for the real-time purpose, and Hamming\ndistance computation suffers inaccurate results. In this paper, we propose a\nnovel search method that utilizes a probability-based index scheme over binary\nhash codes in cross-modal retrieval. The proposed hash code indexing scheme\nexploits a few binary bits of the hash code as the index code. We construct an\ninverted index table based on index codes and train a neural network to improve\nthe indexing accuracy and efficiency. Experiments are performed on two\nbenchmark datasets for retrieval across image and text modalities, where hash\ncodes are generated by three cross-modal hashing methods. Results show the\nproposed method effectively boost the performance on these hash methods.</p>\n", "tags": ["Hashing-Methods","Datasets","Compact-Codes","Efficiency","Evaluation","Multimodal-Retrieval"] },
{"key": "matsubara2019target", "year": "2020", "citations": "8", "title":"Target-Oriented Deformation of Visual-Semantic Embedding Space", "abstract": "<p>Multimodal embedding is a crucial research topic for cross-modal\nunderstanding, data mining, and translation. Many studies have attempted to\nextract representations from given entities and align them in a shared\nembedding space. However, because entities in different modalities exhibit\ndifferent abstraction levels and modality-specific information, it is\ninsufficient to embed related entities close to each other. In this study, we\npropose the Target-Oriented Deformation Network (TOD-Net), a novel module that\ncontinuously deforms the embedding space into a new space under a given\ncondition, thereby adjusting similarities between entities. Unlike methods\nbased on cross-modal attention, TOD-Net is a post-process applied to the\nembedding space learned by existing embedding systems and improves their\nperformances of retrieval. In particular, when combined with cutting-edge\nmodels, TOD-Net gains the state-of-the-art cross-modal retrieval model\nassociated with the MSCOCO dataset. Qualitative analysis reveals that TOD-Net\nsuccessfully emphasizes entity-specific concepts and retrieves diverse targets\nvia handling higher levels of diversity than existing models.</p>\n", "tags": ["Datasets","Multimodal-Retrieval"] },
{"key": "mazumder2021few", "year": "2021", "citations": "22", "title":"Few-Shot Keyword Spotting in Any Language", "abstract": "<p>We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe\npromising initial results on keyword search.</p>\n", "tags": ["Few-Shot-&-Zero-Shot"] },
{"key": "mccauley2018adaptive", "year": "2018", "citations": "6", "title":"Adaptive MapReduce Similarity Joins", "abstract": "<p>Similarity joins are a fundamental database operation. Given data sets S and\nR, the goal of a similarity join is to find all points x in S and y in R with\ndistance at most r. Recent research has investigated how locality-sensitive\nhashing (LSH) can be used for similarity join, and in particular two recent\nlines of work have made exciting progress on LSH-based join performance. Hu,\nTao, and Yi (PODS 17) investigated joins in a massively parallel setting,\nshowing strong results that adapt to the size of the output. Meanwhile, Ahle,\nAum\"uller, and Pagh (SODA 17) showed a sequential algorithm that adapts to the\nstructure of the data, matching classic bounds in the worst case but improving\nthem significantly on more structured data. We show that this adaptive strategy\ncan be adapted to the parallel setting, combining the advantages of these\napproaches. In particular, we show that a simple modification to Hu et al.’s\nalgorithm achieves bounds that depend on the density of points in the dataset\nas well as the total outsize of the output. Our algorithm uses no extra\nparameters over other LSH approaches (in particular, its execution does not\ndepend on the structure of the dataset), and is likely to be efficient in\npractice.</p>\n", "tags": ["Hashing-Methods","Datasets","Evaluation","Locality-Sensitive-Hashing"] },
{"key": "mcfee2010learning", "year": "2011", "citations": "122", "title":"Learning Multi-modal Similarity", "abstract": "<p>In many applications involving multi-media data, the definition of similarity\nbetween items is integral to several key tasks, e.g., nearest-neighbor\nretrieval, classification, and recommendation. Data in such regimes typically\nexhibits multiple modalities, such as acoustic and visual content of video.\nIntegrating such heterogeneous data to form a holistic similarity space is\ntherefore a key challenge to be overcome in many real-world applications.\n  We present a novel multiple kernel learning technique for integrating\nheterogeneous data into a single, unified similarity space. Our algorithm\nlearns an optimal ensemble of kernel transfor- mations which conform to\nmeasurements of human perceptual similarity, as expressed by relative\ncomparisons. To cope with the ubiquitous problems of subjectivity and\ninconsistency in multi- media similarity, we develop graph-based techniques to\nfilter similarity measurements, resulting in a simplified and robust training\nprocedure.</p>\n", "tags": ["Recommender-Systems","Graph-Based-ANN"] },
{"key": "mckeown2022hamming", "year": "2023", "citations": "9", "title":"Hamming Distributions of Popular Perceptual Hashing Techniques", "abstract": "<p>Content-based file matching has been widely deployed for decades, largely for\nthe detection of sources of copyright infringement, extremist materials, and\nabusive sexual media. Perceptual hashes, such as Microsoft’s PhotoDNA, are one\nautomated mechanism for facilitating detection, allowing for machines to\napproximately match visual features of an image or video in a robust manner.\nHowever, there does not appear to be much public evaluation of such approaches,\nparticularly when it comes to how effective they are against content-preserving\nmodifications to media files. In this paper, we present a million-image scale\nevaluation of several perceptual hashing archetypes for popular algorithms\n(including Facebook’s PDQ, Apple’s Neuralhash, and the popular pHash library)\nagainst seven image variants. The focal point is the distribution of Hamming\ndistance scores between both unrelated images and image variants to better\nunderstand the problems faced by each approach.</p>\n", "tags": ["Hashing-Methods","Evaluation","Tools-&-Libraries"] },
{"key": "melekhov2021digging", "year": "2021", "citations": "7", "title":"Digging Into Self-Supervised Learning of Feature Descriptors", "abstract": "<p>Fully-supervised CNN-based approaches for learning local image descriptors\nhave shown remarkable results in a wide range of geometric tasks. However, most\nof them require per-pixel ground-truth keypoint correspondence data which is\ndifficult to acquire at scale. To address this challenge, recent weakly- and\nself-supervised methods can learn feature descriptors from relative camera\nposes or using only synthetic rigid transformations such as homographies. In\nthis work, we focus on understanding the limitations of existing\nself-supervised approaches and propose a set of improvements that combined lead\nto powerful feature descriptors. We show that increasing the search space from\nin-pair to in-batch for hard negative mining brings consistent improvement. To\nenhance the discriminativeness of feature descriptors, we propose a\ncoarse-to-fine method for mining local hard negatives from a wider search space\nby using global visual image descriptors. We demonstrate that a combination of\nsynthetic homography transformation, color augmentation, and photorealistic\nimage stylization produces useful representations that are viewpoint and\nillumination invariant. The feature descriptors learned by the proposed\napproach perform competitively and surpass their fully- and weakly-supervised\ncounterparts on various geometric benchmarks such as image-based localization,\nsparse feature matching, and image retrieval.</p>\n", "tags": ["Self-Supervised","Supervised","Image-Retrieval"] },
{"key": "messina2020fine", "year": "2021", "citations": "116", "title":"Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders", "abstract": "<p>Despite the evolution of deep-learning-based visual-textual processing\nsystems, precise multi-modal matching remains a challenging task. In this work,\nwe tackle the task of cross-modal retrieval through image-sentence matching\nbased on word-region alignments, using supervision only at the global\nimage-sentence level. Specifically, we present a novel approach called\nTransformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a\nfine-grained match between the underlying components of images and sentences,\ni.e., image regions and words, respectively, in order to preserve the\ninformative richness of both modalities. TERAN obtains state-of-the-art results\non the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover,\non MS-COCO, it also outperforms current approaches on the sentence retrieval\ntask.\n  Focusing on scalable cross-modal information retrieval, TERAN is designed to\nkeep the visual and textual data pipelines well separated. Cross-attention\nlinks invalidate any chance to separately extract visual and textual features\nneeded for the online search and the offline indexing steps in large-scale\nretrieval systems. In this respect, TERAN merges the information from the two\ndomains only during the final alignment phase, immediately before the loss\ncomputation. We argue that the fine-grained alignments produced by TERAN pave\nthe way towards the research for effective and efficient methods for\nlarge-scale cross-modal information retrieval. We compare the effectiveness of\nour approach against relevant state-of-the-art methods. On the MS-COCO 1K test\nset, we obtain an improvement of 5.7% and 3.5% respectively on the image and\nthe sentence retrieval tasks on the Recall@1 metric. The code used for the\nexperiments is publicly available on GitHub at\nhttps://github.com/mesnico/TERAN.</p>\n", "tags": ["Image-Retrieval","Datasets","Scalability","Evaluation","Multimodal-Retrieval"] },
{"key": "messina2020transformer", "year": "2021", "citations": "56", "title":"Transformer Reasoning Network for Image-Text Matching and Retrieval", "abstract": "<p>Image-text matching is an interesting and fascinating task in modern AI\nresearch. Despite the evolution of deep-learning-based image and text\nprocessing systems, multi-modal matching remains a challenging problem. In this\nwork, we consider the problem of accurate image-text matching for the task of\nmulti-modal large-scale information retrieval. State-of-the-art results in\nimage-text matching are achieved by inter-playing image and text features from\nthe two different processing pipelines, usually using mutual attention\nmechanisms. However, this invalidates any chance to extract separate visual and\ntextual features needed for later indexing steps in large-scale retrieval\nsystems. In this regard, we introduce the Transformer Encoder Reasoning Network\n(TERN), an architecture built upon one of the modern relationship-aware\nself-attentive architectures, the Transformer Encoder (TE). This architecture\nis able to separately reason on the two different modalities and to enforce a\nfinal common abstract concept space by sharing the weights of the deeper\ntransformer layers. Thanks to this design, the implemented network is able to\nproduce compact and very rich visual and textual features available for the\nsuccessive indexing step. Experiments are conducted on the MS-COCO dataset, and\nwe evaluate the results using a discounted cumulative gain metric with\nrelevance computed exploiting caption similarities, in order to assess possibly\nnon-exact but relevant search results. We demonstrate that on this metric we\nare able to achieve state-of-the-art results in the image retrieval task. Our\ncode is freely available at https://github.com/mesnico/TERN.</p>\n", "tags": ["Datasets","Scalability","Image-Retrieval"] },
{"key": "messina2021towards", "year": "2021", "citations": "7", "title":"Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features", "abstract": "<p>Cross-modal retrieval is an important functionality in modern search engines,\nas it increases the user experience by allowing queries and retrieved objects\nto pertain to different modalities. In this paper, we focus on the\nimage-sentence retrieval task, where the objective is to efficiently find\nrelevant images for a given sentence (image-retrieval) or the relevant\nsentences for a given image (sentence-retrieval). Computer vision literature\nreports the best results on the image-sentence matching task using deep neural\nnetworks equipped with attention and self-attention mechanisms. They evaluate\nthe matching performance on the retrieval task by performing sequential scans\nof the whole dataset. This method does not scale well with an increasing amount\nof images or captions. In this work, we explore different preprocessing\ntechniques to produce sparsified deep multi-modal features extracting them from\nstate-of-the-art deep-learning architectures for image-text matching. Our main\nobjective is to lay down the paths for efficient indexing of complex\nmulti-modal descriptions. We use the recently introduced TERN architecture as\nan image-sentence features extractor. It is designed for producing fixed-size\n1024-d vectors describing whole images and sentences, as well as\nvariable-length sets of 1024-d vectors describing the various building\ncomponents of the two modalities (image regions and sentence words\nrespectively). All these vectors are enforced by the TERN design to lie into\nthe same common space. Our experiments show interesting preliminary results on\nthe explored methods and suggest further experimentation in this important\nresearch direction.</p>\n", "tags": ["Datasets","Evaluation","Multimodal-Retrieval"] },
{"key": "meyer2017deep", "year": "2018", "citations": "29", "title":"Deep Metric Learning and Image Classification with Nearest Neighbour Gaussian Kernels", "abstract": "<p>We present a Gaussian kernel loss function and training algorithm for\nconvolutional neural networks that can be directly applied to both distance\nmetric learning and image classification problems. Our method treats all\ntraining features from a deep neural network as Gaussian kernel centres and\ncomputes loss by summing the influence of a feature’s nearby centres in the\nfeature embedding space. Our approach is made scalable by treating it as an\napproximate nearest neighbour search problem. We show how to make end-to-end\nlearning feasible, resulting in a well formed embedding space, in which\nsemantically related instances are likely to be located near one another,\nregardless of whether or not the network was trained on those classes. Our\napproach outperforms state-of-the-art deep metric learning approaches on\nembedding learning challenges, as well as conventional softmax classification\non several datasets.</p>\n", "tags": ["Similarity-Search","Distance-Metric-Learning","Datasets"] },
{"key": "mikriukov2022deep", "year": "2022", "citations": "19", "title":"Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing", "abstract": "<p>Due to the availability of large-scale multi-modal data (e.g., satellite\nimages acquired by different sensors, text sentences, etc) archives, the\ndevelopment of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in RS. In this paper, we focus our\nattention on cross-modal text-image retrieval, where queries from one modality\n(e.g., text) can be matched to archive entries from another (e.g., image). Most\nof the existing cross-modal text-image retrieval systems require a high number\nof labeled training samples and also do not allow fast and memory-efficient\nretrieval due to their intrinsic characteristics. These issues limit the\napplicability of the existing cross-modal retrieval systems for large-scale\napplications in RS. To address this problem, in this paper we introduce a novel\ndeep unsupervised cross-modal contrastive hashing (DUCH) method for RS\ntext-image retrieval. The proposed DUCH is made up of two main modules: 1)\nfeature extraction module (which extracts deep representations of the\ntext-image modalities); and 2) hashing module (which learns to generate\ncross-modal binary hash codes from the extracted representations). Within the\nhashing module, we introduce a novel multi-objective loss function including:\ni) contrastive objectives that enable similarity preservation in both intra-\nand inter-modal similarities; ii) an adversarial objective that is enforced\nacross two modalities for cross-modal representation consistency; iii)\nbinarization objectives for generating representative hash codes. Experimental\nresults show that the proposed DUCH outperforms state-of-the-art unsupervised\ncross-modal hashing methods on two multi-modal (image and text) benchmark\narchives in RS. Our code is publicly available at\nhttps://git.tu-berlin.de/rsim/duch.</p>\n", "tags": ["Multimodal-Retrieval","Image-Retrieval","Hashing-Methods","Neural-Hashing","Unsupervised","Scalability","Evaluation","Robustness"] },
{"key": "mikriukov2022unsupervised", "year": "2022", "citations": "5", "title":"Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing", "abstract": "<p>The development of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in remote sensing (RS). In this paper,\nwe focus our attention on cross-modal text-image retrieval, where queries from\none modality (e.g., text) can be matched to archive entries from another (e.g.,\nimage). Most of the existing cross-modal text-image retrieval systems in RS\nrequire a high number of labeled training samples and also do not allow fast\nand memory-efficient retrieval. These issues limit the applicability of the\nexisting cross-modal retrieval systems for large-scale applications in RS. To\naddress this problem, in this paper we introduce a novel unsupervised\ncross-modal contrastive hashing (DUCH) method for text-image retrieval in RS.\nTo this end, the proposed DUCH is made up of two main modules: 1) feature\nextraction module, which extracts deep representations of two modalities; 2)\nhashing module that learns to generate cross-modal binary hash codes from the\nextracted representations. We introduce a novel multi-objective loss function\nincluding: i) contrastive objectives that enable similarity preservation in\nintra- and inter-modal similarities; ii) an adversarial objective that is\nenforced across two modalities for cross-modal representation consistency; and\niii) binarization objectives for generating hash codes. Experimental results\nshow that the proposed DUCH outperforms state-of-the-art methods. Our code is\npublicly available at https://git.tu-berlin.de/rsim/duch.</p>\n", "tags": ["Similarity-Search","Multimodal-Retrieval","Image-Retrieval","Hashing-Methods","Neural-Hashing","Unsupervised","Scalability","Robustness"] },
{"key": "misra2018bernoulli", "year": "2018", "citations": "9", "title":"Bernoulli Embeddings for Graphs", "abstract": "<p>Just as semantic hashing can accelerate information retrieval, binary valued\nembeddings can significantly reduce latency in the retrieval of graphical data.\nWe introduce a simple but effective model for learning such binary vectors for\nnodes in a graph. By imagining the embeddings as independent coin flips of\nvarying bias, continuous optimization techniques can be applied to the\napproximate expected loss. Embeddings optimized in this fashion consistently\noutperform the quantization of both spectral graph embeddings and various\nlearned real-valued embeddings, on both ranking and pre-ranking tasks for a\nvariety of datasets.</p>\n", "tags": ["Text-Retrieval","Hashing-Methods","Datasets","Hybrid-ANN-Methods","Re-Ranking","AAAI","Quantization"] },
{"key": "misraa2020multi", "year": "2020", "citations": "5", "title":"Multi-Modal Retrieval using Graph Neural Networks", "abstract": "<p>Most real world applications of image retrieval such as Adobe Stock, which is\na marketplace for stock photography and illustrations, need a way for users to\nfind images which are both visually (i.e. aesthetically) and conceptually (i.e.\ncontaining the same salient objects) as a query image. Learning visual-semantic\nrepresentations from images is a well studied problem for image retrieval.\nFiltering based on image concepts or attributes is traditionally achieved with\nindex-based filtering (e.g. on textual tags) or by re-ranking after an initial\nvisual embedding based retrieval. In this paper, we learn a joint vision and\nconcept embedding in the same high-dimensional space. This joint model gives\nthe user fine-grained control over the semantics of the result set, allowing\nthem to explore the catalog of images more rapidly. We model the visual and\nconcept relationships as a graph structure, which captures the rich information\nthrough node neighborhood. This graph structure helps us learn multi-modal node\nembeddings using Graph Neural Networks. We also introduce a novel inference\ntime control, based on selective neighborhood connectivity allowing the user\ncontrol over the retrieval algorithm. We evaluate these multi-modal embeddings\nquantitatively on the downstream relevance task of image retrieval on MS-COCO\ndataset and qualitatively on MS-COCO and an Adobe Stock dataset.</p>\n", "tags": ["Datasets","Hybrid-ANN-Methods","Re-Ranking","Image-Retrieval"] },
{"key": "mithun2018webly", "year": "2018", "citations": "73", "title":"Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval", "abstract": "<p>Cross-modal retrieval between visual data and natural language description\nremains a long-standing challenge in multimedia. While recent image-text\nretrieval methods offer great promise by learning deep representations aligned\nacross modalities, most of these methods are plagued by the issue of training\nwith small-scale datasets covering a limited number of images with ground-truth\nsentences. Moreover, it is extremely expensive to create a larger dataset by\nannotating millions of images with sentences and may lead to a biased model.\nInspired by the recent success of webly supervised learning in deep neural\nnetworks, we capitalize on readily-available web images with noisy annotations\nto learn robust image-text joint representation. Specifically, our main idea is\nto leverage web images and corresponding tags, along with fully annotated\ndatasets, in training for learning the visual-semantic joint embedding. We\npropose a two-stage approach for the task that can augment a typical supervised\npair-wise ranking loss based formulation with weakly-annotated web images to\nlearn a more robust visual-semantic embedding. Experiments on two standard\nbenchmark datasets demonstrate that our method achieves a significant\nperformance gain in image-text retrieval compared to state-of-the-art\napproaches.</p>\n", "tags": ["Supervised","Text-Retrieval","Datasets","Evaluation","Multimodal-Retrieval"] },
{"key": "mitra2016dual", "year": "2016", "citations": "110", "title":"A Dual Embedding Space Model for Document Ranking", "abstract": "<p>A fundamental goal of search engines is to identify, given a query, documents\nthat have relevant text. This is intrinsically difficult because the query and\nthe document may use different vocabulary, or the document may contain query\nwords without being relevant. We investigate neural word embeddings as a source\nof evidence in document ranking. We train a word2vec embedding model on a large\nunlabelled query corpus, but in contrast to how the model is commonly used, we\nretain both the input and the output projections, allowing us to leverage both\nthe embedding spaces to derive richer distributional relationships. During\nranking we map the query words into the input space and the document words into\nthe output space, and compute a query-document relevance score by aggregating\nthe cosine similarities across all the query-document word pairs.\n  We postulate that the proposed Dual Embedding Space Model (DESM) captures\nevidence on whether a document is about a query term in addition to what is\nmodelled by traditional term-frequency based approaches. Our experiments show\nthat the DESM can re-rank top documents returned by a commercial Web search\nengine, like Bing, better than a term-matching based signal like TF-IDF.\nHowever, when ranking a larger set of candidate documents, we find the\nembeddings-based approach is prone to false positives, retrieving documents\nthat are only loosely related to the query. We demonstrate that this problem\ncan be solved effectively by ranking based on a linear mixture of the DESM and\nthe word counting features.</p>\n", "tags": ["Evaluation"] },
{"key": "mohammadshahi2019aligning", "year": "2019", "citations": "8", "title":"Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task", "abstract": "<p>In this paper, we propose a new approach to learn multimodal multilingual\nembeddings for matching images and their relevant captions in two languages. We\ncombine two existing objective functions to make images and captions close in a\njoint embedding space while adapting the alignment of word embeddings between\nexisting languages in our model. We show that our approach enables better\ngeneralization, achieving state-of-the-art performance in text-to-image and\nimage-to-text retrieval task, and caption-caption similarity task. Two\nmultimodal multilingual datasets are used for evaluation: Multi30k with German\nand English captions and Microsoft-COCO with English and Japanese captions.</p>\n", "tags": ["Datasets","Text-Retrieval","Evaluation","Multimodal-Retrieval"] },
{"key": "mohedano2016bags", "year": "2016", "citations": "154", "title":"Bags of Local Convolutional Features for Scalable Instance Search", "abstract": "<p>This work proposes a simple instance retrieval pipeline based on encoding the\nconvolutional features of CNN using the bag of words aggregation scheme (BoW).\nAssigning each local array of activations in a convolutional layer to a visual\nword produces an \\textit{assignment map}, a compact representation that relates\nregions of an image with a visual word. We use the assignment map for fast\nspatial reranking, obtaining object localizations that are used for query\nexpansion. We demonstrate the suitability of the BoW representation based on\nlocal CNN features for instance retrieval, achieving competitive performance on\nthe Oxford and Paris buildings benchmarks. We show that our proposed system for\nCNN feature aggregation with BoW outperforms state-of-the-art techniques using\nsum pooling at a subset of the challenging TRECVid INS benchmark.</p>\n", "tags": ["Evaluation","Multimodal-Retrieval"] },
{"key": "mohedano2017saliency", "year": "2018", "citations": "37", "title":"Saliency Weighted Convolutional Features for Instance Search", "abstract": "<p>This work explores attention models to weight the contribution of local\nconvolutional representations for the instance search task. We present a\nretrieval framework based on bags of local convolutional features (BLCF) that\nbenefits from saliency weighting to build an efficient image representation.\nThe use of human visual attention models (saliency) allows significant\nimprovements in retrieval performance without the need to conduct region\nanalysis or spatial verification, and without requiring any feature fine\ntuning. We investigate the impact of different saliency models, finding that\nhigher performance on saliency benchmarks does not necessarily equate to\nimproved performance when used in instance search tasks. The proposed approach\noutperforms the state-of-the-art on the challenging INSTRE benchmark by a large\nmargin, and provides similar performance on the Oxford and Paris benchmarks\ncompared to more complex methods that use off-the-shelf representations. The\nsource code used in this project is available at\nhttps://imatge-upc.github.io/salbow/</p>\n", "tags": ["Evaluation","Tools-&-Libraries"] },
{"key": "moran2013neighbourhood", "year": "2013", "citations": "22", "title":"Neighbourhood Preserving Quantisation for LSH", "abstract": "<p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Quantization","SIGIR"] },
{"key": "moran2013variable", "year": "2013", "citations": "21", "title":"Variable Bit Quantisation for LSH", "abstract": "<p>We introduce a scheme for optimally allocating\na variable number of bits per\nLSH hyperplane. Previous approaches assign\na constant number of bits per hyperplane.\nThis neglects the fact that a subset\nof hyperplanes may be more informative\nthan others. Our method, dubbed Variable\nBit Quantisation (VBQ), provides a datadriven\nnon-uniform bit allocation across\nhyperplanes. Despite only using a fraction\nof the available hyperplanes, VBQ outperforms\nuniform quantisation by up to 168%\nfor retrieval across standard text and image\ndatasets.</p>\n", "tags": ["Locality-Sensitive-Hashing","Datasets"] },
{"key": "moran2015regularised", "year": "2015", "citations": "18", "title":"Regularised Cross-Modal Hashing", "abstract": "<p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods","SIGIR"] },
{"key": "moran2016enhancing", "year": "2016", "citations": "29", "title":"Enhancing First Story Detection using Word Embeddings", "abstract": "<p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","SIGIR"] },
{"key": "moran2025enhancing", "year": "2016", "citations": "29", "title":"Enhancing First Story Detection using Word Embeddings", "abstract": "<p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","SIGIR"] },
{"key": "moran2025neighbourhood", "year": "2013", "citations": "22", "title":"Neighbourhood Preserving Quantisation for LSH", "abstract": "<p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Quantization","SIGIR"] },
{"key": "moran2025regularised", "year": "2015", "citations": "18", "title":"Regularised Cross-Modal Hashing", "abstract": "<p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>\n", "tags": ["Hashing-Methods","SIGIR"] },
{"key": "moran2025variable", "year": "2013", "citations": "21", "title":"Variable Bit Quantisation for LSH", "abstract": "<p>We introduce a scheme for optimally allocating\na variable number of bits per\nLSH hyperplane. Previous approaches assign\na constant number of bits per hyperplane.\nThis neglects the fact that a subset\nof hyperplanes may be more informative\nthan others. Our method, dubbed Variable\nBit Quantisation (VBQ), provides a datadriven\nnon-uniform bit allocation across\nhyperplanes. Despite only using a fraction\nof the available hyperplanes, VBQ outperforms\nuniform quantisation by up to 168%\nfor retrieval across standard text and image\ndatasets.</p>\n", "tags": ["Locality-Sensitive-Hashing","Datasets"] },
{"key": "morgado2020deep", "year": "2020", "citations": "6", "title":"Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings", "abstract": "<p>Image hash codes are produced by binarizing\nthe embeddings of convolutional neural networks (CNN)\ntrained for either classification or retrieval. While proxy\nembeddings achieve good performance on both tasks,\nthey are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use\nof a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and\na procedure to design proxy sets that are nearly optimal\nfor both classification and hashing is introduced. The\nresulting hash-consistent large margin (HCLM) proxies\nare shown to encourage saturation of hashing units, thus\nguaranteeing a small binarization error, while producing\nhighly discriminative hash-codes. A semantic extension\n(sHCLM), aimed to improve hashing performance in\na transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant\nimprovements over state-of-the-art hashing procedures\non several small and large datasets, both within and\nbeyond the set of training classes.</p>\n", "tags": ["Hashing-Methods","Evaluation","Datasets","Neural-Hashing"] },
{"key": "morgado2025deep", "year": "2020", "citations": "6", "title":"Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings", "abstract": "<p>Image hash codes are produced by binarizing\nthe embeddings of convolutional neural networks (CNN)\ntrained for either classification or retrieval. While proxy\nembeddings achieve good performance on both tasks,\nthey are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use\nof a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and\na procedure to design proxy sets that are nearly optimal\nfor both classification and hashing is introduced. The\nresulting hash-consistent large margin (HCLM) proxies\nare shown to encourage saturation of hashing units, thus\nguaranteeing a small binarization error, while producing\nhighly discriminative hash-codes. A semantic extension\n(sHCLM), aimed to improve hashing performance in\na transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant\nimprovements over state-of-the-art hashing procedures\non several small and large datasets, both within and\nbeyond the set of training classes.</p>\n", "tags": ["Hashing-Methods","Evaluation","Datasets","Neural-Hashing"] },
{"key": "morozov2019unsupervised", "year": "2019", "citations": "21", "title":"Unsupervised Neural Quantization for Compressed-Domain Similarity Search", "abstract": "<p>We tackle the problem of unsupervised visual descriptors compression, which\nis a key ingredient of large-scale image retrieval systems. While the deep\nlearning machinery has benefited literally all computer vision pipelines, the\nexisting state-of-the-art compression methods employ shallow architectures, and\nwe aim to close this gap by our paper. In more detail, we introduce a DNN\narchitecture for the unsupervised compressed-domain retrieval, based on\nmulti-codebook quantization. The proposed architecture is designed to\nincorporate both fast data encoding and efficient distances computation via\nlookup tables. We demonstrate the exceptional advantage of our scheme over\nexisting quantization approaches on several datasets of visual descriptors via\noutperforming the previous state-of-the-art by a large margin.</p>\n", "tags": ["ICCV","Similarity-Search","Image-Retrieval","Datasets","Quantization","Unsupervised","Scalability"] },
{"key": "morère2016group", "year": "2016", "citations": "6", "title":"Group Invariant Deep Representations for Image Instance Retrieval", "abstract": "<p>Most image instance retrieval pipelines are based on comparison of vectors\nknown as global image descriptors between a query image and the database\nimages. Due to their success in large scale image classification,\nrepresentations extracted from Convolutional Neural Networks (CNN) are quickly\ngaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors\nfor image instance retrieval. While CNN-based descriptors are generally\nremarked for good retrieval performance at lower bitrates, they nevertheless\npresent a number of drawbacks including the lack of robustness to common object\ntransformations such as rotations compared with their interest point based FV\ncounterparts.\n  In this paper, we propose a method for computing invariant global descriptors\nfrom CNNs. Our method implements a recently proposed mathematical theory for\ninvariance in a sensory cortex modeled as a feedforward neural network. The\nresulting global descriptors can be made invariant to multiple arbitrary\ntransformation groups while retaining good discriminativeness.\n  Based on a thorough empirical evaluation using several publicly available\ndatasets, we show that our method is able to significantly and consistently\nimprove retrieval results every time a new type of invariance is incorporated.\nWe also show that our method which has few parameters is not prone to\noverfitting: improvements generalize well across datasets with different\nproperties with regard to invariances. Finally, we show that our descriptors\nare able to compare favourably to other state-of-the-art compact descriptors in\nsimilar bitranges, exceeding the highest retrieval results reported in the\nliterature on some datasets. A dedicated dimensionality reduction step\n–quantization or hashing– may be able to further improve the competitiveness\nof the descriptors.</p>\n", "tags": ["Hashing-Methods","Datasets","Quantization","Evaluation","Robustness"] },
{"key": "morère2016nested", "year": "2017", "citations": "18", "title":"Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval", "abstract": "<p>The goal of this work is the computation of very compact binary hashes for\nimage instance retrieval. Our approach has two novel contributions. The first\none is Nested Invariance Pooling (NIP), a method inspired from i-theory, a\nmathematical theory for computing group invariant transformations with\nfeed-forward neural networks. NIP is able to produce compact and\nwell-performing descriptors with visual representations extracted from\nconvolutional neural networks. We specifically incorporate scale, translation\nand rotation invariances but the scheme can be extended to any arbitrary sets\nof transformations. We also show that using moments of increasing order\nthroughout nesting is important. The NIP descriptors are then hashed to the\ntarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novel\nbatch-level regularization scheme specifically designed for the purpose of\nhashing (RBMH). A thorough empirical evaluation with state-of-the-art shows\nthat the results obtained both with the NIP descriptors and the NIP+RBMH hashes\nare consistently outstanding across a wide range of datasets.</p>\n", "tags": ["Evaluation","Datasets","Hashing-Methods","Multimodal-Retrieval"] },
{"key": "moskvyak2020keypoint", "year": "2021", "citations": "22", "title":"Keypoint-Aligned Embeddings for Image Retrieval and Re-identification", "abstract": "<p>Learning embeddings that are invariant to the pose of the object is crucial\nin visual image retrieval and re-identification. The existing approaches for\nperson, vehicle, or animal re-identification tasks suffer from high intra-class\nvariance due to deformable shapes and different camera viewpoints. To overcome\nthis limitation, we propose to align the image embedding with a predefined\norder of the keypoints. The proposed keypoint aligned embeddings model\n(KAE-Net) learns part-level features via multi-task learning which is guided by\nkeypoint locations. More specifically, KAE-Net extracts channels from a feature\nmap activated by a specific keypoint through learning the auxiliary task of\nheatmap reconstruction for this keypoint. The KAE-Net is compact, generic and\nconceptually simple. It achieves state of the art performance on the benchmark\ndatasets of CUB-200-2011, Cars196 and VeRi-776 for retrieval and\nre-identification tasks.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "moulton2018maximally", "year": "2018", "citations": "15", "title":"Maximally Consistent Sampling and the Jaccard Index of Probability Distributions", "abstract": "<p>We introduce simple, efficient algorithms for computing a MinHash of a\nprobability distribution, suitable for both sparse and dense data, with\nequivalent running times to the state of the art for both cases. The collision\nprobability of these algorithms is a new measure of the similarity of positive\nvectors which we investigate in detail. We describe the sense in which this\ncollision probability is optimal for any Locality Sensitive Hash based on\nsampling. We argue that this similarity measure is more useful for probability\ndistributions than the similarity pursued by other algorithms for weighted\nMinHash, and is the natural generalization of the Jaccard index.</p>\n", "tags": ["Locality-Sensitive-Hashing"] },
{"key": "mu2018towards", "year": "2018", "citations": "11", "title":"Towards Practical Visual Search Engine within Elasticsearch", "abstract": "<p>In this paper, we describe our end-to-end content-based image retrieval\nsystem built upon Elasticsearch, a well-known and popular textual search\nengine. As far as we know, this is the first time such a system has been\nimplemented in eCommerce, and our efforts have turned out to be highly\nworthwhile. We end up with a novel and exciting visual search solution that is\nextremely easy to be deployed, distributed, scaled and monitored in a\ncost-friendly manner. Moreover, our platform is intrinsically flexible in\nsupporting multimodal searches, where visual and textual information can be\njointly leveraged in retrieval.\n  The core idea is to encode image feature vectors into a collection of string\ntokens in a way such that closer vectors will share more string tokens in\ncommon. By doing that, we can utilize Elasticsearch to efficiently retrieve\nsimilar images based on similarities within encoded sting tokens. As part of\nthe development, we propose a novel vector to string encoding method, which is\nshown to substantially outperform the previous ones in terms of both precision\nand latency.\n  First-hand experiences in implementing this Elasticsearch-based platform are\nextensively addressed, which should be valuable to practitioners also\ninterested in building visual search engine on top of Elasticsearch.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "mukherjee2015nmf", "year": "2015", "citations": "14", "title":"An NMF perspective on Binary Hashing", "abstract": "<p>The pervasiveness of massive data repositories has led\nto much interest in efficient methods for indexing, search,\nand retrieval. For image data, a rapidly developing body of\nwork for these applications shows impressive performance\nwith methods that broadly fall under the umbrella term of\nBinary Hashing. Given a distance matrix, a binary hashing\nalgorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the\noriginal distances. The formulation is non-convex — so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective\nthat is numerically more tractable. In this paper, we first\nderive an Augmented Lagrangian approach to optimize the\nstandard binary Hashing objective (i.e., maintain fidelity\nwith a given distance matrix). With appropriate step sizes,\nwe find that this scheme already yields results that match or\nsubstantially outperform state of the art methods on most\nbenchmarks used in the literature. Then, to allow the model\nto scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm — whose parallelization properties\nare exploited to obtain a fast GPU based implementation.\nWe give a probabilistic analysis of our initialization scheme\nand present a range of experiments to show that the method\nis simple to implement and competes favorably with available methods (both for optimization and generalization).</p>\n", "tags": ["ICCV","Datasets","Compact-Codes","Hashing-Methods","Evaluation","Quantization"] },
{"key": "mukherjee2025nmf", "year": "2015", "citations": "14", "title":"An NMF perspective on Binary Hashing", "abstract": "<p>The pervasiveness of massive data repositories has led\nto much interest in efficient methods for indexing, search,\nand retrieval. For image data, a rapidly developing body of\nwork for these applications shows impressive performance\nwith methods that broadly fall under the umbrella term of\nBinary Hashing. Given a distance matrix, a binary hashing\nalgorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the\noriginal distances. The formulation is non-convex — so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective\nthat is numerically more tractable. In this paper, we first\nderive an Augmented Lagrangian approach to optimize the\nstandard binary Hashing objective (i.e., maintain fidelity\nwith a given distance matrix). With appropriate step sizes,\nwe find that this scheme already yields results that match or\nsubstantially outperform state of the art methods on most\nbenchmarks used in the literature. Then, to allow the model\nto scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm — whose parallelization properties\nare exploited to obtain a fast GPU based implementation.\nWe give a probabilistic analysis of our initialization scheme\nand present a range of experiments to show that the method\nis simple to implement and competes favorably with available methods (both for optimization and generalization).</p>\n", "tags": ["ICCV","Datasets","Compact-Codes","Hashing-Methods","Evaluation","Quantization"] },
{"key": "murray2016interferences", "year": "2016", "citations": "30", "title":"Interferences in match kernels", "abstract": "<p>We consider the design of an image representation that embeds and aggregates\na set of local descriptors into a single vector. Popular representations of\nthis kind include the bag-of-visual-words, the Fisher vector and the VLAD. When\ntwo such image representations are compared with the dot-product, the\nimage-to-image similarity can be interpreted as a match kernel. In match\nkernels, one has to deal with interference, i.e. with the fact that even if two\ndescriptors are unrelated, their matching score may contribute to the overall\nsimilarity.\n  We formalise this problem and propose two related solutions, both aimed at\nequalising the individual contributions of the local descriptors in the final\nrepresentation. These methods modify the aggregation stage by including a set\nof per-descriptor weights. They differ by the objective function that is\noptimised to compute those weights. The first is a “democratisation” strategy\nthat aims at equalising the relative importance of each descriptor in the set\ncomparison metric. The second one involves equalising the match of a single\ndescriptor to the aggregated vector.\n  These concurrent methods give a substantial performance boost over the state\nof the art in image search with short or mid-size vectors, as demonstrated by\nour experiments on standard public image retrieval benchmarks.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "nawaz2018revisiting", "year": "2018", "citations": "5", "title":"Revisiting Cross Modal Retrieval", "abstract": "<p>This paper proposes a cross-modal retrieval system that leverages on image\nand text encoding. Most multimodal architectures employ separate networks for\neach modality to capture the semantic relationship between them. However, in\nour work image-text encoding can achieve comparable results in terms of\ncross-modal retrieval without having to use a separate network for each\nmodality. We show that text encodings can capture semantic relationships\nbetween multiple modalities. In our knowledge, this work is the first of its\nkind in terms of employing a single network and fused image-text embedding for\ncross-modal retrieval. We evaluate our approach on two famous multimodal\ndatasets: MS-COCO and Flickr30K.</p>\n", "tags": ["Datasets","Multimodal-Retrieval"] },
{"key": "nawaz2019do", "year": "2019", "citations": "9", "title":"Do Cross Modal Systems Leverage Semantic Relationships?", "abstract": "<p>Current cross-modal retrieval systems are evaluated using R@K measure which\ndoes not leverage semantic relationships rather strictly follows the manually\nmarked image text query pairs. Therefore, current systems do not generalize\nwell for the unseen data in the wild. To handle this, we propose a new measure,\nSemanticMap, to evaluate the performance of cross-modal systems. Our proposed\nmeasure evaluates the semantic similarity between the image and text\nrepresentations in the latent embedding space. We also propose a novel\ncross-modal retrieval system using a single stream network for bidirectional\nretrieval. The proposed system is based on a deep neural network trained using\nextended center loss, minimizing the distance of image and text descriptions in\nthe latent space from the class centers. In our system, the text descriptions\nare also encoded as images which enabled us to use a single stream network for\nboth text and images. To the best of our knowledge, our work is the first of\nits kind in terms of employing a single stream network for cross-modal\nretrieval systems. The proposed system is evaluated on two publicly available\ndatasets including MSCOCO and Flickr30K and has shown comparable results to the\ncurrent state-of-the-art methods.</p>\n", "tags": ["ICCV","Datasets","Evaluation","Multimodal-Retrieval"] },
{"key": "neculai2022probabilistic", "year": "2022", "citations": "21", "title":"Probabilistic Compositional Embeddings for Multimodal Image Retrieval", "abstract": "<p>Existing works in image retrieval often consider retrieving images with one\nor two query inputs, which do not generalize to multiple queries. In this work,\nwe investigate a more challenging scenario for composing multiple multimodal\nqueries in image retrieval. Given an arbitrary number of query images and (or)\ntexts, our goal is to retrieve target images containing the semantic concepts\nspecified in multiple multimodal queries. To learn an informative embedding\nthat can flexibly encode the semantics of various queries, we propose a novel\nmultimodal probabilistic composer (MPC). Specifically, we model input images\nand texts as probabilistic embeddings, which can be further composed by a\nprobabilistic composition rule to facilitate image retrieval with multiple\nmultimodal queries. We propose a new benchmark based on the MS-COCO dataset and\nevaluate our model on various setups that compose multiple images and (or) text\nqueries for multimodal image retrieval. Without bells and whistles, we show\nthat our probabilistic model formulation significantly outperforms existing\nrelated methods on multimodal image retrieval while generalizing well to query\nwith different amounts of inputs given in arbitrary visual and (or) textual\nmodalities. Code is available here: https://github.com/andreineculai/MPC.</p>\n", "tags": ["Datasets","CVPR","Evaluation","Image-Retrieval"] },
{"key": "nedelec2017specializing", "year": "2017", "citations": "17", "title":"Specializing Joint Representations for the task of Product Recommendation", "abstract": "<p>We propose a unified product embedded representation that is optimized for\nthe task of retrieval-based product recommendation. To this end, we introduce a\nnew way to fuse modality-specific product embeddings into a joint product\nembedding, in order to leverage both product content information, such as\ntextual descriptions and images, and product collaborative filtering signal. By\nintroducing the fusion step at the very end of our architecture, we are able to\ntrain each modality separately, allowing us to keep a modular architecture that\nis preferable in real-world recommendation deployments. We analyze our\nperformance on normal and hard recommendation setups such as cold-start and\ncross-category recommendations and achieve good performance on a large product\nshopping dataset.</p>\n", "tags": ["Datasets","Recommender-Systems","Evaluation"] },
{"key": "neelakantan2022text", "year": "2022", "citations": "110", "title":"Text and Code Embeddings by Contrastive Pre-Training", "abstract": "<p>Text embeddings are useful features in many applications such as semantic\nsearch and computing text similarity. Previous work typically trains models\ncustomized for different use cases, varying in dataset choice, training\nobjective and model architecture. In this work, we show that contrastive\npre-training on unsupervised data at scale leads to high quality vector\nrepresentations of text and code. The same unsupervised text embeddings that\nachieve new state-of-the-art results in linear-probe classification also\ndisplay impressive semantic search capabilities and sometimes even perform\ncompetitively with fine-tuned models. On linear-probe classification accuracy\naveraging over 7 tasks, our best unsupervised model achieves a relative\nimprovement of 4% and 1.8% over previous best unsupervised and supervised text\nembedding models respectively. The same text embeddings when evaluated on\nlarge-scale semantic search attains a relative improvement of 23.4%, 14.7%, and\n10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and\nTriviaQA benchmarks, respectively. Similarly to text embeddings, we train code\nembedding models on (text, code) pairs, obtaining a 20.8% relative improvement\nover prior best work on code search.</p>\n", "tags": ["Unsupervised","Scalability","Supervised","Datasets"] },
{"key": "neyshabur2013power", "year": "2013", "citations": "57", "title":"The Power of Asymmetry in Binary Hashing", "abstract": "<p>When approximating binary similarity using the hamming distance between short\nbinary hashes, we show that even if the similarity is symmetric, we can have\nshorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\n0\nas the hamming distance between f(x)\nand g(x0), for two distinct binary codes f, g, rather than as the hamming distance\nbetween f(x) and f(x0).</p>\n", "tags": ["Hashing-Methods","Compact-Codes"] },
{"key": "neyshabur2025power", "year": "2013", "citations": "57", "title":"The Power of Asymmetry in Binary Hashing", "abstract": "<p>When approximating binary similarity using the hamming distance between short\nbinary hashes, we show that even if the similarity is symmetric, we can have\nshorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\n0\nas the hamming distance between f(x)\nand g(x0), for two distinct binary codes f, g, rather than as the hamming distance\nbetween f(x) and f(x0).</p>\n", "tags": ["Hashing-Methods","Compact-Codes"] },
{"key": "nguyen2021deep", "year": "2021", "citations": "24", "title":"A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval", "abstract": "<p>Conventional approaches to image-text retrieval mainly focus on indexing\nvisual objects appearing in pictures but ignore the interactions between these\nobjects. Such objects occurrences and interactions are equivalently useful and\nimportant in this field as they are usually mentioned in the text. Scene graph\npresentation is a suitable method for the image-text matching challenge and\nobtained good results due to its ability to capture the inter-relationship\ninformation. Both images and text are represented in scene graph levels and\nformulate the retrieval challenge as a scene graph matching challenge. In this\npaper, we introduce the Local and Global Scene Graph Matching (LGSGM) model\nthat enhances the state-of-the-art method by integrating an extra graph\nconvolution network to capture the general information of a graph.\nSpecifically, for a pair of scene graphs of an image and its caption, two\nseparate models are used to learn the features of each graph’s nodes and edges.\nThen a Siamese-structure graph convolution model is employed to embed graphs\ninto vector forms. We finally combine the graph-level and the vector-level to\ncalculate the similarity of this image-text pair. The empirical experiments\nshow that our enhancement with the combination of levels can improve the\nperformance of the baseline method by increasing the recall by more than 10% on\nthe Flickr30k dataset.</p>\n", "tags": ["Datasets","Text-Retrieval","Evaluation"] },
{"key": "ning2016scalable", "year": "2016", "citations": "40", "title":"Scalable Image Retrieval by Sparse Product Quantization", "abstract": "<p>Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional\nfeature indexing and retrieval is the crux of large-scale image retrieval. A\nrecent promising technique is Product Quantization, which attempts to index\nhigh-dimensional image features by decomposing the feature space into a\nCartesian product of low dimensional subspaces and quantizing each of them\nseparately. Despite the promising results reported, their quantization approach\nfollows the typical hard assignment of traditional quantization methods, which\nmay result in large quantization errors and thus inferior search performance.\nUnlike the existing approaches, in this paper, we propose a novel approach\ncalled Sparse Product Quantization (SPQ) to encoding the high-dimensional\nfeature vectors into sparse representation. We optimize the sparse\nrepresentations of the feature vectors by minimizing their quantization errors,\nmaking the resulting representation is essentially close to the original data\nin practice. Experiments show that the proposed SPQ technique is not only able\nto compress data, but also an effective encoding technique. We obtain\nstate-of-the-art results for ANN search on four public image datasets and the\npromising results of content-based image retrieval further validate the\nefficacy of our proposed method.</p>\n", "tags": ["Similarity-Search","Image-Retrieval","Datasets","Scalability","Quantization","Evaluation"] },
{"key": "noh2016large", "year": "2017", "citations": "812", "title":"Large-Scale Image Retrieval with Attentive Deep Local Features", "abstract": "<p>We propose an attentive local feature descriptor suitable for large-scale\nimage retrieval, referred to as DELF (DEep Local Feature). The new feature is\nbased on convolutional neural networks, which are trained only with image-level\nannotations on a landmark image dataset. To identify semantically useful local\nfeatures for image retrieval, we also propose an attention mechanism for\nkeypoint selection, which shares most network layers with the descriptor. This\nframework can be used for image retrieval as a drop-in replacement for other\nkeypoint detectors and descriptors, enabling more accurate feature matching and\ngeometric verification. Our system produces reliable confidence scores to\nreject false positives—in particular, it is robust against queries that have\nno correct match in the database. To evaluate the proposed descriptor, we\nintroduce a new large-scale dataset, referred to as Google-Landmarks dataset,\nwhich involves challenges in both database and query such as background\nclutter, partial occlusion, multiple landmarks, objects in variable scales,\netc. We show that DELF outperforms the state-of-the-art global and local\ndescriptors in the large-scale setting by significant margins. Code and dataset\ncan be found at the project webpage:\nhttps://github.com/tensorflow/models/tree/master/research/delf .</p>\n", "tags": ["ICCV","Tools-&-Libraries","Image-Retrieval","Datasets","Scalability"] },
{"key": "norouzi2012hamming", "year": "2012", "citations": "540", "title":"Hamming Distance Metric Learning", "abstract": "<p>Motivated by large-scale multimedia applications we propose to learn mappings\nfrom high-dimensional data to binary codes that preserve semantic similarity.\nBinary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable\nto broad families of mappings, and uses a flexible form of triplet ranking loss.\nWe overcome discontinuous optimization of the discrete mappings by minimizing\na piecewise-smooth upper bound on empirical loss, inspired by latent structural\nSVMs. We develop a new loss-augmented inference algorithm that is quadratic in\nthe code length. We show strong retrieval performance on CIFAR-10 and MNIST,\nwith promising classification results using no more than kNN on the binary codes.</p>\n", "tags": ["Scalability","Distance-Metric-Learning","Tools-&-Libraries","Compact-Codes","Similarity-Search","Evaluation"] },
{"key": "norouzi2025hamming", "year": "2012", "citations": "540", "title":"Hamming Distance Metric Learning", "abstract": "<p>Motivated by large-scale multimedia applications we propose to learn mappings\nfrom high-dimensional data to binary codes that preserve semantic similarity.\nBinary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable\nto broad families of mappings, and uses a flexible form of triplet ranking loss.\nWe overcome discontinuous optimization of the discrete mappings by minimizing\na piecewise-smooth upper bound on empirical loss, inspired by latent structural\nSVMs. We develop a new loss-augmented inference algorithm that is quadratic in\nthe code length. We show strong retrieval performance on CIFAR-10 and MNIST,\nwith promising classification results using no more than kNN on the binary codes.</p>\n", "tags": ["Scalability","Distance-Metric-Learning","Tools-&-Libraries","Compact-Codes","Similarity-Search","Evaluation"] },
{"key": "novotný2018implementation", "year": "2018", "citations": "27", "title":"Implementation Notes for the Soft Cosine Measure", "abstract": "<p>The standard bag-of-words vector space model (VSM) is efficient, and\nubiquitous in information retrieval, but it underestimates the similarity of\ndocuments with the same meaning, but different terminology. To overcome this\nlimitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that\nincorporates term similarity relations. Charlet and Damnati showed that the SCM\nis highly effective in question answering (QA) systems. However, the\northonormalization algorithm proposed by Sidorov et al. has an impractical time\ncomplexity of \\(\\mathcal O(n^4)\\), where n is the size of the vocabulary.\n  In this paper, we prove a tighter lower worst-case time complexity bound of\n\\(\\mathcal O(n^3)\\). We also present an algorithm for computing the similarity\nbetween documents and we show that its worst-case time complexity is \\(\\mathcal\nO(1)\\) given realistic conditions. Lastly, we describe implementation in\ngeneral-purpose vector databases such as Annoy, and Faiss and in the inverted\nindices of text search engines such as Apache Lucene, and ElasticSearch. Our\nresults enable the deployment of the SCM in real-world information retrieval\nsystems.</p>\n", "tags": ["CIKM","Text-Retrieval","Tools-&-Libraries"] },
{"key": "ong2017siamese", "year": "2017", "citations": "37", "title":"Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval", "abstract": "<p>This paper addresses the problem of large scale image retrieval, with the aim\nof accurately ranking the similarity of a large number of images to a given\nquery image. To achieve this, we propose a novel Siamese network. This network\nconsists of two computational strands, each comprising of a CNN component\nfollowed by a Fisher vector component. The CNN component produces dense, deep\nconvolutional descriptors that are then aggregated by the Fisher Vector method.\nCrucially, we propose to simultaneously learn both the CNN filter weights and\nFisher Vector model parameters. This allows us to account for the evolving\ndistribution of deep descriptors over the course of the learning process. We\nshow that the proposed approach gives significant improvements over the\nstate-of-the-art methods on the Oxford and Paris image retrieval datasets.\nAdditionally, we provide a baseline performance measure for both these datasets\nwith the inclusion of 1 million distractors.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "ortega2022unconventional", "year": "2022", "citations": "5", "title":"Unconventional application of k-means for distributed approximate similarity search", "abstract": "<p>Similarity search based on a distance function in metric spaces is a\nfundamental problem for many applications. Queries for similar objects lead to\nthe well-known machine learning task of nearest-neighbours identification. Many\ndata indexing strategies, collectively known as Metric Access Methods (MAM),\nhave been proposed to speed up queries for similar elements in this context.\nMoreover, since exact approaches to solve similarity queries can be complex and\ntime-consuming, alternative options have appeared to reduce query execution\ntime, such as returning approximate results or resorting to distributed\ncomputing platforms. In this paper, we introduce MASK (Multilevel Approximate\nSimilarity search with \\(k\\)-means), an unconventional application of the\n\\(k\\)-means algorithm as the foundation of a multilevel index structure for\napproximate similarity search, suitable for metric spaces. We show that\ninherent properties of \\(k\\)-means, like representing high-density data areas\nwith fewer prototypes, can be leveraged for this purpose. An implementation of\nthis new indexing method is evaluated, using a synthetic dataset and a\nreal-world dataset in a high-dimensional and high-sparsity space. Results are\npromising and underpin the applicability of this novel indexing method in\nmultiple domains.</p>\n", "tags": ["Similarity-Search","Datasets","Vector-Indexing"] },
{"key": "otani2016learning", "year": "2016", "citations": "92", "title":"Learning Joint Representations of Videos and Sentences with Web Image Search", "abstract": "<p>Our objective is video retrieval based on natural language queries. In\naddition, we consider the analogous problem of retrieving sentences or\ngenerating descriptions given an input video. Recent work has addressed the\nproblem by embedding visual and textual inputs into a common space where\nsemantic similarities correlate to distances. We also adopt the embedding\napproach, and make the following contributions: First, we utilize web image\nsearch in sentence embedding process to disambiguate fine-grained visual\nconcepts. Second, we propose embedding models for sentence, image, and video\ninputs whose parameters are learned simultaneously. Finally, we show how the\nproposed model can be applied to description generation. Overall, we observe a\nclear improvement over the state-of-the-art methods in the video and sentence\nretrieval tasks. In description generation, the performance level is comparable\nto the current state-of-the-art, although our embeddings were trained for the\nretrieval tasks.</p>\n", "tags": ["Video-Retrieval","Evaluation"] },
{"key": "oymak2016near", "year": "2017", "citations": "13", "title":"Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding", "abstract": "<p>Binary embedding is the problem of mapping points from a high-dimensional\nspace to a Hamming cube in lower dimension while preserving pairwise distances.\nAn efficient way to accomplish this is to make use of fast embedding techniques\ninvolving Fourier transform e.g.~circulant matrices. While binary embedding has\nbeen studied extensively, theoretical results on fast binary embedding are\nrather limited. In this work, we build upon the recent literature to obtain\nsignificantly better dependencies on the problem parameters. A set of \\(N\\)\npoints in \\(\\mathbb{R}^n\\) can be properly embedded into the Hamming cube \\(\\{\\pm\n1\\}^k\\) with \\(\\delta\\) distortion, by using \\(k\\sim\\delta^{-3}log N\\) samples\nwhich is optimal in the number of points \\(N\\) and compares well with the optimal\ndistortion dependency \\(\\delta^{-2}\\). Our optimal embedding result applies in\nthe regime \\(log N\\lesssim n^{1/3}\\). Furthermore, if the looser condition \\(log\nN\\lesssim \\sqrt{n}\\) holds, we show that all but an arbitrarily small fraction\nof the points can be optimally embedded. We believe our techniques can be\nuseful to obtain improved guarantees for other nonlinear embedding problems.</p>\n", "tags": ["Hashing-Methods","ICASSP"] },
{"key": "oymak2017near", "year": "2017", "citations": "13", "title":"Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding", "abstract": "<p>Binary embedding is the problem of mapping points from a high-dimensional\nspace to a Hamming cube in lower dimension while preserving pairwise distances.\nAn efficient way to accomplish this is to make use of fast embedding techniques\ninvolving Fourier transform e.g.~circulant matrices. While binary embedding has\nbeen studied extensively, theoretical results on fast binary embedding are\nrather limited. In this work, we build upon the recent literature to obtain\nsignificantly better dependencies on the problem parameters. A set of \\(N\\)\npoints in \\(\\mathbb{R}^n\\) can be properly embedded into the Hamming cube \\(\\{\\pm\n1\\}^k\\) with \\(\\delta\\) distortion, by using \\(k\\sim\\delta^{-3}log N\\) samples\nwhich is optimal in the number of points \\(N\\) and compares well with the optimal\ndistortion dependency \\(\\delta^{-2}\\). Our optimal embedding result applies in\nthe regime \\(log N\\lesssim n^{1/3}\\). Furthermore, if the looser condition \\(log\nN\\lesssim \\sqrt{n}\\) holds, we show that all but an arbitrarily small fraction\nof the points can be optimally embedded. We believe our techniques can be\nuseful to obtain improved guarantees for other nonlinear embedding problems.</p>\n", "tags": ["Hashing-Methods","ICASSP"] },
{"key": "pachori2017hashing", "year": "2017", "citations": "24", "title":"Hashing in the Zero Shot Framework with Domain Adaptation", "abstract": "<p>Techniques to learn hash codes which can store and retrieve large dimensional\nmultimedia data efficiently have attracted broad research interests in the\nrecent years. With rapid explosion of newly emerged concepts and online data,\nexisting supervised hashing algorithms suffer from the problem of scarcity of\nground truth annotations due to the high cost of obtaining manual annotations.\nTherefore, we propose an algorithm to learn a hash function from training\nimages belonging to <code class=\"language-plaintext highlighter-rouge\">seen' classes which can efficiently encode images of\n</code>unseen’ classes to binary codes. Specifically, we project the image features\nfrom visual space and semantic features from semantic space into a common\nHamming subspace. Earlier works to generate hash codes have tried to relax the\ndiscrete constraints on hash codes and solve the continuous optimization\nproblem. However, it often leads to quantization errors. In this work, we use\nthe max-margin classifier to learn an efficient hash function. To address the\nconcern of domain-shift which may arise due to the introduction of new classes,\nwe also introduce an unsupervised domain adaptation model in the proposed\nhashing framework. Results on the three datasets show the advantage of using\ndomain adaptation in learning a high-quality hash function and superiority of\nour method for the task of image retrieval performance as compared to several\nstate-of-the-art hashing methods.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Unsupervised","Quantization","Evaluation"] },
{"key": "pacuk2016locality", "year": "2016", "citations": "7", "title":"Locality-Sensitive Hashing without False Negatives for l_p", "abstract": "<p>In this paper, we show a construction of locality-sensitive hash functions\nwithout false negatives, i.e., which ensure collision for every pair of points\nwithin a given radius \\(R\\) in \\(d\\) dimensional space equipped with \\(l_p\\) norm\nwhen \\(p \\in [1,\\infty]\\). Furthermore, we show how to use these hash functions\nto solve the \\(c\\)-approximate nearest neighbor search problem without false\nnegatives. Namely, if there is a point at distance \\(R\\), we will certainly\nreport it and points at distance greater than \\(cR\\) will not be reported for\n\\(c=Ω(\\sqrt{d},d^{1-\\frac{1}{p}})\\). The constructed algorithms work: - with\npreprocessing time \\(\\mathcal{O}(n log(n))\\) and sublinear expected query time,</p>\n<ul>\n  <li>with preprocessing time \\(\\mathcal{O}(\\mathrm{poly}(n))\\) and expected query\ntime \\(\\mathcal{O}(log(n))\\). Our paper reports progress on answering the open\nproblem presented by Pagh [8] who considered the nearest neighbor search\nwithout false negatives for the Hamming distance.</li>\n</ul>\n", "tags": ["Hashing-Methods","Efficiency"] },
{"key": "pagh2016approximate", "year": "2016", "citations": "10", "title":"Approximate Furthest Neighbor with Application to Annulus Query", "abstract": "<p>Much recent work has been devoted to approximate nearest neighbor queries.\nMotivated by applications in recommender systems, we consider approximate\nfurthest neighbor (AFN) queries and present a simple, fast, and highly\npractical data structure for answering AFN queries in high- dimensional\nEuclidean space. The method builds on the technique of In- dyk (SODA 2003),\nstoring random projections to provide sublinear query time for AFN. However, we\nintroduce a different query algorithm, improving on Indyk’s approximation\nfactor and reducing the running time by a logarithmic factor. We also present a\nvariation based on a query- independent ordering of the database points; while\nthis does not have the provable approximation factor of the query-dependent\ndata structure, it offers significant improvement in time and space complexity.\nWe give a theoretical analysis, and experimental results. As an application,\nthe query-dependent approach is used for deriving a data structure for the\napproximate annulus query problem, which is defined as follows: given an input\nset S and two parameters r &gt; 0 and w &gt;= 1, construct a data structure that\nreturns for each query point q a point p in S such that the distance between p\nand q is at least r/w and at most wr.</p>\n", "tags": ["Recommender-Systems","Locality-Sensitive-Hashing","Efficiency"] },
{"key": "pang2018deep", "year": "2018", "citations": "40", "title":"Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for Image Retrieval", "abstract": "<p>Image retrieval based on deep convolutional features has demonstrated\nstate-of-the-art performance in popular benchmarks. In this paper, we present a\nunified solution to address deep convolutional feature aggregation and image\nre-ranking by simulating the dynamics of heat diffusion. A distinctive problem\nin image retrieval is that repetitive or <em>bursty</em> features tend to\ndominate final image representations, resulting in representations less\ndistinguishable. We show that by considering each deep feature as a heat\nsource, our unsupervised aggregation method is able to avoid\nover-representation of <em>bursty</em> features. We additionally provide a\npractical solution for the proposed aggregation method and further show the\nefficiency of our method in experimental evaluation. Inspired by the\naforementioned deep feature aggregation method, we also propose a method to\nre-rank a number of top ranked images for a given query image by considering\nthe query as the heat source. Finally, we extensively evaluate the proposed\napproach with pre-trained and fine-tuned deep networks on common public\nbenchmarks and show superior performance compared to previous work.</p>\n", "tags": ["Image-Retrieval","Hybrid-ANN-Methods","Re-Ranking","Unsupervised","Evaluation","Efficiency"] },
{"key": "paolicelli2022learning", "year": "2022", "citations": "10", "title":"Learning Semantics for Visual Place Recognition through Multi-Scale Attention", "abstract": "<p>In this paper we address the task of visual place recognition (VPR), where\nthe goal is to retrieve the correct GPS coordinates of a given query image\nagainst a huge geotagged gallery. While recent works have shown that building\ndescriptors incorporating semantic and appearance information is beneficial,\ncurrent state-of-the-art methods opt for a top down definition of the\nsignificant semantic content. Here we present the first VPR algorithm that\nlearns robust global embeddings from both visual appearance and semantic\ncontent of the data, with the segmentation process being dynamically guided by\nthe recognition of places through a multi-scale attention module. Experiments\non various scenarios validate this new approach and demonstrate its performance\nagainst state-of-the-art methods. Finally, we propose the first synthetic-world\ndataset suited for both place recognition and segmentation tasks.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "paria2020minimizing", "year": "2020", "citations": "14", "title":"Minimizing FLOPs to Learn Efficient Sparse Representations", "abstract": "<p>Deep representation learning has become one of the most widely adopted\napproaches for visual search, recommendation, and identification. Retrieval of\nsuch representations from a large database is however computationally\nchallenging. Approximate methods based on learning compact representations,\nhave been widely explored for this problem, such as locality sensitive hashing,\nproduct quantization, and PCA. In this work, in contrast to learning compact\nrepresentations, we propose to learn high dimensional and sparse\nrepresentations that have similar representational capacity as dense embeddings\nwhile being more efficient due to sparse matrix multiplication operations which\ncan be much faster than dense multiplication. Following the key insight that\nthe number of operations decreases quadratically with the sparsity of\nembeddings provided the non-zero entries are distributed uniformly across\ndimensions, we propose a novel approach to learn such distributed sparse\nembeddings via the use of a carefully constructed regularization function that\ndirectly minimizes a continuous relaxation of the number of floating-point\noperations (FLOPs) incurred during retrieval. Our experiments show that our\napproach is competitive to the other baselines and yields a similar or better\nspeed-vs-accuracy tradeoff on practical datasets.</p>\n", "tags": ["Locality-Sensitive-Hashing","Image-Retrieval","Hashing-Methods","Datasets","Recommender-Systems","Quantization"] },
{"key": "park2021unsupervised", "year": "2021", "citations": "29", "title":"Unsupervised Representation Learning via Neural Activation Coding", "abstract": "<p>We present neural activation coding (NAC) as a novel approach for learning\ndeep representations from unlabeled data for downstream applications. We argue\nthat the deep encoder should maximize its nonlinear expressivity on the data\nfor downstream predictors to take full advantage of its representation power.\nTo this end, NAC maximizes the mutual information between activation patterns\nof the encoder and the data over a noisy communication channel. We show that\nlearning for a noise-robust activation code increases the number of distinct\nlinear regions of ReLU encoders, hence the maximum nonlinear expressivity. More\ninterestingly, NAC learns both continuous and discrete representations of data,\nwhich we respectively evaluate on two downstream tasks: (i) linear\nclassification on CIFAR-10 and ImageNet-1K and (ii) nearest neighbor retrieval\non CIFAR-10 and FLICKR-25K. Empirical results show that NAC attains better or\ncomparable performance on both tasks over recent baselines including SimCLR and\nDistillHash. In addition, NAC pretraining provides significant benefits to the\ntraining of deep generative models. Our code is available at\nhttps://github.com/yookoon/nac.</p>\n", "tags": ["CVPR","Self-Supervised","Unsupervised","Evaluation"] },
{"key": "patel2019self", "year": "2019", "citations": "7", "title":"Self-Supervised Visual Representations for Cross-Modal Retrieval", "abstract": "<p>Cross-modal retrieval methods have been significantly improved in last years\nwith the use of deep neural networks and large-scale annotated datasets such as\nImageNet and Places. However, collecting and annotating such datasets requires\na tremendous amount of human effort and, besides, their annotations are usually\nlimited to discrete sets of popular visual classes that may not be\nrepresentative of the richer semantics found on large-scale cross-modal\nretrieval datasets. In this paper, we present a self-supervised cross-modal\nretrieval framework that leverages as training data the correlations between\nimages and text on the entire set of Wikipedia articles. Our method consists in\ntraining a CNN to predict: (1) the semantic context of the article in which an\nimage is more probable to appear as an illustration (global context), and (2)\nthe semantic context of its caption (local context). Our experiments\ndemonstrate that the proposed method is not only capable of learning\ndiscriminative visual representations for solving vision tasks like image\nclassification and object detection, but that the learned representations are\nbetter for cross-modal retrieval when compared to supervised pre-training of\nthe network on the ImageNet dataset.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Datasets","Self-Supervised","Scalability","Multimodal-Retrieval"] },
{"key": "patel2019tinysearch", "year": "2019", "citations": "7", "title":"TinySearch -- Semantics based Search Engine using Bert Embeddings", "abstract": "<p>Existing search engines use keyword matching or tf-idf based matching to map\nthe query to the web-documents and rank them. They also consider other factors\nsuch as page rank, hubs-and-authority scores, knowledge graphs to make the\nresults more meaningful. However, the existing search engines fail to capture\nthe meaning of query when it becomes large and complex. BERT, introduced by\nGoogle in 2018, provides embeddings for words as well as sentences. In this\npaper, I have developed a semantics-oriented search engine using neural\nnetworks and BERT embeddings that can search for query and rank the documents\nin the order of the most meaningful to least meaningful. The results shows\nimprovement over one existing search engine for complex queries for given set\nof documents.</p>\n", "tags": ["Evaluation"] },
{"key": "patel2021recall", "year": "2022", "citations": "28", "title":"Recall@k Surrogate Loss with Large Batches and Similarity Mixup", "abstract": "<p>This work focuses on learning deep visual representation models for retrieval\nby exploring the interplay between a new loss function, the batch size, and a\nnew regularization approach. Direct optimization, by gradient descent, of an\nevaluation metric, is not possible when it is non-differentiable, which is the\ncase for recall in retrieval. A differentiable surrogate loss for the recall is\nproposed in this work. Using an implementation that sidesteps the hardware\nconstraints of the GPU memory, the method trains with a very large batch size,\nwhich is essential for metrics computed on the entire retrieval database. It is\nassisted by an efficient mixup regularization approach that operates on\npairwise scalar similarities and virtually increases the batch size further.\nThe suggested method achieves state-of-the-art performance in several image\nretrieval benchmarks when used for deep metric learning. For instance-level\nrecognition, the method outperforms similar approaches that train using an\napproximation of average precision.</p>\n", "tags": ["Distance-Metric-Learning","CVPR","Evaluation"] },
{"key": "peer2023towards", "year": "2023", "citations": "7", "title":"Towards Writer Retrieval for Historical Datasets", "abstract": "<p>This paper presents an unsupervised approach for writer retrieval based on\nclustering SIFT descriptors detected at keypoint locations resulting in\npseudo-cluster labels. With those cluster labels, a residual network followed\nby our proposed NetRVLAD, an encoding layer with reduced complexity compared to\nNetVLAD, is trained on 32x32 patches at keypoint locations. Additionally, we\nsuggest a graph-based reranking algorithm called SGR to exploit similarities of\nthe page embeddings to boost the retrieval performance. Our approach is\nevaluated on two historical datasets (Historical-WI and HisIR19). We include an\nevaluation of different backbones and NetRVLAD. It competes with related work\non historical datasets without using explicit encodings. We set a new\nState-of-the-art on both datasets by applying our reranking scheme and show\nthat our approach achieves comparable performance on a modern dataset as well.</p>\n", "tags": ["Unsupervised","Datasets","Evaluation","Graph-Based-ANN"] },
{"key": "peng2017modality", "year": "2018", "citations": "141", "title":"Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network", "abstract": "<p>Nowadays, cross-modal retrieval plays an indispensable role to flexibly find\ninformation across different modalities of data. Effectively measuring the\nsimilarity between different modalities of data is the key of cross-modal\nretrieval. Different modalities such as image and text have imbalanced and\ncomplementary relationships, which contain unequal amount of information when\ndescribing the same semantics. For example, images often contain more details\nthat cannot be demonstrated by textual descriptions and vice versa. Existing\nworks based on Deep Neural Network (DNN) mostly construct one common space for\ndifferent modalities to find the latent alignments between them, which lose\ntheir exclusive modality-specific characteristics. Different from the existing\nworks, we propose modality-specific cross-modal similarity measurement (MCSM)\napproach by constructing independent semantic space for each modality, which\nadopts end-to-end framework to directly generate modality-specific cross-modal\nsimilarity without explicit common representation. For each semantic space,\nmodality-specific characteristics within one modality are fully exploited by\nrecurrent attention network, while the data of another modality is projected\ninto this space with attention based joint embedding to utilize the learned\nattention weights for guiding the fine-grained cross-modal correlation\nlearning, which can capture the imbalanced and complementary relationships\nbetween different modalities. Finally, the complementarity between the semantic\nspaces for different modalities is explored by adaptive fusion of the\nmodality-specific cross-modal similarities to perform cross-modal retrieval.\nExperiments on the widely-used Wikipedia and Pascal Sentence datasets as well\nas our constructed large-scale XMediaNet dataset verify the effectiveness of\nour proposed approach, outperforming 9 state-of-the-art methods.</p>\n", "tags": ["Datasets","Scalability","Tools-&-Libraries","Multimodal-Retrieval"] },
{"key": "peng2018deep", "year": "2019", "citations": "44", "title":"Deep Reinforcement Learning for Image Hashing", "abstract": "<p>Deep hashing methods have received much attention recently, which achieve\npromising results by taking advantage of the strong representation power of\ndeep networks. However, most existing deep hashing methods learn a whole set of\nhashing functions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accuracy greatly.\nInspired by the sequential decision ability of deep reinforcement learning, we\npropose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).\nOur proposed DRLIH approach models the hashing learning problem as a sequential\ndecision process, which learns each hashing function by correcting the errors\nimposed by previous ones and promotes retrieval accuracy. To the best of our\nknowledge, this is the first work to address hashing problem from deep\nreinforcement learning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose a deep\nreinforcement learning hashing network. In the proposed network, we utilize\nrecurrent neural network (RNN) as agents to model the hashing functions, which\ntake actions of projecting images into binary codes sequentially, so that the\ncurrent hashing function learning can take previous hashing functions’ error\ninto account. (2) We propose a sequential learning strategy based on proposed\nDRLIH. We define the state as a tuple of internal features of RNN’s hidden\nlayers and image features, which can reflect history decisions made by the\nagents. We also propose an action group method to enhance the correlation of\nhash functions in the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes"] },
{"key": "peng2023embedding", "year": "2023", "citations": "14", "title":"Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data", "abstract": "<p>Pest identification is a crucial aspect of pest control in agriculture.\nHowever, most farmers are not capable of accurately identifying pests in the\nfield, and there is a limited number of structured data sources available for\nrapid querying. In this work, we explored using domain-agnostic general\npre-trained large language model(LLM) to extract structured data from\nagricultural documents with minimal or no human intervention. We propose a\nmethodology that involves text retrieval and filtering using embedding-based\nretrieval, followed by LLM question-answering to automatically extract entities\nand attributes from the documents, and transform them into structured data. In\ncomparison to existing methods, our approach achieves consistently better\naccuracy in the benchmark while maintaining efficiency.</p>\n", "tags": ["Text-Retrieval","Evaluation","Efficiency"] },
{"key": "petrovic2010streaming", "year": "2010", "citations": "565", "title":"Streaming First Story Detection with application to Twitter", "abstract": "<p>With the recent rise in popularity and size of\nsocial media, there is a growing need for systems\nthat can extract useful information from\nthis amount of data. We address the problem\nof detecting new events from a stream of\nTwitter posts. To make event detection feasible\non web-scale corpora, we present an algorithm\nbased on locality-sensitive hashing which\nis able overcome the limitations of traditional\napproaches, while maintaining competitive results.\nIn particular, a comparison with a stateof-the-art\nsystem on the first story detection\ntask shows that we achieve over an order of\nmagnitude speedup in processing time, while\nretaining comparable performance. Event detection\nexperiments on a collection of 160 million\nTwitter posts show that celebrity deaths\nare the fastest spreading news on Twitter.</p>\n", "tags": ["Scalability","Efficiency","Large-Scale-Search","Hashing-Methods","Evaluation"] },
{"key": "petrovic2012using", "year": "2012", "citations": "91", "title":"Using paraphrases for improving first story detection in news and Twitter", "abstract": "<p>First story detection (FSD) involves identifying\nfirst stories about events from a continuous\nstream of documents. A major problem in this\ntask is the high degree of lexical variation in\ndocuments which makes it very difficult to detect\nstories that talk about the same event but\nexpressed using different words. We suggest\nusing paraphrases to alleviate this problem,\nmaking this the first work to use paraphrases\nfor FSD. We show a novel way of integrating\nparaphrases with locality sensitive hashing\n(LSH) in order to obtain an efficient FSD system\nthat can scale to very large datasets. Our\nsystem achieves state-of-the-art results on the\nfirst story detection task, beating both the best\nsupervised and unsupervised systems. To test\nour approach on large data, we construct a corpus\nof events for Twitter, consisting of 50 million\ndocuments, and show that paraphrasing is\nalso beneficial in this domain.</p>\n", "tags": ["Datasets","Locality-Sensitive-Hashing","Supervised","Hashing-Methods","Unsupervised"] },
{"key": "petrovic2025streaming", "year": "2010", "citations": "565", "title":"Streaming First Story Detection with application to Twitter", "abstract": "<p>With the recent rise in popularity and size of\nsocial media, there is a growing need for systems\nthat can extract useful information from\nthis amount of data. We address the problem\nof detecting new events from a stream of\nTwitter posts. To make event detection feasible\non web-scale corpora, we present an algorithm\nbased on locality-sensitive hashing which\nis able overcome the limitations of traditional\napproaches, while maintaining competitive results.\nIn particular, a comparison with a stateof-the-art\nsystem on the first story detection\ntask shows that we achieve over an order of\nmagnitude speedup in processing time, while\nretaining comparable performance. Event detection\nexperiments on a collection of 160 million\nTwitter posts show that celebrity deaths\nare the fastest spreading news on Twitter.</p>\n", "tags": ["Scalability","Efficiency","Large-Scale-Search","Hashing-Methods","Evaluation"] },
{"key": "petrovic2025using", "year": "2012", "citations": "91", "title":"Using paraphrases for improving first story detection in news and Twitter", "abstract": "<p>First story detection (FSD) involves identifying\nfirst stories about events from a continuous\nstream of documents. A major problem in this\ntask is the high degree of lexical variation in\ndocuments which makes it very difficult to detect\nstories that talk about the same event but\nexpressed using different words. We suggest\nusing paraphrases to alleviate this problem,\nmaking this the first work to use paraphrases\nfor FSD. We show a novel way of integrating\nparaphrases with locality sensitive hashing\n(LSH) in order to obtain an efficient FSD system\nthat can scale to very large datasets. Our\nsystem achieves state-of-the-art results on the\nfirst story detection task, beating both the best\nsupervised and unsupervised systems. To test\nour approach on large data, we construct a corpus\nof events for Twitter, consisting of 50 million\ndocuments, and show that paraphrasing is\nalso beneficial in this domain.</p>\n", "tags": ["Datasets","Locality-Sensitive-Hashing","Supervised","Hashing-Methods","Unsupervised"] },
{"key": "pham2016scalability", "year": "2016", "citations": "11", "title":"Scalability and Total Recall with Fast CoveringLSH", "abstract": "<p>Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic\ntechnique for similarity search with strong performance guarantees in\nhigh-dimensional spaces. A drawback of traditional LSH schemes is that they may\nhave <em>false negatives</em>, i.e., the recall is less than 100%. This limits\nthe applicability of LSH in settings requiring precise performance guarantees.\nBuilding on the recent theoretical “CoveringLSH” construction that eliminates\nfalse negatives, we propose a fast and practical covering LSH scheme for\nHamming space called <em>Fast CoveringLSH (fcLSH)</em>. Inheriting the design\nbenefits of CoveringLSH our method avoids false negatives and always reports\nall near neighbors. Compared to CoveringLSH we achieve an asymptotic\nimprovement to the hash function computation time from \\(\\mathcal{O}(dL)\\) to\n\\(\\mathcal{O}(d + Llog{L})\\), where \\(d\\) is the dimensionality of data and \\(L\\) is\nthe number of hash tables. Our experiments on synthetic and real-world data\nsets demonstrate that <em>fcLSH</em> is comparable (and often superior) to\ntraditional hashing-based approaches for search radius up to 20 in\nhigh-dimensional Hamming space.</p>\n", "tags": ["Similarity-Search","CIKM","Locality-Sensitive-Hashing","Hashing-Methods","Scalability","Evaluation"] },
{"key": "pham2024composing", "year": "2024", "citations": "11", "title":"Composing Object Relations and Attributes for Image-Text Matching", "abstract": "<p>We study the visual semantic embedding problem for image-text matching. Most\nexisting work utilizes a tailored cross-attention mechanism to perform local\nalignment across the two image and text modalities. This is computationally\nexpensive, even though it is more powerful than the unimodal dual-encoder\napproach. This work introduces a dual-encoder image-text matching model,\nleveraging a scene graph to represent captions with nodes for objects and\nattributes interconnected by relational edges. Utilizing a graph attention\nnetwork, our model efficiently encodes object-attribute and object-object\nsemantic relations, resulting in a robust and fast-performing system.\nRepresenting caption as a scene graph offers the ability to utilize the strong\nrelational inductive bias of graph neural networks to learn object-attribute\nand object-object relations effectively. To train the model, we propose losses\nthat align the image and caption both at the holistic level (image-caption) and\nthe local level (image-object entity), which we show is key to the success of\nthe model. Our model is termed Composition model for Object Relations and\nAttributes, CORA. Experimental results on two prominent image-text retrieval\nbenchmarks, Flickr30K and MSCOCO, demonstrate that CORA outperforms existing\nstate-of-the-art computationally expensive cross-attention methods regarding\nrecall score while achieving fast computation speed of the dual encoder.</p>\n", "tags": ["CVPR","Text-Retrieval","Evaluation"] },
{"key": "pisov2018brain", "year": "2018", "citations": "5", "title":"Brain Tumor Image Retrieval via Multitask Learning", "abstract": "<p>Classification-based image retrieval systems are built by training\nconvolutional neural networks (CNNs) on a relevant classification problem and\nusing the distance in the resulting feature space as a similarity metric.\nHowever, in practical applications, it is often desirable to have\nrepresentations which take into account several aspects of the data (e.g.,\nbrain tumor type and its localization). In our work, we extend the\nclassification-based approach with multitask learning: we train a CNN on brain\nMRI scans with heterogeneous labels and implement a corresponding tumor image\nretrieval system. We validate our approach on brain tumor data which contains\ninformation about tumor types, shapes and localization. We show that our method\nallows us to build representations that contain more relevant information about\ntumors than single-task classification-based approaches.</p>\n", "tags": ["Distance-Metric-Learning","Image-Retrieval"] },
{"key": "plummer2018give", "year": "2019", "citations": "15", "title":"Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback", "abstract": "<p>In this paper, we introduce an attribute-based interactive image search which\ncan leverage human-in-the-loop feedback to iteratively refine image search\nresults. We study active image search where human feedback is solicited\nexclusively in visual form, without using relative attribute annotations used\nby prior work which are not typically found in many datasets. In order to\noptimize the image selection strategy, a deep reinforcement model is trained to\nlearn what images are informative rather than rely on hand-crafted measures\ntypically leveraged in prior work. Additionally, we extend the recently\nintroduced Conditional Similarity Network to incorporate global similarity in\ntraining visual embeddings, which results in more natural transitions as the\nuser explores the learned similarity embeddings. Our experiments demonstrate\nthe effectiveness of our approach, producing compelling results on both active\nimage search and image attribute representation tasks.</p>\n", "tags": ["Datasets","Image-Retrieval"] },
{"key": "podlesnaya2016deep", "year": "2017", "citations": "20", "title":"Deep Learning Based Semantic Video Indexing and Retrieval", "abstract": "<p>We share the implementation details and testing results for video retrieval\nsystem based exclusively on features extracted by convolutional neural\nnetworks. We show that deep learned features might serve as universal signature\nfor semantic content of video useful in many search and retrieval tasks. We\nfurther show that graph-based storage structure for video index allows to\nefficiently retrieving the content with complicated spatial and temporal search\nqueries.</p>\n", "tags": ["Video-Retrieval","Graph-Based-ANN"] },
{"key": "portilloquintero2021straightforward", "year": "2021", "citations": "79", "title":"A Straightforward Framework For Video Retrieval Using CLIP", "abstract": "<p>Video Retrieval is a challenging task where a text query is matched to a\nvideo or vice versa. Most of the existing approaches for addressing such a\nproblem rely on annotations made by the users. Although simple, this approach\nis not always feasible in practice. In this work, we explore the application of\nthe language-image model, CLIP, to obtain video representations without the\nneed for said annotations. This model was explicitly trained to learn a common\nspace where images and text can be compared. Using various techniques described\nin this document, we extended its application to videos, obtaining\nstate-of-the-art results on the MSR-VTT and MSVD benchmarks.</p>\n", "tags": ["Video-Retrieval","Tools-&-Libraries"] },
{"key": "pratap2019efficient", "year": "2019", "citations": "11", "title":"Efficient Sketching Algorithm for Sparse Binary Data", "abstract": "<p>Recent advancement of the WWW, IOT, social network, e-commerce, etc. have\ngenerated a large volume of data. These datasets are mostly represented by high\ndimensional and sparse datasets. Many fundamental subroutines of common data\nanalytic tasks such as clustering, classification, ranking, nearest neighbour\nsearch, etc. scale poorly with the dimension of the dataset. In this work, we\naddress this problem and propose a sketching (alternatively, dimensionality\nreduction) algorithm – \\(\\binsketch\\) (Binary Data Sketch) – for sparse binary\ndatasets. \\(\\binsketch\\) preserves the binary version of the dataset after\nsketching and maintains estimates for multiple similarity measures such as\nJaccard, Cosine, Inner-Product similarities, and Hamming distance, on the same\nsketch. We present a theoretical analysis of our algorithm and complement it\nwith extensive experimentation on several real-world datasets. We compare the\nperformance of our algorithm with the state-of-the-art algorithms on the task\nof mean-square-error and ranking. Our proposed algorithm offers a comparable\naccuracy while suggesting a significant speedup in the dimensionality reduction\ntime, with respect to the other candidate algorithms. Our proposal is simple,\neasy to implement, and therefore can be adopted in practice.</p>\n", "tags": ["Datasets","Evaluation","Efficiency"] },
{"key": "qi2017efficient", "year": "2017", "citations": "8", "title":"An efficient deep learning hashing neural network for mobile visual search", "abstract": "<p>Mobile visual search applications are emerging that enable users to sense\ntheir surroundings with smart phones. However, because of the particular\nchallenges of mobile visual search, achieving a high recognition bitrate has\nbecomes a consistent target of previous related works. In this paper, we\npropose a few-parameter, low-latency, and high-accuracy deep hashing approach\nfor constructing binary hash codes for mobile visual search. First, we exploit\nthe architecture of the MobileNet model, which significantly decreases the\nlatency of deep feature extraction by reducing the number of model parameters\nwhile maintaining accuracy. Second, we add a hash-like layer into MobileNet to\ntrain the model on labeled mobile visual data. Evaluations show that the\nproposed system can exceed state-of-the-art accuracy performance in terms of\nthe MAP. More importantly, the memory consumption is much less than that of\nother deep learning models. The proposed method requires only \\(13\\) MB of memory\nfor the neural network and achieves a MAP of \\(97.80%\\) on the mobile location\nrecognition dataset used for testing.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Evaluation"] },
{"key": "qiao2019deep", "year": "2019", "citations": "21", "title":"Deep Heterogeneous Hashing for Face Video Retrieval", "abstract": "<p>Retrieving videos of a particular person with face image as a query via\nhashing technique has many important applications. While face images are\ntypically represented as vectors in Euclidean space, characterizing face videos\nwith some robust set modeling techniques (e.g. covariance matrices as exploited\nin this study, which reside on Riemannian manifold), has recently shown\nappealing advantages. This hence results in a thorny heterogeneous spaces\nmatching problem. Moreover, hashing with handcrafted features as done in many\nexisting works is clearly inadequate to achieve desirable performance for this\ntask. To address such problems, we present an end-to-end Deep Heterogeneous\nHashing (DHH) method that integrates three stages including image feature\nlearning, video modeling, and heterogeneous hashing in a single framework, to\nlearn unified binary codes for both face images and videos. To tackle the key\nchallenge of hashing on the manifold, a well-studied Riemannian kernel mapping\nis employed to project data (i.e. covariance matrices) into Euclidean space and\nthus enables to embed the two heterogeneous representations into a common\nHamming space, where both intra-space discriminability and inter-space\ncompatibility are considered. To perform network optimization, the gradient of\nthe kernel mapping is innovatively derived via structured matrix\nbackpropagation in a theoretically principled way. Experiments on three\nchallenging datasets show that our method achieves quite competitive\nperformance compared with existing hashing methods.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Compact-Codes","Video-Retrieval","Evaluation"] },
{"key": "qiu2017deep", "year": "2017", "citations": "97", "title":"Deep Semantic Hashing with Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest\nneighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can\nlead to high quality hashing. However, the cost of annotating\ndata is often an obstacle when applying supervised hashing\nto a new domain. Moreover, the results can suffer from the\nrobustness problem as the data at training and test stage\nmay come from different distributions. This paper studies\nthe exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which\nleverages largely unlabeled and limited labeled training data\nto produce highly compelling data with intrinsic invariance\nand global coherence, for better understanding statistical\nstructures of natural data. We demonstrate that the above\ntwo limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic\nhashing with GANs (DSH-GANs) is presented, which mainly\nconsists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary\nstream to distinguish synthetic images from real ones, a hash\nstream for encoding image representations to hash codes and\na classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss\nto correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the\ninput real-synthetic triplets and classification loss to classify\neach sample accurately. Extensive experiments conducted on\nboth CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our\nframework also achieves superior results when compared to\nstate-of-the-art deep hash models.</p>\n", "tags": ["Image-Retrieval","Scalability","Neural-Hashing","Text-Retrieval","Tools-&-Libraries","SIGIR","Hashing-Methods","Supervised","Robustness"] },
{"key": "qiu2018deep", "year": "2017", "citations": "97", "title":"Deep Semantic Hashing with Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models.</p>\n", "tags": ["Supervised","Text-Retrieval","Tools-&-Libraries","Image-Retrieval","Hashing-Methods","SIGIR","Neural-Hashing","Scalability","Robustness"] },
{"key": "qiu2021unsupervised", "year": "2021", "citations": "51", "title":"Unsupervised Hashing with Contrastive Information Bottleneck", "abstract": "<p>Many unsupervised hashing methods are implicitly established on the idea of\nreconstructing the input data, which basically encourages the hashing codes to\nretain as much information of original data as possible. However, this\nrequirement may force the models spending lots of their effort on\nreconstructing the unuseful background information, while ignoring to preserve\nthe discriminative semantic information that is more important for the hashing\ntask. To tackle this problem, inspired by the recent success of contrastive\nlearning in learning continuous representations, we propose to adapt this\nframework to learn binary hashing codes. Specifically, we first propose to\nmodify the objective function to meet the specific requirement of hashing and\nthen introduce a probabilistic binary representation layer into the model to\nfacilitate end-to-end training of the entire model. We further prove the strong\nconnection between the proposed contrastive-learning-based hashing method and\nthe mutual information, and show that the proposed model can be considered\nunder the broader framework of the information bottleneck (IB). Under this\nperspective, a more general hashing model is naturally obtained. Extensive\nexperimental results on three benchmark image datasets demonstrate that the\nproposed hashing method significantly outperforms existing baselines.</p>\n", "tags": ["Evaluation","Datasets","Unsupervised","AAAI","Hashing-Methods","Tools-&-Libraries","Neural-Hashing","IJCAI","Supervised"] },
{"key": "qiu2022pre", "year": "2022", "citations": "13", "title":"Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search", "abstract": "<p>BERT-style models pre-trained on the general corpus (e.g., Wikipedia) and\nfine-tuned on specific task corpus, have recently emerged as breakthrough\ntechniques in many NLP tasks: question answering, text classification, sequence\nlabeling and so on. However, this technique may not always work, especially for\ntwo scenarios: a corpus that contains very different text from the general\ncorpus Wikipedia, or a task that learns embedding spacial distribution for a\nspecific purpose (e.g., approximate nearest neighbor search). In this paper, to\ntackle the above two scenarios that we have encountered in an industrial\ne-commerce search system, we propose customized and novel pre-training tasks\nfor two critical modules: user intent detection and semantic embedding\nretrieval. The customized pre-trained models after fine-tuning, being less than\n10% of BERT-base’s size in order to be feasible for cost-efficient CPU serving,\nsignificantly improve the other baseline models: 1) no pre-training model and\n2) fine-tuned model from the official pre-trained BERT using general corpus, on\nboth offline datasets and online system. We have open sourced our datasets for\nthe sake of reproducibility and future works.</p>\n", "tags": ["Datasets","CIKM"] },
{"key": "qiu2025deep", "year": "2017", "citations": "97", "title":"Deep Semantic Hashing with Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest\nneighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can\nlead to high quality hashing. However, the cost of annotating\ndata is often an obstacle when applying supervised hashing\nto a new domain. Moreover, the results can suffer from the\nrobustness problem as the data at training and test stage\nmay come from different distributions. This paper studies\nthe exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which\nleverages largely unlabeled and limited labeled training data\nto produce highly compelling data with intrinsic invariance\nand global coherence, for better understanding statistical\nstructures of natural data. We demonstrate that the above\ntwo limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic\nhashing with GANs (DSH-GANs) is presented, which mainly\nconsists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary\nstream to distinguish synthetic images from real ones, a hash\nstream for encoding image representations to hash codes and\na classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss\nto correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the\ninput real-synthetic triplets and classification loss to classify\neach sample accurately. Extensive experiments conducted on\nboth CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our\nframework also achieves superior results when compared to\nstate-of-the-art deep hash models.</p>\n", "tags": ["Image-Retrieval","Scalability","Neural-Hashing","Text-Retrieval","Tools-&-Libraries","SIGIR","Hashing-Methods","Supervised","Robustness"] },
{"key": "qu2023learnable", "year": "2023", "citations": "12", "title":"Learnable Pillar-based Re-ranking for Image-Text Retrieval", "abstract": "<p>Image-text retrieval aims to bridge the modality gap and retrieve cross-modal\ncontent based on semantic similarities. Prior work usually focuses on the\npairwise relations (i.e., whether a data sample matches another) but ignores\nthe higher-order neighbor relations (i.e., a matching structure among multiple\ndata samples). Re-ranking, a popular post-processing practice, has revealed the\nsuperiority of capturing neighbor relations in single-modality retrieval tasks.\nHowever, it is ineffective to directly extend existing re-ranking algorithms to\nimage-text retrieval. In this paper, we analyze the reason from four\nperspectives, i.e., generalization, flexibility, sparsity, and asymmetry, and\npropose a novel learnable pillar-based re-ranking paradigm. Concretely, we\nfirst select top-ranked intra- and inter-modal neighbors as pillars, and then\nreconstruct data samples with the neighbor relations between them and the\npillars. In this way, each sample can be mapped into a multimodal pillar space\nonly using similarities, ensuring generalization. After that, we design a\nneighbor-aware graph reasoning module to flexibly exploit the relations and\nexcavate the sparse positive items within a neighborhood. We also present a\nstructure alignment constraint to promote cross-modal collaboration and align\nthe asymmetric modalities. On top of various base backbones, we carry out\nextensive experiments on two benchmark datasets, i.e., Flickr30K and MS-COCO,\ndemonstrating the effectiveness, superiority, generalization, and\ntransferability of our proposed re-ranking paradigm.</p>\n", "tags": ["Text-Retrieval","Datasets","SIGIR","Hybrid-ANN-Methods","Re-Ranking","Evaluation"] },
{"key": "quinn2017semantic", "year": "2018", "citations": "12", "title":"Semantic Image Retrieval via Active Grounding of Visual Situations", "abstract": "<p>We describe a novel architecture for semantic image retrieval—in\nparticular, retrieval of instances of visual situations. Visual situations are\nconcepts such as “a boxing match,” “walking the dog,” “a crowd waiting for a\nbus,” or “a game of ping-pong,” whose instantiations in images are linked more\nby their common spatial and semantic structure than by low-level visual\nsimilarity. Given a query situation description, our architecture—called\nSituate—learns models capturing the visual features of expected objects as\nwell the expected spatial configuration of relationships among objects. Given a\nnew image, Situate uses these models in an attempt to ground (i.e., to create a\nbounding box locating) each expected component of the situation in the image\nvia an active search procedure. Situate uses the resulting grounding to compute\na score indicating the degree to which the new image is judged to contain an\ninstance of the situation. Such scores can be used to rank images in a\ncollection as part of a retrieval system. In the preliminary study described\nhere, we demonstrate the promise of this system by comparing Situate’s\nperformance with that of two baseline methods, as well as with a related\nsemantic image-retrieval system based on “scene graphs.”</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "radenović2017fine", "year": "2018", "citations": "1178", "title":"Fine-tuning CNN Image Retrieval with No Human Annotation", "abstract": "<p>Image descriptors based on activations of Convolutional Neural Networks\n(CNNs) have become dominant in image retrieval due to their discriminative\npower, compactness of representation, and search efficiency. Training of CNNs,\neither from scratch or fine-tuning, requires a large amount of annotated data,\nwhere a high quality of annotation is often crucial. In this work, we propose\nto fine-tune CNNs for image retrieval on a large collection of unordered images\nin a fully automated manner. Reconstructed 3D models obtained by the\nstate-of-the-art retrieval and structure-from-motion methods guide the\nselection of the training data. We show that both hard-positive and\nhard-negative examples, selected by exploiting the geometry and the camera\npositions available from the 3D models, enhance the performance of\nparticular-object retrieval. CNN descriptor whitening discriminatively learned\nfrom the same training data outperforms commonly used PCA whitening. We propose\na novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and\naverage pooling and show that it boosts retrieval performance. Applying the\nproposed method to the VGG network achieves state-of-the-art performance on the\nstandard benchmarks: Oxford Buildings, Paris, and Holidays datasets.</p>\n", "tags": ["Datasets","Image-Retrieval","Evaluation","Efficiency"] },
{"key": "raginsky2009locality", "year": "2009", "citations": "633", "title":"Locality-sensitive binary codes from shift-invariant kernels", "abstract": "<p>This paper addresses the problem of designing binary codes for high-dimensional\ndata such that vectors that are similar in the original space map to similar binary\nstrings. We introduce a simple distribution-free encoding scheme based on\nrandom projections, such that the expected Hamming distance between the binary\ncodes of two vectors is related to the value of a shift-invariant kernel (e.g., a\nGaussian kernel) between the vectors. We present a full theoretical analysis of the\nconvergence properties of the proposed scheme, and report favorable experimental\nperformance as compared to a recent state-of-the-art method, spectral hashing.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Evaluation","Compact-Codes"] },
{"key": "raginsky2025locality", "year": "2009", "citations": "633", "title":"Locality-sensitive binary codes from shift-invariant kernels", "abstract": "<p>This paper addresses the problem of designing binary codes for high-dimensional\ndata such that vectors that are similar in the original space map to similar binary\nstrings. We introduce a simple distribution-free encoding scheme based on\nrandom projections, such that the expected Hamming distance between the binary\ncodes of two vectors is related to the value of a shift-invariant kernel (e.g., a\nGaussian kernel) between the vectors. We present a full theoretical analysis of the\nconvergence properties of the proposed scheme, and report favorable experimental\nperformance as compared to a recent state-of-the-art method, spectral hashing.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Evaluation","Compact-Codes"] },
{"key": "rajput2023recommender", "year": "2023", "citations": "17", "title":"Recommender Systems with Generative Retrieval", "abstract": "<p>Modern recommender systems perform large-scale retrieval by first embedding\nqueries and item candidates in the same unified space, followed by approximate\nnearest neighbor search to select top candidates given a query embedding. In\nthis paper, we propose a novel generative retrieval approach, where the\nretrieval model autoregressively decodes the identifiers of the target\ncandidates. To that end, we create semantically meaningful tuple of codewords\nto serve as a Semantic ID for each item. Given Semantic IDs for items in a user\nsession, a Transformer-based sequence-to-sequence model is trained to predict\nthe Semantic ID of the next item that the user will interact with. To the best\nof our knowledge, this is the first Semantic ID-based generative model for\nrecommendation tasks. We show that recommender systems trained with the\nproposed paradigm significantly outperform the current SOTA models on various\ndatasets. In addition, we show that incorporating Semantic IDs into the\nsequence-to-sequence model enhances its ability to generalize, as evidenced by\nthe improved retrieval performance observed for items with no prior interaction\nhistory.</p>\n", "tags": ["Recommender-Systems","Datasets","Scalability","Evaluation"] },
{"key": "razeghi2017privacy", "year": "2017", "citations": "25", "title":"Privacy Preserving Identification Using Sparse Approximation with Ambiguization", "abstract": "<p>In this paper, we consider a privacy preserving encoding framework for\nidentification applications covering biometrics, physical object security and\nthe Internet of Things (IoT). The proposed framework is based on a sparsifying\ntransform, which consists of a trained linear map, an element-wise\nnonlinearity, and privacy amplification. The sparsifying transform and privacy\namplification are not symmetric for the data owner and data user. We\ndemonstrate that the proposed approach is closely related to sparse ternary\ncodes (STC), a recent information-theoretic concept proposed for fast\napproximate nearest neighbor (ANN) search in high dimensional feature spaces\nthat being machine learning in nature also offers significant benefits in\ncomparison to sparse approximation and binary embedding approaches. We\ndemonstrate that the privacy of the database outsourced to a server as well as\nthe privacy of the data user are preserved at a low computational cost, storage\nand communication burdens.</p>\n", "tags": ["Hashing-Methods","Evaluation","Tools-&-Libraries"] },
{"key": "riazi2016sub", "year": "2016", "citations": "11", "title":"Sub-Linear Privacy-Preserving Near-Neighbor Search", "abstract": "<p>In Near-Neighbor Search (NNS), a new client queries a database (held by a\nserver) for the most similar data (near-neighbors) given a certain similarity\nmetric. The Privacy-Preserving variant (PP-NNS) requires that neither server\nnor the client shall learn information about the other party’s data except what\ncan be inferred from the outcome of NNS. The overwhelming growth in the size of\ncurrent datasets and the lack of a truly secure server in the online world\nrender the existing solutions impractical; either due to their high\ncomputational requirements or non-realistic assumptions which potentially\ncompromise privacy. PP-NNS having query time {\\it sub-linear} in the size of\nthe database has been suggested as an open research direction by Li et al.\n(CCSW’15). In this paper, we provide the first such algorithm, called Secure\nLocality Sensitive Indexing (SLSI) which has a sub-linear query time and the\nability to handle honest-but-curious parties. At the heart of our proposal lies\na secure binary embedding scheme generated from a novel probabilistic\ntransformation over locality sensitive hashing family. We provide information\ntheoretic bound for the privacy guarantees and support our theoretical claims\nusing substantial empirical evidence on real-world datasets.</p>\n", "tags": ["Hashing-Methods","Datasets","Locality-Sensitive-Hashing","Efficiency"] },
{"key": "riba2020learning", "year": "2021", "citations": "26", "title":"Learning Graph Edit Distance by Graph Neural Networks", "abstract": "<p>The emergence of geometric deep learning as a novel framework to deal with\ngraph-based representations has faded away traditional approaches in favor of\ncompletely new methodologies. In this paper, we propose a new framework able to\ncombine the advances on deep metric learning with traditional approximations of\nthe graph edit distance. Hence, we propose an efficient graph distance based on\nthe novel field of geometric deep learning. Our method employs a message\npassing neural network to capture the graph structure, and thus, leveraging\nthis information for its use on a distance computation. The performance of the\nproposed graph distance is validated on two different scenarios. On the one\nhand, in a graph retrieval of handwritten words~\\ie~keyword spotting, showing\nits superior performance when compared with (approximate) graph edit distance\nbenchmarks. On the other hand, demonstrating competitive results for graph\nsimilarity learning when compared with the current state-of-the-art on a recent\nbenchmark dataset.</p>\n", "tags": ["Evaluation","Distance-Metric-Learning","Datasets","Graph-Based-ANN","Tools-&-Libraries","CVPR"] },
{"key": "rippel2014learning", "year": "2014", "citations": "49", "title":"Learning Ordered Representations with Nested Dropout", "abstract": "<p>In this paper, we study ordered representations of data in which different\ndimensions have different degrees of importance. To learn these representations\nwe introduce nested dropout, a procedure for stochastically removing coherent\nnested sets of hidden units in a neural network. We first present a sequence of\ntheoretical results in the simple case of a semi-linear autoencoder. We\nrigorously show that the application of nested dropout enforces identifiability\nof the units, which leads to an exact equivalence with PCA. We then extend the\nalgorithm to deep models and demonstrate the relevance of ordered\nrepresentations to a number of applications. Specifically, we use the ordered\nproperty of the learned codes to construct hash-based data structures that\npermit very fast retrieval, achieving retrieval in time logarithmic in the\ndatabase size and independent of the dimensionality of the representation. This\nallows codes that are hundreds of times longer than currently feasible for\nretrieval. We therefore avoid the diminished quality associated with short\ncodes, while still performing retrieval that is competitive in speed with\nexisting methods. We also show that ordered representations are a promising way\nto learn adaptive compression for efficient online data reconstruction.</p>\n", "tags": ["Efficiency"] },
{"key": "rosin2020event", "year": "2021", "citations": "8", "title":"Event-Driven Query Expansion", "abstract": "<p>A significant number of event-related queries are issued in Web search. In\nthis paper, we seek to improve retrieval performance by leveraging events and\nspecifically target the classic task of query expansion. We propose a method to\nexpand an event-related query by first detecting the events related to it.\nThen, we derive the candidates for expansion as terms semantically related to\nboth the query and the events. To identify the candidates, we utilize a novel\nmechanism to simultaneously embed words and events in the same vector space. We\nshow that our proposed method of leveraging events improves query expansion\nperformance significantly compared with state-of-the-art methods on various\nnewswire TREC datasets.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "rossetto2019query", "year": "2019", "citations": "5", "title":"Query by Semantic Sketch", "abstract": "<p>Sketch-based query formulation is very common in image and video retrieval as\nthese techniques often complement textual retrieval methods that are based on\neither manual or machine generated annotations. In this paper, we present a\nretrieval approach that allows to query visual media collections by sketching\nconcept maps, thereby merging sketch-based retrieval with the search for\nsemantic labels. Users can draw a spatial distribution of different concept\nlabels, such as “sky”, “sea” or “person” and then use these sketches to find\nimages or video scenes that exhibit a similar distribution of these concepts.\nHence, this approach does not only take the semantic concepts themselves into\naccount, but also their semantic relations as well as their spatial context.\nThe efficient vector representation enables efficient retrieval even in large\nmultimedia collections. We have integrated the semantic sketch query mode into\nour retrieval engine vitrivr and demonstrated its effectiveness.</p>\n", "tags": ["Similarity-Search","Video-Retrieval"] },
{"key": "roy2016representing", "year": "2016", "citations": "11", "title":"Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval", "abstract": "<p>A major difficulty in applying word vector embeddings in IR is in devising an\neffective and efficient strategy for obtaining representations of compound\nunits of text, such as whole documents, (in comparison to the atomic words),\nfor the purpose of indexing and scoring documents. Instead of striving for a\nsuitable method for obtaining a single vector representation of a large\ndocument of text, we rather aim for developing a similarity metric that makes\nuse of the similarities between the individual embedded word vectors in a\ndocument and a query. More specifically, we represent a document and a query as\nsets of word vectors, and use a standard notion of similarity measure between\nthese sets, computed as a function of the similarities between each constituent\nword pair from these sets. We then make use of this similarity measure in\ncombination with standard IR based similarities for document ranking. The\nresults of our initial experimental investigations shows that our proposed\nmethod improves MAP by up to \\(5.77%\\), in comparison to standard text-based\nlanguage model similarity, on the TREC ad-hoc dataset.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "roy2019metric", "year": "2020", "citations": "75", "title":"Metric-Learning based Deep Hashing Network for Content Based Retrieval of Remote Sensing Images", "abstract": "<p>Hashing methods have been recently found very effective in retrieval of\nremote sensing (RS) images due to their computational efficiency and fast\nsearch speed. The traditional hashing methods in RS usually exploit\nhand-crafted features to learn hash functions to obtain binary codes, which can\nbe insufficient to optimally represent the information content of RS images. To\novercome this problem, in this paper we introduce a metric-learning based\nhashing network, which learns: 1) a semantic-based metric space for effective\nfeature representation; and 2) compact binary hash codes for fast archive\nsearch. Our network considers an interplay of multiple loss functions that\nallows to jointly learn a metric based semantic space facilitating similar\nimages to be clustered together in that target space and at the same time\nproducing compact final activations that lose negligible information when\nbinarized. Experiments carried out on two benchmark RS archives point out that\nthe proposed network significantly improves the retrieval performance under the\nsame retrieval time when compared to the state-of-the-art hashing methods in\nRS.</p>\n", "tags": ["Hashing-Methods","Neural-Hashing","Compact-Codes","Evaluation","Efficiency"] },
{"key": "royoletelier2018disambiguating", "year": "2018", "citations": "5", "title":"Disambiguating Music Artists at Scale with Audio Metric Learning", "abstract": "<p>We address the problem of disambiguating large scale catalogs through the\ndefinition of an unknown artist clustering task. We explore the use of metric\nlearning techniques to learn artist embeddings directly from audio, and using a\ndedicated homonym artists dataset, we compare our method with a recent approach\nthat learn similar embeddings using artist classifiers. While both systems have\nthe ability to disambiguate unknown artists relying exclusively on audio, we\nshow that our system is more suitable in the case when enough audio data is\navailable for each artist in the train dataset. We also propose a new negative\nsampling method for metric learning that takes advantage of side information\nsuch as music genre during the learning phase and shows promising results for\nthe artist clustering task.</p>\n", "tags": ["Distance-Metric-Learning","Datasets"] },
{"key": "rygl2017semantic", "year": "2017", "citations": "12", "title":"Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines", "abstract": "<p>Vector representations and vector space modeling (VSM) play a central role in\nmodern machine learning. We propose a novel approach to `vector similarity\nsearching’ over dense semantic representations of words and documents that can\nbe deployed on top of traditional inverted-index-based fulltext engines, taking\nadvantage of their robustness, stability, scalability and ubiquity.\n  We show that this approach allows the indexing and querying of dense vectors\nin text domains. This opens up exciting avenues for major efficiency gains,\nalong with simpler deployment, scaling and monitoring.\n  The end result is a fast and scalable vector database with a tunable\ntrade-off between vector search performance and quality, backed by a standard\nfulltext engine such as Elasticsearch.\n  We empirically demonstrate its querying performance and quality by applying\nthis solution to the task of semantic searching over a dense vector\nrepresentation of the entire English Wikipedia.</p>\n", "tags": ["Similarity-Search","Text-Retrieval","Robustness","Scalability","Evaluation","Efficiency"] },
{"key": "sablayrolles2016how", "year": "2017", "citations": "97", "title":"How should we evaluate supervised hashing?", "abstract": "<p>Hashing produces compact representations for documents, to perform tasks like\nclassification or retrieval based on these short codes. When hashing is\nsupervised, the codes are trained using labels on the training data. This paper\nfirst shows that the evaluation protocols used in the literature for supervised\nhashing are not satisfactory: we show that a trivial solution that encodes the\noutput of a classifier significantly outperforms existing supervised or\nsemi-supervised methods, while using much shorter codes. We then propose two\nalternative protocols for supervised hashing: one based on retrieval on a\ndisjoint set of classes, and another based on transfer learning to new classes.\nWe provide two baseline methods for image-related tasks to assess the\nperformance of (semi-)supervised hashing: without coding and with unsupervised\ncodes. These baselines give a lower- and upper-bound on the performance of a\nsupervised hashing scheme.</p>\n", "tags": ["Supervised","ICASSP","Hashing-Methods","Neural-Hashing","Compact-Codes","Unsupervised","Evaluation"] },
{"key": "sain2020cross", "year": "2020", "citations": "21", "title":"Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval", "abstract": "<p>Sketch as an image search query is an ideal alternative to text in capturing\nthe fine-grained visual details. Prior successes on fine-grained sketch-based\nimage retrieval (FG-SBIR) have demonstrated the importance of tackling the\nunique traits of sketches as opposed to photos, e.g., temporal vs. static,\nstrokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a\nfurther trait of sketches that has been overlooked to date, that is, they are\nhierarchical in terms of the levels of detail – a person typically sketches up\nto various extents of detail to depict an object. This hierarchical structure\nis often visually distinct. In this paper, we design a novel network that is\ncapable of cultivating sketch-specific hierarchies and exploiting them to match\nsketch with photo at corresponding hierarchical levels. In particular, features\nfrom a sketch and a photo are enriched using cross-modal co-attention, coupled\nwith hierarchical node fusion at every level to form a better embedding space\nto conduct retrieval. Experiments on common benchmarks show our method to\noutperform state-of-the-arts by a significant margin.</p>\n", "tags": ["Image-Retrieval"] },
{"key": "salakhutdinov2008semantic", "year": "2008", "citations": "1272", "title":"Semantic Hashing", "abstract": "<p>We show how to learn a deep graphical model of the word-count\nvectors obtained from a large set of documents. The values of the\nlatent variables in the deepest layer are easy to infer and give a\nmuch better representation of each document than Latent Semantic\nAnalysis. When the deepest layer is forced to use a small number of\nbinary variables (e.g. 32), the graphical model performs “semantic\nhashing”: Documents are mapped to memory addresses in such a\nway that semantically similar documents are located at nearby addresses.\nDocuments similar to a query document can then be found\nby simply accessing all the addresses that differ by only a few bits\nfrom the address of the query document. This way of extending the\nefficiency of hash-coding to approximate matching is much faster\nthan locality sensitive hashing, which is the fastest current method.\nBy using semantic hashing to filter the documents given to TF-IDF,\nwe achieve higher accuracy than applying TF-IDF to the entire document\nset.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Efficiency","Text-Retrieval"] },
{"key": "salakhutdinov2025semantic", "year": "2008", "citations": "1272", "title":"Semantic Hashing", "abstract": "<p>We show how to learn a deep graphical model of the word-count\nvectors obtained from a large set of documents. The values of the\nlatent variables in the deepest layer are easy to infer and give a\nmuch better representation of each document than Latent Semantic\nAnalysis. When the deepest layer is forced to use a small number of\nbinary variables (e.g. 32), the graphical model performs “semantic\nhashing”: Documents are mapped to memory addresses in such a\nway that semantically similar documents are located at nearby addresses.\nDocuments similar to a query document can then be found\nby simply accessing all the addresses that differ by only a few bits\nfrom the address of the query document. This way of extending the\nefficiency of hash-coding to approximate matching is much faster\nthan locality sensitive hashing, which is the fastest current method.\nBy using semantic hashing to filter the documents given to TF-IDF,\nwe achieve higher accuracy than applying TF-IDF to the entire document\nset.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Efficiency","Text-Retrieval"] },
{"key": "salemi2023symmetric", "year": "2023", "citations": "13", "title":"A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering", "abstract": "<p>Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a\nquestion about an image whose answer does not lie in the image. This paper\npresents a new pipeline for KI-VQA tasks, consisting of a retriever and a\nreader. First, we introduce DEDR, a symmetric dual encoding dense retrieval\nframework in which documents and queries are encoded into a shared embedding\nspace using uni-modal (textual) and multi-modal encoders. We introduce an\niterative knowledge distillation approach that bridges the gap between the\nrepresentation spaces in these two encoders. Extensive evaluation on two\nwell-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR\noutperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA,\nrespectively. Utilizing the passages retrieved by DEDR, we further introduce\nMM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating\na textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and\neach retrieved passage separately and uses all passages jointly in its decoder.\nCompared to competitive baselines in the literature, this approach leads to\n5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA\nand FVQA, respectively.</p>\n", "tags": ["Datasets","SIGIR","Evaluation","Tools-&-Libraries"] },
{"key": "salvador2016faster", "year": "2016", "citations": "121", "title":"Faster R-CNN Features for Instance Search", "abstract": "<p>Image representations derived from pre-trained Convolutional Neural Networks\n(CNNs) have become the new state of the art in computer vision tasks such as\ninstance retrieval. This work explores the suitability for instance retrieval\nof image- and region-wise representations pooled from an object detection CNN\nsuch as Faster R-CNN. We take advantage of the object proposals learned by a\nRegion Proposal Network (RPN) and their associated CNN features to build an\ninstance search pipeline composed of a first filtering stage followed by a\nspatial reranking. We further investigate the suitability of Faster R-CNN\nfeatures when the network is fine-tuned for the same objects one wants to\nretrieve. We assess the performance of our proposed system with the Oxford\nBuildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013,\nachieving competitive results.</p>\n", "tags": ["CVPR","Evaluation"] },
{"key": "salvi2016bloom", "year": "2016", "citations": "5", "title":"Bloom Filters and Compact Hash Codes for Efficient and Distributed Image Retrieval", "abstract": "<p>This paper presents a novel method for efficient image retrieval, based on a\nsimple and effective hashing of CNN features and the use of an indexing\nstructure based on Bloom filters. These filters are used as gatekeepers for the\ndatabase of image features, allowing to avoid to perform a query if the query\nfeatures are not stored in the database and speeding up the query process,\nwithout affecting retrieval performance. Thanks to the limited memory\nrequirements the system is suitable for mobile applications and distributed\ndatabases, associating each filter to a distributed portion of the database.\nExperimental validation has been performed on three standard image retrieval\ndatasets, outperforming state-of-the-art hashing methods in terms of precision,\nwhile the proposed indexing method obtains a \\(2\\times\\) speedup.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Evaluation","Efficiency"] },
{"key": "sami2022benchmarking", "year": "2022", "citations": "5", "title":"Benchmarking Human Face Similarity Using Identical Twins", "abstract": "<p>The problem of distinguishing identical twins and non-twin look-alikes in\nautomated facial recognition (FR) applications has become increasingly\nimportant with the widespread adoption of facial biometrics. Due to the high\nfacial similarity of both identical twins and look-alikes, these face pairs\nrepresent the hardest cases presented to facial recognition tools. This work\npresents an application of one of the largest twin datasets compiled to date to\naddress two FR challenges: 1) determining a baseline measure of facial\nsimilarity between identical twins and 2) applying this similarity measure to\ndetermine the impact of doppelgangers, or look-alikes, on FR performance for\nlarge face datasets. The facial similarity measure is determined via a deep\nconvolutional neural network. This network is trained on a tailored\nverification task designed to encourage the network to group together highly\nsimilar face pairs in the embedding space and achieves a test AUC of 0.9799.\nThe proposed network provides a quantitative similarity score for any two given\nfaces and has been applied to large-scale face datasets to identify similar\nface pairs. An additional analysis which correlates the comparison score\nreturned by a facial recognition tool and the similarity score returned by the\nproposed network has also been performed.</p>\n", "tags": ["Datasets","Scalability","Evaluation"] },
{"key": "sanakoyeu2019divide", "year": "2019", "citations": "122", "title":"Divide and Conquer the Embedding Space for Metric Learning", "abstract": "<p>Learning the embedding space, where semantically similar objects are located\nclose together and dissimilar objects far apart, is a cornerstone of many\ncomputer vision applications. Existing approaches usually learn a single metric\nin the embedding space for all available data points, which may have a very\ncomplex non-uniform distribution with different notions of similarity between\nobjects, e.g. appearance, shape, color or semantic meaning. Approaches for\nlearning a single distance metric often struggle to encode all different types\nof relationships and do not generalize well. In this work, we propose a novel\neasy-to-implement divide and conquer approach for deep metric learning, which\nsignificantly improves the state-of-the-art performance of metric learning. Our\napproach utilizes the embedding space more efficiently by jointly splitting the\nembedding space and data into \\(K\\) smaller sub-problems. It divides both, the\ndata and the embedding space into \\(K\\) subsets and learns \\(K\\) separate distance\nmetrics in the non-overlapping subspaces of the embedding space, defined by\ngroups of neurons in the embedding layer of the neural network. The proposed\napproach increases the convergence speed and improves generalization since the\ncomplexity of each sub-problem is reduced compared to the original one. We show\nthat our approach outperforms the state-of-the-art by a large margin in\nretrieval, clustering and re-identification tasks on CUB200-2011, CARS196,\nStanford Online Products, In-shop Clothes and PKU VehicleID datasets.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","CVPR","Evaluation"] },
{"key": "sanakoyeu2021improving", "year": "2021", "citations": "16", "title":"Improving Deep Metric Learning by Divide and Conquer", "abstract": "<p>Deep metric learning (DML) is a cornerstone of many computer vision\napplications. It aims at learning a mapping from the input domain to an\nembedding space, where semantically similar objects are located nearby and\ndissimilar objects far from another. The target similarity on the training data\nis defined by user in form of ground-truth class labels. However, while the\nembedding space learns to mimic the user-provided similarity on the training\ndata, it should also generalize to novel categories not seen during training.\nBesides user-provided groundtruth training labels, a lot of additional visual\nfactors (such as viewpoint changes or shape peculiarities) exist and imply\ndifferent notions of similarity between objects, affecting the generalization\non the images unseen during training. However, existing approaches usually\ndirectly learn a single embedding space on all available training data,\nstruggling to encode all different types of relationships, and do not\ngeneralize well. We propose to build a more expressive representation by\njointly splitting the embedding space and the data hierarchically into smaller\nsub-parts. We successively focus on smaller subsets of the training data,\nreducing its variance and learning a different embedding subspace for each data\nsubset. Moreover, the subspaces are learned jointly to cover not only the\nintricacies, but the breadth of the data as well. Only after that, we build the\nfinal embedding from the subspaces in the conquering stage. The proposed\nalgorithm acts as a transparent wrapper that can be placed around arbitrary\nexisting DML methods. Our approach significantly improves upon the\nstate-of-the-art on image retrieval, clustering, and re-identification tasks\nevaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop\nClothes, and PKU VehicleID datasets.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Image-Retrieval"] },
{"key": "sankar2019transferable", "year": "2019", "citations": "7", "title":"Transferable Neural Projection Representations", "abstract": "<p>Neural word representations are at the core of many state-of-the-art natural\nlanguage processing models. A widely used approach is to pre-train, store and\nlook up word or character embedding matrices. While useful, such\nrepresentations occupy huge memory making it hard to deploy on-device and often\ndo not generalize to unknown words due to vocabulary pruning.\n  In this paper, we propose a skip-gram based architecture coupled with\nLocality-Sensitive Hashing (LSH) projections to learn efficient dynamically\ncomputable representations. Our model does not need to store lookup tables as\nrepresentations are computed on-the-fly and require low memory footprint. The\nrepresentations can be trained in an unsupervised fashion and can be easily\ntransferred to other NLP tasks. For qualitative evaluation, we analyze the\nnearest neighbors of the word representations and discover semantically similar\nwords even with misspellings. For quantitative evaluation, we plug our\ntransferable projections into a simple LSTM and run it on multiple NLP tasks\nand show how our transferable projections achieve better performance compared\nto prior work.</p>\n", "tags": ["Locality-Sensitive-Hashing","Hashing-Methods","Memory-Efficiency","Unsupervised","Evaluation"] },
{"key": "sankaranarayanan2016triplet", "year": "2016", "citations": "47", "title":"Triplet Similarity Embedding for Face Verification", "abstract": "<p>In this work, we present an unconstrained face verification algorithm and\nevaluate it on the recently released IJB-A dataset that aims to push the\nboundaries of face verification methods. The proposed algorithm couples a deep\nCNN-based approach with a low-dimensional discriminative embedding learnt using\ntriplet similarity constraints in a large margin fashion. Aside from yielding\nperformance improvement, this embedding provides significant advantages in\nterms of memory and post-processing operations like hashing and visualization.\nExperiments on the IJB-A dataset show that the proposed algorithm outperforms\nstate of the art methods in verification and identification metrics, while\nrequiring less training time.</p>\n", "tags": ["Hashing-Methods","Datasets","Re-Ranking","Evaluation"] },
{"key": "sarafijanovicdjukic2020fast", "year": "2019", "citations": "32", "title":"Fast Distance-based Anomaly Detection in Images Using an Inception-like Autoencoder", "abstract": "<p>The goal of anomaly detection is to identify examples that deviate from\nnormal or expected behavior. We tackle this problem for images. We consider a\ntwo-phase approach. First, using normal examples, a convolutional autoencoder\n(CAE) is trained to extract a low-dimensional representation of the images.\nHere, we propose a novel architectural choice when designing the CAE, an\nInception-like CAE. It combines convolutional filters of different kernel sizes\nand it uses a Global Average Pooling (GAP) operation to extract the\nrepresentations from the CAE’s bottleneck layer. Second, we employ a\ndistanced-based anomaly detector in the low-dimensional space of the learned\nrepresentation for the images. However, instead of computing the exact\ndistance, we compute an approximate distance using product quantization. This\nalleviates the high memory and prediction time costs of distance-based anomaly\ndetectors. We compare our proposed approach to a number of baselines and\nstate-of-the-art methods on four image datasets, and we find that our approach\nresulted in improved predictive performance.</p>\n", "tags": ["Datasets","Quantization","Evaluation"] },
{"key": "sarfraz2017pose", "year": "2018", "citations": "521", "title":"A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking", "abstract": "<p>Person re identification is a challenging retrieval task that requires\nmatching a person’s acquired image across non overlapping camera views. In this\npaper we propose an effective approach that incorporates both the fine and\ncoarse pose information of the person to learn a discriminative embedding. In\ncontrast to the recent direction of explicitly modeling body parts or\ncorrecting for misalignment based on these, we show that a rather\nstraightforward inclusion of acquired camera view and/or the detected joint\nlocations into a convolutional neural network helps to learn a very effective\nrepresentation. To increase retrieval performance, re-ranking techniques based\non computed distances have recently gained much attention. We propose a new\nunsupervised and automatic re-ranking framework that achieves state-of-the-art\nre-ranking performance. We show that in contrast to the current\nstate-of-the-art re-ranking methods our approach does not require to compute\nnew rank lists for each image pair (e.g., based on reciprocal neighbors) and\nperforms well by using simple direct rank list based comparison or even by just\nusing the already computed euclidean distances between the images. We show that\nboth our learned representation and our re-ranking method achieve\nstate-of-the-art performance on a number of challenging surveillance image and\nvideo datasets.\n  The code is available online at:\nhttps://github.com/pse-ecn/pose-sensitive-embedding</p>\n", "tags": ["Tools-&-Libraries","Distance-Metric-Learning","Datasets","CVPR","Hybrid-ANN-Methods","Re-Ranking","Unsupervised","Evaluation"] },
{"key": "sbai2018unsupervised", "year": "2020", "citations": "8", "title":"Unsupervised Image Decomposition in Vector Layers", "abstract": "<p>Deep image generation is becoming a tool to enhance artists and designers\ncreativity potential. In this paper, we aim at making the generation process\nmore structured and easier to interact with. Inspired by vector graphics\nsystems, we propose a new deep image reconstruction paradigm where the outputs\nare composed from simple layers, defined by their color and a vector\ntransparency mask. This presents a number of advantages compared to the\ncommonly used convolutional network architectures. In particular, our layered\ndecomposition allows simple user interaction, for example to update a given\nmask, or change the color of a selected layer. From a compact code, our\narchitecture also generates vector images with a virtually infinite resolution,\nthe color at each point in an image being a parametric function of its\ncoordinates. We validate the efficiency of our approach by comparing\nreconstructions with state-of-the-art baselines given similar memory resources\non CelebA and ImageNet datasets. Most importantly, we demonstrate several\napplications of our new image representation obtained in an unsupervised\nmanner, including editing, vectorization and image search.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Compact-Codes","Unsupervised","Efficiency"] },
{"key": "schall2019deep", "year": "2019", "citations": "7", "title":"Deep Metric Learning using Similarities from Nonlinear Rank Approximations", "abstract": "<p>In recent years, deep metric learning has achieved promising results in\nlearning high dimensional semantic feature embeddings where the spatial\nrelationships of the feature vectors match the visual similarities of the\nimages. Similarity search for images is performed by determining the vectors\nwith the smallest distances to a query vector. However, high retrieval quality\ndoes not depend on the actual distances of the feature vectors, but rather on\nthe ranking order of the feature vectors from similar images. In this paper, we\nintroduce a metric learning algorithm that focuses on identifying and modifying\nthose feature vectors that most strongly affect the retrieval quality. We\ncompute normalized approximated ranks and convert them to similarities by\napplying a nonlinear transfer function. These similarities are used in a newly\nproposed loss function that better contracts similar and disperses dissimilar\nsamples. Experiments demonstrate significant improvement over existing deep\nfeature embedding methods on the CUB-200-2011, Cars196, and Stanford Online\nProducts data sets for all embedding sizes.</p>\n", "tags": ["Similarity-Search","Distance-Metric-Learning"] },
{"key": "schlegel2018adding", "year": "2019", "citations": "6", "title":"Adding Cues to Binary Feature Descriptors for Visual Place Recognition", "abstract": "<p>In this paper we propose an approach to embed continuous and selector cues in\nbinary feature descriptors used for visual place recognition. The embedding is\nachieved by extending each feature descriptor with a binary string that encodes\na cue and supports the Hamming distance metric. Augmenting the descriptors in\nsuch a way has the advantage of being transparent to the procedure used to\ncompare them. We present two concrete applications of our methodology,\ndemonstrating the two considered types of cues. In addition to that, we\nconducted on these applications a broad quantitative and comparative evaluation\ncovering five benchmark datasets and several state-of-the-art image retrieval\napproaches in combination with various binary descriptor types.</p>\n", "tags": ["Image-Retrieval","Datasets","Distance-Metric-Learning","Evaluation"] },
{"key": "schroeder2020structured", "year": "2020", "citations": "49", "title":"Structured Query-Based Image Retrieval Using Scene Graphs", "abstract": "<p>A structured query can capture the complexity of object interactions (e.g.\n‘woman rides motorcycle’) unlike single objects (e.g. ‘woman’ or ‘motorcycle’).\nRetrieval using structured queries therefore is much more useful than single\nobject retrieval, but a much more challenging problem. In this paper we present\na method which uses scene graph embeddings as the basis for an approach to\nimage retrieval. We examine how visual relationships, derived from scene\ngraphs, can be used as structured queries. The visual relationships are\ndirected subgraphs of the scene graph with a subject and object as nodes\nconnected by a predicate relationship. Notably, we are able to achieve high\nrecall even on low to medium frequency objects found in the long-tailed\nCOCO-Stuff dataset, and find that adding a visual relationship-inspired loss\nboosts our recall by 10% in the best case.</p>\n", "tags": ["Datasets","CVPR","Evaluation","Image-Retrieval"] },
{"key": "schubert2020graph", "year": "2021", "citations": "9", "title":"Graph-based non-linear least squares optimization for visual place recognition in changing environments", "abstract": "<p>Visual place recognition is an important subproblem of mobile robot\nlocalization. Since it is a special case of image retrieval, the basic source\nof information is the pairwise similarity of image descriptors. However, the\nembedding of the image retrieval problem in this robotic task provides\nadditional structure that can be exploited, e.g. spatio-temporal consistency.\nSeveral algorithms exist to exploit this structure, e.g., sequence processing\napproaches or descriptor standardization approaches for changing environments.\nIn this paper, we propose a graph-based framework to systematically exploit\ndifferent types of additional structure and information. The graphical model is\nused to formulate a non-linear least squares problem that can be optimized with\nstandard tools. Beyond sequences and standardization, we propose the usage of\nintra-set similarities within the database and/or the query image set as\nadditional source of information. If available, our approach also allows to\nseamlessly integrate additional knowledge about poses of database images. We\nevaluate the system on a variety of standard place recognition datasets and\ndemonstrate performance improvements for a large number of different\nconfigurations including different sources of information, different types of\nconstraints, and online or offline place recognition setups.</p>\n", "tags": ["Tools-&-Libraries","Graph-Based-ANN","Image-Retrieval","Datasets","Evaluation"] },
{"key": "schubert2021triangle", "year": "2021", "citations": "20", "title":"A Triangle Inequality for Cosine Similarity", "abstract": "<p>Similarity search is a fundamental problem for many data analysis techniques.\nMany efficient search techniques rely on the triangle inequality of metrics,\nwhich allows pruning parts of the search space based on transitive bounds on\ndistances. Recently, Cosine similarity has become a popular alternative choice\nto the standard Euclidean metric, in particular in the context of textual data\nand neural network embeddings. Unfortunately, Cosine similarity is not metric\nand does not satisfy the standard triangle inequality. Instead, many search\ntechniques for Cosine rely on approximation techniques such as locality\nsensitive hashing. In this paper, we derive a triangle inequality for Cosine\nsimilarity that is suitable for efficient similarity search with many standard\nsearch structures (such as the VP-tree, Cover-tree, and M-tree); show that this\nbound is tight and discuss fast approximations for it. We hope that this spurs\nnew research on accelerating exact similarity search for cosine similarity, and\npossible other similarity measures beyond the existing work for distance\nmetrics.</p>\n", "tags": ["Hashing-Methods","Distance-Metric-Learning","Similarity-Search","Tree-Based-ANN"] },
{"key": "seidenschwarz2021learning", "year": "2021", "citations": "23", "title":"Learning Intra-Batch Connections for Deep Metric Learning", "abstract": "<p>The goal of metric learning is to learn a function that maps samples to a\nlower-dimensional space where similar samples lie closer than dissimilar ones.\nParticularly, deep metric learning utilizes neural networks to learn such a\nmapping. Most approaches rely on losses that only take the relations between\npairs or triplets of samples into account, which either belong to the same\nclass or two different classes. However, these methods do not explore the\nembedding space in its entirety. To this end, we propose an approach based on\nmessage passing networks that takes all the relations in a mini-batch into\naccount. We refine embedding vectors by exchanging messages among all samples\nin a given batch allowing the training process to be aware of its overall\nstructure. Since not all samples are equally important to predict a decision\nboundary, we use an attention mechanism during message passing to allow samples\nto weigh the importance of each neighbor accordingly. We achieve\nstate-of-the-art results on clustering and image retrieval on the CUB-200-2011,\nCars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate\nfurther research, we make available the code and the models at\nhttps://github.com/dvl-tum/intra_batch_connections.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Image-Retrieval"] },
{"key": "settle2017query", "year": "2017", "citations": "56", "title":"Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings", "abstract": "<p>Query-by-example search often uses dynamic time warping (DTW) for comparing\nqueries and proposed matching segments. Recent work has shown that comparing\nspeech segments by representing them as fixed-dimensional vectors — acoustic\nword embeddings — and measuring their vector distance (e.g., cosine distance)\ncan discriminate between words more accurately than DTW-based approaches. We\nconsider an approach to query-by-example search that embeds both the query and\ndatabase segments according to a neural model, followed by nearest-neighbor\nsearch to find the matching segments. Earlier work on embedding-based\nquery-by-example, using template-based acoustic word embeddings, achieved\ncompetitive performance. We find that our embeddings, based on recurrent neural\nnetworks trained to optimize word discrimination, achieve substantial\nimprovements in performance and run-time efficiency over the previous\napproaches.</p>\n", "tags": ["Evaluation","Efficiency"] },
{"key": "shan2018recurrent", "year": "2018", "citations": "10", "title":"Recurrent Binary Embedding for GPU-Enabled Exhaustive Retrieval from Billion-Scale Semantic Vectors", "abstract": "<p>Rapid advances in GPU hardware and multiple areas of Deep Learning open up a\nnew opportunity for billion-scale information retrieval with exhaustive search.\nBuilding on top of the powerful concept of semantic learning, this paper\nproposes a Recurrent Binary Embedding (RBE) model that learns compact\nrepresentations for real-time retrieval. The model has the unique ability to\nrefine a base binary vector by progressively adding binary residual vectors to\nmeet the desired accuracy. The refined vector enables efficient implementation\nof exhaustive similarity computation with bit-wise operations, followed by a\nnear- lossless k-NN selection algorithm, also proposed in this paper. The\nproposed algorithms are integrated into an end-to-end multi-GPU system that\nretrieves thousands of top items from over a billion candidates in real-time.\nThe RBE model and the retrieval system were evaluated with data from a major\npaid search engine. When measured against the state-of-the-art model for binary\nrepresentation and the full precision model for semantic embedding, RBE\nsignificantly outperformed the former, and filled in over 80% of the AUC gap\nin-between. Experiments comparing with our production retrieval system also\ndemonstrated superior performance. While the primary focus of this paper is to\nbuild RBE based on a particular class of semantic models, generalizing to other\ntypes is straightforward, as exemplified by two different models at the end of\nthe paper.</p>\n", "tags": ["Hashing-Methods","KDD","Scalability","Large-Scale-Search","Evaluation","Efficiency"] },
{"key": "shankar2017deep", "year": "2017", "citations": "59", "title":"Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce", "abstract": "<p>In this paper, we present a unified end-to-end approach to build a large\nscale Visual Search and Recommendation system for e-commerce. Previous works\nhave targeted these problems in isolation. We believe a more effective and\nelegant solution could be obtained by tackling them together. We propose a\nunified Deep Convolutional Neural Network architecture, called VisNet, to learn\nembeddings to capture the notion of visual similarity, across several semantic\ngranularities. We demonstrate the superiority of our approach for the task of\nimage retrieval, by comparing against the state-of-the-art on the Exact\nStreet2Shop dataset. We then share the design decisions and trade-offs made\nwhile deploying the model to power Visual Recommendations across a catalog of\n50M products, supporting 2K queries a second at Flipkart, India’s largest\ne-commerce company. The deployment of our solution has yielded a significant\nbusiness impact, as measured by the conversion-rate.</p>\n", "tags": ["Datasets","Recommender-Systems","Image-Retrieval"] },
{"key": "shao2023global", "year": "2023", "citations": "21", "title":"Global Features are All You Need for Image Retrieval and Reranking", "abstract": "<p>Image retrieval systems conventionally use a two-stage paradigm, leveraging\nglobal features for initial retrieval and local features for reranking.\nHowever, the scalability of this method is often limited due to the significant\nstorage and computation cost incurred by local feature matching in the\nreranking stage. In this paper, we present SuperGlobal, a novel approach that\nexclusively employs global features for both stages, improving efficiency\nwithout sacrificing accuracy. SuperGlobal introduces key enhancements to the\nretrieval system, specifically focusing on the global feature extraction and\nreranking processes. For extraction, we identify sub-optimal performance when\nthe widely-used ArcFace loss and Generalized Mean (GeM) pooling methods are\ncombined and propose several new modules to improve GeM pooling. In the\nreranking stage, we introduce a novel method to update the global features of\nthe query and top-ranked images by only considering feature refinement with a\nsmall set of images, thus being very compute and memory efficient. Our\nexperiments demonstrate substantial improvements compared to the state of the\nart in standard benchmarks. Notably, on the Revisited Oxford+1M Hard dataset,\nour single-stage results improve by 7.1%, while our two-stage gain reaches 3.7%\nwith a strong 64,865x speedup. Our two-stage system surpasses the current\nsingle-stage state-of-the-art by 16.3%, offering a scalable, accurate\nalternative for high-performing image retrieval systems with minimal time\noverhead. Code: https://github.com/ShihaoShao-GH/SuperGlobal.</p>\n", "tags": ["ICCV","Image-Retrieval","Datasets","Scalability","Evaluation","Efficiency"] },
{"key": "sharma2016stacked", "year": "2016", "citations": "32", "title":"Stacked Autoencoders for Medical Image Search", "abstract": "<p>Medical images can be a valuable resource for reliable information to support\nmedical diagnosis. However, the large volume of medical images makes it\nchallenging to retrieve relevant information given a particular scenario. To\nsolve this challenge, content-based image retrieval (CBIR) attempts to\ncharacterize images (or image regions) with invariant content information in\norder to facilitate image search. This work presents a feature extraction\ntechnique for medical images using stacked autoencoders, which encode images to\nbinary vectors. The technique is applied to the IRMA dataset, a collection of\n14,410 x-ray images in order to demonstrate the ability of autoencoders to\nretrieve similar x-rays given test queries. Using IRMA dataset as a benchmark,\nit was found that stacked autoencoders gave excellent results with a retrieval\nerror of 376 for 1,733 test images with a compression of 74.61%.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "sharma2018improving", "year": "2018", "citations": "6", "title":"Improving Similarity Search with High-dimensional Locality-sensitive Hashing", "abstract": "<p>We propose a new class of data-independent locality-sensitive hashing (LSH)\nalgorithms based on the fruit fly olfactory circuit. The fundamental difference\nof this approach is that, instead of assigning hashes as dense points in a low\ndimensional space, hashes are assigned in a high dimensional space, which\nenhances their separability. We show theoretically and empirically that this\nnew family of hash functions is locality-sensitive and preserves rank\nsimilarity for inputs in any `p space. We then analyze different variations on\nthis strategy and show empirically that they outperform existing LSH methods\nfor nearest-neighbors search on six benchmark datasets. Finally, we propose a\nmulti-probe version of our algorithm that achieves higher performance for the\nsame query time, or conversely, that maintains performance of prior approaches\nwhile taking significantly less indexing time and memory. Overall, our approach\nleverages the advantages of separability provided by high-dimensional spaces,\nwhile still remaining computationally efficient</p>\n", "tags": ["Similarity-Search","Locality-Sensitive-Hashing","Hashing-Methods","Datasets","Evaluation","Efficiency"] },
{"key": "sharma2019retrieving", "year": "2019", "citations": "20", "title":"Retrieving Similar E-Commerce Images Using Deep Learning", "abstract": "<p>In this paper, we propose a deep convolutional neural network for learning\nthe embeddings of images in order to capture the notion of visual similarity.\nWe present a deep siamese architecture that when trained on positive and\nnegative pairs of images learn an embedding that accurately approximates the\nranking of images in order of visual similarity notion. We also implement a\nnovel loss calculation method using an angular loss metrics based on the\nproblems requirement. The final embedding of the image is combined\nrepresentation of the lower and top-level embeddings. We used fractional\ndistance matrix to calculate the distance between the learned embeddings in\nn-dimensional space. In the end, we compare our architecture with other\nexisting deep architecture and go on to demonstrate the superiority of our\nsolution in terms of image retrieval by testing the architecture on four\ndatasets. We also show how our suggested network is better than the other\ntraditional deep CNNs used for capturing fine-grained image similarities by\nlearning an optimum embedding.</p>\n", "tags": ["Datasets","Image-Retrieval"] },
{"key": "shen2018matchable", "year": "2019", "citations": "49", "title":"Matchable Image Retrieval by Learning from Surface Reconstruction", "abstract": "<p>Convolutional Neural Networks (CNNs) have achieved superior performance on\nobject image retrieval, while Bag-of-Words (BoW) models with handcrafted local\nfeatures still dominate the retrieval of overlapping images in 3D\nreconstruction. In this paper, we narrow down this gap by presenting an\nefficient CNN-based method to retrieve images with overlaps, which we refer to\nas the matchable image retrieval problem. Different from previous methods that\ngenerates training data based on sparse reconstruction, we create a large-scale\nimage database with rich 3D geometrics and exploit information from surface\nreconstruction to obtain fine-grained training data. We propose a batched\ntriplet-based loss function combined with mesh re-projection to effectively\nlearn the CNN representation. The proposed method significantly accelerates the\nimage retrieval process in 3D reconstruction and outperforms the\nstate-of-the-art CNN-based and BoW methods for matchable image retrieval. The\ncode and data are available at https://github.com/hlzz/mirror.</p>\n", "tags": ["Scalability","Evaluation","Image-Retrieval"] },
{"key": "shen2018unsupervised", "year": "2018", "citations": "377", "title":"Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization", "abstract": "<p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing\nwith significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval\nperformance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve\nsatisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a\nsimple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds\nover three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from\nthe widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph\nmatrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise\nan effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive\nexperiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>\n", "tags": ["Neural-Hashing","Tools-&-Libraries","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "shen2018zero", "year": "2018", "citations": "155", "title":"Zero-Shot Sketch-Image Hashing", "abstract": "<p>Recent studies show that large-scale sketch-based image retrieval (SBIR) can\nbe efficiently tackled by cross-modal binary representation learning methods,\nwhere Hamming distance matching significantly speeds up the process of\nsimilarity search. Providing training and test data subjected to a fixed set of\npre-defined categories, the cutting-edge SBIR and cross-modal hashing works\nobtain acceptable retrieval performance. However, most of the existing methods\nfail when the categories of query sketches have never been seen during\ntraining. In this paper, the above problem is briefed as a novel but realistic\nzero-shot SBIR hashing task. We elaborate the challenges of this special task\nand accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An\nend-to-end three-network architecture is built, two of which are treated as the\nbinary encoders. The third network mitigates the sketch-image heterogeneity and\nenhances the semantic relations among data by utilizing the Kronecker fusion\nlayer and graph convolution, respectively. As an important part of ZSIH, we\nformulate a generative hashing scheme in reconstructing semantic knowledge\nrepresentations for zero-shot retrieval. To the best of our knowledge, ZSIH is\nthe first zero-shot hashing work suitable for SBIR and cross-modal search.\nComprehensive experiments are conducted on two extended datasets, i.e., Sketchy\nand TU-Berlin with a novel zero-shot train-test split. The proposed model\nremarkably outperforms related works.</p>\n", "tags": ["Similarity-Search","Few-Shot-&-Zero-Shot","Image-Retrieval","Hashing-Methods","Datasets","CVPR","Scalability","Evaluation"] },
{"key": "shen2019embarrassingly", "year": "2019", "citations": "20", "title":"Embarrassingly Simple Binary Representation Learning", "abstract": "<p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash’ paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods","Scalability","ICCV","Compact-Codes"] },
{"key": "shen2020auto", "year": "2020", "citations": "113", "title":"Auto-Encoding Twin-Bottleneck Hashing", "abstract": "<p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>\n", "tags": ["Datasets","CVPR","Neural-Hashing","Tools-&-Libraries","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised","Graph-Based-ANN"] },
{"key": "shen2025auto", "year": "2020", "citations": "113", "title":"Auto-Encoding Twin-Bottleneck Hashing", "abstract": "<p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>\n", "tags": ["Datasets","CVPR","Neural-Hashing","Tools-&-Libraries","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised","Graph-Based-ANN"] },
{"key": "shen2025embarrassingly", "year": "2019", "citations": "20", "title":"Embarrassingly Simple Binary Representation Learning", "abstract": "<p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash’ paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods","Scalability","ICCV","Compact-Codes"] },
{"key": "shen2025unsupervised", "year": "2018", "citations": "377", "title":"Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization", "abstract": "<p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing\nwith significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval\nperformance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve\nsatisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a\nsimple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds\nover three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from\nthe widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph\nmatrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise\nan effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive\nexperiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>\n", "tags": ["Neural-Hashing","Tools-&-Libraries","Supervised","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "shi2018scalable", "year": "2020", "citations": "11", "title":"A Scalable Optimization Mechanism for Pairwise based Discrete Hashing", "abstract": "<p>Maintaining the pair similarity relationship among originally\nhigh-dimensional data into a low-dimensional binary space is a popular strategy\nto learn binary codes. One simiple and intutive method is to utilize two\nidentical code matrices produced by hash functions to approximate a pairwise\nreal label matrix. However, the resulting quartic problem is difficult to\ndirectly solve due to the non-convex and non-smooth nature of the objective. In\nthis paper, unlike previous optimization methods using various relaxation\nstrategies, we aim to directly solve the original quartic problem using a novel\nalternative optimization mechanism to linearize the quartic problem by\nintroducing a linear regression model. Additionally, we find that gradually\nlearning each batch of binary codes in a sequential mode, i.e. batch by batch,\nis greatly beneficial to the convergence of binary code learning. Based on this\nsignificant discovery and the proposed strategy, we introduce a scalable\nsymmetric discrete hashing algorithm that gradually and smoothly updates each\nbatch of binary codes. To further improve the smoothness, we also propose a\ngreedy symmetric discrete hashing algorithm to update each bit of batch binary\ncodes. Moreover, we extend the proposed optimization mechanism to solve the\nnon-convex optimization problems for binary code learning in many other\npairwise based hashing algorithms. Extensive experiments on benchmark\nsingle-label and multi-label databases demonstrate the superior performance of\nthe proposed mechanism over recent state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods","Compact-Codes","Evaluation"] },
{"key": "shi2019compositional", "year": "2020", "citations": "39", "title":"Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems", "abstract": "<p>Modern deep learning-based recommendation systems exploit hundreds to\nthousands of different categorical features, each with millions of different\ncategories ranging from clicks to posts. To respect the natural diversity\nwithin the categorical data, embeddings map each category to a unique dense\nrepresentation within an embedded space. Since each categorical feature could\ntake on as many as tens of millions of different possible categories, the\nembedding tables form the primary memory bottleneck during both training and\ninference. We propose a novel approach for reducing the embedding size in an\nend-to-end fashion by exploiting complementary partitions of the category set\nto produce a unique embedding vector for each category without explicit\ndefinition. By storing multiple smaller embedding tables based on each\ncomplementary partition and combining embeddings from each table, we define a\nunique embedding for each category at smaller memory cost. This approach may be\ninterpreted as using a specific fixed codebook to ensure uniqueness of each\ncategory’s representation. Our experimental results demonstrate the\neffectiveness of our approach over the hashing trick for reducing the size of\nthe embedding tables in terms of model loss and accuracy, while retaining a\nsimilar reduction in the number of parameters.</p>\n", "tags": ["KDD","Hashing-Methods","Recommender-Systems","Evaluation"] },
{"key": "shin2019semi", "year": "2019", "citations": "9", "title":"Semi-supervised Feature-Level Attribute Manipulation for Fashion Image Retrieval", "abstract": "<p>With a growing demand for the search by image, many works have studied the\ntask of fashion instance-level image retrieval (FIR). Furthermore, the recent\nworks introduce a concept of fashion attribute manipulation (FAM) which\nmanipulates a specific attribute (e.g color) of a fashion item while\nmaintaining the rest of the attributes (e.g shape, and pattern). In this way,\nusers can search not only “the same” items but also “similar” items with the\ndesired attributes. FAM is a challenging task in that the attributes are hard\nto define, and the unique characteristics of a query are hard to be preserved.\nAlthough both FIR and FAM are important in real-life applications, most of the\nprevious studies have focused on only one of these problem. In this study, we\naim to achieve competitive performance on both FIR and FAM. To do so, we\npropose a novel method that converts a query into a representation with the\ndesired attributes. We introduce a new idea of attribute manipulation at the\nfeature level, by matching the distribution of manipulated features with real\nfeatures. In this fashion, the attribute manipulation can be done independently\nfrom learning a representation from the image. By introducing the feature-level\nattribute manipulation, the previous methods for FIR can perform attribute\nmanipulation without sacrificing their retrieval performance.</p>\n", "tags": ["Supervised","Evaluation","Image-Retrieval"] },
{"key": "shrivastava2014asymmetric", "year": "2014", "citations": "268", "title":"Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).", "abstract": "<p>We present the first provably sublinear time hashing algorithm for approximate\nMaximum Inner Product Search (MIPS). Searching with (un-normalized) inner\nproduct as the underlying similarity measure is a known difficult problem and\nfinding hashing schemes for MIPS was considered hard. While the existing Locality\nSensitive Hashing (LSH) framework is insufficient for solving MIPS, in this\npaper we extend the LSH framework to allow asymmetric hashing schemes. Our\nproposal is based on a key observation that the problem of finding maximum inner\nproducts, after independent asymmetric transformations, can be converted into\nthe problem of approximate near neighbor search in classical settings. This key\nobservation makes efficient sublinear hashing scheme for MIPS possible. Under\nthe extended asymmetric LSH (ALSH) framework, this paper provides an example\nof explicit construction of provably fast hashing scheme for MIPS. Our proposed\nalgorithm is simple and easy to implement. The proposed hashing scheme\nleads to significant computational savings over the two popular conventional LSH\nschemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable\ndistributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations\non Netflix and Movielens (10M) datasets.</p>\n", "tags": ["Datasets","Locality-Sensitive-Hashing","Tools-&-Libraries","Hashing-Methods","Recommender-Systems"] },
{"key": "shrivastava2014densifying", "year": "2014", "citations": "86", "title":"Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search", "abstract": "<p>The query complexity of locality sensitive hashing\n(LSH) based similarity search is dominated\nby the number of hash evaluations, and this number\ngrows with the data size (Indyk &amp; Motwani,\n1998). In industrial applications such as search\nwhere the data are often high-dimensional and\nbinary (e.g., text n-grams), minwise hashing is\nwidely adopted, which requires applying a large\nnumber of permutations on the data. This is\ncostly in computation and energy-consumption.\nIn this paper, we propose a hashing technique\nwhich generates all the necessary hash evaluations\nneeded for similarity search, using one\nsingle permutation. The heart of the proposed\nhash function is a “rotation” scheme which densifies\nthe sparse sketches of one permutation\nhashing (Li et al., 2012) in an unbiased fashion\nthereby maintaining the LSH property. This\nmakes the obtained sketches suitable for hash table\nconstruction. This idea of rotation presented\nin this paper could be of independent interest for\ndensifying other types of sparse sketches.\nUsing our proposed hashing method, the query\ntime of a (K, L)-parameterized LSH is reduced\nfrom the typical O(dKL) complexity to merely\nO(KL + dL), where d is the number of nonzeros\nof the data vector, K is the number of hashes\nin each hash table, and L is the number of hash\ntables. Our experimental evaluation on real data\nconfirms that the proposed scheme significantly\nreduces the query processing time over minwise\nhashing without loss in retrieval accuracies.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Evaluation","Similarity-Search"] },
{"key": "shrivastava2016exact", "year": "2016", "citations": "7", "title":"Exact Weighted Minwise Hashing in Constant Time", "abstract": "<p>Weighted minwise hashing (WMH) is one of the fundamental subroutine, required\nby many celebrated approximation algorithms, commonly adopted in industrial\npractice for large scale-search and learning. The resource bottleneck of the\nalgorithms is the computation of multiple (typically a few hundreds to\nthousands) independent hashes of the data. The fastest hashing algorithm is by\nIoffe \\cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data\nvector, \\(O(d)\\) (\\(d\\) is the number of non-zeros), for computing one hash.\nHowever, the requirement of multiple hashes demands hundreds or thousands\npasses over the data. This is very costly for modern massive dataset.\n  In this work, we break this expensive barrier and show an expected constant\namortized time algorithm which computes \\(k\\) independent and unbiased WMH in\ntime \\(O(k)\\) instead of \\(O(dk)\\) required by Ioffe’s method. Moreover, our\nproposal only needs a few bits (5 - 9 bits) of storage per hash value compared\nto around \\(64\\) bits required by the state-of-art-methodologies. Experimental\nevaluations, on real datasets, show that for computing 500 WMH, our proposal\ncan be 60000x faster than the Ioffe’s method without losing any accuracy. Our\nmethod is also around 100x faster than approximate heuristics capitalizing on\nthe efficient “densified” one permutation hashing schemes\n\\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its\nsignificant advantages, we hope that it will replace existing implementations\nin practice.</p>\n", "tags": ["Hashing-Methods","Datasets"] },
{"key": "shrivastava2017optimal", "year": "2017", "citations": "27", "title":"Optimal Densification for Fast and Accurate Minwise Hashing", "abstract": "<p>Minwise hashing is a fundamental and one of the most successful hashing\nalgorithm in the literature. Recent advances based on the idea of\ndensification~\\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown\nthat it is possible to compute \\(k\\) minwise hashes, of a vector with \\(d\\)\nnonzeros, in mere \\((d + k)\\) computations, a significant improvement over the\nclassical \\(O(dk)\\). These advances have led to an algorithmic improvement in the\nquery complexity of traditional indexing algorithms based on minwise hashing.\nUnfortunately, the variance of the current densification techniques is\nunnecessarily high, which leads to significantly poor accuracy compared to\nvanilla minwise hashing, especially when the data is sparse. In this paper, we\nprovide a novel densification scheme which relies on carefully tailored\n2-universal hashes. We show that the proposed scheme is variance-optimal, and\nwithout losing the runtime efficiency, it is significantly more accurate than\nexisting densification techniques. As a result, we obtain a significantly\nefficient hashing scheme which has the same variance and collision probability\nas minwise hashing. Experimental evaluations on real sparse and\nhigh-dimensional datasets validate our claims. We believe that given the\nsignificant advantages, our method will replace minwise hashing implementations\nin practice.</p>\n", "tags": ["Hashing-Methods","Datasets","Efficiency"] },
{"key": "shrivastava2025asymmetric", "year": "2014", "citations": "268", "title":"Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).", "abstract": "<p>We present the first provably sublinear time hashing algorithm for approximate\nMaximum Inner Product Search (MIPS). Searching with (un-normalized) inner\nproduct as the underlying similarity measure is a known difficult problem and\nfinding hashing schemes for MIPS was considered hard. While the existing Locality\nSensitive Hashing (LSH) framework is insufficient for solving MIPS, in this\npaper we extend the LSH framework to allow asymmetric hashing schemes. Our\nproposal is based on a key observation that the problem of finding maximum inner\nproducts, after independent asymmetric transformations, can be converted into\nthe problem of approximate near neighbor search in classical settings. This key\nobservation makes efficient sublinear hashing scheme for MIPS possible. Under\nthe extended asymmetric LSH (ALSH) framework, this paper provides an example\nof explicit construction of provably fast hashing scheme for MIPS. Our proposed\nalgorithm is simple and easy to implement. The proposed hashing scheme\nleads to significant computational savings over the two popular conventional LSH\nschemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable\ndistributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations\non Netflix and Movielens (10M) datasets.</p>\n", "tags": ["Datasets","Locality-Sensitive-Hashing","Tools-&-Libraries","Hashing-Methods","Recommender-Systems"] },
{"key": "shrivastava2025densifying", "year": "2014", "citations": "86", "title":"Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search", "abstract": "<p>The query complexity of locality sensitive hashing\n(LSH) based similarity search is dominated\nby the number of hash evaluations, and this number\ngrows with the data size (Indyk &amp; Motwani,\n1998). In industrial applications such as search\nwhere the data are often high-dimensional and\nbinary (e.g., text n-grams), minwise hashing is\nwidely adopted, which requires applying a large\nnumber of permutations on the data. This is\ncostly in computation and energy-consumption.\nIn this paper, we propose a hashing technique\nwhich generates all the necessary hash evaluations\nneeded for similarity search, using one\nsingle permutation. The heart of the proposed\nhash function is a “rotation” scheme which densifies\nthe sparse sketches of one permutation\nhashing (Li et al., 2012) in an unbiased fashion\nthereby maintaining the LSH property. This\nmakes the obtained sketches suitable for hash table\nconstruction. This idea of rotation presented\nin this paper could be of independent interest for\ndensifying other types of sparse sketches.\nUsing our proposed hashing method, the query\ntime of a (K, L)-parameterized LSH is reduced\nfrom the typical O(dKL) complexity to merely\nO(KL + dL), where d is the number of nonzeros\nof the data vector, K is the number of hashes\nin each hash table, and L is the number of hash\ntables. Our experimental evaluation on real data\nconfirms that the proposed scheme significantly\nreduces the query processing time over minwise\nhashing without loss in retrieval accuracies.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing","Evaluation","Similarity-Search"] },
{"key": "shu2017compressing", "year": "2017", "citations": "95", "title":"Compressing Word Embeddings via Deep Compositional Code Learning", "abstract": "<p>Natural language processing (NLP) models often require a massive number of\nparameters for word embeddings, resulting in a large storage or memory\nfootprint. Deploying neural NLP models to mobile devices requires compressing\nthe word embeddings without any significant sacrifices in performance. For this\npurpose, we propose to construct the embeddings with few basis vectors. For\neach word, the composition of basis vectors is determined by a hash code. To\nmaximize the compression rate, we adopt the multi-codebook quantization\napproach instead of binary coding scheme. Each code is composed of multiple\ndiscrete numbers, such as (3, 2, 1, 8), where the value of each component is\nlimited to a fixed range. We propose to directly learn the discrete codes in an\nend-to-end neural network by applying the Gumbel-softmax trick. Experiments\nshow the compression rate achieves 98% in a sentiment analysis task and 94% ~\n99% in machine translation tasks without performance loss. In both tasks, the\nproposed method can improve the model performance by slightly lowering the\ncompression rate. Compared to other approaches such as character-level\nsegmentation, the proposed method is language-independent and does not require\nmodifications to the network architecture.</p>\n", "tags": ["Hashing-Methods","Quantization","Evaluation"] },
{"key": "shukor2022transformer", "year": "2022", "citations": "23", "title":"Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval", "abstract": "<p>Cross-modal image-recipe retrieval has gained significant attention in recent\nyears. Most work focuses on improving cross-modal embeddings using unimodal\nencoders, that allow for efficient retrieval in large-scale databases, leaving\naside cross-attention between modalities which is more computationally\nexpensive. We propose a new retrieval framework, T-Food (Transformer Decoders\nwith MultiModal Regularization for Cross-Modal Food Retrieval) that exploits\nthe interaction between modalities in a novel regularization scheme, while\nusing only unimodal encoders at test time for efficient retrieval. We also\ncapture the intra-dependencies between recipe entities with a dedicated recipe\nencoder, and propose new variants of triplet losses with dynamic margins that\nadapt to the difficulty of the task. Finally, we leverage the power of the\nrecent Vision and Language Pretraining (VLP) models such as CLIP for the image\nencoder. Our approach outperforms existing approaches by a large margin on the\nRecipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6\nR@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code\nis available here:https://github.com/mshukor/TFood</p>\n", "tags": ["Similarity-Search","Tools-&-Libraries","Distance-Metric-Learning","Datasets","CVPR","Scalability"] },
{"key": "shvetsova2021everything", "year": "2022", "citations": "98", "title":"Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval", "abstract": "<p>Multi-modal learning from video data has seen increased attention recently as\nit allows to train semantically meaningful embeddings without human annotation\nenabling tasks like zero-shot retrieval and classification. In this work, we\npresent a multi-modal, modality agnostic fusion transformer approach that\nlearns to exchange information between multiple modalities, such as video,\naudio, and text, and integrate them into a joined multi-modal representation to\nobtain an embedding that aggregates multi-modal temporal information. We\npropose to train the system with a combinatorial loss on everything at once,\nsingle modalities as well as pairs of modalities, explicitly leaving out any\nadd-ons such as position or modality encoding. At test time, the resulting\nmodel can process and fuse any number of input modalities. Moreover, the\nimplicit properties of the transformer allow to process inputs of different\nlengths. To evaluate the proposed approach, we train the model on the large\nscale HowTo100M dataset and evaluate the resulting embedding space on four\nchallenging benchmark datasets obtaining state-of-the-art results in zero-shot\nvideo retrieval and zero-shot video action localization.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Datasets","Video-Retrieval","CVPR","Evaluation"] },
{"key": "siméoni2019local", "year": "2019", "citations": "70", "title":"Local Features and Visual Words Emerge in Activations", "abstract": "<p>We propose a novel method of deep spatial matching (DSM) for image retrieval.\nInitial ranking is based on image descriptors extracted from convolutional\nneural network activations by global pooling, as in recent state-of-the-art\nwork. However, the same sparse 3D activation tensor is also approximated by a\ncollection of local features. These local features are then robustly matched to\napproximate the optimal alignment of the tensors. This happens without any\nnetwork modification, additional layers or training. No local feature detection\nhappens on the original image. No local feature descriptors and no visual\nvocabulary are needed throughout the whole process.\n  We experimentally show that the proposed method achieves the state-of-the-art\nperformance on standard benchmarks across different network architectures and\ndifferent global pooling methods. The highest gain in performance is achieved\nwhen diffusion on the nearest-neighbor graph of global descriptors is initiated\nfrom spatially verified images.</p>\n", "tags": ["CVPR","Evaluation","Image-Retrieval"] },
{"key": "singh2019one", "year": "2019", "citations": "6", "title":"One Embedding To Do Them All", "abstract": "<p>Online shopping caters to the needs of millions of users daily. Search,\nrecommendations, personalization have become essential building blocks for\nserving customer needs. Efficacy of such systems is dependent on a thorough\nunderstanding of products and their representation. Multiple information\nsources and data types provide a complete picture of the product on the\nplatform. While each of these tasks shares some common characteristics,\ntypically product embeddings are trained and used in isolation.\n  In this paper, we propose a framework to combine multiple data sources and\nlearn unified embeddings for products on our e-commerce platform. Our product\nembeddings are built from three types of data sources - catalog text data, a\nuser’s clickstream session data and product images. We use various techniques\nlike denoising auto-encoders for text, Bayesian personalized ranking (BPR) for\nclickstream data, Siamese neural network architecture for image data and\ncombined ensemble over the above methods for unified embeddings. Further, we\ncompare and analyze the performance of these embeddings across three unrelated\nreal-world e-commerce tasks specifically checking product attribute coverage,\nfinding similar products and predicting returns. We show that unified product\nembeddings perform uniformly well across all these tasks.</p>\n", "tags": ["Evaluation","Tools-&-Libraries"] },
{"key": "song2013inter", "year": "2013", "citations": "619", "title":"Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources", "abstract": "<p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users’ demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query’s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods","Scalability","Datasets","Large-Scale-Search"] },
{"key": "song2015deep", "year": "2016", "citations": "1656", "title":"Deep Metric Learning via Lifted Structured Feature Embedding", "abstract": "<p>Learning the distance metric between pairs of examples is of great importance\nfor learning and visual recognition. With the remarkable success from the state\nof the art convolutional neural networks, recent works have shown promising\nresults on discriminatively training the networks to learn semantic feature\nembeddings where similar examples are mapped close to each other and dissimilar\nexamples are mapped farther apart. In this paper, we describe an algorithm for\ntaking full advantage of the training batches in the neural network training by\nlifting the vector of pairwise distances within the batch to the matrix of\npairwise distances. This step enables the algorithm to learn the state of the\nart feature embedding by optimizing a novel structured prediction objective on\nthe lifted problem. Additionally, we collected Online Products dataset: 120k\nimages of 23k classes of online products for metric learning. Our experiments\non the CUB-200-2011, CARS196, and Online Products datasets demonstrate\nsignificant improvement over existing deep feature embedding methods on all\nexperimented embedding sizes with the GoogLeNet network.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","CVPR"] },
{"key": "song2015top", "year": "2015", "citations": "76", "title":"Top Rank Supervised Binary Coding for Visual Search", "abstract": "<p>In recent years, binary coding techniques are becoming\nincreasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been\ndemonstrated that supervised binary coding techniques that\nleverage supervised information can significantly enhance\nthe coding quality, and hence greatly benefit visual search\ntasks. Typically, a modern binary coding method seeks\nto learn a group of coding functions which compress data\nsamples into binary codes. However, few methods pursued\nthe coding functions such that the precision at the top of\na ranking list according to Hamming distances of the generated binary codes is optimized.\nIn this paper, we propose a novel supervised binary coding approach, namely\nTop Rank Supervised Binary Coding (Top-RSBC), which\nexplicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train\nthe disciplined coding functions, by which the mistakes at\nthe top of a Hamming-distance ranking list are penalized\nmore than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective\nwith a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online\nlearning algorithm to optimize the surrogate objective more\nefficiently. Empirical studies based upon three benchmark\nimage datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over\nthe state-of-the-arts.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","ICCV","Datasets","Compact-Codes","Evaluation","Supervised"] },
{"key": "song2016deep", "year": "2017", "citations": "308", "title":"Deep Metric Learning via Facility Location", "abstract": "<p>Learning the representation and the similarity metric in an end-to-end\nfashion with deep networks have demonstrated outstanding results for clustering\nand retrieval. However, these recent approaches still suffer from the\nperformance degradation stemming from the local metric training procedure which\nis unaware of the global structure of the embedding space.\n  We propose a global metric learning scheme for optimizing the deep metric\nembedding with the learnable clustering function and the clustering metric\n(NMI) in a novel structured prediction framework.\n  Our experiments on CUB200-2011, Cars196, and Stanford online products\ndatasets show state of the art performance both on the clustering and retrieval\ntasks measured in the NMI and Recall@K evaluation metrics.</p>\n", "tags": ["Tools-&-Libraries","Distance-Metric-Learning","Datasets","CVPR","Evaluation"] },
{"key": "song2017binary", "year": "2018", "citations": "164", "title":"Binary Generative Adversarial Networks for Image Retrieval", "abstract": "<p>The most striking successes in image retrieval using deep hashing have mostly\ninvolved discriminative models, which require labels. In this paper, we use\nbinary generative adversarial networks (BGAN) to embed images to binary codes\nin an unsupervised way. By restricting the input noise variable of generative\nadversarial networks (GAN) to be binary and conditioned on the features of each\ninput image, BGAN can simultaneously learn a binary representation per image,\nand generate an image plausibly similar to the original one. In the proposed\nframework, we address two main problems: 1) how to directly generate binary\ncodes without relaxation? 2) how to equip the binary representation with the\nability of accurate image retrieval? We resolve these problems by proposing new\nsign-activation strategy and a loss function steering the learning process,\nwhich consists of new models for adversarial loss, a content loss, and a\nneighborhood structure loss. Experimental results on standard datasets\n(CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly\noutperforms existing hashing methods by up to 107% in terms of~mAP (See Table\ntab.res.map.comp) Our anonymous code is available at:\nhttps://github.com/htconquer/BGAN.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","AAAI","Unsupervised","Evaluation","Robustness"] },
{"key": "song2017deep", "year": "2017", "citations": "308", "title":"Deep Discrete Hashing with Self-supervised Pairwise Labels", "abstract": "<p>Hashing methods have been widely used for applications of large-scale image\nretrieval and classification. Non-deep hashing methods using handcrafted\nfeatures have been significantly outperformed by deep hashing methods due to\ntheir better feature representation and end-to-end learning framework. However,\nthe most striking successes in deep hashing have mostly involved discriminative\nmodels, which require labels. In this paper, we propose a novel unsupervised\ndeep hashing method, named Deep Discrete Hashing (DDH), for large-scale image\nretrieval and classification. In the proposed framework, we address two main\nproblems: 1) how to directly learn discrete binary codes? 2) how to equip the\nbinary representation with the ability of accurate image retrieval and\nclassification in an unsupervised way? We resolve these problems by introducing\nan intermediate variable and a loss function steering the learning process,\nwhich is based on the neighborhood structure in the original space.\nExperimental results on standard datasets (CIFAR-10, NUS-WIDE, and Oxford-17)\ndemonstrate that our DDH significantly outperforms existing hashing methods by\nlarge margin in terms of~mAP for image retrieval and object recognition. Code\nis available at https://github.com/htconquer/ddh.</p>\n", "tags": ["Unsupervised","Supervised","Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","CVPR","Self-Supervised","Scalability","Evaluation"] },
{"key": "song2018cross", "year": "2018", "citations": "7", "title":"Cross-Modal Retrieval with Implicit Concept Association", "abstract": "<p>Traditional cross-modal retrieval assumes explicit association of concepts\nacross modalities, where there is no ambiguity in how the concepts are linked\nto each other, e.g., when we do the image search with a query “dogs”, we expect\nto see dog images. In this paper, we consider a different setting for\ncross-modal retrieval where data from different modalities are implicitly\nlinked via concepts that must be inferred by high-level reasoning; we call this\nsetting implicit concept association. To foster future research in this\nsetting, we present a new dataset containing 47K pairs of animated GIFs and\nsentences crawled from the web, in which the GIFs depict physical or emotional\nreactions to the scenarios described in the text (called “reaction GIFs”). We\nreport on a user study showing that, despite the presence of implicit concept\nassociation, humans are able to identify video-sentence pairs with matching\nconcepts, suggesting the feasibility of our task. Furthermore, we propose a\nnovel visual-semantic embedding network based on multiple instance learning.\nUnlike traditional approaches, we compute multiple embeddings from each\nmodality, each representing different concepts, and measure their similarity by\nconsidering all possible combinations of visual-semantic embeddings in the\nframework of multiple instance learning. We evaluate our approach on two\nvideo-sentence datasets with explicit and implicit concept association and\nreport competitive results compared to existing approaches on cross-modal\nretrieval.</p>\n", "tags": ["Datasets","Image-Retrieval","Tools-&-Libraries","Multimodal-Retrieval"] },
{"key": "song2018self", "year": "2018", "citations": "165", "title":"Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder", "abstract": "<p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>\n", "tags": ["Video-Retrieval","Unsupervised","Datasets","Self-Supervised","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "song2019deep", "year": "2019", "citations": "17", "title":"Deep Hashing Learning for Visual and Semantic Retrieval of Remote Sensing Images", "abstract": "<p>Driven by the urgent demand for managing remote sensing big data, large-scale\nremote sensing image retrieval (RSIR) attracts increasing attention in the\nremote sensing field. In general, existing retrieval methods can be regarded as\nvisual-based retrieval approaches which search and return a set of similar\nimages from a database to a given query image. Although retrieval methods have\nachieved great success, there is still a question that needs to be responded\nto: Can we obtain the accurate semantic labels of the returned similar images\nto further help analyzing and processing imagery? Inspired by the above\nquestion, in this paper, we redefine the image retrieval problem as visual and\nsemantic retrieval of images. Specifically, we propose a novel deep hashing\nconvolutional neural network (DHCNN) to simultaneously retrieve the similar\nimages and classify their semantic labels in a unified framework. In more\ndetail, a convolutional neural network (CNN) is used to extract\nhigh-dimensional deep features. Then, a hash layer is perfectly inserted into\nthe network to transfer the deep features into compact hash codes. In addition,\na fully connected layer with a softmax function is performed on hash layer to\ngenerate class distribution. Finally, a loss function is elaborately designed\nto simultaneously consider the label loss of each image and similarity loss of\npairs of images. Experimental results on two remote sensing datasets\ndemonstrate that the proposed method achieves the state-of-art retrieval and\nclassification performance.</p>\n", "tags": ["Scalability","Evaluation","Datasets","AAAI","Hashing-Methods","Tools-&-Libraries","Neural-Hashing","IJCAI","Image-Retrieval"] },
{"key": "song2019polysemous", "year": "2019", "citations": "223", "title":"Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval", "abstract": "<p>Visual-semantic embedding aims to find a shared latent space where related\nvisual and textual instances are close to each other. Most current methods\nlearn injective embedding functions that map an instance to a single point in\nthe shared space. Unfortunately, injective embedding cannot effectively handle\npolysemous instances with multiple possible meanings; at best, it would find an\naverage representation of different meanings. This hinders its use in\nreal-world scenarios where individual instances and their cross-modal\nassociations are often ambiguous. In this work, we introduce Polysemous\nInstance Embedding Networks (PIE-Nets) that compute multiple and diverse\nrepresentations of an instance by combining global context with locally-guided\nfeatures via multi-head self-attention and residual learning. To learn\nvisual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in\nthe multiple instance learning framework. Most existing work on cross-modal\nretrieval focuses on image-text data. Here, we also tackle a more challenging\ncase of video-text retrieval. To facilitate further research in video-text\nretrieval, we release a new dataset of 50K video-sentence pairs collected from\nsocial media, dubbed MRW (my reaction when). We demonstrate our approach on\nboth image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our\nnew MRW dataset.</p>\n", "tags": ["Text-Retrieval","Tools-&-Libraries","Datasets","CVPR","Evaluation","Multimodal-Retrieval"] },
{"key": "song2022asymmetric", "year": "2022", "citations": "44", "title":"Asymmetric Hash Code Learning for Remote Sensing Image Retrieval", "abstract": "<p>Remote sensing image retrieval (RSIR), aiming at searching for a set of\nsimilar items to a given query image, is a very important task in remote\nsensing applications. Deep hashing learning as the current mainstream method\nhas achieved satisfactory retrieval performance. On one hand, various deep\nneural networks are used to extract semantic features of remote sensing images.\nOn the other hand, the hashing techniques are subsequently adopted to map the\nhigh-dimensional deep features to the low-dimensional binary codes. This kind\nof methods attempts to learn one hash function for both the query and database\nsamples in a symmetric way. However, with the number of database samples\nincreasing, it is typically time-consuming to generate the hash codes of\nlarge-scale database images. In this paper, we propose a novel deep hashing\nmethod, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL\ngenerates the hash codes of query and database images in an asymmetric way. In\nmore detail, the hash codes of query images are obtained by binarizing the\noutput of the network, while the hash codes of database images are directly\nlearned by solving the designed objective function. In addition, we combine the\nsemantic information of each image and the similarity information of pairs of\nimages as supervised information to train a deep hashing network, which\nimproves the representation ability of deep features and hash codes. The\nexperimental results on three public datasets demonstrate that the proposed\nmethod outperforms symmetric methods in terms of retrieval accuracy and\nefficiency. The source code is available at\nhttps://github.com/weiweisong415/Demo AHCL for TGRS2022.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Scalability","Evaluation","Efficiency"] },
{"key": "song2022boosting", "year": "2023", "citations": "28", "title":"Boosting vision transformers for image retrieval", "abstract": "<p>Vision transformers have achieved remarkable progress in vision tasks such as\nimage classification and detection. However, in instance-level image retrieval,\ntransformers have not yet shown good performance compared to convolutional\nnetworks. We propose a number of improvements that make transformers outperform\nthe state of the art for the first time. (1) We show that a hybrid architecture\nis more effective than plain transformers, by a large margin. (2) We introduce\ntwo branches collecting global (classification token) and local (patch tokens)\ninformation, from which we form a global image representation. (3) In each\nbranch, we collect multi-layer features from the transformer encoder,\ncorresponding to skip connections across distant layers. (4) We enhance\nlocality of interactions at the deeper layers of the encoder, which is the\nrelative weakness of vision transformers. We train our model on all commonly\nused training sets and, for the first time, we make fair comparisons separately\nper training set. In all cases, we outperform previous models based on global\nrepresentation. Public code is available at\nhttps://github.com/dealicious-inc/DToP.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "song2025inter", "year": "2013", "citations": "619", "title":"Inter-Media Hashing for Large-Scale Retrieval from Heterogeneous Data Sources", "abstract": "<p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users’ demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query’s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>\n", "tags": ["Hashing-Methods","Scalability","Datasets","Large-Scale-Search"] },
{"key": "song2025self", "year": "2018", "citations": "165", "title":"Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder", "abstract": "<p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>\n", "tags": ["Video-Retrieval","Unsupervised","Datasets","Self-Supervised","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "song2025top", "year": "2015", "citations": "76", "title":"Top Rank Supervised Binary Coding for Visual Search", "abstract": "<p>In recent years, binary coding techniques are becoming\nincreasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been\ndemonstrated that supervised binary coding techniques that\nleverage supervised information can significantly enhance\nthe coding quality, and hence greatly benefit visual search\ntasks. Typically, a modern binary coding method seeks\nto learn a group of coding functions which compress data\nsamples into binary codes. However, few methods pursued\nthe coding functions such that the precision at the top of\na ranking list according to Hamming distances of the generated binary codes is optimized.\nIn this paper, we propose a novel supervised binary coding approach, namely\nTop Rank Supervised Binary Coding (Top-RSBC), which\nexplicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train\nthe disciplined coding functions, by which the mistakes at\nthe top of a Hamming-distance ranking list are penalized\nmore than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective\nwith a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online\nlearning algorithm to optimize the surrogate objective more\nefficiently. Empirical studies based upon three benchmark\nimage datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over\nthe state-of-the-arts.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","ICCV","Datasets","Compact-Codes","Evaluation","Supervised"] },
{"key": "srinivas2018merging", "year": "2018", "citations": "7", "title":"Merging datasets through deep learning", "abstract": "<p>Merging datasets is a key operation for data analytics. A frequent\nrequirement for merging is joining across columns that have different surface\nforms for the same entity (e.g., the name of a person might be represented as\n“Douglas Adams” or “Adams, Douglas”). Similarly, ontology alignment can require\nrecognizing distinct surface forms of the same entity, especially when\nontologies are independently developed. However, data management systems are\ncurrently limited to performing merges based on string equality, or at best\nusing string similarity. We propose an approach to performing merges based on\ndeep learning models. Our approach depends on (a) creating a deep learning\nmodel that maps surface forms of an entity into a set of vectors such that\nalternate forms for the same entity are closest in vector space, (b) indexing\nthese vectors using a nearest neighbors algorithm to find the forms that can be\npotentially joined together. To build these models, we had to adapt techniques\nfrom metric learning due to the characteristics of the data; specifically we\ndescribe novel sample selection techniques and loss functions that work for\nthis problem. To evaluate our approach, we used Wikidata as ground truth and\nbuilt models from datasets with approximately 1.1M people’s names (200K\nidentities) and 130K company names (70K identities). We developed models that\nallow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the\nmodels available for aligning people or companies across multiple datasets.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "srivastava20163d", "year": "2016", "citations": "8", "title":"3D Binary Signatures", "abstract": "<p>In this paper, we propose a novel binary descriptor for 3D point clouds. The\nproposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the\nmatching efficiency of the binary descriptors for 2D images. 3DBS describes\nkeypoints from point clouds with a binary vector resulting in extremely fast\nmatching. The method uses keypoints from standard keypoint detectors. The\ndescriptor is built by constructing a Local Reference Frame and aligning a\nlocal surface patch accordingly. The local surface patch constitutes of\nidentifying nearest neighbours based upon an angular constraint among them. The\npoints are ordered with respect to the distance from the keypoints. The normals\nof the ordered pairs of these keypoints are projected on the axes and the\nrelative magnitude is used to assign a binary digit. The vector thus\nconstituted is used as a signature for representing the keypoints. The matching\nis done by using hamming distance. We show that 3DBS outperforms state of the\nart descriptors on various evaluation metrics.</p>\n", "tags": ["Evaluation","Efficiency"] },
{"key": "srivastava20173d", "year": "2016", "citations": "8", "title":"3D Binary Signatures", "abstract": "<p>In this paper, we propose a novel binary descriptor for 3D point clouds. The\nproposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the\nmatching efficiency of the binary descriptors for 2D images. 3DBS describes\nkeypoints from point clouds with a binary vector resulting in extremely fast\nmatching. The method uses keypoints from standard keypoint detectors. The\ndescriptor is built by constructing a Local Reference Frame and aligning a\nlocal surface patch accordingly. The local surface patch constitutes of\nidentifying nearest neighbours based upon an angular constraint among them. The\npoints are ordered with respect to the distance from the keypoints. The normals\nof the ordered pairs of these keypoints are projected on the axes and the\nrelative magnitude is used to assign a binary digit. The vector thus\nconstituted is used as a signature for representing the keypoints. The matching\nis done by using hamming distance. We show that 3DBS outperforms state of the\nart descriptors on various evaluation metrics.</p>\n", "tags": ["Evaluation","Efficiency"] },
{"key": "staszewski2020new", "year": "2021", "citations": "22", "title":"A new approach to descriptors generation for image retrieval by analyzing activations of deep neural network layers", "abstract": "<p>In this paper, we consider the problem of descriptors construction for the\ntask of content-based image retrieval using deep neural networks. The idea of\nneural codes, based on fully connected layers activations, is extended by\nincorporating the information contained in convolutional layers. It is known\nthat the total number of neurons in the convolutional part of the network is\nlarge and the majority of them have little influence on the final\nclassification decision. Therefore, in the paper we propose a novel algorithm\nthat allows us to extract the most significant neuron activations and utilize\nthis information to construct effective descriptors. The descriptors consisting\nof values taken from both the fully connected and convolutional layers\nperfectly represent the whole image content. The images retrieved using these\ndescriptors match semantically very well to the query image, and also they are\nsimilar in other secondary image characteristics, like background, textures or\ncolor distribution. These features of the proposed descriptors are verified\nexperimentally based on the IMAGENET1M dataset using the VGG16 neural network.</p>\n", "tags": ["Datasets","Image-Retrieval"] },
{"key": "stein2021self", "year": "2021", "citations": "13", "title":"Self-supervised similarity search for large scientific datasets", "abstract": "<p>We present the use of self-supervised learning to explore and exploit large\nunlabeled datasets. Focusing on 42 million galaxy images from the latest data\nrelease of the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging\nSurveys, we first train a self-supervised model to distill low-dimensional\nrepresentations that are robust to symmetries, uncertainties, and noise in each\nimage. We then use the representations to construct and publicly release an\ninteractive semantic similarity search tool. We demonstrate how our tool can be\nused to rapidly discover rare objects given only a single example, increase the\nspeed of crowd-sourcing campaigns, and construct and improve training sets for\nsupervised applications. While we focus on images from sky surveys, the\ntechnique is straightforward to apply to any scientific dataset of any\ndimensionality. The similarity search web app can be found at\nhttps://github.com/georgestein/galaxy_search</p>\n", "tags": ["Similarity-Search","Self-Supervised","Datasets","Supervised"] },
{"key": "studer2019comprehensive", "year": "2019", "citations": "66", "title":"A Comprehensive Study of ImageNet Pre-Training for Historical Document Image Analysis", "abstract": "<p>Automatic analysis of scanned historical documents comprises a wide range of\nimage analysis tasks, which are often challenging for machine learning due to a\nlack of human-annotated learning samples. With the advent of deep neural\nnetworks, a promising way to cope with the lack of training data is to\npre-train models on images from a different domain and then fine-tune them on\nhistorical documents. In the current research, a typical example of such\ncross-domain transfer learning is the use of neural networks that have been\npre-trained on the ImageNet database for object recognition. It remains a\nmostly open question whether or not this pre-training helps to analyse\nhistorical documents, which have fundamentally different image properties when\ncompared with ImageNet. In this paper, we present a comprehensive empirical\nsurvey on the effect of ImageNet pre-training for diverse historical document\nanalysis tasks, including character recognition, style classification,\nmanuscript dating, semantic segmentation, and content-based retrieval. While we\nobtain mixed results for semantic segmentation at pixel-level, we observe a\nclear trend across different network architectures that ImageNet pre-training\nhas a positive effect on classification as well as content-based retrieval.</p>\n", "tags": ["Survey-Paper"] },
{"key": "stylianou2019visualizing", "year": "2019", "citations": "46", "title":"Visualizing Deep Similarity Networks", "abstract": "<p>For convolutional neural network models that optimize an image embedding, we\npropose a method to highlight the regions of images that contribute most to\npairwise similarity. This work is a corollary to the visualization tools\ndeveloped for classification networks, but applicable to the problem domains\nbetter suited to similarity learning. The visualization shows how similarity\nnetworks that are fine-tuned learn to focus on different features. We also\ngeneralize our approach to embedding networks that use different pooling\nstrategies and provide a simple mechanism to support image similarity searches\non objects or sub-regions in the query image.</p>\n", "tags": ["Similarity-Search"] },
{"key": "su2019deep", "year": "2019", "citations": "255", "title":"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval", "abstract": "<p><img src=\"https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true\" alt=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" title=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" /></p>\n\n<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>\n", "tags": ["Scalability","ICCV","Multimodal-Retrieval","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "su2025deep", "year": "2019", "citations": "255", "title":"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval", "abstract": "<p><img src=\"https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true\" alt=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" title=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" /></p>\n\n<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>\n", "tags": ["Scalability","ICCV","Multimodal-Retrieval","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "sumbul2020deep", "year": "2021", "citations": "24", "title":"Deep Learning for Image Search and Retrieval in Large Remote Sensing Archives", "abstract": "<p>This chapter presents recent advances in content based image search and\nretrieval (CBIR) systems in remote sensing (RS) for fast and accurate\ninformation discovery from massive data archives. Initially, we analyze the\nlimitations of the traditional CBIR systems that rely on the hand-crafted RS\nimage descriptors. Then, we focus our attention on the advances in RS CBIR\nsystems for which deep learning (DL) models are at the forefront. In\nparticular, we present the theoretical properties of the most recent DL based\nCBIR systems for the characterization of the complex semantic content of RS\nimages. After discussing their strengths and limitations, we present the deep\nhashing based CBIR systems that have high time-efficient search capability\nwithin huge data archives. Finally, the most promising research directions in\nRS CBIR are discussed.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval"] },
{"key": "sun2019part", "year": "2018", "citations": "10", "title":"Part-based Multi-stream Model for Vehicle Searching", "abstract": "<p>Due to the enormous requirement in public security and intelligent\ntransportation system, searching an identical vehicle has become more and more\nimportant. Current studies usually treat vehicle as an integral object and then\ntrain a distance metric to measure the similarity among vehicles. However,\nthese raw images may be exactly similar to ones with different identification\nand include some pixels in background that may disturb the distance metric\nlearning. In this paper, we propose a novel and useful method to segment an\noriginal vehicle image into several discriminative foreground parts, and these\nparts consist of some fine grained regions that are named discriminative\npatches. After that, these parts combined with the raw image are fed into the\nproposed deep learning network. We can easily measure the similarity of two\nvehicle images by computing the Euclidean distance of the features from FC\nlayer. Two main contributions of this paper are as follows. Firstly, a method\nis proposed to estimate if a patch in a raw vehicle image is discriminative or\nnot. Secondly, a new Part-based Multi-Stream Model (PMSM) is designed and\noptimized for vehicle retrieval and re-identification tasks. We evaluate the\nproposed method on the VehicleID dataset, and the experimental results show\nthat our method can outperform the baseline.</p>\n", "tags": ["Distance-Metric-Learning","Datasets"] },
{"key": "sun2019supervised", "year": "2019", "citations": "48", "title":"Supervised Hierarchical Cross-Modal Hashing", "abstract": "<p>Recently, due to the unprecedented growth of multimedia data,\ncross-modal hashing has gained increasing attention for the\nefficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but\noverlook the correlations among labels. Indeed, in many real-world\nscenarios, like the online fashion domain, instances (items) are\nlabeled with a set of categories correlated by certain hierarchy. In\nthis paper, we propose a new end-to-end solution for supervised\ncross-modal hashing, named HiCHNet, which explicitly exploits the\nhierarchical labels of instances. In particular, by the pre-established\nlabel hierarchy, we comprehensively characterize each modality\nof the instance with a set of layer-wise hash representations. In\nessence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also\nretain the hierarchical discriminative capabilities. Due to the lack\nof benchmark datasets, apart from adapting the existing dataset\nFashionVC from fashion domain, we create a dataset from the\nonline fashion platform Ssense consisting of 15, 696 image-text\npairs labeled by 32 hierarchical categories. Extensive experiments\non two real-world datasets demonstrate the superiority of our model\nover the state-of-the-art methods.</p>\n", "tags": ["Datasets","SIGIR","Hashing-Methods","Evaluation","Supervised"] },
{"key": "sun2020benchmarking", "year": "2020", "citations": "79", "title":"A Benchmarking Study of Embedding-based Entity Alignment for Knowledge Graphs", "abstract": "<p>Entity alignment seeks to find entities in different knowledge graphs (KGs)\nthat refer to the same real-world object. Recent advancement in KG embedding\nimpels the advent of embedding-based entity alignment, which encodes entities\nin a continuous embedding space and measures entity similarities based on the\nlearned embeddings. In this paper, we conduct a comprehensive experimental\nstudy of this emerging field. We survey 23 recent embedding-based entity\nalignment approaches and categorize them based on their techniques and\ncharacteristics. We also propose a new KG sampling algorithm, with which we\ngenerate a set of dedicated benchmark datasets with various heterogeneity and\ndistributions for a realistic evaluation. We develop an open-source library\nincluding 12 representative embedding-based entity alignment approaches, and\nextensively evaluate these approaches, to understand their strengths and\nlimitations. Additionally, for several directions that have not been explored\nin current approaches, we perform exploratory experiments and report our\npreliminary findings for future studies. The benchmark datasets, open-source\nlibrary and experimental results are all accessible online and will be duly\nmaintained.</p>\n", "tags": ["Datasets","Survey-Paper","Evaluation","Tools-&-Libraries"] },
{"key": "sun2020multi", "year": "2019", "citations": "127", "title":"Multi-Graph Convolution Collaborative Filtering", "abstract": "<p>Personalized recommendation is ubiquitous, playing an important role in many\nonline services. Substantial research has been dedicated to learning vector\nrepresentations of users and items with the goal of predicting a user’s\npreference for an item based on the similarity of the representations.\nTechniques range from classic matrix factorization to more recent deep learning\nbased methods. However, we argue that existing methods do not make full use of\nthe information that is available from user-item interaction data and the\nsimilarities between user pairs and item pairs. In this work, we develop a\ngraph convolution-based recommendation framework, named Multi-Graph Convolution\nCollaborative Filtering (Multi-GCCF), which explicitly incorporates multiple\ngraphs in the embedding learning process. Multi-GCCF not only expressively\nmodels the high-order information via a partite user-item interaction graph,\nbut also integrates the proximal information by building and processing\nuser-user and item-item graphs. Furthermore, we consider the intrinsic\ndifference between user nodes and item nodes when performing graph convolution\non the bipartite graph. We conduct extensive experiments on four publicly\naccessible benchmarks, showing significant improvements relative to several\nstate-of-the-art collaborative filtering and graph neural network-based\nrecommendation models. Further experiments quantitatively verify the\neffectiveness of each component of our proposed model and demonstrate that the\nlearned embeddings capture the important relationship structure.</p>\n", "tags": ["Recommender-Systems","Tools-&-Libraries"] },
{"key": "sun2021real", "year": "2021", "citations": "15", "title":"Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model", "abstract": "<p>3D action recognition is referred to as the classification of action\nsequences which consist of 3D skeleton joints. While many research work are\ndevoted to 3D action recognition, it mainly suffers from three problems: highly\ncomplicated articulation, a great amount of noise, and a low implementation\nefficiency. To tackle all these problems, we propose a real-time 3D action\nrecognition framework by integrating the locally aggregated kinematic-guided\nskeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first\ndefine the skeletonlet as a few combinations of joint offsets grouped in terms\nof kinematic principle, and then represent an action sequence using LAKS, which\nconsists of a denoising phase and a locally aggregating phase. The denoising\nphase detects the noisy action data and adjust it by replacing all the features\nwithin it with the features of the corresponding previous frame, while the\nlocally aggregating phase sums the difference between an offset feature of the\nskeletonlet and its cluster center together over all the offset features of the\nsequence. Finally, the SHA model which combines sparse representation with a\nhashing model, aiming at promoting the recognition accuracy while maintaining a\nhigh efficiency. Experimental results on MSRAction3D, UTKinectAction3D and\nFlorence3DAction datasets demonstrate that the proposed method outperforms\nstate-of-the-art methods in both recognition accuracy and implementation\nefficiency.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Efficiency"] },
{"key": "sun2022deep", "year": "2022", "citations": "26", "title":"Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning", "abstract": "<p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>\n", "tags": ["Scalability","Datasets","CVPR","Hashing-Methods","Evaluation"] },
{"key": "sun2025deep", "year": "2022", "citations": "26", "title":"Deep Normalized Cross-Modal Hashing With Bi-Direction Relation Reasoning", "abstract": "<p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>\n", "tags": ["Scalability","Datasets","CVPR","Hashing-Methods","Evaluation"] },
{"key": "sun2025supervised", "year": "2019", "citations": "48", "title":"Supervised Hierarchical Cross-Modal Hashing", "abstract": "<p>Recently, due to the unprecedented growth of multimedia data,\ncross-modal hashing has gained increasing attention for the\nefficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but\noverlook the correlations among labels. Indeed, in many real-world\nscenarios, like the online fashion domain, instances (items) are\nlabeled with a set of categories correlated by certain hierarchy. In\nthis paper, we propose a new end-to-end solution for supervised\ncross-modal hashing, named HiCHNet, which explicitly exploits the\nhierarchical labels of instances. In particular, by the pre-established\nlabel hierarchy, we comprehensively characterize each modality\nof the instance with a set of layer-wise hash representations. In\nessence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also\nretain the hierarchical discriminative capabilities. Due to the lack\nof benchmark datasets, apart from adapting the existing dataset\nFashionVC from fashion domain, we create a dataset from the\nonline fashion platform Ssense consisting of 15, 696 image-text\npairs labeled by 32 hierarchical categories. Extensive experiments\non two real-world datasets demonstrate the superiority of our model\nover the state-of-the-art methods.</p>\n", "tags": ["Datasets","SIGIR","Hashing-Methods","Evaluation","Supervised"] },
{"key": "sundaram2013streaming", "year": "2013", "citations": "109", "title":"Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing", "abstract": "<p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,\nand many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an\napproximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel\nLSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several\nnovel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient\nalgorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration\nalgorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to\noptimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with\nhundreds of millions of new tweets per day, we can achieve query times of 1–2.5 ms. We show that this is an order of magnitude faster\nthan existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,\nwith table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>\n", "tags": ["Efficiency","Datasets","Text-Retrieval","Locality-Sensitive-Hashing","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "sundaram2025streaming", "year": "2013", "citations": "109", "title":"Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing", "abstract": "<p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,\nand many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an\napproximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel\nLSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several\nnovel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient\nalgorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration\nalgorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to\noptimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with\nhundreds of millions of new tweets per day, we can achieve query times of 1–2.5 ms. We show that this is an order of magnitude faster\nthan existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,\nwith table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>\n", "tags": ["Efficiency","Datasets","Text-Retrieval","Locality-Sensitive-Hashing","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "svenstrup2017hash", "year": "2017", "citations": "29", "title":"Hash Embeddings for Efficient Word Representations", "abstract": "<p>We present hash embeddings, an efficient method for representing words in a\ncontinuous vector form. A hash embedding may be seen as an interpolation\nbetween a standard word embedding and a word embedding created using a random\nhash function (the hashing trick). In hash embeddings each token is represented\nby \\(k\\) \\(d\\)-dimensional embeddings vectors and one \\(k\\) dimensional weight\nvector. The final \\(d\\) dimensional representation of the token is the product of\nthe two. Rather than fitting the embedding vectors for each token these are\nselected by the hashing trick from a shared pool of \\(B\\) embedding vectors. Our\nexperiments show that hash embeddings can easily deal with huge vocabularies\nconsisting of millions of tokens. When using a hash embedding there is no need\nto create a dictionary before training nor to perform any kind of vocabulary\npruning after training. We show that models trained using hash embeddings\nexhibit at least the same level of performance as models trained using regular\nembeddings across a wide range of tasks. Furthermore, the number of parameters\nneeded by such an embedding is only a fraction of what is required by a regular\nembedding. Since standard embeddings and embeddings constructed using the\nhashing trick are actually just special cases of a hash embedding, hash\nembeddings can be considered an extension and improvement over the existing\nregular embedding types.</p>\n", "tags": ["Hashing-Methods","Evaluation"] },
{"key": "szeto2016binary", "year": "2016", "citations": "11", "title":"Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders", "abstract": "<p>A Content-Based Image Retrieval (CBIR) system which identifies similar\nmedical images based on a query image can assist clinicians for more accurate\ndiagnosis. The recent CBIR research trend favors the construction and use of\nbinary codes to represent images. Deep architectures could learn the non-linear\nrelationship among image pixels adaptively, allowing the automatic learning of\nhigh-level features from raw pixels. However, most of them require class\nlabels, which are expensive to obtain, particularly for medical images. The\nmethods which do not need class labels utilize a deep autoencoder for binary\nhashing, but the code construction involves a specific training algorithm and\nan ad-hoc regularization technique. In this study, we explored using a deep\nde-noising autoencoder (DDA), with a new unsupervised training scheme using\nonly backpropagation and dropout, to hash images into binary codes. We\nconducted experiments on more than 14,000 x-ray images. By using class labels\nonly for evaluating the retrieval results, we constructed a 16-bit DDA and a\n512-bit DDA independently. Comparing to other unsupervised methods, we\nsucceeded to obtain the lowest total error by using the 512-bit codes for\nretrieval via exhaustive search, and speed up 9.27 times with the use of the\n16-bit codes while keeping a comparable total error. We found that our new\ntraining scheme could reduce the total retrieval error significantly by 21.9%.\nTo further boost the image retrieval performance, we developed Radon\nAutoencoder Barcode (RABC) which are learned from the Radon projections of\nimages using a de-noising autoencoder. Experimental results demonstrated its\nsuperior performance in retrieval when it was combined with DDA binary codes.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Compact-Codes","Unsupervised","Evaluation"] },
{"key": "taherkhani2020error", "year": "2020", "citations": "14", "title":"Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image Retrieval", "abstract": "<p>Cross-modal hashing facilitates mapping of heterogeneous multimedia data into\na common Hamming space, which can beutilized for fast and flexible retrieval\nacross different modalities. In this paper, we propose a novel cross-modal\nhashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which\nuses a binary vector specifying the presence of certainfacial attributes as an\ninput query to retrieve relevant face images from a database. The DNDCMH\nnetwork consists of two separatecomponents: an attribute-based deep cross-modal\nhashing (ADCMH) module, which uses a margin (m)-based loss function\ntoefficiently learn compact binary codes to preserve similarity between\nmodalities in the Hamming space, and a neural error correctingdecoder (NECD),\nwhich is an error correcting decoder implemented with a neural network. The\ngoal of NECD network in DNDCMH isto error correct the hash codes generated by\nADCMH to improve the retrieval efficiency. The NECD network is trained such\nthat it hasan error correcting capability greater than or equal to the margin\n(m) of the margin-based loss function. This results in NECD cancorrect the\ncorrupted hash codes generated by ADCMH up to the Hamming distance of m. We\nhave evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing\nmethods on standard datasets to demonstrate the superiority of our method.</p>\n", "tags": ["Hashing-Methods","Datasets","Compact-Codes","Efficiency"] },
{"key": "talreja2019learning", "year": "2019", "citations": "19", "title":"Learning to Authenticate with Deep Multibiometric Hashing and Neural Network Decoding", "abstract": "<p>In this paper, we propose a novel multimodal deep hashing neural decoder\n(MDHND) architecture, which integrates a deep hashing framework with a neural\nnetwork decoder (NND) to create an effective multibiometric authentication\nsystem. The MDHND consists of two separate modules: a multimodal deep hashing\n(MDH) module, which is used for feature-level fusion and binarization of\nmultiple biometrics, and a neural network decoder (NND) module, which is used\nto refine the intermediate binary codes generated by the MDH and compensate for\nthe difference between enrollment and probe biometrics (variations in pose,\nillumination, etc.). Use of NND helps to improve the performance of the overall\nmultimodal authentication system. The MDHND framework is trained in 3 steps\nusing joint optimization of the two modules. In Step 1, the MDH parameters are\ntrained and learned to generate a shared multimodal latent code; in Step 2, the\nlatent codes from Step 1 are passed through a conventional error-correcting\ncode (ECC) decoder to generate the ground truth to train a neural network\ndecoder (NND); in Step 3, the NND decoder is trained using the ground truth\nfrom Step 2 and the MDH and NND are jointly optimized. Experimental results on\na standard multimodal dataset demonstrate the superiority of our method\nrelative to other current multimodal authentication systems</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Evaluation"] },
{"key": "talreja2019using", "year": "2018", "citations": "25", "title":"Using Deep Cross Modal Hashing and Error Correcting Codes for Improving the Efficiency of Attribute Guided Facial Image Retrieval", "abstract": "<p>With benefits of fast query speed and low storage cost, hashing-based image\nretrieval approaches have garnered considerable attention from the research\ncommunity. In this paper, we propose a novel Error-Corrected Deep Cross Modal\nHashing (CMH-ECC) method which uses a bitmap specifying the presence of certain\nfacial attributes as an input query to retrieve relevant face images from the\ndatabase. In this architecture, we generate compact hash codes using an\nend-to-end deep learning module, which effectively captures the inherent\nrelationships between the face and attribute modality. We also integrate our\ndeep learning module with forward error correction codes to further reduce the\ndistance between different modalities of the same subject. Specifically, the\nproperties of deep hashing and forward error correction codes are exploited to\ndesign a cross modal hashing framework with high retrieval performance.\nExperimental results using two standard datasets with facial attributes-image\nmodalities indicate that our CMH-ECC face image retrieval model outperforms\nmost of the current attribute-based face image retrieval approaches.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Memory-Efficiency","Evaluation","Efficiency"] },
{"key": "talreja2019zero", "year": "2019", "citations": "20", "title":"Zero-Shot Deep Hashing and Neural Network Based Error Correction for Face Template Protection", "abstract": "<p>In this paper, we present a novel architecture that integrates a deep hashing\nframework with a neural network decoder (NND) for application to face template\nprotection. It improves upon existing face template protection techniques to\nprovide better matching performance with one-shot and multi-shot enrollment. A\nkey novelty of our proposed architecture is that the framework can also be used\nwith zero-shot enrollment. This implies that our architecture does not need to\nbe re-trained even if a new subject is to be enrolled into the system. The\nproposed architecture consists of two major components: a deep hashing (DH)\ncomponent, which is used for robust mapping of face images to their\ncorresponding intermediate binary codes, and a NND component, which corrects\nerrors in the intermediate binary codes that are caused by differences in the\nenrollment and probe biometrics due to factors such as variation in pose,\nillumination, and other factors. The final binary code generated by the NND is\nthen cryptographically hashed and stored as a secure face template in the\ndatabase. The efficacy of our approach with zero-shot, one-shot, and multi-shot\nenrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE\nface databases. With zero-shot enrollment, the system achieves approximately\n85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with\none-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at\n0.01% FAR, while providing a high level of template security.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","Compact-Codes","Evaluation"] },
{"key": "talreja2020deep", "year": "2020", "citations": "24", "title":"Deep Hashing for Secure Multimodal Biometrics", "abstract": "<p>When compared to unimodal systems, multimodal biometric systems have several\nadvantages, including lower error rate, higher accuracy, and larger population\ncoverage. However, multimodal systems have an increased demand for integrity\nand privacy because they must store multiple biometric traits associated with\neach user. In this paper, we present a deep learning framework for\nfeature-level fusion that generates a secure multimodal template from each\nuser’s face and iris biometrics. We integrate a deep hashing (binarization)\ntechnique into the fusion architecture to generate a robust binary multimodal\nshared latent representation. Further, we employ a hybrid secure architecture\nby combining cancelable biometrics with secure sketch techniques and integrate\nit with a deep hashing framework, which makes it computationally prohibitive to\nforge a combination of multiple biometrics that pass the authentication. The\nefficacy of the proposed approach is shown using a multimodal database of face\nand iris and it is observed that the matching performance is improved due to\nthe fusion of multiple biometrics. Furthermore, the proposed approach also\nprovides cancelability and unlinkability of the templates along with improved\nprivacy of the biometric data. Additionally, we also test the proposed hashing\nfunction for an image retrieval application using a benchmark dataset. The main\ngoal of this paper is to develop a method for integrating multimodal fusion,\ndeep hashing, and biometric security, with an emphasis on structural data from\nmodalities like face and iris. The proposed approach is in no way a general\nbiometric security framework that can be applied to all biometric modalities,\nas further research is needed to extend the proposed framework to other\nunconstrained biometric modalities.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Evaluation"] },
{"key": "tan2020learning", "year": "2020", "citations": "77", "title":"Learning to Hash with Graph Neural Networks for Recommender Systems", "abstract": "<p>Graph representation learning has attracted much attention in supporting high\nquality candidate search at scale. Despite its effectiveness in learning\nembedding vectors for objects in the user-item interaction network, the\ncomputational costs to infer users’ preferences in continuous embedding space\nare tremendous. In this work, we investigate the problem of hashing with graph\nneural networks (GNNs) for high quality retrieval, and propose a simple yet\neffective discrete representation learning framework to jointly learn\ncontinuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN)\nis presented, which consists of two components, a GNN encoder for learning node\nrepresentations, and a hash layer for encoding representations to hash codes.\nThe whole architecture is trained end-to-end by jointly optimizing two losses,\ni.e., reconstruction loss from reconstructing observed links, and ranking loss\nfrom preserving the relative ordering of hash codes. A novel discrete\noptimization strategy based on straight through estimator (STE) with guidance\nis proposed. The principal idea is to avoid gradient magnification in\nback-propagation of STE with continuous embedding guidance, in which we begin\nfrom learning an easier network that mimic the continuous embedding and let it\nevolve during the training until it finally goes back to STE. Comprehensive\nexperiments over three publicly available and one real-world Alibaba company\ndatasets demonstrate that our model not only can achieve comparable performance\ncompared with its continuous counterpart but also runs multiple times faster\nduring inference.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Recommender-Systems","Neural-Hashing","Evaluation"] },
{"key": "tan2021fast", "year": "2022", "citations": "12", "title":"A Fast Partial Video Copy Detection Using KNN and Global Feature Database", "abstract": "<p>We propose a fast partial video copy detection framework in this paper. In\nthis framework all frame features of the reference videos are organized in a\nKNN searchable database. Instead of scanning all reference videos, the query\nvideo segment does a fast KNN search in the global feature database. The\nreturned results are used to generate a short list of candidate videos. A\nmodified temporal network is then used to localize the copy segment in the\ncandidate videos. We evaluate different choice of CNN features on the VCDB\ndataset. Our benchmark F1 score exceeds the state of the art by a big margin.</p>\n", "tags": ["Similarity-Search","Datasets","Evaluation","Tools-&-Libraries"] },
{"key": "tan2021instance", "year": "2021", "citations": "71", "title":"Instance-level Image Retrieval using Reranking Transformers", "abstract": "<p>Instance-level image retrieval is the task of searching in a large database\nfor images that match an object in a query image. To address this task, systems\nusually rely on a retrieval step that uses global image descriptors, and a\nsubsequent step that performs domain-specific refinements or reranking by\nleveraging operations such as geometric verification based on local features.\nIn this work, we propose Reranking Transformers (RRTs) as a general model to\nincorporate both local and global features to rerank the matching images in a\nsupervised fashion and thus replace the relatively expensive process of\ngeometric verification. RRTs are lightweight and can be easily parallelized so\nthat reranking a set of top matching results can be performed in a single\nforward-pass. We perform extensive experiments on the Revisited Oxford and\nParis datasets, and the Google Landmarks v2 dataset, showing that RRTs\noutperform previous reranking approaches while using much fewer local\ndescriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs\ncan be optimized jointly with the feature extractor, which can lead to feature\nrepresentations tailored to downstream tasks and further accuracy improvements.\nThe code and trained models are publicly available at\nhttps://github.com/uvavision/RerankingTransformer.</p>\n", "tags": ["ICCV","Supervised","Image-Retrieval","Datasets","Re-Ranking"] },
{"key": "tan2022multilingual", "year": "2023", "citations": "7", "title":"Multilingual Representation Distillation with Contrastive Learning", "abstract": "<p>Multilingual sentence representations from large models encode semantic\ninformation from two or more languages and can be used for different\ncross-lingual information retrieval and matching tasks. In this paper, we\nintegrate contrastive learning into multilingual representation distillation\nand use it for quality estimation of parallel sentences (i.e., find\nsemantically similar sentences that can be used as translations of each other).\nWe validate our approach with multilingual similarity search and corpus\nfiltering tasks. Experiments across different low-resource languages show that\nour method greatly outperforms previous sentence encoders such as LASER,\nLASER3, and LaBSE.</p>\n", "tags": ["Self-Supervised","Similarity-Search"] },
{"key": "tang2021improving", "year": "2021", "citations": "23", "title":"Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval", "abstract": "<p>Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.</p>\n", "tags": ["Text-Retrieval","Tools-&-Libraries","Datasets","Evaluation","Efficiency"] },
{"key": "tanioka2019fast", "year": "2019", "citations": "10", "title":"A Fast Content-Based Image Retrieval Method Using Deep Visual Features", "abstract": "<p>Fast and scalable Content-Based Image Retrieval using visual features is\nrequired for document analysis, Medical image analysis, etc. in the present\nage. Convolutional Neural Network (CNN) activations as features achieved their\noutstanding performance in this area. Deep Convolutional representations using\nthe softmax function in the output layer are also ones among visual features.\nHowever, almost all the image retrieval systems hold their index of visual\nfeatures on main memory in order to high responsiveness, limiting their\napplicability for big data applications. In this paper, we propose a fast\ncalculation method of cosine similarity with L2 norm indexed in advance on\nElasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16\npre-trained model. The evaluation results show the effectiveness and efficiency\nof our proposed method.</p>\n", "tags": ["Image-Retrieval","Distance-Metric-Learning","Datasets","Evaluation","Efficiency"] },
{"key": "tchayekondi2020new", "year": "2021", "citations": "5", "title":"A new hashing based nearest neighbors selection technique for big datasets", "abstract": "<p>KNN has the reputation to be the word simplest but efficient supervised\nlearning algorithm used for either classification or regression. KNN prediction\nefficiency highly depends on the size of its training data but when this\ntraining data grows KNN suffers from slowness in making decisions since it\nneeds to search nearest neighbors within the entire dataset at each decision\nmaking. This paper proposes a new technique that enables the selection of\nnearest neighbors directly in the neighborhood of a given observation. The\nproposed approach consists of dividing the data space into subcells of a\nvirtual grid built on top of data space. The mapping between the data points\nand subcells is performed using hashing. When it comes to select the nearest\nneighbors of a given observation, we firstly identify the cell the observation\nbelongs by using hashing, and then we look for nearest neighbors from that\ncentral cell and cells around it layer by layer. From our experiment\nperformance analysis on publicly available datasets, our algorithm outperforms\nthe original KNN in time efficiency with a prediction quality as good as that\nof KNN it also offers competitive performance with solutions like KDtree</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Evaluation","Efficiency"] },
{"key": "thakur2022injecting", "year": "2022", "citations": "6", "title":"Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval", "abstract": "<p>Dense retrieval overcome the lexical gap and has shown great success in\nad-hoc information retrieval (IR). Despite their success, dense retrievers are\nexpensive to serve across practical use cases. For use cases requiring to\nsearch from millions of documents, the dense index becomes bulky and requires\nhigh memory usage for storing the index. More recently, learning-to-hash (LTH)\ntechniques, for e.g., BPR and JPQ, produce binary document vectors, thereby\nreducing the memory requirement to efficiently store the dense index. LTH\ntechniques are supervised and finetune the retriever using a ranking loss. They\noutperform their counterparts, i.e., traditional out-of-the-box vector\ncompression techniques such as PCA or PQ. A missing piece from prior work is\nthat existing techniques have been evaluated only in-domain, i.e., on a single\ndataset such as MS MARCO. In our work, we evaluate LTH and vector compression\ntechniques for improving the downstream zero-shot retrieval accuracy of the\nTAS-B dense retriever while maintaining efficiency at inference. Our results\ndemonstrate that, unlike prior work, LTH strategies when applied naively can\nunderperform the zero-shot TAS-B dense retriever on average by up to 14%\nnDCG@10 on the BEIR benchmark. To solve this limitation, in our work, we\npropose an easy yet effective solution of injecting domain adaptation with\nexisting supervised LTH techniques. We experiment with two well-known\nunsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptation\ninjection technique can improve the downstream zero-shot retrieval\neffectiveness for both BPR and JPQ variants of the TAS-B model by on average\n11.5% and 8.2% nDCG@10 while both maintaining 32\\(\\times\\) memory efficiency and\n14\\(\\times\\) and 2\\(\\times\\) speedup respectively in CPU retrieval latency on BEIR.\nAll our code, models, and data are publicly available at\nhttps://github.com/thakur-nandan/income.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Supervised","Datasets","Memory-Efficiency","Unsupervised","Quantization","Evaluation","Efficiency"] },
{"key": "tharani2018unsupervised", "year": "2018", "citations": "5", "title":"Unsupervised Deep Features for Remote Sensing Image Matching via Discriminator Network", "abstract": "<p>The advent of deep perceptual networks brought about a paradigm shift in\nmachine vision and image perception. Image apprehension lately carried out by\nhand-crafted features in the latent space have been replaced by deep features\nacquired from supervised networks for improved understanding. However, such\ndeep networks require strict supervision with a substantial amount of the\nlabeled data for authentic training process. These methods perform poorly in\ndomains lacking labeled data especially in case of remote sensing image\nretrieval. Resolving this, we propose an unsupervised encoder-decoder feature\nfor remote sensing image matching (RSIM). Moreover, we replace the conventional\ndistance metrics with a deep discriminator network to identify the similarity\nof the image pairs. To the best of our knowledge, discriminator network has\nnever been used before for solving RSIM problem. Results have been validated\nwith two publicly available benchmark remote sensing image datasets. The\ntechnique has also been investigated for content-based remote sensing image\nretrieval (CBRSIR); one of the widely used applications of RSIM. Results\ndemonstrate that our technique supersedes the state-of-the-art methods used for\nunsupervised image matching with mean average precision (mAP) of 81%, and image\nretrieval with an overall improvement in mAP score of about 12%.</p>\n", "tags": ["Supervised","Distance-Metric-Learning","Datasets","Unsupervised","Evaluation"] },
{"key": "thomas2020preserving", "year": "2020", "citations": "27", "title":"Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval", "abstract": "<p>The abundance of multimodal data (e.g. social media posts) has inspired\ninterest in cross-modal retrieval methods. Popular approaches rely on a variety\nof metric learning losses, which prescribe what the proximity of image and text\nshould be, in the learned space. However, most prior methods have focused on\nthe case where image and text convey redundant information; in contrast,\nreal-world image-text pairs convey complementary information with little\noverlap. Further, images in news articles and media portray topics in a\nvisually diverse fashion; thus, we need to take special care to ensure a\nmeaningful image representation. We propose novel within-modality losses which\nencourage semantic coherency in both the text and image subspaces, which does\nnot necessarily align with visual coherency. Our method ensures that not only\nare paired images and texts close, but the expected image-image and text-text\nrelationships are also observed. Our approach improves the results of\ncross-modal retrieval on four datasets compared to five baselines.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Multimodal-Retrieval"] },
{"key": "tian2019global", "year": "2016", "citations": "14", "title":"Global Hashing System for Fast Image Search", "abstract": "<p>Hashing methods have been widely investigated for fast approximate nearest\nneighbor searching in large data sets. Most existing methods use binary vectors\nin lower dimensional spaces to represent data points that are usually real\nvectors of higher dimensionality. We divide the hashing process into two steps.\nData points are first embedded in a low-dimensional space, and the global\npositioning system method is subsequently introduced but modified for binary\nembedding. We devise dataindependent and data-dependent methods to distribute\nthe satellites at appropriate locations. Our methods are based on finding the\ntradeoff between the information losses in these two steps. Experiments show\nthat our data-dependent method outperforms other methods in different-sized\ndata sets from 100k to 10M. By incorporating the orthogonality of the code\nmatrix, both our data-independent and data-dependent methods are particularly\nimpressive in experiments on longer bits.</p>\n", "tags": ["Hashing-Methods","Image-Retrieval"] },
{"key": "tian2022learned", "year": "2022", "citations": "12", "title":"A Learned Index for Exact Similarity Search in Metric Spaces", "abstract": "<p>Indexing is an effective way to support efficient query processing in large\ndatabases. Recently the concept of learned index, which replaces or complements\ntraditional index structures with machine learning models, has been actively\nexplored to reduce storage and search costs. However, accurate and efficient\nsimilarity query processing in high-dimensional metric spaces remains to be an\nopen challenge. In this paper, we propose a novel indexing approach called LIMS\nthat uses data clustering, pivot-based data transformation techniques and\nlearned indexes to support efficient similarity query processing in metric\nspaces. In LIMS, the underlying data is partitioned into clusters such that\neach cluster follows a relatively uniform data distribution. Data\nredistribution is achieved by utilizing a small number of pivots for each\ncluster. Similar data are mapped into compact regions and the mapped values are\ntotally ordinal. Machine learning models are developed to approximate the\nposition of each data record on disk. Efficient algorithms are designed for\nprocessing range queries and nearest neighbor queries based on LIMS, and for\nindex maintenance with dynamic updates. Extensive experiments on real-world and\nsynthetic datasets demonstrate the superiority of LIMS compared with\ntraditional indexes and state-of-the-art learned indexes.</p>\n", "tags": ["Similarity-Search","Datasets","Vector-Indexing"] },
{"key": "tissier2018near", "year": "2019", "citations": "43", "title":"Near-lossless Binarization of Word Embeddings", "abstract": "<p>Word embeddings are commonly used as a starting point in many NLP models to\nachieve state-of-the-art performances. However, with a large vocabulary and\nmany dimensions, these floating-point representations are expensive both in\nterms of memory and calculations which makes them unsuitable for use on\nlow-resource devices. The method proposed in this paper transforms real-valued\nembeddings into binary embeddings while preserving semantic information,\nrequiring only 128 or 256 bits for each vector. This leads to a small memory\nfootprint and fast vector operations. The model is based on an autoencoder\narchitecture, which also allows to reconstruct original vectors from the binary\nones. Experimental results on semantic similarity, text classification and\nsentiment analysis tasks show that the binarization of word embeddings only\nleads to a loss of ~2% in accuracy while vector size is reduced by 97%.\nFurthermore, a top-k benchmark demonstrates that using these binary vectors is\n30 times faster than using real-valued vectors.</p>\n", "tags": ["AAAI","Hashing-Methods","Evaluation"] },
{"key": "tizhoosh2016barcodes", "year": "2016", "citations": "16", "title":"Barcodes for Medical Image Retrieval Using Autoencoded Radon Transform", "abstract": "<p>Using content-based binary codes to tag digital images has emerged as a\npromising retrieval technology. Recently, Radon barcodes (RBCs) have been\nintroduced as a new binary descriptor for image search. RBCs are generated by\nbinarization of Radon projections and by assembling them into a vector, namely\nthe barcode. A simple local thresholding has been suggested for binarization.\nIn this paper, we put forward the idea of “autoencoded Radon barcodes”. Using\nimages in a training dataset, we autoencode Radon projections to perform\nbinarization on outputs of hidden layers. We employed the mini-batch stochastic\ngradient descent approach for the training. Each hidden layer of the\nautoencoder can produce a barcode using a threshold determined based on the\nrange of the logistic function used. The compressing capability of autoencoders\napparently reduces the redundancies inherent in Radon projections leading to\nmore accurate retrieval results. The IRMA dataset with 14,410 x-ray images is\nused to validate the performance of the proposed method. The experimental\nresults, containing comparison with RBCs, SURF and BRISK, show that autoencoded\nRadon barcode (ARBC) has the capacity to capture important information and to\nlearn richer representations resulting in lower retrieval errors for image\nretrieval measured with the accuracy of the first hit only.</p>\n", "tags": ["Datasets","Compact-Codes","Evaluation","Image-Retrieval"] },
{"key": "tizhoosh2016minmax", "year": "2016", "citations": "38", "title":"MinMax Radon Barcodes for Medical Image Retrieval", "abstract": "<p>Content-based medical image retrieval can support diagnostic decisions by\nclinical experts. Examining similar images may provide clues to the expert to\nremove uncertainties in his/her final diagnosis. Beyond conventional feature\ndescriptors, binary features in different ways have been recently proposed to\nencode the image content. A recent proposal is “Radon barcodes” that employ\nbinarized Radon projections to tag/annotate medical images with content-based\nbinary vectors, called barcodes. In this paper, MinMax Radon barcodes are\nintroduced which are superior to “local thresholding” scheme suggested in the\nliterature. Using IRMA dataset with 14,410 x-ray images from 193 different\nclasses, the advantage of using MinMax Radon barcodes over <em>thresholded</em>\nRadon barcodes are demonstrated. The retrieval error for direct search drops by\nmore than 15%. As well, SURF, as a well-established non-binary approach, and\nBRISK, as a recent binary method are examined to compare their results with\nMinMax Radon barcodes when retrieving images from IRMA dataset. The results\ndemonstrate that MinMax Radon barcodes are faster and more accurate when\napplied on IRMA images.</p>\n", "tags": ["Datasets","Image-Retrieval"] },
{"key": "tolias2017asymmetric", "year": "2017", "citations": "39", "title":"Asymmetric Feature Maps with Application to Sketch Based Retrieval", "abstract": "<p>We propose a novel concept of asymmetric feature maps (AFM), which allows to\nevaluate multiple kernels between a query and database entries without\nincreasing the memory requirements. To demonstrate the advantages of the AFM\nmethod, we derive a short vector image representation that, due to asymmetric\nfeature maps, supports efficient scale and translation invariant sketch-based\nimage retrieval. Unlike most of the short-code based retrieval systems, the\nproposed method provides the query localization in the retrieved image. The\nefficiency of the search is boosted by approximating a 2D translation search\nvia trigonometric polynomial of scores by 1D projections. The projections are a\nspecial case of AFM. An order of magnitude speed-up is achieved compared to\ntraditional trigonometric polynomials. The results are boosted by an\nimage-based average query expansion, exceeding significantly the state of the\nart on standard benchmarks.</p>\n", "tags": ["CVPR","Image-Retrieval","Efficiency"] },
{"key": "tonellotto2021query", "year": "2021", "citations": "16", "title":"Query Embedding Pruning for Dense Retrieval", "abstract": "<p>Recent advances in dense retrieval techniques have offered the promise of\nbeing able not just to re-rank documents using contextualised language models\nsuch as BERT, but also to use such models to identify documents from the\ncollection in the first place. However, when using dense retrieval approaches\nthat use multiple embedded representations for each query, a large number of\ndocuments can be retrieved for each query, hindering the efficiency of the\nmethod. Hence, this work is the first to consider efficiency improvements in\nthe context of a dense retrieval approach (namely ColBERT), by pruning query\nterm embeddings that are estimated not to be useful for retrieving relevant\ndocuments. Our proposed query embeddings pruning reduces the cost of the dense\nretrieval operation, as well as reducing the number of documents that are\nretrieved and hence require to be fully scored. Experiments conducted on the\nMSMARCO passage ranking corpus demonstrate that, when reducing the number of\nquery embeddings used from 32 to 3 based on the collection frequency of the\ncorresponding tokens, query embedding pruning results in no statistically\nsignificant differences in effectiveness, while reducing the number of\ndocuments retrieved by 70%. In terms of mean response time for the end-to-end\nto end system, this results in a 2.65x speedup.</p>\n", "tags": ["Efficiency","CIKM"] },
{"key": "tonioni2018deep", "year": "2018", "citations": "43", "title":"A deep learning pipeline for product recognition on store shelves", "abstract": "<p>Recognition of grocery products in store shelves poses peculiar challenges.\nFirstly, the task mandates the recognition of an extremely high number of\ndifferent items, in the order of several thousands for medium-small shops, with\nmany of them featuring small inter and intra class variability. Then, available\nproduct databases usually include just one or a few studio-quality images per\nproduct (referred to herein as reference images), whilst at test time\nrecognition is performed on pictures displaying a portion of a shelf containing\nseveral products and taken in the store by cheap cameras (referred to as query\nimages). Moreover, as the items on sale in a store as well as their appearance\nchange frequently over time, a practical recognition system should handle\nseamlessly new products/packages. Inspired by recent advances in object\ndetection and image retrieval, we propose to leverage on state of the art\nobject detectors based on deep learning to obtain an initial productagnostic\nitem detection. Then, we pursue product recognition through a similarity search\nbetween global descriptors computed on reference and cropped query images. To\nmaximize performance, we learn an ad-hoc global descriptor by a CNN trained on\nreference images based on an image embedding loss. Our system is\ncomputationally expensive at training time but can perform recognition rapidly\nand accurately at test time.</p>\n", "tags": ["Similarity-Search","Evaluation","Image-Retrieval"] },
{"key": "torres2021compact", "year": "2021", "citations": "12", "title":"Compact and Effective Representations for Sketch-based Image Retrieval", "abstract": "<p>Sketch-based image retrieval (SBIR) has undergone an increasing interest in\nthe community of computer vision bringing high impact in real applications. For\ninstance, SBIR brings an increased benefit to eCommerce search engines because\nit allows users to formulate a query just by drawing what they need to buy.\nHowever, current methods showing high precision in retrieval work in a high\ndimensional space, which negatively affects aspects like memory consumption and\ntime processing. Although some authors have also proposed compact\nrepresentations, these drastically degrade the performance in a low dimension.\nTherefore in this work, we present different results of evaluating methods for\nproducing compact embeddings in the context of sketch-based image retrieval.\nOur main interest is in strategies aiming to keep the local structure of the\noriginal space. The recent unsupervised local-topology preserving dimension\nreduction method UMAP fits our requirements and shows outstanding performance,\nimproving even the precision achieved by SOTA methods. We evaluate six methods\nin two different datasets. We use Flickr15K and eCommerce datasets; the latter\nis another contribution of this work. We show that UMAP allows us to have\nfeature vectors of 16 bytes improving precision by more than 35%.</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Unsupervised","Evaluation"] },
{"key": "tran2018device", "year": "2018", "citations": "38", "title":"On-device Scalable Image-based Localization via Prioritized Cascade Search and Fast One-Many RANSAC", "abstract": "<p>We present the design of an entire on-device system for large-scale urban\nlocalization using images. The proposed design integrates compact image\nretrieval and 2D-3D correspondence search to estimate the location in extensive\ncity regions. Our design is GPS agnostic and does not require network\nconnection. In order to overcome the resource constraints of mobile devices, we\npropose a system design that leverages the scalability advantage of image\nretrieval and accuracy of 3D model-based localization. Furthermore, we propose\na new hashing-based cascade search for fast computation of 2D-3D\ncorrespondences. In addition, we propose a new one-many RANSAC for accurate\npose estimation. The new one-many RANSAC addresses the challenge of repetitive\nbuilding structures (e.g. windows, balconies) in urban localization. Extensive\nexperiments demonstrate that our 2D-3D correspondence search achieves\nstate-of-the-art localization accuracy on multiple benchmark datasets.\nFurthermore, our experiments on a large Google Street View (GSV) image dataset\nshow the potential of large-scale localization entirely on a typical mobile\ndevice.</p>\n", "tags": ["Hashing-Methods","Datasets","Scalability","Evaluation"] },
{"key": "tsai2017learning", "year": "2017", "citations": "148", "title":"Learning Robust Visual-Semantic Embeddings", "abstract": "<p>Many of the existing methods for learning joint embedding of images and text\nuse only supervised information from paired images and its textual attributes.\nTaking advantage of the recent success of unsupervised learning in deep neural\nnetworks, we propose an end-to-end learning framework that is able to extract\nmore robust multi-modal representations across domains. The proposed method\ncombines representation learning models (i.e., auto-encoders) together with\ncross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn\njoint embeddings for semantic and visual features. A novel technique of\nunsupervised-data adaptation inference is introduced to construct more\ncomprehensive embeddings for both labeled and unlabeled data. We evaluate our\nmethod on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with\na wide range of applications, including zero and few-shot image recognition and\nretrieval, from inductive to transductive settings. Empirically, we show that\nour framework improves over the current state of the art on many of the\nconsidered tasks.</p>\n", "tags": ["ICCV","Few-Shot-&-Zero-Shot","Supervised","Tools-&-Libraries","Datasets","Unsupervised"] },
{"key": "tu2018object", "year": "2019", "citations": "10", "title":"Object Detection based Deep Unsupervised Hashing", "abstract": "<p>Recently, similarity-preserving hashing methods have been extensively studied\nfor large-scale image retrieval. Compared with unsupervised hashing, supervised\nhashing methods for labeled data have usually better performance by utilizing\nsemantic label information. Intuitively, for unlabeled data, it will improve\nthe performance of unsupervised hashing methods if we can first mine some\nsupervised semantic ‘label information’ from unlabeled data and then\nincorporate the ‘label information’ into the training process. Thus, in this\npaper, we propose a novel Object Detection based Deep Unsupervised Hashing\nmethod (ODDUH). Specifically, a pre-trained object detection model is utilized\nto mining supervised ‘label information’, which is used to guide the learning\nprocess to generate high-quality hash codes.Extensive experiments on two public\ndatasets demonstrate that the proposed method outperforms the state-of-the-art\nunsupervised hashing methods in the image retrieval task.</p>\n", "tags": ["Scalability","Evaluation","Datasets","Unsupervised","AAAI","Hashing-Methods","Neural-Hashing","IJCAI","Image-Retrieval","Supervised"] },
{"key": "tu2019deep", "year": "2020", "citations": "62", "title":"Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning", "abstract": "<p>Due to their high retrieval efficiency and low storage cost, cross-modal\nhashing methods have attracted considerable attention. Generally, compared with\nshallow cross-modal hashing methods, deep cross-modal hashing methods can\nachieve a more satisfactory performance by integrating feature learning and\nhash codes optimizing into a same framework. However, most existing deep\ncross-modal hashing methods either cannot learn a unified hash code for the two\ncorrelated data-points of different modalities in a database instance or cannot\nguide the learning of unified hash codes by the feedback of hashing function\nlearning procedure, to enhance the retrieval accuracy. To address the issues\nabove, in this paper, we propose a novel end-to-end Deep Cross-Modal Hashing\nwith Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC).\nSpecifically, by an iterative optimization algorithm, DCHUC jointly learns\nunified hash codes for image-text pairs in a database and a pair of hash\nfunctions for unseen query image-text pairs. With the iterative optimization\nalgorithm, the learned unified hash codes can be used to guide the hashing\nfunction learning procedure; Meanwhile, the learned hashing functions can\nfeedback to guide the unified hash codes optimizing procedure. Extensive\nexperiments on three public datasets demonstrate that the proposed method\noutperforms the state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Memory-Efficiency","Evaluation","Efficiency"] },
{"key": "tu2022unsupervised", "year": "2023", "citations": "6", "title":"Unsupervised Hashing with Semantic Concept Mining", "abstract": "<p>Recently, to improve the unsupervised image retrieval performance, plenty of\nunsupervised hashing methods have been proposed by designing a semantic\nsimilarity matrix, which is based on the similarities between image features\nextracted by a pre-trained CNN model. However, most of these methods tend to\nignore high-level abstract semantic concepts contained in images. Intuitively,\nconcepts play an important role in calculating the similarity among images. In\nreal-world scenarios, each image is associated with some concepts, and the\nsimilarity between two images will be larger if they share more identical\nconcepts. Inspired by the above intuition, in this work, we propose a novel\nUnsupervised Hashing with Semantic Concept Mining, called UHSCM, which\nleverages a VLP model to construct a high-quality similarity matrix.\nSpecifically, a set of randomly chosen concepts is first collected. Then, by\nemploying a vision-language pretraining (VLP) model with the prompt engineering\nwhich has shown strong power in visual representation learning, the set of\nconcepts is denoised according to the training images. Next, the proposed\nmethod UHSCM applies the VLP model with prompting again to mine the concept\ndistribution of each image and construct a high-quality semantic similarity\nmatrix based on the mined concept distributions. Finally, with the semantic\nsimilarity matrix as guiding information, a novel hashing loss with a modified\ncontrastive loss based regularization item is proposed to optimize the hashing\nnetwork. Extensive experiments on three benchmark datasets show that the\nproposed method outperforms the state-of-the-art baselines in the image\nretrieval task.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Datasets","Neural-Hashing","Unsupervised","Evaluation"] },
{"key": "tuinhof2018image", "year": "2019", "citations": "65", "title":"Image Based Fashion Product Recommendation with Deep Learning", "abstract": "<p>We develop a two-stage deep learning framework that recommends fashion images\nbased on other input images of similar style. For that purpose, a neural\nnetwork classifier is used as a data-driven, visually-aware feature extractor.\nThe latter then serves as input for similarity-based recommendations using a\nranking algorithm. Our approach is tested on the publicly available Fashion\ndataset. Initialization strategies using transfer learning from larger product\ndatabases are presented. Combined with more traditional content-based\nrecommendation systems, our framework can help to increase robustness and\nperformance, for example, by better matching a particular customer style.</p>\n", "tags": ["Tools-&-Libraries","Datasets","Recommender-Systems","Evaluation","Robustness"] },
{"key": "tuinhof2019image", "year": "2019", "citations": "65", "title":"Image Based Fashion Product Recommendation with Deep Learning", "abstract": "<p>We develop a two-stage deep learning framework that recommends fashion images\nbased on other input images of similar style. For that purpose, a neural\nnetwork classifier is used as a data-driven, visually-aware feature extractor.\nThe latter then serves as input for similarity-based recommendations using a\nranking algorithm. Our approach is tested on the publicly available Fashion\ndataset. Initialization strategies using transfer learning from larger product\ndatabases are presented. Combined with more traditional content-based\nrecommendation systems, our framework can help to increase robustness and\nperformance, for example, by better matching a particular customer style.</p>\n", "tags": ["Tools-&-Libraries","Datasets","Recommender-Systems","Evaluation","Robustness"] },
{"key": "uno2009efficient", "year": "2009", "citations": "10", "title":"Efficient Construction of Neighborhood Graphs by the Multiple Sorting Method", "abstract": "<p>Neighborhood graphs are gaining popularity as a concise data representation\nin machine learning. However, naive graph construction by pairwise distance\ncalculation takes \\(O(n^2)\\) runtime for \\(n\\) data points and this is\nprohibitively slow for millions of data points. For strings of equal length,\nthe multiple sorting method (Uno, 2008) can construct an \\(\\epsilon\\)-neighbor\ngraph in \\(O(n+m)\\) time, where \\(m\\) is the number of \\(\\epsilon\\)-neighbor pairs in\nthe data. To introduce this remarkably efficient algorithm to continuous\ndomains such as images, signals and texts, we employ a random projection method\nto convert vectors to strings. Theoretical results are presented to elucidate\nthe trade-off between approximation quality and computation time. Empirical\nresults show the efficiency of our method in comparison to fast nearest\nneighbor alternatives.</p>\n", "tags": ["Graph-Based-ANN","Evaluation","Locality-Sensitive-Hashing","Efficiency"] },
{"key": "vaccaro2020image", "year": "2020", "citations": "16", "title":"Image Retrieval using Multi-scale CNN Features Pooling", "abstract": "<p>In this paper, we address the problem of image retrieval by learning images\nrepresentation based on the activations of a Convolutional Neural Network. We\npresent an end-to-end trainable network architecture that exploits a novel\nmulti-scale local pooling based on NetVLAD and a triplet mining procedure based\non samples difficulty to obtain an effective image representation. Extensive\nexperiments show that our approach is able to reach state-of-the-art results on\nthree standard datasets.</p>\n", "tags": ["Datasets","Image-Retrieval","Multimodal-Retrieval"] },
{"key": "vanblokland2020indexing", "year": "2020", "citations": "14", "title":"An Indexing Scheme and Descriptor for 3D Object Retrieval Based on Local Shape Querying", "abstract": "<p>A binary descriptor indexing scheme based on Hamming distance called the\nHamming tree for local shape queries is presented. A new binary clutter\nresistant descriptor named Quick Intersection Count Change Image (QUICCI) is\nalso introduced. This local shape descriptor is extremely small and fast to\ncompare. Additionally, a novel distance function called Weighted Hamming\napplicable to QUICCI images is proposed for retrieval applications. The\neffectiveness of the indexing scheme and QUICCI is demonstrated on 828 million\nQUICCI images derived from the SHREC2017 dataset, while the clutter resistance\nof QUICCI is shown using the clutterbox experiment.</p>\n", "tags": ["Datasets"] },
{"key": "vanblokland2021partial", "year": "2021", "citations": "6", "title":"Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and Dissimilarity Tree Indexing", "abstract": "<p>A complete pipeline is presented for accurate and efficient partial 3D object\nretrieval based on Quick Intersection Count Change Image (QUICCI) binary local\ndescriptors and a novel indexing tree. It is shown how a modification to the\nQUICCI query descriptor makes it ideal for partial retrieval. An indexing\nstructure called Dissimilarity Tree is proposed which can significantly\naccelerate searching the large space of local descriptors; this is applicable\nto QUICCI and other binary descriptors. The index exploits the distribution of\nbits within descriptors for efficient retrieval. The retrieval pipeline is\ntested on the artificial part of SHREC’16 dataset with near-ideal retrieval\nresults.</p>\n", "tags": ["Similarity-Search","Datasets"] },
{"key": "vasile2016meta", "year": "2016", "citations": "120", "title":"Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation", "abstract": "<p>We propose Meta-Prod2vec, a novel method to compute item similarities for\nrecommendation that leverages existing item metadata. Such scenarios are\nfrequently encountered in applications such as content recommendation, ad\ntargeting and web search. Our method leverages past user interactions with\nitems and their attributes to compute low-dimensional embeddings of items.\nSpecifically, the item metadata is in- jected into the model as side\ninformation to regularize the item embeddings. We show that the new item\nrepresenta- tions lead to better performance on recommendation tasks on an open\nmusic dataset.</p>\n", "tags": ["Datasets","Recommender-Systems","Evaluation"] },
{"key": "veit2016conditional", "year": "2017", "citations": "168", "title":"Conditional Similarity Networks", "abstract": "<p>What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately.</p>\n", "tags": ["CVPR"] },
{"key": "veit2017conditional", "year": "2017", "citations": "168", "title":"Conditional Similarity Networks", "abstract": "<p>What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately.</p>\n", "tags": ["CVPR"] },
{"key": "vemulapalli2018compact", "year": "2019", "citations": "102", "title":"A Compact Embedding for Facial Expression Similarity", "abstract": "<p>Most of the existing work on automatic facial expression analysis focuses on\ndiscrete emotion recognition, or facial action unit detection. However, facial\nexpressions do not always fall neatly into pre-defined semantic categories.\nAlso, the similarity between expressions measured in the action unit space need\nnot correspond to how humans perceive expression similarity. Different from\nprevious work, our goal is to describe facial expressions in a continuous\nfashion using a compact embedding space that mimics human visual preferences.\nTo achieve this goal, we collect a large-scale faces-in-the-wild dataset with\nhuman annotations in the form: Expressions A and B are visually more similar\nwhen compared to expression C, and use this dataset to train a neural network\nthat produces a compact (16-dimensional) expression embedding. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications such as expression retrieval, photo album\nsummarization, and emotion recognition. We also show that the embedding learned\nusing the proposed dataset performs better than several other embeddings\nlearned using existing emotion or action unit datasets.</p>\n", "tags": ["Datasets","Scalability","CVPR"] },
{"key": "vemulapalli2019compact", "year": "2019", "citations": "102", "title":"A Compact Embedding for Facial Expression Similarity", "abstract": "<p>Most of the existing work on automatic facial expression analysis focuses on\ndiscrete emotion recognition, or facial action unit detection. However, facial\nexpressions do not always fall neatly into pre-defined semantic categories.\nAlso, the similarity between expressions measured in the action unit space need\nnot correspond to how humans perceive expression similarity. Different from\nprevious work, our goal is to describe facial expressions in a continuous\nfashion using a compact embedding space that mimics human visual preferences.\nTo achieve this goal, we collect a large-scale faces-in-the-wild dataset with\nhuman annotations in the form: Expressions A and B are visually more similar\nwhen compared to expression C, and use this dataset to train a neural network\nthat produces a compact (16-dimensional) expression embedding. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications such as expression retrieval, photo album\nsummarization, and emotion recognition. We also show that the embedding learned\nusing the proposed dataset performs better than several other embeddings\nlearned using existing emotion or action unit datasets.</p>\n", "tags": ["Datasets","Scalability","CVPR"] },
{"key": "verma2019diversity", "year": "2018", "citations": "25", "title":"Diversity in Fashion Recommendation using Semantic Parsing", "abstract": "<p>Developing recommendation system for fashion images is challenging due to the\ninherent ambiguity associated with what criterion a user is looking at.\nSuggesting multiple images where each output image is similar to the query\nimage on the basis of a different feature or part is one way to mitigate the\nproblem. Existing works for fashion recommendation have used Siamese or Triplet\nnetwork to learn features between a similar pair and a similar-dissimilar\ntriplet respectively. However, these methods do not provide basic information\nsuch as, how two clothing images are similar, or which parts present in the two\nimages make them similar. In this paper, we propose to recommend images by\nexplicitly learning and exploiting part based similarity. We propose a novel\napproach of learning discriminative features from weakly-supervised data by\nusing visual attention over the parts and a texture encoding network. We show\nthat the learned features surpass the state-of-the-art in retrieval task on\nDeepFashion dataset. We then use the proposed model to recommend fashion images\nhaving an explicit variation with respect to similarity of any of the parts.</p>\n", "tags": ["Datasets","Recommender-Systems","Supervised"] },
{"key": "vo2018composing", "year": "2019", "citations": "317", "title":"Composing Text and Image for Image Retrieval - An Empirical Odyssey", "abstract": "<p>In this paper, we study the task of image retrieval, where the input query is\nspecified in the form of an image plus some text that describes desired\nmodifications to the input image. For example, we may present an image of the\nEiffel tower, and ask the system to find images which are visually similar but\nare modified in small ways, such as being taken at nighttime instead of during\nthe day. To tackle this task, we learn a similarity metric between a target\nimage and a source image plus source text, an embedding and composing function\nsuch that target image feature is close to the source image plus text\ncomposition feature. We propose a new way to combine image and text using such\nfunction that is designed for the retrieval task. We show this outperforms\nexisting approaches on 3 different datasets, namely Fashion-200k, MIT-States\nand a new synthetic dataset we create based on CLEVR. We also show that our\napproach can be used to classify input queries, in addition to image retrieval.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","CVPR","Image-Retrieval"] },
{"key": "vu2016search", "year": "2017", "citations": "48", "title":"Search Personalization with Embeddings", "abstract": "<p>Recent research has shown that the performance of search personalization\ndepends on the richness of user profiles which normally represent the user’s\ntopical interests. In this paper, we propose a new embedding approach to\nlearning user profiles, where users are embedded on a topical interest space.\nWe then directly utilize the user profiles for search personalization.\nExperiments on query logs from a major commercial web search engine demonstrate\nthat our embedding approach improves the performance of the search engine and\nalso achieves better search performance than other strong baselines.</p>\n", "tags": ["Evaluation"] },
{"key": "vu2017search", "year": "2017", "citations": "48", "title":"Search Personalization with Embeddings", "abstract": "<p>Recent research has shown that the performance of search personalization\ndepends on the richness of user profiles which normally represent the user’s\ntopical interests. In this paper, we propose a new embedding approach to\nlearning user profiles, where users are embedded on a topical interest space.\nWe then directly utilize the user profiles for search personalization.\nExperiments on query logs from a major commercial web search engine demonstrate\nthat our embedding approach improves the performance of the search engine and\nalso achieves better search performance than other strong baselines.</p>\n", "tags": ["Evaluation"] },
{"key": "wang2010semi", "year": "2010", "citations": "626", "title":"Semi-supervised Deep Quantization for Cross-modal Search", "abstract": "<p>The problem of cross-modal similarity search, which aims at making efficient and accurate queries across multiple domains, has become a significant and important research topic. Composite quantization, a compact coding solution superior to hashing techniques, has shown its effectiveness for similarity search. However, most existing works utilizing composite quantization to search multi-domain content only consider either pairwise similarity information or class label information across different domains, which fails to tackle the semi-supervised problem in composite quantization. In this paper, we address the semi-supervised quantization problem by considering: (i) pairwise similarity information (without class label information) across different domains, which captures the intra-document relation, (ii) cross-domain data with class label which can help capture inter-document relation, and (iii) cross-domain data with neither pairwise similarity nor class label which enables the full use of abundant unlabelled information. To the best of our knowledge, we are the first to consider both supervised information (pairwise similarity + class label) and unsupervised information (neither pairwise similarity nor class label) simultaneously in composite quantization. A challenging problem arises: how can we jointly handle these three sorts of information across multiple domains in an efficient way? To tackle this challenge, we propose a novel semi-supervised deep quantization (SSDQ) model that takes both supervised and unsupervised information into account. The proposed SSDQ model is capable of incorporating the above three kinds of information into one single framework when utilizing composite quantization for accurate and efficient queries across different domains. More specifically, we employ a modified deep autoencoder for better latent representation and formulate pairwise similarity loss, supervised quantization loss as well as unsupervised distribution match loss to handle all three types of information. The extensive experiments demonstrate the significant improvement of SSDQ over several state-of-the-art methods on various datasets.</p>\n", "tags": ["Datasets","CVPR","Tools-&-Libraries","Supervised","Similarity-Search","Quantization","Hashing-Methods","Unsupervised"] },
{"key": "wang2010sequential", "year": "2010", "citations": "328", "title":"Sequential projection learning for hashing with compact codes", "abstract": "<p>Hashing based Approximate Nearest Neighbor\n(ANN) search has attracted much attention\ndue to its fast query time and drastically\nreduced storage. However, most of the hashing\nmethods either use random projections or\nextract principal directions from the data to\nderive hash functions. The resulting embedding\nsuffers from poor discrimination when\ncompact codes are used. In this paper, we\npropose a novel data-dependent projection\nlearning method such that each hash function\nis designed to correct the errors made by\nthe previous one sequentially. The proposed\nmethod easily adapts to both unsupervised\nand semi-supervised scenarios and shows significant\nperformance gains over the state-ofthe-art\nmethods on two large datasets containing\nup to 1 million points.</p>\n", "tags": ["Efficiency","Unsupervised","Datasets","Locality-Sensitive-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "wang2015hamming", "year": "2015", "citations": "20", "title":"Hamming Compatible Quantization for Hashing", "abstract": "<p>Hashing is one of the effective techniques for fast\nApproximate Nearest Neighbour (ANN) search.\nTraditional single-bit quantization (SBQ) in most\nhashing methods incurs lots of quantization error\nwhich seriously degrades the search performance.\nTo address the limitation of SBQ, researchers have\nproposed promising multi-bit quantization (MBQ)\nmethods to quantize each projection dimension\nwith multiple bits. However, some MBQ methods\nneed to adopt specific distance for binary code\nmatching instead of the original Hamming distance,\nwhich would significantly decrease the retrieval\nspeed. Two typical MBQ methods Hierarchical\nQuantization and Double Bit Quantization\nretain the Hamming distance, but both of them only\nconsider the projection dimensions during quantization,\nignoring the neighborhood structure of raw\ndata inherent in Euclidean space. In this paper,\nwe propose a multi-bit quantization method named\nHamming Compatible Quantization (HCQ) to preserve\nthe capability of similarity metric between\nEuclidean space and Hamming space by utilizing\nthe neighborhood structure of raw data. Extensive\nexperiment results have shown our approach significantly\nimproves the performance of various stateof-the-art\nhashing methods while maintaining fast\nretrieval speed.</p>\n", "tags": ["Distance-Metric-Learning","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation","Quantization"] },
{"key": "wang2015semantic", "year": "2015", "citations": "149", "title":"Semantic Topic Multimodal Hashing for Cross-Media Retrieval", "abstract": "<p>Multimodal hashing is essential to cross-media\nsimilarity search for its low storage cost and fast\nquery speed. Most existing multimodal hashing\nmethods embedded heterogeneous data into a common low-dimensional Hamming space, and then\nrounded the continuous embeddings to obtain the\nbinary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For\nthis purpose, a novel Semantic Topic Multimodal\nHashing (STMH) is developed by considering latent semantic information in coding procedure.\nIt\nfirst discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.\nThen the learned multimodal semantic features are\ntransformed into a common subspace by their correlations. Finally, each bit of unified hash code\ncan be generated directly by figuring out whether a\ntopic or concept is contained in a text or an image.\nTherefore, the obtained model by STMH is more\nsuitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method\noutperforms several state-of-the-art methods.</p>\n", "tags": ["Memory-Efficiency","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "wang2016comprehensive", "year": "2016", "citations": "230", "title":"A Comprehensive Survey on Cross-modal Retrieval", "abstract": "<p>In recent years, cross-modal retrieval has drawn much attention due to the\nrapid growth of multimodal data. It takes one type of data as the query to\nretrieve relevant data of another type. For example, a user can use a text to\nretrieve relevant pictures or videos. Since the query and its retrieved results\ncan be of different modalities, how to measure the content similarity between\ndifferent modalities of data remains a challenge. Various methods have been\nproposed to deal with such a problem. In this paper, we first review a number\nof representative methods for cross-modal retrieval and classify them into two\nmain groups: 1) real-valued representation learning, and 2) binary\nrepresentation learning. Real-valued representation learning methods aim to\nlearn real-valued common representations for different modalities of data. To\nspeed up the cross-modal retrieval, a number of binary representation learning\nmethods are proposed to map different modalities of data into a common Hamming\nspace. Then, we introduce several multimodal datasets in the community, and\nshow the experimental results on two commonly used multimodal datasets. The\ncomparison reveals the characteristic of different kinds of cross-modal\nretrieval methods, which is expected to benefit both practical applications and\nfuture research. Finally, we discuss open problems and future research\ndirections.</p>\n", "tags": ["Hashing-Methods","Datasets","Survey-Paper","Evaluation","Multimodal-Retrieval"] },
{"key": "wang2016contextual", "year": "2016", "citations": "7", "title":"Contextual Visual Similarity", "abstract": "<p>Measuring visual similarity is critical for image understanding. But what\nmakes two images similar? Most existing work on visual similarity assumes that\nimages are similar because they contain the same object instance or category.\nHowever, the reason why images are similar is much more complex. For example,\nfrom the perspective of category, a black dog image is similar to a white dog\nimage. However, in terms of color, a black dog image is more similar to a black\nhorse image than the white dog image. This example serves to illustrate that\nvisual similarity is ambiguous but can be made precise when given an explicit\ncontextual perspective. Based on this observation, we propose the concept of\ncontextual visual similarity. To be concrete, we examine the concept of\ncontextual visual similarity in the application domain of image search. Instead\nof providing only a single image for image similarity search (\\eg, Google image\nsearch), we require three images. Given a query image, a second positive image\nand a third negative image, dissimilar to the first two images, we define a\ncontextualized similarity search criteria. In particular, we learn feature\nweights over all the feature dimensions of each image such that the distance\nbetween the query image and the positive image is small and their distances to\nthe negative image are large after reweighting their features. The learned\nfeature weights encode the contextualized visual similarity specified by the\nuser and can be used for attribute specific image search. We also show the\nusefulness of our contextualized similarity weighting scheme for different\ntasks, such as answering visual analogy questions and unsupervised attribute\ndiscovery.</p>\n", "tags": ["Similarity-Search","Unsupervised","Image-Retrieval"] },
{"key": "wang2016deep", "year": "2017", "citations": "192", "title":"Deep Supervised Hashing with Triplet Labels", "abstract": "<p>Hashing is one of the most popular and powerful approximate nearest neighbor\nsearch techniques for large-scale image retrieval. Most traditional hashing\nmethods first represent images as off-the-shelf visual features and then\nproduce hashing codes in a separate stage. However, off-the-shelf visual\nfeatures may not be optimally compatible with the hash code learning procedure,\nwhich may result in sub-optimal hash codes. Recently, deep hashing methods have\nbeen proposed to simultaneously learn image features and hash codes using deep\nneural networks and have shown superior performance over traditional hashing\nmethods. Most deep hashing methods are given supervised information in the form\nof pairwise labels or triplet labels. The current state-of-the-art deep hashing\nmethod DPSH~\\cite{li2015feature}, which is based on pairwise labels, performs\nimage feature learning and hash code learning simultaneously by maximizing the\nlikelihood of pairwise similarities. Inspired by DPSH~\\cite{li2015feature}, we\npropose a triplet label based deep hashing method which aims to maximize the\nlikelihood of the given triplet labels. Experimental results show that our\nmethod outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets,\nincluding the state-of-the-art method DPSH~\\cite{li2015feature} and all the\nprevious triplet label based deep hashing methods.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Scalability","Evaluation"] },
{"key": "wang2016survey", "year": "2017", "citations": "929", "title":"A Survey on Learning to Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the\ndatabase such that the distances from them to the query point are the smallest.\nLearning to hash is one of the major solutions to this problem and has been\nwidely studied recently. In this paper, we present a comprehensive survey of\nthe learning to hash algorithms, categorize them according to the manners of\npreserving the similarities into: pairwise similarity preserving, multiwise\nsimilarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity\npreserving as the objective function is very different though quantization, as\nwe show, can be derived from preserving the pairwise similarities. In addition,\nwe present the evaluation protocols, and the general performance analysis, and\npoint out that the quantization algorithms perform superiorly in terms of\nsearch accuracy, search time cost, and space cost. Finally, we introduce a few\nemerging topics.</p>\n", "tags": ["Hashing-Methods","Quantization","Survey-Paper","Evaluation"] },
{"key": "wang2017attention", "year": "2017", "citations": "12", "title":"An Attention-Based Deep Net for Learning to Rank", "abstract": "<p>In information retrieval, learning to rank constructs a machine-based ranking\nmodel which given a query, sorts the search results by their degree of\nrelevance or importance to the query. Neural networks have been successfully\napplied to this problem, and in this paper, we propose an attention-based deep\nneural network which better incorporates different embeddings of the queries\nand search results with an attention-based mechanism. This model also applies a\ndecoder mechanism to learn the ranks of the search results in a listwise\nfashion. The embeddings are trained with convolutional neural networks or the\nword2vec model. We demonstrate the performance of this model with image\nretrieval and text querying data sets.</p>\n", "tags": ["Evaluation"] },
{"key": "wang2017composite", "year": "2018", "citations": "27", "title":"Composite Quantization", "abstract": "<p>This paper studies the compact coding approach to approximate nearest\nneighbor search. We introduce a composite quantization framework. It uses the\ncomposition of several (\\(M\\)) elements, each of which is selected from a\ndifferent dictionary, to accurately approximate a \\(D\\)-dimensional vector, thus\nyielding accurate search, and represents the data vector by a short code\ncomposed of the indices of the selected elements in the corresponding\ndictionaries. Our key contribution lies in introducing a near-orthogonality\nconstraint, which makes the search efficiency is guaranteed as the cost of the\ndistance computation is reduced to \\(O(M)\\) from \\(O(D)\\) through a distance table\nlookup scheme. The resulting approach is called near-orthogonal composite\nquantization. We theoretically justify the equivalence between near-orthogonal\ncomposite quantization and minimizing an upper bound of a function formed by\njointly considering the quantization error and the search cost according to a\ngeneralized triangle inequality. We empirically show the efficacy of the\nproposed approach over several benchmark datasets. In addition, we demonstrate\nthe superior performances in other three applications: combination with\ninverted multi-index, quantizing the query for mobile search, and inner-product\nsimilarity search.</p>\n", "tags": ["Similarity-Search","Vector-Indexing","Tools-&-Libraries","Datasets","Quantization","Evaluation","Efficiency"] },
{"key": "wang2017learning", "year": "2018", "citations": "529", "title":"Learning Two-Branch Neural Networks for Image-Text Matching Tasks", "abstract": "<p>Image-language matching tasks have recently attracted a lot of attention in\nthe computer vision field. These tasks include image-sentence matching, i.e.,\ngiven an image query, retrieving relevant sentences and vice versa, and\nregion-phrase matching or visual grounding, i.e., matching a phrase to relevant\nregions. This paper investigates two-branch neural networks for learning the\nsimilarity between these two data modalities. We propose two network structures\nthat produce different output representations. The first one, referred to as an\nembedding network, learns an explicit shared latent embedding space with a\nmaximum-margin ranking loss and novel neighborhood constraints. Compared to\nstandard triplet sampling, we perform improved neighborhood sampling that takes\nneighborhood information into consideration while constructing mini-batches.\nThe second network structure, referred to as a similarity network, fuses the\ntwo branches via element-wise product and is trained with regression loss to\ndirectly predict a similarity score. Extensive experiments show that our\nnetworks achieve high accuracies for phrase localization on the Flickr30K\nEntities dataset and for bi-directional image-sentence retrieval on Flickr30K\nand MSCOCO datasets.</p>\n", "tags": ["Datasets"] },
{"key": "wang2017supervised", "year": "2018", "citations": "26", "title":"Supervised Deep Hashing for Hierarchical Labeled Data", "abstract": "<p>Recently, hashing methods have been widely used in large-scale image\nretrieval. However, most existing hashing methods did not consider the\nhierarchical relation of labels, which means that they ignored the rich\ninformation stored in the hierarchy. Moreover, most of previous works treat\neach bit in a hash code equally, which does not meet the scenario of\nhierarchical labeled data. In this paper, we propose a novel deep hashing\nmethod, called supervised hierarchical deep hashing (SHDH), to perform hash\ncode learning for hierarchical labeled data. Specifically, we define a novel\nsimilarity formula for hierarchical labeled data by weighting each layer, and\ndesign a deep convolutional neural network to obtain a hash code for each data\npoint. Extensive experiments on several real-world public datasets show that\nthe proposed method outperforms the state-of-the-art baselines in the image\nretrieval task.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Neural-Hashing","AAAI","Scalability"] },
{"key": "wang2017survey", "year": "2017", "citations": "929", "title":"A Survey on Learning to Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the\nquery point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this\npaper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving\nthe similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different\nthough quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation\nprotocols, and the general performance analysis, and point out that the quantization algori</p>\n", "tags": ["Hashing-Methods","Evaluation","Quantization","Survey-Paper"] },
{"key": "wang2018deep", "year": "2019", "citations": "45", "title":"Deep Metric Learning by Online Soft Mining and Class-Aware Attention", "abstract": "<p>Deep metric learning aims to learn a deep embedding that can capture the\nsemantic similarity of data points. Given the availability of massive training\nsamples, deep metric learning is known to suffer from slow convergence due to a\nlarge fraction of trivial samples. Therefore, most existing methods generally\nresort to sample mining strategies for selecting nontrivial samples to\naccelerate convergence and improve performance. In this work, we identify two\ncritical limitations of the sample mining methods, and provide solutions for\nboth of them. First, previous mining methods assign one binary score to each\nsample, i.e., dropping or keeping it, so they only selects a subset of relevant\nsamples in a mini-batch. Therefore, we propose a novel sample mining method,\ncalled Online Soft Mining (OSM), which assigns one continuous score to each\nsample to make use of all samples in the mini-batch. OSM learns extended\nmanifolds that preserve useful intraclass variances by focusing on more similar\npositives. Second, the existing methods are easily influenced by outliers as\nthey are generally included in the mined subset. To address this, we introduce\nClass-Aware Attention (CAA) that assigns little attention to abnormal data\nsamples. Furthermore, by combining OSM and CAA, we propose a novel weighted\ncontrastive loss to learn discriminative embeddings. Extensive experiments on\ntwo fine-grained visual categorisation datasets and two video-based person\nre-identification benchmarks show that our method significantly outperforms the\nstate-of-the-art.</p>\n", "tags": ["AAAI","Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "wang2019cluster", "year": "2020", "citations": "25", "title":"Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search", "abstract": "<p>Large-scale cross-modal hashing similarity retrieval has attracted more and\nmore attention in modern search applications such as search engines and\nautopilot, showing great superiority in computation and storage. However,\ncurrent unsupervised cross-modal hashing methods still have some limitations:\n(1)many methods relax the discrete constraints to solve the optimization\nobjective which may significantly degrade the retrieval performance;(2)most\nexisting hashing model project heterogenous data into a common latent space,\nwhich may always lose sight of diversity in heterogenous data;(3)transforming\nreal-valued data point to binary codes always results in abundant loss of\ninformation, producing the suboptimal continuous latent space. To overcome\nabove problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)\nmethod is proposed. Specifically, CUH jointly performs the multi-view\nclustering that projects the original data points from different modalities\ninto its own low-dimensional latent semantic space and finds the cluster\ncentroid points and the common clustering indicators in its own low-dimensional\nspace, and learns the compact hash codes and the corresponding linear hash\nfunctions. An discrete optimization framework is developed to learn the unified\nbinary codes across modalities under the guidance cluster-wise code-prototypes.\nThe reasonableness and effectiveness of CUH is well demonstrated by\ncomprehensive experiments on diverse benchmark datasets.</p>\n", "tags": ["Similarity-Search","Scalability","Evaluation","Datasets","Unsupervised","Compact-Codes","Hashing-Methods","Tools-&-Libraries","Neural-Hashing","Supervised","CVPR"] },
{"key": "wang2019cross", "year": "2020", "citations": "219", "title":"Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval", "abstract": "<p>Image-text retrieval of natural scenes has been a popular research topic.\nSince image and text are heterogeneous cross-modal data, one of the key\nchallenges is how to learn comprehensive yet unified representations to express\nthe multi-modal data. A natural scene image mainly involves two kinds of visual\nconcepts, objects and their relationships, which are equally essential to\nimage-text retrieval. Therefore, a good representation should account for both\nof them. In the light of recent success of scene graph in many CV and NLP tasks\nfor describing complex natural scenes, we propose to represent image and text\nwith two kinds of scene graphs: visual scene graph (VSG) and textual scene\ngraph (TSG), each of which is exploited to jointly characterize objects and\nrelationships in the corresponding modality. The image-text retrieval task is\nthen naturally formulated as cross-modal scene graph matching. Specifically, we\ndesign two particular scene graph encoders in our model for VSG and TSG, which\ncan refine the representation of each node on the graph by aggregating\nneighborhood information. As a result, both object-level and relationship-level\ncross-modal features can be obtained, which favorably enables us to evaluate\nthe similarity of image and text in the two levels in a more plausible way. We\nachieve state-of-the-art results on Flickr30k and MSCOCO, which verifies the\nadvantages of our graph matching based approach for image-text retrieval.</p>\n", "tags": ["Text-Retrieval"] },
{"key": "wang2019fusion", "year": "2019", "citations": "15", "title":"Fusion-supervised Deep Cross-modal Hashing", "abstract": "<p>Deep hashing has recently received attention in cross-modal retrieval for its\nimpressive advantages. However, existing hashing methods for cross-modal\nretrieval cannot fully capture the heterogeneous multi-modal correlation and\nexploit the semantic information. In this paper, we propose a novel\n<em>Fusion-supervised Deep Cross-modal Hashing</em> (FDCH) approach. Firstly,\nFDCH learns unified binary codes through a fusion hash network with paired\nsamples as input, which effectively enhances the modeling of the correlation of\nheterogeneous multi-modal data. Then, these high-quality unified hash codes\nfurther supervise the training of the modality-specific hash networks for\nencoding out-of-sample queries. Meanwhile, both pair-wise similarity\ninformation and classification information are embedded in the hash networks\nunder one stream framework, which simultaneously preserves cross-modal\nsimilarity and keeps semantic consistency. Experimental results on two\nbenchmark datasets demonstrate the state-of-the-art performance of FDCH.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Evaluation","Multimodal-Retrieval"] },
{"key": "wang2019learning", "year": "2019", "citations": "127", "title":"Learning Cross-Modal Embeddings with Adversarial Networks for Cooking Recipes and Food Images", "abstract": "<p>Food computing is playing an increasingly important role in human daily life,\nand has found tremendous applications in guiding human behavior towards smart\nfood consumption and healthy lifestyle. An important task under the\nfood-computing umbrella is retrieval, which is particularly helpful for health\nrelated applications, where we are interested in retrieving important\ninformation about food (e.g., ingredients, nutrition, etc.). In this paper, we\ninvestigate an open research task of cross-modal retrieval between cooking\nrecipes and food images, and propose a novel framework Adversarial Cross-Modal\nEmbedding (ACME) to resolve the cross-modal retrieval task in food domains.\nSpecifically, the goal is to learn a common embedding feature space between the\ntwo modalities, in which our approach consists of several novel ideas: (i)\nlearning by using a new triplet loss scheme together with an effective sampling\nstrategy, (ii) imposing modality alignment using an adversarial learning\nstrategy, and (iii) imposing cross-modal translation consistency such that the\nembedding of one modality is able to recover some important information of\ncorresponding instances in the other modality. ACME achieves the\nstate-of-the-art performance on the benchmark Recipe1M dataset, validating the\nefficacy of the proposed technique.</p>\n", "tags": ["Tools-&-Libraries","Distance-Metric-Learning","Datasets","CVPR","Robustness","Evaluation","Multimodal-Retrieval"] },
{"key": "wang2019memory", "year": "2019", "citations": "33", "title":"A Memory-Efficient Sketch Method for Estimating High Similarities in Streaming Sets", "abstract": "<p>Estimating set similarity and detecting highly similar sets are fundamental\nproblems in areas such as databases, machine learning, and information\nretrieval. MinHash is a well-known technique for approximating Jaccard\nsimilarity of sets and has been successfully used for many applications such as\nsimilarity search and large scale learning. Its two compressed versions, b-bit\nMinHash and Odd Sketch, can significantly reduce the memory usage of the\noriginal MinHash method, especially for estimating high similarities (i.e.,\nsimilarities around 1). Although MinHash can be applied to static sets as well\nas streaming sets, of which elements are given in a streaming fashion and\ncardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd\nSketch fail to deal with streaming data. To solve this problem, we design a\nmemory efficient sketch method, MaxLogHash, to accurately estimate Jaccard\nsimilarities in streaming sets. Compared to MinHash, our method uses smaller\nsized registers (each register consists of less than 7 bits) to build a compact\nsketch for each set. We also provide a simple yet accurate estimator for\ninferring Jaccard similarity from MaxLogHash sketches. In addition, we derive\nformulas for bounding the estimation error and determine the smallest necessary\nmemory usage (i.e., the number of registers used for a MaxLogHash sketch) for\nthe desired accuracy. We conduct experiments on a variety of datasets, and\nexperimental results show that our method MaxLogHash is about 5 times more\nmemory efficient than MinHash with the same accuracy and computational cost for\nestimating high similarities.</p>\n", "tags": ["Similarity-Search","Locality-Sensitive-Hashing","Datasets","KDD","Memory-Efficiency"] },
{"key": "wang2019multi", "year": "2019", "citations": "736", "title":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning", "abstract": "<p>A family of loss functions built on pair-based computation have been proposed\nin the literature which provide a myriad of solutions for deep metric learning.\nIn this paper, we provide a general weighting framework for understanding\nrecent pair-based loss functions. Our contributions are three-fold: (1) we\nestablish a General Pair Weighting (GPW) framework, which casts the sampling\nproblem of deep metric learning into a unified view of pair weighting through\ngradient analysis, providing a powerful tool for understanding recent\npair-based loss functions; (2) we show that with GPW, various existing\npair-based methods can be compared and discussed comprehensively, with clear\ndifferences and key limitations identified; (3) we propose a new loss called\nmulti-similarity loss (MS loss) under the GPW, which is implemented in two\niterative steps (i.e., mining and weighting). This allows it to fully consider\nthree similarities for pair weighting, providing a more principled approach for\ncollecting and weighting informative pairs. Finally, the proposed MS loss\nobtains new state-of-the-art performance on four image retrieval benchmarks,\nwhere it outperforms the most recent approaches, such as\nABE\\cite{Kim_2018_ECCV} and HTL by a large margin: 60.6% to 65.7% on CUB200,\nand 80.9% to 88.0% on In-Shop Clothes Retrieval dataset at Recall@1. Code is\navailable at https://github.com/MalongTech/research-ms-loss.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Distance-Metric-Learning","Datasets","CVPR","Evaluation"] },
{"key": "wang2019ranked", "year": "2019", "citations": "233", "title":"Ranked List Loss for Deep Metric Learning", "abstract": "<p>The objective of deep metric learning (DML) is to learn embeddings that can\ncapture semantic similarity and dissimilarity information among data points.\nExisting pairwise or tripletwise loss functions used in DML are known to suffer\nfrom slow convergence due to a large proportion of trivial pairs or triplets as\nthe model improves. To improve this, ranking-motivated structured losses are\nproposed recently to incorporate multiple examples and exploit the structured\ninformation among them. They converge faster and achieve state-of-the-art\nperformance. In this work, we unveil two limitations of existing\nranking-motivated structured losses and propose a novel ranked list loss to\nsolve both of them. First, given a query, only a fraction of data points is\nincorporated to build the similarity structure. Consequently, some useful\nexamples are ignored and the structure is less informative. To address this, we\npropose to build a set-based similarity structure by exploiting all instances\nin the gallery. The learning setting can be interpreted as few-shot retrieval:\ngiven a mini-batch, every example is iteratively used as a query, and the rest\nones compose the gallery to search, i.e., the support set in few-shot setting.\nThe rest examples are split into a positive set and a negative set. For every\nmini-batch, the learning objective of ranked list loss is to make the query\ncloser to the positive set than to the negative set by a margin. Second,\nprevious methods aim to pull positive pairs as close as possible in the\nembedding space. As a result, the intraclass data distribution tends to be\nextremely compressed. In contrast, we propose to learn a hypersphere for each\nclass in order to preserve useful similarity structure inside it, which\nfunctions as regularisation. Extensive experiments demonstrate the superiority\nof our proposal by comparing with the state-of-the-art methods.</p>\n", "tags": ["Distance-Metric-Learning","Few-Shot-&-Zero-Shot","CVPR","Evaluation"] },
{"key": "wang2019supervised", "year": "2016", "citations": "82", "title":"Supervised Quantization for Similarity Search", "abstract": "<p>In this paper, we address the problem of searching for semantically similar\nimages from a large database. We present a compact coding approach, supervised\nquantization. Our approach simultaneously learns feature selection that\nlinearly transforms the database points into a low-dimensional discriminative\nsubspace, and quantizes the data points in the transformed space. The\noptimization criterion is that the quantized points not only approximate the\ntransformed points accurately, but also are semantically separable: the points\nbelonging to a class lie in a cluster that is not overlapped with other\nclusters corresponding to other classes, which is formulated as a\nclassification problem. The experiments on several standard datasets show the\nsuperiority of our approach over the state-of-the art supervised hashing and\nunsupervised quantization algorithms.</p>\n", "tags": ["Similarity-Search","Supervised","Hashing-Methods","Datasets","Neural-Hashing","CVPR","Unsupervised","Quantization"] },
{"key": "wang2020asymmetric", "year": "2021", "citations": "21", "title":"Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval", "abstract": "<p>Due to the superiority in similarity computation and database storage for\nlarge-scale multiple modalities data, cross-modal hashing methods have\nattracted extensive attention in similarity retrieval across the heterogeneous\nmodalities. However, there are still some limitations to be further taken into\naccount: (1) most current CMH methods transform real-valued data points into\ndiscrete compact binary codes under the binary constraints, limiting the\ncapability of representation for original data on account of abundant loss of\ninformation and producing suboptimal hash codes; (2) the discrete binary\nconstraint learning model is hard to solve, where the retrieval performance may\ngreatly reduce by relaxing the binary constraints for large quantization error;\n(3) handling the learning problem of CMH in a symmetric framework, leading to\ndifficult and complex optimization objective. To address above challenges, in\nthis paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method\nis proposed. Specifically, ACQH learns the projection matrixs of heterogeneous\nmodalities data points for transforming query into a low-dimensional\nreal-valued vector in latent semantic space and constructs the stacked\ncompositional quantization embedding in a coarse-to-fine manner for indicating\ndatabase points by a series of learnt real-valued codeword in the codebook with\nthe help of pointwise label information regression simultaneously. Besides, the\nunified hash codes across modalities can be directly obtained by the discrete\niterative optimization framework devised in the paper. Comprehensive\nexperiments on diverse three benchmark datasets have shown the effectiveness\nand rationality of ACQH.</p>\n", "tags": ["Similarity-Search","Tools-&-Libraries","Hashing-Methods","Datasets","Quantization","Compact-Codes","Scalability","Evaluation","Multimodal-Retrieval"] },
{"key": "wang2020consensus", "year": "2020", "citations": "149", "title":"Consensus-Aware Visual-Semantic Embedding for Image-Text Matching", "abstract": "<p>Image-text matching plays a central role in bridging vision and language.\nMost existing approaches only rely on the image-text instance pair to learn\ntheir representations, thereby exploiting their matching relationships and\nmaking the corresponding alignments. Such approaches only exploit the\nsuperficial associations contained in the instance pairwise data, with no\nconsideration of any external commonsense knowledge, which may hinder their\ncapabilities to reason the higher-level relationships between image and text.\nIn this paper, we propose a Consensus-aware Visual-Semantic Embedding (CVSE)\nmodel to incorporate the consensus information, namely the commonsense\nknowledge shared between both modalities, into image-text matching.\nSpecifically, the consensus information is exploited by computing the\nstatistical co-occurrence correlations between the semantic concepts from the\nimage captioning corpus and deploying the constructed concept correlation graph\nto yield the consensus-aware concept (CAC) representations. Afterwards, CVSE\nlearns the associations and alignments between image and text based on the\nexploited consensus as well as the instance-level representations for both\nmodalities. Extensive experiments conducted on two public datasets verify that\nthe exploited consensus makes significant contributions to constructing more\nmeaningful visual-semantic embeddings, with the superior performances over the\nstate-of-the-art approaches on the bidirectional image and text retrieval task.\nOur code of this paper is available at: https://github.com/BruceW91/CVSE.</p>\n", "tags": ["Datasets","Text-Retrieval"] },
{"key": "wang2020deep", "year": "2020", "citations": "9", "title":"Deep Collaborative Discrete Hashing with Semantic-Invariant Structure", "abstract": "<p>Existing deep hashing approaches fail to fully explore semantic correlations and neglect the effect of linguistic context on visual attention learning, leading to inferior performance. This paper proposes a dual-stream learning framework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantic labels. Specifically, the context-aware representations are generated by employing the outer product of visual embeddings and semantic encodings. Moreover, we reconstruct the labels and introduce the focal loss to take advantage of frequent and rare concepts. The common binary code space is built on the joint learning of the visual representations attended by language, the semantic-invariant structure construction and the label distribution correction. Extensive experiments demonstrate the superiority of our method.</p>\n", "tags": ["Neural-Hashing","Tools-&-Libraries","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "wang2020distilling", "year": "2021", "citations": "21", "title":"Distilling Knowledge by Mimicking Features", "abstract": "<p>Knowledge distillation (KD) is a popular method to train efficient networks\n(“student”) with the help of high-capacity networks (“teacher”). Traditional\nmethods use the teacher’s soft logits as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher’s features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature’s magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy. We provide theoretical analyses of how LSH facilitates\nfeature direction mimicking, and further extend feature mimicking to\nmulti-label recognition and object detection.</p>\n", "tags": ["Hashing-Methods","Locality-Sensitive-Hashing"] },
{"key": "wang2020faster", "year": "2020", "citations": "54", "title":"Faster Person Re-Identification", "abstract": "<p>Fast person re-identification (ReID) aims to search person images quickly and\naccurately. The main idea of recent fast ReID methods is the hashing algorithm,\nwhich learns compact binary codes and performs fast Hamming distance and\ncounting sort. However, a very long code is needed for high accuracy (e.g.\n2048), which compromises search speed. In this work, we introduce a new\nsolution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code\nsearch strategy, which complementarily uses short and long codes, achieving\nboth faster speed and better accuracy. It uses shorter codes to coarsely rank\nbroad matching similarities and longer codes to refine only a few top\ncandidates for more accurate instance ReID. Specifically, we design an\nAll-in-One (AiO) framework together with a Distance Threshold Optimization\n(DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of\ndifferent lengths in a single model. It learns multiple codes in a pyramid\nstructure, and encourage shorter codes to mimic longer codes by\nself-distillation. DTO solves a complex threshold search problem by a simple\noptimization process, and the balance between accuracy and speed is easily\ncontrolled by a single parameter. It formulates the optimization target as a\n\\(F_{\\beta}\\) score that can be optimised by Gaussian cumulative distribution\nfunctions. Experimental results on 2 datasets show that our proposed method\n(CtF) is not only 8% more accurate but also 5x faster than contemporary hashing\nReID methods. Compared with non-hashing ReID methods, CtF is \\(50\\times\\) faster\nwith comparable accuracy. Code is available at\nhttps://github.com/wangguanan/light-reid.</p>\n", "tags": ["Hashing-Methods","Datasets","Compact-Codes","Tools-&-Libraries"] },
{"key": "wang2020online", "year": "2020", "citations": "47", "title":"Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval", "abstract": "<p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>\n", "tags": ["Scalability","Efficiency","SIGIR","Hashing-Methods","Evaluation"] },
{"key": "wang2021comprehensive", "year": "2021", "citations": "119", "title":"A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor search (ANNS) constitutes an important operation\nin a multitude of applications, including recommendation systems, information\nretrieval, and pattern recognition. In the past decade, graph-based ANNS\nalgorithms have been the leading paradigm in this domain, with dozens of\ngraph-based ANNS algorithms proposed. Such algorithms aim to provide effective,\nefficient solutions for retrieving the nearest neighbors for a given query.\nNevertheless, these efforts focus on developing and optimizing algorithms with\ndifferent approaches, so there is a real need for a comprehensive survey about\nthe approaches’ relative performance, strengths, and pitfalls. Thus here we\nprovide a thorough comparative analysis and experimental evaluation of 13\nrepresentative graph-based ANNS algorithms via a new taxonomy and fine-grained\npipeline. We compared each algorithm in a uniform test environment on eight\nreal-world datasets and 12 synthetic datasets with varying sizes and\ncharacteristics. Our study yields novel discoveries, offerings several useful\nprinciples to improve algorithms, thus designing an optimized method that\noutperforms the state-of-the-art algorithms. This effort also helped us\npinpoint algorithms’ working portions, along with rule-of-thumb recommendations\nabout promising research directions and suitable algorithms for practitioners\nin different fields.</p>\n", "tags": ["Graph-Based-ANN","Datasets","Recommender-Systems","Survey-Paper","Evaluation"] },
{"key": "wang2021cross", "year": "2021", "citations": "31", "title":"Cross-modal Zero-shot Hashing by Label Attributes Embedding", "abstract": "<p>Cross-modal hashing (CMH) is one of the most promising methods in cross-modal\napproximate nearest neighbor search. Most CMH solutions ideally assume the\nlabels of training and testing set are identical. However, the assumption is\noften violated, causing a zero-shot CMH problem. Recent efforts to address this\nissue focus on transferring knowledge from the seen classes to the unseen ones\nusing label attributes. However, the attributes are isolated from the features\nof multi-modal data. To reduce the information gap, we introduce an approach\ncalled LAEH (Label Attributes Embedding for zero-shot cross-modal Hashing).\nLAEH first gets the initial semantic attribute vectors of labels by word2vec\nmodel and then uses a transformation network to transform them into a common\nsubspace. Next, it leverages the hash vectors and the feature similarity matrix\nto guide the feature extraction network of different modalities. At the same\ntime, LAEH uses the attribute similarity as the supplement of label similarity\nto rectify the label embedding and common subspace. Experiments show that LAEH\noutperforms related representative zero-shot and cross-modal hashing methods.</p>\n", "tags": ["Hashing-Methods","Few-Shot-&-Zero-Shot","SIGIR"] },
{"key": "wang2021domain", "year": "2021", "citations": "28", "title":"Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval", "abstract": "<p>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal\nretrieval task, where abstract sketches are used as queries to retrieve natural\nimages under zero-shot scenario. Most existing methods regard ZS-SBIR as a\ntraditional classification problem and employ a cross-entropy or triplet-based\nloss to achieve retrieval, which neglect the problems of the domain gap between\nsketches and natural images and the large intra-class diversity in sketches.\nToward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.\nSpecifically, a cross-modal contrastive method is proposed to learn generalized\nrepresentations to smooth the domain gap by mining relations with additional\naugmented samples. Furthermore, a category-specific memory bank with sketch\nfeatures is explored to reduce intra-class diversity in the sketch domain.\nExtensive experiments demonstrate that our approach notably outperforms the\nstate-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source\ncode is publicly available at https://github.com/haowang1992/DSN.</p>\n", "tags": ["AAAI","Datasets","Few-Shot-&-Zero-Shot","IJCAI","Image-Retrieval"] },
{"key": "wang2021prototype", "year": "2021", "citations": "43", "title":"Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing", "abstract": "<p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","CVPR","Neural-Hashing","Tools-&-Libraries","Hashing-Methods","Evaluation","Supervised","Robustness"] },
{"key": "wang2021pseudo", "year": "2021", "citations": "54", "title":"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval", "abstract": "<p>Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,\nhave shown the usefulness of expanding and reweighting the users’ initial\nqueries using information occurring in an initial set of retrieved documents,\nknown as the pseudo-relevant set. Recently, dense retrieval – through the use\nof neural contextual language models such as BERT for analysing the documents’\nand queries’ contents and computing their relevance scores – has shown a\npromising performance on several information retrieval tasks still relying on\nthe traditional inverted index for identifying documents relevant to a query.\nTwo different dense retrieval families have emerged: the use of single embedded\nrepresentations for each passage and query (e.g. using BERT’s [CLS] token), or\nvia multiple representations (e.g. using an embedding for each token of the\nquery and document). In this work, we conduct the first study into the\npotential for multiple representation dense retrieval to be enhanced using\npseudo-relevance feedback. In particular, based on the pseudo-relevant set of\ndocuments identified using a first-pass dense retrieval, we extract\nrepresentative feedback embeddings (using KMeans clustering) – while ensuring\nthat these embeddings discriminate among passages (based on IDF) – which are\nthen added to the query representation. These additional feedback embeddings\nare shown to both enhance the effectiveness of a reranking as well as an\nadditional dense retrieval operation. Indeed, experiments on the MSMARCO\npassage ranking dataset show that MAP can be improved by upto 26% on the TREC\n2019 query set and 10% on the TREC 2020 query set by the application of our\nproposed ColBERT-PRF method on a ColBERT dense retrieval approach.</p>\n", "tags": ["Datasets","SIGIR","Evaluation"] },
{"key": "wang2021scene", "year": "2021", "citations": "41", "title":"Scene Text Retrieval via Joint Text Detection and Similarity Learning", "abstract": "<p>Scene text retrieval aims to localize and search all text instances from an\nimage gallery, which are the same or similar to a given query text. Such a task\nis usually realized by matching a query text to the recognized words, outputted\nby an end-to-end scene text spotter. In this paper, we address this problem by\ndirectly learning a cross-modal similarity between a query text and each text\ninstance from natural images. Specifically, we establish an end-to-end\ntrainable network, jointly optimizing the procedures of scene text detection\nand cross-modal similarity learning. In this way, scene text retrieval can be\nsimply performed by ranking the detected text instances with the learned\nsimilarity. Experiments on three benchmark datasets demonstrate our method\nconsistently outperforms the state-of-the-art scene text spotting/retrieval\napproaches. In particular, the proposed framework of joint detection and\nsimilarity learning achieves significantly better performance than separated\nmethods. Code is available at: https://github.com/lanfeng4659/STR-TDSL.</p>\n", "tags": ["Text-Retrieval","Tools-&-Libraries","Datasets","CVPR","Evaluation"] },
{"key": "wang2022binary", "year": "2022", "citations": "18", "title":"Binary Representation via Jointly Personalized Sparse Hashing", "abstract": "<p>Unsupervised hashing has attracted much attention for binary representation\nlearning due to the requirement of economical storage and efficiency of binary\ncodes. It aims to encode high-dimensional features in the Hamming space with\nsimilarity preservation between instances. However, most existing methods learn\nhash functions in manifold-based approaches. Those methods capture the local\ngeometric structures (i.e., pairwise relationships) of data, and lack\nsatisfactory performance in dealing with real-world scenarios that produce\nsimilar features (e.g. color and shape) with different semantic information. To\naddress this challenge, in this work, we propose an effective unsupervised\nmethod, namely Jointly Personalized Sparse Hashing (JPSH), for binary\nrepresentation learning. To be specific, firstly, we propose a novel\npersonalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different\npersonalized subspaces are constructed to reflect category-specific attributes\nfor different clusters, adaptively mapping instances within the same cluster to\nthe same Hamming space. In addition, we deploy sparse constraints for different\npersonalized subspaces to select important features. We also collect the\nstrengths of the other clusters to build the PSH module with avoiding\nover-fitting. Then, to simultaneously preserve semantic and pairwise\nsimilarities in our JPSH, we incorporate the PSH and manifold-based hash\nlearning into the seamless formulation. As such, JPSH not only distinguishes\nthe instances from different clusters, but also preserves local neighborhood\nstructures within the cluster. Finally, an alternating optimization algorithm\nis adopted to iteratively capture analytical solutions of the JPSH model.\nExtensive experiments on four benchmark datasets verify that the JPSH\noutperforms several hashing algorithms on the similarity search task.</p>\n", "tags": ["Similarity-Search","Supervised","Hashing-Methods","Datasets","Neural-Hashing","Unsupervised","Evaluation","Efficiency"] },
{"key": "wang2022contrastive", "year": "2022", "citations": "146", "title":"Contrastive Masked Autoencoders for Self-Supervised Video Hashing", "abstract": "<p>Self-Supervised Video Hashing (SSVH) models learn to generate short binary\nrepresentations for videos without ground-truth supervision, facilitating\nlarge-scale video retrieval efficiency and attracting increasing research\nattention. The success of SSVH lies in the understanding of video content and\nthe ability to capture the semantic relation among unlabeled videos. Typically,\nstate-of-the-art SSVH methods consider these two points in a two-stage training\npipeline, where they firstly train an auxiliary network by instance-wise\nmask-and-predict tasks and secondly train a hashing model to preserve the\npseudo-neighborhood structure transferred from the auxiliary network. This\nconsecutive training strategy is inflexible and also unnecessary. In this\npaper, we propose a simple yet effective one-stage SSVH method called ConMH,\nwhich incorporates video semantic information and video similarity relationship\nunderstanding in a single stage. To capture video semantic information for\nbetter hashing learning, we adopt an encoder-decoder structure to reconstruct\nthe video from its temporal-masked frames. Particularly, we find that a higher\nmasking ratio helps video understanding. Besides, we fully exploit the\nsimilarity relationship between videos by maximizing agreement between two\naugmented views of a video, which contributes to more discriminative and robust\nhash codes. Extensive experiments on three large-scale video datasets (i.e.,\nFCVID, ActivityNet and YFCC) indicate that ConMH achieves state-of-the-art\nresults. Code is available at https://github.com/huangmozhi9527/ConMH.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Video-Retrieval","Self-Supervised","Scalability","Efficiency"] },
{"key": "wang2022cross", "year": "2022", "citations": "18", "title":"Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning", "abstract": "<p>Despite the recent developments in the field of cross-modal retrieval, there\nhas been less research focusing on low-resource languages due to the lack of\nmanually annotated datasets. In this paper, we propose a noise-robust\ncross-lingual cross-modal retrieval method for low-resource languages. To this\nend, we use Machine Translation (MT) to construct pseudo-parallel sentence\npairs for low-resource languages. However, as MT is not perfect, it tends to\nintroduce noise during translation, rendering textual embeddings corrupted and\nthereby compromising the retrieval performance. To alleviate this, we introduce\na multi-view self-distillation method to learn noise-robust target-language\nrepresentations, which employs a cross-attention module to generate soft\npseudo-targets to provide direct supervision from the similarity-based view and\nfeature-based view. Besides, inspired by the back-translation in unsupervised\nMT, we minimize the semantic discrepancies between origin sentences and\nback-translated sentences to further improve the noise robustness of the\ntextual encoder. Extensive experiments are conducted on three video-text and\nimage-text cross-modal retrieval benchmarks across different languages, and the\nresults demonstrate that our method significantly improves the overall\nperformance without using extra human-labeled data. In addition, equipped with\na pre-trained visual encoder from a recent vision-and-language pre-training\nframework, i.e., CLIP, our model achieves a significant performance gain,\nshowing that our method is compatible with popular pre-training models. Code\nand data are available at https://github.com/HuiGuanLab/nrccr.</p>\n", "tags": ["Tools-&-Libraries","Datasets","Unsupervised","Robustness","Evaluation","Multimodal-Retrieval"] },
{"key": "wang2022english", "year": "2022", "citations": "9", "title":"English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings", "abstract": "<p>Universal cross-lingual sentence embeddings map semantically similar\ncross-lingual sentences into a shared embedding space. Aligning cross-lingual\nsentence embeddings usually requires supervised cross-lingual parallel\nsentences. In this work, we propose mSimCSE, which extends SimCSE to\nmultilingual settings and reveal that contrastive learning on English data can\nsurprisingly learn high-quality universal cross-lingual sentence embeddings\nwithout any parallel data. In unsupervised and weakly supervised settings,\nmSimCSE significantly improves previous sentence embedding methods on\ncross-lingual retrieval and multilingual STS tasks. The performance of\nunsupervised mSimCSE is comparable to fully supervised methods in retrieving\nlow-resource languages and multilingual STS. The performance can be further\nenhanced when cross-lingual NLI data is available. Our code is publicly\navailable at https://github.com/yaushian/mSimCSE.</p>\n", "tags": ["Unsupervised","Supervised","Evaluation","Self-Supervised","EMNLP"] },
{"key": "wang2022hybrid", "year": "2022", "citations": "5", "title":"Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval", "abstract": "<p>With the recent boom of video-based social platforms (e.g., YouTube and\nTikTok), video retrieval using sentence queries has become an important demand\nand attracts increasing research attention. Despite the decent performance,\nexisting text-video retrieval models in vision and language communities are\nimpractical for large-scale Web search because they adopt brute-force search\nbased on high-dimensional embeddings. To improve efficiency, Web search engines\nwidely apply vector compression libraries (e.g., FAISS) to post-process the\nlearned embeddings. Unfortunately, separate compression from feature encoding\ndegrades the robustness of representations and incurs performance decay. To\npursue a better balance between performance and efficiency, we propose the\nfirst quantized representation learning method for cross-view video retrieval,\nnamely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both\ncoarse-grained and fine-grained quantizations with transformers, which provide\ncomplementary understandings for texts and videos and preserve comprehensive\nsemantic information. By performing Asymmetric-Quantized Contrastive Learning\n(AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and\nmultiple fine-grained levels. This hybrid-grained learning strategy serves as\nstrong supervision on the cross-view video quantization model, where\ncontrastive learning at different levels can be mutually promoted. Extensive\nexperiments on three Web video benchmark datasets demonstrate that HCQ achieves\ncompetitive performance with state-of-the-art non-compressed retrieval methods\nwhile showing high efficiency in storage and computation. Code and\nconfigurations are available at https://github.com/gimpong/WWW22-HCQ.</p>\n", "tags": ["Tools-&-Libraries","Datasets","Quantization","Video-Retrieval","Efficiency","Self-Supervised","Scalability","Evaluation","Robustness"] },
{"key": "wang2022neural", "year": "2022", "citations": "43", "title":"A Neural Corpus Indexer for Document Retrieval", "abstract": "<p>Current state-of-the-art document retrieval solutions mainly follow an\nindex-retrieve paradigm, where the index is hard to be directly optimized for\nthe final retrieval target. In this paper, we aim to show that an end-to-end\ndeep neural network unifying training and indexing stages can significantly\nimprove the recall performance of traditional methods. To this end, we propose\nNeural Corpus Indexer (NCI), a sequence-to-sequence network that generates\nrelevant document identifiers directly for a designated query. To optimize the\nrecall performance of NCI, we invent a prefix-aware weight-adaptive decoder\narchitecture, and leverage tailored techniques including query generation,\nsemantic document identifiers, and consistency-based regularization. Empirical\nstudies demonstrated the superiority of NCI on two commonly used academic\nbenchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on\nNQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to\nthe best baseline method.</p>\n", "tags": ["Datasets","Text-Retrieval","Evaluation"] },
{"key": "wang2022text", "year": "2022", "citations": "74", "title":"Text Embeddings by Weakly-Supervised Contrastive Pre-training", "abstract": "<p>This paper presents E5, a family of state-of-the-art text embeddings that\ntransfer well to a wide range of tasks. The model is trained in a contrastive\nmanner with weak supervision signals from our curated large-scale text pair\ndataset (called CCPairs). E5 can be readily used as a general-purpose embedding\nmodel for any tasks requiring a single-vector representation of texts such as\nretrieval, clustering, and classification, achieving strong performance in both\nzero-shot and fine-tuned settings. We conduct extensive evaluations on 56\ndatasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the\nfirst model that outperforms the strong BM25 baseline on the BEIR retrieval\nbenchmark without using any labeled data. When fine-tuned, E5 obtains the best\nresults on the MTEB benchmark, beating existing embedding models with 40x more\nparameters.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Supervised","Datasets","Scalability","Evaluation"] },
{"key": "wang2023correspondence", "year": "2023", "citations": "7", "title":"Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval", "abstract": "<p>Cross-domain image retrieval aims at retrieving images across different\ndomains to excavate cross-domain classificatory or correspondence\nrelationships. This paper studies a less-touched problem of cross-domain image\nretrieval, i.e., unsupervised cross-domain image retrieval, considering the\nfollowing practical assumptions: (i) no correspondence relationship, and (ii)\nno category annotations. It is challenging to align and bridge distinct domains\nwithout cross-domain correspondence. To tackle the challenge, we present a\nnovel Correspondence-free Domain Alignment (CoDA) method to effectively\neliminate the cross-domain gap through In-domain Self-matching Supervision\n(ISS) and Cross-domain Classifier Alignment (CCA). To be specific, ISS is\npresented to encapsulate discriminative information into the latent common\nspace by elaborating a novel self-matching supervision mechanism. To alleviate\nthe cross-domain discrepancy, CCA is proposed to align distinct domain-specific\nclassifiers. Thanks to the ISS and CCA, our method could encode the\ndiscrimination into the domain-invariant embedding space for unsupervised\ncross-domain image retrieval. To verify the effectiveness of the proposed\nmethod, extensive experiments are conducted on four benchmark datasets compared\nwith six state-of-the-art methods.</p>\n", "tags": ["Image-Retrieval","Datasets","AAAI","Unsupervised","Evaluation"] },
{"key": "wang2023unified", "year": "2023", "citations": "20", "title":"Unified Coarse-to-Fine Alignment for Video-Text Retrieval", "abstract": "<p>The canonical approach to video-text retrieval leverages a coarse-grained or\nfine-grained alignment between visual and textual information. However,\nretrieving the correct video according to the text query is often challenging\nas it requires the ability to reason about both high-level (scene) and\nlow-level (object) visual clues and how they relate to the text query. To this\nend, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.\nSpecifically, our model captures the cross-modal similarity information at\ndifferent granularity levels. To alleviate the effect of irrelevant visual\nclues, we also apply an Interactive Similarity Aggregation module (ISA) to\nconsider the importance of different visual features while aggregating the\ncross-modal similarity to obtain a similarity score for each granularity.\nFinally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of\neach level before summing them, alleviating over- and under-representation\nissues at different levels. By jointly considering the crossmodal similarity of\ndifferent granularity, UCoFiA allows the effective unification of multi-grained\nalignments. Empirically, UCoFiA outperforms previous state-of-the-art\nCLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,\n1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,\nActivity-Net, and DiDeMo, respectively. Our code is publicly available at\nhttps://github.com/Ziyang412/UCoFiA.</p>\n", "tags": ["ICCV","Video-Retrieval","Text-Retrieval"] },
{"key": "wang2024weakly", "year": "2021", "citations": "9", "title":"Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval", "abstract": "<p>Deep quantization methods have shown high efficiency on large-scale image\nretrieval. However, current models heavily rely on ground-truth information,\nhindering the application of quantization in label-hungry scenarios. A more\nrealistic demand is to learn from inexhaustible uploaded images that are\nassociated with informal tags provided by amateur users. Though such sketchy\ntags do not obviously reveal the labels, they actually contain useful semantic\ninformation for supervising deep quantization. To this end, we propose\nWeakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first\nwork to learn deep quantization from weakly tagged images. Specifically, 1) we\nuse word embeddings to represent the tags and enhance their semantic\ninformation based on a tag correlation graph. 2) To better preserve semantic\ninformation in quantization codes and reduce quantization error, we jointly\nlearn semantics-preserving embeddings and supervised quantizer on hypersphere\nby employing a well-designed fusion layer and tailor-made loss functions.\nExtensive experiments show that WSDHQ can achieve state-of-art performance on\nweakly-supervised compact coding. Code is available at\nhttps://github.com/gimpong/AAAI21-WSDHQ.</p>\n", "tags": ["Supervised","Image-Retrieval","Quantization","AAAI","Scalability","Evaluation","Efficiency"] },
{"key": "wang2025deep", "year": "2020", "citations": "9", "title":"Deep Hashing with Active Pairwise Supervision", "abstract": "<p>In this paper, we propose a Deep Hashing method with Active Pairwise Supervision(DH-APS). Conventional methods with passive\npairwise supervision obtain labeled data for training and require large\namount of annotations to reach their full potential, which are not feasible in realistic retrieval tasks. On the contrary, we actively select a small\nquantity of informative samples for annotation to provide effective pairwise supervision so that discriminative hash codes can be obtained with\nlimited annotation budget. Specifically, we generalize the structural risk\nminimization principle and obtain three criteria for the pairwise supervision acquisition: uncertainty, representativeness and diversity. Accordingly, samples involved in the following training pairs should be labeled:\npairs with most uncertain similarity, pairs that minimize the discrepancy\nbetween labeled and unlabeled data, and pairs which are most different\nfrom the annotated data, so that the discriminality and generalization ability of the learned hash codes are significantly strengthened. Moreover,\nour DH-APS can also be employed as a plug-and-play module for semisupervised hashing methods to further enhance the performance. Experiments demonstrate that the presented DH-APS achieves the accuracy\nof supervised hashing methods with only 30% labeled training samples\nand improves the semi-supervised binary codes by a sizable margin.</p>\n", "tags": ["Neural-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "wang2025hamming", "year": "2015", "citations": "20", "title":"Hamming Compatible Quantization for Hashing", "abstract": "<p>Hashing is one of the effective techniques for fast\nApproximate Nearest Neighbour (ANN) search.\nTraditional single-bit quantization (SBQ) in most\nhashing methods incurs lots of quantization error\nwhich seriously degrades the search performance.\nTo address the limitation of SBQ, researchers have\nproposed promising multi-bit quantization (MBQ)\nmethods to quantize each projection dimension\nwith multiple bits. However, some MBQ methods\nneed to adopt specific distance for binary code\nmatching instead of the original Hamming distance,\nwhich would significantly decrease the retrieval\nspeed. Two typical MBQ methods Hierarchical\nQuantization and Double Bit Quantization\nretain the Hamming distance, but both of them only\nconsider the projection dimensions during quantization,\nignoring the neighborhood structure of raw\ndata inherent in Euclidean space. In this paper,\nwe propose a multi-bit quantization method named\nHamming Compatible Quantization (HCQ) to preserve\nthe capability of similarity metric between\nEuclidean space and Hamming space by utilizing\nthe neighborhood structure of raw data. Extensive\nexperiment results have shown our approach significantly\nimproves the performance of various stateof-the-art\nhashing methods while maintaining fast\nretrieval speed.</p>\n", "tags": ["Distance-Metric-Learning","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation","Quantization"] },
{"key": "wang2025online", "year": "2020", "citations": "47", "title":"Online Collective Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval", "abstract": "<p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>\n", "tags": ["Scalability","Efficiency","SIGIR","Hashing-Methods","Evaluation"] },
{"key": "wang2025prototype", "year": "2021", "citations": "43", "title":"Prototype-Supervised Adversarial Network for Targeted Attack of Deep Hashing", "abstract": "<p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","CVPR","Neural-Hashing","Tools-&-Libraries","Hashing-Methods","Evaluation","Supervised","Robustness"] },
{"key": "wang2025semantic", "year": "2015", "citations": "149", "title":"Semantic Topic Multimodal Hashing for Cross-Media Retrieval", "abstract": "<p>Multimodal hashing is essential to cross-media\nsimilarity search for its low storage cost and fast\nquery speed. Most existing multimodal hashing\nmethods embedded heterogeneous data into a common low-dimensional Hamming space, and then\nrounded the continuous embeddings to obtain the\nbinary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For\nthis purpose, a novel Semantic Topic Multimodal\nHashing (STMH) is developed by considering latent semantic information in coding procedure.\nIt\nfirst discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.\nThen the learned multimodal semantic features are\ntransformed into a common subspace by their correlations. Finally, each bit of unified hash code\ncan be generated directly by figuring out whether a\ntopic or concept is contained in a text or an image.\nTherefore, the obtained model by STMH is more\nsuitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method\noutperforms several state-of-the-art methods.</p>\n", "tags": ["Memory-Efficiency","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "wang2025semi", "year": "2010", "citations": "626", "title":"Semi-supervised hashing for scalable image retrieval", "abstract": "<p>Large scale image search has recently attracted considerable\nattention due to easy availability of huge amounts of\ndata. Several hashing methods have been proposed to allow\napproximate but highly efficient search. Unsupervised\nhashing methods show good performance with metric distances\nbut, in image search, semantic similarity is usually\ngiven in terms of labeled pairs of images. There exist supervised\nhashing methods that can handle such semantic similarity\nbut they are prone to overfitting when labeled data\nis small or noisy. Moreover, these methods are usually very\nslow to train. In this work, we propose a semi-supervised\nhashing method that is formulated as minimizing empirical\nerror on the labeled data while maximizing variance\nand independence of hash bits over the labeled and unlabeled\ndata. The proposed method can handle both metric as\nwell as semantic similarity. The experimental results on two\nlarge datasets (up to one million samples) demonstrate its\nsuperior performance over state-of-the-art supervised and\nunsupervised methods.</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Neural-Hashing","Supervised","Hashing-Methods","Evaluation","Unsupervised"] },
{"key": "wang2025sequential", "year": "2010", "citations": "328", "title":"Sequential projection learning for hashing with compact codes", "abstract": "<p>Hashing based Approximate Nearest Neighbor\n(ANN) search has attracted much attention\ndue to its fast query time and drastically\nreduced storage. However, most of the hashing\nmethods either use random projections or\nextract principal directions from the data to\nderive hash functions. The resulting embedding\nsuffers from poor discrimination when\ncompact codes are used. In this paper, we\npropose a novel data-dependent projection\nlearning method such that each hash function\nis designed to correct the errors made by\nthe previous one sequentially. The proposed\nmethod easily adapts to both unsupervised\nand semi-supervised scenarios and shows significant\nperformance gains over the state-ofthe-art\nmethods on two large datasets containing\nup to 1 million points.</p>\n", "tags": ["Efficiency","Unsupervised","Datasets","Locality-Sensitive-Hashing","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "wang2025survey", "year": "2017", "citations": "929", "title":"A Survey on Learning to Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the\nquery point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this\npaper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving\nthe similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different\nthough quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation\nprotocols, and the general performance analysis, and point out that the quantization algori</p>\n", "tags": ["Hashing-Methods","Evaluation","Quantization","Survey-Paper"] },
{"key": "wehrmann2017order", "year": "2017", "citations": "21", "title":"Order embeddings and character-level convolutions for multimodal alignment", "abstract": "<p>With the novel and fast advances in the area of deep neural networks, several\nchallenging image-based tasks have been recently approached by researchers in\npattern recognition and computer vision. In this paper, we address one of these\ntasks, which is to match image content with natural language descriptions,\nsometimes referred as multimodal content retrieval. Such a task is particularly\nchallenging considering that we must find a semantic correspondence between\ncaptions and the respective image, a challenge for both computer vision and\nnatural language processing areas. For such, we propose a novel multimodal\napproach based solely on convolutional neural networks for aligning images with\ntheir captions by directly convolving raw characters. Our proposed\ncharacter-based textual embeddings allow the replacement of both\nword-embeddings and recurrent neural networks for text understanding, saving\nprocessing time and requiring fewer learnable parameters. Our method is based\non the idea of projecting both visual and textual information into a common\nembedding space. For training such embeddings we optimize a contrastive loss\nfunction that is computed to minimize order-violations between images and their\nrespective descriptions. We achieve state-of-the-art performance in the largest\nand most well-known image-text alignment dataset, namely Microsoft COCO, with a\nmethod that is conceptually much simpler and that possesses considerably fewer\nparameters than current approaches.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "wei2016selective", "year": "2017", "citations": "434", "title":"Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval", "abstract": "<p>Deep convolutional neural network models pre-trained for the ImageNet\nclassification task have been successfully adopted to tasks in other domains,\nsuch as texture description and object proposal generation, but these tasks\nrequire annotations for images in the new domain. In this paper, we focus on a\nnovel and challenging task in the pure unsupervised setting: fine-grained image\nretrieval. Even with image labels, fine-grained images are difficult to\nclassify, let alone the unsupervised retrieval task. We propose the Selective\nConvolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes the\nmain object in fine-grained images, a step that discards the noisy background\nand keeps useful deep descriptors. The selected descriptors are then aggregated\nand dimensionality reduced into a short feature vector using the best practices\nwe found. SCDA is unsupervised, using no image label or bounding box\nannotation. Experiments on six fine-grained datasets confirm the effectiveness\nof SCDA for fine-grained image retrieval. Besides, visualization of the SCDA\nfeatures shows that they correspond to visual attributes (even subtle ones),\nwhich might explain SCDA’s high mean average precision in fine-grained\nretrieval. Moreover, on general image retrieval datasets, SCDA achieves\ncomparable retrieval results with state-of-the-art general image retrieval\napproaches.</p>\n", "tags": ["Supervised","Image-Retrieval","Datasets","Unsupervised","Evaluation"] },
{"key": "wei2022accurate", "year": "2022", "citations": "5", "title":"Accurate Instance-Level CAD Model Retrieval in a Large-Scale Database", "abstract": "<p>We present a new solution to the fine-grained retrieval of clean CAD models\nfrom a large-scale database in order to recover detailed object shape\ngeometries for RGBD scans. Unlike previous work simply indexing into a\nmoderately small database using an object shape descriptor and accepting the\ntop retrieval result, we argue that in the case of a large-scale database a\nmore accurate model may be found within a neighborhood of the descriptor. More\nimportantly, we propose that the distinctiveness deficiency of shape\ndescriptors at the instance level can be compensated by a geometry-based\nre-ranking of its neighborhood. Our approach first leverages the discriminative\npower of learned representations to distinguish between different categories of\nmodels and then uses a novel robust point set distance metric to re-rank the\nCAD neighborhood, enabling fine-grained retrieval in a large shape database.\nEvaluation on a real-world dataset shows that our geometry-based re-ranking is\na conceptually simple but highly effective method that can lead to a\nsignificant improvement in retrieval accuracy compared to the state-of-the-art.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Hybrid-ANN-Methods","Re-Ranking","Scalability","Evaluation"] },
{"key": "wei2023attribute", "year": "2023", "citations": "16", "title":"Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale Fine-Grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as\nranking the images depicting the concept of interests (i.e., the same\nsub-category labels) highest based on the fine-grained details in the query. It\nis desirable to alleviate the challenges of both fine-grained nature of small\ninter-class variations with large intra-class variations and explosive growth\nof fine-grained data for such a practical task. In this paper, we propose\nattribute-aware hashing networks with self-consistency for generating\nattribute-aware hash codes to not only make the retrieval process efficient,\nbut also establish explicit correspondences between hash codes and visual\nattributes. Specifically, based on the captured visual representations by\nattention, we develop an encoder-decoder structure network of a reconstruction\ntask to unsupervisedly distill high-level attribute-specific vectors from the\nappearance-specific visual representations without attribute annotations. Our\nmodels are also equipped with a feature decorrelation constraint upon these\nattribute vectors to strengthen their representative abilities. Then, driven by\npreserving original entities’ similarity, the required hash codes can be\ngenerated from these attribute-specific vectors and thus become\nattribute-aware. Furthermore, to combat simplicity bias in deep hashing, we\nconsider the model design from the perspective of the self-consistency\nprinciple and propose to further enhance models’ self-consistency by equipping\nan additional image reconstruction path. Comprehensive quantitative experiments\nunder diverse empirical settings on six fine-grained retrieval datasets and two\ngeneric retrieval datasets show the superiority of our models over competing\nmethods.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Scalability"] },
{"key": "weinzaepfel2022learning", "year": "2022", "citations": "11", "title":"Learning Super-Features for Image Retrieval", "abstract": "<p>Methods that combine local and global features have recently shown excellent\nperformance on multiple challenging deep image retrieval benchmarks, but their\nuse of local features raises at least two issues. First, these local features\nsimply boil down to the localized map activations of a neural network, and\nhence can be extremely redundant. Second, they are typically trained with a\nglobal loss that only acts on top of an aggregation of local features; by\ncontrast, testing is based on local feature matching, which creates a\ndiscrepancy between training and testing. In this paper, we propose a novel\narchitecture for deep image retrieval, based solely on mid-level features that\nwe call Super-features. These Super-features are constructed by an iterative\nattention module and constitute an ordered set in which each element focuses on\na localized and discriminant image pattern. For training, they require only\nimage labels. A contrastive loss operates directly at the level of\nSuper-features and focuses on those that match across images. A second\ncomplementary loss encourages diversity. Experiments on common landmark\nretrieval benchmarks validate that Super-features substantially outperform\nstate-of-the-art methods when using the same number of features, and only\nrequire a significantly smaller memory footprint to match their performance.\nCode and models are available at: https://github.com/naver/FIRe.</p>\n", "tags": ["Memory-Efficiency","Distance-Metric-Learning","Evaluation","Image-Retrieval"] },
{"key": "weiss2008spectral", "year": "2008", "citations": "2154", "title":"Spectral Hashing", "abstract": "<p>Semantic hashing seeks compact binary codes of data-points so that the\nHamming distance between codewords correlates with semantic similarity.\nIn this paper, we show that the problem of finding a best code for a given\ndataset is closely related to the problem of graph partitioning and can\nbe shown to be NP hard. By relaxing the original problem, we obtain a\nspectral method whose solutions are simply a subset of thresholded eigenvectors\nof the graph Laplacian. By utilizing recent results on convergence\nof graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of\nmanifolds, we show how to efficiently calculate the code of a novel datapoint.\nTaken together, both learning the code and applying it to a novel\npoint are extremely simple. Our experiments show that our codes outperform\nthe state-of-the art.</p>\n", "tags": ["Hashing-Methods","Datasets","Compact-Codes","Text-Retrieval"] },
{"key": "weiss2012multidimensional", "year": "2012", "citations": "162", "title":"Multidimensional Spectral Hashing", "abstract": "<p>en a surge of interest in methods based on “semantic hashing”,\ni.e. compact binary codes of data-points so that the Hamming distance\nbetween codewords correlates with similarity. In reviewing and\ncomparing existing methods, we show that their relative performance can\nchange drastically depending on the definition of ground-truth neighbors.\nMotivated by this finding, we propose a new formulation for learning binary\ncodes which seeks to reconstruct the affinity between datapoints,\nrather than their distances. We show that this criterion is intractable\nto solve exactly, but a spectral relaxation gives an algorithm where the\nbits correspond to thresholded eigenvectors of the affinity matrix, and\nas the number of datapoints goes to infinity these eigenvectors converge\nto eigenfunctions of Laplace-Beltrami operators, similar to the recently\nproposed Spectral Hashing (SH) method. Unlike SH whose performance\nmay degrade as the number of bits increases, the optimal code using\nour formulation is guaranteed to faithfully reproduce the affinities as\nthe number of bits increases. We show that the number of eigenfunctions\nneeded may increase exponentially with dimension, but introduce a “kernel\ntrick” to allow us to compute with an exponentially large number of\nbits but using only memory and computation that grows linearly with\ndimension. Experiments shows that MDSH outperforms the state-of-the\nart, especially in the challenging regime of small distance thresholds.</p>\n", "tags": ["Hashing-Methods","Evaluation","Compact-Codes","Text-Retrieval"] },
{"key": "weiss2025multidimensional", "year": "2012", "citations": "162", "title":"Multidimensional Spectral Hashing", "abstract": "<p>en a surge of interest in methods based on “semantic hashing”,\ni.e. compact binary codes of data-points so that the Hamming distance\nbetween codewords correlates with similarity. In reviewing and\ncomparing existing methods, we show that their relative performance can\nchange drastically depending on the definition of ground-truth neighbors.\nMotivated by this finding, we propose a new formulation for learning binary\ncodes which seeks to reconstruct the affinity between datapoints,\nrather than their distances. We show that this criterion is intractable\nto solve exactly, but a spectral relaxation gives an algorithm where the\nbits correspond to thresholded eigenvectors of the affinity matrix, and\nas the number of datapoints goes to infinity these eigenvectors converge\nto eigenfunctions of Laplace-Beltrami operators, similar to the recently\nproposed Spectral Hashing (SH) method. Unlike SH whose performance\nmay degrade as the number of bits increases, the optimal code using\nour formulation is guaranteed to faithfully reproduce the affinities as\nthe number of bits increases. We show that the number of eigenfunctions\nneeded may increase exponentially with dimension, but introduce a “kernel\ntrick” to allow us to compute with an exponentially large number of\nbits but using only memory and computation that grows linearly with\ndimension. Experiments shows that MDSH outperforms the state-of-the\nart, especially in the challenging regime of small distance thresholds.</p>\n", "tags": ["Hashing-Methods","Evaluation","Compact-Codes","Text-Retrieval"] },
{"key": "weiss2025spectral", "year": "2008", "citations": "2154", "title":"Spectral Hashing", "abstract": "<p>Semantic hashing seeks compact binary codes of data-points so that the\nHamming distance between codewords correlates with semantic similarity.\nIn this paper, we show that the problem of finding a best code for a given\ndataset is closely related to the problem of graph partitioning and can\nbe shown to be NP hard. By relaxing the original problem, we obtain a\nspectral method whose solutions are simply a subset of thresholded eigenvectors\nof the graph Laplacian. By utilizing recent results on convergence\nof graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of\nmanifolds, we show how to efficiently calculate the code of a novel datapoint.\nTaken together, both learning the code and applying it to a novel\npoint are extremely simple. Our experiments show that our codes outperform\nthe state-of-the art.</p>\n", "tags": ["Hashing-Methods","Datasets","Compact-Codes","Text-Retrieval"] },
{"key": "weissman2014identifying", "year": "2015", "citations": "10", "title":"Identifying Duplicate and Contradictory Information in Wikipedia", "abstract": "<p>Our study identifies sentences in Wikipedia articles that are either\nidentical or highly similar by applying techniques for near-duplicate detection\nof web pages. This is accomplished with a MapReduce implementation of minhash\nto identify clusters of sentences with high Jaccard similarity. We show that\nthese clusters can be categorized into six different types, two of which are\nparticularly interesting: identical sentences quantify the extent to which\ncontent in Wikipedia is copied and pasted, and near-duplicate sentences that\nstate contradictory facts point to quality issues in Wikipedia.</p>\n", "tags": ["Locality-Sensitive-Hashing"] },
{"key": "wen2019adversarial", "year": "2019", "citations": "18", "title":"Adversarial Cross-Modal Retrieval via Learning and Transferring Single-Modal Similarities", "abstract": "<p>Cross-modal retrieval aims to retrieve relevant data across different\nmodalities (e.g., texts vs. images). The common strategy is to apply\nelement-wise constraints between manually labeled pair-wise items to guide the\ngenerators to learn the semantic relationships between the modalities, so that\nthe similar items can be projected close to each other in the common\nrepresentation subspace. However, such constraints often fail to preserve the\nsemantic structure between unpaired but semantically similar items (e.g. the\nunpaired items with the same class label are more similar than items with\ndifferent labels). To address the above problem, we propose a novel cross-modal\nsimilarity transferring (CMST) method to learn and preserve the semantic\nrelationships between unpaired items in an unsupervised way. The key idea is to\nlearn the quantitative similarities in single-modal representation subspace,\nand then transfer them to the common representation subspace to establish the\nsemantic relationships between unpaired items across modalities. Experiments\nshow that our method outperforms the state-of-the-art approaches both in the\nclass-based and pair-based retrieval tasks.</p>\n", "tags": ["Unsupervised","Multimodal-Retrieval","Robustness"] },
{"key": "weng2019online", "year": "2020", "citations": "15", "title":"Online Hashing with Efficient Updating of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the\nstreaming data. However, when the hash functions change, the binary codes for\nthe database have to be recomputed to guarantee the retrieval accuracy.\nRecomputing the binary codes by accumulating the whole database brings a\ntimeliness challenge to the online retrieval process. In this paper, we propose\na novel online hashing framework to update the binary codes efficiently without\naccumulating the whole database. In our framework, the hash functions are fixed\nand the projection functions are introduced to learn online from the streaming\ndata. Therefore, inefficient updating of the binary codes by accumulating the\nwhole database can be transformed to efficient updating of the binary codes by\nprojecting the binary codes into another binary space. The queries and the\nbinary code database are projected asymmetrically to further improve the\nretrieval accuracy. The experiments on two multi-label image databases\ndemonstrate the effectiveness and the efficiency of our method for multi-label\nimage retrieval.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Compact-Codes","AAAI","Efficiency"] },
{"key": "weng2020online", "year": "2020", "citations": "15", "title":"Online Hashing with Efficient Updating of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>\n", "tags": ["Image-Retrieval","Efficiency","Tools-&-Libraries","Compact-Codes","Hashing-Methods","AAAI"] },
{"key": "weng2025online", "year": "2020", "citations": "15", "title":"Online Hashing with Efficient Updating of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>\n", "tags": ["Image-Retrieval","Efficiency","Tools-&-Libraries","Compact-Codes","Hashing-Methods","AAAI"] },
{"key": "westermann2021sentence", "year": "2020", "citations": "22", "title":"Sentence Embeddings and High-speed Similarity Search for Fast Computer Assisted Annotation of Legal Documents", "abstract": "<p>Human-performed annotation of sentences in legal documents is an important\nprerequisite to many machine learning based systems supporting legal tasks.\nTypically, the annotation is done sequentially, sentence by sentence, which is\noften time consuming and, hence, expensive. In this paper, we introduce a\nproof-of-concept system for annotating sentences “laterally.” The approach is\nbased on the observation that sentences that are similar in meaning often have\nthe same label in terms of a particular type system. We use this observation in\nallowing annotators to quickly view and annotate sentences that are\nsemantically similar to a given sentence, across an entire corpus of documents.\nHere, we present the interface of the system and empirically evaluate the\napproach. The experiments show that lateral annotation has the potential to\nmake the annotation process quicker and more consistent.</p>\n", "tags": ["Similarity-Search"] },
{"key": "weyand2020google", "year": "2020", "citations": "235", "title":"Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "abstract": "<p>While image retrieval and instance recognition techniques are progressing\nrapidly, there is a need for challenging datasets to accurately measure their\nperformance – while posing novel challenges that are relevant for practical\napplications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new\nbenchmark for large-scale, fine-grained instance recognition and image\nretrieval in the domain of human-made and natural landmarks. GLDv2 is the\nlargest such dataset to date by a large margin, including over 5M images and\n200k distinct instance labels. Its test set consists of 118k images with ground\ntruth annotations for both the retrieval and recognition tasks. The ground\ntruth construction involved over 800 hours of human annotator work. Our new\ndataset has several challenging properties inspired by real world applications\nthat previous datasets did not consider: An extremely long-tailed class\ndistribution, a large fraction of out-of-domain test photos and large\nintra-class variability. The dataset is sourced from Wikimedia Commons, the\nworld’s largest crowdsourced collection of landmark photos. We provide baseline\nresults for both recognition and retrieval tasks based on state-of-the-art\nmethods as well as competitive results from a public challenge. We further\ndemonstrate the suitability of the dataset for transfer learning by showing\nthat image embeddings trained on it achieve competitive retrieval performance\non independent datasets. The dataset images, ground-truth and metric scoring\ncode are available at https://github.com/cvdfoundation/google-landmark.</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Scalability","Evaluation"] },
{"key": "wieczorek2021unreasonable", "year": "2021", "citations": "84", "title":"On the Unreasonable Effectiveness of Centroids in Image Retrieval", "abstract": "<p>Image retrieval task consists of finding similar images to a query image from\na set of gallery (database) images. Such systems are used in various\napplications e.g. person re-identification (ReID) or visual product search.\nDespite active development of retrieval models it still remains a challenging\ntask mainly due to large intra-class variance caused by changes in view angle,\nlighting, background clutter or occlusion, while inter-class variance may be\nrelatively low. A large portion of current research focuses on creating more\nrobust features and modifying objective functions, usually based on Triplet\nLoss. Some works experiment with using centroid/proxy representation of a class\nto alleviate problems with computing speed and hard samples mining used with\nTriplet Loss. However, these approaches are used for training alone and\ndiscarded during the retrieval stage. In this paper we propose to use the mean\ncentroid representation both during training and retrieval. Such an aggregated\nrepresentation is more robust to outliers and assures more stable features. As\neach class is represented by a single embedding - the class centroid - both\nretrieval time and storage requirements are reduced significantly. Aggregating\nmultiple embeddings results in a significant reduction of the search space due\nto lowering the number of candidate target vectors, which makes the method\nespecially suitable for production deployments. Comprehensive experiments\nconducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness\nof our method, which outperforms the current state-of-the-art. We propose\ncentroid training and retrieval as a viable method for both Fashion Retrieval\nand ReID applications.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Image-Retrieval"] },
{"key": "wieschollek2017efficient", "year": "2016", "citations": "55", "title":"Efficient Large-scale Approximate Nearest Neighbor Search on the GPU", "abstract": "<p>We present a new approach for efficient approximate nearest neighbor (ANN)\nsearch in high dimensional spaces, extending the idea of Product Quantization.\nWe propose a two-level product and vector quantization tree that reduces the\nnumber of vector comparisons required during tree traversal. Our approach also\nincludes a novel highly parallelizable re-ranking method for candidate vectors\nby efficiently reusing already computed intermediate values. Due to its small\nmemory footprint during traversal, the method lends itself to an efficient,\nparallel GPU implementation. This Product Quantization Tree (PQT) approach\nsignificantly outperforms recent state of the art methods for high dimensional\nnearest neighbor queries on standard reference datasets. Ours is the first work\nthat demonstrates GPU performance superior to CPU performance on high\ndimensional, large scale ANN problems in time-critical real-world applications,\nlike loop-closing in videos.</p>\n", "tags": ["Datasets","Quantization","Memory-Efficiency","CVPR","Hybrid-ANN-Methods","Re-Ranking","Scalability","Evaluation"] },
{"key": "wiggers2019image", "year": "2019", "citations": "37", "title":"Image Retrieval and Pattern Spotting using Siamese Neural Network", "abstract": "<p>This paper presents a novel approach for image retrieval and pattern spotting\nin document image collections. The manual feature engineering is avoided by\nlearning a similarity-based representation using a Siamese Neural Network\ntrained on a previously prepared subset of image pairs from the ImageNet\ndataset. The learned representation is used to provide the similarity-based\nfeature maps used to find relevant image candidates in the data collection\ngiven an image query. A robust experimental protocol based on the public\nTobacco800 document image collection shows that the proposed method compares\nfavorably against state-of-the-art document image retrieval methods, reaching\n0.94 and 0.83 of mean average precision (mAP) for retrieval and pattern\nspotting (IoU=0.7), respectively. Besides, we have evaluated the proposed\nmethod considering feature maps of different sizes, showing the impact of\nreducing the number of features in the retrieval performance and\ntime-consuming.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "won2020multimodal", "year": "2021", "citations": "28", "title":"Multimodal Metric Learning for Tag-based Music Retrieval", "abstract": "<p>Tag-based music retrieval is crucial to browse large-scale music libraries\nefficiently. Hence, automatic music tagging has been actively explored, mostly\nas a classification task, which has an inherent limitation: a fixed vocabulary.\nOn the other hand, metric learning enables flexible vocabularies by using\npretrained word embeddings as side information. Also, metric learning has\nalready proven its suitability for cross-modal retrieval tasks in other domains\n(e.g., text-to-image) by jointly learning a multimodal embedding space. In this\npaper, we investigate three ideas to successfully introduce multimodal metric\nlearning for tag-based music retrieval: elaborate triplet sampling, acoustic\nand cultural music information, and domain-specific word embeddings. Our\nexperimental results show that the proposed ideas enhance the retrieval system\nquantitatively, and qualitatively. Furthermore, we release the MSD500, a subset\nof the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually\nannotated tag categories, and user taste profiles.</p>\n", "tags": ["ICASSP","Distance-Metric-Learning","Datasets","Scalability","Multimodal-Retrieval"] },
{"key": "woodbridge2018detecting", "year": "2018", "citations": "34", "title":"Detecting Homoglyph Attacks with a Siamese Neural Network", "abstract": "<p>A homoglyph (name spoofing) attack is a common technique used by adversaries\nto obfuscate file and domain names. This technique creates process or domain\nnames that are visually similar to legitimate and recognized names. For\ninstance, an attacker may create malware with the name svch0st.exe so that in a\nvisual inspection of running processes or a directory listing, the process or\nfile name might be mistaken as the Windows system process svchost.exe. There\nhas been limited published research on detecting homoglyph attacks. Current\napproaches rely on string comparison algorithms (such as Levenshtein distance)\nthat result in computationally heavy solutions with a high number of false\npositives. In addition, there is a deficiency in the number of publicly\navailable datasets for reproducible research, with most datasets focused on\nphishing attacks, in which homoglyphs are not always used. This paper presents\na fundamentally different solution to this problem using a Siamese\nconvolutional neural network (CNN). Rather than leveraging similarity based on\ncharacter swaps and deletions, this technique uses a learned metric on strings\nrendered as images: a CNN learns features that are optimized to detect visual\nsimilarity of the rendered strings. The trained model is used to convert\nthousands of potentially targeted process or domain names to feature vectors.\nThese feature vectors are indexed using randomized KD-Trees to make similarity\nsearches extremely fast with minimal computational processing. This technique\nshows a considerable 13% to 45% improvement over baseline techniques in terms\nof area under the receiver operating characteristic curve (ROC AUC). In\naddition, we provide both code and data to further future research.</p>\n", "tags": ["Datasets","Tree-Based-ANN","Evaluation"] },
{"key": "wray2019fine", "year": "2019", "citations": "130", "title":"Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings", "abstract": "<p>We address the problem of cross-modal fine-grained action retrieval between\ntext and video. Cross-modal retrieval is commonly achieved through learning a\nshared embedding space, that can indifferently embed modalities. In this paper,\nwe propose to enrich the embedding by disentangling parts-of-speech (PoS) in\nthe accompanying captions. We build a separate multi-modal embedding space for\neach PoS tag. The outputs of multiple PoS embeddings are then used as input to\nan integrated multi-modal space, where we perform action retrieval. All\nembeddings are trained jointly through a combination of PoS-aware and\nPoS-agnostic losses. Our proposal enables learning specialised embedding spaces\nthat offer multiple views of the same embedded entities.\n  We report the first retrieval results on fine-grained actions for the\nlarge-scale EPIC dataset, in a generalised zero-shot setting. Results show the\nadvantage of our approach for both video-to-text and text-to-video action\nretrieval. We also demonstrate the benefit of disentangling the PoS for the\ngeneric task of cross-modal video retrieval on the MSR-VTT dataset.</p>\n", "tags": ["ICCV","Few-Shot-&-Zero-Shot","Datasets","Video-Retrieval","Scalability","Multimodal-Retrieval"] },
{"key": "wu2017deep", "year": "2017", "citations": "67", "title":"Deep Incremental Hashing Network for Efficient Image Retrieval", "abstract": "<p>Hashing has shown great potential in large-scale image retrieval due to its storage and computation efficiency, especially the recent deep supervised hashing methods. To achieve promising performance, deep supervised hashing methods require a large amount of training data from different classes. However, when images of new categories emerge, existing deep hashing methods have to retrain the CNN model and generate hash codes for all the database images again, which is impractical for large-scale retrieval system.\nIn this paper, we propose a novel deep hashing framework, called Deep Incremental Hashing Network (DIHN), for learning hash codes in an incremental manner. DIHN learns the hash codes for the new coming images directly, while keeping the old ones unchanged. Simultaneously, a deep hash function for query set is learned by preserving the similarities between training points. Extensive experiments on two widely used image retrieval benchmarks demonstrate that the proposed DIHN framework can significantly decrease the training time while keeping the state-of-the-art retrieval accuracy.</p>\n", "tags": ["Scalability","Efficiency","Evaluation","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","Multimodal-Retrieval","Image-Retrieval","Supervised"] },
{"key": "wu2017sampling", "year": "2017", "citations": "863", "title":"Sampling Matters in Deep Embedding Learning", "abstract": "<p>Deep embeddings answer one simple question: How similar are two images?\nLearning these embeddings is the bedrock of verification, zero-shot learning,\nand visual search. The most prominent approaches optimize a deep convolutional\nnetwork with a suitable loss function, such as contrastive loss or triplet\nloss. While a rich line of work focuses solely on the loss functions, we show\nin this paper that selecting training examples plays an equally important role.\nWe propose distance weighted sampling, which selects more informative and\nstable examples than traditional approaches. In addition, we show that a simple\nmargin based loss is sufficient to outperform all other loss functions. We\nevaluate our approach on the Stanford Online Products, CAR196, and the\nCUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset\nfor face verification. Our method achieves state-of-the-art performance on all\nof them.</p>\n", "tags": ["ICCV","Few-Shot-&-Zero-Shot","Image-Retrieval","Distance-Metric-Learning","Datasets","Evaluation"] },
{"key": "wu2017structured", "year": "2017", "citations": "71", "title":"Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification", "abstract": "<p>Given a pedestrian image as a query, the purpose of person re-identification\nis to identify the correct match from a large collection of gallery images\ndepicting the same person captured by disjoint camera views. The critical\nchallenge is how to construct a robust yet discriminative feature\nrepresentation to capture the compounded variations in pedestrian appearance.\nTo this end, deep learning methods have been proposed to extract hierarchical\nfeatures against extreme variability of appearance. However, existing methods\nin this category generally neglect the efficiency in the matching stage whereas\nthe searching speed of a re-identification system is crucial in real-world\napplications. In this paper, we present a novel deep hashing framework with\nConvolutional Neural Networks (CNNs) for fast person re-identification.\nTechnically, we simultaneously learn both CNN features and hash functions/codes\nto get robust yet discriminative features and similarity-preserving hash codes.\nThereby, person re-identification can be resolved by efficiently computing and\nranking the Hamming distances between images. A structured loss function\ndefined over positive pairs and hard negatives is proposed to formulate a novel\noptimization problem so that fast convergence and more stable optimized\nsolution can be obtained. Extensive experiments on two benchmarks CUHK03\n\\cite{FPNN} and Market-1501 \\cite{Market1501} show that the proposed deep\narchitecture is efficacy over state-of-the-arts.</p>\n", "tags": ["Hashing-Methods","Neural-Hashing","Tools-&-Libraries","Efficiency"] },
{"key": "wu2018cycle", "year": "2018", "citations": "189", "title":"Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval", "abstract": "<p>In this paper, we propose a novel deep generative approach to cross-modal\nretrieval to learn hash functions in the absence of paired training samples\nthrough the cycle consistency loss. Our proposed approach employs adversarial\ntraining scheme to lean a couple of hash functions enabling translation between\nmodalities while assuming the underlying semantic relationship. To induce the\nhash codes with semantics to the input-output pair, cycle consistency loss is\nfurther proposed upon the adversarial training to strengthen the correlations\nbetween inputs and corresponding outputs. Our approach is generative to learn\nhash functions such that the learned hash codes can maximally correlate each\ninput-output correspondence, meanwhile can also regenerate the inputs so as to\nminimize the information loss. The learning to hash embedding is thus performed\nto jointly optimize the parameters of the hash functions across modalities as\nwell as the associated generative models. Extensive experiments on a variety of\nlarge-scale cross-modal data sets demonstrate that our proposed method achieves\nbetter retrieval results than the state-of-the-arts.</p>\n", "tags": ["Hashing-Methods","Scalability","Multimodal-Retrieval","Robustness"] },
{"key": "wu2018local", "year": "2018", "citations": "7", "title":"Local Density Estimation in High Dimensions", "abstract": "<p>An important question that arises in the study of high dimensional vector\nrepresentations learned from data is: given a set \\(\\mathcal{D}\\) of vectors and\na query \\(q\\), estimate the number of points within a specified distance\nthreshold of \\(q\\). We develop two estimators, LSH Count and Multi-Probe Count\nthat use locality sensitive hashing to preprocess the data to accurately and\nefficiently estimate the answers to such questions via importance sampling. A\nkey innovation is the ability to maintain a small number of hash tables via\npreprocessing data structures and algorithms that sample from multiple buckets\nin each hash table. We give bounds on the space requirements and sample\ncomplexity of our schemes, and demonstrate their effectiveness in experiments\non a standard word embedding dataset.</p>\n", "tags": ["Hashing-Methods","Datasets","Locality-Sensitive-Hashing"] },
{"key": "wu2018review", "year": "2020", "citations": "31", "title":"A Review for Weighted MinHash Algorithms", "abstract": "<p>Data similarity (or distance) computation is a fundamental research topic\nwhich underpins many high-level applications based on similarity measures in\nmachine learning and data mining. However, in large-scale real-world scenarios,\nthe exact similarity computation has become daunting due to “3V” nature\n(volume, velocity and variety) of big data. In such cases, the hashing\ntechniques have been verified to efficiently conduct similarity estimation in\nterms of both theory and practice. Currently, MinHash is a popular technique\nfor efficiently estimating the Jaccard similarity of binary sets and\nfurthermore, weighted MinHash is generalized to estimate the generalized\nJaccard similarity of weighted sets. This review focuses on categorizing and\ndiscussing the existing works of weighted MinHash algorithms. In this review,\nwe mainly categorize the Weighted MinHash algorithms into quantization-based\napproaches, “active index”-based ones and others, and show the evolution and\ninherent connection of the weighted MinHash algorithms, from the integer\nweighted MinHash algorithms to real-valued weighted MinHash ones (particularly\nthe Consistent Weighted Sampling scheme). Also, we have developed a python\ntoolbox for the algorithms, and released it in our github. Based on the\ntoolbox, we experimentally conduct a comprehensive comparative study of the\nstandard MinHash algorithm and the weighted MinHash ones.</p>\n", "tags": ["Locality-Sensitive-Hashing","Hashing-Methods","Quantization","Scalability","Survey-Paper"] },
{"key": "wu2019scalable", "year": "2020", "citations": "290", "title":"Scalable Zero-shot Entity Linking with Dense Entity Retrieval", "abstract": "<p>This paper introduces a conceptually simple, scalable, and highly effective\nBERT-based entity linking model, along with an extensive evaluation of its\naccuracy-speed trade-off. We present a two-stage zero-shot linking algorithm,\nwhere each entity is defined only by a short textual description. The first\nstage does retrieval in a dense space defined by a bi-encoder that\nindependently embeds the mention context and the entity descriptions. Each\ncandidate is then re-ranked with a cross-encoder, that concatenates the mention\nand entity text. Experiments demonstrate that this approach is state of the art\non recent zero-shot benchmarks (6 point absolute gains) and also on more\nestablished non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative\nsimplicity (e.g. no explicit entity embeddings or manually engineered mention\ntables). We also show that bi-encoder linking is very fast with nearest\nneighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds),\nand that much of the accuracy gain from the more expensive cross-encoder can be\ntransferred to the bi-encoder via knowledge distillation. Our code and models\nare available at https://github.com/facebookresearch/BLINK.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","EMNLP","Evaluation"] },
{"key": "wu2021hashing", "year": "2021", "citations": "30", "title":"Hashing-Accelerated Graph Neural Networks for Link Prediction", "abstract": "<p>Networks are ubiquitous in the real world. Link prediction, as one of the key\nproblems for network-structured data, aims to predict whether there exists a\nlink between two nodes. The traditional approaches are based on the explicit\nsimilarity computation between the compact node representation by embedding\neach node into a low-dimensional space. In order to efficiently handle the\nintensive similarity computation in link prediction, the hashing technique has\nbeen successfully used to produce the node representation in the Hamming space.\nHowever, the hashing-based link prediction algorithms face accuracy loss from\nthe randomized hashing techniques or inefficiency from the learning to hash\ntechniques in the embedding process. Currently, the Graph Neural Network (GNN)\nframework has been widely applied to the graph-related tasks in an end-to-end\nmanner, but it commonly requires substantial computational resources and memory\ncosts due to massive parameter learning, which makes the GNN-based algorithms\nimpractical without the help of a powerful workhorse. In this paper, we propose\na simple and effective model called #GNN, which balances the trade-off between\naccuracy and efficiency. #GNN is able to efficiently acquire node\nrepresentation in the Hamming space for link prediction by exploiting the\nrandomized hashing technique to implement message passing and capture\nhigh-order proximity in the GNN framework. Furthermore, we characterize the\ndiscriminative power of #GNN in probability. The extensive experimental results\ndemonstrate that the proposed #GNN algorithm achieves accuracy comparable to\nthe learning-based algorithms and outperforms the randomized algorithm, while\nrunning significantly faster than the learning-based algorithms. Also, the\nproposed algorithm shows excellent scalability on a large-scale network with\nthe limited resources.</p>\n", "tags": ["Hashing-Methods","Scalability","Tools-&-Libraries","Efficiency"] },
{"key": "wu2022learning", "year": "2022", "citations": "6", "title":"Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus", "abstract": "<p>Semantic code search is the task of retrieving relevant code snippet given a\nnatural language query. Different from typical information retrieval tasks,\ncode search requires to bridge the semantic gap between the programming\nlanguage and natural language, for better describing intrinsic concepts and\nsemantics. Recently, deep neural network for code search has been a hot\nresearch topic. Typical methods for neural code search first represent the code\nsnippet and query text as separate embeddings, and then use vector distance\n(e.g. dot-product or cosine) to calculate the semantic similarity between them.\nThere exist many different ways for aggregating the variable length of code or\nquery tokens into a learnable embedding, including bi-encoder, cross-encoder,\nand poly-encoder. The goal of the query encoder and code encoder is to produce\nembeddings that are close with each other for a related pair of query and the\ncorresponding desired code snippet, in which the choice and design of encoder\nis very significant.\n  In this paper, we propose a novel deep semantic model which makes use of the\nutilities of not only the multi-modal sources, but also feature extractors such\nas self-attention, the aggregated vectors, combination of the intermediate\nrepresentations. We apply the proposed model to tackle the CodeSearchNet\nchallenge about semantic code search. We align cross-lingual embedding for\nmulti-modality learning with large batches and hard example mining, and combine\ndifferent learned representations for better enhancing the representation\nlearning. Our model is trained on CodeSearchNet corpus and evaluated on the\nheld-out data, the final model achieves 0.384 NDCG and won the first place in\nthis benchmark. Models and code are available at\nhttps://github.com/overwindows/SemanticCodeSearch.git.</p>\n", "tags": ["Evaluation"] },
{"key": "wu2024interpretable", "year": "2020", "citations": "30", "title":"Interpretable Embedding for Ad-hoc Video Search", "abstract": "<p>Answering query with semantic concepts has long been the mainstream approach\nfor video search. Until recently, its performance is surpassed by concept-free\napproach, which embeds queries in a joint space as videos. Nevertheless, the\nembedded features as well as search results are not interpretable, hindering\nsubsequent steps in video browsing and query reformulation. This paper\nintegrates feature embedding and concept interpretation into a neural network\nfor unified dual-task learning. In this way, an embedding is associated with a\nlist of semantic concepts as an interpretation of video content. This paper\nempirically demonstrates that, by using either the embedding features or\nconcepts, considerable search improvement is attainable on TRECVid benchmarked\ndatasets. Concepts are not only effective in pruning false positive videos, but\nalso highly complementary to concept-free search, leading to large margin of\nimprovement compared to state-of-the-art approaches.</p>\n", "tags": ["Datasets","Video-Retrieval","Evaluation"] },
{"key": "wu2025deep", "year": "2017", "citations": "67", "title":"Deep Supervised Hashing for Multi-Label and Large-Scale Image Retrieval", "abstract": "<p>One of the most challenging tasks in large-scale multi-label image retrieval is to map images into binary codes while preserving multilevel semantic similarity. Recently, several deep supervised hashing methods have been proposed to learn hash functions that preserve multilevel semantic similarity with deep convolutional neural networks. However, these triplet label based methods try to preserve the ranking order of images according to their similarity degrees to the queries while not putting direct constraints on the distance between the codes of very similar images. Besides, the current evaluation criteria are not able to measure the performance of existing hashing methods on preserving fine-grained multilevel semantic similarity. To tackle these issues, we propose a novel Deep Multilevel Semantic Similarity Preserving Hashing (DMSSPH) method to learn compact similarity-preserving binary codes for the huge body of multi-label image data with deep convolutional neural networks. In our approach, we make the best of the supervised information in the form of pairwise labels to maximize the discriminability of output binary codes. Extensive evaluations conducted on several benchmark datasets demonstrate that the proposed method significantly outperforms the state-of-the-art supervised and unsupervised hashing methods at the accuracies of top returned images, especially for shorter binary codes. Meanwhile, the proposed method shows better performance on preserving fine-grained multilevel semantic similarity according to the results under the Jaccard coefficient based evaluation criteria we propose.</p>\n", "tags": ["Scalability","Evaluation","Datasets","Unsupervised","Compact-Codes","Hashing-Methods","Multimodal-Retrieval","Neural-Hashing","Image-Retrieval","Supervised"] },
{"key": "xia2016unsupervised", "year": "2016", "citations": "17", "title":"Unsupervised Deep Hashing for Large-scale Visual Search", "abstract": "<p>Learning based hashing plays a pivotal role in large-scale visual search.\nHowever, most existing hashing algorithms tend to learn shallow models that do\nnot seek representative binary codes. In this paper, we propose a novel hashing\napproach based on unsupervised deep learning to hierarchically transform\nfeatures into hash codes. Within the heterogeneous deep hashing framework, the\nautoencoder layers with specific constraints are considered to model the\nnonlinear mapping between features and binary codes. Then, a Restricted\nBoltzmann Machine (RBM) layer with constraints is utilized to reduce the\ndimension in the hamming space. Extensive experiments on the problem of visual\nsearch demonstrate the competitiveness of our proposed approach compared to\nstate-of-the-art.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Neural-Hashing","Compact-Codes","Unsupervised","Scalability"] },
{"key": "xiao2020deeply", "year": "2022", "citations": "5", "title":"Deeply Activated Salient Region for Instance Search", "abstract": "<p>The performance of instance search depends heavily on the ability to locate\nand describe a wide variety of object instances in a video/image collection.\nDue to the lack of proper mechanism in locating instances and deriving feature\nrepresentation, instance search is generally only effective for retrieving\ninstances of known object categories. In this paper, a simple but effective\ninstance-level feature representation is presented. Different from other\napproaches, the issues in class-agnostic instance localization and distinctive\nfeature representation are considered. The former is achieved by detecting\nsalient instance regions from an image by a layer-wise back-propagation\nprocess. The back-propagation starts from the last convolution layer of a\npre-trained CNN that is originally used for classification. The\nback-propagation proceeds layer-by-layer until it reaches the input layer. This\nallows the salient instance regions in the input image from both known and\nunknown categories to be activated. Each activated salient region covers the\nfull or more usually a major range of an instance. The distinctive feature\nrepresentation is produced by average-pooling on the feature map of certain\nlayer with the detected instance region. Experiments show that such kind of\nfeature representation demonstrates considerably better performance over most\nof the existing approaches. In addition, we show that the proposed feature\ndescriptor is also suitable for content-based image search.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "xiao2021neural", "year": "2021", "citations": "7", "title":"Neural PathSim for Inductive Similarity Search in Heterogeneous Information Networks", "abstract": "<p>PathSim is a widely used meta-path-based similarity in heterogeneous\ninformation networks. Numerous applications rely on the computation of PathSim,\nincluding similarity search and clustering. Computing PathSim scores on large\ngraphs is computationally challenging due to its high time and storage\ncomplexity. In this paper, we propose to transform the problem of approximating\nthe ground truth PathSim scores into a learning problem. We design an\nencoder-decoder based framework, NeuPath, where the algorithmic structure of\nPathSim is considered. Specifically, the encoder module identifies Top T\noptimized path instances, which can approximate the ground truth PathSim, and\nmaps each path instance to an embedding vector. The decoder transforms each\nembedding vector into a scalar respectively, which identifies the similarity\nscore. We perform extensive experiments on two real-world datasets in different\ndomains, ACM and IMDB. Our results demonstrate that NeuPath performs better\nthan state-of-the-art baselines in the PathSim approximation task and\nsimilarity search task.</p>\n", "tags": ["Similarity-Search","CIKM","Tools-&-Libraries","Datasets"] },
{"key": "xiao2022deeply", "year": "2022", "citations": "5", "title":"Deeply Activated Salient Region for Instance Search", "abstract": "<p>The performance of instance search depends heavily on the ability to locate\nand describe a wide variety of object instances in a video/image collection.\nDue to the lack of proper mechanism in locating instances and deriving feature\nrepresentation, instance search is generally only effective for retrieving\ninstances of known object categories. In this paper, a simple but effective\ninstance-level feature representation is presented. Different from other\napproaches, the issues in class-agnostic instance localization and distinctive\nfeature representation are considered. The former is achieved by detecting\nsalient instance regions from an image by a layer-wise back-propagation\nprocess. The back-propagation starts from the last convolution layer of a\npre-trained CNN that is originally used for classification. The\nback-propagation proceeds layer-by-layer until it reaches the input layer. This\nallows the salient instance regions in the input image from both known and\nunknown categories to be activated. Each activated salient region covers the\nfull or more usually a major range of an instance. The distinctive feature\nrepresentation is produced by average-pooling on the feature map of certain\nlayer with the detected instance region. Experiments show that such kind of\nfeature representation demonstrates considerably better performance over most\nof the existing approaches. In addition, we show that the proposed feature\ndescriptor is also suitable for content-based image search.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "xiao2022progressively", "year": "2022", "citations": "14", "title":"Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval", "abstract": "<p>Ad-hoc search calls for the selection of appropriate answers from a\nmassive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a\npromising solution, where deep learning based document representation and ANN\nsearch techniques are allied to handle this task. However, a major challenge is\nthat the ANN index can be too large to fit into memory, given the considerable\nsize of answer corpus. In this work, we tackle this problem with Bi-Granular\nDocument Representation, where the lightweight sparse embeddings are indexed\nand standby in memory for coarse-grained candidate search, and the heavyweight\ndense embeddings are hosted in disk for fine-grained post verification. For the\nbest of retrieval accuracy, a Progressive Optimization framework is designed.\nThe sparse embeddings are learned ahead for high-quality search of candidates.\nConditioned on the candidate distribution induced by the sparse embeddings, the\ndense embeddings are continuously learned to optimize the discrimination of\nground-truth from the shortlisted candidates. Besides, two techniques: the\ncontrastive quantization and the locality-centric sampling are introduced for\nthe learning of sparse and dense embeddings, which substantially contribute to\ntheir performances. Thanks to the above features, our method effectively\nhandles massive-scale EBR with strong advantages in accuracy: with up to +4.3%\nrecall gain on million-scale corpus, and up to +17.5% recall gain on\nbillion-scale corpus. Besides, Our method is applied to a major sponsored\nsearch platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and\nCTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.</p>\n", "tags": ["Vector-Indexing","Tools-&-Libraries","Quantization","Scalability","Large-Scale-Search","Evaluation"] },
{"key": "xie2018unsupervised", "year": "2018", "citations": "27", "title":"Unsupervised User Identity Linkage via Factoid Embedding", "abstract": "<p>User identity linkage (UIL), the problem of matching user account across\nmultiple online social networks (OSNs), is widely studied and important to many\nreal-world applications. Most existing UIL solutions adopt a supervised or\nsemi-supervised approach which generally suffer from scarcity of labeled data.\nIn this paper, we propose Factoid Embedding, a novel framework that adopts an\nunsupervised approach. It is designed to cope with different profile\nattributes, content types and network links of different OSNs. The key idea is\nthat each piece of information about a user identity describes the real\nidentity owner, and thus distinguishes the owner from other users. We represent\nsuch a piece of information by a factoid and model it as a triplet consisting\nof user identity, predicate, and an object or another user identity. By\nembedding these factoids, we learn the user identity latent representations and\nlink two user identities from different OSNs if they are close to each other in\nthe user embedding space. Our Factoid Embedding algorithm is designed such that\nas we learn the embedding space, each embedded factoid is “translated” into a\nmotion in the user embedding space to bring similar user identities closer, and\ndifferent user identities further apart. Extensive experiments are conducted to\nevaluate Factoid Embedding on two real-world OSNs data sets. The experiment\nresults show that Factoid Embedding outperforms the state-of-the-art methods\neven without training data.</p>\n", "tags": ["Unsupervised","Supervised","Tools-&-Libraries"] },
{"key": "xie2019unsupervised", "year": "2018", "citations": "27", "title":"Unsupervised User Identity Linkage via Factoid Embedding", "abstract": "<p>User identity linkage (UIL), the problem of matching user account across\nmultiple online social networks (OSNs), is widely studied and important to many\nreal-world applications. Most existing UIL solutions adopt a supervised or\nsemi-supervised approach which generally suffer from scarcity of labeled data.\nIn this paper, we propose Factoid Embedding, a novel framework that adopts an\nunsupervised approach. It is designed to cope with different profile\nattributes, content types and network links of different OSNs. The key idea is\nthat each piece of information about a user identity describes the real\nidentity owner, and thus distinguishes the owner from other users. We represent\nsuch a piece of information by a factoid and model it as a triplet consisting\nof user identity, predicate, and an object or another user identity. By\nembedding these factoids, we learn the user identity latent representations and\nlink two user identities from different OSNs if they are close to each other in\nthe user embedding space. Our Factoid Embedding algorithm is designed such that\nas we learn the embedding space, each embedded factoid is “translated” into a\nmotion in the user embedding space to bring similar user identities closer, and\ndifferent user identities further apart. Extensive experiments are conducted to\nevaluate Factoid Embedding on two real-world OSNs data sets. The experiment\nresults show that Factoid Embedding outperforms the state-of-the-art methods\neven without training data.</p>\n", "tags": ["Unsupervised","Supervised","Tools-&-Libraries"] },
{"key": "xie2021learning", "year": "2021", "citations": "25", "title":"Learning Text-Image Joint Embedding for Efficient Cross-Modal Retrieval with Deep Feature Engineering", "abstract": "<p>This paper introduces a two-phase deep feature engineering framework for\nefficient learning of semantics enhanced joint embedding, which clearly\nseparates the deep feature engineering in data preprocessing from training the\ntext-image joint embedding model. We use the Recipe1M dataset for the technical\ndescription and empirical validation. In preprocessing, we perform deep feature\nengineering by combining deep feature engineering with semantic context\nfeatures derived from raw text-image input data. We leverage LSTM to identify\nkey terms, deep NLP models from the BERT family, TextRank, or TF-IDF to produce\nranking scores for key terms before generating the vector representation for\neach key term by using word2vec. We leverage wideResNet50 and word2vec to\nextract and encode the image category semantics of food images to help semantic\nalignment of the learned recipe and image embeddings in the joint latent space.\nIn joint embedding learning, we perform deep feature engineering by optimizing\nthe batch-hard triplet loss function with soft-margin and double negative\nsampling, taking into account also the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with deep feature engineering significantly outperforms the\nstate-of-the-art approaches.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Tools-&-Libraries","Multimodal-Retrieval"] },
{"key": "xin2021zero", "year": "2022", "citations": "17", "title":"Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations", "abstract": "<p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts\nin the embedding space and then matching them by nearest neighbor search. This\nrequires strong locality properties from the representation space, i.e, the\nclose allocations of each small group of relevant texts, which are hard to\ngeneralize to domains without sufficient training data. In this paper, we aim\nto improve the generalization ability of DR models from source training domains\nwith rich supervision signals to target domains without any relevant labels, in\nthe zero-shot setting. To achieve that, we propose Momentum adversarial Domain\nInvariant Representation learning (MoDIR), which introduces a momentum method\nin the DR training process to train a domain classifier distinguishing source\nversus target, and then adversarially updates the DR encoder to learn domain\ninvariant representations. Our experiments show that MoDIR robustly outperforms\nits baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot\nsetup, with more than 10% relative gains on datasets with enough sensitivity\nfor DR models’ evaluation. Source code of this paper will be released.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Text-Retrieval","Datasets","Evaluation","Robustness"] },
{"key": "xiong2020approximate", "year": "2021", "citations": "359", "title":"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval", "abstract": "<p>Conducting text retrieval in a dense learned representation space has many\nintriguing advantages over sparse retrieval. Yet the effectiveness of dense\nretrieval (DR) often requires combination with sparse retrieval. In this paper,\nwe identify that the main bottleneck is in the training mechanisms, where the\nnegative instances used in training are not representative of the irrelevant\ndocuments in testing. This paper presents Approximate nearest neighbor Negative\nContrastive Estimation (ANCE), a training mechanism that constructs negatives\nfrom an Approximate Nearest Neighbor (ANN) index of the corpus, which is\nparallelly updated with the learning process to select more realistic negative\ntraining instances. This fundamentally resolves the discrepancy between the\ndata distribution used in the training and testing of DR. In our experiments,\nANCE boosts the BERT-Siamese DR model to outperform all competitive dense and\nsparse retrieval baselines. It nearly matches the accuracy of\nsparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned\nrepresentation space and provides almost 100x speed-up.</p>\n", "tags": ["Self-Supervised","Text-Retrieval"] },
{"key": "xu2013harmonious", "year": "2013", "citations": "49", "title":"Harmonious Hashing", "abstract": "<p>Hashing-based fast nearest neighbor search technique\nhas attracted great attention in both research\nand industry areas recently. Many existing hashing\napproaches encode data with projection-based hash\nfunctions and represent each projected dimension\nby 1-bit. However, the dimensions with high variance\nhold large energy or information of data but\ntreated equivalently as dimensions with low variance,\nwhich leads to a serious information loss. In\nthis paper, we introduce a novel hashing algorithm\ncalled Harmonious Hashing which aims at learning\nhash functions with low information loss. Specifically,\nwe learn a set of optimized projections to\npreserve the maximum cumulative energy and meet\nthe constraint of equivalent variance on each dimension\nas much as possible. In this way, we could\nminimize the information loss after binarization.\nDespite the extreme simplicity, our method outperforms\nsuperiorly to many state-of-the-art hashing\nmethods in large-scale and high-dimensional nearest\nneighbor search experiments.</p>\n", "tags": ["Hashing-Methods","Scalability"] },
{"key": "xu2015convolutional", "year": "2015", "citations": "33", "title":"Convolutional Neural Networks for Text Hashing", "abstract": "<p>Hashing, as a popular approximate nearest neighbor\nsearch, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning\nmethods are utilized to learn similarity-preserving\nbinary codes. However, most of them directly encode the explicit features, keywords, which fail to\npreserve the accurate semantic similarities in binary code beyond keyword matching, especially on\nshort texts. Here we propose a novel text hashing\nframework with convolutional neural networks. In\nparticular, we first embed the keyword features into\ncompact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to\nlearn the implicit features which are further incorporated with the explicit features to fit the pre-trained\nbinary code. Such base method can be successfully\naccomplished without any external tags/labels, and\nother three model variations are designed to integrate tags/labels. Experimental results show the\nsuperiority of our proposed approach over several\nstate-of-the-art hashing methods when tested on one\nshort text dataset as well as one normal text dataset.</p>\n", "tags": ["Scalability","Datasets","Tools-&-Libraries","Compact-Codes","Similarity-Search","Hashing-Methods"] },
{"key": "xu2017iterative", "year": "2018", "citations": "21", "title":"Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval", "abstract": "<p>Existing manifold learning methods are not appropriate for image retrieval\ntask, because most of them are unable to process query image and they have much\nadditional computational cost especially for large scale database. Therefore,\nwe propose the iterative manifold embedding (IME) layer, of which the weights\nare learned off-line by unsupervised strategy, to explore the intrinsic\nmanifolds by incomplete data. On the large scale database that contains 27000\nimages, IME layer is more than 120 times faster than other manifold learning\nmethods to embed the original representations at query time. We embed the\noriginal descriptors of database images which lie on manifold in a high\ndimensional space into manifold-based representations iteratively to generate\nthe IME representations in off-line learning stage. According to the original\ndescriptors and the IME representations of database images, we estimate the\nweights of IME layer by ridge regression. In on-line retrieval stage, we employ\nthe IME layer to map the original representation of query image with ignorable\ntime cost (2 milliseconds). We experiment on five public standard datasets for\nimage retrieval. The proposed IME layer significantly outperforms related\ndimension reduction methods and manifold learning methods. Without\npost-processing, Our IME layer achieves a boost in performance of\nstate-of-the-art image retrieval methods with post-processing on most datasets,\nand needs less computational cost.</p>\n", "tags": ["Image-Retrieval","Datasets","Re-Ranking","Unsupervised","Scalability","Evaluation","Efficiency"] },
{"key": "xu2017neural", "year": "2017", "citations": "545", "title":"Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection", "abstract": "<p>The problem of cross-platform binary code similarity detection aims at\ndetecting whether two binary functions coming from different platforms are\nsimilar or not. It has many security applications, including plagiarism\ndetection, malware detection, vulnerability search, etc. Existing approaches\nrely on approximate graph matching algorithms, which are inevitably slow and\nsometimes inaccurate, and hard to adapt to a new task. To address these issues,\nin this work, we propose a novel neural network-based approach to compute the\nembedding, i.e., a numeric vector, based on the control flow graph of each\nbinary function, then the similarity detection can be done efficiently by\nmeasuring the distance between the embeddings for two functions. We implement a\nprototype called Gemini. Our extensive evaluation shows that Gemini outperforms\nthe state-of-the-art approaches by large margins with respect to similarity\ndetection accuracy. Further, Gemini can speed up prior art’s embedding\ngeneration time by 3 to 4 orders of magnitude and reduce the required training\ntime from more than 1 week down to 30 minutes to 10 hours. Our real world case\nstudies demonstrate that Gemini can identify significantly more vulnerable\nfirmware images than the state-of-the-art, i.e., Genius. Our research showcases\na successful application of deep learning on computer security problems.</p>\n", "tags": ["Compact-Codes","Evaluation"] },
{"key": "xu2017unsupervised", "year": "2018", "citations": "48", "title":"Unsupervised Part-based Weighting Aggregation of Deep Convolutional Features for Image Retrieval", "abstract": "<p>In this paper, we propose a simple but effective semantic part-based\nweighting aggregation (PWA) for image retrieval. The proposed PWA utilizes the\ndiscriminative filters of deep convolutional layers as part detectors.\nMoreover, we propose the effective unsupervised strategy to select some part\ndetectors to generate the “probabilistic proposals”, which highlight certain\ndiscriminative parts of objects and suppress the noise of background. The final\nglobal PWA representation could then be acquired by aggregating the regional\nrepresentations weighted by the selected “probabilistic proposals”\ncorresponding to various semantic content. We conduct comprehensive experiments\non four standard datasets and show that our unsupervised PWA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods. Code is\navailable at https://github.com/XJhaoren/PWA.</p>\n", "tags": ["Supervised","Image-Retrieval","Datasets","AAAI","Unsupervised"] },
{"key": "xu2019hashing", "year": "2020", "citations": "8", "title":"Hashing based Answer Selection", "abstract": "<p>Answer selection is an important subtask of question answering (QA), where\ndeep models usually achieve better performance. Most deep models adopt\nquestion-answer interaction mechanisms, such as attention, to get vector\nrepresentations for answers. When these interaction based deep models are\ndeployed for online prediction, the representations of all answers need to be\nrecalculated for each question. This procedure is time-consuming for deep\nmodels with complex encoders like BERT which usually have better accuracy than\nsimple encoders. One possible solution is to store the matrix representation\n(encoder output) of each answer in memory to avoid recalculation. But this will\nbring large memory cost. In this paper, we propose a novel method, called\nhashing based answer selection (HAS), to tackle this problem. HAS adopts a\nhashing strategy to learn a binary matrix representation for each answer, which\ncan dramatically reduce the memory cost for storing the matrix representations\nof answers. Hence, HAS can adopt complex encoders like BERT in the model, but\nthe online prediction of HAS is still fast with a low memory cost. Experimental\nresults on three popular answer selection datasets show that HAS can outperform\nexisting models to achieve state-of-the-art performance.</p>\n", "tags": ["AAAI","Datasets","Hashing-Methods","Evaluation"] },
{"key": "xu2020multi", "year": "2020", "citations": "21", "title":"Multi-Feature Discrete Collaborative Filtering for Fast Cold-start Recommendation", "abstract": "<p>Hashing is an effective technique to address the large-scale recommendation\nproblem, due to its high computation and storage efficiency on calculating the\nuser preferences on items. However, existing hashing-based recommendation\nmethods still suffer from two important problems: 1) Their recommendation\nprocess mainly relies on the user-item interactions and single specific content\nfeature. When the interaction history or the content feature is unavailable\n(the cold-start problem), their performance will be seriously deteriorated. 2)\nExisting methods learn the hash codes with relaxed optimization or adopt\ndiscrete coordinate descent to directly solve binary hash codes, which results\nin significant quantization loss or consumes considerable computation time. In\nthis paper, we propose a fast cold-start recommendation method, called\nMulti-Feature Discrete Collaborative Filtering (MFDCF), to solve these\nproblems. Specifically, a low-rank self-weighted multi-feature fusion module is\ndesigned to adaptively project the multiple content features into binary yet\ninformative hash codes by fully exploiting their complementarity. Additionally,\nwe develop a fast discrete optimization algorithm to directly compute the\nbinary hash codes with simple operations. Experiments on two public\nrecommendation datasets demonstrate that MFDCF outperforms the\nstate-of-the-arts on various aspects.</p>\n", "tags": ["Hashing-Methods","Datasets","Recommender-Systems","Quantization","AAAI","Scalability","Evaluation","Efficiency"] },
{"key": "xu2025convolutional", "year": "2015", "citations": "33", "title":"Convolutional Neural Networks for Text Hashing", "abstract": "<p>Hashing, as a popular approximate nearest neighbor\nsearch, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning\nmethods are utilized to learn similarity-preserving\nbinary codes. However, most of them directly encode the explicit features, keywords, which fail to\npreserve the accurate semantic similarities in binary code beyond keyword matching, especially on\nshort texts. Here we propose a novel text hashing\nframework with convolutional neural networks. In\nparticular, we first embed the keyword features into\ncompact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to\nlearn the implicit features which are further incorporated with the explicit features to fit the pre-trained\nbinary code. Such base method can be successfully\naccomplished without any external tags/labels, and\nother three model variations are designed to integrate tags/labels. Experimental results show the\nsuperiority of our proposed approach over several\nstate-of-the-art hashing methods when tested on one\nshort text dataset as well as one normal text dataset.</p>\n", "tags": ["Scalability","Datasets","Tools-&-Libraries","Compact-Codes","Similarity-Search","Hashing-Methods"] },
{"key": "xu2025harmonious", "year": "2013", "citations": "49", "title":"Harmonious Hashing", "abstract": "<p>Hashing-based fast nearest neighbor search technique\nhas attracted great attention in both research\nand industry areas recently. Many existing hashing\napproaches encode data with projection-based hash\nfunctions and represent each projected dimension\nby 1-bit. However, the dimensions with high variance\nhold large energy or information of data but\ntreated equivalently as dimensions with low variance,\nwhich leads to a serious information loss. In\nthis paper, we introduce a novel hashing algorithm\ncalled Harmonious Hashing which aims at learning\nhash functions with low information loss. Specifically,\nwe learn a set of optimized projections to\npreserve the maximum cumulative energy and meet\nthe constraint of equivalent variance on each dimension\nas much as possible. In this way, we could\nminimize the information loss after binarization.\nDespite the extreme simplicity, our method outperforms\nsuperiorly to many state-of-the-art hashing\nmethods in large-scale and high-dimensional nearest\nneighbor search experiments.</p>\n", "tags": ["Hashing-Methods","Scalability"] },
{"key": "xuan2018deep", "year": "2018", "citations": "124", "title":"Deep Randomized Ensembles for Metric Learning", "abstract": "<p>Learning embedding functions, which map semantically related inputs to nearby\nlocations in a feature space supports a variety of classification and\ninformation retrieval tasks. In this work, we propose a novel, generalizable\nand fast method to define a family of embedding functions that can be used as\nan ensemble to give improved results. Each embedding function is learned by\nrandomly bagging the training labels into small subsets. We show experimentally\nthat these embedding ensembles create effective embedding functions. The\nensemble output defines a metric space that improves state of the art\nperformance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes\nRetrieval and VehicleID.</p>\n", "tags": ["Distance-Metric-Learning","Evaluation","Image-Retrieval"] },
{"key": "xuan2019improved", "year": "2020", "citations": "131", "title":"Improved Embeddings with Easy Positive Triplet Mining", "abstract": "<p>Deep metric learning seeks to define an embedding where semantically similar\nimages are embedded to nearby locations, and semantically dissimilar images are\nembedded to distant locations. Substantial work has focused on loss functions\nand strategies to learn these embeddings by pushing images from the same class\nas close together in the embedding space as possible. In this paper, we propose\nan alternative, loosened embedding strategy that requires the embedding\nfunction only map each training image to the most similar examples from the\nsame class, an approach we call “Easy Positive” mining. We provide a collection\nof experiments and visualizations that highlight that this Easy Positive mining\nleads to embeddings that are more flexible and generalize better to new unseen\ndata. This simple mining strategy yields recall performance that exceeds state\nof the art approaches (including those with complicated loss functions and\nensemble methods) on image retrieval datasets including CUB, Stanford Online\nProducts, In-Shop Clothes and Hotels-50K.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Evaluation","Image-Retrieval"] },
{"key": "xue2022cross", "year": "2022", "citations": "5", "title":"Cross-Scale Context Extracted Hashing for Fine-Grained Image Binary Encoding", "abstract": "<p>Deep hashing has been widely applied to large-scale image retrieval tasks\nowing to efficient computation and low storage cost by encoding\nhigh-dimensional image data into binary codes. Since binary codes do not\ncontain as much information as float features, the essence of binary encoding\nis preserving the main context to guarantee retrieval quality. However, the\nexisting hashing methods have great limitations on suppressing redundant\nbackground information and accurately encoding from Euclidean space to Hamming\nspace by a simple sign function. In order to solve these problems, a\nCross-Scale Context Extracted Hashing Network (CSCE-Net) is proposed in this\npaper. Firstly, we design a two-branch framework to capture fine-grained local\ninformation while maintaining high-level global semantic information. Besides,\nAttention guided Information Extraction module (AIE) is introduced between two\nbranches, which suppresses areas of low context information cooperated with\nglobal sliding windows. Unlike previous methods, our CSCE-Net learns a\ncontent-related Dynamic Sign Function (DSF) to replace the original simple sign\nfunction. Therefore, the proposed CSCE-Net is context-sensitive and able to\nperform well on accurate image binary encoding. We further demonstrate that our\nCSCE-Net is superior to the existing hashing methods, which improves retrieval\nperformance on standard benchmarks.</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Neural-Hashing","Compact-Codes","Memory-Efficiency","Scalability","Evaluation"] },
{"key": "yamada2021efficient", "year": "2021", "citations": "46", "title":"Efficient Passage Retrieval with Hashing for Open-domain Question Answering", "abstract": "<p>Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.</p>\n", "tags": ["Hashing-Methods","Compact-Codes"] },
{"key": "yan2019deep", "year": "2019", "citations": "27", "title":"Deep Hashing by Discriminating Hard Examples", "abstract": "<p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","Hashing-Methods"] },
{"key": "yan2020deep", "year": "2020", "citations": "413", "title":"Deep Multi-View Enhancement Hashing for Image Retrieval", "abstract": "<p>Hashing is an efficient method for nearest neighbor search in large-scale\ndata space by embedding high-dimensional feature descriptors into a similarity\npreserving Hamming space with a low dimension. However, large-scale high-speed\nretrieval through binary code has a certain degree of reduction in retrieval\naccuracy compared to traditional retrieval methods. We have noticed that\nmulti-view methods can well preserve the diverse characteristics of data.\nTherefore, we try to introduce the multi-view deep neural network into the hash\nlearning field, and design an efficient and innovative retrieval model, which\nhas achieved a significant improvement in retrieval performance. In this paper,\nwe propose a supervised multi-view hash model which can enhance the multi-view\ninformation through neural networks. This is a completely new hash learning\nmethod that combines multi-view and deep learning methods. The proposed method\nutilizes an effective view stability evaluation method to actively explore the\nrelationship among views, which will affect the optimization direction of the\nentire network. We have also designed a variety of multi-data fusion methods in\nthe Hamming space to preserve the advantages of both convolution and\nmulti-view. In order to avoid excessive computing resources on the enhancement\nprocedure during retrieval, we set up a separate structure called memory\nnetwork which participates in training together. The proposed method is\nsystematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and\nthe results show that our method significantly outperforms the state-of-the-art\nsingle-view and multi-view hashing methods.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Compact-Codes","Scalability","Evaluation"] },
{"key": "yan2021binary", "year": "2021", "citations": "10", "title":"Binary Code based Hash Embedding for Web-scale Applications", "abstract": "<p>Nowadays, deep learning models are widely adopted in web-scale applications\nsuch as recommender systems, and online advertising. In these applications,\nembedding learning of categorical features is crucial to the success of deep\nlearning models. In these models, a standard method is that each categorical\nfeature value is assigned a unique embedding vector which can be learned and\noptimized. Although this method can well capture the characteristics of the\ncategorical features and promise good performance, it can incur a huge memory\ncost to store the embedding table, especially for those web-scale applications.\nSuch a huge memory cost significantly holds back the effectiveness and\nusability of EDRMs. In this paper, we propose a binary code based hash\nembedding method which allows the size of the embedding table to be reduced in\narbitrary scale without compromising too much performance. Experimental\nevaluation results show that one can still achieve 99% performance even if the\nembedding table size is reduced 1000\\(\\times\\) smaller than the original one with\nour proposed method.</p>\n", "tags": ["Scalability","Evaluation","CIKM","Large-Scale-Search","Recommender-Systems","Compact-Codes"] },
{"key": "yan2021hierarchical", "year": "2021", "citations": "32", "title":"Hierarchical Attention Fusion for Geo-Localization", "abstract": "<p>Geo-localization is a critical task in computer vision. In this work, we cast\nthe geo-localization as a 2D image retrieval task. Current state-of-the-art\nmethods for 2D geo-localization are not robust to locate a scene with drastic\nscale variations because they only exploit features from one semantic level for\nimage representations. To address this limitation, we introduce a hierarchical\nattention fusion network using multi-scale features for geo-localization. We\nextract the hierarchical feature maps from a convolutional neural network (CNN)\nand organically fuse the extracted features for image representations. Our\ntraining is self-supervised using adaptive weights to control the attention of\nfeature emphasis from each hierarchical level. Evaluation results on the image\nretrieval and the large-scale geo-localization benchmarks indicate that our\nmethod outperforms the existing state-of-the-art methods. Code is available\nhere: https://github.com/YanLiqi/HAF.</p>\n", "tags": ["Supervised","ICASSP","Image-Retrieval","Self-Supervised","Scalability","Evaluation"] },
{"key": "yan2025deep", "year": "2019", "citations": "27", "title":"Deep Hashing by Discriminating Hard Examples", "abstract": "<p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Datasets","Neural-Hashing","Hashing-Methods"] },
{"key": "yang2016zero", "year": "2016", "citations": "139", "title":"Zero-Shot Hashing via Transferring Supervised Knowledge", "abstract": "<p>Hashing has shown its efficiency and effectiveness in facilitating\nlarge-scale multimedia applications. Supervised knowledge e.g. semantic labels\nor pair-wise relationship) associated to data is capable of significantly\nimproving the quality of hash codes and hash functions. However, confronted\nwith the rapid growth of newly-emerging concepts and multimedia data on the\nWeb, existing supervised hashing approaches may easily suffer from the scarcity\nand validity of supervised information due to the expensive cost of manual\nlabelling. In this paper, we propose a novel hashing scheme, termed\n<em>zero-shot hashing</em> (ZSH), which compresses images of “unseen” categories\nto binary codes with hash functions learned from limited training data of\n“seen” categories. Specifically, we project independent data labels i.e.\n0/1-form label vectors) into semantic embedding space, where semantic\nrelationships among all the labels can be precisely characterized and thus seen\nsupervised knowledge can be transferred to unseen classes. Moreover, in order\nto cope with the semantic shift problem, we rotate the embedded space to more\nsuitably align the embedded semantics with the low-level visual feature space,\nthereby alleviating the influence of semantic gap. In the meantime, to exert\npositive effects on learning high-quality hash functions, we further propose to\npreserve local structural property and discrete nature in binary codes.\nBesides, we develop an efficient alternating algorithm to solve the ZSH model.\nExtensive experiments conducted on various real-life datasets show the superior\nzero-shot image retrieval performance of ZSH as compared to several\nstate-of-the-art hashing methods.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Supervised","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Scalability","Evaluation","Efficiency"] },
{"key": "yang2018deep", "year": "2019", "citations": "17", "title":"Deep Attention-guided Hashing", "abstract": "<p>With the rapid growth of multimedia data (e.g., image, audio and video etc.)\non the web, learning-based hashing techniques such as Deep Supervised Hashing\n(DSH) have proven to be very efficient for large-scale multimedia search. The\nrecent successes seen in Learning-based hashing methods are largely due to the\nsuccess of deep learning-based hashing methods. However, there are some\nlimitations to previous learning-based hashing methods (e.g., the learned hash\ncodes containing repetitive and highly correlated information). In this paper,\nwe propose a novel learning-based hashing method, named Deep Attention-guided\nHashing (DAgH). DAgH is implemented using two stream frameworks. The core idea\nis to use guided hash codes which are generated by the hashing network of the\nfirst stream framework (called first hashing network) to guide the training of\nthe hashing network of the second stream framework (called second hashing\nnetwork). Specifically, in the first network, it leverages an attention network\nand hashing network to generate the attention-guided hash codes from the\noriginal images. The loss function we propose contains two components: the\nsemantic loss and the attention loss. The attention loss is used to punish the\nattention network to obtain the salient region from pairs of images; in the\nsecond network, these attention-guided hash codes are used to guide the\ntraining of the second hashing network (i.e., these codes are treated as\nsupervised labels to train the second network). By doing this, DAgH can make\nfull use of the most critical information contained in images to guide the\nsecond hashing network in order to learn efficient hash codes in a true\nend-to-end fashion. Results from our experiments demonstrate that DAgH can\ngenerate high quality hash codes and it outperforms current state-of-the-art\nmethods on three benchmark datasets, CIFAR-10, NUS-WIDE, and ImageNet.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Scalability","Evaluation"] },
{"key": "yang2018efficient", "year": "2019", "citations": "49", "title":"Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing", "abstract": "<p>Diffusion is commonly used as a ranking or re-ranking method in retrieval\ntasks to achieve higher retrieval performance, and has attracted lots of\nattention in recent years. A downside to diffusion is that it performs slowly\nin comparison to the naive k-NN search, which causes a non-trivial online\ncomputational cost on large datasets. To overcome this weakness, we propose a\nnovel diffusion technique in this paper. In our work, instead of applying\ndiffusion to the query, we pre-compute the diffusion results of each element in\nthe database, making the online search a simple linear combination on top of\nthe k-NN search process. Our proposed method becomes 10~ times faster in terms\nof online search speed. Moreover, we propose to use late truncation instead of\nearly truncation in previous works to achieve better retrieval performance.</p>\n", "tags": ["Image-Retrieval","Datasets","Hybrid-ANN-Methods","Re-Ranking","AAAI","Evaluation"] },
{"key": "yang2019adaptive", "year": "2019", "citations": "8", "title":"Adaptive Labeling for Deep Learning to Hash", "abstract": "<p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary\nhash function learning approach via deep neural networks\nin this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward\nnetwork training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class\nlabel representations together with the network weights are\nupdated in the learning process. As the label representations (or referred to as codewords in this work), are learned\nfrom data, semantically similar classes will be assigned\nwith the codewords that are close to each other in terms\nof Hamming distance in the label space. The codewords\nthen serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which\ncan jointly learn label representations and infer compact\nbinary codes from data. It is applicable to both supervised\nand semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance\nof AdaLabelHash.</p>\n", "tags": ["Image-Retrieval","Efficiency","CVPR","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "yang2019asymmetric", "year": "2019", "citations": "5", "title":"Asymmetric Deep Semantic Quantization for Image Retrieval", "abstract": "<p>Due to its fast retrieval and storage efficiency capabilities, hashing has\nbeen widely used in nearest neighbor retrieval tasks. By using deep learning\nbased techniques, hashing can outperform non-learning based hashing technique\nin many applications. However, we argue that the current deep learning based\nhashing methods ignore some critical problems (e.g., the learned hash codes are\nnot discriminative due to the hashing methods being unable to discover rich\nsemantic information and the training strategy having difficulty optimizing the\ndiscrete binary codes). In this paper, we propose a novel image hashing method,\ntermed as \\textbf{\\underline{A}}symmetric \\textbf{\\underline{D}}eep\n\\textbf{\\underline{S}}emantic \\textbf{\\underline{Q}}uantization\n(\\textbf{ADSQ}). \\textbf{ADSQ} is implemented using three stream frameworks,\nwhich consist of one <em>LabelNet</em> and two <em>ImgNets</em>. The\n<em>LabelNet</em> leverages the power of three fully-connected layers, which are\nused to capture rich semantic information between image pairs. For the two\n<em>ImgNets</em>, they each adopt the same convolutional neural network\nstructure, but with different weights (i.e., asymmetric convolutional neural\nnetworks). The two <em>ImgNets</em> are used to generate discriminative compact\nhash codes. Specifically, the function of the <em>LabelNet</em> is to capture\nrich semantic information that is used to guide the two <em>ImgNets</em> in\nminimizing the gap between the real-continuous features and the discrete binary\ncodes. Furthermore, \\textbf{ADSQ} can utilize the most critical semantic\ninformation to guide the feature learning process and consider the consistency\nof the common semantic space and Hamming space. Experimental results on three\nbenchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the\nproposed \\textbf{ADSQ} can outperforms current state-of-the-art methods.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Compact-Codes","Quantization","Efficiency"] },
{"key": "yang2019feature", "year": "2019", "citations": "22", "title":"Feature Pyramid Hashing", "abstract": "<p>In recent years, deep-networks-based hashing has become a leading approach\nfor large-scale image retrieval. Most deep hashing approaches use the high\nlayer to extract the powerful semantic representations. However, these methods\nhave limited ability for fine-grained image retrieval because the semantic\nfeatures extracted from the high layer are difficult in capturing the subtle\ndifferences. To this end, we propose a novel two-pyramid hashing architecture\nto learn both the semantic information and the subtle appearance details for\nfine-grained image search. Inspired by the feature pyramids of convolutional\nneural network, a vertical pyramid is proposed to capture the high-layer\nfeatures and a horizontal pyramid combines multiple low-layer features with\nstructural information to capture the subtle differences. To fuse the low-level\nfeatures, a novel combination strategy, called consensus fusion, is proposed to\ncapture all subtle information from several low-layers for finer retrieval.\nExtensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford\nDogs demonstrate that the proposed method achieves significant performance\ncompared with the state-of-art baselines.</p>\n", "tags": ["Scalability","Evaluation","Datasets","Hashing-Methods","Multimodal-Retrieval","Neural-Hashing","Image-Retrieval"] },
{"key": "yang2019learning", "year": "2020", "citations": "28", "title":"Learning Shared Semantic Space with Correlation Alignment for Cross-modal Event Retrieval", "abstract": "<p>In this paper, we propose to learn shared semantic space with correlation\nalignment (\\({S}^{3}CA\\)) for multimodal data representations, which aligns\nnonlinear correlations of multimodal data distributions in deep neural networks\ndesigned for heterogeneous data. In the context of cross-modal (event)\nretrieval, we design a neural network with convolutional layers and\nfully-connected layers to extract features for images, including images on\nFlickr-like social media. Simultaneously, we exploit a fully-connected neural\nnetwork to extract semantic features for texts, including news articles from\nnews media. In particular, nonlinear correlations of layer activations in the\ntwo neural networks are aligned with correlation alignment during the joint\ntraining of the networks. Furthermore, we project the multimodal data into a\nshared semantic space for cross-modal (event) retrieval, where the distances\nbetween heterogeneous data samples can be measured directly. In addition, we\ncontribute a Wiki-Flickr Event dataset, where the multimodal data samples are\nnot describing each other in pairs like the existing paired datasets, but all\nof them are describing semantic events. Extensive experiments conducted on both\npaired and unpaired datasets manifest the effectiveness of \\({S}^{3}CA\\),\noutperforming the state-of-the-art methods.</p>\n", "tags": ["Datasets"] },
{"key": "yang2019shared", "year": "2018", "citations": "151", "title":"Shared Predictive Cross-Modal Deep Quantization", "abstract": "<p>With explosive growth of data volume and ever-increasing diversity of data\nmodalities, cross-modal similarity search, which conducts nearest neighbor\nsearch across different modalities, has been attracting increasing interest.\nThis paper presents a deep compact code learning solution for efficient\ncross-modal similarity search. Many recent studies have proven that\nquantization-based approaches perform generally better than hashing-based\napproaches on single-modal similarity search. In this paper, we propose a deep\nquantization approach, which is among the early attempts of leveraging deep\nneural networks into quantization-based cross-modal similarity search. Our\napproach, dubbed shared predictive deep quantization (SPDQ), explicitly\nformulates a shared subspace across different modalities and two private\nsubspaces for individual modalities, and representations in the shared subspace\nand the private subspaces are learned simultaneously by embedding them to a\nreproducing kernel Hilbert space, where the mean embedding of different\nmodality distributions can be explicitly compared. In addition, in the shared\nsubspace, a quantizer is learned to produce the semantics preserving compact\ncodes with the help of label alignment. Thanks to this novel network\narchitecture in cooperation with supervised quantization training, SPDQ can\npreserve intramodal and intermodal similarities as much as possible and greatly\nreduce quantization error. Experiments on two popular benchmarks corroborate\nthat our approach outperforms state-of-the-art methods.</p>\n", "tags": ["Similarity-Search","Supervised","Hashing-Methods","Compact-Codes","Quantization"] },
{"key": "yang2020nonlinear", "year": "2020", "citations": "22", "title":"Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval", "abstract": "<p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Multimodal-Retrieval","SIGIR","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "yang2020tree", "year": "2020", "citations": "123", "title":"Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval", "abstract": "<p>The rapid growth of user-generated videos on the Internet has intensified the\nneed for text-based video retrieval systems. Traditional methods mainly favor\nthe concept-based paradigm on retrieval with simple queries, which are usually\nineffective for complex queries that carry far more complex semantics.\nRecently, embedding-based paradigm has emerged as a popular approach. It aims\nto map the queries and videos into a shared embedding space where\nsemantically-similar texts and videos are much closer to each other. Despite\nits simplicity, it forgoes the exploitation of the syntactic structure of text\nqueries, making it suboptimal to model the complex queries.\n  To facilitate video retrieval with complex queries, we propose a\nTree-augmented Cross-modal Encoding method by jointly learning the linguistic\nstructure of queries and the temporal representation of videos. Specifically,\ngiven a complex user query, we first recursively compose a latent semantic tree\nto structurally describe the text query. We then design a tree-augmented query\nencoder to derive structure-aware query representation and a temporal attentive\nvideo encoder to model the temporal characteristics of videos. Finally, both\nthe query and videos are mapped into a joint embedding space for matching and\nranking. In this approach, we have a better understanding and modeling of the\ncomplex queries, thereby achieving a better video retrieval performance.\nExtensive experiments on large scale video retrieval benchmark datasets\ndemonstrate the effectiveness of our approach.</p>\n", "tags": ["Datasets","Video-Retrieval","SIGIR","Evaluation"] },
{"key": "yang2022transformer", "year": "2023", "citations": "6", "title":"Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training", "abstract": "<p>In this paper, we present a cross-modal recipe retrieval framework,\nTransformer-based Network for Large Batch Training (TNLBT), which is inspired\nby ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).\nTNLBT aims to accomplish retrieval tasks while generating images from recipe\nembeddings. We apply the Hierarchical Transformer-based recipe text encoder,\nthe Vision Transformer~(ViT)-based recipe image encoder, and an adversarial\nnetwork architecture to enable better cross-modal embedding learning for recipe\ntexts and images. In addition, we use self-supervised learning to exploit the\nrich information in the recipe texts having no corresponding images. Since\ncontrastive learning could benefit from a larger batch size according to the\nrecent literature on self-supervised learning, we adopt a large batch size\nduring training and have validated its effectiveness. In the experiments, the\nproposed framework significantly outperformed the current state-of-the-art\nframeworks in both cross-modal recipe retrieval and image generation tasks on\nthe benchmark Recipe1M. This is the first work which confirmed the\neffectiveness of large batch training on cross-modal recipe embeddings.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Self-Supervised","Evaluation","Robustness"] },
{"key": "yang2025adaptive", "year": "2019", "citations": "8", "title":"Adaptive Labeling for Deep Learning to Hash", "abstract": "<p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary\nhash function learning approach via deep neural networks\nin this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward\nnetwork training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class\nlabel representations together with the network weights are\nupdated in the learning process. As the label representations (or referred to as codewords in this work), are learned\nfrom data, semantically similar classes will be assigned\nwith the codewords that are close to each other in terms\nof Hamming distance in the label space. The codewords\nthen serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which\ncan jointly learn label representations and infer compact\nbinary codes from data. It is applicable to both supervised\nand semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance\nof AdaLabelHash.</p>\n", "tags": ["Image-Retrieval","Efficiency","CVPR","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "yang2025nonlinear", "year": "2020", "citations": "22", "title":"Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval", "abstract": "<p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Multimodal-Retrieval","SIGIR","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "yao2019efficient", "year": "2019", "citations": "29", "title":"Efficient Discrete Supervised Hashing for Large-scale Cross-modal Retrieval", "abstract": "<p>Supervised cross-modal hashing has gained increasing research interest on\nlarge-scale retrieval task owning to its satisfactory performance and\nefficiency. However, it still has some challenging issues to be further\nstudied: 1) most of them fail to well preserve the semantic correlations in\nhash codes because of the large heterogenous gap; 2) most of them relax the\ndiscrete constraint on hash codes, leading to large quantization error and\nconsequent low performance; 3) most of them suffer from relatively high memory\ncost and computational complexity during training procedure, which makes them\nunscalable. In this paper, to address above issues, we propose a supervised\ncross-modal hashing method based on matrix factorization dubbed Efficient\nDiscrete Supervised Hashing (EDSH). Specifically, collective matrix\nfactorization on heterogenous features and semantic embedding with class labels\nare seamlessly integrated to learn hash codes. Therefore, the feature based\nsimilarities and semantic correlations can be both preserved in hash codes,\nwhich makes the learned hash codes more discriminative. Then an efficient\ndiscrete optimal algorithm is proposed to handle the scalable issue. Instead of\nlearning hash codes bit-by-bit, hash codes matrix can be obtained directly\nwhich is more efficient. Extensive experimental results on three public\nreal-world datasets demonstrate that EDSH produces a superior performance in\nboth accuracy and scalability over some existing cross-modal hashing methods.</p>\n", "tags": ["Supervised","Hashing-Methods","Datasets","Quantization","Neural-Hashing","Scalability","Evaluation","Efficiency"] },
{"key": "yeh2022embedding", "year": "2022", "citations": "13", "title":"Embedding Compression with Hashing for Efficient Representation Learning in Large-Scale Graph", "abstract": "<p>Graph neural networks (GNNs) are deep learning models designed specifically\nfor graph data, and they typically rely on node features as the input to the\nfirst layer. When applying such a type of network on the graph without node\nfeatures, one can extract simple graph-based node features (e.g., number of\ndegrees) or learn the input node representations (i.e., embeddings) when\ntraining the network. While the latter approach, which trains node embeddings,\nmore likely leads to better performance, the number of parameters associated\nwith the embeddings grows linearly with the number of nodes. It is therefore\nimpractical to train the input node embeddings together with GNNs within\ngraphics processing unit (GPU) memory in an end-to-end fashion when dealing\nwith industrial-scale graph data. Inspired by the embedding compression methods\ndeveloped for natural language processing (NLP) tasks, we develop a node\nembedding compression method where each node is compactly represented with a\nbit vector instead of a floating-point vector. The parameters utilized in the\ncompression method can be trained together with GNNs. We show that the proposed\nnode embedding compression method achieves superior performance compared to the\nalternatives.</p>\n", "tags": ["Graph-Based-ANN","Hashing-Methods","KDD","Scalability","Evaluation"] },
{"key": "yi2014deep", "year": "2014", "citations": "143", "title":"Deep Metric Learning for Practical Person Re-Identification", "abstract": "<p>Various hand-crafted features and metric learning methods prevail in the\nfield of person re-identification. Compared to these methods, this paper\nproposes a more general way that can learn a similarity metric from image\npixels directly. By using a “siamese” deep neural network, the proposed method\ncan jointly learn the color feature, texture feature and metric in a unified\nframework. The network has a symmetry structure with two sub-networks which are\nconnected by Cosine function. To deal with the big variations of person images,\nbinomial deviance is used to evaluate the cost between similarities and labels,\nwhich is proved to be robust to outliers.\n  Compared to existing researches, a more practical setting is studied in the\nexperiments that is training and test on different datasets (cross dataset\nperson re-identification). Both in “intra dataset” and “cross dataset”\nsettings, the superiorities of the proposed method are illustrated on VIPeR and\nPRID.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Tools-&-Libraries"] },
{"key": "ying2018graph", "year": "2018", "citations": "3167", "title":"Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "abstract": "<p>Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. However,\nmaking these methods practical and scalable to web-scale recommendation tasks\nwith billions of items and hundreds of millions of users remains a challenge.\nHere we describe a large-scale deep recommendation engine that we developed and\ndeployed at Pinterest. We develop a data-efficient Graph Convolutional Network\n(GCN) algorithm PinSage, which combines efficient random walks and graph\nconvolutions to generate embeddings of nodes (i.e., items) that incorporate\nboth graph structure as well as node feature information. Compared to prior GCN\napproaches, we develop a novel method based on highly efficient random walks to\nstructure the convolutions and design a novel training strategy that relies on\nharder-and-harder training examples to improve robustness and convergence of\nthe model. We also develop an efficient MapReduce model inference algorithm to\ngenerate embeddings using a trained model. We deploy PinSage at Pinterest and\ntrain it on 7.5 billion examples on a graph with 3 billion nodes representing\npins and boards, and 18 billion edges. According to offline metrics, user\nstudies and A/B tests, PinSage generates higher-quality recommendations than\ncomparable deep learning and graph-based alternatives. To our knowledge, this\nis the largest application of deep graph embeddings to date and paves the way\nfor a new generation of web-scale recommender systems based on graph\nconvolutional architectures.</p>\n", "tags": ["Graph-Based-ANN","Recommender-Systems","KDD","Scalability","Large-Scale-Search","Evaluation","Robustness"] },
{"key": "yokoo2020two", "year": "2020", "citations": "20", "title":"Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval", "abstract": "<p>We propose an efficient pipeline for large-scale landmark image retrieval\nthat addresses the diversity of the dataset through two-stage discriminative\nre-ranking. Our approach is based on embedding the images in a feature-space\nusing a convolutional neural network trained with a cosine softmax loss. Due to\nthe variance of the images, which include extreme viewpoint changes such as\nhaving to retrieve images of the exterior of a landmark from images of the\ninterior, this is very challenging for approaches based exclusively on visual\nsimilarity. Our proposed re-ranking approach improves the results in two steps:\nin the sort-step, \\(k\\)-nearest neighbor search with soft-voting to sort the\nretrieved results based on their label similarity to the query images, and in\nthe insert-step, we add additional samples from the dataset that were not\nretrieved by image-similarity. This approach allows overcoming the low visual\ndiversity in retrieved images. In-depth experimental results show that the\nproposed approach significantly outperforms existing approaches on the\nchallenging Google Landmarks Datasets. Using our methods, we achieved 1st place\nin the Google Landmark Retrieval 2019 challenge and 3rd place in the Google\nLandmark Recognition 2019 challenge on Kaggle. Our code is publicly available\nhere: https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Hybrid-ANN-Methods","Re-Ranking","Scalability"] },
{"key": "yu2014circulant", "year": "2014", "citations": "132", "title":"Circulant Binary Embedding", "abstract": "<p>Binary embedding of high-dimensional data requires\nlong codes to preserve the discriminative\npower of the input space. Traditional binary coding\nmethods often suffer from very high computation\nand storage costs in such a scenario. To\naddress this problem, we propose Circulant Binary\nEmbedding (CBE) which generates binary\ncodes by projecting the data with a circulant matrix.\nThe circulant structure enables the use of\nFast Fourier Transformation to speed up the computation.\nCompared to methods that use unstructured\nmatrices, the proposed method improves\nthe time complexity from O(d^2\n) to O(d log d),\nand the space complexity from O(d^2) to O(d)\nwhere d is the input dimensionality. We also\npropose a novel time-frequency alternating optimization\nto learn data-dependent circulant projections,\nwhich alternatively minimizes the objective\nin original and Fourier domains. We show\nby extensive experiments that the proposed approach\ngives much better performance than the\nstate-of-the-art approaches for fixed time, and\nprovides much faster computation with no performance\ndegradation for fixed number of bits.</p>\n", "tags": ["Hashing-Methods","Evaluation","Memory-Efficiency"] },
{"key": "yu2018discriminative", "year": "2019", "citations": "15", "title":"Discriminative Supervised Hashing for Cross-Modal similarity Search", "abstract": "<p>With the advantage of low storage cost and high retrieval efficiency, hashing\ntechniques have recently been an emerging topic in cross-modal similarity\nsearch. As multiple modal data reflect similar semantic content, many\nresearches aim at learning unified binary codes. However, discriminative\nhashing features learned by these methods are not adequate. This results in\nlower accuracy and robustness. We propose a novel hashing learning framework\nwhich jointly performs classifier learning, subspace learning and matrix\nfactorization to preserve class-specific semantic content, termed\nDiscriminative Supervised Hashing (DSH), to learn the discrimative unified\nbinary codes for multi-modal data. Besides, reducing the loss of information\nand preserving the non-linear structure of data, DSH non-linearly projects\ndifferent modalities into the common space in which the similarity among\nheterogeneous data points can be measured. Extensive experiments conducted on\nthe three publicly available datasets demonstrate that the framework proposed\nin this paper outperforms several state-of -the-art methods.</p>\n", "tags": ["Similarity-Search","Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Memory-Efficiency","Efficiency","Robustness"] },
{"key": "yu2018modeling", "year": "2018", "citations": "44", "title":"Modeling Text with Graph Convolutional Network for Cross-Modal Information Retrieval", "abstract": "<p>Cross-modal information retrieval aims to find heterogeneous data of various\nmodalities from a given query of one modality. The main challenge is to map\ndifferent modalities into a common semantic space, in which distance between\nconcepts in different modalities can be well modeled. For cross-modal\ninformation retrieval between images and texts, existing work mostly uses\noff-the-shelf Convolutional Neural Network (CNN) for image feature extraction.\nFor texts, word-level features such as bag-of-words or word2vec are employed to\nbuild deep learning models to represent texts. Besides word-level semantics,\nthe semantic relations between words are also informative but less explored. In\nthis paper, we model texts by graphs using similarity measure based on\nword2vec. A dual-path neural network model is proposed for couple feature\nlearning in cross-modal information retrieval. One path utilizes Graph\nConvolutional Network (GCN) for text modeling based on graph representations.\nThe other path uses a neural network with layers of nonlinearities for image\nmodeling based on off-the-shelf features. The model is trained by a pairwise\nsimilarity loss function to maximize the similarity of relevant text-image\npairs and minimize the similarity of irrelevant pairs. Experimental results\nshow that the proposed model outperforms the state-of-the-art methods\nsignificantly, with 17% improvement on accuracy for the best case.</p>\n", "tags": ["Evaluation"] },
{"key": "yu2018semi", "year": "2018", "citations": "10", "title":"Semi-supervised Hashing for Semi-Paired Cross-View Retrieval", "abstract": "<p>Recently, hashing techniques have gained importance in large-scale retrieval\ntasks because of their retrieval speed. Most of the existing cross-view\nframeworks assume that data are well paired. However, the fully-paired\nmultiview situation is not universal in real applications. The aim of the\nmethod proposed in this paper is to learn the hashing function for semi-paired\ncross-view retrieval tasks. To utilize the label information of partial data,\nwe propose a semi-supervised hashing learning framework which jointly performs\nfeature extraction and classifier learning. The experimental results on two\ndatasets show that our method outperforms several state-of-the-art methods in\nterms of retrieval accuracy.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Scalability"] },
{"key": "yu2019unsupervised", "year": "2021", "citations": "9", "title":"Unsupervised Multi-modal Hashing for Cross-modal retrieval", "abstract": "<p>With the advantage of low storage cost and high efficiency, hashing learning\nhas received much attention in the domain of Big Data. In this paper, we\npropose a novel unsupervised hashing learning method to cope with this open\nproblem to directly preserve the manifold structure by hashing. To address this\nproblem, both the semantic correlation in textual space and the locally\ngeometric structure in the visual space are explored simultaneously in our\nframework. Besides, the `2;1-norm constraint is imposed on the projection\nmatrices to learn the discriminative hash function for each modality. Extensive\nexperiments are performed to evaluate the proposed method on the three publicly\navailable datasets and the experimental results show that our method can\nachieve superior performance over the state-of-the-art methods.</p>\n", "tags": ["Supervised","Multimodal-Retrieval","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Memory-Efficiency","Unsupervised","Evaluation","Efficiency"] },
{"key": "yu2020retrieval", "year": "2020", "citations": "13", "title":"Retrieval of Family Members Using Siamese Neural Network", "abstract": "<p>Retrieval of family members in the wild aims at finding family members of the\ngiven subject in the dataset, which is useful in finding the lost children and\nanalyzing the kinship. However, due to the diversity in age, gender, pose and\nillumination of the collected data, this task is always challenging. To solve\nthis problem, we propose our solution with deep Siamese neural network. Our\nsolution can be divided into two parts: similarity computation and ranking. In\ntraining procedure, the Siamese network firstly takes two candidate images as\ninput and produces two feature vectors. And then, the similarity between the\ntwo vectors is computed with several fully connected layers. While in inference\nprocedure, we try another similarity computing method by dropping the followed\nseveral fully connected layers and directly computing the cosine similarity of\nthe two feature vectors. After similarity computation, we use the ranking\nalgorithm to merge the similarity scores with the same identity and output the\nordered list according to their similarities. To gain further improvement, we\ntry different combinations of backbones, training methods and similarity\ncomputing methods. Finally, we submit the best combination as our solution and\nour team(ustc-nelslip) obtains favorable result in the track3 of the RFIW2020\nchallenge with the first runner-up, which verifies the effectiveness of our\nmethod. Our code is available at: https://github.com/gniknoil/FG2020-kinship</p>\n", "tags": ["Distance-Metric-Learning","Datasets"] },
{"key": "yu2021deep", "year": "2021", "citations": "136", "title":"Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data’s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data’s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["Datasets","Evaluation","Quantization","AAAI","Hashing-Methods","Unsupervised"] },
{"key": "yu2021improving", "year": "2021", "citations": "44", "title":"Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback", "abstract": "<p>Dense retrieval systems conduct first-stage retrieval using embedded\nrepresentations and simple similarity metrics to match a query to documents.\nIts effectiveness depends on encoded embeddings to capture the semantics of\nqueries and documents, a challenging task due to the shortness and ambiguity of\nsearch queries. This paper proposes ANCE-PRF, a new query encoder that uses\npseudo relevance feedback (PRF) to improve query representations for dense\nretrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top\nretrieved documents from a dense retrieval model, ANCE, and it learns to\nproduce better query embeddings directly from relevance labels. It also keeps\nthe document index unchanged to reduce overhead. ANCE-PRF significantly\noutperforms ANCE and other recent dense retrieval systems on several datasets.\nAnalysis shows that the PRF encoder effectively captures the relevant and\ncomplementary information from PRF documents, while ignoring the noise with its\nlearned attention mechanism.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","CIKM"] },
{"key": "yu2022live", "year": "2023", "citations": "5", "title":"Live Laparoscopic Video Retrieval with Compressed Uncertainty", "abstract": "<p>Searching through large volumes of medical data to retrieve relevant\ninformation is a challenging yet crucial task for clinical care. However the\nprimitive and most common approach to retrieval, involving text in the form of\nkeywords, is severely limited when dealing with complex media formats.\nContent-based retrieval offers a way to overcome this limitation, by using rich\nmedia as the query itself. Surgical video-to-video retrieval in particular is a\nnew and largely unexplored research problem with high clinical value,\nespecially in the real-time case: using real-time video hashing, search can be\nachieved directly inside of the operating room. Indeed, the process of hashing\nconverts large data entries into compact binary arrays or hashes, enabling\nlarge-scale search operations at a very fast rate. However, due to fluctuations\nover the course of a video, not all bits in a given hash are equally reliable.\nIn this work, we propose a method capable of mitigating this uncertainty while\nmaintaining a light computational footprint. We present superior retrieval\nresults (3-4 % top 10 mean average precision) on a multi-task evaluation\nprotocol for surgery, using cholecystectomy phases, bypass phases, and coming\nfrom an entirely new dataset introduced here, critical events across six\ndifferent surgery types. Success on this multi-task benchmark shows the\ngeneralizability of our approach for surgical video retrieval.</p>\n", "tags": ["Hashing-Methods","Datasets","Video-Retrieval","Scalability","Large-Scale-Search","Evaluation","Efficiency"] },
{"key": "yu2023learning", "year": "2015", "citations": "8", "title":"Learning cross space mapping via DNN using large scale click-through logs", "abstract": "<p>The gap between low-level visual signals and high-level semantics has been\nprogressively bridged by continuous development of deep neural network (DNN).\nWith recent progress of DNN, almost all image classification tasks have\nachieved new records of accuracy. To extend the ability of DNN to image\nretrieval tasks, we proposed a unified DNN model for image-query similarity\ncalculation by simultaneously modeling image and query in one network. The\nunified DNN is named the cross space mapping (CSM) model, which contains two\nparts, a convolutional part and a query-embedding part. The image and query are\nmapped to a common vector space via these two parts respectively, and\nimage-query similarity is naturally defined as an inner product of their\nmappings in the space. To ensure good generalization ability of the DNN, we\nlearn weights of the DNN from a large number of click-through logs which\nconsists of 23 million clicked image-query pairs between 1 million images and\n11.7 million queries. Both the qualitative results and quantitative results on\nan image retrieval evaluation task with 1000 queries demonstrate the\nsuperiority of the proposed method.</p>\n", "tags": ["Evaluation","Image-Retrieval"] },
{"key": "yu2025circulant", "year": "2014", "citations": "132", "title":"Circulant Binary Embedding", "abstract": "<p>Binary embedding of high-dimensional data requires\nlong codes to preserve the discriminative\npower of the input space. Traditional binary coding\nmethods often suffer from very high computation\nand storage costs in such a scenario. To\naddress this problem, we propose Circulant Binary\nEmbedding (CBE) which generates binary\ncodes by projecting the data with a circulant matrix.\nThe circulant structure enables the use of\nFast Fourier Transformation to speed up the computation.\nCompared to methods that use unstructured\nmatrices, the proposed method improves\nthe time complexity from O(d^2\n) to O(d log d),\nand the space complexity from O(d^2) to O(d)\nwhere d is the input dimensionality. We also\npropose a novel time-frequency alternating optimization\nto learn data-dependent circulant projections,\nwhich alternatively minimizes the objective\nin original and Fourier domains. We show\nby extensive experiments that the proposed approach\ngives much better performance than the\nstate-of-the-art approaches for fixed time, and\nprovides much faster computation with no performance\ndegradation for fixed number of bits.</p>\n", "tags": ["Hashing-Methods","Evaluation","Memory-Efficiency"] },
{"key": "yu2025deep", "year": "2021", "citations": "136", "title":"Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data’s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data’s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["Datasets","Evaluation","Quantization","AAAI","Hashing-Methods","Unsupervised"] },
{"key": "yuan2019central", "year": "2020", "citations": "288", "title":"Central Similarity Quantization for Efficient Image and Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn hash functions from\npairwise or triplet data relationships, which only capture the data similarity\nlocally, and often suffer from low learning efficiency and low collision rate.\nIn this work, we propose a new <em>global</em> similarity metric, termed as\n<em>central similarity</em>, with which the hash codes of similar data pairs are\nencouraged to approach a common center and those for dissimilar pairs to\nconverge to different centers, to improve hash learning efficiency and\nretrieval accuracy. We principally formulate the computation of the proposed\ncentral similarity metric by introducing a new concept, i.e., <em>hash\ncenter</em> that refers to a set of data points scattered in the Hamming space with\na sufficient mutual distance between each other. We then provide an efficient\nmethod to construct well separated hash centers by leveraging the Hadamard\nmatrix and Bernoulli distributions. Finally, we propose the Central Similarity\nQuantization (CSQ) that optimizes the central similarity between data points\nw.r.t.\\ their hash centers instead of optimizing the local similarity. CSQ is\ngeneric and applicable to both image and video hashing scenarios. Extensive\nexperiments on large-scale image and video retrieval tasks demonstrate that CSQ\ncan generate cohesive hash codes for similar data pairs and dispersed hash\ncodes for dissimilar pairs, achieving a noticeable boost in retrieval\nperformance, i.e. 3%-20% in mAP over the previous state-of-the-arts. The code\nis at: https://github.com/yuanli2333/Hadamard-Matrix-for-hashing</p>\n", "tags": ["Hashing-Methods","Distance-Metric-Learning","Quantization","Video-Retrieval","CVPR","Scalability","Evaluation","Efficiency"] },
{"key": "yuan2020central", "year": "2020", "citations": "288", "title":"Central Similarity Hashing for Efficient Image and Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn\nhash functions from the pairwise or triplet data relationships, which only capture the data similarity locally, and\noften suffer low learning efficiency and low collision rate.\nIn this work, we propose a new global similarity metric,\ntermed as central similarity, with which the hash codes for\nsimilar data pairs are encouraged to approach a common\ncenter and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e. hash center that refers to a set\nof data points scattered in the Hamming space with sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash\ncenters by leveraging the Hadamard matrix and Bernoulli\ndistributions. Finally, we propose the Central Similarity\nHashing (CSH) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The CSH is generic and applicable to both image and video hashing. Extensive experiments on large-scale image and video retrieval demonstrate CSH can generate cohesive hash codes for similar\ndata pairs and dispersed hash codes for dissimilar pairs,\nand achieve noticeable boost in retrieval performance, i.e.\n3%-20% in mAP over the previous state-of-the-art. The\ncodes are in: https://github.com/yuanli2333/\nHadamard-Matrix-for-hashing</p>\n", "tags": ["Video-Retrieval","Scalability","Efficiency","Distance-Metric-Learning","CVPR","Hashing-Methods","Evaluation"] },
{"key": "yuan2021multimodal", "year": "2021", "citations": "125", "title":"Multimodal Contrastive Training for Visual Representation Learning", "abstract": "<p>We develop an approach to learning visual representations that embraces\nmultimodal data, driven by a combination of intra- and inter-modal similarity\npreservation objectives. Unlike existing visual pre-training methods, which\nsolve a proxy prediction task in a single domain, our method exploits intrinsic\ndata properties within each modality and semantic information from cross-modal\ncorrelation simultaneously, hence improving the quality of learned visual\nrepresentations. By including multimodal training in a unified framework with\ndifferent types of contrastive losses, our method can learn more powerful and\ngeneric visual features. We first train our model on COCO and evaluate the\nlearned visual representations on various downstream tasks including image\nclassification, object detection, and instance segmentation. For example, the\nvisual representations pre-trained on COCO by our method achieve\nstate-of-the-art top-1 validation accuracy of \\(55.3%\\) on ImageNet\nclassification, under the common transfer protocol. We also evaluate our method\non the large-scale Stock images dataset and show its effectiveness on\nmulti-label image tagging, and cross-modal retrieval tasks.</p>\n", "tags": ["Tools-&-Libraries","Distance-Metric-Learning","Datasets","CVPR","Scalability","Multimodal-Retrieval"] },
{"key": "yuan2022exploring", "year": "2021", "citations": "110", "title":"Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval", "abstract": "<p>Remote sensing (RS) cross-modal text-image retrieval has attracted extensive\nattention for its advantages of flexible input and efficient query. However,\ntraditional methods ignore the characteristics of multi-scale and redundant\ntargets in RS image, leading to the degradation of retrieval accuracy. To cope\nwith the problem of multi-scale scarcity and target redundancy in RS multimodal\nretrieval task, we come up with a novel asymmetric multimodal feature matching\nnetwork (AMFMN). Our model adapts to multi-scale feature inputs, favors\nmulti-source retrieval methods, and can dynamically filter redundant features.\nAMFMN employs the multi-scale visual self-attention (MVSA) module to extract\nthe salient features of RS image and utilizes visual features to guide the text\nrepresentation. Furthermore, to alleviate the positive samples ambiguity caused\nby the strong intraclass similarity in RS image, we propose a triplet loss\nfunction with dynamic variable margin based on prior similarity of sample\npairs. Finally, unlike the traditional RS image-text dataset with coarse text\nand higher intraclass similarity, we construct a fine-grained and more\nchallenging Remote sensing Image-Text Match dataset (RSITMD), which supports RS\nimage retrieval through keywords and sentence separately and jointly.\nExperiments on four RS text-image datasets demonstrate that the proposed model\ncan achieve state-of-the-art performance in cross-modal RS text-image retrieval\ntask.</p>\n", "tags": ["Distance-Metric-Learning","Datasets","Evaluation","Image-Retrieval"] },
{"key": "yuan2023semantic", "year": "2023", "citations": "13", "title":"Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval", "abstract": "<p>Deep hashing has been intensively studied and successfully applied in\nlarge-scale image retrieval systems due to its efficiency and effectiveness.\nRecent studies have recognized that the existence of adversarial examples poses\na security threat to deep hashing models, that is, adversarial vulnerability.\nNotably, it is challenging to efficiently distill reliable semantic\nrepresentatives for deep hashing to guide adversarial learning, and thereby it\nhinders the enhancement of adversarial robustness of deep hashing-based\nretrieval models. Moreover, current researches on adversarial training for deep\nhashing are hard to be formalized into a unified minimax structure. In this\npaper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the\nadversarial robustness of deep hashing models. Specifically, we conceive a\ndiscriminative mainstay features learning (DMFL) scheme to construct semantic\nrepresentatives for guiding adversarial learning in deep hashing. Particularly,\nour DMFL with the strict theoretical guarantee is adaptively optimized in a\ndiscriminative learning manner, where both discriminative and semantic\nproperties are jointly considered. Moreover, adversarial examples are\nfabricated by maximizing the Hamming distance between the hash codes of\nadversarial samples and mainstay features, the efficacy of which is validated\nin the adversarial attack trials. Further, we, for the first time, formulate\nthe formalized adversarial training of deep hashing into a unified minimax\noptimization under the guidance of the generated mainstay codes. Extensive\nexperiments on benchmark datasets show superb attack performance against the\nstate-of-the-art algorithms, meanwhile, the proposed adversarial training can\neffectively eliminate adversarial perturbations for trustworthy deep\nhashing-based retrieval. Our code is available at\nhttps://github.com/xandery-geek/SAAT.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Efficiency","Scalability","Evaluation","Robustness"] },
{"key": "yuan2025central", "year": "2020", "citations": "288", "title":"Central Similarity Quantization for Efficient Image and Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships, which only capture the data similarity locally, and often suffer from low learning efficiency and low collision rate. In this work, we propose a new global similarity metric, termed as central similarity, with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e., hash center that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally, we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs, achieving a noticeable boost in retrieval performance, i.e. 3%-20% in mAP over the previous state-of-the-arts.</p>\n", "tags": ["Video-Retrieval","Scalability","Efficiency","Distance-Metric-Learning","CVPR","Hashing-Methods","Evaluation","Quantization"] },
{"key": "zadeh2012dimension", "year": "2013", "citations": "53", "title":"Dimension Independent Similarity Computation", "abstract": "<p>We present a suite of algorithms for Dimension Independent Similarity\nComputation (DISCO) to compute all pairwise similarities between very high\ndimensional sparse vectors. All of our results are provably independent of\ndimension, meaning apart from the initial cost of trivially reading in the\ndata, all subsequent operations are independent of the dimension, thus the\ndimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard\nsimilarity measures. For Jaccard similiarity we include an improved version of\nMinHash. Our results are geared toward the MapReduce framework. We empirically\nvalidate our theorems at large scale using data from the social networking site\nTwitter. At time of writing, our algorithms are live in production at\ntwitter.com.</p>\n", "tags": ["Locality-Sensitive-Hashing","Tools-&-Libraries"] },
{"key": "zemene2017large", "year": "2018", "citations": "38", "title":"Large-scale Image Geo-Localization Using Dominant Sets", "abstract": "<p>This paper presents a new approach for the challenging problem of\ngeo-locating an image using image matching in a structured database of\ncity-wide reference images with known GPS coordinates. We cast the\ngeo-localization as a clustering problem on local image features. Akin to\nexisting approaches on the problem, our framework builds on low-level features\nwhich allow partial matching between images. For each local feature in the\nquery image, we find its approximate nearest neighbors in the reference set.\nNext, we cluster the features from reference images using Dominant Set\nclustering, which affords several advantages over existing approaches. First,\nit permits variable number of nodes in the cluster which we use to dynamically\nselect the number of nearest neighbors (typically coming from multiple\nreference images) for each query feature based on its discrimination value.\nSecond, as we also quantify in our experiments, this approach is several orders\nof magnitude faster than existing approaches. Thus, we obtain multiple clusters\n(different local maximizers) and obtain a robust final solution to the problem\nusing multiple weak solutions through constrained Dominant Set clustering on\nglobal image features, where we enforce the constraint that the query image\nmust be included in the cluster. This second level of clustering also bypasses\nheuristic approaches to voting and selecting the reference image that matches\nto the query. We evaluated the proposed framework on an existing dataset of\n102k street view images as well as a new dataset of 300k images, and show that\nit outperforms the state-of-the-art by 20% and 7%, respectively, on the two\ndatasets.</p>\n", "tags": ["Datasets","Scalability","Tools-&-Libraries"] },
{"key": "zeng2019simultaneous", "year": "2019", "citations": "5", "title":"Simultaneous Region Localization and Hash Coding for Fine-grained Image Retrieval", "abstract": "<p>Fine-grained image hashing is a challenging problem due to the difficulties\nof discriminative region localization and hash code generation. Most existing\ndeep hashing approaches solve the two tasks independently. While these two\ntasks are correlated and can reinforce each other. In this paper, we propose a\ndeep fine-grained hashing to simultaneously localize the discriminative regions\nand generate the efficient binary codes. The proposed approach consists of a\nregion localization module and a hash coding module. The region localization\nmodule aims to provide informative regions to the hash coding module. The hash\ncoding module aims to generate effective binary codes and give feedback for\nlearning better localizer. Moreover, to better capture subtle differences,\nmulti-scale regions at different layers are learned without the need of\nbounding-box/part annotations. Extensive experiments are conducted on two\npublic benchmark fine-grained datasets. The results demonstrate significant\nimprovements in the performance of our method relative to other fine-grained\nhashing algorithms.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Compact-Codes","Evaluation"] },
{"key": "zhai2018classification", "year": "2018", "citations": "132", "title":"Classification is a Strong Baseline for Deep Metric Learning", "abstract": "<p>Deep metric learning aims to learn a function mapping image pixels to\nembedding feature vectors that model the similarity between images. Two major\napplications of metric learning are content-based image retrieval and face\nverification. For the retrieval tasks, the majority of current state-of-the-art\n(SOTA) approaches are triplet-based non-parametric training. For the face\nverification tasks, however, recent SOTA approaches have adopted\nclassification-based parametric training. In this paper, we look into the\neffectiveness of classification based approaches on image retrieval datasets.\nWe evaluate on several standard retrieval datasets such as CAR-196,\nCUB-200-2011, Stanford Online Product, and In-Shop datasets for image retrieval\nand clustering, and establish that our classification-based approach is\ncompetitive across different feature dimensions and base feature networks. We\nfurther provide insights into the performance effects of subsampling classes\nfor scalable classification-based training, and the effects of binarization,\nenabling efficient storage and computation for practical applications.</p>\n", "tags": ["Image-Retrieval","Distance-Metric-Learning","Datasets","Compact-Codes","Evaluation"] },
{"key": "zhai2019learning", "year": "2019", "citations": "33", "title":"Learning a Unified Embedding for Visual Search at Pinterest", "abstract": "<p>At Pinterest, we utilize image embeddings throughout our search and\nrecommendation systems to help our users navigate through visual content by\npowering experiences like browsing of related content and searching for exact\nproducts for shopping. In this work we describe a multi-task deep metric\nlearning system to learn a single unified image embedding which can be used to\npower our multiple visual search products. The solution we present not only\nallows us to train for multiple application objectives in a single deep neural\nnetwork architecture, but takes advantage of correlated information in the\ncombination of all training data from each application to generate a unified\nembedding that outperforms all specialized embeddings previously deployed for\neach product. We discuss the challenges of handling images from different\ndomains such as camera photos, high quality web images, and clean product\ncatalog images. We also detail how to jointly train for multiple product\nobjectives and how to leverage both engagement data and human labeled data. In\naddition, our trained embeddings can also be binarized for efficient storage\nand retrieval without compromising precision and recall. Through comprehensive\nevaluations on offline metrics, user studies, and online A/B experiments, we\ndemonstrate that our proposed unified embedding improves both relevance and\nengagement of our visual search products for both browsing and searching\npurposes when compared to existing specialized embeddings. Finally, the\ndeployment of the unified embedding at Pinterest has drastically reduced the\noperation and engineering cost of maintaining multiple embeddings while\nimproving quality.</p>\n", "tags": ["Image-Retrieval","Recommender-Systems","Compact-Codes","KDD","Evaluation"] },
{"key": "zhan2020weakly", "year": "2021", "citations": "9", "title":"Weakly-Supervised Online Hashing", "abstract": "<p>With the rapid development of social websites, recent years have witnessed an\nexplosive growth of social images with user-provided tags which continuously\narrive in a streaming fashion. Due to the fast query speed and low storage\ncost, hashing-based methods for image search have attracted increasing\nattention. However, existing hashing methods for social image retrieval are\nbased on batch mode which violates the nature of social images, i.e., social\nimages are usually generated periodically or collected in a stream fashion.\nAlthough there exist many online image hashing methods, they either adopt\nunsupervised learning which ignore the relevant tags, or are designed in the\nsupervised manner which needs high-quality labels. In this paper, to overcome\nthe above limitations, we propose a new method named Weakly-supervised Online\nHashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak\nsupervision by considering the semantics of tags and removing the noise.\nBesides, We develop a discrete online optimization algorithm for WOH, which is\nefficient and scalable. Extensive experiments conducted on two real-world\ndatasets demonstrate the superiority of WOH compared with several\nstate-of-the-art hashing baselines.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Unsupervised"] },
{"key": "zhan2021jointly", "year": "2021", "citations": "58", "title":"Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance", "abstract": "<p>Recently, Information Retrieval community has witnessed fast-paced advances\nin Dense Retrieval (DR), which performs first-stage retrieval with\nembedding-based search. Despite the impressive ranking performance, previous\nstudies usually adopt brute-force search to acquire candidates, which is\nprohibitive in practical Web search scenarios due to its tremendous memory\nusage and time cost. To overcome these problems, vector compression methods\nhave been adopted in many practical embedding-based retrieval applications. One\nof the most popular methods is Product Quantization (PQ). However, although\nexisting vector compression methods including PQ can help improve the\nefficiency of DR, they incur severely decayed retrieval performance due to the\nseparation between encoding and compression. To tackle this problem, we present\nJPQ, which stands for Joint optimization of query encoding and Product\nQuantization. It trains the query encoder and PQ index jointly in an end-to-end\nmanner based on three optimization strategies, namely ranking-oriented loss, PQ\ncentroid optimization, and end-to-end negative sampling. We evaluate JPQ on two\npublicly available retrieval benchmarks. Experimental results show that JPQ\nsignificantly outperforms popular vector compression methods. Compared with\nprevious DR models that use brute-force search, JPQ almost matches the best\nretrieval performance with 30x compression on index size. The compressed index\nfurther brings 10x speedup on CPU and 2x speedup on GPU in query latency.</p>\n", "tags": ["Evaluation","Efficiency","Quantization","CIKM"] },
{"key": "zhan2021learning", "year": "2022", "citations": "32", "title":"Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval", "abstract": "<p>Dense Retrieval (DR) has achieved state-of-the-art first-stage ranking\neffectiveness. However, the efficiency of most existing DR models is limited by\nthe large memory cost of storing dense vectors and the time-consuming nearest\nneighbor search (NNS) in vector space. Therefore, we present RepCONC, a novel\nretrieval model that learns discrete Representations via CONstrained\nClustering. RepCONC jointly trains dual-encoders and the Product Quantization\n(PQ) method to learn discrete document representations and enables fast\napproximate NNS with compact indexes. It models quantization as a constrained\nclustering process, which requires the document embeddings to be uniformly\nclustered around the quantization centroids and supports end-to-end\noptimization of the quantization method and dual-encoders. We theoretically\ndemonstrate the importance of the uniform clustering constraint in RepCONC and\nderive an efficient approximate solution for constrained clustering by reducing\nit to an instance of the optimal transport problem. Besides constrained\nclustering, RepCONC further adopts a vector-based inverted file system (IVF) to\nsupport highly efficient vector search on CPUs. Extensive experiments on two\npopular ad-hoc retrieval benchmarks show that RepCONC achieves better ranking\neffectiveness than competitive vector quantization baselines under different\ncompression ratio settings. It also substantially outperforms a wide range of\nexisting retrieval models in terms of retrieval effectiveness, memory\nefficiency, and time efficiency.</p>\n", "tags": ["Quantization","Vector-Indexing","Efficiency"] },
{"key": "zhang2010self", "year": "2010", "citations": "354", "title":"Self-Taught Hashing for Fast Similarity Search", "abstract": "<p>The ability of fast similarity search at large scale is of great\nimportance to many Information Retrieval (IR) applications.\nA promising way to accelerate similarity search is semantic\nhashing which designs compact binary codes for a large number\nof documents so that semantically similar documents\nare mapped to similar codes (within a short Hamming distance).\nAlthough some recently proposed techniques are\nable to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents\nremains to be a very challenging problem. In this\npaper, we emphasise this issue and propose a novel SelfTaught\nHashing (STH) approach to semantic hashing: we\nfirst find the optimal l-bit binary codes for all documents in\nthe given corpus via unsupervised learning, and then train\nl classifiers via supervised learning to predict the l-bit code\nfor any query document unseen before. Our experiments on\nthree real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and\nlinear Support Vector Machine (SVM) outperforms stateof-the-art\ntechniques significantly.</p>\n", "tags": ["Datasets","Text-Retrieval","SIGIR","Supervised","Compact-Codes","Similarity-Search","Hashing-Methods","Unsupervised"] },
{"key": "zhang2011composite", "year": "2011", "citations": "221", "title":"Composite Hashing with Multiple Information Sources", "abstract": "<p>Similarity search applications with a large amount of text\nand image data demands an efficient and effective solution.\nOne useful strategy is to represent the examples in databases\nas compact binary codes through semantic hashing, which\nhas attracted much attention due to its fast query/search\nspeed and drastically reduced storage requirement. All of\nthe current semantic hashing methods only deal with the\ncase when each example is represented by one type of features.\nHowever, examples are often described from several\ndifferent information sources in many real world applications.\nFor example, the characteristics of a webpage can be\nderived from both its content part and its associated links.\nTo address the problem of learning good hashing codes in\nthis scenario, we propose a novel research problem – Composite\nHashing with Multiple Information Sources (CHMIS).\nThe focus of the new research problem is to design an algorithm\nfor incorporating the features from different information\nsources into the binary hashing codes efficiently and\neffectively. In particular, we propose an algorithm CHMISAW\n(CHMIS with Adjusted Weights) for learning the codes.\nThe proposed algorithm integrates information from several\ndifferent sources into the binary hashing codes by adjusting\nthe weights on each individual source for maximizing\nthe coding performance, and enables fast conversion from\nquery examples to their binary hashing codes. Experimental\nresults on five different datasets demonstrate the superior\nperformance of the proposed method against several other\nstate-of-the-art semantic hashing techniques.</p>\n", "tags": ["Datasets","Text-Retrieval","SIGIR","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "zhang2013binary", "year": "2013", "citations": "98", "title":"Binary Code Ranking with Weighted Hamming Distance", "abstract": "<p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most\nexisting binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or\nsimilarity of two points are approximated by the Hamming\ndistance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are\noften lots of results sharing the same Hamming distance to\na query, which makes this distance measure ambiguous and\nposes a critical issue for similarity search where ranking is\nimportant. In this paper, we propose a weighted Hamming\ndistance ranking algorithm (WhRank) to rank the binary\ncodes of hashing methods. By assigning different bit-level\nweights to different hash bits, the returned binary codes\nare ranked at a finer-grained binary code level. We give\nan algorithm to learn the data-adaptive and query-sensitive\nweight for each hash bit. Evaluations on two large-scale\nimage data sets demonstrate the efficacy of our weighted\nHamming distance for binary code ranking.</p>\n", "tags": ["Scalability","Efficiency","CVPR","Compact-Codes","Similarity-Search","Hashing-Methods"] },
{"key": "zhang2014large", "year": "2014", "citations": "619", "title":"Large-scale supervised multimodal hashing with semantic correlation maximization", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for similarity search in multimedia\ndata. In particular, more and more attentions\nhave been payed to multimodal hashing for search in\nmultimedia data with multiple modalities, such as images\nwith tags. Typically, supervised information of semantic\nlabels is also available for the data points in\nmany real applications. Hence, many supervised multimodal\nhashing (SMH) methods have been proposed\nto utilize such semantic labels to further improve the\nsearch accuracy. However, the training time complexity\nof most existing SMH methods is too high, which\nmakes them unscalable to large-scale datasets. In this\npaper, a novel SMH method, called semantic correlation\nmaximization (SCM), is proposed to seamlessly integrate\nsemantic labels into the hashing learning procedure\nfor large-scale data modeling. Experimental results\non two real-world datasets show that SCM can signifi-\ncantly outperform the state-of-the-art SMH methods, in\nterms of both accuracy and scalability.</p>\n", "tags": ["Scalability","Datasets","Memory-Efficiency","Similarity-Search","Hashing-Methods","AAAI","Supervised"] },
{"key": "zhang2014supervised", "year": "2014", "citations": "286", "title":"Supervised Hashing with Latent Factor Models", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for approximate nearest neighbor\nsearch in large-scale datasets. Traditional hashing methods\ntry to learn the hash codes in an unsupervised way where\nthe metric (Euclidean) structure of the training data is preserved.\nVery recently, supervised hashing methods, which\ntry to preserve the semantic structure constructed from the\nsemantic labels of the training points, have exhibited higher\naccuracy than unsupervised methods. In this paper, we\npropose a novel supervised hashing method, called latent\nfactor hashing (LFH), to learn similarity-preserving binary\ncodes based on latent factor models. An algorithm with\nconvergence guarantee is proposed to learn the parameters\nof LFH. Furthermore, a linear-time variant with stochastic\nlearning is proposed for training LFH on large-scale datasets.\nExperimental results on two large datasets with semantic\nlabels show that LFH can achieve superior accuracy than\nstate-of-the-art methods with comparable training time.</p>\n", "tags": ["Scalability","Unsupervised","Datasets","Neural-Hashing","Memory-Efficiency","SIGIR","Hashing-Methods","Supervised"] },
{"key": "zhang2015bit", "year": "2015", "citations": "418", "title":"Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification", "abstract": "<p>Extracting informative image features and learning\neffective approximate hashing functions are two crucial steps in\nimage retrieval . Conventional methods often study these two\nsteps separately, e.g., learning hash functions from a predefined\nhand-crafted feature space. Meanwhile, the bit lengths of output\nhashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised\nlearning framework to generate compact and bit-scalable hashing\ncodes directly from raw images. We pose hashing learning as\na problem of regularized similarity learning. Specifically, we\norganize the training images into a batch of triplet samples,\neach sample containing two images with the same label and one\nwith a different label. With these triplet samples, we maximize\nthe margin between matched pairs and mismatched pairs in the\nHamming space. In addition, a regularization term is introduced\nto enforce the adjacency consistency, i.e., images of similar\nappearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end\nfashion, where discriminative image features and hash functions\nare simultaneously optimized. Furthermore, each bit of our\nhashing codes is unequally weighted so that we can manipulate\nthe code lengths by truncating the insignificant bits. Our\nframework outperforms state-of-the-arts on public benchmarks\nof similar image search and also achieves promising results in\nthe application of person re-identification in surveillance. It is\nalso shown that the generated bit-scalable hashing codes well\npreserve the discriminative powers with shorter code lengths.</p>\n", "tags": ["Image-Retrieval","Neural-Hashing","Tools-&-Libraries","Hashing-Methods","Supervised"] },
{"key": "zhang2016efficient", "year": "2016", "citations": "112", "title":"Efficient Training of Very Deep Neural Networks for Supervised Hashing", "abstract": "<p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively “shallow” networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>\n", "tags": ["Efficiency","Datasets","CVPR","Neural-Hashing","Hashing-Methods","Evaluation","Supervised"] },
{"key": "zhang2016query", "year": "2018", "citations": "55", "title":"Query-adaptive Image Retrieval by Deep Weighted Hashing", "abstract": "<p>Hashing methods have attracted much attention for large scale image\nretrieval. Some deep hashing methods have achieved promising results by taking\nadvantage of the strong representation power of deep networks recently.\nHowever, existing deep hashing methods treat all hash bits equally. On one\nhand, a large number of images share the same distance to a query image due to\nthe discrete Hamming distance, which raises a critical issue of image retrieval\nwhere fine-grained rankings are very important. On the other hand, different\nhash bits actually contribute to the image retrieval differently, and treating\nthem equally greatly affects the retrieval accuracy of image. To address the\nabove two problems, we propose the query-adaptive deep weighted hashing (QaDWH)\napproach, which can perform fine-grained ranking for different queries by\nweighted Hamming distance. First, a novel deep hashing network is proposed to\nlearn the hash codes and corresponding class-wise weights jointly, so that the\nlearned weights can reflect the importance of different hash bits for different\nimage classes. Second, a query-adaptive image retrieval method is proposed,\nwhich rapidly generates hash bit weights for different query images by fusing\nits semantic probability and the learned class-wise weights. Fine-grained image\nretrieval is then performed by the weighted Hamming distance, which can provide\nmore accurate ranking than the traditional Hamming distance. Experiments on\nfour widely used datasets show that the proposed approach outperforms eight\nstate-of-the-art hashing methods.</p>\n", "tags": ["Hashing-Methods","Datasets","Neural-Hashing","Image-Retrieval"] },
{"key": "zhang2017effective", "year": "2019", "citations": "24", "title":"Effective Image Retrieval via Multilinear Multi-index Fusion", "abstract": "<p>Multi-index fusion has demonstrated impressive performances in retrieval task\nby integrating different visual representations in a unified framework.\nHowever, previous works mainly consider propagating similarities via neighbor\nstructure, ignoring the high order information among different visual\nrepresentations. In this paper, we propose a new multi-index fusion scheme for\nimage retrieval. By formulating this procedure as a multilinear based\noptimization problem, the complementary information hidden in different indexes\ncan be explored more thoroughly. Specially, we first build our multiple indexes\nfrom various visual representations. Then a so-called index-specific functional\nmatrix, which aims to propagate similarities, is introduced for updating the\noriginal index. The functional matrices are then optimized in a unified tensor\nspace to achieve a refinement, such that the relevant images can be pushed more\ncloser. The optimization problem can be efficiently solved by the augmented\nLagrangian method with theoretical convergence guarantee. Unlike the\ntraditional multi-index fusion scheme, our approach embeds the multi-index\nsubspace structure into the new indexes with sparse constraint, thus it has\nlittle additional memory consumption in online query stage. Experimental\nevaluation on three benchmark datasets reveals that the proposed approach\nachieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP\n94.1% on Holiday and 62.39% on Market-1501.</p>\n", "tags": ["Vector-Indexing","Tools-&-Libraries","Image-Retrieval","Datasets","Evaluation"] },
{"key": "zhang2017hashgan", "year": "2017", "citations": "16", "title":"HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal Retrieval", "abstract": "<p>As the rapid growth of multi-modal data, hashing methods for cross-modal\nretrieval have received considerable attention. Deep-networks-based cross-modal\nhashing methods are appealing as they can integrate feature learning and hash\ncoding into end-to-end trainable frameworks. However, it is still challenging\nto find content similarities between different modalities of data due to the\nheterogeneity gap. To further address this problem, we propose an adversarial\nhashing network with attention mechanism to enhance the measurement of content\nsimilarities by selectively focusing on informative parts of multi-modal data.\nThe proposed new adversarial network, HashGAN, consists of three building\nblocks: 1) the feature learning module to obtain feature representations, 2)\nthe generative attention module to generate an attention mask, which is used to\nobtain the attended (foreground) and the unattended (background) feature\nrepresentations, 3) the discriminative hash coding module to learn hash\nfunctions that preserve the similarities between different modalities. In our\nframework, the generative module and the discriminative module are trained in\nan adversarial way: the generator is learned to make the discriminator cannot\npreserve the similarities of multi-modal data w.r.t. the background feature\nrepresentations, while the discriminator aims to preserve the similarities of\nmulti-modal data w.r.t. both the foreground and the background feature\nrepresentations. Extensive evaluations on several benchmark datasets\ndemonstrate that the proposed HashGAN brings substantial improvements over\nother state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Evaluation","Robustness"] },
{"key": "zhang2017unsupervised", "year": "2018", "citations": "203", "title":"Unsupervised Generative Adversarial Cross-modal Hashing", "abstract": "<p>Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Unsupervised cross-modal hashing is more flexible and applicable\nthan supervised methods, since no intensive labeling work is involved. However,\nexisting unsupervised methods learn hashing functions by preserving inter and\nintra correlations, while ignoring the underlying manifold structure across\ndifferent modalities, which is extremely helpful to capture meaningful nearest\nneighbors of different modalities for cross-modal retrieval. To address the\nabove problem, in this paper we propose an Unsupervised Generative Adversarial\nCross-modal Hashing approach (UGACH), which makes full use of GAN’s ability for\nunsupervised representation learning to exploit the underlying manifold\nstructure of cross-modal data. The main contributions can be summarized as\nfollows: (1) We propose a generative adversarial network to model cross-modal\nhashing in an unsupervised fashion. In the proposed UGACH, given a data of one\nmodality, the generative model tries to fit the distribution over the manifold\nstructure, and select informative data of another modality to challenge the\ndiscriminative model. The discriminative model learns to distinguish the\ngenerated data and the true positive data sampled from correlation graph to\nachieve better retrieval accuracy. These two models are trained in an\nadversarial way to improve each other and promote hashing function learning.\n(2) We propose a correlation graph based approach to capture the underlying\nmanifold structure across different modalities, so that data of different\nmodalities but within the same manifold can have smaller Hamming distance and\npromote retrieval accuracy. Extensive experiments compared with 6\nstate-of-the-art methods verify the effectiveness of our proposed approach.</p>\n", "tags": ["Supervised","Multimodal-Retrieval","Hashing-Methods","AAAI","Unsupervised","Evaluation","Robustness"] },
{"key": "zhang2018improved", "year": "2019", "citations": "144", "title":"Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval", "abstract": "<p>Hash coding has been widely used in the approximate nearest neighbor search\nfor large-scale image retrieval. Recently, many deep hashing methods have been\nproposed and shown largely improved performance over traditional\nfeature-learning-based methods. Most of these methods examine the pairwise\nsimilarity on the semantic-level labels, where the pairwise similarity is\ngenerally defined in a hard-assignment way. That is, the pairwise similarity is\n‘1’ if they share no less than one class label and ‘0’ if they do not share\nany. However, such similarity definition cannot reflect the similarity ranking\nfor pairwise images that hold multiple labels. In this paper, a new deep\nhashing method is proposed for multi-label image retrieval by re-defining the\npairwise similarity into an instance similarity, where the instance similarity\nis quantified into a percentage based on the normalized semantic labels. Based\non the instance similarity, a weighted cross-entropy loss and a minimum mean\nsquare error loss are tailored for loss-function construction, and are\nefficiently used for simultaneous feature learning and hash coding. Experiments\non three popular datasets demonstrate that, the proposed method outperforms the\ncompeting methods and achieves the state-of-the-art performance in multi-label\nimage retrieval.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Scalability","Evaluation"] },
{"key": "zhang2018semantic", "year": "2019", "citations": "13", "title":"Semantic Cluster Unary Loss for Efficient Deep Hashing", "abstract": "<p>Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, which has received a broad attention due to its low storage cost and\nfast retrieval speed. With the rapid development of deep learning, deep hashing\nmethods have achieved promising results in efficient information retrieval.\nMost of the existing deep hashing methods adopt pairwise or triplet losses to\ndeal with similarities underlying the data, but the training is difficult and\nless efficient because \\(O(n^2)\\) data pairs and \\(O(n^3)\\) triplets are involved.\nTo address these issues, we propose a novel deep hashing algorithm with unary\nloss which can be trained very efficiently. We first of all introduce a Unary\nUpper Bound of the traditional triplet loss, thus reducing the complexity to\n\\(O(n)\\) and bridging the classification-based unary loss and the triplet loss.\nSecond, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by\nintroducing a modified Unary Upper Bound loss, named Semantic Cluster Unary\nLoss (SCUL). The resultant hashcodes form several compact clusters, which means\nhashcodes in the same cluster have similar semantic information. We also\ndemonstrate that the proposed SCDH is easy to be extended to semi-supervised\nsettings by incorporating the state-of-the-art semi-supervised learning\nalgorithms. Experiments on large-scale datasets show that the proposed method\nis superior to state-of-the-art hashing algorithms.</p>\n", "tags": ["Supervised","Hashing-Methods","Distance-Metric-Learning","Datasets","Neural-Hashing","Memory-Efficiency","Scalability","Efficiency"] },
{"key": "zhang2019collaborative", "year": "2016", "citations": "62", "title":"Collaborative Quantization for Cross-Modal Similarity Search", "abstract": "<p>Cross-modal similarity search is a problem about designing a search system\nsupporting querying across content modalities, e.g., using an image to search\nfor texts or using a text to search for images. This paper presents a compact\ncoding solution for efficient search, with a focus on the quantization approach\nwhich has already shown the superior performance over the hashing solutions in\nthe single-modal similarity search. We propose a cross-modal quantization\napproach, which is among the early attempts to introduce quantization into\ncross-modal search. The major contribution lies in jointly learning the\nquantizers for both modalities through aligning the quantized representations\nfor each pair of image and text belonging to a document. In addition, our\napproach simultaneously learns the common space for both modalities in which\nquantization is conducted to enable efficient and effective search using the\nEuclidean distance computed in the common space with fast distance table\nlookup. Experimental results compared with several competitive algorithms over\nthree benchmark datasets demonstrate that the proposed approach achieves the\nstate-of-the-art performance.</p>\n", "tags": ["Similarity-Search","Hashing-Methods","Distance-Metric-Learning","Datasets","CVPR","Quantization","Evaluation"] },
{"key": "zhang2019generic", "year": "2019", "citations": "33", "title":"Generic Intent Representation in Web Search", "abstract": "<p>This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a\ndistributed representation space for user intent in search. Leveraging large\nscale user clicks from Bing search logs as weak supervision of user intent, GEN\nEncoder learns to map queries with shared clicks into similar embeddings\nend-to-end and then finetunes on multiple paraphrase tasks. Experimental\nresults on an intrinsic evaluation task - query intent similarity modeling -\ndemonstrate GEN Encoder’s robust and significant advantages over previous\nrepresentation methods. Ablation studies reveal the crucial role of learning\nfrom implicit user feedback in representing user intent and the contributions\nof multi-task learning in representation generality. We also demonstrate that\nGEN Encoder alleviates the sparsity of tail search traffic and cuts down half\nof the unseen queries by using an efficient approximate nearest neighbor search\nto effectively identify previous queries with the same search intent. Finally,\nwe demonstrate distances between GEN encodings reflect certain information\nseeking behaviors in search sessions.</p>\n", "tags": ["SIGIR","Evaluation"] },
{"key": "zhang2019pairwise", "year": "2019", "citations": "7", "title":"Pairwise Teacher-Student Network for Semi-Supervised Hashing", "abstract": "<p>Hashing method maps similar high-dimensional data to binary hashcodes with\nsmaller hamming distance, and it has received broad attention due to its low\nstorage cost and fast retrieval speed. Pairwise similarity is easily obtained\nand widely used for retrieval, and most supervised hashing algorithms are\ncarefully designed for the pairwise supervisions. As labeling all data pairs is\ndifficult, semi-supervised hashing is proposed which aims at learning efficient\ncodes with limited labeled pairs and abundant unlabeled ones. Existing methods\nbuild graphs to capture the structure of dataset, but they are not working well\nfor complex data as the graph is built based on the data representations and\ndetermining the representations of complex data is difficult. In this paper, we\npropose a novel teacher-student semi-supervised hashing framework in which the\nstudent is trained with the pairwise information produced by the teacher\nnetwork. The network follows the smoothness assumption, which achieves\nconsistent distances for similar data pairs so that the retrieval results are\nsimilar for neighborhood queries. Experiments on large-scale datasets show that\nthe proposed method reaches impressive gain over the supervised baselines and\nis superior to state-of-the-art semi-supervised hashing methods.</p>\n", "tags": ["Supervised","Tools-&-Libraries","Hashing-Methods","Datasets","Neural-Hashing","Memory-Efficiency","CVPR","Scalability","Efficiency"] },
{"key": "zhang2019part", "year": "2020", "citations": "56", "title":"Part-Guided Attention Learning for Vehicle Instance Retrieval", "abstract": "<p>Vehicle instance retrieval often requires one to recognize the fine-grained\nvisual differences between vehicles. Besides the holistic appearance of\nvehicles which is easily affected by the viewpoint variation and distortion,\nvehicle parts also provide crucial cues to differentiate near-identical\nvehicles. Motivated by these observations, we introduce a Part-Guided Attention\nNetwork (PGAN) to pinpoint the prominent part regions and effectively combine\nthe global and part information for discriminative feature learning. PGAN first\ndetects the locations of different part components and salient regions\nregardless of the vehicle identity, which serve as the bottom-up attention to\nnarrow down the possible searching regions. To estimate the importance of\ndetected parts, we propose a Part Attention Module (PAM) to adaptively locate\nthe most discriminative regions with high-attention weights and suppress the\ndistraction of irrelevant parts with relatively low weights. The PAM is guided\nby the instance retrieval loss and therefore provides top-down attention that\nenables attention to be calculated at the level of car parts and other salient\nregions. Finally, we aggregate the global appearance and part features to\nimprove the feature performance further. The PGAN combines part-guided\nbottom-up and top-down attention, global and part visual features in an\nend-to-end framework. Extensive experiments demonstrate that the proposed\nmethod achieves new state-of-the-art vehicle instance retrieval performance on\nfour large-scale benchmark datasets.</p>\n", "tags": ["Datasets","Scalability","Evaluation","Tools-&-Libraries"] },
{"key": "zhang2020collaborative", "year": "2020", "citations": "6", "title":"Collaborative Generative Hashing for Marketing and Fast Cold-start Recommendation", "abstract": "<p>Cold-start has being a critical issue in recommender systems with the\nexplosion of data in e-commerce. Most existing studies proposed to alleviate\nthe cold-start problem are also known as hybrid recommender systems that learn\nrepresentations of users and items by combining user-item interactive and\nuser/item content information. However, previous hybrid methods regularly\nsuffered poor efficiency bottlenecking in online recommendations with\nlarge-scale items, because they were designed to project users and items into\ncontinuous latent space where the online recommendation is expensive. To this\nend, we propose a collaborative generated hashing (CGH) framework to improve\nthe efficiency by denoting users and items as binary codes, then fast hashing\nsearch techniques can be used to speed up the online recommendation. In\naddition, the proposed CGH can generate potential users or items for marketing\napplication where the generative network is designed with the principle of\nMinimum Description Length (MDL), which is used to learn compact and\ninformative binary codes. Extensive experiments on two public datasets show the\nadvantages for recommendations in various settings over competing baselines and\nanalyze its feasibility in marketing application.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Recommender-Systems","Compact-Codes","Scalability","Efficiency"] },
{"key": "zhang2020deep", "year": "2020", "citations": "9", "title":"Deep Pairwise Hashing for Cold-start Recommendation", "abstract": "<p>Recommendation efficiency and data sparsity problems have been regarded as\ntwo challenges of improving performance for online recommendation. Most of the\nprevious related work focus on improving recommendation accuracy instead of\nefficiency. In this paper, we propose a Deep Pairwise Hashing (DPH) to map\nusers and items to binary vectors in Hamming space, where a user’s preference\nfor an item can be efficiently calculated by Hamming distance, which\nsignificantly improves the efficiency of online recommendation. To alleviate\ndata sparsity and cold-start problems, the user-item interactive information\nand item content information are unified to learn effective representations of\nitems and users. Specifically, we first pre-train robust item representation\nfrom item content data by a Denoising Auto-encoder instead of other\ndeterministic deep learning frameworks; then we finetune the entire framework\nby adding a pairwise loss objective with discrete constraints; moreover, DPH\naims to minimize a pairwise ranking loss that is consistent with the ultimate\ngoal of recommendation. Finally, we adopt the alternating optimization method\nto optimize the proposed model with discrete constraints. Extensive experiments\non three different datasets show that DPH can significantly advance the\nstate-of-the-art frameworks regarding data sparsity and item cold-start\nrecommendation.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Recommender-Systems","Evaluation","Efficiency"] },
{"key": "zhang2020learning", "year": "2020", "citations": "23", "title":"Learning to Represent Image and Text with Denotation Graph", "abstract": "<p>Learning to fuse vision and language information and representing them is an\nimportant research problem with many applications. Recent progresses have\nleveraged the ideas of pre-training (from language modeling) and attention\nlayers in Transformers to learn representation from datasets containing images\naligned with linguistic expressions that describe the images. In this paper, we\npropose learning representations from a set of implied, visually grounded\nexpressions between image and text, automatically mined from those datasets. In\nparticular, we use denotation graphs to represent how specific concepts (such\nas sentences describing images) can be linked to abstract and generic concepts\n(such as short phrases) that are also visually grounded. This type of\ngeneric-to-specific relations can be discovered using linguistic analysis\ntools. We propose methods to incorporate such relations into learning\nrepresentation. We show that state-of-the-art multimodal learning models can be\nfurther improved by leveraging automatically harvested structural relations.\nThe representations lead to stronger empirical results on downstream tasks of\ncross-modal image retrieval, referring expression, and compositional\nattribute-object recognition. Both our codes and the extracted denotation\ngraphs on the Flickr30K and the COCO datasets are publically available on\nhttps://sha-lab.github.io/DG.</p>\n", "tags": ["Datasets","EMNLP","Image-Retrieval"] },
{"key": "zhang2020leveraging", "year": "2021", "citations": "13", "title":"Leveraging Local and Global Descriptors in Parallel to Search Correspondences for Visual Localization", "abstract": "<p>Visual localization to compute 6DoF camera pose from a given image has wide\napplications such as in robotics, virtual reality, augmented reality, etc. Two\nkinds of descriptors are important for the visual localization. One is global\ndescriptors that extract the whole feature from each image. The other is local\ndescriptors that extract the local feature from each image patch usually\nenclosing a key point. More and more methods of the visual localization have\ntwo stages: at first to perform image retrieval by global descriptors and then\nfrom the retrieval feedback to make 2D-3D point correspondences by local\ndescriptors. The two stages are in serial for most of the methods. This simple\ncombination has not achieved superiority of fusing local and global\ndescriptors. The 3D points obtained from the retrieval feedback are as the\nnearest neighbor candidates of the 2D image points only by global descriptors.\nEach of the 2D image points is also called a query local feature when\nperforming the 2D-3D point correspondences. In this paper, we propose a novel\nparallel search framework, which leverages advantages of both local and global\ndescriptors to get nearest neighbor candidates of a query local feature.\nSpecifically, besides using deep learning based global descriptors, we also\nutilize local descriptors to construct random tree structures for obtaining\nnearest neighbor candidates of the query local feature. We propose a new\nprobabilistic model and a new deep learning based local descriptor when\nconstructing the random trees. A weighted Hamming regularization term to keep\ndiscriminativeness after binarization is given in the loss function for the\nproposed local descriptor. The loss function co-trains both real and binary\ndescriptors of which the results are integrated into the random trees.</p>\n", "tags": ["Image-Retrieval","Tools-&-Libraries","CVPR"] },
{"key": "zhang2020model", "year": "2020", "citations": "31", "title":"Model Size Reduction Using Frequency Based Double Hashing for Recommender Systems", "abstract": "<p>Deep Neural Networks (DNNs) with sparse input features have been widely used\nin recommender systems in industry. These models have large memory requirements\nand need a huge amount of training data. The large model size usually entails a\ncost, in the range of millions of dollars, for storage and communication with\nthe inference services. In this paper, we propose a hybrid hashing method to\ncombine frequency hashing and double hashing techniques for model size\nreduction, without compromising performance. We evaluate the proposed models on\ntwo product surfaces. In both cases, experiment results demonstrated that we\ncan reduce the model size by around 90 % while keeping the performance on par\nwith the original baselines.</p>\n", "tags": ["Hashing-Methods","Recommender-Systems","Evaluation"] },
{"key": "zhang2021deep", "year": "2021", "citations": "21", "title":"Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval", "abstract": "<p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>\n", "tags": ["Scalability","Efficiency","Evaluation","Datasets","Compact-Codes","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","Memory-Efficiency","Image-Retrieval","CVPR"] },
{"key": "zhang2021fast", "year": "2021", "citations": "9", "title":"Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization", "abstract": "<p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>\n", "tags": ["Neural-Hashing","Multimodal-Retrieval","Quantization","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "zhang2021high", "year": "2021", "citations": "62", "title":"High-order nonlocal Hashing for unsupervised cross-modal retrieval", "abstract": "<p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>\n", "tags": ["Neural-Hashing","Multimodal-Retrieval","Compact-Codes","Hashing-Methods","Unsupervised"] },
{"key": "zhang2021improved", "year": "2021", "citations": "8", "title":"Improved Deep Classwise Hashing With Centers Similarity Learning for Image Retrieval", "abstract": "<p>Deep supervised hashing for image retrieval has attracted researchers’\nattention due to its high efficiency and superior retrieval performance. Most\nexisting deep supervised hashing works, which are based on pairwise/triplet\nlabels, suffer from the expensive computational cost and insufficient\nutilization of the semantics information. Recently, deep classwise hashing\nintroduced a classwise loss supervised by class labels information\nalternatively; however, we find it still has its drawback. In this paper, we\npropose an improved deep classwise hashing, which enables hashing learning and\nclass centers learning simultaneously. Specifically, we design a two-step\nstrategy on center similarity learning. It interacts with the classwise loss to\nattract the class center to concentrate on the intra-class samples while\npushing other class centers as far as possible. The centers similarity learning\ncontributes to generating more compact and discriminative hashing codes. We\nconduct experiments on three benchmark datasets. It shows that the proposed\nmethod effectively surpasses the original method and outperforms\nstate-of-the-art baselines under various commonly-used evaluation metrics for\nimage retrieval.</p>\n", "tags": ["Supervised","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Evaluation","Efficiency"] },
{"key": "zhang2021joint", "year": "2021", "citations": "6", "title":"Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index", "abstract": "<p>Embedding index that enables fast approximate nearest neighbor(ANN) search,\nserves as an indispensable component for state-of-the-art deep retrieval\nsystems. Traditional approaches, often separating the two steps of embedding\nlearning and index building, incur additional indexing time and decayed\nretrieval accuracy. In this paper, we propose a novel method called Poeem,\nwhich stands for product quantization based embedding index jointly trained\nwith deep retrieval model, to unify the two separate steps within an end-to-end\ntraining, by utilizing a few techniques including the gradient straight-through\nestimator, warm start strategy, optimal space decomposition and Givens\nrotation. Extensive experimental results show that the proposed method not only\nimproves retrieval accuracy significantly but also reduces the indexing time to\nalmost none. We have open sourced our approach for the sake of comparison and\nreproducibility.</p>\n", "tags": ["Quantization","SIGIR","Vector-Indexing","Evaluation"] },
{"key": "zhang2021orthonormal", "year": "2023", "citations": "9", "title":"Orthonormal Product Quantization Network for Scalable Face Image Retrieval", "abstract": "<p>Existing deep quantization methods provided an efficient solution for\nlarge-scale image retrieval. However, the significant intra-class variations\nlike pose, illumination, and expressions in face images, still pose a challenge\nfor face image retrieval. In light of this, face image retrieval requires\nsufficiently powerful learning metrics, which are absent in current deep\nquantization works. Moreover, to tackle the growing unseen identities in the\nquery stage, face image retrieval drives more demands regarding model\ngeneralization and system scalability than general image retrieval tasks. This\npaper integrates product quantization with orthonormal constraints into an\nend-to-end deep learning framework to effectively retrieve face images.\nSpecifically, a novel scheme that uses predefined orthonormal vectors as\ncodewords is proposed to enhance the quantization informativeness and reduce\ncodewords’ redundancy. A tailored loss function maximizes discriminability\namong identities in each quantization subspace for both the quantized and\noriginal features. An entropy-based regularization term is imposed to reduce\nthe quantization error. Experiments are conducted on four commonly-used face\ndatasets under both seen and unseen identities retrieval settings. Our method\noutperforms all the compared deep hashing/quantization state-of-the-arts under\nboth settings. Results validate the effectiveness of the proposed orthonormal\ncodewords in improving models’ standard retrieval performance and\ngeneralization ability. Combing with further experiments on two general image\ndatasets, it demonstrates the broad superiority of our method for scalable\nimage retrieval.</p>\n", "tags": ["Scalability","Quantization","Evaluation","Datasets","Hashing-Methods","Tools-&-Libraries","Neural-Hashing","Image-Retrieval","CVPR"] },
{"key": "zhang2021visual", "year": "2018", "citations": "58", "title":"Visual Search at Alibaba", "abstract": "<p>This paper introduces the large scale visual search algorithm and system\ninfrastructure at Alibaba. The following challenges are discussed under the\nE-commercial circumstance at Alibaba (a) how to handle heterogeneous image data\nand bridge the gap between real-shot images from user query and the online\nimages. (b) how to deal with large scale indexing for massive updating data.\n(c) how to train deep models for effective feature representation without huge\nhuman annotations. (d) how to improve the user engagement by considering the\nquality of the content. We take advantage of large image collection of Alibaba\nand state-of-the-art deep learning techniques to perform visual search at\nscale. We present solutions and implementation details to overcome those\nproblems and also share our learnings from building such a large scale\ncommercial visual search engine. Specifically, model and search-based fusion\napproach is introduced to effectively predict categories. Also, we propose a\ndeep CNN model for joint detection and feature learning by mining user click\nbehavior. The binary index engine is designed to scale up indexing without\ncompromising recall and precision. Finally, we apply all the stages into an\nend-to-end system architecture, which can simultaneously achieve highly\nefficient and scalable performance adapting to real-shot images. Extensive\nexperiments demonstrate the advancement of each module in our system. We hope\nvisual search at Alibaba becomes more widely incorporated into today’s\ncommercial applications.</p>\n", "tags": ["KDD","Evaluation","Image-Retrieval"] },
{"key": "zhang2022multi", "year": "2022", "citations": "38", "title":"Multi-View Document Representation Learning for Open-Domain Dense Retrieval", "abstract": "<p>Dense retrieval has achieved impressive advances in first-stage retrieval\nfrom a large-scale document collection, which is built on bi-encoder\narchitecture to produce single vector representation of query and document.\nHowever, a document can usually answer multiple potential queries from\ndifferent views. So the single vector representation of a document is hard to\nmatch with multi-view queries, and faces a semantic mismatch problem. This\npaper proposes a multi-view document representation learning framework, aiming\nto produce multi-view embeddings to represent documents and enforce them to\nalign with different queries. First, we propose a simple yet effective method\nof generating multiple embeddings through viewers. Second, to prevent\nmulti-view embeddings from collapsing to the same one, we further propose a\nglobal-local loss with annealed temperature to encourage the multiple viewers\nto better align with different potential queries. Experiments show our method\noutperforms recent works and achieves state-of-the-art results.</p>\n", "tags": ["Scalability","Tools-&-Libraries"] },
{"key": "zhang2023graph", "year": "2023", "citations": "9", "title":"Graph Convolution Based Efficient Re-Ranking for Visual Retrieval", "abstract": "<p>Visual retrieval tasks such as image retrieval and person re-identification\n(Re-ID) aim at effectively and thoroughly searching images with similar content\nor the same identity. After obtaining retrieved examples, re-ranking is a\nwidely adopted post-processing step to reorder and improve the initial\nretrieval results by making use of the contextual information from semantically\nneighboring samples. Prevailing re-ranking approaches update distance metrics\nand mostly rely on inefficient crosscheck set comparison operations while\ncomputing expanded neighbors based distances. In this work, we present an\nefficient re-ranking method which refines initial retrieval results by updating\nfeatures. Specifically, we reformulate re-ranking based on Graph Convolution\nNetworks (GCN) and propose a novel Graph Convolution based Re-ranking (GCR) for\nvisual retrieval tasks via feature propagation. To accelerate computation for\nlarge-scale retrieval, a decentralized and synchronous feature propagation\nalgorithm which supports parallel or distributed computing is introduced. In\nparticular, the plain GCR is extended for cross-camera retrieval and an\nimproved feature propagation formulation is presented to leverage affinity\nrelationships across different cameras. It is also extended for video-based\nretrieval, and Graph Convolution based Re-ranking for Video (GCRV) is proposed\nby mathematically deriving a novel profile vector generation method for the\ntracklet. Without bells and whistles, the proposed approaches achieve\nstate-of-the-art performances on seven benchmark datasets from three different\ntasks, i.e., image retrieval, person Re-ID and video-based person Re-ID.</p>\n", "tags": ["Image-Retrieval","Distance-Metric-Learning","Datasets","Hybrid-ANN-Methods","Re-Ranking","Scalability","Evaluation"] },
{"key": "zhang2025binary", "year": "2013", "citations": "98", "title":"Binary Code Ranking with Weighted Hamming Distance", "abstract": "<p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most\nexisting binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or\nsimilarity of two points are approximated by the Hamming\ndistance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are\noften lots of results sharing the same Hamming distance to\na query, which makes this distance measure ambiguous and\nposes a critical issue for similarity search where ranking is\nimportant. In this paper, we propose a weighted Hamming\ndistance ranking algorithm (WhRank) to rank the binary\ncodes of hashing methods. By assigning different bit-level\nweights to different hash bits, the returned binary codes\nare ranked at a finer-grained binary code level. We give\nan algorithm to learn the data-adaptive and query-sensitive\nweight for each hash bit. Evaluations on two large-scale\nimage data sets demonstrate the efficacy of our weighted\nHamming distance for binary code ranking.</p>\n", "tags": ["Scalability","Efficiency","CVPR","Compact-Codes","Similarity-Search","Hashing-Methods"] },
{"key": "zhang2025bit", "year": "2015", "citations": "418", "title":"Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification", "abstract": "<p>Extracting informative image features and learning\neffective approximate hashing functions are two crucial steps in\nimage retrieval . Conventional methods often study these two\nsteps separately, e.g., learning hash functions from a predefined\nhand-crafted feature space. Meanwhile, the bit lengths of output\nhashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised\nlearning framework to generate compact and bit-scalable hashing\ncodes directly from raw images. We pose hashing learning as\na problem of regularized similarity learning. Specifically, we\norganize the training images into a batch of triplet samples,\neach sample containing two images with the same label and one\nwith a different label. With these triplet samples, we maximize\nthe margin between matched pairs and mismatched pairs in the\nHamming space. In addition, a regularization term is introduced\nto enforce the adjacency consistency, i.e., images of similar\nappearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end\nfashion, where discriminative image features and hash functions\nare simultaneously optimized. Furthermore, each bit of our\nhashing codes is unequally weighted so that we can manipulate\nthe code lengths by truncating the insignificant bits. Our\nframework outperforms state-of-the-arts on public benchmarks\nof similar image search and also achieves promising results in\nthe application of person re-identification in surveillance. It is\nalso shown that the generated bit-scalable hashing codes well\npreserve the discriminative powers with shorter code lengths.</p>\n", "tags": ["Image-Retrieval","Neural-Hashing","Tools-&-Libraries","Hashing-Methods","Supervised"] },
{"key": "zhang2025composite", "year": "2011", "citations": "221", "title":"Composite Hashing with Multiple Information Sources", "abstract": "<p>Similarity search applications with a large amount of text\nand image data demands an efficient and effective solution.\nOne useful strategy is to represent the examples in databases\nas compact binary codes through semantic hashing, which\nhas attracted much attention due to its fast query/search\nspeed and drastically reduced storage requirement. All of\nthe current semantic hashing methods only deal with the\ncase when each example is represented by one type of features.\nHowever, examples are often described from several\ndifferent information sources in many real world applications.\nFor example, the characteristics of a webpage can be\nderived from both its content part and its associated links.\nTo address the problem of learning good hashing codes in\nthis scenario, we propose a novel research problem – Composite\nHashing with Multiple Information Sources (CHMIS).\nThe focus of the new research problem is to design an algorithm\nfor incorporating the features from different information\nsources into the binary hashing codes efficiently and\neffectively. In particular, we propose an algorithm CHMISAW\n(CHMIS with Adjusted Weights) for learning the codes.\nThe proposed algorithm integrates information from several\ndifferent sources into the binary hashing codes by adjusting\nthe weights on each individual source for maximizing\nthe coding performance, and enables fast conversion from\nquery examples to their binary hashing codes. Experimental\nresults on five different datasets demonstrate the superior\nperformance of the proposed method against several other\nstate-of-the-art semantic hashing techniques.</p>\n", "tags": ["Datasets","Text-Retrieval","SIGIR","Compact-Codes","Similarity-Search","Hashing-Methods","Evaluation"] },
{"key": "zhang2025deep", "year": "2021", "citations": "21", "title":"Deep Center-Based Dual-Constrained Hashing for Discriminative Face Image Retrieval", "abstract": "<p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>\n", "tags": ["Scalability","Efficiency","Evaluation","Datasets","Compact-Codes","Tools-&-Libraries","Hashing-Methods","Neural-Hashing","Memory-Efficiency","Image-Retrieval","CVPR"] },
{"key": "zhang2025efficient", "year": "2016", "citations": "112", "title":"Efficient Training of Very Deep Neural Networks for Supervised Hashing", "abstract": "<p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively “shallow” networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>\n", "tags": ["Efficiency","Datasets","CVPR","Neural-Hashing","Hashing-Methods","Evaluation","Supervised"] },
{"key": "zhang2025fast", "year": "2021", "citations": "9", "title":"Fast Discrete Cross-Modal Hashing Based on Label Relaxation and Matrix Factorization", "abstract": "<p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>\n", "tags": ["Neural-Hashing","Multimodal-Retrieval","Quantization","Compact-Codes","Hashing-Methods","Evaluation","Supervised"] },
{"key": "zhang2025high", "year": "2021", "citations": "62", "title":"High-order nonlocal Hashing for unsupervised cross-modal retrieval", "abstract": "<p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>\n", "tags": ["Neural-Hashing","Multimodal-Retrieval","Compact-Codes","Hashing-Methods","Unsupervised"] },
{"key": "zhang2025large", "year": "2014", "citations": "619", "title":"Large-scale supervised multimodal hashing with semantic correlation maximization", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for similarity search in multimedia\ndata. In particular, more and more attentions\nhave been payed to multimodal hashing for search in\nmultimedia data with multiple modalities, such as images\nwith tags. Typically, supervised information of semantic\nlabels is also available for the data points in\nmany real applications. Hence, many supervised multimodal\nhashing (SMH) methods have been proposed\nto utilize such semantic labels to further improve the\nsearch accuracy. However, the training time complexity\nof most existing SMH methods is too high, which\nmakes them unscalable to large-scale datasets. In this\npaper, a novel SMH method, called semantic correlation\nmaximization (SCM), is proposed to seamlessly integrate\nsemantic labels into the hashing learning procedure\nfor large-scale data modeling. Experimental results\non two real-world datasets show that SCM can signifi-\ncantly outperform the state-of-the-art SMH methods, in\nterms of both accuracy and scalability.</p>\n", "tags": ["Scalability","Datasets","Memory-Efficiency","Similarity-Search","Hashing-Methods","AAAI","Supervised"] },
{"key": "zhang2025self", "year": "2010", "citations": "354", "title":"Self-Taught Hashing for Fast Similarity Search", "abstract": "<p>The ability of fast similarity search at large scale is of great\nimportance to many Information Retrieval (IR) applications.\nA promising way to accelerate similarity search is semantic\nhashing which designs compact binary codes for a large number\nof documents so that semantically similar documents\nare mapped to similar codes (within a short Hamming distance).\nAlthough some recently proposed techniques are\nable to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents\nremains to be a very challenging problem. In this\npaper, we emphasise this issue and propose a novel SelfTaught\nHashing (STH) approach to semantic hashing: we\nfirst find the optimal l-bit binary codes for all documents in\nthe given corpus via unsupervised learning, and then train\nl classifiers via supervised learning to predict the l-bit code\nfor any query document unseen before. Our experiments on\nthree real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and\nlinear Support Vector Machine (SVM) outperforms stateof-the-art\ntechniques significantly.</p>\n", "tags": ["Datasets","Text-Retrieval","SIGIR","Supervised","Compact-Codes","Similarity-Search","Hashing-Methods","Unsupervised"] },
{"key": "zhang2025supervised", "year": "2014", "citations": "286", "title":"Supervised Hashing with Latent Factor Models", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for approximate nearest neighbor\nsearch in large-scale datasets. Traditional hashing methods\ntry to learn the hash codes in an unsupervised way where\nthe metric (Euclidean) structure of the training data is preserved.\nVery recently, supervised hashing methods, which\ntry to preserve the semantic structure constructed from the\nsemantic labels of the training points, have exhibited higher\naccuracy than unsupervised methods. In this paper, we\npropose a novel supervised hashing method, called latent\nfactor hashing (LFH), to learn similarity-preserving binary\ncodes based on latent factor models. An algorithm with\nconvergence guarantee is proposed to learn the parameters\nof LFH. Furthermore, a linear-time variant with stochastic\nlearning is proposed for training LFH on large-scale datasets.\nExperimental results on two large datasets with semantic\nlabels show that LFH can achieve superior accuracy than\nstate-of-the-art methods with comparable training time.</p>\n", "tags": ["Scalability","Unsupervised","Datasets","Neural-Hashing","Memory-Efficiency","SIGIR","Hashing-Methods","Supervised"] },
{"key": "zhao2015deep", "year": "2015", "citations": "463", "title":"Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval", "abstract": "<p>With the rapid growth of web images, hashing has received\nincreasing interests in large scale image retrieval.\nResearch efforts have been devoted to learning compact binary\ncodes that preserve semantic similarity based on labels.\nHowever, most of these hashing methods are designed\nto handle simple binary similarity. The complex multilevel\nsemantic structure of images associated with multiple labels\nhave not yet been well explored. Here we propose a deep\nsemantic ranking based method for learning hash functions\nthat preserve multilevel semantic similarity between multilabel\nimages. In our approach, deep convolutional neural\nnetwork is incorporated into hash functions to jointly\nlearn feature representations and mappings from them to\nhash codes, which avoids the limitation of semantic representation\npower of hand-crafted features. Meanwhile, a\nranking list that encodes the multilevel similarity information\nis employed to guide the learning of such deep hash\nfunctions. An effective scheme based on surrogate loss is\nused to solve the intractable optimization problem of nonsmooth\nand multivariate ranking measures involved in the\nlearning procedure. Experimental results show the superiority\nof our proposed approach over several state-of-theart\nhashing methods in term of ranking evaluation metrics\nwhen tested on multi-label image datasets.</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Neural-Hashing","Hashing-Methods","Evaluation"] },
{"key": "zhao2017scalable", "year": "2017", "citations": "7", "title":"Scalable Nearest Neighbor Search based on kNN Graph", "abstract": "<p>Nearest neighbor search is known as a challenging issue that has been studied\nfor several decades. Recently, this issue becomes more and more imminent in\nviewing that the big data problem arises from various fields. In this paper, a\nscalable solution based on hill-climbing strategy with the support of k-nearest\nneighbor graph (kNN) is presented. Two major issues have been considered in the\npaper. Firstly, an efficient kNN graph construction method based on two means\ntree is presented. For the nearest neighbor search, an enhanced hill-climbing\nprocedure is proposed, which sees considerable performance boost over original\nprocedure. Furthermore, with the support of inverted indexing derived from\nresidue vector quantization, our method achieves close to 100% recall with high\nspeed efficiency in two state-of-the-art evaluation benchmarks. In addition, a\ncomparative study on both the compressional and traditional nearest neighbor\nsearch methods is presented. We show that our method achieves the best\ntrade-off between search quality, efficiency and memory complexity.</p>\n", "tags": ["Graph-Based-ANN","Quantization","Survey-Paper","Evaluation","Efficiency"] },
{"key": "zhao2019weakly", "year": "2019", "citations": "22", "title":"A weakly supervised adaptive triplet loss for deep metric learning", "abstract": "<p>We address the problem of distance metric learning in visual similarity\nsearch, defined as learning an image embedding model which projects images into\nEuclidean space where semantically and visually similar images are closer and\ndissimilar images are further from one another. We present a weakly supervised\nadaptive triplet loss (ATL) capable of capturing fine-grained semantic\nsimilarity that encourages the learned image embedding models to generalize\nwell on cross-domain data. The method uses weakly labeled product description\ndata to implicitly determine fine grained semantic classes, avoiding the need\nto annotate large amounts of training data. We evaluate on the Amazon fashion\nretrieval benchmark and DeepFashion in-shop retrieval data. The method boosts\nthe performance of triplet loss baseline by 10.6% on cross-domain data and\nout-performs the state-of-art model on all evaluation metrics.</p>\n", "tags": ["ICCV","Distance-Metric-Learning","Supervised","Evaluation"] },
{"key": "zhao2020stacked", "year": "2020", "citations": "11", "title":"Stacked Convolutional Deep Encoding Network for Video-Text Retrieval", "abstract": "<p>Existing dominant approaches for cross-modal video-text retrieval task are to\nlearn a joint embedding space to measure the cross-modal similarity. However,\nthese methods rarely explore long-range dependency inside video frames or\ntextual words leading to insufficient textual and visual details. In this\npaper, we propose a stacked convolutional deep encoding network for video-text\nretrieval task, which considers to simultaneously encode long-range and\nshort-range dependency in the videos and texts. Specifically, a multi-scale\ndilated convolutional (MSDC) block within our approach is able to encode\nshort-range temporal cues between video frames or text words by adopting\ndifferent scales of kernel size and dilation size of convolutional layer. A\nstacked structure is designed to expand the receptive fields by repeatedly\nadopting the MSDC block, which further captures the long-range relations\nbetween these cues. Moreover, to obtain more robust textual representations, we\nfully utilize the powerful language model named Transformer in two stages:\npretraining phrase and fine-tuning phrase. Extensive experiments on two\ndifferent benchmark datasets (MSR-VTT, MSVD) show that our proposed method\noutperforms other state-of-the-art approaches.</p>\n", "tags": ["Datasets","Text-Retrieval","Evaluation"] },
{"key": "zhao2021feature", "year": "2022", "citations": "20", "title":"A Feature Consistency Driven Attention Erasing Network for Fine-Grained Image Retrieval", "abstract": "<p>Large-scale fine-grained image retrieval has two main problems. First, low\ndimensional feature embedding can fasten the retrieval process but bring\naccuracy reduce due to overlooking the feature of significant attention regions\nof images in fine-grained datasets. Second, fine-grained images lead to the\nsame category query hash codes mapping into the different cluster in database\nhash latent space. To handle these two issues, we propose a feature consistency\ndriven attention erasing network (FCAENet) for fine-grained image retrieval.\nFor the first issue, we propose an adaptive augmentation module in FCAENet,\nwhich is selective region erasing module (SREM). SREM makes the network more\nrobust on subtle differences of fine-grained task by adaptively covering some\nregions of raw images. The feature extractor and hash layer can learn more\nrepresentative hash code for fine-grained images by SREM. With regard to the\nsecond issue, we fully exploit the pair-wise similarity information and add the\nenhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation\nstabler between the query hash code and database hash code. We conduct\nextensive experiments on five fine-grained benchmark datasets (CUB2011,\nAircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash\ncode. The results show that FCAENet achieves the state-of-the-art (SOTA)\nfine-grained retrieval performance compared with other methods.</p>\n", "tags": ["Scalability","Evaluation","Datasets","Hashing-Methods","Image-Retrieval","CVPR"] },
{"key": "zhao2021large", "year": "2019", "citations": "10", "title":"Large-Scale Visual Search with Binary Distributed Graph at Alibaba", "abstract": "<p>Graph-based approximate nearest neighbor search has attracted more and more\nattentions due to its online search advantages. Numbers of methods studying the\nenhancement of speed and recall have been put forward. However, few of them\nfocus on the efficiency and scale of offline graph-construction. For a deployed\nvisual search system with several billions of online images in total, building\na billion-scale offline graph in hours is essential, which is almost\nunachievable by most existing methods. In this paper, we propose a novel\nalgorithm called Binary Distributed Graph to solve this problem. Specifically,\nwe combine binary codes with graph structure to speedup online and offline\nprocedures, and achieve comparable performance with the ones in real-value\nbased scenarios by recalling more binary candidates. Furthermore, the\ngraph-construction is optimized to completely distributed implementation, which\nsignificantly accelerates the offline process and gets rid of the limitation of\nmemory and disk within a single machine. Experimental comparisons on Alibaba\nCommodity Data Set (more than three billion images) show that the proposed\nmethod outperforms the state-of-the-art with respect to the online/offline\ntrade-off.</p>\n", "tags": ["CIKM","Graph-Based-ANN","Image-Retrieval","Compact-Codes","Scalability","Large-Scale-Search","Evaluation","Efficiency"] },
{"key": "zhao2022progressive", "year": "2022", "citations": "21", "title":"Progressive Learning for Image Retrieval with Hybrid-Modality Queries", "abstract": "<p>Image retrieval with hybrid-modality queries, also known as composing text\nand image for image retrieval (CTI-IR), is a retrieval task where the search\nintention is expressed in a more complex query format, involving both vision\nand text modalities. For example, a target product image is searched using a\nreference product image along with text about changing certain attributes of\nthe reference image as the query. It is a more challenging image retrieval task\nthat requires both semantic space learning and cross-modal fusion. Previous\napproaches that attempt to deal with both aspects achieve unsatisfactory\nperformance. In this paper, we decompose the CTI-IR task into a three-stage\nlearning problem to progressively learn the complex knowledge for image\nretrieval with hybrid-modality queries. We first leverage the semantic\nembedding space for open-domain image-text retrieval, and then transfer the\nlearned knowledge to the fashion-domain with fashion-related pre-training\ntasks. Finally, we enhance the pre-trained model from single-query to\nhybrid-modality query for the CTI-IR task. Furthermore, as the contribution of\nindividual modality in the hybrid-modality query varies for different retrieval\nscenarios, we propose a self-supervised adaptive weighting strategy to\ndynamically determine the importance of image and text in the hybrid-modality\nquery for better retrieval. Extensive experiments show that our proposed model\nsignificantly outperforms state-of-the-art methods in the mean of Recall@K by\n24.9% and 9.5% on the Fashion-IQ and Shoes benchmark datasets respectively.</p>\n", "tags": ["Supervised","Text-Retrieval","Image-Retrieval","Datasets","SIGIR","Self-Supervised","Evaluation"] },
{"key": "zhao2025deep", "year": "2015", "citations": "463", "title":"Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval", "abstract": "<p>With the rapid growth of web images, hashing has received\nincreasing interests in large scale image retrieval.\nResearch efforts have been devoted to learning compact binary\ncodes that preserve semantic similarity based on labels.\nHowever, most of these hashing methods are designed\nto handle simple binary similarity. The complex multilevel\nsemantic structure of images associated with multiple labels\nhave not yet been well explored. Here we propose a deep\nsemantic ranking based method for learning hash functions\nthat preserve multilevel semantic similarity between multilabel\nimages. In our approach, deep convolutional neural\nnetwork is incorporated into hash functions to jointly\nlearn feature representations and mappings from them to\nhash codes, which avoids the limitation of semantic representation\npower of hand-crafted features. Meanwhile, a\nranking list that encodes the multilevel similarity information\nis employed to guide the learning of such deep hash\nfunctions. An effective scheme based on surrogate loss is\nused to solve the intractable optimization problem of nonsmooth\nand multivariate ranking measures involved in the\nlearning procedure. Experimental results show the superiority\nof our proposed approach over several state-of-theart\nhashing methods in term of ranking evaluation metrics\nwhen tested on multi-label image datasets.</p>\n", "tags": ["Image-Retrieval","Datasets","CVPR","Neural-Hashing","Hashing-Methods","Evaluation"] },
{"key": "zhe2018directional", "year": "2019", "citations": "72", "title":"Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval", "abstract": "<p>Deep distance metric learning (DDML), which is proposed to learn image\nsimilarity metrics in an end-to-end manner based on the convolution neural\nnetwork, has achieved encouraging results in many computer vision\ntasks.\\(L2\\)-normalization in the embedding space has been used to improve the\nperformance of several DDML methods. However, the commonly used Euclidean\ndistance is no longer an accurate metric for \\(L2\\)-normalized embedding space,\ni.e., a hyper-sphere. Another challenge of current DDML methods is that their\nloss functions are usually based on rigid data formats, such as the triplet\ntuple. Thus, an extra process is needed to prepare data in specific formats. In\naddition, their losses are obtained from a limited number of samples, which\nleads to a lack of the global view of the embedding space. In this paper, we\nreplace the Euclidean distance with the cosine similarity to better utilize the\n\\(L2\\)-normalization, which is able to attenuate the curse of dimensionality.\nMore specifically, a novel loss function based on the von Mises-Fisher\ndistribution is proposed to learn a compact hyper-spherical embedding space.\nMoreover, a new efficient learning algorithm is developed to better capture the\nglobal structure of the embedding space. Experiments for both classification\nand retrieval tasks on several standard datasets show that our method achieves\nstate-of-the-art performance with a simpler training procedure. Furthermore, we\ndemonstrate that, even with a small number of convolutional layers, our model\ncan still obtain significantly better classification performance than the\nwidely used softmax loss.</p>\n", "tags": ["Evaluation","Distance-Metric-Learning","Datasets","CVPR"] },
{"key": "zhen2012co", "year": "2012", "citations": "195", "title":"Co-Regularized Hashing for Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity\nsearch. To obtain compact hash codes, a recent trend seeks to learn the hash\nfunctions from data automatically. In this paper, we study hash function learning\nin the context of multimodal data. We propose a novel multimodal hash function\nlearning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization\nframework. The hash functions for each bit of the hash codes are\nlearned by solving DC (difference of convex functions) programs, while the learning\nfor multiple bits proceeds via a boosting procedure so that the bias introduced\nby the hash functions can be sequentially minimized. We empirically compare\nCRH with two state-of-the-art multimodal hash function learning methods on two\npublicly available data sets.</p>\n", "tags": ["Hashing-Methods","Scalability","Tools-&-Libraries"] },
{"key": "zhen2025co", "year": "2012", "citations": "195", "title":"Co-Regularized Hashing for Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity\nsearch. To obtain compact hash codes, a recent trend seeks to learn the hash\nfunctions from data automatically. In this paper, we study hash function learning\nin the context of multimodal data. We propose a novel multimodal hash function\nlearning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization\nframework. The hash functions for each bit of the hash codes are\nlearned by solving DC (difference of convex functions) programs, while the learning\nfor multiple bits proceeds via a boosting procedure so that the bias introduced\nby the hash functions can be sequentially minimized. We empirically compare\nCRH with two state-of-the-art multimodal hash function learning methods on two\npublicly available data sets.</p>\n", "tags": ["Hashing-Methods","Scalability","Tools-&-Libraries"] },
{"key": "zheng2015person", "year": "2015", "citations": "60", "title":"Person Re-identification Meets Image Search", "abstract": "<p>For long time, person re-identification and image search are two separately\nstudied tasks. However, for person re-identification, the effectiveness of\nlocal features and the “query-search” mode make it well posed for image search\ntechniques.\n  In the light of recent advances in image search, this paper proposes to treat\nperson re-identification as an image search problem. Specifically, this paper\nclaims two major contributions. 1) By designing an unsupervised Bag-of-Words\nrepresentation, we are devoted to bridging the gap between the two tasks by\nintegrating techniques from image search in person re-identification. We show\nthat our system sets up an effective yet efficient baseline that is amenable to\nfurther supervised/unsupervised improvements. 2) We contribute a new high\nquality dataset which uses DPM detector and includes a number of distractor\nimages. Our dataset reaches closer to realistic settings, and new perspectives\nare provided.\n  Compared with approaches that rely on feature-feature match, our method is\nfaster by over two orders of magnitude. Moreover, on three datasets, we report\ncompetitive results compared with the state-of-the-art methods.</p>\n", "tags": ["Unsupervised","Datasets","Supervised","Image-Retrieval"] },
{"key": "zheng2020generative", "year": "2020", "citations": "10", "title":"Generative Semantic Hashing Enhanced via Boltzmann Machines", "abstract": "<p>Generative semantic hashing is a promising technique for large-scale\ninformation retrieval thanks to its fast retrieval speed and small memory\nfootprint. For the tractability of training, existing generative-hashing\nmethods mostly assume a factorized form for the posterior distribution,\nenforcing independence among the bits of hash codes. From the perspectives of\nboth model representation and code space size, independence is always not the\nbest assumption. In this paper, to introduce correlations among the bits of\nhash codes, we propose to employ the distribution of Boltzmann machine as the\nvariational posterior. To address the intractability issue of training, we\nfirst develop an approximate method to reparameterize the distribution of a\nBoltzmann machine by augmenting it as a hierarchical concatenation of a\nGaussian-like distribution and a Bernoulli distribution. Based on that, an\nasymptotically-exact lower bound is further derived for the evidence lower\nbound (ELBO). With these novel techniques, the entire model can be optimized\nefficiently. Extensive experimental results demonstrate that by effectively\nmodeling correlations among different bits within a hash code, our model can\nachieve significant performance gains.</p>\n", "tags": ["Text-Retrieval","Hashing-Methods","Scalability","Evaluation","Efficiency"] },
{"key": "zheng2021deep", "year": "2021", "citations": "36", "title":"Deep Relational Metric Learning", "abstract": "<p>This paper presents a deep relational metric learning (DRML) framework for\nimage clustering and retrieval. Most existing deep metric learning methods\nlearn an embedding space with a general objective of increasing interclass\ndistances and decreasing intraclass distances. However, the conventional losses\nof metric learning usually suppress intraclass variations which might be\nhelpful to identify samples of unseen classes. To address this problem, we\npropose to adaptively learn an ensemble of features that characterizes an image\nfrom different aspects to model both interclass and intraclass distributions.\nWe further employ a relational module to capture the correlations among each\nfeature in the ensemble and construct a graph to represent an image. We then\nperform relational inference on the graph to integrate the ensemble and obtain\na relation-aware embedding to measure the similarities. Extensive experiments\non the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets\ndemonstrate that our framework improves existing deep metric learning methods\nand achieves very competitive results.</p>\n", "tags": ["ICCV","Distance-Metric-Learning","Datasets","Tools-&-Libraries"] },
{"key": "zhong2017re", "year": "2017", "citations": "1369", "title":"Re-ranking Person Re-identification with k-reciprocal Encoding", "abstract": "<p>When considering person re-identification (re-ID) as a retrieval process,\nre-ranking is a critical step to improve its accuracy. Yet in the re-ID\ncommunity, limited effort has been devoted to re-ranking, especially those\nfully automatic, unsupervised solutions. In this paper, we propose a\nk-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is\nthat if a gallery image is similar to the probe in the k-reciprocal nearest\nneighbors, it is more likely to be a true match. Specifically, given an image,\na k-reciprocal feature is calculated by encoding its k-reciprocal nearest\nneighbors into a single vector, which is used for re-ranking under the Jaccard\ndistance. The final distance is computed as the combination of the original\ndistance and the Jaccard distance. Our re-ranking method does not require any\nhuman interaction or any labeled data, so it is applicable to large-scale\ndatasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW\ndatasets confirm the effectiveness of our method.</p>\n", "tags": ["Datasets","CVPR","Hybrid-ANN-Methods","Re-Ranking","Unsupervised","Scalability"] },
{"key": "zhong2020compact", "year": "2019", "citations": "8", "title":"Compact Deep Aggregation for Set Retrieval", "abstract": "<p>The objective of this work is to learn a compact embedding of a set of\ndescriptors that is suitable for efficient retrieval and ranking, whilst\nmaintaining discriminability of the individual descriptors. We focus on a\nspecific example of this general problem – that of retrieving images\ncontaining multiple faces from a large scale dataset of images. Here the set\nconsists of the face descriptors in each image, and given a query for multiple\nidentities, the goal is then to retrieve, in order, images which contain all\nthe identities, all but one, \\etc\n  To this end, we make the following contributions: first, we propose a CNN\narchitecture – {\\em SetNet} – to achieve the objective: it learns face\ndescriptors and their aggregation over a set to produce a compact fixed length\ndescriptor designed for set retrieval, and the score of an image is a count of\nthe number of identities that match the query; second, we show that this\ncompact descriptor has minimal loss of discriminability up to two faces per\nimage, and degrades slowly after that – far exceeding a number of baselines;\nthird, we explore the speed vs.\\ retrieval quality trade-off for set retrieval\nusing this compact descriptor; and, finally, we collect and annotate a large\ndataset of images containing various number of celebrities, which we use for\nevaluation and is publicly released.</p>\n", "tags": ["Similarity-Search","Datasets","Evaluation"] },
{"key": "zhong2022evaluating", "year": "2022", "citations": "12", "title":"Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval", "abstract": "<p>With the recent success of dense retrieval methods based on bi-encoders,\nstudies have applied this approach to various interesting downstream retrieval\ntasks with good efficiency and in-domain effectiveness. Recently, we have also\nseen the presence of dense retrieval models in Math Information Retrieval (MIR)\ntasks, but the most effective systems remain classic retrieval methods that\nconsider hand-crafted structure features. In this work, we try to combine the\nbest of both worlds:\\ a well-defined structure search method for effective\nformula search and efficient bi-encoder dense retrieval models to capture\ncontextual similarities. Specifically, we have evaluated two representative\nbi-encoder models for token-level and passage-level dense retrieval on recent\nMIR tasks. Our results show that bi-encoder models are highly complementary to\nexisting structure search methods, and we are able to advance the\nstate-of-the-art on MIR datasets.</p>\n", "tags": ["Datasets","EMNLP","Efficiency"] },
{"key": "zhou2016learning", "year": "2017", "citations": "139", "title":"Learning Low Dimensional Convolutional Neural Networks for High-Resolution Remote Sensing Image Retrieval", "abstract": "<p>Learning powerful feature representations for image retrieval has always been\na challenging task in the field of remote sensing. Traditional methods focus on\nextracting low-level hand-crafted features which are not only time-consuming\nbut also tend to achieve unsatisfactory performance due to the content\ncomplexity of remote sensing images. In this paper, we investigate how to\nextract deep feature representations based on convolutional neural networks\n(CNN) for high-resolution remote sensing image retrieval (HRRSIR). To this end,\ntwo effective schemes are proposed to generate powerful feature representations\nfor HRRSIR. In the first scheme, the deep features are extracted from the\nfully-connected and convolutional layers of the pre-trained CNN models,\nrespectively; in the second scheme, we propose a novel CNN architecture based\non conventional convolution layers and a three-layer perceptron. The novel CNN\nmodel is then trained on a large remote sensing dataset to learn low\ndimensional features. The two schemes are evaluated on several public and\nchallenging datasets, and the results indicate that the proposed schemes and in\nparticular the novel CNN are able to achieve state-of-the-art performance.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "zhou2016transfer", "year": "2016", "citations": "25", "title":"Transfer Hashing with Privileged Information", "abstract": "<p>Most existing learning to hash methods assume that there are sufficient data,\neither labeled or unlabeled, on the domain of interest (i.e., the target\ndomain) for training. However, this assumption cannot be satisfied in some\nreal-world applications. To address this data sparsity issue in hashing,\ninspired by transfer learning, we propose a new framework named Transfer\nHashing with Privileged Information (THPI). Specifically, we extend the\nstandard learning to hash method, Iterative Quantization (ITQ), in a transfer\nlearning manner, namely ITQ+. In ITQ+, a new slack function is learned from\nauxiliary data to approximate the quantization error in ITQ. We developed an\nalternating optimization approach to solve the resultant optimization problem\nfor ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure\namong the auxiliary data for learning more precise binary codes in the target\ndomain. Extensive experiments on several benchmark datasets verify the\neffectiveness of our proposed approaches through comparisons with several\nstate-of-the-art baselines.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Datasets","Compact-Codes","Quantization","Evaluation"] },
{"key": "zhou2019ladder", "year": "2020", "citations": "27", "title":"Ladder Loss for Coherent Visual-Semantic Embedding", "abstract": "<p>For visual-semantic embedding, the existing methods normally treat the\nrelevance between queries and candidates in a bipolar way – relevant or\nirrelevant, and all “irrelevant” candidates are uniformly pushed away from the\nquery by an equal margin in the embedding space, regardless of their various\nproximity to the query. This practice disregards relatively discriminative\ninformation and could lead to suboptimal ranking in the retrieval results and\npoorer user experience, especially in the long-tail query scenario where a\nmatching candidate may not necessarily exist. In this paper, we introduce a\ncontinuous variable to model the relevance degree between queries and multiple\ncandidates, and propose to learn a coherent embedding space, where candidates\nwith higher relevance degrees are mapped closer to the query than those with\nlower relevance degrees. In particular, the new ladder loss is proposed by\nextending the triplet loss inequality to a more general inequality chain, which\nimplements variable push-away margins according to respective relevance\ndegrees. In addition, a proper Coherent Score metric is proposed to better\nmeasure the ranking results including those “irrelevant” candidates. Extensive\nexperiments on multiple datasets validate the efficacy of our proposed method,\nwhich achieves significant improvement over existing state-of-the-art methods.</p>\n", "tags": ["AAAI","Distance-Metric-Learning","Datasets"] },
{"key": "zhou2020ladder", "year": "2020", "citations": "27", "title":"Ladder Loss for Coherent Visual-Semantic Embedding", "abstract": "<p>For visual-semantic embedding, the existing methods normally treat the\nrelevance between queries and candidates in a bipolar way – relevant or\nirrelevant, and all “irrelevant” candidates are uniformly pushed away from the\nquery by an equal margin in the embedding space, regardless of their various\nproximity to the query. This practice disregards relatively discriminative\ninformation and could lead to suboptimal ranking in the retrieval results and\npoorer user experience, especially in the long-tail query scenario where a\nmatching candidate may not necessarily exist. In this paper, we introduce a\ncontinuous variable to model the relevance degree between queries and multiple\ncandidates, and propose to learn a coherent embedding space, where candidates\nwith higher relevance degrees are mapped closer to the query than those with\nlower relevance degrees. In particular, the new ladder loss is proposed by\nextending the triplet loss inequality to a more general inequality chain, which\nimplements variable push-away margins according to respective relevance\ndegrees. In addition, a proper Coherent Score metric is proposed to better\nmeasure the ranking results including those “irrelevant” candidates. Extensive\nexperiments on multiple datasets validate the efficacy of our proposed method,\nwhich achieves significant improvement over existing state-of-the-art methods.</p>\n", "tags": ["AAAI","Distance-Metric-Learning","Datasets"] },
{"key": "zhu2013linear", "year": "2013", "citations": "278", "title":"Linear cross-modal hashing for efficient multimedia search", "abstract": "<p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel \ncross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia \nsearch across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals \ninto consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. \nMore specifically, for each modal, we first partition the training data into \\(k\\) clusters and then represent each training data \npoint with its distances to \\(k\\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce \nthe time complexity of the training phase from traditional O(n2) or higher to O(n), where \\(n\\) is the training data size, leading to \npractical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. \nTo preserve the inter-similarity among data points across different modals, we transform the derived data representations into a \ncommon binary subspace in which binary codes from all the modals are “consistent” and comparable. The transformation simultaneously \noutputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, \nit is first mapped into the binary codes using the modal’s hash functions, followed by matching the database binary codes of any other \nmodals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in \ncomparison with the state of the art.</p>\n", "tags": ["Scalability","Datasets","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "zhu2014personalized", "year": "2014", "citations": "25", "title":"Personalized recommendation with corrected similarity", "abstract": "<p>Personalized recommendation attracts a surge of interdisciplinary researches.\nEspecially, similarity based methods in applications of real recommendation\nsystems achieve great success. However, the computations of similarities are\noverestimated or underestimated outstandingly due to the defective strategy of\nunidirectional similarity estimation. In this paper, we solve this drawback by\nleveraging mutual correction of forward and backward similarity estimations,\nand propose a new personalized recommendation index, i.e., corrected similarity\nbased inference (CSI). Through extensive experiments on four benchmark\ndatasets, the results show a greater improvement of CSI in comparison with\nthese mainstream baselines. And the detailed analysis is presented to unveil\nand understand the origin of such difference between CSI and mainstream\nindices.</p>\n", "tags": ["Datasets","Recommender-Systems","Evaluation"] },
{"key": "zhu2016deep", "year": "2016", "citations": "636", "title":"Deep Hashing Network for Efficient Similarity Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.\nHowever, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.\nThe DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","Datasets","Neural-Hashing","Quantization","Compact-Codes","Similarity-Search","Hashing-Methods","AAAI","Supervised"] },
{"key": "zhu2016radon", "year": "2016", "citations": "12", "title":"Radon Features and Barcodes for Medical Image Retrieval via SVM", "abstract": "<p>For more than two decades, research has been performed on content-based image\nretrieval (CBIR). By combining Radon projections and the support vector\nmachines (SVM), a content-based medical image retrieval method is presented in\nthis work. The proposed approach employs the normalized Radon projections with\ncorresponding image category labels to build an SVM classifier, and the Radon\nbarcode database which encodes every image in a binary format is also generated\nsimultaneously to tag all images. To retrieve similar images when a query image\nis given, Radon projections and the barcode of the query image are generated.\nSubsequently, the k-nearest neighbor search method is applied to find the\nimages with minimum Hamming distance of the Radon barcode within the same class\npredicted by the trained SVM classifier that uses Radon features. The\nperformance of the proposed method is validated by using the IRMA 2009 dataset\nwith 14,410 x-ray images in 57 categories. The results demonstrate that our\nmethod has the capacity to retrieve similar responses for the correctly\nidentified query image and even for those mistakenly classified by SVM. The\napproach further is very fast and has low memory requirement.</p>\n", "tags": ["Datasets","Evaluation","Image-Retrieval"] },
{"key": "zhu2017discrete", "year": "2017", "citations": "9", "title":"Discrete Multi-modal Hashing with Canonical Views for Robust Mobile Landmark Search", "abstract": "<p>Mobile landmark search (MLS) recently receives increasing attention for its\ngreat practical values. However, it still remains unsolved due to two important\nchallenges. One is high bandwidth consumption of query transmission, and the\nother is the huge visual variations of query images sent from mobile devices.\nIn this paper, we propose a novel hashing scheme, named as canonical view based\ndiscrete multi-modal hashing (CV-DMH), to handle these problems via a novel\nthree-stage learning procedure. First, a submodular function is designed to\nmeasure visual representativeness and redundancy of a view set. With it,\ncanonical views, which capture key visual appearances of landmark with limited\nredundancy, are efficiently discovered with an iterative mining strategy.\nSecond, multi-modal sparse coding is applied to transform visual features from\nmultiple modalities into an intermediate representation. It can robustly and\nadaptively characterize visual contents of varied landmark images with certain\ncanonical views. Finally, compact binary codes are learned on intermediate\nrepresentation within a tailored discrete binary embedding model which\npreserves visual relations of images measured with canonical views and removes\nthe involved noises. In this part, we develop a new augmented Lagrangian\nmultiplier (ALM) based optimization method to directly solve the discrete\nbinary codes. We can not only explicitly deal with the discrete constraint, but\nalso consider the bit-uncorrelated constraint and balance constraint together.\nExperiments on real world landmark datasets demonstrate the superior\nperformance of CV-DMH over several state-of-the-art methods.</p>\n", "tags": ["Hashing-Methods","Datasets","Compact-Codes","Evaluation"] },
{"key": "zhu2017part", "year": "2017", "citations": "70", "title":"Part-based Deep Hashing for Large-scale Person Re-identification", "abstract": "<p>Large-scale is a trend in person re-identification (re-id). It is important\nthat real-time search be performed in a large gallery. While previous methods\nmostly focus on discriminative learning, this paper makes the attempt in\nintegrating deep learning and hashing into one framework to evaluate the\nefficiency and accuracy for large-scale person re-id. We integrate spatial\ninformation for discriminative visual representation by partitioning the\npedestrian image into horizontal parts. Specifically, Part-based Deep Hashing\n(PDH) is proposed, in which batches of triplet samples are employed as the\ninput of the deep hashing architecture. Each triplet sample contains two\npedestrian images (or parts) with the same identity and one pedestrian image\n(or part) of the different identity. A triplet loss function is employed with a\nconstraint that the Hamming distance of pedestrian images (or parts) with the\nsame identity is smaller than ones with the different identity. In the\nexperiment, we show that the proposed Part-based Deep Hashing method yields\nvery competitive re-id accuracy on the large-scale Market-1501 and\nMarket-1501+500K datasets.</p>\n", "tags": ["Tools-&-Libraries","Hashing-Methods","Distance-Metric-Learning","Datasets","Neural-Hashing","Scalability","Efficiency"] },
{"key": "zhu2018attention", "year": "2018", "citations": "89", "title":"Attention-based Pyramid Aggregation Network for Visual Place Recognition", "abstract": "<p>Visual place recognition is challenging in the urban environment and is\nusually viewed as a large scale image retrieval task. The intrinsic challenges\nin place recognition exist that the confusing objects such as cars and trees\nfrequently occur in the complex urban scene, and buildings with repetitive\nstructures may cause over-counting and the burstiness problem degrading the\nimage representations. To address these problems, we present an Attention-based\nPyramid Aggregation Network (APANet), which is trained in an end-to-end manner\nfor place recognition. One main component of APANet, the spatial pyramid\npooling, can effectively encode the multi-size buildings containing\ngeo-information. The other one, the attention block, is adopted as a region\nevaluator for suppressing the confusing regional features while highlighting\nthe discriminative ones. When testing, we further propose a simple yet\neffective PCA power whitening strategy, which significantly improves the widely\nused PCA whitening by reasonably limiting the impact of over-counting.\nExperimental evaluations demonstrate that the proposed APANet outperforms the\nstate-of-the-art methods on two place recognition benchmarks, and generalizes\nwell on standard image retrieval datasets.</p>\n", "tags": ["Datasets","Image-Retrieval"] },
{"key": "zhu2020dual", "year": "2020", "citations": "38", "title":"Dual-level Semantic Transfer Deep Hashing for Efficient Social Image Retrieval", "abstract": "<p>Social network stores and disseminates a tremendous amount of user shared\nimages. Deep hashing is an efficient indexing technique to support large-scale\nsocial image retrieval, due to its deep representation capability, fast\nretrieval speed and low storage cost. Particularly, unsupervised deep hashing\nhas well scalability as it does not require any manually labelled data for\ntraining. However, owing to the lacking of label guidance, existing methods\nsuffer from severe semantic shortage when optimizing a large amount of deep\nneural network parameters. Differently, in this paper, we propose a Dual-level\nSemantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a\nunified deep hash learning framework. Our model targets at learning the\nsemantically enhanced deep hash codes by specially exploiting the\nuser-generated tags associated with the social images. Specifically, we design\na complementary dual-level semantic transfer mechanism to efficiently discover\nthe potential semantics of tags and seamlessly transfer them into binary hash\ncodes. On the one hand, instance-level semantics are directly preserved into\nhash codes from the associated tags with adverse noise removing. Besides, an\nimage-concept hypergraph is constructed for indirectly transferring the latent\nhigh-order semantic correlations of images and tags into hash codes. Moreover,\nthe hash codes are obtained simultaneously with the deep representation\nlearning by the discrete hash optimization strategy. Extensive experiments on\ntwo public social image retrieval datasets validate the superior performance of\nour method compared with state-of-the-art hashing methods. The source codes of\nour method can be obtained at https://github.com/research2020-1/DSTDH</p>\n", "tags": ["Tools-&-Libraries","Image-Retrieval","Hashing-Methods","Datasets","Neural-Hashing","Memory-Efficiency","Unsupervised","Scalability","Evaluation"] },
{"key": "zhu2025deep", "year": "2016", "citations": "636", "title":"Deep Hashing Network for Efficient Similarity Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.\nHowever, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.\nThe DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>\n", "tags": ["Image-Retrieval","Scalability","Efficiency","Datasets","Neural-Hashing","Quantization","Compact-Codes","Similarity-Search","Hashing-Methods","AAAI","Supervised"] },
{"key": "zhu2025linear", "year": "2013", "citations": "278", "title":"Linear cross-modal hashing for efficient multimedia search", "abstract": "<p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel \ncross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia \nsearch across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals \ninto consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. \nMore specifically, for each modal, we first partition the training data into \\(k\\) clusters and then represent each training data \npoint with its distances to \\(k\\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce \nthe time complexity of the training phase from traditional O(n2) or higher to O(n), where \\(n\\) is the training data size, leading to \npractical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. \nTo preserve the inter-similarity among data points across different modals, we transform the derived data representations into a \ncommon binary subspace in which binary codes from all the modals are “consistent” and comparable. The transformation simultaneously \noutputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, \nit is first mapped into the binary codes using the modal’s hash functions, followed by matching the database binary codes of any other \nmodals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in \ncomparison with the state of the art.</p>\n", "tags": ["Scalability","Datasets","Compact-Codes","Hashing-Methods","Evaluation"] },
{"key": "zhuang2016fast", "year": "2016", "citations": "136", "title":"Fast Training of Triplet-based Deep Binary Embedding Networks", "abstract": "<p>In this paper, we aim to learn a mapping (or embedding) from images to a\ncompact binary space in which Hamming distances correspond to a ranking measure\nfor the image retrieval task.\n  We make use of a triplet loss because this has been shown to be most\neffective for ranking problems.\n  However, training in previous works can be prohibitively expensive due to the\nfact that optimization is directly performed on the triplet space, where the\nnumber of possible triplets for training is cubic in the number of training\nexamples.\n  To address this issue, we propose to formulate high-order binary codes\nlearning as a multi-label classification problem by explicitly separating\nlearning into two interleaved stages.\n  To solve the first stage, we design a large-scale high-order binary codes\ninference algorithm to reduce the high-order objective to a standard binary\nquadratic problem such that graph cuts can be used to efficiently infer the\nbinary code which serve as the label of each training datum.\n  In the second stage we propose to map the original image to compact binary\ncodes via carefully designed deep convolutional neural networks (CNNs) and the\nhashing function fitting can be solved by training binary CNN classifiers.\n  An incremental/interleaved optimization strategy is proffered to ensure that\nthese two steps are interactive with each other during training for better\naccuracy.\n  We conduct experiments on several benchmark datasets, which demonstrate both\nimproved training time (by as much as two orders of magnitude) as well as\nproducing state-of-the-art hashing for various retrieval tasks.</p>\n", "tags": ["Image-Retrieval","Hashing-Methods","Distance-Metric-Learning","Datasets","Compact-Codes","CVPR","Scalability","Evaluation"] },
{"key": "zoran2017learning", "year": "2017", "citations": "9", "title":"Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees", "abstract": "<p>Nearest neighbor (kNN) methods have been gaining popularity in recent years\nin light of advances in hardware and efficiency of algorithms. There is a\nplethora of methods to choose from today, each with their own advantages and\ndisadvantages. One requirement shared between all kNN based methods is the need\nfor a good representation and distance measure between samples.\n  We introduce a new method called differentiable boundary tree which allows\nfor learning deep kNN representations. We build on the recently proposed\nboundary tree algorithm which allows for efficient nearest neighbor\nclassification, regression and retrieval. By modelling traversals in the tree\nas stochastic events, we are able to form a differentiable cost function which\nis associated with the tree’s predictions. Using a deep neural network to\ntransform the data and back-propagating through the tree allows us to learn\ngood representations for kNN methods.\n  We demonstrate that our method is able to learn suitable representations\nallowing for very efficient trees with a clearly interpretable structure.</p>\n", "tags": ["Efficiency"] },
{"key": "zou2019transductive", "year": "2020", "citations": "14", "title":"Transductive Zero-Shot Hashing for Multilabel Image Retrieval", "abstract": "<p>Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.</p>\n", "tags": ["Few-Shot-&-Zero-Shot","Image-Retrieval","Hashing-Methods","Datasets","Quantization","Compact-Codes","Scalability"] },
{"key": "ñanculef2020self", "year": "2021", "citations": "5", "title":"Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing", "abstract": "<p>Semantic hashing is an emerging technique for large-scale similarity search\nbased on representing high-dimensional data using similarity-preserving binary\ncodes used for efficient indexing and search. It has recently been shown that\nvariational autoencoders, with Bernoulli latent representations parametrized by\nneural nets, can be successfully trained to learn such codes in supervised and\nunsupervised scenarios, improving on more traditional methods thanks to their\nability to handle the binary constraints architecturally. However, the scenario\nwhere labels are scarce has not been studied yet.\n  This paper investigates the robustness of hashing methods based on\nvariational autoencoders to the lack of supervision, focusing on two\nsemi-supervised approaches currently in use. The first augments the variational\nautoencoder’s training objective to jointly model the distribution over the\ndata and the class labels. The second approach exploits the annotations to\ndefine an additional pairwise loss that enforces consistency between the\nsimilarity in the code (Hamming) space and the similarity in the label space.\nOur experiments show that both methods can significantly increase the hash\ncodes’ quality. The pairwise approach can exhibit an advantage when the number\nof labelled points is large. However, we found that this method degrades\nquickly and loses its advantage when labelled samples decrease. To circumvent\nthis problem, we propose a novel supervision method in which the model uses its\nlabel distribution predictions to implement the pairwise objective. Compared to\nthe best baseline, this procedure yields similar performance in fully\nsupervised settings but improves the results significantly when labelled data\nis scarce. Our code is made publicly available at\nhttps://github.com/amacaluso/SSB-VAE.</p>\n", "tags": ["Similarity-Search","Unsupervised","Supervised","Text-Retrieval","Hashing-Methods","Neural-Hashing","Self-Supervised","Scalability","Evaluation","Robustness"] }

]
