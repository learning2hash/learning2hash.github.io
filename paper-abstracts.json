[
{"key": "2024scratch", "year": "2024", "title":"SCRATCH A Scalable Discrete Matrix Factorization Hashing For Cross-modal Retrieval", "abstract": "<p>In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method—Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "a2016ensemble", "year": "2016", "title":"An Ensemble Diversity Approach To Supervised Binary Hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.</p>\n", "tags": ["Image Retrieval","NEURIPS","Supervised"] },
{"key": "aamand2018non", "year": "2018", "title":"Non-empty Bins With Simple Tabulation Hashing", "abstract": "<p>We consider the hashing of a set \\(X\\subseteq U\\) with \\(|X|=m\\) using a simple\ntabulation hash function \\(h:U\\to [n]=\\{0,\\dots,n-1\\}\\) and analyse the number of\nnon-empty bins, that is, the size of \\(h(X)\\). We show that the expected size of\n\\(h(X)\\) matches that with fully random hashing to within low-order terms. We\nalso provide concentration bounds. The number of non-empty bins is a\nfundamental measure in the balls and bins paradigm, and it is critical in\napplications such as Bloom filters and Filter hashing. For example, normally\nBloom filters are proportioned for a desired low false-positive probability\nassuming fully random hashing (see \\url{en.wikipedia.org/wiki/Bloom_filter}).\nOur results imply that if we implement the hashing with simple tabulation, we\nobtain the same low false-positive probability for any possible input.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "aamand2019fast", "year": "2019", "title":"Fast Hashing With Strong Concentration Bounds", "abstract": "<p>Previous work on tabulation hashing by Patrascu and Thorup from STOC’11 on\nsimple tabulation and from SODA’13 on twisted tabulation offered Chernoff-style\nconcentration bounds on hash based sums, e.g., the number of balls/keys hashing\nto a given bin, but under some quite severe restrictions on the expected values\nof these sums. The basic idea in tabulation hashing is to view a key as\nconsisting of \\(c=O(1)\\) characters, e.g., a 64-bit key as \\(c=8\\) characters of\n8-bits. The character domain \\(\\Sigma\\) should be small enough that character\ntables of size \\(|\\Sigma|\\) fit in fast cache. The schemes then use \\(O(1)\\) tables\nof this size, so the space of tabulation hashing is \\(O(|\\Sigma|)\\). However, the\nconcentration bounds by Patrascu and Thorup only apply if the expected sums are\n\\(\\ll |\\Sigma|\\).\n  To see the problem, consider the very simple case where we use tabulation\nhashing to throw \\(n\\) balls into \\(m\\) bins and want to analyse the number of\nballs in a given bin. With their concentration bounds, we are fine if \\(n=m\\),\nfor then the expected value is \\(1\\). However, if \\(m=2\\), as when tossing \\(n\\)\nunbiased coins, the expected value \\(n/2\\) is \\(\\gg |\\Sigma|\\) for large data sets,\ne.g., data sets that do not fit in fast cache.\n  To handle expectations that go beyond the limits of our small space, we need\na much more advanced analysis of simple tabulation, plus a new tabulation\ntechnique that we call <em>tabulation-permutation</em> hashing which is at most\ntwice as slow as simple tabulation. No other hashing scheme of comparable speed\noffers similar Chernoff-style concentration bounds.</p>\n", "tags": ["ARXIV"] },
{"key": "aamand2020no", "year": "2020", "title":"No Repetition Fast Streaming With Highly Concentrated Hashing", "abstract": "<p>To get estimators that work within a certain error bound with high\nprobability, a common strategy is to design one that works with constant\nprobability, and then boost the probability using independent repetitions.\nImportant examples of this approach are small space algorithms for estimating\nthe number of distinct elements in a stream, or estimating the set similarity\nbetween large sets. Using standard strongly universal hashing to process each\nelement, we get a sketch based estimator where the probability of a too large\nerror is, say, 1/4. By performing \\(r\\) independent repetitions and taking the\nmedian of the estimators, the error probability falls exponentially in \\(r\\).\nHowever, running \\(r\\) independent experiments increases the processing time by a\nfactor \\(r\\).\n  Here we make the point that if we have a hash function with strong\nconcentration bounds, then we get the same high probability bounds without any\nneed for repetitions. Instead of \\(r\\) independent sketches, we have a single\nsketch that is \\(r\\) times bigger, so the total space is the same. However, we\nonly apply a single hash function, so we save a factor \\(r\\) in time, and the\noverall algorithms just get simpler.\n  Fast practical hash functions with strong concentration bounds were recently\nproposed by Aamand em et al. (to appear in STOC 2020). Using their hashing\nschemes, the algorithms thus become very fast and practical, suitable for\nonline processing of high volume data streams.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ablayev2014quantum", "year": "2014", "title":"Quantum Hashing Via Classical epsilon-universal Hashing Constructions", "abstract": "<p>In the paper, we define the concept of the quantum hash generator and offer\ndesign, which allows to build a large amount of different quantum hash\nfunctions. The construction is based on composition of classical\n\\(\\epsilon\\)-universal hash family and a given family of functions – quantum\nhash generator.\n  The proposed construction combines the properties of robust presentation of\ninformation by classical error-correcting codes together with the possibility\nof highly compressed presentation of information by quantum systems.\n  In particularly, we present quantum hash function based on Reed-Solomon code,\nand we proved, that this construction is optimal in the sense of number of\nqubits needed.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "adimoolam2023efficient", "year": "2023", "title":"Efficient Deduplication And Leakage Detection In Large Scale Image Datasets With A Focus On The Crowdai Mapping Challenge Dataset", "abstract": "<p>Recent advancements in deep learning and computer vision have led to\nwidespread use of deep neural networks to extract building footprints from\nremote-sensing imagery. The success of such methods relies on the availability\nof large databases of high-resolution remote sensing images with high-quality\nannotations. The CrowdAI Mapping Challenge Dataset is one of these datasets\nthat has been used extensively in recent years to train deep neural networks.\nThis dataset consists of \\( \\sim\\ \\)280k training images and \\( \\sim\\ \\)60k testing\nimages, with polygonal building annotations for all images. However, issues\nsuch as low-quality and incorrect annotations, extensive duplication of image\nsamples, and data leakage significantly reduce the utility of deep neural\nnetworks trained on the dataset. Therefore, it is an imperative pre-condition\nto adopt a data validation pipeline that evaluates the quality of the dataset\nprior to its use. To this end, we propose a drop-in pipeline that employs\nperceptual hashing techniques for efficient de-duplication of the dataset and\nidentification of instances of data leakage between training and testing\nsplits. In our experiments, we demonstrate that nearly 250k(\\( \\sim\\ \\)90%)\nimages in the training split were identical. Moreover, our analysis on the\nvalidation split demonstrates that roughly 56k of the 60k images also appear in\nthe training split, resulting in a data leakage of 93%. The source code used\nfor the analysis and de-duplication of the CrowdAI Mapping Challenge dataset is\npublicly available at https://github.com/yeshwanth95/CrowdAI_Hash_and_search .</p>\n", "tags": ["ARXIV","Deep Learning","Has Code","Supervised"] },
{"key": "adir2022privacy", "year": "2022", "title":"Privacy-preserving Record Linkage Using Local Sensitive Hash And Private Set Intersection", "abstract": "<p>The amount of data stored in data repositories increases every year. This\nmakes it challenging to link records between different datasets across\ncompanies and even internally, while adhering to privacy regulations. Address\nor name changes, and even different spelling used for entity data, can prevent\ncompanies from using private deduplication or record-linking solutions such as\nprivate set intersection (PSI). To this end, we propose a new and efficient\nprivacy-preserving record linkage (PPRL) protocol that combines PSI and local\nsensitive hash (LSH) functions, and runs in linear time. We explain the privacy\nguarantees that our protocol provides and demonstrate its practicality by\nexecuting the protocol over two datasets with \\(2^{20}\\) records each, in \\(11-45\\)\nminutes, depending on network settings.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "agarwal2021dynamic", "year": "2021", "title":"Dynamic Enumeration Of Similarity Joins", "abstract": "<p>This paper considers enumerating answers to similarity-join queries under\ndynamic updates: Given two sets of \\(n\\) points \\(A,B\\) in \\(\\mathbb{R}^d\\), a metric\n\\(\\phi(\\cdot)\\), and a distance threshold \\(r &gt; 0\\), report all pairs of points\n\\((a, b) \\in A \\times B\\) with \\(\\phi(a,b) \\le r\\). Our goal is to store \\(A,B\\) into\na dynamic data structure that, whenever asked, can enumerate all result pairs\nwith worst-case delay guarantee, i.e., the time between enumerating two\nconsecutive pairs is bounded. Furthermore, the data structure can be\nefficiently updated when a point is inserted into or deleted from \\(A\\) or \\(B\\).\n  We propose several efficient data structures for answering similarity-join\nqueries in low dimension. For exact enumeration of similarity join, we present\nnear-linear-size data structures for \\(\\ell_1, \\ell_\\infty\\) metrics with\n\\(log^{O(1)} n\\) update time and delay. We show that such a data structure is\nnot feasible for the \\(ℓ₂\\) metric for \\(d \\ge 4\\). For approximate enumeration\nof similarity join, where the distance threshold is a soft constraint, we\nobtain a unified linear-size data structure for \\(\\ell_p\\) metric, with\n\\(log^{O(1)} n\\) delay and update time. In high dimensions, we present an\nefficient data structure with worst-case delay-guarantee using locality\nsensitive hashing (LSH).</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "aghamolaei2023massively", "year": "2023", "title":"Massively-parallel Heat Map Sorting And Applications To Explainable Clustering", "abstract": "<p>Given a set of points labeled with \\(k\\) labels, we introduce the heat map\nsorting problem as reordering and merging the points and dimensions while\npreserving the clusters (labels). A cluster is preserved if it remains\nconnected, i.e., if it is not split into several clusters and no two clusters\nare merged.\n  We prove the problem is NP-hard and we give a fixed-parameter algorithm with\na constant number of rounds in the massively parallel computation model, where\neach machine has a sublinear memory and the total memory of the machines is\nlinear. We give an approximation algorithm for a NP-hard special case of the\nproblem. We empirically compare our algorithm with k-means and density-based\nclustering (DBSCAN) using a dimensionality reduction via locality-sensitive\nhashing on several directed and undirected graphs of email and computer\nnetworks.</p>\n", "tags": ["ARXIV","Graph","Unsupervised"] },
{"key": "aghazadeh2016near", "year": "2016", "title":"Near-isometric Binary Hashing For Large-scale Datasets", "abstract": "<p>We develop a scalable algorithm to learn binary hash codes for indexing\nlarge-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependent\nhashing scheme that quantizes the output of a learned low-dimensional embedding\nto obtain a binary hash code. In contrast to conventional hashing schemes,\nwhich typically rely on an \\(ℓ₂\\)-norm (i.e., average distortion)\nminimization, NIBH is based on a \\(\\ell_{\\infty}\\)-norm (i.e., worst-case\ndistortion) minimization that provides several benefits, including superior\ndistance, ranking, and near-neighbor preservation performance. We develop a\npractical and efficient algorithm for NIBH based on column generation that\nscales well to large datasets. A range of experimental evaluations demonstrate\nthe superiority of NIBH over ten state-of-the-art binary hashing schemes.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "agrell2023glued", "year": "2023", "title":"Glued Lattices Are Better Quantizers Than K_12", "abstract": "<p>40 years ago, Conway and Sloane proposed using the highly symmetrical\nCoxeter-Todd lattice \\(K_{12}\\) for quantization, and estimated its second\nmoment. Since then, all published lists identify \\(K_{12}\\) as the best\n12-dimensional lattice quantizer. Surprisingly, \\(K_{12}\\) is not optimal: we\nconstruct two new 12-dimensional lattices with lower normalized second moments.\nThe new lattices are obtained by gluing together 6-dimensional lattices.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "aguerrebere2023similarity", "year": "2023", "title":"Similarity Search In The Blink Of An Eye With Compressed Indices", "abstract": "<p>Nowadays, data is represented by vectors. Retrieving those vectors, among\nmillions and billions, that are similar to a given query is a ubiquitous\nproblem, known as similarity search, of relevance for a wide range of\napplications. Graph-based indices are currently the best performing techniques\nfor billion-scale similarity search. However, their random-access memory\npattern presents challenges to realize their full potential. In this work, we\npresent new techniques and systems for creating faster and smaller graph-based\nindices. To this end, we introduce a novel vector compression method,\nLocally-adaptive Vector Quantization (LVQ), that uses per-vector scaling and\nscalar quantization to improve search performance with fast similarity\ncomputations and a reduced effective bandwidth, while decreasing memory\nfootprint and barely impacting accuracy. LVQ, when combined with a new\nhigh-performance computing system for graph-based similarity search,\nestablishes the new state of the art in terms of performance and memory\nfootprint. For billions of vectors, LVQ outcompetes the second-best\nalternatives: (1) in the low-memory regime, by up to 20.7x in throughput with\nup to a 3x memory footprint reduction, and (2) in the high-throughput regime by\n5.8x with 1.4x less memory.</p>\n", "tags": ["ARXIV","Graph","Independent","Quantisation"] },
{"key": "aguerrebere2024locally", "year": "2024", "title":"Locally-adaptive Quantization For Streaming Vector Search", "abstract": "<p>Retrieving the most similar vector embeddings to a given query among a\nmassive collection of vectors has long been a key component of countless\nreal-world applications. The recently introduced Retrieval-Augmented Generation\nis one of the most prominent examples. For many of these applications, the\ndatabase evolves over time by inserting new data and removing outdated data. In\nthese cases, the retrieval problem is known as streaming similarity search.\nWhile Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector\ncompression method, yields state-of-the-art search performance for non-evolving\ndatabases, its usefulness in the streaming setting has not been yet\nestablished. In this work, we study LVQ in streaming similarity search. In\nsupport of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and\nmulti-means LVQ that boost its search performance by up to 28% and 27%,\nrespectively. Our studies show that LVQ and its new variants enable blazing\nfast vector search, outperforming its closest competitor by up to 9.4x for\nidentically distributed data and by up to 8.8x under the challenging scenario\nof data distribution shifts (i.e., where the statistical distribution of the\ndata changes over time). We release our contributions as part of Scalable\nVector Search, an open-source library for high-performance similarity search.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "ahle2015complexity", "year": "2015", "title":"On The Complexity Of Inner Product Similarity Join", "abstract": "<p>A number of tasks in classification, information retrieval, recommendation\nsystems, and record linkage reduce to the core problem of inner product\nsimilarity join (IPS join): identifying pairs of vectors in a collection that\nhave a sufficiently large inner product. IPS join is well understood when\nvectors are normalized and some approximation of inner products is allowed.\nHowever, the general case where vectors may have any length appears much more\nchallenging. Recently, new upper bounds based on asymmetric locality-sensitive\nhashing (ALSH) and asymmetric embeddings have emerged, but little has been\nknown on the lower bound side. In this paper we initiate a systematic study of\ninner product similarity join, showing new lower and upper bounds. Our main\nresults are:</p>\n<ul>\n  <li>Approximation hardness of IPS join in subquadratic time, assuming the\nstrong exponential time hypothesis.</li>\n  <li>New upper and lower bounds for (A)LSH-based algorithms. In particular, we\nshow that asymmetry can be avoided by relaxing the LSH definition to only\nconsider the collision probability of distinct elements.</li>\n  <li>A new indexing method for IPS based on linear sketches, implying that our\nhardness results are not far from being tight.\n  Our technical contributions include new asymmetric embeddings that may be of\nindependent interest. At the conceptual level we strive to provide greater\nclarity, for example by distinguishing among signed and unsigned variants of\nIPS join and shedding new light on the effect of asymmetry.</li>\n</ul>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "ahle2016parameter", "year": "2016", "title":"Parameter-free Locality Sensitive Hashing For Spherical Range Reporting", "abstract": "<p>We present a data structure for <em>spherical range reporting</em> on a point set\n\\(S\\), i.e., reporting all points in \\(S\\) that lie within radius \\(r\\) of a given\nquery point \\(q\\). Our solution builds upon the Locality-Sensitive Hashing (LSH)\nframework of Indyk and Motwani, which represents the asymptotically best\nsolutions to near neighbor problems in high dimensions. While traditional LSH\ndata structures have several parameters whose optimal values depend on the\ndistance distribution from \\(q\\) to the points of \\(S\\), our data structure is\nparameter-free, except for the space usage, which is configurable by the user.\nNevertheless, its expected query time basically matches that of an LSH data\nstructure whose parameters have been <em>optimally chosen for the data and query</em>\nin question under the given space constraints. In particular, our data\nstructure provides a smooth trade-off between hard queries (typically addressed\nby standard LSH) and easy queries such as those where the number of points to\nreport is a constant fraction of \\(S\\), or where almost all points in \\(S\\) are far\naway from the query point. In contrast, known data structures fix LSH\nparameters based on certain parameters of the input alone.\n  The algorithm has expected query time bounded by \\(O(t (n/t)^\\rho)\\), where \\(t\\)\nis the number of points to report and \\(\\rho\\in (0,1)\\) depends on the data\ndistribution and the strength of the LSH family used. We further present a\nparameter-free way of using multi-probing, for LSH families that support it,\nand show that for many such families this approach allows us to get expected\nquery time close to \\(O(n^\\rho+t)\\), which is the best we can hope to achieve\nusing LSH. The previously best running time in high dimensions was \\(Ω(t\nn^\\rho)\\). For many data distributions where the intrinsic dimensionality of the\npoint set close to \\(q\\) is low, we can give improved upper bounds on the\nexpected query time.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "ahle2017optimal", "year": "2017", "title":"Optimal Las Vegas Locality Sensitive Data Structures", "abstract": "<p>We show that approximate similarity (near neighbour) search can be solved in\nhigh dimensions with performance matching state of the art (data independent)\nLocality Sensitive Hashing, but with a guarantee of no false negatives.\n  Specifically, we give two data structures for common problems.\n  For \\(c\\)-approximate near neighbour in Hamming space we get query time\n\\(dn^{1/c+o(1)}\\) and space \\(dn^{1+1/c+o(1)}\\) matching that of\n\\cite{indyk1998approximate} and answering a long standing open question\nfrom~\\cite{indyk2000dimensionality} and~\\cite{pagh2016locality} in the\naffirmative.\n  By means of a new deterministic reduction from \\(\\ell_1\\) to Hamming we also\nsolve \\(\\ell_1\\) and \\(ℓ₂\\) with query time \\(d^2n^{1/c+o(1)}\\) and space \\(d^2\nn^{1+1/c+o(1)}\\).\n  For \\((s_1,s_2)\\)-approximate Jaccard similarity we get query time\n\\(dn^{\\rho+o(1)}\\) and space \\(dn^{1+\\rho+o(1)}\\),\n\\(\\rho=log\\frac{1+s_1}{2s_1}\\big/log\\frac{1+s_2}{2s_2}\\), when sets have equal\nsize, matching the performance of~\\cite{tobias2016}.\n  The algorithms are based on space partitions, as with classic LSH, but we\nconstruct these using a combination of brute force, tensoring, perfect hashing\nand splitter functions `a la~\\cite{naor1995splitters}. We also show a new\ndimensionality reduction lemma with 1-sided error.</p>\n", "tags": ["ARXIV","LSH","Unsupervised"] },
{"key": "ahle2020power", "year": "2020", "title":"The Power Of Hashing With Mersenne Primes", "abstract": "<p>The classic way of computing a \\(k\\)-universal hash function is to use a random\ndegree-\\((k-1)\\) polynomial over a prime field \\(\\mathbb Z_p\\). For a fast\ncomputation of the polynomial, the prime \\(p\\) is often chosen as a Mersenne\nprime \\(p=2^b-1\\).\n  In this paper, we show that there are other nice advantages to using Mersenne\nprimes. Our view is that the hash function’s output is a \\(b\\)-bit integer that\nis uniformly distributed in \\(\\{0, \\dots, 2^b-1\\}\\), except that \\(p\\) (the all\n\\texttt1s value in binary) is missing. Uniform bit strings have many nice\nproperties, such as splitting into substrings which gives us two or more hash\nfunctions for the cost of one, while preserving strong theoretical qualities.\nWe call this trick “Two for one” hashing, and we demonstrate it on 4-universal\nhashing in the classic Count Sketch algorithm for second-moment estimation.\n  We also provide a new fast branch-free code for division and modulus with\nMersenne primes. Contrasting our analytic work, this code generalizes to any\nPseudo-Mersenne primes \\(p=2^b-c\\) for small \\(c\\).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ahle2020problem", "year": "2020", "title":"On The Problem Of p_1^-1 In Locality-sensitive Hashing", "abstract": "<p>A Locality-Sensitive Hash (LSH) function is called\n\\((r,cr,p_1,p_2)\\)-sensitive, if two data-points with a distance less than \\(r\\)\ncollide with probability at least \\(p_1\\) while data points with a distance\ngreater than \\(cr\\) collide with probability at most \\(p_2\\). These functions form\nthe basis of the successful Indyk-Motwani algorithm (STOC 1998) for nearest\nneighbour problems. In particular one may build a \\(c\\)-approximate nearest\nneighbour data structure with query time \\(\\tilde O(n^\\rho/p_1)\\) where\n\\(\\rho=\\frac{log1/p_1}{log1/p_2}\\in(0,1)\\). That is, sub-linear time, as long\nas \\(p_1\\) is not too small. This is significant since most high dimensional\nnearest neighbour problems suffer from the curse of dimensionality, and can’t\nbe solved exact, faster than a brute force linear-time scan of the database.\n  Unfortunately, the best LSH functions tend to have very low collision\nprobabilities, \\(p_1\\) and \\(p_2\\). Including the best functions for Cosine and\nJaccard Similarity. This means that the \\(n^\\rho/p_1\\) query time of LSH is often\nnot sub-linear after all, even for approximate nearest neighbours!\n  In this paper, we improve the general Indyk-Motwani algorithm to reduce the\nquery time of LSH to \\(\\tilde O(n^\\rho/p_1^{1-\\rho})\\) (and the space usage\ncorrespondingly.) Since \\(n^\\rho p_1^{\\rho-1} &lt; n \\Leftrightarrow p_1 &gt; n^{-1}\\),\nour algorithm always obtains sublinear query time, for any collision\nprobabilities at least \\(1/n\\). For \\(p_1\\) and \\(p_2\\) small enough, our improvement\nover all previous methods can be <em>up to a factor \\(n\\)</em> in both query time\nand space.\n  The improvement comes from a simple change to the Indyk-Motwani algorithm,\nwhich can easily be implemented in existing software packages.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "aiger2023yes", "year": "2023", "title":"Yes We CANN Constrained Approximate Nearest Neighbors For Local Feature-based Visual Localization", "abstract": "<p>Large-scale visual localization systems continue to rely on 3D point clouds\nbuilt from image collections using structure-from-motion. While the 3D points\nin these models are represented using local image features, directly matching a\nquery image’s local features against the point cloud is challenging due to the\nscale of the nearest-neighbor search problem. Many recent approaches to visual\nlocalization have thus proposed a hybrid method, where first a global (per\nimage) embedding is used to retrieve a small subset of database images, and\nlocal features of the query are matched only against those. It seems to have\nbecome common belief that global embeddings are critical for said\nimage-retrieval in visual localization, despite the significant downside of\nhaving to compute two feature types for each query image. In this paper, we\ntake a step back from this assumption and propose Constrained Approximate\nNearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both\nthe geometry and appearance space using only local features. We first derive\nthe theoretical foundation for k-nearest-neighbor retrieval across multiple\nmetrics and then showcase how CANN improves visual localization. Our\nexperiments on public localization benchmarks demonstrate that our method\nsignificantly outperforms both state-of-the-art global feature-based retrieval\nand approaches using local feature aggregation schemes. Moreover, it is an\norder of magnitude faster in both index and query time than feature aggregation\nschemes for these datasets. Code:\n\\url{https://github.com/google-research/google-research/tree/master/cann}</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval"] },
{"key": "aizenbud2017similarity", "year": "2017", "title":"Similarity Search Over Graphs Using Localized Spectral Analysis", "abstract": "<p>This paper provides a new similarity detection algorithm. Given an input set\nof multi-dimensional data points, where each data point is assumed to be\nmulti-dimensional, and an additional reference data point for similarity\nfinding, the algorithm uses kernel method that embeds the data points into a\nlow dimensional manifold. Unlike other kernel methods, which consider the\nentire data for the embedding, our method selects a specific set of kernel\neigenvectors. The eigenvectors are chosen to separate between the data points\nand the reference data point so that similar data points can be easily\nidentified as being distinct from most of the members in the dataset.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "akhremtsev2017high", "year": "2017", "title":"High-quality Shared-memory Graph Partitioning", "abstract": "<p>Partitioning graphs into blocks of roughly equal size such that few edges run\nbetween blocks is a frequently needed operation in processing graphs. Recently,\nsize, variety, and structural complexity of these networks has grown\ndramatically. Unfortunately, previous approaches to parallel graph partitioning\nhave problems in this context since they often show a negative trade-off\nbetween speed and quality. We present an approach to multi-level shared-memory\nparallel graph partitioning that guarantees balanced solutions, shows high\nspeed-ups for a variety of large graphs and yields very good quality\nindependently of the number of cores used. For example, on 31 cores, our\nalgorithm partitions our largest test instance into 16 blocks cutting less than\nhalf the number of edges than our main competitor when both algorithms are\ngiven the same amount of time. Important ingredients include parallel label\npropagation for both coarsening and improvement, parallel initial partitioning,\na simple yet effective approach to parallel localized local search, and fast\nlocality preserving hash tables.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "akhtman2012approximate", "year": "2012", "title":"An Approximate Coding-rate Versus Minimum Distance Formula For Binary Codes", "abstract": "<p>We devise an analytically simple as well as invertible approximate\nexpression, which describes the relation between the minimum distance of a\nbinary code and the corresponding maximum attainable code-rate. For example,\nfor a rate-(1/4), length-256 binary code the best known bounds limit the\nattainable minimum distance to 65&lt;d(n=256,k=64)&lt;90, while our solution yields\nd(n=256,k=64)=74.4. The proposed formula attains the approximation accuracy\nwithin the rounding error for ~97% of (n,k) scenarios, where the exact value of\nthe minimum distance is known. The results provided may be utilized for the\nanalysis and design of efficient communication systems.</p>\n", "tags": ["ARXIV"] },
{"key": "akinwalle2009usefulness", "year": "2009", "title":"The Usefulness Of Multilevel Hash Tables With Multiple Hash Functions In Large Databases", "abstract": "<p>In this work, attempt is made to select three good hash functions which\nuniformly distribute hash values that permute their internal states and allow\nthe input bits to generate different output bits. These functions are used in\ndifferent levels of hash tables that are coded in Java Programming Language and\na quite number of data records serve as primary data for testing the\nperformances. The result shows that the two-level hash tables with three\ndifferent hash functions give a superior performance over one-level hash table\nwith two hash functions or zero-level hash table with one function in term of\nreducing the conflict keys and quick lookup for a particular element. The\nresult assists to reduce the complexity of join operation in query language\nfrom O(n2) to O(1) by placing larger query result, if any, in multilevel hash\ntables with multiple hash functions and generate shorter query result.</p>\n", "tags": ["Independent"] },
{"key": "aksoy2022satellite", "year": "2022", "title":"Satellite Image Search In Agoraeo", "abstract": "<p>The growing operational capability of global Earth Observation (EO) creates\nnew opportunities for data-driven approaches to understand and protect our\nplanet. However, the current use of EO archives is very restricted due to the\nhuge archive sizes and the limited exploration capabilities provided by EO\nplatforms. To address this limitation, we have recently proposed MiLaN, a\ncontent-based image retrieval approach for fast similarity search in satellite\nimage archives. MiLaN is a deep hashing network based on metric learning that\nencodes high-dimensional image features into compact binary hash codes. We use\nthese codes as keys in a hash table to enable real-time nearest neighbor search\nand highly accurate retrieval. In this demonstration, we showcase the\nefficiency of MiLaN by integrating it with EarthQube, a browser and search\nengine within AgoraEO. EarthQube supports interactive visual exploration and\nQuery-by-Example over satellite image repositories. Demo visitors will interact\nwith EarthQube playing the role of different users that search images in a\nlarge-scale remote sensing archive by their semantic content and apply other\nfilters.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "alakuijala2016fast", "year": "2016", "title":"Fast Keyed Hash/pseudo-random Function Using SIMD Multiply And Permute", "abstract": "<p>HighwayHash is a new pseudo-random function based on SIMD multiply and\npermute instructions for thorough and fast hashing. It is 5.2 times as fast as\nSipHash for 1 KiB inputs. An open-source implementation is available under a\npermissive license. We discuss design choices and provide statistical analysis,\nspeed measurements and preliminary cryptanalysis. Assuming it withstands\nfurther analysis, strengthened variants may also substantially accelerate file\nchecksums and stream ciphers.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "alexe2011exploiting", "year": "2011", "title":"Exploiting Spatial Overlap To Efficiently Compute Appearance Distances Between Image Windows", "abstract": "<p>We present a computationally efficient technique to compute the distance of high-dimensional appearance descriptor vectors between image windows.  The method exploits the relation between appearance distance and spatial overlap.  We derive an upper bound on appearance distance given the spatial overlap of two windows in an image,  and use it to bound the distances of many pairs between two images.  We propose algorithms that build on these basic operations to efficiently solve tasks relevant to many computer vision applications, such as finding all pairs of windows between two images with distance smaller than a threshold,  or finding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18]and on hashing [21].    For example, our algorithm finds the most similar pair of windows between two images while computing only 1% of all distances on average.</p>\n", "tags": ["NEURIPS"] },
{"key": "ali2020cross", "year": "2020", "title":"Cross Hashing Anonymizing Encounters In Decentralised Contact Tracing Protocols", "abstract": "<p>During the COVID-19 (SARS-CoV-2) epidemic, Contact Tracing emerged as an\nessential tool for managing the epidemic. App-based solutions have emerged for\nContact Tracing, including a protocol designed by Apple and Google (influenced\nby an open-source protocol known as DP3T). This protocol contains two\nwell-documented de-anonymisation attacks. Firstly that when someone is marked\nas having tested positive and their keys are made public, they can be tracked\nover a large geographic area for 24 hours at a time. Secondly, whilst the app\nrequires a minimum exposure duration to register a contact, there is no\ncryptographic guarantee for this property. This means an adversary can scan\nBluetooth networks and retrospectively find who is infected. We propose a novel\n“cross hashing” approach to cryptographically guarantee minimum exposure\ndurations. We further mitigate the 24-hour data exposure of infected\nindividuals and reduce computational time for identifying if a user has been\nexposed using \\(k\\)-Anonymous buckets of hashes and Private Set Intersection. We\nempirically demonstrate that this modified protocol can offer like-for-like\nefficacy to the existing protocol.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "ali2020practical", "year": "2020", "title":"Practical Hash-based Anonymity For MAC Addresses", "abstract": "<p>Given that a MAC address can uniquely identify a person or a vehicle,\ncontinuous tracking over a large geographical scale has raised serious privacy\nconcerns amongst governments and the general public. Prior work has\ndemonstrated that simple hash-based approaches to anonymization can be easily\ninverted due to the small search space of MAC addresses. In particular, it is\npossible to represent the entire allocated MAC address space in 39 bits and\nthat frequency-based attacks allow for 50% of MAC addresses to be enumerated in\n31 bits. We present a practical approach to MAC address anonymization using\nboth computationally expensive hash functions and truncating the resulting\nhashes to allow for k-anonymity. We provide an expression for computing the\npercentage of expected collisions, demonstrating that for digests of 24 bits it\nis possible to store up to 168,617 MAC addresses with the rate of collisions\nless than 1%. We experimentally demonstrate that a rate of collision of 1% or\nless can be achieved by storing data sets of 100 MAC addresses in 13 bits,\n1,000 MAC addresses in 17 bits and 10,000 MAC addresses in 20 bits.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "allgood2020quantum", "year": "2020", "title":"A Quantum Algorithm To Locate Unknown Hashgrams", "abstract": "<p>Quantum computing has evolved quickly in recent years and is showing\nsignificant benefits in a variety of fields, especially in the realm of\ncybersecurity. The combination of software used to locate the most frequent\nhashes and \\(n\\)-grams that identify malicious software could greatly benefit\nfrom a quantum algorithm. By loading the table of hashes and \\(n\\)-grams into a\nquantum computer we can speed up the process of mapping \\(n\\)-grams to their\nhashes. The first phase will be to use KiloGram to find the top-\\(k\\) hashes and\n\\(n\\)-grams for a large malware corpus. From here, the resulting hash table is\nthen loaded into a quantum simulator. A quantum search algorithm is then used\nsearch among every permutation of the entangled key and value pairs to find the\ndesired hash value. This prevents one from having to re-compute hashes for a\nset of \\(n\\)-grams, which can take on average \\(O(MN)\\) time, whereas the quantum\nalgorithm could take \\(O(\\sqrt{N})\\) in the number of table lookups to find the\ndesired hash values.</p>\n", "tags": ["ARXIV"] },
{"key": "alomari2019scalable", "year": "2019", "title":"Scalable Source Code Similarity Detection In Large Code Repositories", "abstract": "<p>Source code similarity are increasingly used in application development to\nidentify clones, isolate bugs, and find copy-rights violations. Similar code\nfragments can be very problematic due to the fact that errors in the original\ncode must be fixed in every copy. Other maintenance changes, such as extensions\nor patches, must be applied multiple times. Furthermore, the diversity of\ncoding styles and flexibility of modern languages makes it difficult and cost\nineffective to manually inspect large code repositories. Therefore, detection\nis only feasible by automatic techniques. We present an efficient and scalable\napproach for similar code fragment identification based on source code control\nflow graphs fingerprinting. The source code is processed to generate control\nflow graphs that are then hashed to create a unique fingerprint of the code\ncapturing semantics as well as syntax similarity. The fingerprints can then be\nefficiently stored and retrieved to perform similarity search between code\nfragments. Experimental results from our prototype implementation supports the\nvalidity of our approach and show its effectiveness and efficiency in\ncomparison with other solutions.</p>\n", "tags": ["Graph"] },
{"key": "alparslan2020towards", "year": "2020", "title":"Towards Evaluating Gaussian Blurring In Perceptual Hashing As A Facial Image Filter", "abstract": "<p>With the growth in social media, there is a huge amount of images of faces\navailable on the internet. Often, people use other people’s pictures on their\nown profile. Perceptual hashing is often used to detect whether two images are\nidentical. Therefore, it can be used to detect whether people are misusing\nothers’ pictures. In perceptual hashing, a hash is calculated for a given\nimage, and a new test image is mapped to one of the existing hashes if\nduplicate features are present. Therefore, it can be used as an image filter to\nflag banned image content or adversarial attacks –which are modifications that\nare made on purpose to deceive the filter– even though the content might be\nchanged to deceive the filters. For this reason, it is critical for perceptual\nhashing to be robust enough to take transformations such as resizing, cropping,\nand slight pixel modifications into account. In this paper, we would like to\npropose to experiment with effect of gaussian blurring in perceptual hashing\nfor detecting misuse of personal images specifically for face images. We\nhypothesize that use of gaussian blurring on the image before calculating its\nhash will increase the accuracy of our filter that detects adversarial attacks\nwhich consist of image cropping, adding text annotation, and image rotation.</p>\n", "tags": ["ARXIV"] },
{"key": "amara2021nearest", "year": "2021", "title":"Nearest Neighbor Search With Compact Codes A Decoder Perspective", "abstract": "<p>Modern approaches for fast retrieval of similar vectors on billion-scaled\ndatasets rely on compressed-domain approaches such as binary sketches or\nproduct quantization. These methods minimize a certain loss, typically the mean\nsquared error or other objective functions tailored to the retrieval problem.\nIn this paper, we re-interpret popular methods such as binary hashing or\nproduct quantizers as auto-encoders, and point out that they implicitly make\nsuboptimal assumptions on the form of the decoder. We design\nbackward-compatible decoders that improve the reconstruction of the vectors\nfrom the same codes, which translates to a better performance in nearest\nneighbor search. Our method significantly improves over binary hashing methods\nor product quantization on popular benchmarks.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "amato2016reducing", "year": "2016", "title":"On Reducing The Number Of Visual Words In The Bag-of-features Representation", "abstract": "<p>A new class of applications based on visual search engines are emerging,\nespecially on smart-phones that have evolved into powerful tools for processing\nimages and videos. The state-of-the-art algorithms for large visual content\nrecognition and content based similarity search today use the “Bag of Features”\n(BoF) or “Bag of Words” (BoW) approach. The idea, borrowed from text retrieval,\nenables the use of inverted files. A very well known issue with this approach\nis that the query images, as well as the stored data, are described with\nthousands of words. This poses obvious efficiency problems when using inverted\nfiles to perform efficient image matching. In this paper, we propose and\ncompare various techniques to reduce the number of words describing an image to\nimprove efficiency and we study the effects of this reduction on effectiveness\nin landmark recognition and retrieval scenarios. We show that very relevant\nimprovement in performance are achievable still preserving the advantages of\nthe BoF base approach.</p>\n", "tags": ["Text Retrieval"] },
{"key": "amato2016using", "year": "2016", "title":"Using Apache Lucene To Search Vector Of Locally Aggregated Descriptors", "abstract": "<p>Surrogate Text Representation (STR) is a profitable solution to efficient\nsimilarity search on metric space using conventional text search engines, such\nas Apache Lucene. This technique is based on comparing the permutations of some\nreference objects in place of the original metric distance. However, the\nAchilles heel of STR approach is the need to reorder the result set of the\nsearch according to the metric distance. This forces to use a support database\nto store the original objects, which requires efficient random I/O on a fast\nsecondary memory (such as flash-based storages). In this paper, we propose to\nextend the Surrogate Text Representation to specifically address a class of\nvisual metric objects known as Vector of Locally Aggregated Descriptors (VLAD).\nThis approach is based on representing the individual sub-vectors forming the\nVLAD vector with the STR, providing a finer representation of the vector and\nenabling us to get rid of the reordering phase. The experiments on a publicly\navailable dataset show that the extended STR outperforms the baseline STR\nachieving satisfactory performance near to the one obtained with the original\nVLAD vectors.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "amir2014polynomials", "year": "2014", "title":"Polynomials A New Tool For Length Reduction In Binary Discrete Convolutions", "abstract": "<p>Efficient handling of sparse data is a key challenge in Computer Science.\nBinary convolutions, such as polynomial multiplication or the Walsh Transform\nare a useful tool in many applications and are efficiently solved.\n  In the last decade, several problems required efficient solution of sparse\nbinary convolutions. both randomized and deterministic algorithms were\ndeveloped for efficiently computing the sparse polynomial multiplication. The\nkey operation in all these algorithms was length reduction. The sparse data is\nmapped into small vectors that preserve the convolution result. The reduction\nmethod used to-date was the modulo function since it preserves location (of the\n“1” bits) up to cyclic shift.\n  To date there is no known efficient algorithm for computing the sparse Walsh\ntransform. Since the modulo function does not preserve the Walsh transform a\nnew method for length reduction is needed. In this paper we present such a new\nmethod - polynomials. This method enables the development of an efficient\nalgorithm for computing the binary sparse Walsh transform. To our knowledge,\nthis is the first such algorithm. We also show that this method allows a faster\ndeterministic computation of sparse polynomial multiplication than currently\nknown in the literature.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "amrouche2021hashing", "year": "2021", "title":"Hashing And Metric Learning For Charged Particle Tracking", "abstract": "<p>We propose a novel approach to charged particle tracking at high intensity\nparticle colliders based on Approximate Nearest Neighbors search. With hundreds\nof thousands of measurements per collision to be reconstructed e.g. at the High\nLuminosity Large Hadron Collider, the currently employed combinatorial track\nfinding approaches become inadequate. Here, we use hashing techniques to\nseparate measurements into buckets of 20-50 hits and increase their purity\nusing metric learning. Two different approaches are studied to further resolve\ntracks inside buckets: Local Fisher Discriminant Analysis and Neural Networks\nfor triplet similarity learning. We demonstrate the proposed approach on\nsimulated collisions and show significant speed improvement with bucket\ntracking efficiency of 96% and a fake rate of 8% on unseen particle events.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "andoni2013beyond", "year": "2013", "title":"Beyond Locality-sensitive Hashing", "abstract": "<p>We present a new data structure for the c-approximate near neighbor problem\n(ANN) in the Euclidean space. For n points in R^d, our algorithm achieves\nO(n^{\\rho} + d log n) query time and O(n^{1 + \\rho} + d log n) space, where\n\\rho &lt;= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over the\nresult by Andoni and Indyk (FOCS 2006) and the first data structure that\nbypasses a locality-sensitive hashing lower bound proved by O’Donnell, Wu and\nZhou (ICS 2011). By a standard reduction we obtain a data structure for the\nHamming space and \\ell_1 norm with \\rho &lt;= 7/(8c) + O(1/c^{3/2}) + o(1), which\nis the first improvement over the result of Indyk and Motwani (STOC 1998).</p>\n", "tags": ["ARXIV","FOCS"] },
{"key": "andoni2015optimal", "year": "2015", "title":"Optimal Data-dependent Hashing For Approximate Near Neighbors", "abstract": "<p>We show an optimal data-dependent hashing scheme for the approximate near\nneighbor problem. For an \\(n\\)-point data set in a \\(d\\)-dimensional space our data\nstructure achieves query time \\(O(d n^{\\rho+o(1)})\\) and space \\(O(n^{1+\\rho+o(1)}</p>\n<ul>\n  <li>dn)\\), where \\(\\rho=\\tfrac{1}{2c^2-1}\\) for the Euclidean space and\napproximation \\(c&gt;1\\). For the Hamming space, we obtain an exponent of\n\\(\\rho=\\tfrac{1}{2c-1}\\).\nOur result completes the direction set forth in [AINR14] who gave a\nproof-of-concept that data-dependent hashing can outperform classical Locality\nSensitive Hashing (LSH). In contrast to [AINR14], the new bound is not only\noptimal, but in fact improves over the best (optimal) LSH data structures\n[IM98,AI06] for all approximation factors \\(c&gt;1\\).\nFrom the technical perspective, we proceed by decomposing an arbitrary\ndataset into several subsets that are, in a certain sense, pseudo-random.</li>\n</ul>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "andoni2015practical", "year": "2015", "title":"Practical And Optimal LSH For Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal  running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH (Andoni-Indyk-Nguyen-Razenshteyn 2014) (Andoni-Razenshteyn 2015)), our algorithm is also practical, improving upon the well-studied hyperplane LSH (Charikar 2002) in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "andoni2015tight", "year": "2015", "title":"Tight Lower Bounds For Data-dependent Locality-sensitive Hashing", "abstract": "<p>We prove a tight lower bound for the exponent \\(\\rho\\) for data-dependent\nLocality-Sensitive Hashing schemes, recently used to design efficient solutions\nfor the \\(c\\)-approximate nearest neighbor search. In particular, our lower bound\nmatches the bound of \\(\\rho\\le \\frac{1}{2c-1}+o(1)\\) for the \\(\\ell_1\\) space,\nobtained via the recent algorithm from [Andoni-Razenshteyn, STOC’15].\n  In recent years it emerged that data-dependent hashing is strictly superior\nto the classical Locality-Sensitive Hashing, when the hash function is\ndata-independent. In the latter setting, the best exponent has been already\nknown: for the \\(\\ell_1\\) space, the tight bound is \\(\\rho=1/c\\), with the upper\nbound from [Indyk-Motwani, STOC’98] and the matching lower bound from\n[O’Donnell-Wu-Zhou, ITCS’11].\n  We prove that, even if the hashing is data-dependent, it must hold that\n\\(\\rho\\ge \\frac{1}{2c-1}-o(1)\\). To prove the result, we need to formalize the\nexact notion of data-dependent hashing that also captures the complexity of the\nhash functions (in addition to their collision properties). Without restricting\nsuch complexity, we would allow for obviously infeasible solutions such as the\nVoronoi diagram of a dataset. To preclude such solutions, we require our hash\nfunctions to be succinct. This condition is satisfied by all the known\nalgorithmic results.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "andoni2016approximate", "year": "2016", "title":"Approximate Near Neighbors For General Symmetric Norms", "abstract": "<p>We show that every symmetric normed space admits an efficient nearest\nneighbor search data structure with doubly-logarithmic approximation.\nSpecifically, for every \\(n\\), \\(d = n^{o(1)}\\), and every \\(d\\)-dimensional\nsymmetric norm \\(|\\cdot|\\), there exists a data structure for\n\\(\\mathrm{poly}(log log n)\\)-approximate nearest neighbor search over\n\\(|\\cdot|\\) for \\(n\\)-point datasets achieving \\(n^{o(1)}\\) query time and\n\\(n^{1+o(1)}\\) space. The main technical ingredient of the algorithm is a\nlow-distortion embedding of a symmetric norm into a low-dimensional iterated\nproduct of top-\\(k\\) norms.\n  We also show that our techniques cannot be extended to general norms.</p>\n", "tags": ["ARXIV"] },
{"key": "andoni2016lower", "year": "2016", "title":"Lower Bounds On Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>We show tight lower bounds for the entire trade-off between space and query\ntime for the Approximate Near Neighbor search problem. Our lower bounds hold in\na restricted model of computation, which captures all hashing-based approaches.\nIn articular, our lower bound matches the upper bound recently shown in\n[Laarhoven 2015] for the random instance on a Euclidean sphere (which we show\nin fact extends to the entire space \\(\\mathbb{R}^d\\) using the techniques from\n[Andoni, Razenshteyn 2015]).\n  We also show tight, unconditional cell-probe lower bounds for one and two\nprobes, improving upon the best known bounds from [Panigrahy, Talwar, Wieder\n2010]. In particular, this is the first space lower bound (for any static data\nstructure) for two probes which is not polynomially smaller than for one probe.\nTo show the result for two probes, we establish and exploit a connection to\nlocally-decodable codes.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "andoni2016optimal", "year": "2016", "title":"Optimal Hashing-based Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>[See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n\\(c\\)-Approximate Near Neighbor Search problem. For the \\(d\\)-dimensional Euclidean\nspace and \\(n\\)-point datasets, we develop a data structure with space \\(n^{1 +\n\\rho_u + o(1)} + O(dn)\\) and query time \\(n^{\\rho_q + o(1)} + d n^{o(1)}\\) for\nevery \\(\\rho_u, \\rho_q \\geq 0\\) such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor \\(c &gt; 1\\), improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for \\(\\rho_q = 0\\), improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes.</p>\n", "tags": ["ARXIV","FOCS"] },
{"key": "andoni2021from", "year": "2021", "title":"From Average Embeddings To Nearest Neighbor Search", "abstract": "<p>In this note, we show that one can use average embeddings, introduced\nrecently in [Naor’20, arXiv:1905.01280], to obtain efficient algorithms for\napproximate nearest neighbor search. In particular, a metric \\(X\\) embeds into\n\\(ℓ₂\\) on average, with distortion \\(D\\), if, for any distribution \\(\\mu\\) on\n\\(X\\), the embedding is \\(D\\) Lipschitz and the (square of) distance does not\ndecrease on average (wrt \\(\\mu\\)). In particular existence of such an embedding\n(assuming it is efficient) implies a \\(O(D^3)\\) approximate nearest neighbor\nsearch under \\(X\\). This can be seen as a strengthening of the classic\n(bi-Lipschitz) embedding approach to nearest neighbor search, and is another\napplication of data-dependent hashing paradigm.</p>\n", "tags": ["ARXIV"] },
{"key": "andoni2021learning", "year": "2021", "title":"Learning To Hash Robustly Guaranteed", "abstract": "<p>The indexing algorithms for the high-dimensional nearest neighbor search\n(NNS) with the best worst-case guarantees are based on the randomized Locality\nSensitive Hashing (LSH), and its derivatives. In practice, many heuristic\napproaches exist to “learn” the best indexing method in order to speed-up NNS,\ncrucially adapting to the structure of the given dataset.\n  Oftentimes, these heuristics outperform the LSH-based algorithms on real\ndatasets, but, almost always, come at the cost of losing the guarantees of\neither correctness or robust performance on adversarial queries, or apply to\ndatasets with an assumed extra structure/model. In this paper, we design an NNS\nalgorithm for the Hamming space that has worst-case guarantees essentially\nmatching that of theoretical algorithms, while optimizing the hashing to the\nstructure of the dataset (think instance-optimal algorithms) for performance on\nthe minimum-performing query. We evaluate the algorithm’s ability to optimize\nfor a given dataset both theoretically and practically. On the theoretical\nside, we exhibit a natural setting (dataset model) where our algorithm is much\nbetter than the standard theoretical one. On the practical side, we run\nexperiments that show that our algorithm has a 1.8x and 2.1x better recall on\nthe worst-performing queries to the MNIST and ImageNet datasets.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "andoni2024learning", "year": "2024", "title":"Learning To Hash Robustly With Guarantees", "abstract": "<p>The indexing algorithms for the high-dimensional nearest neighbor search (NNS) with the best worst-case guarantees are based on the randomized Locality Sensitive Hashing (LSH), and its derivatives. In practice, many heuristic approaches exist to “learn” the best indexing method in order to speed-up NNS, crucially adapting to the structure of the given dataset. Oftentimes, these heuristics outperform the LSH-based algorithms on real datasets, but, almost always, come at the cost of losing the guarantees of either correctness or robust performance on adversarial queries, or apply to datasets with an assumed extra structure/model. In this paper, we design an NNS algorithm for the Hamming space that has worst-case guarantees essentially matching that of theoretical algorithms, while optimizing the hashing to the structure of the dataset (think instance-optimal algorithms) for performance on the minimum-performing query. We evaluate the algorithm’s ability to optimize for a given dataset both theoretically and practically. On the theoretical side, we exhibit a natural setting (dataset model) where our algorithm is much better than the standard theoretical one. On the practical side, we run experiments that show that our algorithm has a 1.8x and 2.1x better recall on the worst-performing queries to the MNIST and ImageNet datasets.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "andoni2024near", "year": "2024", "title":"Near-optimal Hashing Algorithms For Approximate Nearest Neighbor In High Dimensions", "abstract": "<p>We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice</p>\n", "tags": ["ARXIV"] },
{"key": "andoni2024practical", "year": "2024", "title":"Practical And Optimal LSH For Angular Distance", "abstract": "<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular\ndistance that yields an approximate Near Neighbor Search algorithm with the\nasymptotically optimal running time exponent. Unlike earlier algorithms with this\nproperty (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe\nversion of this algorithm and conduct an experimental evaluation on real\nand synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the\nquality of any LSH family for angular distance. Our lower bound implies that the\nabove LSH family exhibits a trade-off between evaluation time and quality that is\nclose to optimal for a natural class of LSH functions.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "andrecut2021additive", "year": "2021", "title":"Additive Feature Hashing", "abstract": "<p>The hashing trick is a machine learning technique used to encode categorical\nfeatures into a numerical vector representation of pre-defined fixed length. It\nworks by using the categorical hash values as vector indices, and updating the\nvector values at those indices. Here we discuss a different approach based on\nadditive-hashing and the “almost orthogonal” property of high-dimensional\nrandom vectors. That is, we show that additive feature hashing can be performed\ndirectly by adding the hash values and converting them into high-dimensional\nnumerical vectors. We show that the performance of additive feature hashing is\nsimilar to the hashing trick, and we illustrate the results numerically using\nsynthetic, language recognition, and SMS spam detection data.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "andreeva2021compactness", "year": "2021", "title":"Compactness Of Hashing Modes And Efficiency Beyond Merkle Tree", "abstract": "<p>We revisit the classical problem of designing optimally efficient\ncryptographically secure hash functions. Hash functions are traditionally\ndesigned via applying modes of operation on primitives with smaller domains.\nThe results of Shrimpton and Stam (ICALP 2008), Rogaway and Steinberger (CRYPTO\n2008), and Mennink and Preneel (CRYPTO 2012) show how to achieve optimally\nefficient designs of \\(2n\\)-to-\\(n\\)-bit compression functions from non-compressing\nprimitives with asymptotically optimal \\(2^{n/2-\\epsilon}\\)-query collision\nresistance. Designing optimally efficient and secure hash functions for larger\ndomains (\\(&gt; 2n\\) bits) is still an open problem.\n  In this work we propose the new \\textit{compactness} efficiency notion. It\nallows us to focus on asymptotically optimally collision resistant hash\nfunction and normalize their parameters based on Stam’s bound from CRYPTO 2008\nto obtain maximal efficiency.\n  We then present two tree-based modes of operation\n  -Our first construction is an \\underline{A}ugmented \\underline{B}inary\nT\\underline{r}ee (ABR) mode. The design is a \\((2^{\\ell}+2^{\\ell-1}\n-1)n\\)-to-\\(n\\)-bit hash function making a total of \\((2^{\\ell}-1)\\) calls to\n\\(2n\\)-to-\\(n\\)-bit compression functions for any \\(\\ell\\geq 2\\). Our construction is\noptimally compact with asymptotically (optimal) \\(2^{n/2-\\epsilon}\\)-query\ncollision resistance in the ideal model. For a tree of height \\(\\ell\\), in\ncomparison with Merkle tree, the \\(ABR\\) mode processes additional\n\\((2^{\\ell-1}-1)\\) data blocks making the same number of internal compression\nfunction calls.\n  -While the \\(ABR\\) mode achieves collision resistance, it fails to achieve\nindifferentiability from a random oracle within \\(2^{n/3}\\) queries. \\(ABR^{+}\\)\ncompresses only \\(1\\) less data block than \\(ABR\\) with the same number of\ncompression calls and achieves in addition indifferentiability up to\n\\(2^{n/2-\\epsilon}\\) queries.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "andré2017accelerated", "year": "2017", "title":"Accelerated Nearest Neighbor Search With Quick ADC", "abstract": "<p>Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "andré2017exploiting", "year": "2017", "title":"Exploiting Modern Hardware For High-dimensional Nearest Neighbor Search", "abstract": "<p>Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "andré2019derived", "year": "2019", "title":"Derived Codebooks For High-accuracy Nearest Neighbor Search", "abstract": "<p>High-dimensional Nearest Neighbor (NN) search is central in multimedia search\nsystems. Product Quantization (PQ) is a widespread NN search technique which\nhas a high performance and good scalability. PQ compresses high-dimensional\nvectors into compact codes thanks to a combination of quantizers. Large\ndatabases can, therefore, be stored entirely in RAM, enabling fast responses to\nNN queries. In almost all cases, PQ uses 8-bit quantizers as they offer low\nresponse times. In this paper, we advocate the use of 16-bit quantizers.\nCompared to 8-bit quantizers, 16-bit quantizers boost accuracy but they\nincrease response time by a factor of 3 to 10. We propose a novel approach that\nallows 16-bit quantizers to offer the same response time as 8-bit quantizers,\nwhile still providing a boost of accuracy. Our approach builds on two key\nideas: (i) the construction of derived codebooks that allow a fast and\napproximate distance evaluation, and (ii) a two-pass NN search procedure which\nbuilds a candidate set using the derived codebooks, and then refines it using\n16-bit quantizers. On 1 billion SIFT vectors, with an inverted index, our\napproach offers a Recall@100 of 0.85 in 5.2 ms. By contrast, 16-bit quantizers\nalone offer a Recall@100 of 0.85 in 39 ms, and 8-bit quantizers a Recall@100 of\n0.82 in 3.8 ms.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "annaji2009parallelization", "year": "2009", "title":"Parallelization Of The LBG Vector Quantization Algorithm For Shared Memory Systems", "abstract": "<p>This paper proposes a parallel approach for the Vector Quantization (VQ)\nproblem in image processing. VQ deals with codebook generation from the input\ntraining data set and replacement of any arbitrary data with the nearest\ncodevector. Most of the efforts in VQ have been directed towards designing\nparallel search algorithms for the codebook, and little has hitherto been done\nin evolving a parallelized procedure to obtain an optimum codebook. This\nparallel algorithm addresses the problem of designing an optimum codebook using\nthe traditional LBG type of vector quantization algorithm for shared memory\nsystems and for the efficient usage of parallel processors. Using the codebook\nformed from a training set, any arbitrary input data is replaced with the\nnearest codevector from the codebook. The effectiveness of the proposed\nalgorithm is indicated.</p>\n", "tags": ["ICIP","Quantisation","Supervised"] },
{"key": "apple2021halftimehash", "year": "2021", "title":"Halftimehash Modern Hashing Without 64-bit Multipliers Or Finite Fields", "abstract": "<p>HalftimeHash is a new algorithm for hashing long strings. The goals are few\ncollisions (different inputs that produce identical output hash values) and\nhigh performance.\n  Compared to the fastest universal hash functions on long strings (clhash and\nUMASH) HalftimeHash decreases collision probability while also increasing\nperformance by over 50%, exceeding 16 bytes per cycle. In addition,\nHalftimeHash does not use any widening 64-bit multiplications or any finite\nfield arithmetic that could limit its portability.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "appleton2015multi", "year": "2015", "title":"Multi-probe Consistent Hashing", "abstract": "<p>We describe a consistent hashing algorithm which performs multiple lookups\nper key in a hash table of nodes. It requires no additional storage beyond the\nhash table, and achieves a peak-to-average load ratio of 1 + epsilon with just\n1 + 1/epsilon lookups per key.</p>\n", "tags": ["ARXIV"] },
{"key": "argerich2016feature", "year": "2016", "title":"Hash2vec Feature Hashing For Word Embeddings", "abstract": "<p>In this paper we propose the application of feature hashing to create word\nembeddings for natural language processing. Feature hashing has been used\nsuccessfully to create document vectors in related tasks like document\nclassification. In this work we show that feature hashing can be applied to\nobtain word embeddings in linear time with the size of the data. The results\nshow that this algorithm, that does not need training, is able to capture the\nsemantic meaning of words. We compare the results against GloVe showing that\nthey are similar. As far as we know this is the first application of feature\nhashing to the word embeddings problem and the results indicate this is a\nscalable technique with practical results for NLP applications.</p>\n", "tags": ["Supervised"] },
{"key": "argerich2017generic", "year": "2017", "title":"Generic LSH Families For The Angular Distance Based On Johnson-lindenstrauss Projections And Feature Hashing LSH", "abstract": "<p>In this paper we propose the creation of generic LSH families for the angular\ndistance based on Johnson-Lindenstrauss projections. We show that feature\nhashing is a valid J-L projection and propose two new LSH families based on\nfeature hashing. These new LSH families are tested on both synthetic and real\ndatasets with very good results and a considerable performance improvement over\nother LSH families. While the theoretical analysis is done for the angular\ndistance, these families can also be used in practice for the euclidean\ndistance with excellent results [2]. Our tests using real datasets show that\nthe proposed LSH functions work well for the euclidean distance.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "arponen2019shrewd", "year": "2019", "title":"SHREWD Semantic Hierarchy-based Relational Embeddings For Weakly-supervised Deep Hashing", "abstract": "<p>Using class labels to represent class similarity is a typical approach to\ntraining deep hashing systems for retrieval; samples from the same or different\nclasses take binary 1 or 0 similarity values. This similarity does not model\nthe full rich knowledge of semantic relations that may be present between data\npoints. In this work we build upon the idea of using semantic hierarchies to\nform distance metrics between all available sample labels; for example cat to\ndog has a smaller distance than cat to guitar. We combine this type of semantic\ndistance into a loss function to promote similar distances between the deep\nneural network embeddings. We also introduce an empirical Kullback-Leibler\ndivergence loss term to promote binarization and uniformity of the embeddings.\nWe test the resulting SHREWD method and demonstrate improvements in\nhierarchical retrieval scores using compact, binary hash codes instead of real\nvalued ones, and show that in a weakly supervised hashing setting we are able\nto learn competitively without explicitly relying on class labels, but instead\non similarities between labels.</p>\n", "tags": ["ARXIV","Supervised","Weakly Supervised"] },
{"key": "artetxe2018massively", "year": "2018", "title":"Massively Multilingual Sentence Embeddings For Zero-shot Cross-lingual Transfer And Beyond", "abstract": "<p>We introduce an architecture to learn joint multilingual sentence\nrepresentations for 93 languages, belonging to more than 30 different families\nand written in 28 different scripts. Our system uses a single BiLSTM encoder\nwith a shared BPE vocabulary for all languages, which is coupled with an\nauxiliary decoder and trained on publicly available parallel corpora. This\nenables us to learn a classifier on top of the resulting embeddings using\nEnglish annotated data only, and transfer it to any of the 93 languages without\nany modification. Our experiments in cross-lingual natural language inference\n(XNLI dataset), cross-lingual document classification (MLDoc dataset) and\nparallel corpus mining (BUCC dataset) show the effectiveness of our approach.\nWe also introduce a new test set of aligned sentences in 112 languages, and\nshow that our sentence embeddings obtain strong results in multilingual\nsimilarity search even for low-resource languages. Our implementation, the\npre-trained encoder and the multilingual test set are available at\nhttps://github.com/facebookresearch/LASER</p>\n", "tags": ["ARXIV","Has Code","Supervised"] },
{"key": "arthur2010reverse", "year": "2010", "title":"Reverse Nearest Neighbors Search In High Dimensions Using Locality-sensitive Hashing", "abstract": "<p>We investigate the problem of finding reverse nearest neighbors efficiently.\nAlthough provably good solutions exist for this problem in low or fixed\ndimensions, to this date the methods proposed in high dimensions are mostly\nheuristic. We introduce a method that is both provably correct and efficient in\nall dimensions, based on a reduction of the problem to one instance of\n\\(\\e\\)-nearest neighbor search plus a controlled number of instances of {\\em\nexhaustive \\(r\\)-\\pleb}, a variant of {\\em Point Location among Equal Balls}\nwhere all the \\(r\\)-balls centered at the data points that contain the query\npoint are sought for, not just one. The former problem has been extensively\nstudied and elegantly solved in high dimensions using Locality-Sensitive\nHashing (LSH) techniques. By contrast, the latter problem has a complexity that\nis still not fully understood. We revisit the analysis of the LSH scheme for\nexhaustive \\(r\\)-\\pleb using a somewhat refined notion of locality-sensitive\nfamily of hash function, which brings out a meaningful output-sensitive term in\nthe complexity of the problem. Our analysis, combined with a non-isometric\nlifting of the data, enables us to answer exhaustive \\(r\\)-\\pleb queries (and\ndown the road reverse nearest neighbors queries) efficiently. Along the way, we\nobtain a simple algorithm for answering exact nearest neighbor queries, whose\ncomplexity is parametrized by some {\\em condition number} measuring the\ninherent difficulty of a given instance of the problem.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "ashutosh2024ai", "year": "2024", "title":"Ai-based Copyright Detection Of An Image In A Video Using Degree Of Similarity And Image Hashing", "abstract": "<p>The expanse of information available over the internet makes it difficult to\nidentify whether a specific work is a replica or a duplication of a protected\nwork, especially if we talk about visual representations. Strategies are\nplanned to identify the utilization of the copyrighted image in a report.\nStill, we want to resolve the issue of involving a copyrighted image in a video\nand a calculation that could recognize the degree of similarity of the\ncopyrighted picture utilized in the video, even for the pieces of the video\nthat are not featured a lot and in the end perform characterization errands on\nthose edges. Machine learning (ML) and artificial intelligence (AI) are vital\nto address this problem. Numerous associations have been creating different\ncalculations to screen the identification of copyrighted work. This work means\nconcentrating on those calculations, recognizing designs inside the\ninformation, and fabricating a more reasonable model for copyrighted image\nclassification and detection. We have used different algorithms like- Image\nProcessing, Convolutional Neural Networks (CNN), Image hashing, etc. Keywords-\nCopyright, Artificial Intelligence(AI), Copyrighted Image, Convolutional Neural\nNetwork(CNN), Image processing, Degree of similarity, Image Hashing.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "assadi2022tight", "year": "2022", "title":"Tight Bounds For Monotone Minimal Perfect Hashing", "abstract": "<p>The monotone minimal perfect hash function (MMPHF) problem is the following\nindexing problem. Given a set \\(S= \\{s_1,\\ldots,s_n\\}\\) of \\(n\\) distinct keys from\na universe \\(U\\) of size \\(u\\), create a data structure \\(DS\\) that answers the\nfollowing query:\n  [ RankOp(q) = \\text{rank of } q \\text{ in } S \\text{ for all } q\\in S\n  ~\\text{ and arbitrary answer otherwise.}\n  ]\n  Solutions to the MMPHF problem are in widespread use in both theory and\npractice.\n  The best upper bound known for the problem encodes \\(DS\\) in \\(O(nlogloglog\nu)\\) bits and performs queries in \\(O(log u)\\) time. It has been an open problem\nto either improve the space upper bound or to show that this somewhat odd\nlooking bound is tight.\n  In this paper, we show the latter: specifically that any data structure\n(deterministic or randomized) for monotone minimal perfect hashing of any\ncollection of \\(n\\) elements from a universe of size \\(u\\) requires \\(Ω(n \\cdot\nlogloglog{u})\\) expected bits to answer every query correctly.\n  We achieve our lower bound by defining a graph \\(\\mathbf{G}\\) where the nodes\nare the possible \\({u \\choose n}\\) inputs and where two nodes are adjacent if\nthey cannot share the same \\(DS\\). The size of \\(DS\\) is then lower bounded by the\nlog of the chromatic number of \\(\\mathbf{G}\\). Finally, we show that the\nfractional chromatic number (and hence the chromatic number) of \\(\\mathbf{G}\\) is\nlower bounded by \\(2^{Ω(n logloglog u)}\\).</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "atighehchi2015optimization", "year": "2015", "title":"Optimization Of Tree Modes For Parallel Hash Functions A Case Study", "abstract": "<p>This paper focuses on parallel hash functions based on tree modes of\noperation for an inner Variable-Input-Length function. This inner function can\nbe either a single-block-length (SBL) and prefix-free MD hash function, or a\nsponge-based hash function. We discuss the various forms of optimality that can\nbe obtained when designing parallel hash functions based on trees where all\nleaves have the same depth. The first result is a scheme which optimizes the\ntree topology in order to decrease the running time. Then, without affecting\nthe optimal running time we show that we can slightly change the corresponding\ntree topology so as to minimize the number of required processors as well.\nConsequently, the resulting scheme decreases in the first place the running\ntime and in the second place the number of required processors.</p>\n", "tags": ["ARXIV","Case Study","Independent"] },
{"key": "atighehchi2016note", "year": "2016", "title":"Note On Optimal Trees For Parallel Hash Functions", "abstract": "<p>A recent work shows how we can optimize a tree based mode of operation for a\nrate 1 hash function. In particular, an algorithm and a theorem are presented\nfor selecting a good tree topology in order to optimize both the running time\nand the number of processors at each step of the computation. Because this\npaper deals only with trees having their leaves at the same depth, the number\nof saved computing resources is perfectly optimal only for this category of\ntrees. In this note, we address the more general case and describe a simple\nalgorithm which, starting from such a tree topology, reworks it to further\nreduce the number of processors and the total amount of work done to hash a\nmessage.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "aumüller2012explicit", "year": "2012", "title":"Explicit And Efficient Hash Families Suffice For Cuckoo Hashing With A Stash", "abstract": "<p>It is shown that for cuckoo hashing with a stash as proposed by Kirsch,\nMitzenmacher, and Wieder (2008) families of very simple hash functions can be\nused, maintaining the favorable performance guarantees: with stash size \\(s\\) the\nprobability of a rehash is \\(O(1/n^{s+1})\\), and the evaluation time is \\(O(s)\\).\nInstead of the full randomness needed for the analysis of Kirsch et al. and of\nKutzelnigg (2010) (resp. \\(\\Theta(log n)\\)-wise independence for standard cuckoo\nhashing) the new approach even works with 2-wise independent hash families as\nbuilding blocks. Both construction and analysis build upon the work of\nDietzfelbinger and Woelfel (2003). The analysis, which can also be applied to\nthe fully random case, utilizes a graph counting argument and is much simpler\nthan previous proofs. As a byproduct, an algorithm for simulating uniform\nhashing is obtained. While it requires about twice as much space as the most\nspace efficient solutions, it is attractive because of its simple and direct\nstructure.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "aumüller2016simple", "year": "2016", "title":"A Simple Hash Class With Strong Randomness Properties In Graphs And Hypergraphs", "abstract": "<p>We study randomness properties of graphs and hypergraphs generated by simple\nhash functions. Several hashing applications can be analyzed by studying the\nstructure of \\(d\\)-uniform random (\\(d\\)-partite) hypergraphs obtained from a set\n\\(S\\) of \\(n\\) keys and \\(d\\) randomly chosen hash functions \\(h_1,\\dots,h_d\\) by\nassociating each key \\(x\\in S\\) with a hyperedge \\(\\{h_1(x),\\dots, h_d(x)\\}\\).\nOften it is assumed that \\(h_1,\\dots,h_d\\) exhibit a high degree of independence.\nWe present a simple construction of a hash class whose hash functions have\nsmall constant evaluation time and can be stored in sublinear space. We devise\ngeneral techniques to analyze the randomness properties of the graphs and\nhypergraphs generated by these hash functions, and we show that they can\nreplace other, less efficient constructions in cuckoo hashing (with and without\nstash), the simulation of a uniform hash function, the construction of a\nperfect hash function, generalized cuckoo hashing and different load balancing\nscenarios.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "aumüller2017distance", "year": "2017", "title":"Distance-sensitive Hashing", "abstract": "<p>Locality-sensitive hashing (LSH) is an important tool for managing\nhigh-dimensional noisy or uncertain data, for example in connection with data\ncleaning (similarity join) and noise-robust search (similarity search).\nHowever, for a number of problems the LSH framework is not known to yield good\nsolutions, and instead ad hoc solutions have been designed for particular\nsimilarity and distance measures. For example, this is true for\noutput-sensitive similarity search/join, and for indexes supporting annulus\nqueries that aim to report a point close to a certain given distance from the\nquery point.\n  In this paper we initiate the study of distance-sensitive hashing (DSH), a\ngeneralization of LSH that seeks a family of hash functions such that the\nprobability of two points having the same hash value is a given function of the\ndistance between them. More precisely, given a distance space \\((X,\n\\text{dist})\\) and a “collision probability function” (CPF) \\(f\\colon\n\\mathbb{R}\\rightarrow [0,1]\\) we seek a distribution over pairs of functions\n\\((h,g)\\) such that for every pair of points \\(x, y \\in X\\) the collision\nprobability is \\(\\Pr[h(x)=g(y)] = f(\\text{dist}(x,y))\\). Locality-sensitive\nhashing is the study of how fast a CPF can decrease as the distance grows. For\nmany spaces, \\(f\\) can be made exponentially decreasing even if we restrict\nattention to the symmetric case where \\(g=h\\). We show that the asymmetry\nachieved by having a pair of functions makes it possible to achieve CPFs that\nare, for example, increasing or unimodal, and show how this leads to principled\nsolutions to problems not addressed by the LSH framework. This includes a novel\napplication to privacy-preserving distance estimation. We believe that the DSH\nframework will find further applications in high-dimensional data management.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "aumüller2018ann", "year": "2018", "title":"Ann-benchmarks A Benchmarking Tool For Approximate Nearest Neighbor Algorithms", "abstract": "<p>This paper describes ANN-Benchmarks, a tool for evaluating the performance of\nin-memory approximate nearest neighbor algorithms. It provides a standard\ninterface for measuring the performance and quality achieved by nearest\nneighbor algorithms on different standard data sets. It supports several\ndifferent ways of integrating \\(k\\)-NN algorithms, and its configuration system\nautomatically tests a range of parameter settings for each algorithm.\nAlgorithms are compared with respect to many different (approximate) quality\nmeasures, and adding more is easy and fast; the included plotting front-ends\ncan visualise these as images, \\(\\LaTeX\\) plots, and websites with interactive\nplots. ANN-Benchmarks aims to provide a constantly updated overview of the\ncurrent state of the art of \\(k\\)-NN algorithms. In the short term, this overview\nallows users to choose the correct \\(k\\)-NN algorithm and parameters for their\nsimilarity search task; in the longer term, algorithm designers will be able to\nuse this overview to test and refine automatic parameter tuning. The paper\ngives an overview of the system, evaluates the results of the benchmark, and\npoints out directions for future work. Interestingly, very different approaches\nto \\(k\\)-NN search yield comparable quality-performance trade-offs. The system is\navailable at http://ann-benchmarks.com .</p>\n", "tags": ["ARXIV"] },
{"key": "aumüller2019fair", "year": "2019", "title":"Fair Near Neighbor Search Independent Range Sampling In High Dimensions", "abstract": "<p>Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. There are several variants of the similarity\nsearch problem, and one of the most relevant is the \\(r\\)-near neighbor (\\(r\\)-NN)\nproblem: given a radius \\(r&gt;0\\) and a set of points \\(S\\), construct a data\nstructure that, for any given query point \\(q\\), returns a point \\(p\\) within\ndistance at most \\(r\\) from \\(q\\). In this paper, we study the \\(r\\)-NN problem in\nthe light of fairness. We consider fairness in the sense of equal opportunity:\nall points that are within distance \\(r\\) from the query should have the same\nprobability to be returned. In the low-dimensional case, this problem was first\nstudied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the\ntheoretically strongest approach to similarity search in high dimensions, does\nnot provide such a fairness guarantee. To address this, we propose efficient\ndata structures for \\(r\\)-NN where all points in \\(S\\) that are near \\(q\\) have the\nsame probability to be selected and returned by the query. Specifically, we\nfirst propose a black-box approach that, given any LSH scheme, constructs a\ndata structure for uniformly sampling points in the neighborhood of a query.\nThen, we develop a data structure for fair similarity search under inner\nproduct that requires nearly-linear space and exploits locality sensitive\nfilters. The paper concludes with an experimental evaluation that highlights\n(un)fairness in a recommendation setting on real-world datasets and discusses\nthe inherent unfairness introduced by solving other variants of the problem.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "aumüller2019puffinn", "year": "2019", "title":"PUFFINN Parameterless And Universally Fast Finding Of Nearest Neighbors", "abstract": "<p>We present PUFFINN, a parameterless LSH-based index for solving the\n\\(k\\)-nearest neighbor problem with probabilistic guarantees. By parameterless we\nmean that the user is only required to specify the amount of memory the index\nis supposed to use and the result quality that should be achieved. The index\ncombines several heuristic ideas known in the literature. By small adaptions to\nthe query algorithm, we make heuristics rigorous. We perform experiments on\nreal-world and synthetic inputs to evaluate implementation choices and show\nthat the implementation satisfies the quality guarantees while being\ncompetitive with other state-of-the-art approaches to nearest neighbor search.\n  We describe a novel synthetic data set that is difficult to solve for almost\nall existing nearest neighbor search approaches, and for which PUFFINN\nsignificantly outperform previous methods.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "aumüller2021sampling", "year": "2021", "title":"Sampling A Near Neighbor In High Dimensions -- Who Is The Fairest Of Them All", "abstract": "<p>Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. Given a set of points \\(S\\) and a radius parameter\n\\(r&gt;0\\), the \\(r\\)-near neighbor (\\(r\\)-NN) problem asks for a data structure that,\ngiven any query point \\(q\\), returns a point \\(p\\) within distance at most \\(r\\) from\n\\(q\\). In this paper, we study the \\(r\\)-NN problem in the light of individual\nfairness and providing equal opportunities: all points that are within distance\n\\(r\\) from the query should have the same probability to be returned. In the\nlow-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS\n2014). Locality sensitive hashing (LSH), the theoretically strongest approach\nto similarity search in high dimensions, does not provide such a fairness\nguarantee. In this work, we show that LSH based algorithms can be made fair,\nwithout a significant loss in efficiency. We propose several efficient data\nstructures for the exact and approximate variants of the fair NN problem. Our\napproach works more generally for sampling uniformly from a sub-collection of\nsets of a given collection and can be used in a few other applications. We also\ndevelop a data structure for fair similarity search under inner product that\nrequires nearly-linear space and exploits locality sensitive filters. The paper\nconcludes with an experimental evaluation that highlights the inherent\nunfairness of NN data structures and shows the performance of our algorithms on\nreal-world datasets.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "auvolat2015clustering", "year": "2015", "title":"Clustering Is Efficient For Approximate Maximum Inner Product Search", "abstract": "<p>Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise.</p>\n", "tags": ["ARXIV","LSH","Unsupervised"] },
{"key": "awad2021better", "year": "2021", "title":"Better GPU Hash Tables", "abstract": "<p>We revisit the problem of building static hash tables on the GPU and design\nand build three bucketed hash tables that use different probing schemes. Our\nimplementations are lock-free and offer efficient memory access patterns; thus,\nonly the probing scheme is the factor affecting the performance of the hash\ntable’s different operations. Our results show that a bucketed cuckoo hash\ntable that uses three hash functions (BCHT) outperforms alternative methods\nthat use power-of-two choices, iceberg hashing, and a cuckoo hash table that\nuses a bucket size one. At high load factors as high as 0.99, BCHT enjoys an\naverage probe count of 1.43 during insertion. Using three hash functions only,\npositive and negative queries require at most 1.39 and 2.8 average probes per\nkey, respectively.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "azarafrooz2018fuzzy", "year": "2018", "title":"Fuzzy Hashing As Perturbation-consistent Adversarial Kernel Embedding", "abstract": "<p>Measuring the similarity of two files is an important task in malware\nanalysis, with fuzzy hash functions being a popular approach. Traditional fuzzy\nhash functions are data agnostic: they do not learn from a particular dataset\nhow to determine similarity; their behavior is fixed across all datasets. In\nthis paper, we demonstrate that fuzzy hash functions can be learned in a novel\nminimax training framework and that these learned fuzzy hash functions\noutperform traditional fuzzy hash functions at the file similarity task for\nPortable Executable files. In our approach, hash digests can be extracted from\nthe kernel embeddings of two kernel networks, trained in a minimax framework,\nwhere the roles of players during training (i.e adversary versus generator)\nalternate along with the input data. We refer to this new minimax architecture\nas perturbation-consistent. The similarity score for a pair of files is the\nutility of the minimax game in equilibrium. Our experiments show that learned\nfuzzy hash functions generalize well, capable of determining that two files are\nsimilar even when one of those files was generated using insertion and deletion\noperations.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "b2018fully", "year": "2018", "title":"Fully Understanding The Hashing Trick", "abstract": "<p>Feature hashing, also known as {\\em the hashing trick}, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix \\(A : \\mathbb{R}^n \\to \\mathbb{R}^m\\) (where \\(m \\ll n\\)) in order to reduce the dimension of the data from \\(n\\) to \\(m\\) while approximately preserving the Euclidean norm. Every column of \\(A\\) contains exactly one non-zero entry, equals to either \\(-1\\) or \\(1\\).</p>\n\n<p>Weinberger et al. showed tail bounds on \\(|Ax|<em>2^2\\). Specifically they showed that for every \\(\\epsilon, \\delta\\), if \\(|x|</em>{\\infty} / |x|<em>2\\) is sufficiently small, and \\(m\\) is sufficiently large, then \n\\begin{equation*}\\Pr[ \\; | \\;|Ax|_2^2 - |x|_2^2\\; | &lt; \\epsilon |x|_2^2 \\;] \\ge 1 - \\delta \\;.\\end{equation*}\nThese bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017), however, the true nature of the performance of this key technique, and specifically the correct tradeoff between the pivotal parameters \\(|x|</em>{\\infty} / |x|_2, m, \\epsilon, \\delta\\) remained an open question.</p>\n\n<p>We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters, thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data, which shows that the constants “hiding” in the asymptotic notation are, in fact, very close to \\(1\\), thus further illustrating the tightness of the presented bounds in practice.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "babenko2014improving", "year": "2014", "title":"Improving Bilayer Product Quantization For Billion-scale Approximate Nearest Neighbors In High Dimensions", "abstract": "<p>The top-performing systems for billion-scale high-dimensional approximate\nnearest neighbor (ANN) search are all based on two-layer architectures that\ninclude an indexing structure and a compressed datapoints layer. An indexing\nstructure is crucial as it allows to avoid exhaustive search, while the lossy\ndata compression is needed to fit the dataset into RAM. Several of the most\nsuccessful systems use product quantization (PQ) for both the indexing and the\ndataset compression layers. These systems are however limited in the way they\nexploit the interaction of product quantization processes that happen at\ndifferent stages of these systems.\n  Here we introduce and evaluate two approximate nearest neighbor search\nsystems that both exploit the synergy of product quantization processes in a\nmore efficient way. The first system, called Fast Bilayer Product Quantization\n(FBPQ), speeds up the runtime of the baseline system (Multi-D-ADC) by several\ntimes, while achieving the same accuracy. The second system, Hierarchical\nBilayer Product Quantization (HBPQ) provides a significantly better recall for\nthe same runtime at a cost of small memory footprint increase. For the BIGANN\ndataset of billion SIFT descriptors, the 10% increase in Recall@1 and the 17%\nincrease in Recall@10 is observed.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "babka2014expected", "year": "2014", "title":"Expected Number Of Uniformly Distributed Balls In A Most Loaded Bin Using Placement With Simple Linear Functions", "abstract": "<p>We estimate the size of a most loaded bin in the setting when the balls are\nplaced into the bins using a random linear function in a finite field. The\nballs are chosen from a transformed interval. We show that in this setting the\nexpected load of the most loaded bins is constant.\n  This is an interesting fact because using fully random hash functions with\nthe same class of input sets leads to an expectation of \\(\\Theta\\left(\\frac{log\nm}{log log m}\\right)\\) balls in most loaded bins where \\(m\\) is the number of\nballs and bins.\n  Although the family of the functions is quite common the size of largest bins\nwas not known even in this simple case.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "bachmann2019superm", "year": "2019", "title":"The Superm-tree Indexing Metric Spaces With Sized Objects", "abstract": "<p>A common approach to implementing similarity search applications is the usage\nof distance functions, where small distances indicate high similarity. In the\ncase of metric distance functions, metric index structures can be used to\naccelerate nearest neighbor queries. On the other hand, many applications ask\nfor approximate subsequences or subsets, e.g. searching for a similar partial\nsequence of a gene, for a similar scene in a movie, or for a similar object in\na picture which is represented by a set of multidimensional features. Metric\nindex structures such as the M-Tree cannot be utilized for these tasks because\nof the symmetry of the metric distance functions. In this work, we propose the\nSuperM-Tree as an extension of the M-Tree where approximate subsequence and\nsubset queries become nearest neighbor queries. In order to do this, we\nintroduce metric subset spaces as a generalized concept of metric spaces.\nVarious metric distance functions can be extended to metric subset distance\nfunctions, e.g. the Euclidean distance (on windows), the Hausdorff distance (on\nsubsets), the Edit distance and the Dog-Keeper distance (on subsequences). We\nshow that these examples subsume the applications mentioned above.</p>\n", "tags": ["ARXIV"] },
{"key": "bachoc2010bounds", "year": "2010", "title":"Bounds For Binary Codes Relative To Pseudo-distances Of K Points", "abstract": "<p>We apply Schrijver’s semidefinite programming method to obtain improved upper\nbounds on generalized distances and list decoding radii of binary codes.</p>\n", "tags": ["ARXIV"] },
{"key": "backurs2019space", "year": "2019", "title":"Space And Time Efficient Kernel Density Estimation In High Dimensions", "abstract": "<p>Recently, Charikar and Siminelakis (2017) presented a framework for kernel density estimation in provably sublinear query time, for kernels that possess a certain hashing-based property. However, their data structure requires a significantly increased super-linear storage space, as well as super-linear preprocessing time. These limitations inhibit the practical applicability of their approach on large datasets.\nIn this work, we present an improvement to their framework that retains the same query time, while requiring only linear space and linear preprocessing time. We instantiate our framework with the Laplacian and Exponential kernels, two popular kernels which possess the aforementioned property. Our experiments on various datasets verify that our approach attains accuracy and query time similar to Charikar and Siminelakis (2017), with significantly improved space and preprocessing time.</p>\n", "tags": ["NEURIPS"] },
{"key": "bagaria2015new", "year": "2015", "title":"New Hashing Algorithm For Use In TCP Reassembly Module Of IPS", "abstract": "<p>Since last decade, IDS/ IPS has gained popularity in protecting large\nnetworks. They can employ signature based techniques and/or flow-based\ntechniques to prevent intrusion from outside/ inside the network they are\ntrying to protect. Signature based IDS/ IPS can be stateless or stateful.\nStateful IDS can store the state of the protocol and use it for better\ndetection of malware. In the case of TCP/IP networks, an attacker can also\nlaunch an attack such that the malicious code is distributed over many packets.\nThese packets pass through the traditional IDS/ IPS and reassemble inside the\nnetwork. Once re-assembled inside the network by the TCP/IP layer, the\nmalicious code launches an attack.\n  The TCP state and a copy of last few packets for each active connection has\nto be maintained in IDS/IPS. In TCP re-assembly, packets are re-assembled at\nIDS/IPS and searched for signature matches. A connection table has to be\nmaintained for active connections and their list of last few (atmost 11)\npackets that have already arrived. We need data structures for searching the\nconnection that the latest incoming packet belongs to. Popular hashing\nalgorithms like CRC, XOR, summing tuple, taking modulus are inefficient as hash\nkeys are not evenly distributed in hash-key space. Thus we show how an\nalgorithm based on cryptography concepts can be used for efficient hashing in\nnetwork connection management. We also show how to use full four tuple for\ncalculating hash key instead of simply summing the tuple and taking the modulus\nof the sum.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "bahou2018xnorbin", "year": "2018", "title":"XNORBIN A 95 Top/s/w Hardware Accelerator For Binary Convolutional Neural Networks", "abstract": "<p>Deploying state-of-the-art CNNs requires power-hungry processors and off-chip\nmemory. This precludes the implementation of CNNs in low-power embedded\nsystems. Recent research shows CNNs sustain extreme quantization, binarizing\ntheir weights and intermediate feature maps, thereby saving 8-32\\x memory and\ncollapsing energy-intensive sum-of-products into XNOR-and-popcount operations.\n  We present XNORBIN, an accelerator for binary CNNs with computation tightly\ncoupled to memory for aggressive data reuse. Implemented in UMC 65nm technology\nXNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of\n2.0 TOp/s/MGE at 0.8 V.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "bai2018convolutional", "year": "2018", "title":"Convolutional Set Matching For Graph Similarity", "abstract": "<p>We introduce GSimCNN (Graph Similarity Computation via Convolutional Neural\nNetworks) for predicting the similarity score between two graphs. As the core\noperation of graph similarity search, pairwise graph similarity computation is\na challenging problem due to the NP-hard nature of computing many graph\ndistance/similarity metrics. We demonstrate our model using the Graph Edit\nDistance (GED) as the example metric. Experiments on three real graph datasets\ndemonstrate that our model achieves the state-of-the-art performance on graph\nsimilarity search.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "bai2018learning", "year": "2018", "title":"Learning-based Efficient Graph Similarity Computation Via Multi-scale Convolutional Set Matching", "abstract": "<p>Graph similarity computation is one of the core operations in many\ngraph-based applications, such as graph similarity search, graph database\nanalysis, graph clustering, etc. Since computing the exact distance/similarity\nbetween two graphs is typically NP-hard, a series of approximate methods have\nbeen proposed with a trade-off between accuracy and speed. Recently, several\ndata-driven approaches based on neural networks have been proposed, most of\nwhich model the graph-graph similarity as the inner product of their\ngraph-level representations, with different techniques proposed for generating\none embedding per graph. However, using one fixed-dimensional embedding per\ngraph may fail to fully capture graphs in varying sizes and link structures, a\nlimitation that is especially problematic for the task of graph similarity\ncomputation, where the goal is to find the fine-grained difference between two\ngraphs. In this paper, we address the problem of graph similarity computation\nfrom another perspective, by directly matching two sets of node embeddings\nwithout the need to use fixed-dimensional vectors to represent whole graphs for\ntheir similarity computation. The model, GraphSim, achieves the\nstate-of-the-art performance on four real-world graph datasets under six out of\neight settings (here we count a specific dataset and metric combination as one\nsetting), compared to existing popular methods for approximate Graph Edit\nDistance (GED) and Maximum Common Subgraph (MCS) computation.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "bai2020targeted", "year": "2020", "title":"Targeted Attack For Deep Hashing Based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale\nimage and video retrieval. However, there is little investigation on its\nsecurity. In this paper, we propose a novel method, dubbed deep hashing\ntargeted attack (DHTA), to study the targeted attack on such retrieval.\nSpecifically, we first formulate the targeted attack as a point-to-set\noptimization, which minimizes the average distance between the hash code of an\nadversarial example and those of a set of objects with the target label. Then\nwe design a novel component-voting scheme to obtain an anchor code as the\nrepresentative of the set of hash codes of objects with the target label, whose\noptimality guarantee is also theoretically derived. To balance the performance\nand perceptibility, we propose to minimize the Hamming distance between the\nhash code of the adversarial example and the anchor code under the\n\\(\\ell^\\infty\\) restriction on the perturbation. Extensive experiments verify\nthat DHTA is effective in attacking both deep hashing based image retrieval and\nvideo retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Video Retrieval"] },
{"key": "bai2024targeted", "year": "2024", "title":"Targeted Attack For Deep Hashing Based Retrieval", "abstract": "<p>The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the ℓ∞ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Video Retrieval"] },
{"key": "bakhtiary2015speeding", "year": "2015", "title":"Speeding Up Neural Networks For Large Scale Classification Using WTA Hashing", "abstract": "<p>In this paper we propose to use the Winner Takes All hashing technique to\nspeed up forward propagation and backward propagation in fully connected layers\nin convolutional neural networks. The proposed technique reduces significantly\nthe computational complexity, which in turn, allows us to train layers with a\nlarge number of kernels with out the associated time penalty.\n  As a consequence we are able to train convolutional neural network on a very\nlarge number of output classes with only a small increase in the computational\ncost. To show the effectiveness of the technique we train a new output layer on\na pretrained network using both the regular multiplicative approach and our\nproposed hashing methodology. Our results showed no drop in performance and\ndemonstrate, with our implementation, a 7 fold speed up during the training.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "ballard2015diamond", "year": "2015", "title":"Diamond Sampling For Approximate Maximum All-pairs Dot-product (MAD) Search", "abstract": "<p>Given two sets of vectors, \\(A = \\{{a_1}, \\dots, {a_m}\\}\\) and\n\\(B=\\{{b_1},\\dots,{b_n}\\}\\), our problem is to find the top-\\(t\\) dot products,\ni.e., the largest \\(|{a_i}\\cdot{b_j}|\\) among all possible pairs. This is a\nfundamental mathematical problem that appears in numerous data applications\ninvolving similarity search, link prediction, and collaborative filtering. We\npropose a sampling-based approach that avoids direct computation of all \\(mn\\)\ndot products. We select diamonds (i.e., four-cycles) from the weighted\ntripartite representation of \\(A\\) and \\(B\\). The probability of selecting a\ndiamond corresponding to pair \\((i,j)\\) is proportional to \\(({a_i}\\cdot{b_j})^2\\),\namplifying the focus on the largest-magnitude entries. Experimental results\nindicate that diamond sampling is orders of magnitude faster than direct\ncomputation and requires far fewer samples than any competing approach. We also\napply diamond sampling to the special case of maximum inner product search, and\nget significantly better results than the state-of-the-art hashing methods.</p>\n", "tags": [] },
{"key": "bansal2016extraction", "year": "2016", "title":"Extraction Of Layout Entities And Sub-layout Query-based Retrieval Of Document Images", "abstract": "<p>Layouts and sub-layouts constitute an important clue while searching a\ndocument on the basis of its structure, or when textual content is\nunknown/irrelevant. A sub-layout specifies the arrangement of document entities\nwithin a smaller portion of the document. We propose an efficient graph-based\nmatching algorithm, integrated with hash-based indexing, to prune a possibly\nlarge search space. A user can specify a combination of sub-layouts of interest\nusing sketch-based queries. The system supports partial matching for\nunspecified layout entities. We handle cases of segmentation pre-processing\nerrors (for text/non-text blocks) with a symmetry maximization-based strategy,\nand accounting for multiple domain-specific plausible segmentation hypotheses.\nWe show promising results of our system on a database of unstructured entities,\ncontaining 4776 newspaper images.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "baranchuk2018revisiting", "year": "2018", "title":"Revisiting The Inverted Indices For Billion-scale Approximate Nearest Neighbors", "abstract": "<p>This work addresses the problem of billion-scale nearest neighbor search. The\nstate-of-the-art retrieval systems for billion-scale databases are currently\nbased on the inverted multi-index, the recently proposed generalization of the\ninverted index structure. The multi-index provides a very fine-grained\npartition of the feature space that allows extracting concise and accurate\nshort-lists of candidates for the search queries. In this paper, we argue that\nthe potential of the simple inverted index was not fully exploited in previous\nworks and advocate its usage both for the highly-entangled deep descriptors and\nrelatively disentangled SIFT descriptors. We introduce a new retrieval system\nthat is based on the inverted index and outperforms the multi-index by a large\nmargin for the same memory consumption and construction complexity. For\nexample, our system achieves the state-of-the-art recall rates several times\nfaster on the dataset of one billion deep descriptors compared to the efficient\nimplementation of the inverted multi-index from the FAISS library.</p>\n", "tags": ["ARXIV"] },
{"key": "baranchuk2019learning", "year": "2019", "title":"Learning To Route In Similarity Graphs", "abstract": "<p>Recently similarity graphs became the leading paradigm for efficient nearest\nneighbor search, outperforming traditional tree-based and LSH-based methods.\nSimilarity graphs perform the search via greedy routing: a query traverses the\ngraph and in each vertex moves to the adjacent vertex that is the closest to\nthis query. In practice, similarity graphs are often susceptible to local\nminima, when queries do not reach its nearest neighbors, getting stuck in\nsuboptimal vertices. In this paper we propose to learn the routing function\nthat overcomes local minima via incorporating information about the graph\nglobal structure. In particular, we augment the vertices of a given graph with\nadditional representations that are learned to provide the optimal routing from\nthe start vertex to the query nearest neighbor. By thorough experiments, we\ndemonstrate that the proposed learnable routing successfully diminishes the\nlocal minima problem and significantly improves the overall search performance.</p>\n", "tags": ["ARXIV","Graph","Independent","LSH"] },
{"key": "baranchuk2023dedrift", "year": "2023", "title":"Dedrift Robust Similarity Search Under Content Drift", "abstract": "<p>The statistical distribution of content uploaded and searched on media\nsharing sites changes over time due to seasonal, sociological and technical\nfactors. We investigate the impact of this “content drift” for large-scale\nsimilarity search tools, based on nearest neighbor search in embedding space.\nUnless a costly index reconstruction is performed frequently, content drift\ndegrades the search accuracy and efficiency. The degradation is especially\nsevere since, in general, both the query and database distributions change.\n  We introduce and analyze real-world image and video datasets for which\ntemporal information is available over a long time period. Based on the\nlearnings, we devise DeDrift, a method that updates embedding quantizers to\ncontinuously adapt large-scale indexing structures on-the-fly. DeDrift almost\neliminates the accuracy degradation due to the query and database content drift\nwhile being up to 100x faster than a full index reconstruction.</p>\n", "tags": ["ARXIV"] },
{"key": "barg2022size", "year": "2022", "title":"On The Size Of Maximal Binary Codes With 2 3 And 4 Distances", "abstract": "<p>We address the maximum size of binary codes and binary constant weight codes\nwith few distances. Previous works established a number of bounds for these\nquantities as well as the exact values for a range of small code lengths. As\nour main results, we determine the exact size of maximal binary codes with two\ndistances for all lengths \\(n\\ge 6\\) as well as the exact size of maximal binary\nconstant weight codes with 2,3, and 4 distances for several values of the\nweight and for all but small lengths.</p>\n", "tags": ["ARXIV"] },
{"key": "bartal2015approximate", "year": "2015", "title":"Approximate Nearest Neighbor Search For ell_p-spaces (2 < P < infty) Via Embeddings", "abstract": "<p>While the problem of approximate nearest neighbor search has been\nwell-studied for Euclidean space and \\(\\ell_1\\), few non-trivial algorithms are\nknown for \\(\\ell_p\\) when (\\(2 &lt; p &lt; \\infty\\)). In this paper, we revisit this\nfundamental problem and present approximate nearest-neighbor search algorithms\nwhich give the first non-trivial approximation factor guarantees in this\nsetting.</p>\n", "tags": ["ARXIV"] },
{"key": "bawa2024lsh", "year": "2024", "title":"LSH Forest Self-tuning Indexes For Similarity Search", "abstract": "<p>We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search\nengines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images;\npeer-to-peer systems desire distributed similarity indexes with low\ncommunication cost. We propose an indexing scheme called LSH\nForest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH),\nbut improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH’s performance guarantees for skewed data distributions while retaining the same storage\nand query overhead. We show how to construct this index in main\nmemory, on disk, in parallel systems, and in peer-to-peer systems.\nWe evaluate the design with experiments on multiple text corpora\nand demonstrate both the self-tuning nature and the superior performance of LSH Forest.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "beame2016massively", "year": "2016", "title":"Massively-parallel Similarity Join Edge-isoperimetry And Distance Correlations On The Hypercube", "abstract": "<p>We study distributed protocols for finding all pairs of similar vectors in a\nlarge dataset. Our results pertain to a variety of discrete metrics, and we\ngive concrete instantiations for Hamming distance. In particular, we give\nimproved upper bounds on the overhead required for similarity defined by\nHamming distance \\(r&gt;1\\) and prove a lower bound showing qualitative optimality\nof the overhead required for similarity over any Hamming distance \\(r\\). Our main\nconceptual contribution is a connection between similarity search algorithms\nand certain graph-theoretic quantities. For our upper bounds, we exhibit a\ngeneral method for designing one-round protocols using edge-isoperimetric\nshapes in similarity graphs. For our lower bounds, we define a new\ncombinatorial optimization problem, which can be stated in purely\ngraph-theoretic terms yet also captures the core of the analysis in previous\ntheoretical work on distributed similarity joins. As one of our main technical\nresults, we prove new bounds on distance correlations in subsets of the Hamming\ncube.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "beierle2018do", "year": "2018", "title":"Do You Like What I Like Similarity Estimation In Proximity-based Mobile Social Networks", "abstract": "<p>While existing social networking services tend to connect people who know\neach other, people show a desire to also connect to yet unknown people in\nphysical proximity. Existing research shows that people tend to connect to\nsimilar people. Utilizing technology in order to stimulate human interaction\nbetween strangers, we consider the scenario of two strangers meeting. On the\nexample of similarity in musical taste, we develop a solution for the problem\nof similarity estimation in proximity-based mobile social networks. We show\nthat a single exchange of a probabilistic data structure between two devices\ncan closely estimate the similarity of two users - without the need to contact\na third-party server.We introduce metrics for fast and space-efficient\napproximation of the Dice coefficient of two multisets - based on the\ncomparison of two Counting Bloom Filters or two Count-Min Sketches. Our\nanalysis shows that utilizing a single hash function minimizes the error when\ncomparing these probabilistic data structures. The size that should be chosen\nfor the data structure depends on the expected average number of unique input\nelements. Using real user data, we show that a Counting Bloom Filter with a\nsingle hash function and a length of 128 is sufficient to accurately estimate\nthe similarity between two multisets representing the musical tastes of two\nusers. Our approach is generalizable for any other similarity estimation of\nfrequencies represented as multisets.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "benbassat2020locality", "year": "2020", "title":"Locality-sensitive Hashing For Efficient Web Application Security Testing", "abstract": "<p>Web application security has become a major concern in recent years, as more\nand more content and services are available online. A useful method for\nidentifying security vulnerabilities is black-box testing, which relies on an\nautomated crawling of web applications. However, crawling Rich Internet\nApplications (RIAs) is a very challenging task. One of the key obstacles\ncrawlers face is the state similarity problem: how to determine if two\nclient-side states are equivalent. As current methods do not completely solve\nthis problem, a successful scan of many real-world RIAs is still not possible.\nWe present a novel approach to detect redundant content for security testing\npurposes. The algorithm applies locality-sensitive hashing using MinHash\nsketches in order to analyze the Document Object Model (DOM) structure of web\npages, and to efficiently estimate similarity between them. Our experimental\nresults show that this approach allows a successful scan of RIAs that cannot be\ncrawled otherwise.</p>\n", "tags": ["COLT","Independent"] },
{"key": "bender2021iceberg", "year": "2021", "title":"Iceberg Hashing Optimizing Many Hash-table Criteria At Once", "abstract": "<p>Despite being one of the oldest data structures in computer science, hash\ntables continue to be the focus of a great deal of both theoretical and\nempirical research. A central reason for this is that many of the fundamental\nproperties that one desires from a hash table are difficult to achieve\nsimultaneously; thus many variants offering different trade-offs have been\nproposed.\n  This paper introduces Iceberg hashing, a hash table that simultaneously\noffers the strongest known guarantees on a large number of core properties.\nIceberg hashing supports constant-time operations while improving on the state\nof the art for space efficiency, cache efficiency, and low failure probability.\nIceberg hashing is also the first hash table to support a load factor of up to\n\\(1 - o(1)\\) while being stable, meaning that the position where an element is\nstored only ever changes when resizes occur. In fact, in the setting where keys\nare \\(\\Theta(log n)\\) bits, the space guarantees that Iceberg hashing offers,\nnamely that it uses at most \\(log \\binom{|U|}{n} + O(n log log n)\\) bits to\nstore \\(n\\) items from a universe \\(U\\), matches a lower bound by Demaine et al.\nthat applies to any stable hash table.\n  Iceberg hashing introduces new general-purpose techniques for some of the\nmost basic aspects of hash-table design. Notably, our indirection-free\ntechnique for dynamic resizing, which we call waterfall addressing, and our\ntechniques for achieving stability and very-high probability guarantees, can be\napplied to any hash table that makes use of the front-yard/backyard paradigm\nfor hash table design.</p>\n", "tags": ["ARXIV"] },
{"key": "bender2021linear", "year": "2021", "title":"Linear Probing Revisited Tombstones Mark The Death Of Primary Clustering", "abstract": "<p>First introduced in 1954, linear probing is one of the oldest data structures\nin computer science, and due to its unrivaled data locality, it continues to be\none of the fastest hash tables in practice. It is widely believed and taught,\nhowever, that linear probing should never be used at high load factors; this is\nbecause primary-clustering effects cause insertions at load factor \\(1 - 1 /x\\)\nto take expected time \\(\\Theta(x^2)\\) (rather than the ideal \\(\\Theta(x)\\)). The\ndangers of primary clustering, first discovered by Knuth in 1963, have been\ntaught to generations of computer scientists, and have influenced the design of\nsome of many widely used hash tables.\n  We show that primary clustering is not a foregone conclusion. We demonstrate\nthat small design decisions in how deletions are implemented have dramatic\neffects on the asymptotic performance of insertions, so that, even if a hash\ntable operates continuously at a load factor \\(1 - \\Theta(1/x)\\), the expected\namortized cost per operation is \\(\\tilde{O}(x)\\). This is because tombstones\ncreated by deletions actually cause an anti-clustering effect that combats\nprimary clustering.\n  We also present a new variant of linear probing (which we call graveyard\nhashing) that completely eliminates primary clustering on <em>any</em> sequence\nof operations: if, when an operation is performed, the current load factor is\n\\(1 - 1/x\\) for some \\(x\\), then the expected cost of the operation is \\(O(x)\\). One\ncorollary is that, in the external-memory model with a data blocks of size \\(B\\),\ngraveyard hashing offers the following remarkable guarantee: at any load factor\n\\(1 - 1/x\\) satisfying \\(x = o(B)\\), graveyard hashing achieves \\(1 + o(1)\\) expected\nblock transfers per operation. Past external-memory hash tables have only been\nable to offer a \\(1 + o(1)\\) guarantee when the block size \\(B\\) is at least\n\\(Ω(x^2)\\).</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "bender2021optimal", "year": "2021", "title":"On The Optimal Time/space Tradeoff For Hash Tables", "abstract": "<p>For nearly six decades, the central open question in the study of hash tables\nhas been to determine the optimal achievable tradeoff curve between time and\nspace. State-of-the-art hash tables offer the following guarantee: If\nkeys/values are Theta(log n) bits each, then it is possible to achieve\nconstant-time insertions/deletions/queries while wasting only O(loglog n) bits\nof space per key when compared to the information-theoretic optimum. Even prior\nto this bound being achieved, the target of O(loglog n) wasted bits per key was\nknown to be a natural end goal, and was proven to be optimal for a number of\nclosely related problems (e.g., stable hashing, dynamic retrieval, and\ndynamically-resized filters).\n  This paper shows that O(loglog n) wasted bits per key is not the end of the\nline for hashing. In fact, for any k \\in [log* n], it is possible to achieve\nO(k)-time insertions/deletions, O(1)-time queries, and O(log^{(k)} n) wasted\nbits per key (all with high probability in n). This means that, each time we\nincrease insertion/deletion time by an <em>additive constant</em>, we reduce the\nwasted bits per key <em>exponentially</em>. We further show that this tradeoff\ncurve is the best achievable by any of a large class of hash tables, including\nany hash table designed using the current framework for making constant-time\nhash tables succinct.</p>\n", "tags": ["ARXIV"] },
{"key": "bengio2009group", "year": "2009", "title":"Group Sparse Coding", "abstract": "<p>Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency, the proposed approach yields better mean average precision in classification.</p>\n", "tags": ["NEURIPS","Quantisation","Supervised"] },
{"key": "bera2021quint", "year": "2021", "title":"QUINT Node Embedding Using Network Hashing", "abstract": "<p>Representation learning using network embedding has received tremendous\nattention due to its efficacy to solve downstream tasks. Popular embedding\nmethods (such as deepwalk, node2vec, LINE) are based on a neural architecture,\nthus unable to scale on large networks both in terms of time and space usage.\nRecently, we proposed BinSketch, a sketching technique for compressing binary\nvectors to binary vectors. In this paper, we show how to extend BinSketch and\nuse it for network hashing. Our proposal named QUINT is built upon BinSketch,\nand it embeds nodes of a sparse network onto a low-dimensional space using\nsimple bi-wise operations. QUINT is the first of its kind that provides\ntremendous gain in terms of speed and space usage without compromising much on\nthe accuracy of the downstream tasks. Extensive experiments are conducted to\ncompare QUINT with seven state-of-the-art network embedding methods for two end\ntasks - link prediction and node classification. We observe huge performance\ngain for QUINT in terms of speedup (up to 7000x) and space saving (up to 80x)\ndue to its bit-wise nature to obtain node embedding. Moreover, QUINT is a\nconsistent top-performer for both the tasks among the baselines across all the\ndatasets. Our empirical observations are backed by rigorous theoretical\nanalysis to justify the effectiveness of QUINT. In particular, we prove that\nQUINT retains enough structural information which can be used further to\napproximate many topological properties of networks with high confidence.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "bercea2023locally", "year": "2023", "title":"Locally Uniform Hashing", "abstract": "<p>Hashing is a common technique used in data processing, with a strong impact\non the time and resources spent on computation. Hashing also affects the\napplicability of theoretical results that often assume access to (unrealistic)\nuniform/fully-random hash functions. In this paper, we are concerned with\ndesigning hash functions that are practical and come with strong theoretical\nguarantees on their performance.\n  To this end, we present tornado tabulation hashing, which is simple, fast,\nand exhibits a certain full, local randomness property that provably makes\ndiverse algorithms perform almost as if (abstract) fully-random hashing was\nused. For example, this includes classic linear probing, the widely used\nHyperLogLog algorithm of Flajolet, Fusy, Gandouet, Meunier [AOFA 97] for\ncounting distinct elements, and the one-permutation hashing of Li, Owen, and\nZhang [NIPS 12] for large-scale machine learning. We also provide a very\nefficient solution for the classical problem of obtaining fully-random hashing\non a fixed (but unknown to the hash function) set of \\(n\\) keys using \\(O(n)\\)\nspace. As a consequence, we get more efficient implementations of the splitting\ntrick of Dietzfelbinger and Rink [ICALP’09] and the succinct space uniform\nhashing of Pagh and Pagh [SICOMP’08].\n  Tornado tabulation hashing is based on a simple method to systematically\nbreak dependencies in tabulation-based hashing techniques.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "bergstra2013instruction", "year": "2013", "title":"Instruction Sequence Expressions For The Secure Hash Algorithm SHA-256", "abstract": "<p>The secure hash function SHA-256 is a function on bit strings. This means\nthat its restriction to the bit strings of any given length can be computed by\na finite instruction sequence that contains only instructions to set and get\nthe content of Boolean registers, forward jump instructions, and a termination\ninstruction. We describe such instruction sequences for the restrictions to bit\nstrings of the different possible lengths by means of uniform terms from an\nalgebraic theory.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "berman2018supermodular", "year": "2018", "title":"Supermodular Locality Sensitive Hashes", "abstract": "<p>In this work, we show deep connections between Locality Sensitive Hashability\nand submodular analysis. We show that the LSHablility of the most commonly\nanalyzed set similarities is in one-to-one correspondance with the\nsupermodularity of these similarities when taken with respect to the symmetric\ndifference of their arguments. We find that the supermodularity of equivalent\nLSHable similarities can be dependent on the set encoding. While monotonicity\nand supermodularity does not imply the metric condition necessary for\nsupermodularity, this condition is guaranteed for the more restricted class of\nsupermodular Hamming similarities that we introduce. We show moreover that LSH\npreserving transformations are also supermodular-preserving, yielding a way to\ngenerate families of similarities both LSHable and supermodular. Finally, we\nshow that even the more restricted family of cardinality-based supermodular\nHamming similarities presents promising aspects for the study of the link\nbetween LSHability and supermodularity. We hope that the several bridges that\nwe introduce between LSHability and supermodularity paves the way to a better\nunderstanding both of supermodular analysis and LSHability, notably in the\ncontext of large-scale supermodular optimization.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "berriche2024leveraging", "year": "2024", "title":"Leveraging High-resolution Features For Improved Deep Hashing-based Image Retrieval", "abstract": "<p>Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "bessa2023weighted", "year": "2023", "title":"Weighted Minwise Hashing Beats Linear Sketching For Inner Product Estimation", "abstract": "<p>We present a new approach for computing compact sketches that can be used to\napproximate the inner product between pairs of high-dimensional vectors. Based\non the Weighted MinHash algorithm, our approach admits strong accuracy\nguarantees that improve on the guarantees of popular linear sketching\napproaches for inner product estimation, such as CountSketch and\nJohnson-Lindenstrauss projection. Specifically, while our method admits\nguarantees that exactly match linear sketching for dense vectors, it yields\nsignificantly lower error for sparse vectors with limited overlap between\nnon-zero entries. Such vectors arise in many applications involving sparse\ndata. They are also important in increasingly popular dataset search\napplications, where inner product sketches are used to estimate data\ncovariance, conditional means, and other quantities involving columns in\nunjoined tables. We complement our theoretical results by showing that our\napproach empirically outperforms existing linear sketches and unweighted\nhashing-based sketches for sparse vectors.</p>\n", "tags": ["Independent"] },
{"key": "bez2022high", "year": "2022", "title":"High Performance Construction Of Recsplit Based Minimal Perfect Hash Functions", "abstract": "<p>A minimal perfect hash function (MPHF) bijectively maps a set S of objects to\nthe first |S| integers. It can be used as a building block in databases and\ndata compression. RecSplit [Esposito et al., ALENEX’20] is currently the most\nspace efficient practical minimal perfect hash function. It heavily relies on\ntrying out hash functions in a brute force way. We introduce rotation fitting,\na new technique that makes the search more efficient by drastically reducing\nthe number of tried hash functions. Additionally, we greatly improve the\nconstruction time of RecSplit by harnessing parallelism on the level of bits,\nvectors, cores, and GPUs. In combination, the resulting improvements yield\nspeedups up to 239 on an 8-core CPU and up to 5438 using a GPU. The original\nsingle-threaded RecSplit implementation needs 1.5 hours to construct an MPHF\nfor 5 Million objects with 1.56 bits per object. On the GPU, we achieve the\nsame space usage in just 5 seconds. Given that the speedups are larger than the\nincrease in energy consumption, our implementation is more energy efficient\nthan the original implementation.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "bhunia2018texture", "year": "2018", "title":"Texture Synthesis Guided Deep Hashing For Texture Image Retrieval", "abstract": "<p>With the large-scale explosion of images and videos over the internet,\nefficient hashing methods have been developed to facilitate memory and time\nefficient retrieval of similar images. However, none of the existing works uses\nhashing to address texture image retrieval mostly because of the lack of\nsufficiently large texture image databases. Our work addresses this problem by\ndeveloping a novel deep learning architecture that generates binary hash codes\nfor input texture images. For this, we first pre-train a Texture Synthesis\nNetwork (TSN) which takes a texture patch as input and outputs an enlarged view\nof the texture by injecting newer texture content. Thus it signifies that the\nTSN encodes the learnt texture specific information in its intermediate layers.\nIn the next stage, a second network gathers the multi-scale feature\nrepresentations from the TSN’s intermediate layers using channel-wise\nattention, combines them in a progressive manner to a dense continuous\nrepresentation which is finally converted into a binary hash code with the help\nof individual and pairwise label information. The new enlarged texture patches\nalso help in data augmentation to alleviate the problem of insufficient texture\ndata and are used to train the second stage of the network. Experiments on\nthree public texture image retrieval datasets indicate the superiority of our\ntexture synthesis guided hashing approach over current state-of-the-art\nmethods.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Independent"] },
{"key": "bibak2020mmh", "year": "2020", "title":"MMH With Arbitrary Modulus Is Always Almost-universal", "abstract": "<p>Universal hash functions, discovered by Carter and Wegman in 1979, are of\ngreat importance in computer science with many applications. MMH\\(^<em>\\) is a\nwell-known \\(\\triangle\\)-universal hash function family, based on the evaluation\nof a dot product modulo a prime. In this paper, we introduce a generalization\nof MMH\\(^</em>\\), that we call GMMH\\(^<em>\\), using the same construction as MMH\\(^</em>\\) but\nwith an arbitrary integer modulus \\(n&gt;1\\), and show that GMMH\\(^*\\) is\n\\(\\frac{1}{p}\\)-almost-\\(\\triangle\\)-universal, where \\(p\\) is the smallest prime\ndivisor of \\(n\\). This bound is tight.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "bigann", "year": "2009", "title":"Datasets for approximate nearest neighbor search", "abstract": "<p>BIGANN consists of SIFT descriptors applied to images from extracted from a large image dataset.</p>\n", "tags": ["Dataset"] },
{"key": "billings2018gradient", "year": "2018", "title":"Gradient Augmented Information Retrieval With Autoencoders And Semantic Hashing", "abstract": "<p>This paper will explore the use of autoencoders for semantic hashing in the\ncontext of Information Retrieval. This paper will summarize how to efficiently\ntrain an autoencoder in order to create meaningful and low-dimensional\nencodings of data. This paper will demonstrate how computing and storing the\nclosest encodings to an input query can help speed up search time and improve\nthe quality of our search results. The novel contributions of this paper\ninvolve using the representation of the data learned by an auto-encoder in\norder to augment our search query in various ways. I present and evaluate the\nnew gradient search augmentation (GSA) approach, as well as the more well-known\npseudo-relevance-feedback (PRF) adjustment. I find that GSA helps to improve\nthe performance of the TF-IDF based information retrieval system, and PRF\ncombined with GSA works best overall for the systems compared in this paper.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "binder2019massively", "year": "2019", "title":"Massively Parallel Path Space Filtering", "abstract": "<p>Restricting path tracing to a small number of paths per pixel for performance\nreasons rarely achieves a satisfactory image quality for scenes of interest.\nHowever, path space filtering may dramatically improve the visual quality by\nsharing information across vertices of paths classified as proximate. Unlike\nscreen space-based approaches, these paths neither need to be present on the\nscreen, nor is filtering restricted to the first intersection with the scene.\nWhile searching proximate vertices had been more expensive than filtering in\nscreen space, we greatly improve over this performance penalty by storing,\nupdating, and looking up the required information in a hash table. The keys are\nconstructed from jittered and quantized information, such that only a single\nquery very likely replaces costly neighborhood searches. A massively parallel\nimplementation of the algorithm is demonstrated on a graphics processing unit\n(GPU).</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "biswas2020perceptual", "year": "2020", "title":"Perceptual Hashing Applied To Tor Domains Recognition", "abstract": "<p>The Tor darknet hosts different types of illegal content, which are monitored\nby cybersecurity agencies. However, manually classifying Tor content can be\nslow and error-prone. To support this task, we introduce Frequency-Dominant\nNeighborhood Structure (F-DNS), a new perceptual hashing method for\nautomatically classifying domains by their screenshots. First, we evaluated\nF-DNS using images subject to various content preserving operations. We\ncompared them with their original images, achieving better correlation\ncoefficients than other state-of-the-art methods, especially in the case of\nrotation. Then, we applied F-DNS to categorize Tor domains using the Darknet\nUsage Service Images-2K (DUSI-2K), a dataset with screenshots of active Tor\nservice domains. Finally, we measured the performance of F-DNS against an image\nclassification approach and a state-of-the-art hashing method. Our proposal\nobtained 98.75% accuracy in Tor images, surpassing all other methods compared.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "biswas2021state", "year": "2021", "title":"State Of The Art Image Hashing", "abstract": "<p>Perceptual image hashing methods are often applied in various objectives,\nsuch as image retrieval, finding duplicate or near-duplicate images, and\nfinding similar images from large-scale image content. The main challenge in\nimage hashing techniques is robust feature extraction, which generates the same\nor similar hashes in images that are visually identical. In this article, we\npresent a short review of the state-of-the-art traditional perceptual hashing\nand deep learning-based perceptual hashing methods, identifying the best\napproaches.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Survey Paper"] },
{"key": "black2021compositional", "year": "2021", "title":"Compositional Sketch Search", "abstract": "<p>We present an algorithm for searching image collections using free-hand\nsketches that describe the appearance and relative positions of multiple\nobjects. Sketch based image retrieval (SBIR) methods predominantly match\nqueries containing a single, dominant object invariant to its position within\nan image. Our work exploits drawings as a concise and intuitive representation\nfor specifying entire scene compositions. We train a convolutional neural\nnetwork (CNN) to encode masked visual features from sketched objects, pooling\nthese into a spatial descriptor encoding the spatial relationships and\nappearances of objects in the composition. Training the CNN backbone as a\nSiamese network under triplet loss yields a metric search embedding for\nmeasuring compositional similarity which may be efficiently leveraged for\nvisual search by applying product quantization.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Quantisation"] },
{"key": "blake2020embedded", "year": "2020", "title":"Embedded Blockchains A Synthesis Of Blockchains Spread Spectrum Watermarking Perceptual Hashing Digital Signatures", "abstract": "<p>In this paper we introduce a scheme for detecting manipulated audio and\nvideo. The scheme is a synthesis of blockchains, encrypted spread spectrum\nwatermarks, perceptual hashing and digital signatures, which we call an\nEmbedded Blockchain. Within this scheme, we use the blockchain for its data\nstructure of a cryptographically linked list, cryptographic hashing for\nabsolute comparisons, perceptual hashing for flexible comparisons, digital\nsignatures for proof of ownership, and encrypted spread spectrum watermarking\nto embed the blockchain into the background noise of the media. So each media\nrecording has its own unique blockchain, with each block holding information\ndescribing the media segment. The problem of verifying the integrity of the\nmedia is recast to traversing the blockchain, block-by-block, and\nsegment-by-segment of the media. If any chain is broken, the difference in the\ncomputed and extracted perceptual hash is used to estimate the level of\nmanipulation.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "bo2009efficient", "year": "2009", "title":"Efficient Match Kernel Between Sets Of Features For Visual Recognition", "abstract": "<p>In visual recognition, the images are frequently modeled as sets of local features (bags). We show that bag of words, a common method to handle such cases, can be viewed as a special match kernel, which counts 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse. It is, therefore, appealing to design match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels on large datasets due to their significant computational cost. To address this problem, we propose an efficient match kernel (EMK), which maps local features to a low dimensional feature space, average the resulting feature vectors to form a set-level feature, then apply a linear classifier. The local feature maps are learned so that their inner products preserve, to the best possible, the values of the specified kernel function. EMK is linear both in the number of images and in the number of local features. We demonstrate that EMK is extremely efficient and achieves the current state of the art performance on three difficult real world datasets: Scene-15, Caltech-101 and Caltech-256.</p>\n", "tags": ["NEURIPS","Quantisation"] },
{"key": "bolettieri2009cophir", "year": "2009", "title":"Cophir A Test Collection For Content-based Image Retrieval", "abstract": "<p>The scalability, as well as the effectiveness, of the different Content-based\nImage Retrieval (CBIR) approaches proposed in literature, is today an important\nresearch issue. Given the wealth of images on the Web, CBIR systems must in\nfact leap towards Web-scale datasets. In this paper, we report on our\nexperience in building a test collection of 100 million images, with the\ncorresponding descriptive features, to be used in experimenting new scalable\ntechniques for similarity searching, and comparing their results. In the\ncontext of the SAPIR (Search on Audio-visual content using Peer-to-peer\nInformation Retrieval) European project, we had to experiment our distributed\nsimilarity searching technology on a realistic data set. Therefore, since no\nlarge-scale collection was available for research purposes, we had to tackle\nthe non-trivial process of image crawling and descriptive feature extraction\n(we used five MPEG-7 features) using the European EGEE computer GRID. The\nresult of this effort is CoPhIR, the first CBIR test collection of such scale.\nCoPhIR is now open to the research community for experiments and comparisons,\nand access to the collection was already granted to more than 50 research\ngroups worldwide.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "bondugula2015shoe", "year": "2015", "title":"SHOE Supervised Hashing With Output Embeddings", "abstract": "<p>We present a supervised binary encoding scheme for image retrieval that\nlearns projections by taking into account similarity between classes obtained\nfrom output embeddings. Our motivation is that binary hash codes learned in\nthis way improve both the visual quality of retrieval results and existing\nsupervised hashing schemes. We employ a sequential greedy optimization that\nlearns relationship aware projections by minimizing the difference between\ninner products of binary codes and output embedding vectors. We develop a joint\noptimization framework to learn projections which improve the accuracy of\nsupervised hashing over the current state of the art with respect to standard\nand sibling evaluation metrics. We further boost performance by applying the\nsupervised dimensionality reduction technique on kernelized input CNN features.\nExperiments are performed on three datasets: CUB-2011, SUN-Attribute and\nImageNet ILSVRC 2010. As a by-product of our method, we show that using a\nsimple k-nn pooling classifier with our discriminative codes improves over the\ncomplex classification models on fine grained datasets like CUB and offer an\nimpressive compression ratio of 1024 on CNN features.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "borgersen2023corrembed", "year": "2023", "title":"Corrembed Evaluating Pre-trained Model Image Similarity Efficacy With A Novel Metric", "abstract": "<p>Detecting visually similar images is a particularly useful attribute to look\nto when calculating product recommendations. Embedding similarity, which\nutilizes pre-trained computer vision models to extract high-level image\nfeatures, has demonstrated remarkable efficacy in identifying images with\nsimilar compositions. However, there is a lack of methods for evaluating the\nembeddings generated by these models, as conventional loss and performance\nmetrics do not adequately capture their performance in image similarity search\ntasks.\n  In this paper, we evaluate the viability of the image embeddings from\nnumerous pre-trained computer vision models using a novel approach named\nCorrEmbed. Our approach computes the correlation between distances in image\nembeddings and distances in human-generated tag vectors. We extensively\nevaluate numerous pre-trained Torchvision models using this metric, revealing\nan intuitive relationship of linear scaling between ImageNet1k accuracy scores\nand tag-correlation scores. Importantly, our method also identifies deviations\nfrom this pattern, providing insights into how different models capture\nhigh-level image features.\n  By offering a robust performance evaluation of these pre-trained models,\nCorrEmbed serves as a valuable tool for researchers and practitioners seeking\nto develop effective, data-driven approaches to similar item recommendations in\nfashion retail.</p>\n", "tags": ["ARXIV"] },
{"key": "borthwick2020scalable", "year": "2020", "title":"Scalable Blocking For Very Large Databases", "abstract": "<p>In the field of database deduplication, the goal is to find approximately\nmatching records within a database. Blocking is a typical stage in this process\nthat involves cheaply finding candidate pairs of records that are potential\nmatches for further processing. We present here Hashed Dynamic Blocking, a new\napproach to blocking designed to address datasets larger than those studied in\nmost prior work. Hashed Dynamic Blocking (HDB) extends Dynamic Blocking, which\nleverages the insight that rare matching values and rare intersections of\nvalues are predictive of a matching relationship. We also present a novel use\nof Locality Sensitive Hashing (LSH) to build blocking key values for huge\ndatabases with a convenient configuration to control the trade-off between\nprecision and recall. HDB achieves massive scale by minimizing data movement,\nusing compact block representation, and greedily pruning ineffective candidate\nblocks using a Count-min Sketch approximate counting data structure. We\nbenchmark the algorithm by focusing on real-world datasets in excess of one\nmillion rows, demonstrating that the algorithm displays linear time complexity\nscaling in this range. Furthermore, we execute HDB on a 530 million row\nindustrial dataset, detecting 68 billion candidate pairs in less than three\nhours at a cost of $307 on a major cloud service.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "botelho2007perfect", "year": "2007", "title":"Perfect Hashing For Data Management Applications", "abstract": "<p>Perfect hash functions can potentially be used to compress data in connection\nwith a variety of data management tasks. Though there has been considerable\nwork on how to construct good perfect hash functions, there is a gap between\ntheory and practice among all previous methods on minimal perfect hashing. On\none side, there are good theoretical results without experimentally proven\npracticality for large key sets. On the other side, there are the theoretically\nanalyzed time and space usage algorithms that assume that truly random hash\nfunctions are available for free, which is an unrealistic assumption. In this\npaper we attempt to bridge this gap between theory and practice, using a number\nof techniques from the literature to obtain a novel scheme that is\ntheoretically well-understood and at the same time achieves an\norder-of-magnitude increase in performance compared to previous ``practical’’\nmethods. This improvement comes from a combination of a novel, theoretically\noptimal perfect hashing scheme that greatly simplifies previous methods, and\nthe fact that our algorithm is designed to make good use of the memory\nhierarchy. We demonstrate the scalability of our algorithm by considering a set\nof over one billion URLs from the World Wide Web of average length 64, for\nwhich we construct a minimal perfect hash function on a commodity PC in a\nlittle more than 1 hour. Our scheme produces minimal perfect hash functions\nusing slightly more than 3 bits per key. For perfect hash functions in the\nrange \\(\\{0,…,2n-1\\}\\) the space usage drops to just over 2 bits per key (i.e.,\none bit more than optimal for representing the key). This is significantly\nbelow of what has been achieved previously for very large values of \\(n\\).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "boytsov2013learning", "year": "2013", "title":"Learning To Prune In Metric And Non-metric Spaces", "abstract": "<p>Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-to prune approaches: density estimation through sampling and “stretching” of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches: the bbtree, the multi-probe locality sensitive hashing (LSH), and permutation methods. Our method was competitive against state-of-the-art methods and, in most cases, was more efficient for the same rank approximation quality.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "breuel2007note", "year": "2007", "title":"A Note On Approximate Nearest Neighbor Methods", "abstract": "<p>A number of authors have described randomized algorithms for solving the\nepsilon-approximate nearest neighbor problem. In this note I point out that the\nepsilon-approximate nearest neighbor property often fails to be a useful\napproximation property, since epsilon-approximate solutions fail to satisfy the\nnecessary preconditions for using nearest neighbors for classification and\nrelated tasks.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "bringer2009identification", "year": "2009", "title":"Identification With Encrypted Biometric Data", "abstract": "<p>Biometrics make human identification possible with a sample of a biometric\ntrait and an associated database. Classical identification techniques lead to\nprivacy concerns. This paper introduces a new method to identify someone using\nhis biometrics in an encrypted way. Our construction combines Bloom Filters\nwith Storage and Locality-Sensitive Hashing. We apply this error-tolerant\nscheme, in a Hamming space, to achieve biometric identification in an efficient\nway. This is the first non-trivial identification scheme dealing with fuzziness\nand encrypted data.</p>\n", "tags": ["ARXIV"] },
{"key": "broder2024min", "year": "2024", "title":"Min-wise Independent Permutations", "abstract": "<p>We define and study the notion of min-wise independent families of permutations. Our research was motivated by the fact that such a family (under some relaxations) is essential to the algorithm used in practice by the AltaVista web index software to detect and filter near-duplicate documents. However, in the course of our investigation we have discovered interesting and challenging theoretical questions related to this concept we present the solutions to some of them and we list the rest as open problems.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "bronstein2011kernel", "year": "2011", "title":"Kernel Diff-hash", "abstract": "<p>This paper presents a kernel formulation of the recently introduced diff-hash\nalgorithm for the construction of similarity-sensitive hash functions. Our\nkernel diff-hash algorithm that shows superior performance on the problem of\nimage feature descriptor matching.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "bronstein2011multimodal", "year": "2011", "title":"Multimodal Diff-hash", "abstract": "<p>Many applications require comparing multimodal data with different structure\nand dimensionality that cannot be compared directly. Recently, there has been\nincreasing interest in methods for learning and efficiently representing such\nmultimodal similarity. In this paper, we present a simple algorithm for\nmultimodal similarity-preserving hashing, trying to map multimodal data into\nthe Hamming space while preserving the intra- and inter-modal similarities. We\nshow that our method significantly outperforms the state-of-the-art method in\nthe field.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "brooks2017multi", "year": "2017", "title":"Multi-level Spherical Locality Sensitive Hashing For Approximate Near Neighbors", "abstract": "<p>This paper introduces “Multi-Level Spherical LSH”: parameter-free, a\nmulti-level, data-dependant Locality Sensitive Hashing data structure for\nsolving the Approximate Near Neighbors Problem (ANN). This data structure uses\na modified version of a multi-probe adaptive querying algorithm, with the\npotential of achieving a \\(O(n^p + t)\\) query run time, for all inputs n where \\(t\n&lt;= n\\).</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "brouwer2017uniqueness", "year": "2017", "title":"Uniqueness Of Codes Using Semidefinite Programming", "abstract": "<p>For \\(n,d,w \\in \\mathbb{N}\\), let \\(A(n,d,w)\\) denote the maximum size of a\nbinary code of word length \\(n\\), minimum distance \\(d\\) and constant weight \\(w\\).\nSchrijver recently showed using semidefinite programming that\n\\(A(23,8,11)=1288\\), and the second author that \\(A(22,8,11)=672\\) and\n\\(A(22,8,10)=616\\). Here we show uniqueness of the codes achieving these bounds.\n  Let \\(A(n,d)\\) denote the maximum size of a binary code of word length \\(n\\) and\nminimum distance \\(d\\). Gijswijt, Mittelmann and Schrijver showed that\n\\(A(20,8)=256\\). We show that there are several nonisomorphic codes achieving\nthis bound, and classify all such codes with all distances divisible by 4.</p>\n", "tags": ["ARXIV"] },
{"key": "buchanan2023review", "year": "2023", "title":"Review Of The NIST Light-weight Cryptography Finalists", "abstract": "<p>Since 2016, NIST has been assessing lightweight encryption methods, and, in\n2022, NIST published the final 10: ASCON, Elephant, GIFT-COFB, Grain128-AEAD,\nISAP, Photon-Beetle, Romulus, Sparkle, TinyJambu, and Xoodyak. At the time that\nthe article was written, NISC announced ASCOn as the chosen method that will be\npublished as NIST’S lightweight cryptography standard later in 2023. In this\narticle, we provide a comparison between these methods in terms of energy\nefficiency, time for encryption, and time for hashing.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "budikova2014disa", "year": "2014", "title":"DISA At Imageclef 2014 Revised Search-based Image Annotation With Decaf Features", "abstract": "<p>This paper constitutes an extension to the report on DISA-MU team\nparticipation in the ImageCLEF 2014 Scalable Concept Image Annotation Task as\npublished in [3]. Specifically, we introduce a new similarity search component\nthat was implemented into the system, report on the results achieved by\nutilizing this component, and analyze the influence of different similarity\nsearch parameters on the annotation quality.</p>\n", "tags": ["ARXIV"] },
{"key": "burkhardt2023simple", "year": "2023", "title":"Simple And Efficient Four-cycle Counting On Sparse Graphs", "abstract": "<p>We consider the problem of counting 4-cycles (\\(C_4\\)) in an undirected graph\n\\(G\\) of \\(n\\) vertices and \\(m\\) edges (in bipartite graphs, 4-cycles are also often\nreferred to as \\(\\textit{butterflies}\\)). Most recently, Wang et al. (2019, 2022)\ndeveloped algorithms for this problem based on hash tables and sorting the\ngraph by degree. Their algorithm takes \\(O(m\\bar\\delta)\\) expected time and\n\\(O(m)\\) space, where \\(\\bar \\delta \\leq O(\\sqrt{m})\\) is the \\(\\textit{average\ndegeneracy}\\) parameter introduced by Burkhardt, Faber \\&amp; Harris (2020). We\ndevelop a streamlined version of this algorithm requiring \\(O(m\\bar\\delta)\\) time\nand precisely \\(n\\) words of space. It has several practical improvements and\noptimizations; for example, it is fully deterministic, does not require any\nauxiliary storage or sorting of the input graph, and uses only addition and\narray access in its inner loops.\n  Our algorithm is very simple and easily adapted to count 4-cycles incident to\neach vertex and edge. Empirical tests demonstrate that our array-based approach\nis \\(4\\times\\) – \\(7\\times\\) faster on average compared to popular hash table\nimplementations.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "burlacu2021hash", "year": "2021", "title":"Hash-based Tree Similarity And Simplification In Genetic Programming For Symbolic Regression", "abstract": "<p>We introduce in this paper a runtime-efficient tree hashing algorithm for the\nidentification of isomorphic subtrees, with two important applications in\ngenetic programming for symbolic regression: fast, online calculation of\npopulation diversity and algebraic simplification of symbolic expression trees.\nBased on this hashing approach, we propose a simple diversity-preservation\nmechanism with promising results on a collection of symbolic regression\nbenchmark problems.</p>\n", "tags": ["Supervised"] },
{"key": "bury2016efficient", "year": "2016", "title":"Efficient Similarity Search In Dynamic Data Streams", "abstract": "<p>The Jaccard index is an important similarity measure for item sets and\nBoolean data. On large datasets, an exact similarity computation is often\ninfeasible for all item pairs both due to time and space constraints, giving\nrise to faster approximate methods. The algorithm of choice used to quickly\ncompute the Jaccard index \\(\\frac{\\vert A \\cap B \\vert}{\\vert A\\cup B\\vert}\\) of\ntwo item sets \\(A\\) and \\(B\\) is usually a form of min-hashing. Most min-hashing\nschemes are maintainable in data streams processing only additions, but none\nare known to work when facing item-wise deletions. In this paper, we\ninvestigate scalable approximation algorithms for rational set similarities, a\nbroad class of similarity measures including Jaccard. Motivated by a result of\nChierichetti and Kumar [J. ACM 2015] who showed any rational set similarity \\(S\\)\nadmits a locality sensitive hashing (LSH) scheme if and only if the\ncorresponding distance \\(1-S\\) is a metric, we can show that there exists a space\nefficient summary maintaining a \\((1\\pm \\epsilon)\\) multiplicative\napproximation to \\(1-S\\) in dynamic data streams. This in turn also yields a\n\\(\\epsilon\\) additive approximation of the similarity. The existence of these\napproximations hints at, but does not directly imply a LSH scheme in dynamic\ndata streams. Our second and main contribution now lies in the design of such a\nLSH scheme maintainable in dynamic data streams. The scheme is space efficient,\neasy to implement and to the best of our knowledge the first of its kind able\nto process deletions.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "busolin2024early", "year": "2024", "title":"Early Exit Strategies For Approximate K-nn Search In Dense Retrieval", "abstract": "<p>Learned dense representations are a popular family of techniques for encoding\nqueries and documents using high-dimensional embeddings, which enable retrieval\nby performing approximate k nearest-neighbors search (A-kNN). A popular\ntechnique for making A-kNN search efficient is based on a two-level index,\nwhere the embeddings of documents are clustered offline and, at query\nprocessing, a fixed number N of clusters closest to the query is visited\nexhaustively to compute the result set. In this paper, we build upon\nstate-of-the-art for early exit A-kNN and propose an unsupervised method based\non the notion of patience, which can reach competitive effectiveness with large\nefficiency gains. Moreover, we discuss a cascade approach where we first\nidentify queries that find their nearest neighbor within the closest t « N\nclusters, and then we decide how many more to visit based on our patience\napproach or other state-of-the-art strategies. Reproducible experiments\nemploying state-of-the-art dense retrieval models and publicly available\nresources show that our techniques improve the A-kNN efficiency with up to 5x\nspeedups while achieving negligible effectiveness losses. All the code used is\navailable at https://github.com/francescobusolin/faiss_pEE</p>\n", "tags": ["ARXIV","Has Code","Unsupervised"] },
{"key": "böhm2020massively", "year": "2020", "title":"Massively Parallel Graph Drawing And Representation Learning", "abstract": "<p>To fully exploit the performance potential of modern multi-core processors,\nmachine learning and data mining algorithms for big data must be parallelized\nin multiple ways. Today’s CPUs consist of multiple cores, each following an\nindependent thread of control, and each equipped with multiple arithmetic units\nwhich can perform the same operation on a vector of multiple data objects.\nGraph embedding, i.e. converting the vertices of a graph into numerical vectors\nis a data mining task of high importance and is useful for graph drawing\n(low-dimensional vectors) and graph representation learning (high-dimensional\nvectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by\nMinimizing the Predictive Entropy), an information-theoretic method which can\ngenerate low and high-dimensional vectors. MulticoreGEMPE applies MIMD\n(Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single\nInstructions Multiple Data, using AVX-512) parallelism. We propose general\nideas applicable in other graph-based algorithms like <em>vectorized hashing</em>\nand <em>vectorized reduction</em>. Our experimental evaluation demonstrates the\nsuperiority of our approach.</p>\n", "tags": ["Graph","Independent"] },
{"key": "cai2016revisit", "year": "2016", "title":"A Revisit Of Hashing Algorithms For Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many\nareas of machine learning and data mining. During the past decade, numerous\nhashing algorithms are proposed to solve this problem. Every proposed algorithm\nclaims outperform other state-of-the-art hashing methods. However, the\nevaluation of these hashing papers was not thorough enough, and those claims\nshould be re-examined. The ultimate goal of an ANNS method is returning the\nmost accurate answers (nearest neighbors) in the shortest time. If implemented\ncorrectly, almost all the hashing methods will have their performance improved\nas the code length increases. However, many existing hashing papers only report\nthe performance with the code length shorter than 128. In this paper, we\ncarefully revisit the problem of search with a hash index, and analyze the pros\nand cons of two popular hash index search procedures. Then we proposed a very\nsimple but effective two level index structures and make a thorough comparison\nof eleven popular hashing algorithms. Surprisingly, the random-projection-based\nLocality Sensitive Hashing (LSH) is the best performed algorithm, which is in\ncontradiction to the claims in all the other ten hashing papers. Despite the\nextreme simplicity of random-projection-based LSH, our results show that the\ncapability of this algorithm has been far underestimated. For the sake of\nreproducibility, all the codes used in the paper are released on GitHub, which\ncan be used as a testing platform for a fair comparison between various hashing\nalgorithms.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "cai2017revisit", "year": "2017", "title":"A Revisit On Deep Hashings For Large-scale Content Based Image Retrieval", "abstract": "<p>There is a growing trend in studying deep hashing methods for content-based\nimage retrieval (CBIR), where hash functions and binary codes are learnt using\ndeep convolutional neural networks and then the binary codes can be used to do\napproximate nearest neighbor (ANN) search. All the existing deep hashing papers\nreport their methods’ superior performance over the traditional hashing methods\naccording to their experimental results. However, there are serious flaws in\nthe evaluations of existing deep hashing papers: (1) The datasets they used are\ntoo small and simple to simulate the real CBIR situation. (2) They did not\ncorrectly include the search time in their evaluation criteria, while the\nsearch time is crucial in real CBIR systems. (3) The performance of some\nunsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses\nmultiple hash tables, which is an important factor should be considered in the\nevaluation while most of the deep hashing papers failed to do so.\n  We re-evaluate several state-of-the-art deep hashing methods with a carefully\ndesigned experimental setting. Empirical results reveal that the performance of\nthese deep hashing methods are inferior to multi-table IsoH, a very simple\nunsupervised hashing method. Thus, the conclusions in all the deep hashing\npapers should be carefully re-examined.</p>\n", "tags": ["ARXIV","Image Retrieval","LSH","Unsupervised"] },
{"key": "cakir2015online", "year": "2015", "title":"Online Supervised Hashing For Ever-growing Datasets", "abstract": "<p>Supervised hashing methods are widely-used for nearest neighbor search in\ncomputer vision applications. Most state-of-the-art supervised hashing\napproaches employ batch-learners. Unfortunately, batch-learning strategies can\nbe inefficient when confronted with large training datasets. Moreover, with\nbatch-learners, it is unclear how to adapt the hash functions as a dataset\ncontinues to grow and diversify over time. Yet, in many practical scenarios the\ndataset grows and diversifies; thus, both the hash functions and the indexing\nmust swiftly accommodate these changes. To address these issues, we propose an\nonline hashing method that is amenable to changes and expansions of the\ndatasets. Since it is an online algorithm, our approach offers linear\ncomplexity with the dataset size. Our solution is supervised, in that we\nincorporate available label information to preserve the semantic neighborhood.\nSuch an adaptive hashing method is attractive; but it requires recomputing the\nhash table as the hash functions are updated. If the frequency of update is\nhigh, then recomputing the hash table entries may cause inefficiencies in the\nsystem, especially for large indexes. Thus, we also propose a framework to\nreduce hash table updates. We compare our method to state-of-the-art solutions\non two benchmarks and demonstrate significant improvements over previous work.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "cakir2017mihash", "year": "2017", "title":"Mihash Online Hashing With Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for nearest neighbor\nretrieval, and recently, online hashing methods have demonstrated good\nperformance-complexity trade-offs by learning hash functions from streaming\ndata. In this paper, we first address a key challenge for online hashing: the\nbinary codes for indexed data must be recomputed to keep pace with updates to\nthe hash functions. We propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information, and use it\nsuccessfully as a criterion to eliminate unnecessary hash table updates. Next,\nwe also show how to optimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method, MIHash, that can be\nused in both online and batch settings. Experiments on image retrieval\nbenchmarks (including a 2.5M image dataset) confirm the effectiveness of our\nformulation, both in reducing hash table recomputations and in learning\nhigh-quality hash functions.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Streaming Data"] },
{"key": "cakir2018hashing", "year": "2018", "title":"Hashing With Binary Matrix Pursuit", "abstract": "<p>We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "cakir2024adaptive", "year": "2024", "title":"Adaptive Hashing For Fast Similarity Search", "abstract": "<p>With the staggering growth in image and video datasets,\nalgorithms that provide fast similarity search and compact\nstorage are crucial. Hashing methods that map the\ndata into Hamming space have shown promise; however,\nmany of these methods employ a batch-learning strategy\nin which the computational cost and memory requirements\nmay become intractable and infeasible with larger and\nlarger datasets. To overcome these challenges, we propose\nan online learning algorithm based on stochastic gradient\ndescent in which the hash functions are updated iteratively\nwith streaming data. In experiments with three image retrieval\nbenchmarks, our online algorithm attains retrieval\naccuracy that is comparable to competing state-of-the-art\nbatch-learning solutions, while our formulation is orders\nof magnitude faster and being online it is adaptable to the\nvariations of the data. Moreover, our formulation yields improved\nretrieval performance over a recently reported online\nhashing technique, Online Kernel Hashing.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Streaming Data"] },
{"key": "cakir2024hashing", "year": "2024", "title":"Hashing With Binary Matrix Pursuit", "abstract": "<p>We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that, under mild assumptions, a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly, we show that with high-capacity hash functions such as CNNs, binary code inference can be greatly simplified for many standard neighborhood definitions, yielding smaller optimization problems and more robust codes. Incorporating our findings, we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "cakir2024mihash", "year": "2024", "title":"Mihash Online Hashing With Mutual Information", "abstract": "<p>Learning-based hashing methods are widely used for\nnearest neighbor retrieval, and recently, online hashing\nmethods have demonstrated good performance-complexity\ntrade-offs by learning hash functions from streaming data.\nIn this paper, we first address a key challenge for online\nhashing: the binary codes for indexed data must be recomputed\nto keep pace with updates to the hash functions.\nWe propose an efficient quality measure for hash functions,\nbased on an information-theoretic quantity, mutual information,\nand use it successfully as a criterion to eliminate\nunnecessary hash table updates. Next, we also show how to\noptimize the mutual information objective using stochastic\ngradient descent. We thus develop a novel hashing method,\nMIHash, that can be used in both online and batch settings.\nExperiments on image retrieval benchmarks (including a\n2.5M image dataset) confirm the effectiveness of our formulation,\nboth in reducing hash table recomputations and\nin learning high-quality hash functions.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Streaming Data"] },
{"key": "cao2016correlation", "year": "2016", "title":"Correlation Hashing Network For Efficient Cross-modal Retrieval", "abstract": "<p>Hashing is widely applied to approximate nearest neighbor search for\nlarge-scale multimodal retrieval with storage and computation efficiency.\nCross-modal hashing improves the quality of hash coding by exploiting semantic\ncorrelations across different modalities. Existing cross-modal hashing methods\nfirst transform data into low-dimensional feature vectors, and then generate\nbinary codes by another separate quantization step. However, suboptimal hash\ncodes may be generated since the quantization error is not explicitly minimized\nand the feature representation is not jointly optimized with the binary codes.\nThis paper presents a Correlation Hashing Network (CHN) approach to cross-modal\nhashing, which jointly learns good data representation tailored to hash coding\nand formally controls the quantization error. The proposed CHN is a hybrid deep\narchitecture that constitutes a convolutional neural network for learning good\nimage representations, a multilayer perception for learning good text\nrepresentations, two hashing layers for generating compact binary codes, and a\nstructured max-margin loss that integrates all things together to enable\nlearning similarity-preserving and high-quality hash codes. Extensive empirical\nstudy shows that CHN yields state of the art cross-modal retrieval performance\non standard benchmarks.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "cao2016transitive", "year": "2016", "title":"Transitive Hashing Network For Heterogeneous Multimedia Retrieval", "abstract": "<p>Hashing has been widely applied to large-scale multimedia retrieval due to\nthe storage and retrieval efficiency. Cross-modal hashing enables efficient\nretrieval from database of one modality in response to a query of another\nmodality. Existing work on cross-modal hashing assumes heterogeneous\nrelationship across modalities for hash function learning. In this paper, we\nrelax the strong assumption by only requiring such heterogeneous relationship\nin an auxiliary dataset different from the query/database domain. We craft a\nhybrid deep architecture to simultaneously learn the cross-modal correlation\nfrom the auxiliary dataset, and align the dataset distributions between the\nauxiliary dataset and the query/database domain, which generates transitive\nhash codes for heterogeneous multimedia retrieval. Extensive experiments\nexhibit that the proposed approach yields state of the art multimedia retrieval\nperformance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "cao2017hashnet", "year": "2017", "title":"Hashnet Deep Learning To Hash By Continuation", "abstract": "<p>Learning to hash has been widely applied to approximate nearest neighbor\nsearch for large-scale multimedia retrieval, due to its computation efficiency\nand retrieval quality. Deep learning to hash, which improves retrieval quality\nby end-to-end representation learning and hash encoding, has received\nincreasing attention recently. Subject to the ill-posed gradient difficulty in\nthe optimization with sign activations, existing deep learning to hash methods\nneed to first learn continuous representations and then generate binary hash\ncodes in a separated binarization step, which suffer from substantial loss of\nretrieval quality. This work presents HashNet, a novel deep architecture for\ndeep learning to hash by continuation method with convergence guarantees, which\nlearns exactly binary hash codes from imbalanced similarity data. The key idea\nis to attack the ill-posed gradient problem in optimizing deep networks with\nnon-smooth binary activations by continuation method, in which we begin from\nlearning an easier network with smoothed activation function and let it evolve\nduring the training, until it eventually goes back to being the original,\ndifficult to optimize, deep network with the sign activation function.\nComprehensive empirical evidence shows that HashNet can generate exactly binary\nhash codes and yield state-of-the-art multimedia retrieval performance on\nstandard benchmarks.</p>\n", "tags": ["ARXIV","Deep Learning","Independent"] },
{"key": "cao2017transfer", "year": "2017", "title":"Transfer Adversarial Hashing For Hamming Space Retrieval", "abstract": "<p>Hashing is widely applied to large-scale image retrieval due to the storage\nand retrieval efficiency. Existing work on deep hashing assumes that the\ndatabase in the target domain is identically distributed with the training set\nin the source domain. This paper relaxes this assumption to a transfer\nretrieval setting, which allows the database and the training set to come from\ndifferent but relevant domains. However, the transfer retrieval setting will\nintroduce two technical difficulties: first, the hash model trained on the\nsource domain cannot work well on the target domain due to the large\ndistribution gap; second, the domain gap makes it difficult to concentrate the\ndatabase points to be within a small Hamming ball. As a consequence, transfer\nretrieval performance within Hamming Radius 2 degrades significantly in\nexisting hashing methods. This paper presents Transfer Adversarial Hashing\n(TAH), a new hybrid deep architecture that incorporates a pairwise\n\\(t\\)-distribution cross-entropy loss to learn concentrated hash codes and an\nadversarial network to align the data distributions between the source and\ntarget domains. TAH can generate compact transfer hash codes for efficient\nimage retrieval on both source and target domains. Comprehensive experiments\nvalidate that TAH yields state of the art Hamming space retrieval performance\non standard datasets.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "cao2018deep", "year": "2018", "title":"Deep Priority Hashing", "abstract": "<p>Deep hashing enables image retrieval by end-to-end learning of deep\nrepresentations and hash codes from training data with pairwise similarity\ninformation. Subject to the distribution skewness underlying the similarity\ninformation, most existing deep hashing methods may underperform for imbalanced\ndata due to misspecified loss functions. This paper presents Deep Priority\nHashing (DPH), an end-to-end architecture that generates compact and balanced\nhash codes in a Bayesian learning framework. The main idea is to reshape the\nstandard cross-entropy loss for similarity-preserving learning such that it\ndown-weighs the loss associated to highly-confident pairs. This idea leads to a\nnovel priority cross-entropy loss, which prioritizes the training on uncertain\npairs over confident pairs. Also, we propose another priority quantization\nloss, which prioritizes hard-to-quantize examples for generation of nearly\nlossless hash codes. Extensive experiments demonstrate that DPH can generate\nhigh-quality hash codes and yield state-of-the-art image retrieval results on\nthree datasets, ImageNet, NUS-WIDE, and MS-COCO.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "cao2024collective", "year": "2024", "title":"Collective Deep Quantization For Efficient Cross-modal Retrieval", "abstract": "<p>Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across\ncontent modalities, e.g., using an image to retrieve for\ntexts. This paper presents a compact coding solution for\nefficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior\nperformance over the hashing solutions in single-modal\nsimilarity retrieval. We propose a collective deep quantization (CDQ) approach, which is the first attempt to\nintroduce quantization in end-to-end deep architecture\nfor cross-modal retrieval. The major contribution lies in\njointly learning deep representations and the quantizers\nfor both modalities using carefully-crafted hybrid networks and well-specified loss functions. In addition, our\napproach simultaneously learns the common quantizer\ncodebook for both modalities through which the crossmodal correlation can be substantially enhanced. CDQ\nenables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art\ncross-modal retrieval results on standard benchmarks.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation"] },
{"key": "cao2024correlation", "year": "2024", "title":"Correlation Autoencoder Hashing For Supervised Cross-modal Search", "abstract": "<p>Due to its storage and query efficiency, hashing has been widely\napplied to approximate nearest neighbor search from large-scale\ndatasets. While there is increasing interest in cross-modal hashing\nwhich facilitates cross-media retrieval by embedding data from different modalities into a common Hamming space, how to distill the\ncross-modal correlation structure effectively remains a challenging\nproblem. In this paper, we propose a novel supervised cross-modal\nhashing method, Correlation Autoencoder Hashing (CAH), to learn\ndiscriminative and compact binary codes based on deep autoencoders. Specifically, CAH jointly maximizes the feature correlation\nrevealed by bimodal data and the semantic correlation conveyed in\nsimilarity labels, while embeds them into hash codes by nonlinear\ndeep autoencoders. Extensive experiments clearly show the superior effectiveness and efficiency of CAH against the state-of-the-art\nhashing methods on standard cross-modal retrieval benchmarks.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "cao2024deep", "year": "2024", "title":"Deep Cauchy Hashing For Hamming Space Retrieval", "abstract": "<p>Due to its computation efficiency and retrieval quality,\nhashing has been widely applied to approximate nearest\nneighbor search for large-scale image retrieval, while deep\nhashing further improves the retrieval quality by end-toend representation learning and hash coding. With compact\nhash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a\ngiven Hamming radius to each query, by hash table lookups\ninstead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small\nHamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming\nspace retrieval.  This work presents Deep Cauchy Hashing\n(DCH), a novel deep hashing model that generates compact\nand concentrated binary hash codes to enable efficient and\neffective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs\nwith Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate\nthat DCH can generate highly concentrated hash codes and\nyield state-of-the-art Hamming space retrieval performance\non three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "cao2024hashgan", "year": "2024", "title":"Hashgan Deep Learning To Hash With Pair Conditional Wasserstein GAN", "abstract": "<p>Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information.\nSubject to the scarcity of similarity information that is often\nexpensive to collect for many application domains, existing\ndeep learning to hash methods may overfit the training data\nand result in substantial loss of retrieval quality. This paper\npresents HashGAN, a novel architecture for deep learning\nto hash, which learns compact binary hash codes from both\nreal images and diverse images synthesized by generative\nmodels. The main idea is to augment the training data with\nnearly real images synthesized from a new Pair Conditional\nWasserstein GAN (PC-WGAN) conditioned on the pairwise\nsimilarity information. Extensive experiments demonstrate\nthat HashGAN can generate high-quality binary hash codes\nand yield state-of-the-art image retrieval performance on\nthree benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["ARXIV","Deep Learning","GAN","Image Retrieval","Supervised"] },
{"key": "cao2024hashnet", "year": "2024", "title":"Hashnet Deep Learning To Hash By Continuation", "abstract": "<p>Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality\nby end-to-end representation learning and hash encoding,\nhas received increasing attention recently. Subject to the illposed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first\nlearn continuous representations and then generate binary\nhash codes in a separated binarization step, which suffer\nfrom substantial loss of retrieval quality.  This work presents\nHashNet, a novel deep architecture for deep learning to\nhash by continuation method with convergence guarantees,\nwhich learns exactly binary hash codes from imbalanced\nsimilarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth\nbinary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it\neventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art\nmultimedia retrieval performance on standard benchmarks.</p>\n", "tags": ["ARXIV","Deep Learning","Independent"] },
{"key": "capelis2005data", "year": "2005", "title":"Data Tastes Better Seasoned Introducing The ASH Family Of Hashing Algorithms", "abstract": "<p>Over the recent months it has become clear that the current generation of\ncryptographic hashing algorithms are insufficient to meet future needs. The ASH\nfamily of algorithms provides modifications to the existing SHA-2 family. These\nmodifications are designed with two main goals: 1) Providing increased\ncollision resistance. 2) Increasing mitigation of security risks\npost-collision. The unique public/private sections and salt/pepper design\nelements provide increased flexibility for a broad range of applications. The\nASH family is a new generation of cryptographic hashing algorithms.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "cappellari2007trellis", "year": "2007", "title":"Trellis-coded Quantization Based On Maximum-hamming-distance Binary Codes", "abstract": "<p>Most design approaches for trellis-coded quantization take advantage of the\nduality of trellis-coded quantization with trellis-coded modulation, and use\nthe same empirically-found convolutional codes to label the trellis branches.\nThis letter presents an alternative approach that instead takes advantage of\nmaximum-Hamming-distance convolutional codes. The proposed source codes are\nshown to be competitive with the best in the literature for the same\ncomputational complexity.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "carreiraperpinan2024hashing", "year": "2024", "title":"Hashing With Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image\ndatabases is binary hashing, where each high-dimensional,\nreal-valued image is mapped onto a low-dimensional, binary\nvector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves\nbinary constraints, and most approaches approximate\nthe optimization by relaxing the constraints and then\nbinarizing the result. Here, we focus on the binary autoencoder\nmodel, which seeks to reconstruct an image from the\nbinary code produced by the hash function. We show that\nthe optimization can be simplified with the method of auxiliary\ncoordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder\nand decoder separately, and one that optimizes the code for\neach image. Image retrieval experiments show the resulting\nhash function outperforms or is competitive with state-ofthe-art\nmethods for binary hashing.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "carreiraperpiñán2015hashing", "year": "2015", "title":"Hashing With Binary Autoencoders", "abstract": "<p>An attractive approach for fast search in image databases is binary hashing,\nwhere each high-dimensional, real-valued image is mapped onto a\nlow-dimensional, binary vector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves binary\nconstraints, and most approaches approximate the optimization by relaxing the\nconstraints and then binarizing the result. Here, we focus on the binary\nautoencoder model, which seeks to reconstruct an image from the binary code\nproduced by the hash function. We show that the optimization can be simplified\nwith the method of auxiliary coordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder and decoder\nseparately, and one that optimizes the code for each image. Image retrieval\nexperiments, using precision/recall and a measure of code utilization, show the\nresulting hash function outperforms or is competitive with state-of-the-art\nmethods for binary hashing.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "carreiraperpiñán2016ensemble", "year": "2016", "title":"An Ensemble Diversity Approach To Supervised Binary Hashing", "abstract": "<p>Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "cartis2021hashing", "year": "2021", "title":"Hashing Embeddings Of Optimal Dimension With Applications To Linear Least Squares", "abstract": "<p>The aim of this paper is two-fold: firstly, to present subspace embedding\nproperties for \\(s\\)-hashing sketching matrices, with \\(s\\geq 1\\), that are optimal\nin the projection dimension \\(m\\) of the sketch, namely, \\(m=\\mathcal{O}(d)\\),\nwhere \\(d\\) is the dimension of the subspace. A diverse set of results are\npresented that address the case when the input matrix has sufficiently low\ncoherence (thus removing the \\(log^2 d\\) factor dependence in \\(m\\), in the\nlow-coherence result of Bourgain et al (2015) at the expense of a smaller\ncoherence requirement); how this coherence changes with the number \\(s\\) of\ncolumn nonzeros (allowing a scaling of \\(\\sqrt{s}\\) of the coherence bound), or\nis reduced through suitable transformations (when considering hashed – instead\nof subsampled – coherence reducing transformations such as randomised\nHadamard). Secondly, we apply these general hashing sketching results to the\nspecial case of Linear Least Squares (LLS), and develop Ski-LLS, a generic\nsoftware package for these problems, that builds upon and improves the\nBlendenpik solver on dense input and the (sequential) LSRN performance on\nsparse problems. In addition to the hashing sketching improvements, we add\nsuitable linear algebra tools for rank-deficient and for sparse problems that\nlead Ski-LLS to outperform not only sketching-based routines on randomly\ngenerated input, but also state of the art direct solver SPQR and iterative\ncode HSL on certain subsets of the sparse Florida matrix collection; namely, on\nleast squares problems that are significantly overdetermined, or moderately\nsparse, or difficult.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "cassee2017analysing", "year": "2017", "title":"Analysing The Performance Of GPU Hash Tables For State Space Exploration", "abstract": "<p>In the past few years, General Purpose Graphics Processors (GPUs) have been\nused to significantly speed up numerous applications. One of the areas in which\nGPUs have recently led to a significant speed-up is model checking. In model\nchecking, state spaces, i.e., large directed graphs, are explored to verify\nwhether models satisfy desirable properties. GPUexplore is a GPU-based model\nchecker that uses a hash table to efficiently keep track of already explored\nstates. As a large number of states is discovered and stored during such an\nexploration, the hash table should be able to quickly handle many inserts and\nqueries concurrently. In this paper, we experimentally compare two different\nhash tables optimised for the GPU, one being the GPUexplore hash table, and the\nother using Cuckoo hashing. We compare the performance of both hash tables\nusing random and non-random data obtained from model checking experiments, to\nanalyse the applicability of the two hash tables for state space exploration.\nWe conclude that Cuckoo hashing is three times faster than GPUexplore hashing\nfor random data, and that Cuckoo hashing is five to nine times faster for\nnon-random data. This suggests great potential to further speed up GPUexplore\nin the near future.</p>\n", "tags": ["Graph","Independent"] },
{"key": "cayton2007learning", "year": "2007", "title":"A Learning Framework For Nearest Neighbor Search", "abstract": "<p>Can we leverage learning techniques to build a fast nearest-neighbor (NN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.</p>\n", "tags": ["NEURIPS","Theory"] },
{"key": "cayton2011accelerating", "year": "2011", "title":"Accelerating Nearest Neighbor Search On Manycore Systems", "abstract": "<p>We develop methods for accelerating metric similarity search that are\neffective on modern hardware. Our algorithms factor into easily parallelizable\ncomponents, making them simple to deploy and efficient on multicore CPUs and\nGPUs. Despite the simple structure of our algorithms, their search performance\nis provably sublinear in the size of the database, with a factor dependent only\non its intrinsic dimensionality. We demonstrate that our methods provide\nsubstantial speedups on a range of datasets and hardware platforms. In\nparticular, we present results on a 48-core server machine, on graphics\nhardware, and on a multicore desktop.</p>\n", "tags": ["COLT","Graph"] },
{"key": "ceccarello2018fresh", "year": "2018", "title":"FRESH Frechet Similarity With Hashing", "abstract": "<p>This paper studies the \\(r\\)-range search problem for curves under the\ncontinuous Fr'echet distance: given a dataset \\(S\\) of \\(n\\) polygonal curves and\na threshold \\(r&gt;0\\), construct a data structure that, for any query curve \\(q\\),\nefficiently returns all entries in \\(S\\) with distance at most \\(r\\) from \\(q\\). We\npropose FRESH, an approximate and randomized approach for \\(r\\)-range search,\nthat leverages on a locality sensitive hashing scheme for detecting candidate\nnear neighbors of the query curve, and on a subsequent pruning step based on a\ncascade of curve simplifications. We experimentally compare \\fresh to exact and\ndeterministic solutions, and we show that high performance can be reached by\nsuitably relaxing precision and recall.</p>\n", "tags": ["Independent"] },
{"key": "celebi2010fast", "year": "2010", "title":"Fast Color Quantization Using Weighted Sort-means Clustering", "abstract": "<p>Color quantization is an important operation with numerous applications in\ngraphics and image processing. Most quantization methods are essentially based\non data clustering algorithms. However, despite its popularity as a general\npurpose clustering algorithm, k-means has not received much respect in the\ncolor quantization literature because of its high computational requirements\nand sensitivity to initialization. In this paper, a fast color quantization\nmethod based on k-means is presented. The method involves several modifications\nto the conventional (batch) k-means algorithm including data reduction, sample\nweighting, and the use of triangle inequality to speed up the nearest neighbor\nsearch. Experiments on a diverse set of images demonstrate that, with the\nproposed modifications, k-means becomes very competitive with state-of-the-art\ncolor quantization methods in terms of both effectiveness and efficiency.</p>\n", "tags": ["Graph","Quantisation","Unsupervised"] },
{"key": "celebi2011improving", "year": "2011", "title":"Improving The Performance Of K-means For Color Quantization", "abstract": "<p>Color quantization is an important operation with many applications in\ngraphics and image processing. Most quantization methods are essentially based\non data clustering algorithms. However, despite its popularity as a general\npurpose clustering algorithm, k-means has not received much respect in the\ncolor quantization literature because of its high computational requirements\nand sensitivity to initialization. In this paper, we investigate the\nperformance of k-means as a color quantizer. We implement fast and exact\nvariants of k-means with several initialization schemes and then compare the\nresulting quantizers to some of the most popular quantizers in the literature.\nExperiments on a diverse set of images demonstrate that an efficient\nimplementation of k-means with an appropriate initialization strategy can in\nfact serve as a very effective color quantizer.</p>\n", "tags": ["Graph","Quantisation","Unsupervised"] },
{"key": "chaidaroon2017variational", "year": "2017", "title":"Variational Deep Semantic Hashing For Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over the past\ndecade, efficient similarity search methods have become a crucial component of\nlarge-scale information retrieval systems. A popular strategy is to represent\noriginal data samples by compact binary codes through hashing. A spectrum of\nmachine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The recent\nadvances of deep learning in a wide range of applications has demonstrated its\ncapability to learn robust and powerful feature representations for complex\ndata. Especially, deep generative models naturally combine the expressiveness\nof probabilistic generative models with the high capacity of deep neural\nnetworks, which is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing.\n  In this paper, we propose a series of novel deep document generative models\nfor text hashing. The first proposed model is unsupervised while the second one\nis supervised by utilizing document labels/tags for hashing. The third model\nfurther considers document-specific factors that affect the generation of\nwords. The probabilistic generative formulation of the proposed models provides\na principled framework for model extension, uncertainty estimation, simulation,\nand interpretability. Based on variational inference and reparameterization,\nthe proposed models can be interpreted as encoder-decoder deep neural networks\nand thus they are capable of learning complex nonlinear distributed\nrepresentations of the original documents. We conduct a comprehensive set of\nexperiments on four public testbeds. The experimental results have demonstrated\nthe effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "chaidaroon2024deep", "year": "2024", "title":"Deep Semantic Text Hashing With Weak Supervision", "abstract": "<p>With an ever increasing amount of data available on the web, fast similarity search has become the critical component for large-scale information retrieval systems. One solution is semantic hashing which designs binary codes to accelerate similarity search. Recently, deep learning has been successfully applied to the semantic hashing problem and produces high-quality compact binary codes compared to traditional methods. However, most state-of-the-art semantic hashing approaches require large amounts of hand-labeled training data which are often expensive and time consuming to collect. The cost of getting labeled data is the key bottleneck in deploying these hashing methods. Motivated by the recent success in machine learning that makes use of weak supervision, we employ unsupervised ranking methods such as BM25 to extract weak signals from training data. We further introduce two deep generative semantic hashing models to leverage weak signals for text hashing. The experimental results on four public datasets show that our models can generate high-quality binary codes without using hand-labeled training data and significantly outperform the competitive unsupervised semantic hashing baselines.</p>\n", "tags": ["ARXIV","Deep Learning","Unsupervised"] },
{"key": "chaidaroon2024variational", "year": "2024", "title":"Variational Deep Semantic Hashing For Text Documents", "abstract": "<p>As the amount of textual data has been rapidly increasing over\nthe past decade, efficient similarity search methods have become\na crucial component of large-scale information retrieval systems.\nA popular strategy is to represent original data samples by compact binary codes through hashing. A spectrum of machine learning methods have been utilized, but they often lack expressiveness\nand flexibility in modeling to learn effective representations. The\nrecent advances of deep learning in a wide range of applications\nhas demonstrated its capability to learn robust and powerful feature representations for complex data. Especially, deep generative\nmodels naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks,\nwhich is very suitable for text modeling. However, little work has\nleveraged the recent progress in deep learning for text hashing. In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers document-specific factors that affect the generation of words. The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability. Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set of experiments on four public testbeds. The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "chakrabarti2014sequential", "year": "2014", "title":"Sequential Hypothesis Tests For Adaptive Locality Sensitive Hashing", "abstract": "<p>All pairs similarity search is a problem where a set of data objects is given\nand the task is to find all pairs of objects that have similarity above a\ncertain threshold for a given similarity measure-of-interest. When the number\nof points or dimensionality is high, standard solutions fail to scale\ngracefully. Approximate solutions such as Locality Sensitive Hashing (LSH) and\nits Bayesian variants (BayesLSH and BayesLSHLite) alleviate the problem to some\nextent and provides substantial speedup over traditional index based\napproaches. BayesLSH is used for pruning the candidate space and computation of\napproximate similarity, whereas BayesLSHLite can only prune the candidates, but\nsimilarity needs to be computed exactly on the original data. Thus where ever\nthe explicit data representation is available and exact similarity computation\nis not too expensive, BayesLSHLite can be used to aggressively prune candidates\nand provide substantial speedup without losing too much on quality. However,\nthe loss in quality is higher in the BayesLSH variant, where explicit data\nrepresentation is not available, rather only a hash sketch is available and\nsimilarity has to be estimated approximately. In this work we revisit the LSH\nproblem from a Frequentist setting and formulate sequential tests for composite\nhypothesis (similarity greater than or less than threshold) that can be\nleveraged by such LSH algorithms for adaptively pruning candidates\naggressively. We propose a vanilla sequential probability ration test (SPRT)\napproach based on this idea and two novel variants. We extend these variants to\nthe case where approximate similarity needs to be computed using fixed-width\nsequential confidence interval generation technique.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "chakraborty2017improved", "year": "2017", "title":"An Improved Video Analysis Using Context Based Extension Of LSH", "abstract": "<p>Locality Sensitive Hashing (LSH) based algorithms have already shown their\npromise in finding approximate nearest neighbors in high dimen- sional data\nspace. However, there are certain scenarios, as in sequential data, where the\nproximity of a pair of points cannot be captured without considering their\nsurroundings or context. In videos, as for example, a particular frame is\nmeaningful only when it is seen in the context of its preceding and following\nframes. LSH has no mechanism to handle the con- texts of the data points. In\nthis article, a novel scheme of Context based Locality Sensitive Hashing\n(conLSH) has been introduced, in which points are hashed together not only\nbased on their closeness, but also because of similar context. The contribution\nmade in this article is three fold. First, conLSH is integrated with a recently\nproposed fast optimal sequence alignment algorithm (FOGSAA) using a layered\napproach. The resultant method is applied to video retrieval for extracting\nsimilar sequences. The pro- posed algorithm yields more than 80% accuracy on an\naverage in different datasets. It has been found to save 36.3% of the total\ntime, consumed by the exhaustive search. conLSH reduces the search space to\napproximately 42% of the entire dataset, when compared with an exhaustive\nsearch by the aforementioned FOGSAA, Bag of Words method and the standard LSH\nimplementations. Secondly, the effectiveness of conLSH is demon- strated in\naction recognition of the video clips, which yields an average gain of 12.83%\nin terms of classification accuracy over the state of the art methods using\nSTIP descriptors. The last but of great significance is that this article\nprovides a way of automatically annotating long and composite real life videos.\nThe source code of conLSH is made available at\nhttp://www.isical.ac.in/~bioinfo_miu/conLSH/conLSH.html</p>\n", "tags": ["ARXIV","Has Code","Independent","LSH","Video Retrieval"] },
{"key": "chakraborty2019conlsh", "year": "2019", "title":"Conlsh Context Based Locality Sensitive Hashing For Mapping Of Noisy SMRT Reads", "abstract": "<p>Single Molecule Real-Time (SMRT) sequencing is a recent advancement of Next\nGen technology developed by Pacific Bio (PacBio). It comes with an explosion of\nlong and noisy reads demanding cutting edge research to get most out of it. To\ndeal with the high error probability of SMRT data, a novel contextual Locality\nSensitive Hashing (conLSH) based algorithm is proposed in this article, which\ncan effectively align the noisy SMRT reads to the reference genome. Here,\nsequences are hashed together based not only on their closeness, but also on\nsimilarity of context. The algorithm has \\(\\mathcal{O}(n^{\\rho+1})\\) space\nrequirement, where \\(n\\) is the number of sequences in the corpus and \\(\\rho\\) is a\nconstant. The indexing time and querying time are bounded by \\(\\mathcal{O}(\n\\frac{n^{\\rho+1} \\cdot \\ln n}{\\ln \\frac{1}{P_2}})\\) and \\(\\mathcal{O}(n^\\rho)\\)\nrespectively, where \\(P_2 &gt; 0\\), is a probability value. This algorithm is\nparticularly useful for retrieving similar sequences, a widely used task in\nbiology. The proposed conLSH based aligner is compared with rHAT, popularly\nused for aligning SMRT reads, and is found to comprehensively beat it in speed\nas well as in memory requirements. In particular, it takes approximately\n\\(24.2\\%\\) less processing time, while saving about \\(70.3\\%\\) in peak memory\nrequirement for H.sapiens PacBio dataset.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "chakraborty2023distinct", "year": "2023", "title":"Distinct Elements In Streams An Algorithm For The (text) Book", "abstract": "<p>Given a data stream \\(\\mathcal{A} = \\langle a_1, a_2, \\ldots, a_m \\rangle\\) of\n\\(m\\) elements where each \\(a_i \\in [n]\\), the Distinct Elements problem is to\nestimate the number of distinct elements in \\(\\mathcal{A}\\).Distinct Elements has\nbeen a subject of theoretical and empirical investigations over the past four\ndecades resulting in space optimal algorithms for it.All the current\nstate-of-the-art algorithms are, however, beyond the reach of an undergraduate\ntextbook owing to their reliance on the usage of notions such as pairwise\nindependence and universal hash functions. We present a simple, intuitive,\nsampling-based space-efficient algorithm whose description and the proof are\naccessible to undergraduates with the knowledge of basic probability theory.</p>\n", "tags": ["Independent"] },
{"key": "chan2023hashing", "year": "2023", "title":"Hashing Neural Video Decomposition With Multiplicative Residuals In Space-time", "abstract": "<p>We present a video decomposition method that facilitates layer-based editing\nof videos with spatiotemporally varying lighting and motion effects. Our neural\nmodel decomposes an input video into multiple layered representations, each\ncomprising a 2D texture map, a mask for the original video, and a\nmultiplicative residual characterizing the spatiotemporal variations in\nlighting conditions. A single edit on the texture maps can be propagated to the\ncorresponding locations in the entire video frames while preserving other\ncontents’ consistencies. Our method efficiently learns the layer-based neural\nrepresentations of a 1080p video in 25s per frame via coordinate hashing and\nallows real-time rendering of the edited result at 71 fps on a single GPU.\nQualitatively, we run our method on various videos to show its effectiveness in\ngenerating high-quality editing effects. Quantitatively, we propose to adopt\nfeature-tracking evaluation metrics for objectively assessing the consistency\nof video editing. Project page: https://lightbulb12294.github.io/hashing-nvd/</p>\n", "tags": ["ARXIV"] },
{"key": "chandrasekaran2017lattice", "year": "2017", "title":"Lattice-based Locality Sensitive Hashing Is Optimal", "abstract": "<p>Locality sensitive hashing (LSH) was introduced by Indyk and Motwani (STOC\n<code class=\"language-plaintext highlighter-rouge\">98) to give the first sublinear time algorithm for the c-approximate nearest\nneighbor (ANN) problem using only polynomial space. At a high level, an LSH\nfamily hashes \"nearby\" points to the same bucket and \"far away\" points to\ndifferent buckets. The quality of measure of an LSH family is its LSH exponent,\nwhich helps determine both query time and space usage.\n  In a seminal work, Andoni and Indyk (FOCS </code>06) constructed an LSH family\nbased on random ball partitioning of space that achieves an LSH exponent of\n1/c^2 for the l_2 norm, which was later shown to be optimal by Motwani, Naor\nand Panigrahy (SIDMA <code class=\"language-plaintext highlighter-rouge\">07) and O'Donnell, Wu and Zhou (TOCT </code>14). Although\noptimal in the LSH exponent, the ball partitioning approach is computationally\nexpensive. So, in the same work, Andoni and Indyk proposed a simpler and more\npractical hashing scheme based on Euclidean lattices and provided computational\nresults using the 24-dimensional Leech lattice. However, no theoretical\nanalysis of the scheme was given, thus leaving open the question of finding the\nexponent of lattice based LSH.\n  In this work, we resolve this question by showing the existence of lattices\nachieving the optimal LSH exponent of 1/c^2 using techniques from the geometry\nof numbers. At a more conceptual level, our results show that optimal LSH space\npartitions can have periodic structure. Understanding the extent to which\nadditional structure can be imposed on these partitions, e.g. to yield low\nspace and query complexity, remains an important open problem.</p>\n", "tags": ["ARXIV","FOCS","Independent","LSH"] },
{"key": "chandrasekhar2017compression", "year": "2017", "title":"Compression Of Deep Neural Networks For Image Instance Retrieval", "abstract": "<p>Image instance retrieval is the problem of retrieving images from a database\nwhich contain the same object. Convolutional Neural Network (CNN) based\ndescriptors are becoming the dominant approach for generating {\\it global image\ndescriptors} for the instance retrieval problem. One major drawback of\nCNN-based {\\it global descriptors} is that uncompressed deep neural network\nmodels require hundreds of megabytes of storage making them inconvenient to\ndeploy in mobile applications or in custom hardware. In this work, we study the\nproblem of neural network model compression focusing on the image instance\nretrieval task. We study quantization, coding, pruning and weight sharing\ntechniques for reducing model size for the instance retrieval problem. We\nprovide extensive experimental results on the trade-off between retrieval\nperformance and model size for different types of networks on several data sets\nproviding the most comprehensive study on this topic. We compress models to the\norder of a few MBs: two orders of magnitude smaller than the uncompressed\nmodels while achieving negligible loss in retrieval performance.</p>\n", "tags": ["ARXIV","CNN","Quantisation","Supervised","Survey Paper"] },
{"key": "charikar2018hashing", "year": "2018", "title":"Hashing-based-estimators For Kernel Density In High Dimensions", "abstract": "<p>Given a set of points \\(P\\subset \\mathbb{R}^{d}\\) and a kernel \\(k\\), the Kernel\nDensity Estimate at a point \\(x\\in\\mathbb{R}^{d}\\) is defined as\n\\(\\mathrm{KDE}<em>{P}(x)=\\frac{1}{|P|}\\sum</em>{y\\in P} k(x,y)\\). We study the problem\nof designing a data structure that given a data set \\(P\\) and a kernel function,\nreturns <em>approximations to the kernel density</em> of a query point in <em>sublinear\ntime</em>. We introduce a class of unbiased estimators for kernel density\nimplemented through locality-sensitive hashing, and give general theorems\nbounding the variance of such estimators. These estimators give rise to\nefficient data structures for estimating the kernel density in high dimensions\nfor a variety of commonly used kernels. Our work is the first to provide\ndata-structures with theoretical guarantees that improve upon simple random\nsampling in high dimensions.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "charikar2018multi", "year": "2018", "title":"Multi-resolution Hashing For Fast Pairwise Summations", "abstract": "<p>A basic computational primitive in the analysis of massive datasets is\nsumming simple functions over a large number of objects. Modern applications\npose an additional challenge in that such functions often depend on a parameter\nvector \\(y\\) (query) that is unknown a priori. Given a set of points \\(X\\subset\n\\mathbb{R}^{d}\\) and a pairwise function \\(w:\\mathbb{R}^{d}\\times\n\\mathbb{R}^{d}\\to [0,1]\\), we study the problem of designing a data-structure\nthat enables sublinear-time approximation of the summation\n\\(Z_{w}(y)=\\frac{1}{|X|}\\sum_{x\\in X}w(x,y)\\) for any query \\(y\\in\n\\mathbb{R}^{d}\\). By combining ideas from Harmonic Analysis (partitions of unity\nand approximation theory) with Hashing-Based-Estimators [Charikar, Siminelakis\nFOCS’17], we provide a general framework for designing such data structures\nthrough hashing that reaches far beyond what previous techniques allowed.\n  A key design principle is a collection of \\(T\\geq 1\\) hashing schemes with\ncollision probabilities \\(p_{1},\\ldots, p_{T}\\) such that \\(\\sup_{t\\in\n[T]}\\{p_{t}(x,y)\\} = \\Theta(\\sqrt{w(x,y)})\\). This leads to a data-structure\nthat approximates \\(Z_{w}(y)\\) using a sub-linear number of samples from each\nhash family. Using this new framework along with Distance Sensitive Hashing\n[Aumuller, Christiani, Pagh, Silvestri PODS’18], we show that such a collection\ncan be constructed and evaluated efficiently for any log-convex function\n\\(w(x,y)=e^{\\phi(\\langle x,y\\rangle)}\\) of the inner product on the unit sphere\n\\(x,y\\in \\mathcal{S}^{d-1}\\).\n  Our method leads to data structures with sub-linear query time that\nsignificantly improve upon random sampling and can be used for Kernel Density\nor Partition Function Estimation. We provide extensions of our result from the\nsphere to \\(\\mathbb{R}^{d}\\) and from scalar functions to vector functions.</p>\n", "tags": ["ARXIV","FOCS","Independent"] },
{"key": "charikar2020kernel", "year": "2020", "title":"Kernel Density Estimation Through Density Constrained Near Neighbor Search", "abstract": "<p>In this paper we revisit the kernel density estimation problem: given a\nkernel \\(K(x, y)\\) and a dataset of \\(n\\) points in high dimensional Euclidean\nspace, prepare a data structure that can quickly output, given a query \\(q\\), a\n\\((1+\\epsilon)\\)-approximation to \\(\\mu:=\\frac1{|P|}\\sum_{p\\in P} K(p, q)\\). First,\nwe give a single data structure based on classical near neighbor search\ntechniques that improves upon or essentially matches the query time and space\ncomplexity for all radial kernels considered in the literature so far. We then\nshow how to improve both the query complexity and runtime by using recent\nadvances in data-dependent near neighbor search.\n  We achieve our results by giving a new implementation of the natural\nimportance sampling scheme. Unlike previous approaches, our algorithm first\nsamples the dataset uniformly (considering a geometric sequence of sampling\nrates), and then uses existing approximate near neighbor search techniques on\nthe resulting smaller dataset to retrieve the sampled points that lie at an\nappropriate distance from the query. We show that the resulting sampled dataset\nhas strong geometric structure, making approximate near neighbor search return\nthe required samples much more efficiently than for worst case datasets of the\nsame size. As an example application, we show that this approach yields a data\nstructure that achieves query time \\(\\mu^{-(1+o(1))/4}\\) and space complexity\n\\(\\mu^{-(1+o(1))}\\) for the Gaussian kernel. Our data dependent approach achieves\nquery time \\(\\mu^{-0.173-o(1)}\\) and space \\(\\mu^{-(1+o(1))}\\) for the Gaussian\nkernel. The data dependent analysis relies on new techniques for tracking the\ngeometric structure of the input datasets in a recursive hashing process that\nwe hope will be of interest in other applications in near neighbor search.</p>\n", "tags": ["ARXIV"] },
{"key": "charte2020showcase", "year": "2020", "title":"A Showcase Of The Use Of Autoencoders In Feature Learning Applications", "abstract": "<p>Autoencoders are techniques for data representation learning based on\nartificial neural networks. Differently to other feature learning methods which\nmay be focused on finding specific transformations of the feature space, they\ncan be adapted to fulfill many purposes, such as data visualization, denoising,\nanomaly detection and semantic hashing. This work presents these applications\nand provides details on how autoencoders can perform them, including code\nsamples making use of an R package with an easy-to-use interface for\nautoencoder design and training, \\texttt{ruta}. Along the way, the explanations\non how each learning task has been achieved are provided with the aim to help\nthe reader design their own autoencoders for these or other objectives.</p>\n", "tags": ["Unsupervised"] },
{"key": "chassaing2005phase", "year": "2005", "title":"Phase Transition For Parking Blocks Brownian Excursion And Coalescence", "abstract": "<p>In this paper, we consider hashing with linear probing for a hashing table\nwith m places, n items (n &lt; m), and l = m&lt;n empty places. For a non computer\nscience-minded reader, we shall use the metaphore of n cars parking on m\nplaces: each car chooses a place at random, and if this place k is occupied,\nthe car tries successively k+1, k+2, … until it finds an empty place (with\nthe convention that place m+1 is actually place 1). Pittel [42] proves that\nwhen l/m goes to some positive limit a &lt; 1, the size of the largest block of\nconsecutive cars is O(log m). In this paper we examine at which level for n a\nphase transition occurs for the largest block of consecutive cars between o(m)\nand O(m). The intermediate case reveals an interesting behaviour of sizes of\nblocks, related to the standard additive coalescent in the same way as the\nsizes of connected components of the random graph are related to the\nmultiplicative coalescent.</p>\n", "tags": ["Graph","Independent"] },
{"key": "chatfield2014efficient", "year": "2014", "title":"Efficient On-the-fly Category Retrieval Using Convnets And Gpus", "abstract": "<p>We investigate the gains in precision and speed, that can be obtained by\nusing Convolutional Networks (ConvNets) for on-the-fly retrieval - where\nclassifiers are learnt at run time for a textual query from downloaded images,\nand used to rank large image or video datasets.\n  We make three contributions: (i) we present an evaluation of state-of-the-art\nimage representations for object category retrieval over standard benchmark\ndatasets containing 1M+ images; (ii) we show that ConvNets can be used to\nobtain features which are incredibly performant, and yet much lower dimensional\nthan previous state-of-the-art image representations, and that their\ndimensionality can be reduced further without loss in performance by\ncompression using product quantization or binarization. Consequently, features\nwith the state-of-the-art performance on large-scale datasets of millions of\nimages can fit in the memory of even a commodity GPU card; (iii) we show that\nan SVM classifier can be learnt within a ConvNet framework on a GPU in parallel\nwith downloading the new training images, allowing for a continuous refinement\nof the model as more images become available, and simultaneous training and\nranking. The outcome is an on-the-fly system that significantly outperforms its\npredecessors in terms of: precision of retrieval, memory requirements, and\nspeed, facilitating accurate on-the-fly learning and ranking in under a second\non a single GPU.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "chee2007new", "year": "2007", "title":"A New Lower Bound For A(1766)", "abstract": "<p>We construct a record-breaking binary code of length 17, minimal distance 6,\nconstant weight 6, and containing 113 codewords.</p>\n", "tags": [] },
{"key": "chegrane2013simple", "year": "2013", "title":"Simple Compact And Robust Approximate String Dictionary", "abstract": "<p>This paper is concerned with practical implementations of approximate string\ndictionaries that allow edit errors. In this problem, we have as input a\ndictionary \\(D\\) of \\(d\\) strings of total length \\(n\\) over an alphabet of size\n\\(\\sigma\\). Given a bound \\(k\\) and a pattern \\(x\\) of length \\(m\\), a query has to\nreturn all the strings of the dictionary which are at edit distance at most \\(k\\)\nfrom \\(x\\), where the edit distance between two strings \\(x\\) and \\(y\\) is defined as\nthe minimum-cost sequence of edit operations that transform \\(x\\) into \\(y\\). The\ncost of a sequence of operations is defined as the sum of the costs of the\noperations involved in the sequence. In this paper, we assume that each of\nthese operations has unit cost and consider only three operations: deletion of\none character, insertion of one character and substitution of a character by\nanother. We present a practical implementation of the data structure we\nrecently proposed and which works only for one error. We extend the scheme to\n\\(2\\leq k&lt;m\\). Our implementation has many desirable properties: it has a very\nfast and space-efficient building algorithm. The dictionary data structure is\ncompact and has fast and robust query time. Finally our data structure is\nsimple to implement as it only uses basic techniques from the literature,\nmainly hashing (linear probing and hash signatures) and succinct data\nstructures (bitvectors supporting rank queries).</p>\n", "tags": ["ARXIV"] },
{"key": "chegrane2017approximate", "year": "2017", "title":"Approximate String Matching Theory And Applications (la Recherche Approchee De Motifs Theorie Et Applications)", "abstract": "<p>The approximate string matching is a fundamental and recurrent problem that\narises in most computer science fields. This problem can be defined as follows:\n  Let \\(D=\\{x_1,x_2,\\ldots x_d\\}\\) be a set of \\(d\\) words defined on an alphabet\n\\(\\Sigma\\), let \\(q\\) be a query defined also on \\(\\Sigma\\), and let \\(k\\) be a\npositive integer. We want to build a data structure on \\(D\\) capable of answering\nthe following query: find all words in \\(D\\) that are at most different from the\nquery word \\(q\\) with \\(k\\) errors.\n  In this thesis, we study the approximate string matching methods in\ndictionaries, texts, and indexes, to propose practical methods that solve this\nproblem efficiently. We explore this problem in three complementary directions:\n  1) The approximate string matching in the dictionary. We propose two\nsolutions to this problem, the first one uses hash tables for \\(k \\geq 2\\), the\nsecond uses the Trie and reverse Trie, and it is restricted to (k = 1). The two\nsolutions are adaptable, without loss of performance, to the approximate string\nmatching in a text.\n  2) The approximate string matching for \\textit{autocompletion}, which is,\nfind all suffixes of a given prefix that may contain errors. We give a new\nsolution better in practice than all the previous proposed solutions.\n  3) The problem of the alignment of biological sequences can be interpreted as\nan approximate string matching problem. We propose a solution for peers and\nmultiple sequences alignment.\n  \\medskip All the results obtained showed that our algorithms, give the best\nperformance on sets of practical data (benchmark from the real world). All our\nmethods are proposed as libraries, and they are published online.</p>\n", "tags": ["ARXIV"] },
{"key": "chen2012greedy", "year": "2012", "title":"Greedy Multiple Instance Learning Via Codebook Learning And Nearest Neighbor Voting", "abstract": "<p>Multiple instance learning (MIL) has attracted great attention recently in\nmachine learning community. However, most MIL algorithms are very slow and\ncannot be applied to large datasets. In this paper, we propose a greedy\nstrategy to speed up the multiple instance learning process. Our contribution\nis two fold. First, we propose a density ratio model, and show that maximizing\na density ratio function is the low bound of the DD model under certain\nconditions. Secondly, we make use of a histogram ratio between positive bags\nand negative bags to represent the density ratio function and find codebooks\nseparately for positive bags and negative bags by a greedy strategy. For\ntesting, we make use of a nearest neighbor strategy to classify new bags. We\ntest our method on both small benchmark datasets and the large TRECVID MED11\ndataset. The experimental results show that our method yields comparable\naccuracy to the current state of the art, while being up to at least one order\nof magnitude faster.</p>\n", "tags": ["ARXIV"] },
{"key": "chen2015compressing", "year": "2015", "title":"Compressing Convolutional Neural Networks", "abstract": "<p>Convolutional neural networks (CNN) are increasingly used in many areas of\ncomputer vision. They are particularly attractive because of their ability to\n“absorb” great quantities of labeled data through millions of parameters.\nHowever, as model sizes increase, so do the storage and memory requirements of\nthe classifiers. We present a novel network architecture, Frequency-Sensitive\nHashed Nets (FreshNets), which exploits inherent redundancy in both\nconvolutional layers and fully-connected layers of a deep learning model,\nleading to dramatic savings in memory and storage consumption. Based on the key\nobservation that the weights of learned convolutional filters are typically\nsmooth and low-frequency, we first convert filter weights to the frequency\ndomain with a discrete cosine transform (DCT) and use a low-cost hash function\nto randomly group frequency parameters into hash buckets. All parameters\nassigned the same hash bucket share a single value learned with standard\nback-propagation. To further reduce model size we allocate fewer hash buckets\nto high-frequency components, which are generally less important. We evaluate\nFreshNets on eight data sets, and show that it leads to drastically better\ncompressed performance than several relevant baselines.</p>\n", "tags": ["ARXIV","CNN","Deep Learning","Supervised"] },
{"key": "chen2016revisiting", "year": "2016", "title":"Revisiting Winner Take All (WTA) Hashing For Sparse Datasets", "abstract": "<p>WTA (Winner Take All) hashing has been successfully applied in many large\nscale vision applications. This hashing scheme was tailored to take advantage\nof the comparative reasoning (or order based information), which showed\nsignificant accuracy improvements. In this paper, we identify a subtle issue\nwith WTA, which grows with the sparsity of the datasets. This issue limits the\ndiscriminative power of WTA. We then propose a solution for this problem based\non the idea of Densification which provably fixes the issue. Our experiments\nshow that Densified WTA Hashing outperforms Vanilla WTA both in image\nclassification and retrieval tasks consistently and significantly.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "chen2017derandomized", "year": "2017", "title":"Derandomized Balanced Allocation", "abstract": "<p>In this paper, we study the maximum loads of explicit hash families in the\n\\(d\\)-choice schemes when allocating sequentially \\(n\\) balls into \\(n\\) bins. We\nconsider the <em>Uniform-Greedy</em> scheme, which provides \\(d\\) independent bins\nfor each ball and places the ball into the bin with the least load, and its\nnon-uniform variant — the <em>Always-Go-Left</em> scheme introduced by\nV\"ocking. We construct a hash family with \\(O(log n log log n)\\) random bits\nbased on the previous work of Celis et al. and show the following results.</p>\n<ol>\n  <li>With high probability, this hash family has a maximum load of \\(\\frac{log\nlog n}{log d} + O(1)\\) in the <em>Uniform-Greedy</em> scheme.</li>\n  <li>With high probability, it has a maximum load of \\(\\frac{log log n}{d log\n\\phi_d} + O(1)\\) in the <em>Always-Go-Left</em> scheme for a constant\n\\(\\phi_d&gt;1.61\\).\n  The maximum loads of our hash family match the maximum loads of a perfectly\nrandom hash function in the <em>Uniform-Greedy</em> and <em>Always-Go-Left</em>\nscheme separately, up to the low order term of constants. Previously, the best\nknown hash families matching the same maximum loads of a perfectly random hash\nfunction in \\(d\\)-choice schemes were \\(O(log n)\\)-wise independent functions,\nwhich needs \\(\\Theta(log^2 n)\\) random bits.</li>\n</ol>\n", "tags": ["ARXIV","Independent"] },
{"key": "chen2018distributed", "year": "2018", "title":"Distributed Collaborative Hashing And Its Applications In Ant Financial", "abstract": "<p>Collaborative filtering, especially latent factor model, has been popularly\nused in personalized recommendation. Latent factor model aims to learn user and\nitem latent factors from user-item historic behaviors. To apply it into real\nbig data scenarios, efficiency becomes the first concern, including offline\nmodel training efficiency and online recommendation efficiency. In this paper,\nwe propose a Distributed Collaborative Hashing (DCH) model which can\nsignificantly improve both efficiencies. Specifically, we first propose a\ndistributed learning framework, following the state-of-the-art parameter server\nparadigm, to learn the offline collaborative model. Our model can be learnt\nefficiently by distributedly computing subgradients in minibatches on workers\nand updating model parameters on servers asynchronously. We then adopt hashing\ntechnique to speedup the online recommendation procedure. Recommendation can be\nquickly made through exploiting lookup hash tables. We conduct thorough\nexperiments on two real large-scale datasets. The experimental results\ndemonstrate that, comparing with the classic and state-of-the-art (distributed)\nlatent factor models, DCH has comparable performance in terms of recommendation\naccuracy but has both fast convergence speed in offline model training\nprocedure and realtime efficiency in online recommendation procedure.\nFurthermore, the encouraging performance of DCH is also shown for several\nreal-world applications in Ant Financial.</p>\n", "tags": ["ARXIV"] },
{"key": "chen2019hadamard", "year": "2019", "title":"Hadamard Codebook Based Deep Hashing", "abstract": "<p>As an approximate nearest neighbor search technique, hashing has been widely\napplied in large-scale image retrieval due to its excellent efficiency. Most\nsupervised deep hashing methods have similar loss designs with embedding\nlearning, while quantizing the continuous high-dim feature into compact binary\nspace. We argue that the existing deep hashing schemes are defective in two\nissues that seriously affect the performance, i.e., bit independence and bit\nbalance. The former refers to hash codes of different classes should be\nindependent of each other, while the latter means each bit should have a\nbalanced distribution of +1s and -1s. In this paper, we propose a novel\nsupervised deep hashing method, termed Hadamard Codebook based Deep Hashing\n(HCDH), which solves the above two problems in a unified formulation.\nSpecifically, we utilize an off-the-shelf algorithm to generate a binary\nHadamard codebook to satisfy the requirement of bit independence and bit\nbalance, which subsequently serves as the desired outputs of the hash functions\nlearning. We also introduce a projection matrix to solve the inconsistency\nbetween the order of Hadamard matrix and the number of classes. Besides, the\nproposed HCDH further exploits the supervised labels by constructing a\nclassifier on top of the outputs of hash functions. Extensive experiments\ndemonstrate that HCDH can yield discriminative and balanced binary codes, which\nwell outperforms many state-of-the-arts on three widely-used benchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "chen2019literature", "year": "2019", "title":"A Literature Study Of Embeddings On Source Code", "abstract": "<p>Natural language processing has improved tremendously after the success of\nword embedding techniques such as word2vec. Recently, the same idea has been\napplied on source code with encouraging results. In this survey, we aim to\ncollect and discuss the usage of word embedding techniques on programs and\nsource code. The articles in this survey have been collected by asking authors\nof related work and with an extensive search on Google Scholar. Each article is\ncategorized into five categories: 1. embedding of tokens 2. embedding of\nfunctions or methods 3. embedding of sequences or sets of method calls 4.\nembedding of binary code 5. other embeddings. We also provide links to\nexperimental data and show some remarkable visualization of code embeddings. In\nsummary, word embedding has been successfully applied on different\ngranularities of source code. With access to countless open-source\nrepositories, we see a great potential of applying other data-driven natural\nlanguage processing techniques on source code in the future.</p>\n", "tags": ["ARXIV","Survey Paper"] },
{"key": "chen2019locality", "year": "2019", "title":"Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond", "abstract": "<p>Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "chen2019revisiting", "year": "2019", "title":"Revisiting Consistent Hashing With Bounded Loads", "abstract": "<p>Dynamic load balancing lies at the heart of distributed caching. Here, the\ngoal is to assign objects (load) to servers (computing nodes) in a way that\nprovides load balancing while at the same time dynamically adjusts to the\naddition or removal of servers. One essential requirement is that the addition\nor removal of small servers should not require us to recompute the complete\nassignment. A popular and widely adopted solution is the two-decade-old\nConsistent Hashing (CH). Recently, an elegant extension was provided to account\nfor server bounds. In this paper, we identify that existing methodologies for\nCH and its variants suffer from cascaded overflow, leading to poor load\nbalancing. This cascading effect leads to decreasing performance of the hashing\nprocedure with increasing load. To overcome the cascading effect, we propose a\nsimple solution to CH based on recent advances in fast minwise hashing. We\nshow, both theoretically and empirically, that our proposed solution is\nsignificantly superior for load balancing and is optimal in many senses. On the\nAOL search dataset and Indiana University Clicks dataset with real user\nactivity, our proposed solution reduces cache misses by several magnitudes.</p>\n", "tags": ["ARXIV"] },
{"key": "chen2019vector", "year": "2019", "title":"Vector And Line Quantization For Billion-scale Similarity Search On Gpus", "abstract": "<p>Billion-scale high-dimensional approximate nearest neighbour (ANN) search has\nbecome an important problem for searching similar objects among the vast amount\nof images and videos available online. The existing ANN methods are usually\ncharacterized by their specific indexing structures, including the inverted\nindex and the inverted multi-index structure. The inverted index structure is\namenable to GPU-based implementations, and the state-of-the-art systems such as\nFaiss are able to exploit the massive parallelism offered by GPUs. However, the\ninverted index requires high memory overhead to index the dataset effectively.\nThe inverted multi-index structure is difficult to implement for GPUs, and also\nineffective in dealing with database with different data distributions. In this\npaper we propose a novel hierarchical inverted index structure generated by\nvector and line quantization methods. Our quantization method improves both\nsearch efficiency and accuracy, while maintaining comparable memory\nconsumption. This is achieved by reducing search space and increasing the\nnumber of indexed regions. We introduce a new ANN search system, VLQ-ADC, that\nis based on the proposed inverted index, and perform extensive evaluation on\ntwo public billion-scale benchmark datasets SIFT1B and DEEP1B. Our evaluation\nshows that VLQ-ADC significantly outperforms the state-of-the-art GPU- and\nCPU-based systems in terms of both accuracy and search speed. The source code\nof VLQ-ADC is available at\nhttps://github.com/zjuchenwei/vector-line-quantization.</p>\n", "tags": ["ARXIV","Has Code","Quantisation"] },
{"key": "chen2020making", "year": "2020", "title":"Making Online Sketching Hashing Even Faster", "abstract": "<p>Data-dependent hashing methods have demonstrated good performance in various\nmachine learning applications to learn a low-dimensional representation from\nthe original data. However, they still suffer from several obstacles: First,\nmost of existing hashing methods are trained in a batch mode, yielding\ninefficiency for training streaming data. Second, the computational cost and\nthe memory consumption increase extraordinarily in the big data setting, which\nperplexes the training procedure. Third, the lack of labeled data hinders the\nimprovement of the model performance. To address these difficulties, we utilize\nonline sketching hashing (OSH) and present a FasteR Online Sketching Hashing\n(FROSH) algorithm to sketch the data in a more compact form via an independent\ntransformation. We provide theoretical justification to guarantee that our\nproposed FROSH consumes less time and achieves a comparable sketching precision\nunder the same memory cost of OSH. We also extend FROSH to its distributed\nimplementation, namely DFROSH, to further reduce the training time cost of\nFROSH while deriving the theoretical bound of the sketching precision. Finally,\nwe conduct extensive experiments on both synthetic and real datasets to\ndemonstrate the attractive merits of FROSH and DFROSH.</p>\n", "tags": ["Streaming Data","Supervised"] },
{"key": "chen2021deep", "year": "2021", "title":"Deep Learning To Ternary Hash Codes By Continuation", "abstract": "<p>Recently, it has been observed that {0,1,-1}-ternary codes which are simply\ngenerated from deep features by hard thresholding, tend to outperform\n{-1,1}-binary codes in image retrieval. To obtain better ternary codes, we for\nthe first time propose to jointly learn the features with the codes by\nappending a smoothed function to the networks. During training, the function\ncould evolve into a non-smoothed ternary function by a continuation method. The\nmethod circumvents the difficulty of directly training discrete functions and\nreduces the quantization errors of ternary codes. Experiments show that the\ngenerated codes indeed could achieve higher retrieval accuracy.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Independent","Quantisation"] },
{"key": "chen2021dvhn", "year": "2021", "title":"DVHN A Deep Hashing Framework For Large-scale Vehicle Re-identification", "abstract": "<p>In this paper, we make the very first attempt to investigate the integration\nof deep hash learning with vehicle re-identification. We propose a deep\nhash-based vehicle re-identification framework, dubbed DVHN, which\nsubstantially reduces memory usage and promotes retrieval efficiency while\nreserving nearest neighbor search accuracy. Concretely,~DVHN directly learns\ndiscrete compact binary hash codes for each image by jointly optimizing the\nfeature learning network and the hash code generating module. Specifically, we\ndirectly constrain the output from the convolutional neural network to be\ndiscrete binary codes and ensure the learned binary codes are optimal for\nclassification. To optimize the deep discrete hashing framework, we further\npropose an alternating minimization method for learning binary\nsimilarity-preserved hashing codes. Extensive experiments on two widely-studied\nvehicle re-identification datasets- \\textbf{VehicleID} and \\textbf{VeRi}-~have\ndemonstrated the superiority of our method against the state-of-the-art deep\nhash methods. \\textbf{DVHN} of \\(2048\\) bits can achieve 13.94\\% and 10.21\\%\naccuracy improvement in terms of \\textbf{mAP} and \\textbf{Rank@1} for\n\\textbf{VehicleID (800)} dataset. For \\textbf{VeRi}, we achieve 35.45\\% and\n32.72\\% performance gains for \\textbf{Rank@1} and \\textbf{mAP}, respectively.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "chen2021high", "year": "2021", "title":"A High-dimensional Sparse Fourier Transform In The Continuous Setting", "abstract": "<p>In this paper, we theoretically propose a new hashing scheme to establish the\nsparse Fourier transform in high-dimensional space. The estimation of the\nalgorithm complexity shows that this sparse Fourier transform can overcome the\ncurse of dimensionality. To the best of our knowledge, this is the first\npolynomial-time algorithm to recover the high-dimensional continuous\nfrequencies.</p>\n", "tags": [] },
{"key": "chen2021spann", "year": "2021", "title":"SPANN Highly-efficient Billion-scale Approximate Nearest Neighborhood Search", "abstract": "<p>The in-memory algorithms for approximate nearest neighbor search (ANNS) have achieved great success for fast high-recall search, but are extremely expensive when handling very large scale database. Thus, there is an increasing request for the hybrid ANNS solutions with small memory and inexpensive solid-state drive (SSD). In this paper, we present a simple but efficient memory-disk hybrid indexing and search system, named SPANN, that follows the inverted index methodology. It stores the centroid points of the posting lists in the memory and the large posting lists in the disk. We guarantee both disk-access efficiency (low  latency) and high recall by effectively reducing the disk-access number and retrieving high-quality posting lists. In the index-building stage, we adopt a hierarchical balanced clustering algorithm to balance the length of posting lists and augment the posting list by adding the points in the closure of the corresponding clusters. In the search stage, we use a query-aware scheme to dynamically prune the access of unnecessary posting lists.  Experiment results demonstrate that SPANN is 2X faster than the state-of-the-art ANNS solution DiskANN to reach the same recall quality 90% with same memory cost in three billion-scale datasets. It can reach 90% recall@1 and recall@10 in just around one millisecond with only 32GB memory cost.  Code is available at: https://github.com/microsoft/SPTAG.</p>\n", "tags": ["Has Code","NEURIPS","Unsupervised"] },
{"key": "chen2021towards", "year": "2021", "title":"Towards Low-loss 1-bit Quantization Of User-item Representations For Top-k Recommendation", "abstract": "<p>Due to the promising advantages in space compression and inference\nacceleration, quantized representation learning for recommender systems has\nbecome an emerging research direction recently. As the target is to embed\nlatent features in the discrete embedding space, developing quantization for\nuser-item representations with a few low-precision integers confronts the\nchallenge of high information loss, thus leading to unsatisfactory performance\nin Top-K recommendation.\n  In this work, we study the problem of representation learning for\nrecommendation with 1-bit quantization. We propose a model named Low-loss\nQuantized Graph Convolutional Network (L^2Q-GCN). Different from previous work\nthat plugs quantization as the final encoder of user-item embeddings, L^2Q-GCN\nlearns the quantized representations whilst capturing the structural\ninformation of user-item interaction graphs at different semantic levels. This\nachieves the substantial retention of intermediate interactive information,\nalleviating the feature smoothing issue for ranking caused by numerical\nquantization. To further improve the model performance, we also present an\nadvanced solution named L^2Q-GCN-anl with quantization approximation and\nannealing training strategy. We conduct extensive experiments on four\nbenchmarks over Top-K recommendation task. The experimental results show that,\nwith nearly 9x representation storage compression, L^2Q-GCN-anl attains about\n90~99% performance recovery compared to the state-of-the-art model.</p>\n", "tags": ["ARXIV","Graph","Quantisation"] },
{"key": "chen2021transhash", "year": "2021", "title":"Transhash Transformer-based Hamming Hashing For Efficient Image Retrieval", "abstract": "<p>Deep hamming hashing has gained growing popularity in approximate nearest\nneighbour search for large-scale image retrieval. Until now, the deep hashing\nfor the image retrieval community has been dominated by convolutional neural\nnetwork architectures, e.g. \\texttt{Resnet}\\cite{he2016deep}. In this paper,\ninspired by the recent advancements of vision transformers, we present\n\\textbf{Transhash}, a pure transformer-based framework for deep hashing\nlearning. Concretely, our framework is composed of two major modules: (1) Based\non \\textit{Vision Transformer} (ViT), we design a siamese vision transformer\nbackbone for image feature extraction. To learn fine-grained features, we\ninnovate a dual-stream feature learning on top of the transformer to learn\ndiscriminative global and local features. (2) Besides, we adopt a Bayesian\nlearning scheme with a dynamically constructed similarity matrix to learn\ncompact binary hash codes. The entire framework is jointly trained in an\nend-to-end manner.~To the best of our knowledge, this is the first work to\ntackle deep hashing learning problems without convolutional neural networks\n(\\textit{CNNs}). We perform comprehensive experiments on three widely-studied\ndatasets: \\textbf{CIFAR-10}, \\textbf{NUSWIDE} and \\textbf{IMAGENET}. The\nexperiments have evidenced our superiority against the existing\nstate-of-the-art deep hashing methods. Specifically, we achieve 8.2\\%, 2.6\\%,\n12.7\\% performance gains in terms of average \\textit{mAP} for different hash\nbit lengths on three public datasets, respectively.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "chen2022approximate", "year": "2022", "title":"Approximate Nearest Neighbor Search Under Neural Similarity Metric For Large-scale Recommendation", "abstract": "<p>Model-based methods for recommender systems have been studied extensively for\nyears. Modern recommender systems usually resort to 1) representation learning\nmodels which define user-item preference as the distance between their\nembedding representations, and 2) embedding-based Approximate Nearest Neighbor\n(ANN) search to tackle the efficiency problem introduced by large-scale corpus.\nWhile providing efficient retrieval, the embedding-based retrieval pattern also\nlimits the model capacity since the form of user-item preference measure is\nrestricted to the distance between their embedding representations. However,\nfor other more precise user-item preference measures, e.g., preference scores\ndirectly derived from a deep neural network, they are computationally\nintractable because of the lack of an efficient retrieval method, and an\nexhaustive search for all user-item pairs is impractical. In this paper, we\npropose a novel method to extend ANN search to arbitrary matching functions,\ne.g., a deep neural network. Our main idea is to perform a greedy walk with a\nmatching function in a similarity graph constructed from all items. To solve\nthe problem that the similarity measures of graph construction and user-item\nmatching function are heterogeneous, we propose a pluggable adversarial\ntraining task to ensure the graph search with arbitrary matching function can\nachieve fairly high precision. Experimental results in both open source and\nindustry datasets demonstrate the effectiveness of our method. The proposed\nmethod has been fully deployed in the Taobao display advertising platform and\nbrings a considerable advertising revenue increase. We also summarize our\ndetailed experiences in deployment in this paper.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Supervised"] },
{"key": "chen2022finger", "year": "2022", "title":"FINGER Fast Inference For Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate K-Nearest Neighbor Search (AKNNS) has now become ubiquitous in\nmodern applications, for example, as a fast search procedure with two tower\ndeep learning models. Graph-based methods for AKNNS in particular have received\ngreat attention due to their superior performance. These methods rely on greedy\ngraph search to traverse the data points as embedding vectors in a database.\nUnder this greedy search scheme, we make a key observation: many distance\ncomputations do not influence search updates so these computations can be\napproximated without hurting performance. As a result, we propose FINGER, a\nfast inference method to achieve efficient graph search. FINGER approximates\nthe distance function by estimating angles between neighboring residual vectors\nwith low-rank bases and distribution matching. The approximated distance can be\nused to bypass unnecessary computations, which leads to faster searches.\nEmpirically, accelerating a popular graph-based method named HNSW by FINGER is\nshown to outperform existing graph-based methods by 20%-60% across different\nbenchmark datasets.</p>\n", "tags": ["ARXIV","Deep Learning","Graph"] },
{"key": "chen2022learning", "year": "2022", "title":"Learning Binarized Graph Representations With Multi-faceted Quantization Reinforcement For Top-k Recommendation", "abstract": "<p>Learning vectorized embeddings is at the core of various recommender systems\nfor user-item matching. To perform efficient online inference, representation\nquantization, aiming to embed the latent features by a compact sequence of\ndiscrete numbers, recently shows the promising potentiality in optimizing both\nmemory and computation overheads. However, existing work merely focuses on\nnumerical quantization whilst ignoring the concomitant information loss issue,\nwhich, consequently, leads to conspicuous performance degradation. In this\npaper, we propose a novel quantization framework to learn Binarized Graph\nRepresentations for Top-K Recommendation (BiGeaR). BiGeaR introduces\nmulti-faceted quantization reinforcement at the pre-, mid-, and post-stage of\nbinarized representation learning, which substantially retains the\nrepresentation informativeness against embedding binarization. In addition to\nsaving the memory footprint, BiGeaR further develops solid online inference\nacceleration with bitwise operations, providing alternative flexibility for the\nrealistic deployment. The empirical results over five large real-world\nbenchmarks show that BiGeaR achieves about 22%~40% performance improvement over\nthe state-of-the-art quantization-based recommender system, and recovers about\n95%~102% of the performance capability of the best full-precision counterpart\nwith over 8x time and space reduction.</p>\n", "tags": ["ARXIV","Graph","Quantisation"] },
{"key": "chen2022locality", "year": "2022", "title":"Locality-sensitive Bucketing Functions For The Edit Distance", "abstract": "<p>Many bioinformatics applications involve bucketing a set of sequences where\neach sequence is allowed to be assigned into multiple buckets. To achieve both\nhigh sensitivity and precision, bucketing methods are desired to assign similar\nsequences into the same bucket while assigning dissimilar sequences into\ndistinct buckets. Existing \\(k\\)-mer-based bucketing methods have been efficient\nin processing sequencing data with low error rate, but encounter much reduced\nsensitivity on data with high error rate. Locality-sensitive hashing (LSH)\nschemes are able to mitigate this issue through tolerating the edits in similar\nsequences, but state-of-the-art methods still have large gaps. Here we\ngeneralize the LSH function by allowing it to hash one sequence into multiple\nbuckets. Formally, a bucketing function, which maps a sequence (of fixed\nlength) into a subset of buckets, is defined to be \\((d_1, d_2)\\)-sensitive if\nany two sequences within an edit distance of \\(d_1\\) are mapped into at least one\nshared bucket, and any two sequences with distance at least \\(d_2\\) are mapped\ninto disjoint subsets of buckets. We construct locality-sensitive bucketing\n(LSB) functions with a variety of values of \\((d_1,d_2)\\) and analyze their\nefficiency with respect to the total number of buckets needed as well as the\nnumber of buckets that a specific sequence is mapped to. We also prove lower\nbounds of these two parameters in different settings and show that some of our\nconstructed LSB functions are optimal. These results provide theoretical\nfoundations for their practical use in analyzing sequences with high error rate\nwhile also providing insights for the hardness of designing ungapped LSH\nfunctions.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "chen2022unitail", "year": "2022", "title":"Unitail Detecting Reading And Matching In Retail Scene", "abstract": "<p>To make full use of computer vision technology in stores, it is required to\nconsider the actual needs that fit the characteristics of the retail scene.\nPursuing this goal, we introduce the United Retail Datasets (Unitail), a\nlarge-scale benchmark of basic visual tasks on products that challenges\nalgorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped\ninstances annotated, the Unitail offers a detection dataset to align product\nappearance better. Furthermore, it provides a gallery-style OCR dataset\ncontaining 1454 product categories, 30k text regions, and 21k transcriptions to\nenable robust reading on products and motivate enhanced product matching.\nBesides benchmarking the datasets using various state-of-the-arts, we customize\na new detector for product detection and provide a simple OCR-based matching\nsolution that verifies its effectiveness.</p>\n", "tags": ["ARXIV"] },
{"key": "chen2023bipartite", "year": "2023", "title":"Bipartite Graph Convolutional Hashing For Effective And Efficient Top-n Search In Hamming Space", "abstract": "<p>Searching on bipartite graphs is basal and versatile to many real-world Web\napplications, e.g., online recommendation, database retrieval, and\nquery-document searching. Given a query node, the conventional approaches rely\non the similarity matching with the vectorized node embeddings in the\ncontinuous Euclidean space. To efficiently manage intensive similarity\ncomputation, developing hashing techniques for graph structured data has\nrecently become an emerging research direction. Despite the retrieval\nefficiency in Hamming space, prior work is however confronted with catastrophic\nperformance decay. In this work, we investigate the problem of hashing with\nGraph Convolutional Network on bipartite graphs for effective Top-N search. We\npropose an end-to-end Bipartite Graph Convolutional Hashing approach, namely\nBGCH, which consists of three novel and effective modules: (1) adaptive graph\nconvolutional hashing, (2) latent feature dispersion, and (3) Fourier\nserialized gradient estimation. Specifically, the former two modules achieve\nthe substantial retention of the structural information against the inevitable\ninformation loss in hash encoding; the last module develops Fourier Series\ndecomposition to the hashing function in the frequency domain mainly for more\naccurate gradient estimation. The extensive experiments on six real-world\ndatasets not only show the performance superiority over the competing\nhashing-based counterparts, but also demonstrate the effectiveness of all\nproposed model components contained therein.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "chen2023homomorphic", "year": "2023", "title":"Homomorphic Hashing Based On Elliptic Curve Cryptography", "abstract": "<p>For avoiding the exposure of plaintexts in cloud environments, some\nhomomorphic hashing algorithms have been proposed to generate the hash value of\neach plaintext, and cloud environments only store the hash values and calculate\nthe hash values for future needs. However, longer hash value generation time\nand longer hash value summary time may be required by these homomorphic hashing\nalgorithms with higher security strengths. Therefore, this study proposes a\nhomomorphic hashing based on elliptic curve cryptography (ECC) to provide a\nhomomorphic hashing function in accordance with the characteristics of ECC.\nFurthermore, mathematical models and practical cases have been given to prove\nthe proposed method. In experiments, the results show that the proposed method\nhave higher efficiency with different security strengths.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "chen2023supervised", "year": "2023", "title":"Supervised Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Deep hashing has shown to be a complexity-efficient solution for the\nApproximate Nearest Neighbor search problem in high dimensional space. Many\nmethods usually build the loss function from pairwise or triplet data points to\ncapture the local similarity structure. Other existing methods construct the\nsimilarity graph and consider all points simultaneously. Auto-encoding\nTwin-bottleneck Hashing is one such method that dynamically builds the graph.\nSpecifically, each input data is encoded into a binary code and a continuous\nvariable, or the so-called twin bottlenecks. The similarity graph is then\ncomputed from these binary codes, which get updated consistently during the\ntraining. In this work, we generalize the original model into a supervised deep\nhashing network by incorporating the label information. In addition, we examine\nthe differences of codes structure between these two networks and consider the\nclass imbalance problem especially in multi-labeled datasets. Experiments on\nthree datasets yield statistically significant improvement against the original\nmodel. Results are also comparable and competitive to other supervised methods.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "chen2024deep", "year": "2024", "title":"Deep Supervised Hashing With Anchor Graph", "abstract": "<p>Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware’s memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "chen2024enhanced", "year": "2024", "title":"Enhanced Discrete Multi-modal Hashing More Constraints Yet Less Time To Learn", "abstract": "<p>Due to the exponential growth of multimedia data, multi-modal hashing as a promising technique to make cross-view retrieval scalable is attracting more and more attention. However, most of the existing multi-modal hashing methods either divide the learning process unnaturally into two separate stages or treat the discrete optimization problem simplistically as a continuous one, which leads to suboptimal results. Recently, a few discrete multi-modal hashing methods that try to address such issues have emerged, but they still ignore several important discrete constraints (such as the balance and decorrelation of hash bits). In this paper, we overcome those limitations by proposing a novel method named “Enhanced Discrete Multi-modal Hashing (EDMH)” which learns binary codes and hashing functions simultaneously from the pairwise similarity matrix of data, under the aforementioned discrete constraints. Although the model of EDMH looks a lot more complex than the other models for multi-modal hashing, we are actually able to develop a fast iterative learning algorithm for it, since the subproblems of its optimization all have closed-form solutions after introducing two auxiliary variables. Our experimental results on three real-world datasets have demonstrated that EDMH not only performs much better than state-of-the-art competitors but also runs much faster than them.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "chen2024hac", "year": "2024", "title":"HAC Hash-grid Assisted Context For 3D Gaussian Splatting Compression", "abstract": "<p>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To address this, we make use of the relations between the\nunorganized anchors and the structured hash grid, leveraging their mutual\ninformation for context modeling, and propose a Hash-grid Assisted Context\n(HAC) framework for highly compact 3DGS representation. Our approach introduces\na binary hash grid to establish continuous spatial consistencies, allowing us\nto unveil the inherent spatial relations of anchors through a carefully\ndesigned context model. To facilitate entropy coding, we utilize Gaussian\ndistributions to accurately estimate the probability of each quantized\nattribute, where an adaptive quantization module is proposed to enable\nhigh-precision quantization of these attributes for improved fidelity\nrestoration. Additionally, we incorporate an adaptive masking strategy to\neliminate invalid Gaussians and anchors. Importantly, our work is the pioneer\nto explore context-based compression for 3DGS representation, resulting in a\nremarkable size reduction of over \\(75\\times\\) compared to vanilla 3DGS, while\nsimultaneously improving fidelity, and achieving over \\(11\\times\\) size reduction\nover SOTA 3DGS compression approach Scaffold-GS. Our code is available here:\nhttps://github.com/YihangChen-ee/HAC</p>\n", "tags": ["Has Code","Quantisation"] },
{"key": "chen2024locality", "year": "2024", "title":"Locality-sensitive Hashing For F-divergences Mutual Information Loss And Beyond", "abstract": "<p>Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "chen2024long", "year": "2024", "title":"Long-tail Hashing", "abstract": "<p>Hashing, which represents data items as compact binary codes, has\nbeen becoming a more and more popular technique, e.g., for large-scale image retrieval, owing to its super fast search speed as well\nas its extremely economical memory consumption. However, existing hashing methods all try to learn binary codes from artificially\nbalanced datasets which are not commonly available in real-world\nscenarios. In this paper, we propose Long-Tail Hashing Network\n(LTHNet), a novel two-stage deep hashing approach that addresses\nthe problem of learning to hash for more realistic datasets where\nthe data labels roughly exhibit a long-tail distribution. Specifically,\nthe first stage is to learn relaxed embeddings of the given dataset\nwith its long-tail characteristic taken into account via an end-to-end deep neural network; the second stage is to binarize those\nobtained embeddings. A critical part of LTHNet is its extended dynamic meta-embedding module which can adaptively realize visual\nknowledge transfer between head and tail classes, and thus enrich\nimage representations for hashing. Our experiments have shown\nthat LTHNet achieves dramatic performance improvements over all\nstate-of-the-art competitors on long-tail datasets, with no or little\nsacrifice on balanced datasets. Further analyses reveal that while to\nour surprise directly manipulating class weights in the loss function\nhas little effect, the extended dynamic meta-embedding module, the\nusage of cross-entropy loss instead of square loss, and the relatively\nsmall batch-size for training all contribute to LTHNet’s success.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "chen2024strongly", "year": "2024", "title":"Strongly Constrained Discrete Hashing", "abstract": "<p>Learning to hash is a fundamental technique widely used in large-scale image retrieval. Most existing methods for learning to hash address the involved discrete optimization problem by the continuous relaxation of the binary constraint, which usually leads to large quantization errors and consequently suboptimal binary codes. A few discrete hashing methods have emerged recently. However, they either completely ignore some useful constraints (specifically the balance and decorrelation of hash bits) or just turn those constraints into regularizers that would make the optimization easier but less accurate. In this paper, we propose a novel supervised hashing method named Strongly Constrained Discrete Hashing (SCDH) which overcomes such limitations. It can learn the binary codes for all examples in the training set, and meanwhile obtain a hash function for unseen samples with the above mentioned constraints preserved. Although the model of SCDH is fairly sophisticated, we are able to find closed-form solutions to all of its optimization subproblems and thus design an efficient algorithm that converges quickly. In addition, we extend SCDH to a kernelized version SCDH K . Our experiments on three large benchmark datasets have demonstrated that not only can SCDH and SCDH K achieve substantially higher MAP scores than state-of-the-art baselines, but they train much faster than those that are also supervised as well.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "chen2024supervised", "year": "2024", "title":"Supervised Consensus Anchor Graph Hashing For Cross Modal Retrieval", "abstract": "<p>The target of cross-modal hashing is to embed heterogeneous multimedia data into a common low-dimensional Hamming space, which plays a pivotal part in multimedia retrieval due to the emergence of big multimodal data. Recently, matrix factorization has achieved great success in cross-modal hashing. However, how to effectively use label information and local geometric structure is still a challenging problem for these approaches. To address this issue, we propose a cross-modal hashing method based on collective matrix factorization, which considers both the label consistency across different modalities and the local geometric consistency in each modality. These two elements are formulated as a graph Laplacian term in the objective function, leading to a substantial improvement on the discriminative power of latent semantic features obtained by collective matrix factorization. Moreover, the proposed method learns unified hash codes for different modalities of an instance to facilitate cross-modal search, and the objective function is solved using an iterative strategy. The experimental results on two benchmark data sets show the effectiveness of the proposed method and its superiority over state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Supervised"] },
{"key": "chen2024towards", "year": "2024", "title":"Towards Effective Top-n Hamming Search Via Bipartite Graph Contrastive Hashing", "abstract": "<p>Searching on bipartite graphs serves as a fundamental task for various\nreal-world applications, such as recommendation systems, database retrieval,\nand document querying. Conventional approaches rely on similarity matching in\ncontinuous Euclidean space of vectorized node embeddings. To handle intensive\nsimilarity computation efficiently, hashing techniques for graph-structured\ndata have emerged as a prominent research direction. However, despite the\nretrieval efficiency in Hamming space, previous studies have encountered\ncatastrophic performance decay. To address this challenge, we investigate the\nproblem of hashing with Graph Convolutional Network for effective Top-N search.\nOur findings indicate the learning effectiveness of incorporating hashing\ntechniques within the exploration of bipartite graph reception fields, as\nopposed to simply treating hashing as post-processing to output embeddings. To\nfurther enhance the model performance, we advance upon these findings and\npropose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel\ndual augmentation approach to both intermediate information and hash code\noutputs in the latent feature spaces, thereby producing more expressive and\nrobust hash codes within a dual self-supervised learning paradigm.\nComprehensive empirical analyses on six real-world benchmarks validate the\neffectiveness of our dual feature contrastive learning in boosting the\nperformance of BGCH+ compared to existing approaches.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "chen2024two", "year": "2024", "title":"A Two-step Cross-modal Hashing By Exploiting Label Correlations And Preserving Similarity In Both Steps", "abstract": "<p>In this paper, we present a novel Two-stEp Cross-modal Hashing method, TECH for short, for cross-modal retrieval tasks. As a two-step method, it first learns hash codes based on semantic labels, while preserving the similarity in the original space and exploiting the label correlations in the label space. In the light of this, it is able to make better use of label information and generate better binary codes. In addition, different from other two-step methods that mainly focus on the hash codes learning, TECH adopts a new hash function learning strategy in the second step, which also preserves the similarity in the original space. Moreover, with the help of well designed objective function and optimization scheme, it is able to generate hash codes discretely and scalable for large scale data. To the best of our knowledge, it is the first cross-modal hashing method exploiting label correlations, and also the first two-step hashing model preserving the similarity while leaning hash function. Extensive experiments demonstrate that the proposed approach outperforms some state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "cheng2018crh", "year": "2018", "title":"CRH A Simple Benchmark Approach To Continuous Hashing", "abstract": "<p>In recent years, the distinctive advancement of handling huge data promotes\nthe evolution of ubiquitous computing and analysis technologies. With the\nconstantly upward system burden and computational complexity, adaptive coding\nhas been a fascinating topic for pattern analysis, with outstanding\nperformance. In this work, a continuous hashing method, termed continuous\nrandom hashing (CRH), is proposed to encode sequential data stream, while\nignorance of previously hashing knowledge is possible. Instead, a random\nselection idea is adopted to adaptively approximate the differential encoding\npatterns of data stream, e.g., streaming media, and iteration is avoided for\nstepwise learning. Experimental results demonstrate our method is able to\nprovide outstanding performance, as a benchmark approach to continuous hashing.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "cheng2024robust", "year": "2024", "title":"Robust Unsupervised Cross-modal Hashing For Multimedia Retrieval", "abstract": "<p>With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "choromanska2015binary", "year": "2015", "title":"Binary Embeddings With Structured Hashed Projections", "abstract": "<p>We consider the hashing mechanism for constructing binary embeddings, that\ninvolves pseudo-random projections followed by nonlinear (sign function)\nmappings. The pseudo-random projection is described by a matrix, where not all\nentries are independent random variables but instead a fixed “budget of\nrandomness” is distributed across the matrix. Such matrices can be efficiently\nstored in sub-quadratic or even linear space, provide reduction in randomness\nusage (i.e. number of required random values), and very often lead to\ncomputational speed ups. We prove several theoretical results showing that\nprojections via various structured matrices followed by nonlinear mappings\naccurately preserve the angular distance between input high-dimensional\nvectors. To the best of our knowledge, these results are the first that give\ntheoretical ground for the use of general structured matrices in the nonlinear\nsetting. In particular, they generalize previous extensions of the\nJohnson-Lindenstrauss lemma and prove the plausibility of the approach that was\nso far only heuristically confirmed for some special structured matrices.\nConsequently, we show that many structured matrices can be used as an efficient\ninformation compression mechanism. Our findings build a better understanding of\ncertain deep architectures, which contain randomly weighted and untrained\nlayers, and yet achieve high performance on different learning tasks. We\nempirically verify our theoretical findings and show the dependence of learning\nvia structured hashed projections on the performance of neural network as well\nas nearest neighbor classifier.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "choromanski2015efficient", "year": "2015", "title":"Efficient Data Hashing With Structured Binary Embeddings", "abstract": "<p>We present here new mechanisms for hashing data via binary embeddings.\nContrary to most of the techniques presented before, the embedding matrix of\nour mechanism is highly structured. That enables us to perform hashing more\nefficiently and use less memory. What is crucial and nonintuitive is the fact\nthat imposing structured mechanism does not affect the quality of the produced\nhash. To the best of our knowledge, we are the first to give strong theoretical\nguarantees of the proposed binary hashing method by proving the efficiency of\nthe mechanism for several classes of structured projection matrices. As a\ncorollary, we obtain binary hashing mechanisms with strong concentration\nresults for circulant and Topelitz matrices. Our approach is however much more\ngeneral.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "chowdhury2019efficient", "year": "2019", "title":"An Efficient Approach For Super And Nested Term Indexing And Retrieval", "abstract": "<p>This paper describes a new approach, called Terminological Bucket Indexing\n(TBI), for efficient indexing and retrieval of both nested and super terms\nusing a single method. We propose a hybrid data structure for facilitating\nfaster indexing building. An evaluation of our approach with respect to widely\nused existing approaches on several publicly available dataset is provided.\nCompared to Trie based approaches, TBI provides comparable performance on\nnested term retrieval and far superior performance on super term retrieval.\nCompared to traditional hash table, TBI needs 80\\% less time for indexing.</p>\n", "tags": ["ARXIV"] },
{"key": "chowdhury2019visir", "year": "2019", "title":"VISIR Visual And Semantic Image Label Refinement", "abstract": "<p>The social media explosion has populated the Internet with a wealth of\nimages. There are two existing paradigms for image retrieval: 1) content-based\nimage retrieval (CBIR), which has traditionally used visual features for\nsimilarity search (e.g., SIFT features), and 2) tag-based image retrieval\n(TBIR), which has relied on user tagging (e.g., Flickr tags). CBIR now gains\nsemantic expressiveness by advances in deep-learning-based detection of visual\nlabels. TBIR benefits from query-and-click logs to automatically infer more\ninformative labels. However, learning-based tagging still yields noisy labels\nand is restricted to concrete objects, missing out on generalizations and\nabstractions. Click-based tagging is limited to terms that appear in the\ntextual context of an image or in queries that lead to a click. This paper\naddresses the above limitations by semantically refining and expanding the\nlabels suggested by learning-based object detection. We consider the semantic\ncoherence between the labels for different objects, leverage lexical and\ncommonsense knowledge, and cast the label assignment into a constrained\noptimization problem solved by an integer linear program. Experiments show that\nour method, called VISIR, improves the quality of the state-of-the-art visual\nlabeling tools like LSDA and YOLO.</p>\n", "tags": ["Deep Learning","Image Retrieval"] },
{"key": "christiani2015from", "year": "2015", "title":"From Independence To Expansion And Back Again", "abstract": "<p>We consider the following fundamental problems: (1) Constructing\n\\(k\\)-independent hash functions with a space-time tradeoff close to Siegel’s\nlower bound. (2) Constructing representations of unbalanced expander graphs\nhaving small size and allowing fast computation of the neighbor function. It is\nnot hard to show that these problems are intimately connected in the sense that\na good solution to one of them leads to a good solution to the other one. In\nthis paper we exploit this connection to present efficient, recursive\nconstructions of \\(k\\)-independent hash functions (and hence expanders with a\nsmall representation). While the previously most efficient construction\n(Thorup, FOCS 2013) needed time quasipolynomial in Siegel’s lower bound, our\ntime bound is just a logarithmic factor from the lower bound.</p>\n", "tags": ["ARXIV","FOCS","Graph","Independent"] },
{"key": "christiani2016framework", "year": "2016", "title":"A Framework For Similarity Search With Space-time Tradeoffs Using Locality-sensitive Filtering", "abstract": "<p>We present a framework for similarity search based on Locality-Sensitive\nFiltering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive\nHashing (LSH) framework to support space-time tradeoffs. Given a family of\nfilters, defined as a distribution over pairs of subsets of space with certain\nlocality-sensitivity properties, we can solve the approximate near neighbor\nproblem in \\(d\\)-dimensional space for an \\(n\\)-point data set with query time\n\\(dn^{\\rho_q+o(1)}\\), update time \\(dn^{\\rho_u+o(1)}\\), and space usage \\(dn + n^{1</p>\n<ul>\n  <li>\\rho_u + o(1)}\\). The space-time tradeoff is tied to the tradeoff between\nquery time and update time, controlled by the exponents \\(\\rho_q, \\rho_u\\) that\nare determined by the filter family. Locality-sensitive filtering was\nintroduced by Becker et al. (SODA 2016) together with a framework yielding a\nsingle, balanced, tradeoff between query time and space, further relying on the\nassumption of an efficient oracle for the filter evaluation algorithm. We\nextend the LSF framework to support space-time tradeoffs and through a\ncombination of existing techniques we remove the oracle assumption.\nBuilding on a filter family for the unit sphere by Laarhoven (arXiv 2015) we\nuse a kernel embedding technique by Rahimi &amp; Recht (NIPS 2007) to show a\nsolution to the \\((r,cr)\\)-near neighbor problem in \\(\\ell_s^d\\)-space for \\(0 &lt; s\n\\leq 2\\) with query and update exponents\n\\(\\rho_q=\\frac{c^s(1+\\lambda)^2}{(c^s+\\lambda)^2}\\) and\n\\(\\rho_u=\\frac{c^s(1-\\lambda)^2}{(c^s+\\lambda)^2}\\) where \\(\\lambda\\in[-1,1]\\) is a\ntradeoff parameter. This result improves upon the space-time tradeoff of\nKapralov (PODS 2015) and is shown to be optimal in the case of a balanced\ntradeoff. Finally, we show a lower bound for the space-time tradeoff on the\nunit sphere that matches Laarhoven’s and our own upper bound in the case of\nrandom data.</li>\n</ul>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "christiani2016set", "year": "2016", "title":"Set Similarity Search Beyond Minhash", "abstract": "<p>We consider the problem of approximate set similarity search under\nBraun-Blanquet similarity \\(B(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} \\cap\n\\mathbf{y}| / \\max(|\\mathbf{x}|, |\\mathbf{y}|)\\). The \\((b_2, b_2)\\)-approximate\nBraun-Blanquet similarity search problem is to preprocess a collection of sets\n\\(P\\) such that, given a query set \\(\\mathbf{q}\\), if there exists \\(\\mathbf{x} \\in\nP\\) with \\(B(\\mathbf{q}, \\mathbf{x}) \\geq b_1\\), then we can efficiently return\n\\(\\mathbf{x}’ \\in P\\) with \\(B(\\mathbf{q}, \\mathbf{x}’) &gt; b_2\\).\n  We present a simple data structure that solves this problem with space usage\n\\(O(n^{1+\\rho}log n + \\sum_{\\mathbf{x} \\in P}|\\mathbf{x}|)\\) and query time\n\\(O(|\\mathbf{q}|n^{\\rho} log n)\\) where \\(n = |P|\\) and \\(\\rho =\nlog(1/b_1)/log(1/b_2)\\). Making use of existing lower bounds for\nlocality-sensitive hashing by O’Donnell et al. (TOCT 2014) we show that this\nvalue of \\(\\rho\\) is tight across the parameter space, i.e., for every choice of\nconstants \\(0 &lt; b_2 &lt; b_1 &lt; 1\\).\n  In the case where all sets have the same size our solution strictly improves\nupon the value of \\(\\rho\\) that can be obtained through the use of\nstate-of-the-art data-independent techniques in the Indyk-Motwani\nlocality-sensitive hashing framework (STOC 1998) such as Broder’s MinHash (CCS\n1997) for Jaccard similarity and Andoni et al.’s cross-polytope LSH (NIPS 2015)\nfor cosine similarity. Surprisingly, even though our solution is\ndata-independent, for a large part of the parameter space we outperform the\ncurrently best data-dependent method by Andoni and Razenshteyn (STOC 2015).</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "christiani2017fast", "year": "2017", "title":"Fast Locality-sensitive Hashing Frameworks For Approximate Near Neighbor Search", "abstract": "<p>The Indyk-Motwani Locality-Sensitive Hashing (LSH) framework (STOC 1998) is a\ngeneral technique for constructing a data structure to answer approximate near\nneighbor queries by using a distribution \\(\\mathcal{H}\\) over locality-sensitive\nhash functions that partition space. For a collection of \\(n\\) points, after\npreprocessing, the query time is dominated by \\(O(n^{\\rho} log n)\\) evaluations\nof hash functions from \\(\\mathcal{H}\\) and \\(O(n^{\\rho})\\) hash table lookups and\ndistance computations where \\(\\rho \\in (0,1)\\) is determined by the\nlocality-sensitivity properties of \\(\\mathcal{H}\\). It follows from a recent\nresult by Dahlgaard et al. (FOCS 2017) that the number of locality-sensitive\nhash functions can be reduced to \\(O(log^2 n)\\), leaving the query time to be\ndominated by \\(O(n^{\\rho})\\) distance computations and \\(O(n^{\\rho} log n)\\)\nadditional word-RAM operations. We state this result as a general framework and\nprovide a simpler analysis showing that the number of lookups and distance\ncomputations closely match the Indyk-Motwani framework, making it a viable\nreplacement in practice. Using ideas from another locality-sensitive hashing\nframework by Andoni and Indyk (SODA 2006) we are able to reduce the number of\nadditional word-RAM operations to \\(O(n^\\rho)\\).</p>\n", "tags": ["ARXIV","FOCS","Independent","LSH"] },
{"key": "christiani2018confirmation", "year": "2018", "title":"Confirmation Sampling For Exact Nearest Neighbor Search", "abstract": "<p>Locality-sensitive hashing (LSH), introduced by Indyk and Motwani in STOC\n‘98, has been an extremely influential framework for nearest neighbor search in\nhigh-dimensional data sets. While theoretical work has focused on the\napproximate nearest neighbor problems, in practice LSH data structures with\nsuitably chosen parameters are used to solve the exact nearest neighbor problem\n(with some error probability). Sublinear query time is often possible in\npractice even for exact nearest neighbor search, intuitively because the\nnearest neighbor tends to be significantly closer than other data points.\nHowever, theory offers little advice on how to choose LSH parameters outside of\npre-specified worst-case settings.\n  We introduce the technique of confirmation sampling for solving the exact\nnearest neighbor problem using LSH. First, we give a general reduction that\ntransforms a sequence of data structures that each find the nearest neighbor\nwith a small, unknown probability, into a data structure that returns the\nnearest neighbor with probability \\(1-\\delta\\), using as few queries as possible.\nSecond, we present a new query algorithm for the LSH Forest data structure with\n\\(L\\) trees that is able to return the exact nearest neighbor of a query point\nwithin the same time bound as an LSH Forest of \\(Ω(L)\\) trees with internal\nparameters specifically tuned to the query and data.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "christiani2019algorithms", "year": "2019", "title":"Algorithms For Similarity Search And Pseudorandomness", "abstract": "<p>We study the problem of approximate near neighbor (ANN) search and show the\nfollowing results:</p>\n<ul>\n  <li>An improved framework for solving the ANN problem using locality-sensitive\nhashing, reducing the number of evaluations of locality-sensitive hash\nfunctions and the word-RAM complexity compared to the standard framework.</li>\n  <li>A framework for solving the ANN problem with space-time tradeoffs as well\nas tight upper and lower bounds for the space-time tradeoff of framework\nsolutions to the ANN problem under cosine similarity.</li>\n  <li>A novel approach to solving the ANN problem on sets along with a matching\nlower bound, improving the state of the art.</li>\n  <li>A self-tuning version of the algorithm is shown through experiments to\noutperform existing similarity join algorithms.</li>\n  <li>Tight lower bounds for asymmetric locality-sensitive hashing which has\napplications to the approximate furthest neighbor problem, orthogonal vector\nsearch, and annulus queries.</li>\n  <li>A proof of the optimality of a well-known Boolean locality-sensitive\nhashing scheme.\n  We study the problem of efficient algorithms for producing high-quality\npseudorandom numbers and obtain the following results:</li>\n  <li>A deterministic algorithm for generating pseudorandom numbers of\narbitrarily high quality in constant time using near-optimal space.</li>\n  <li>A randomized construction of a family of hash functions that outputs\npseudorandom numbers of arbitrarily high quality with space usage and running\ntime nearly matching known cell-probe lower bounds.</li>\n</ul>\n", "tags": ["ARXIV","Independent"] },
{"key": "christiani2020dartminhash", "year": "2020", "title":"Dartminhash Fast Sketching For Weighted Sets", "abstract": "<p>Weighted minwise hashing is a standard dimensionality reduction technique\nwith applications to similarity search and large-scale kernel machines. We\nintroduce a simple algorithm that takes a weighted set \\(x \\in \\mathbb{R}<em>{\\geq\n0}^{d}\\) and computes \\(k\\) independent minhashes in expected time \\(O(k log k +\n\\Vert x \\Vert</em>{0}log( \\Vert x \\Vert_1 + 1/\\Vert x \\Vert_1))\\), improving upon\nthe state-of-the-art BagMinHash algorithm (KDD ‘18) and representing the\nfastest weighted minhash algorithm for sparse data. Our experiments show\nrunning times that scale better with \\(k\\) and \\(\\Vert x \\Vert_0\\) compared to ICWS\n(ICDM ‘10) and BagMinhash, obtaining \\(10\\)x speedups in common use cases. Our\napproach also gives rise to a technique for computing fully independent\nlocality-sensitive hash values for \\((L, K)\\)-parameterized approximate near\nneighbor search under weighted Jaccard similarity in optimal expected time\n\\(O(LK + \\Vert x \\Vert_0)\\), improving on prior work even in the case of\nunweighted sets.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "chua2024nus", "year": "2024", "title":"NUS-WIDE A Real-world Web Image Database From National University Of Singapore", "abstract": "<p>This paper introduces a web image dataset created by NUS’s Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "chuklin2011effective", "year": "2011", "title":"Effective Protocols For Low-distance File Synchronization", "abstract": "<p>Suppose that we have two similar files stored on different computers. We need\nto send the file from the first computer to the second one trying to minimize\nthe number of bits transmitted. This article presents a survey of results known\nfor this communication complexity problem in the case when files are “similar”\nin the sense of Hamming distance. We mainly systematize earlier results\nobtained by various authors in 1990s and 2000s and discuss its connection with\ncoding theory, hashing algorithms and other domains of computer science. In\nparticular cases we propose some improvements of previous constructions.</p>\n", "tags": ["ARXIV","Survey Paper"] },
{"key": "chum2011hash", "year": "2011", "title":"Hash Function Based Secret Sharing Scheme Designs", "abstract": "<p>Secret sharing schemes create an effective method to safeguard a secret by\ndividing it among several participants. By using hash functions and the herding\nhashes technique, we first set up a (t+1, n) threshold scheme which is perfect\nand ideal, and then extend it to schemes for any general access structure. The\nschemes can be further set up as proactive or verifiable if necessary. The\nsetup and recovery of the secret is efficient due to the fast calculation of\nthe hash function. The proposed scheme is flexible because of the use of\nexisting hash functions.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "chung2008tight", "year": "2008", "title":"Tight Bounds For Hashing Block Sources", "abstract": "<p>It is known that if a 2-universal hash function \\(H\\) is applied to elements of\na {\\em block source} \\((X_1,…,X_T)\\), where each item \\(X_i\\) has enough\nmin-entropy conditioned on the previous items, then the output distribution\n\\((H,H(X_1),…,H(X_T))\\) will be ``close’’ to the uniform distribution. We\nprovide improved bounds on how much min-entropy per item is required for this\nto hold, both when we ask that the output be close to uniform in statistical\ndistance and when we only ask that it be statistically close to a distribution\nwith small collision probability. In both cases, we reduce the dependence of\nthe min-entropy on the number \\(T\\) of items from \\(2log T\\) in previous work to\n\\(log T\\), which we show to be optimal. This leads to corresponding improvements\nto the recent results of Mitzenmacher and Vadhan (SODA `08) on the analysis of\nhashing-based algorithms and data structures when the data items come from a\nblock source.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "cifar2009learning", "year": "2009", "title":"Learning Multiple Layers of Features from Tiny Images", "abstract": "<p>Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It\nis, in principle, an excellent dataset for unsupervised training of deep generative models, but previous\nresearchers who have tried this have found it difficult to learn a good set of\nfilters from the images.\nWe show how to train a multi-layer generative model that learns to extract meaningful features which\nresemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute\nthe work among multiple machines connected on a network, we show how training such a model can be\ndone in reasonable time.\nA second problematic aspect of the tiny images dataset is that there are no reliable class labels\nwhich makes it hard to use for object recognition experiments. We created two sets of reliable labels.\nThe CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of\neach of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly\nimproved by pre-training a layer of features on a large set of unlabeled tiny images.</p>\n", "tags": ["Dataset","Supervised"] },
{"key": "ciro2021lsh", "year": "2021", "title":"LSH Methods For Data Deduplication In A Wikipedia Artificial Dataset", "abstract": "<p>This paper illustrates locality sensitive hasing (LSH) models for the\nidentification and removal of nearly redundant data in a text dataset. To\nevaluate the different models, we create an artificial dataset for data\ndeduplication using English Wikipedia articles. Area-Under-Curve (AUC) over 0.9\nwere observed for most models, with the best model reaching 0.96. Deduplication\nenables more effective model training by preventing the model from learning a\ndistribution that differs from the real one as a result of the repeated data.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "coco2014new", "year": "2014", "title":"Microsoft COCO: Common Objects in Context", "abstract": "<p>We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old.\nWith a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</p>\n", "tags": ["Dataset"] },
{"key": "cohen2005duality", "year": "2005", "title":"Duality Between Packings And Coverings Of The Hamming Space", "abstract": "<p>We investigate the packing and covering densities of linear and nonlinear\nbinary codes, and establish a number of duality relationships between the\npacking and covering problems. Specifically, we prove that if almost all codes\n(in the class of linear or nonlinear codes) are good packings, then only a\nvanishing fraction of codes are good coverings, and vice versa: if almost all\ncodes are good coverings, then at most a vanishing fraction of codes are good\npackings. We also show that any specific maximal binary code is either a good\npacking or a good covering, in a certain well-defined sense.</p>\n", "tags": ["ARXIV"] },
{"key": "cohen2009class", "year": "2009", "title":"A Class Of Structured P2P Systems Supporting Browsing", "abstract": "<p>Browsing is a way of finding documents in a large amount of data which is\ncomplementary to querying and which is particularly suitable for multimedia\ndocuments. Locating particular documents in a very large collection of\nmultimedia documents such as the ones available in peer to peer networks is a\ndifficult task. However, current peer to peer systems do not allow to do this\nby browsing. In this report, we show how one can build a peer to peer system\nsupporting a kind of browsing. In our proposal, one must extend an existing\ndistributed hash table system with a few features : handling partial hash-keys\nand providing appropriate routing mechanisms for these hash-keys. We give such\nan algorithm for the particular case of the Tapestry distributed hash table.\nThis is a work in progress as no proper validation has been done yet.</p>\n", "tags": ["ARXIV"] },
{"key": "coleman2019sub", "year": "2019", "title":"Sub-linear Memory Sketches For Near Neighbor Search On Streaming Data", "abstract": "<p>We present the first sublinear memory sketch that can be queried to find the\nnearest neighbors in a dataset. Our online sketching algorithm compresses an N\nelement dataset to a sketch of size \\(O(N^b log^3 N)\\) in \\(O(N^{(b+1)} log^3\nN)\\) time, where \\(b &lt; 1\\). This sketch can correctly report the nearest neighbors\nof any query that satisfies a stability condition parameterized by \\(b\\). We\nachieve sublinear memory performance on stable queries by combining recent\nadvances in locality sensitive hash (LSH)-based estimators, online kernel\ndensity estimation, and compressed sensing. Our theoretical results shed new\nlight on the memory-accuracy tradeoff for nearest neighbor search, and our\nsketch, which consists entirely of short integer arrays, has a variety of\nattractive features in practice. We evaluate the memory-recall tradeoff of our\nmethod on a friend recommendation task in the Google Plus social media network.\nWe obtain orders of magnitude better compression than the random projection\nbased alternative while retaining the ability to report the nearest neighbors\nof practical queries.</p>\n", "tags": ["ARXIV","Independent","LSH","Streaming Data"] },
{"key": "collyer2024know", "year": "2024", "title":"Know Your Neighborhood General And Zero-shot Capable Binary Function Search Powered By Call Graphlets", "abstract": "<p>Binary code similarity detection is an important problem with applications in\nareas like malware analysis, vulnerability research and plagiarism detection.\nThis paper proposes a novel graph neural network architecture combined with a\nnovel graph data representation called call graphlets. A call graphlet encodes\nthe neighborhood around each function in a binary executable, capturing the\nlocal and global context through a series of statistical features. A\nspecialized graph neural network model is then designed to operate on this\ngraph representation, learning to map it to a feature vector that encodes\nsemantic code similarities using deep metric learning. The proposed approach is\nevaluated across four distinct datasets covering different architectures,\ncompiler toolchains, and optimization levels. Experimental results demonstrate\nthat the combination of call graphlets and the novel graph neural network\narchitecture achieves state-of-the-art performance compared to baseline\ntechniques across cross-architecture, mono-architecture and zero shot tasks. In\naddition, our proposed approach also performs well when evaluated against an\nout-of-domain function inlining task. Overall, the work provides a general and\neffective graph neural network-based solution for conducting binary code\nsimilarity detection.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "coluzzi2023mementohash", "year": "2023", "title":"Mementohash A Stateful Minimal Memory Best Performing Consistent Hash Algorithm", "abstract": "<p>Consistent hashing is used in distributed systems and networking applications\nto spread data evenly and efficiently across a cluster of nodes. In this paper,\nwe present MementoHash, a novel consistent hashing algorithm that eliminates\nknown limitations of state-of-the-art algorithms while keeping optimal\nperformance and minimal memory usage. We describe the algorithm in detail,\nprovide a pseudo-code implementation, and formally establish its solid\ntheoretical guarantees. To measure the efficacy of MementoHash, we compare its\nperformance, in terms of memory usage and lookup time, to that of\nstate-of-the-art algorithms, namely, AnchorHash, DxHash, and JumpHash. Unlike\nJumpHash, MementoHash can handle random failures. Moreover, MementoHash does\nnot require fixing the overall capacity of the cluster (as AnchorHash and\nDxHash do), allowing it to scale indefinitely. The number of removed nodes\naffects the performance of all the considered algorithms. Therefore, we conduct\nexperiments considering three different scenarios: stable (no removed nodes),\none-shot removals (90% of the nodes removed at once), and incremental removals.\nWe report experimental results that averaged a varying number of nodes from ten\nto one million. Results indicate that our algorithm shows optimal lookup\nperformance and minimal memory usage in its best-case scenario. It behaves\nbetter than AnchorHash and DxHash in its average-case scenario and at least as\nwell as those two algorithms in its worst-case scenario. However, the\nworst-case scenario for MementoHash occurs when more than 70% of the nodes\nfail, which describes a unlikely scenario. Therefore, MementoHash shows the\nbest performance during the regular life cycle of a cluster.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "coluzzi2024binomialhash", "year": "2024", "title":"Binomialhash A Constant Time Minimal Memory Consistent Hash Algorithm", "abstract": "<p>Consistent hashing is employed in distributed systems and networking\napplications to evenly and effectively distribute data across a cluster of\nnodes. This paper introduces BinomialHash, a consistent hashing algorithm that\noperates in constant time and requires minimal memory. We provide a detailed\nexplanation of the algorithm, offer a pseudo-code implementation, and formally\nestablish its strong theoretical guarantees.</p>\n", "tags": ["ARXIV"] },
{"key": "conjeti2016deep", "year": "2016", "title":"Deep Residual Hashing", "abstract": "<p>Hashing aims at generating highly compact similarity preserving code words\nwhich are well suited for large-scale image retrieval tasks.\n  Most existing hashing methods first encode the images as a vector of\nhand-crafted features followed by a separate binarization step to generate hash\ncodes. This two-stage process may produce sub-optimal encoding. In this paper,\nfor the first time, we propose a deep architecture for supervised hashing\nthrough residual learning, termed Deep Residual Hashing (DRH), for an\nend-to-end simultaneous representation learning and hash coding. The DRH model\nconstitutes four key elements: (1) a sub-network with multiple stacked residual\nblocks; (2) hashing layer for binarization; (3) supervised retrieval loss\nfunction based on neighbourhood component analysis for similarity preserving\nembedding; and (4) hashing related losses and regularisation to control the\nquantization error and improve the quality of hash coding. We present results\nof extensive experiments on a large public chest x-ray image database with\nco-morbidities and discuss the outcome showing substantial improvements over\nthe latest state-of-the art methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "conjeti2017learning", "year": "2017", "title":"Learning Robust Hash Codes For Multiple Instance Image Retrieval", "abstract": "<p>In this paper, for the first time, we introduce a multiple instance (MI) deep\nhashing technique for learning discriminative hash codes with weak bag-level\nsupervision suited for large-scale retrieval. We learn such hash codes by\naggregating deeply learnt hierarchical representations across bag members\nthrough a dedicated MI pool layer. For better trainability and retrieval\nquality, we propose a two-pronged approach that includes robust optimization\nand training with an auxiliary single instance hashing arm which is\ndown-regulated gradually. We pose retrieval for tumor assessment as an MI\nproblem because tumors often coexist with benign masses and could exhibit\ncomplementary signatures when scanned from different anatomical views.\nExperimental validations on benchmark mammography and histology datasets\ndemonstrate improved retrieval performance over the state-of-the-art methods.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Independent"] },
{"key": "connor2016hilbert", "year": "2016", "title":"Hilbert Exclusion Improved Metric Search Through Finite Isometric Embeddings", "abstract": "<p>Most research into similarity search in metric spaces relies upon the\ntriangle inequality property. This property allows the space to be arranged\naccording to relative distances to avoid searching some subspaces. We show that\nmany common metric spaces, notably including those using Euclidean and\nJensen-Shannon distances, also have a stronger property, sometimes called the\nfour-point property: in essence, these spaces allow an isometric embedding of\nany four points in three-dimensional Euclidean space, as well as any three\npoints in two-dimensional Euclidean space. In fact, we show that any space\nwhich is isometrically embeddable in Hilbert space has the stronger property.\nThis property gives stronger geometric guarantees, and one in particular, which\nwe name the Hilbert Exclusion property, allows any indexing mechanism which\nuses hyperplane partitioning to perform better. One outcome of this observation\nis that a number of state-of-the-art indexing mechanisms over high dimensional\nspaces can be easily extended to give a significant increase in performance;\nfurthermore, the improvement given is greater in higher dimensions. This\ntherefore leads to a significant improvement in the cost of metric search in\nthese spaces.</p>\n", "tags": [] },
{"key": "connor2017high", "year": "2017", "title":"High-dimensional Simplexes For Supermetric Search", "abstract": "<p>In 1953, Blumenthal showed that every semi-metric space that is isometrically\nembeddable in a Hilbert space has the n-point property; we have previously\ncalled such spaces supermetric spaces. Although this is a strictly stronger\nproperty than triangle inequality, it is nonetheless closely related and many\nuseful metric spaces possess it. These include Euclidean, Cosine and\nJensen-Shannon spaces of any dimension. A simple corollary of the n-point\nproperty is that, for any (n+1) objects sampled from the space, there exists an\nn-dimensional simplex in Euclidean space whose edge lengths correspond to the\ndistances among the objects. We show how the construction of such simplexes in\nhigher dimensions can be used to give arbitrarily tight lower and upper bounds\non distances within the original space. This allows the construction of an\nn-dimensional Euclidean space, from which lower and upper bounds of the\noriginal space can be calculated, and which is itself an indexable space with\nthe n-point property. For similarity search, the engineering tradeoffs are\ngood: we show significant reductions in data size and metric cost with little\nloss of accuracy, leading to a significant overall improvement in search\nperformance.</p>\n", "tags": ["ARXIV"] },
{"key": "corlatescu2023embersim", "year": "2023", "title":"Embersim A Large-scale Databank For Boosting Similarity Search In Malware Analysis", "abstract": "<p>In recent years there has been a shift from heuristics-based malware\ndetection towards machine learning, which proves to be more robust in the\ncurrent heavily adversarial threat landscape. While we acknowledge machine\nlearning to be better equipped to mine for patterns in the increasingly high\namounts of similar-looking files, we also note a remarkable scarcity of the\ndata available for similarity-targeted research. Moreover, we observe that the\nfocus in the few related works falls on quantifying similarity in malware,\noften overlooking the clean data. This one-sided quantification is especially\ndangerous in the context of detection bypass. We propose to address the\ndeficiencies in the space of similarity research on binary files, starting from\nEMBER - one of the largest malware classification data sets. We enhance EMBER\nwith similarity information as well as malware class tags, to enable further\nresearch in the similarity space. Our contribution is threefold: (1) we publish\nEMBERSim, an augmented version of EMBER, that includes similarity-informed\ntags; (2) we enrich EMBERSim with automatically determined malware class tags\nusing the open-source tool AVClass on VirusTotal data and (3) we describe and\nshare the implementation for our class scoring technique and leaf similarity\nmethod.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "corlay2011fast", "year": "2011", "title":"A Fast Nearest Neighbor Search Algorithm Based On Vector Quantization", "abstract": "<p>In this article, we propose a new fast nearest neighbor search algorithm,\nbased on vector quantization. Like many other branch and bound search\nalgorithms [1,10], a preprocessing recursively partitions the data set into\ndisjointed subsets until the number of points in each part is small enough. In\ndoing so, a search-tree data structure is built. This preliminary recursive\ndata-set partition is based on the vector quantization of the empirical\ndistribution of the initial data-set. Unlike previously cited methods, this\nkind of partitions does not a priori allow to eliminate several brother nodes\nin the search tree with a single test. To overcome this difficulty, we propose\nan algorithm to reduce the number of tested brother nodes to a minimal list\nthat we call “friend Voronoi cells”. The complete description of the method\nrequires a deeper insight into the properties of Delaunay triangulations and\nVoronoi diagrams</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "costa2020new", "year": "2020", "title":"New Bounds For Perfect k-hashing", "abstract": "<p>Let \\(C\\subseteq \\{1,\\ldots,k\\}^n\\) be such that for any \\(k\\) distinct elements\nof \\(C\\) there exists a coordinate where they all differ simultaneously. Fredman\nand Koml'os studied upper and lower bounds on the largest cardinality of such\na set \\(C\\), in particular proving that as \\(n\\to\\infty\\), \\(|C|\\leq \\exp(n\nk!/k^{k-1}+o(n))\\). Improvements over this result where first derived by\ndifferent authors for \\(k=4\\). More recently, Guruswami and Riazanov showed that\nthe coefficient \\(k!/k^{k-1}\\) is certainly not tight for any \\(k&gt;3\\), although\nthey could only determine explicit improvements for \\(k=5,6\\). For larger \\(k\\),\ntheir method gives numerical values modulo a conjecture on the maxima of\ncertain polynomials.\n  In this paper, we first prove their conjecture, completing the explicit\ncomputation of an improvement over the Fredman-Koml'os bound for any \\(k\\).\nThen, we develop a different method which gives substantial improvements for\n\\(k=5,6\\).</p>\n", "tags": ["ARXIV"] },
{"key": "cox2005unifying", "year": "2005", "title":"A Unifying Class Of Skorokhod Embeddings Connecting The Azema-yor And Vallois Embeddings", "abstract": "<p>In this paper we consider the Skorokhod embedding problem in Brownian motion.\nIn particular, we give a solution based on the local time at zero of a variably\nskewed Brownian motion related to the underlying Brownian motion. Special cases\nof the construction include the Azema-Yor and Vallois embeddings. In turn, the\nconstruction has an interpretation in the Chacon-Walsh framework.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "coz2022post", "year": "2022", "title":"Post-quantum Hash Functions Using _n(_p)", "abstract": "<p>We define new families of Tillich-Z'emor hash functions, using higher\ndimensional special linear groups over finite fields as platforms. The Cayley\ngraphs of these groups combine fast mixing properties and high girth, which\ntogether give rise to good preimage and collision resistance of the\ncorresponding hash functions. We justify the claim that the resulting hash\nfunctions are post-quantum secure.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "cui2020exchnet", "year": "2020", "title":"Exchnet A Unified Hashing Network For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Retrieving content relevant images from a large-scale fine-grained dataset\ncould suffer from intolerably slow query speed and highly redundant storage\ncost, due to high-dimensional real-valued embeddings which aim to distinguish\nsubtle visual differences of fine-grained objects. In this paper, we study the\nnovel fine-grained hashing topic to generate compact binary codes for\nfine-grained images, leveraging the search and storage efficiency of hash\nlearning to alleviate the aforementioned problems. Specifically, we propose a\nunified end-to-end trainable network, termed as ExchNet. Based on attention\nmechanisms and proposed attention constraints, it can firstly obtain both local\nand global features to represent object parts and whole fine-grained objects,\nrespectively. Furthermore, to ensure the discriminative ability and semantic\nmeaning’s consistency of these part-level features across images, we design a\nlocal feature alignment approach by performing a feature exchanging operation.\nLater, an alternative learning algorithm is employed to optimize the whole\nExchNet and then generate the final binary hash codes. Validated by extensive\nexperiments, our proposal consistently outperforms state-of-the-art generic\nhashing methods on five fine-grained datasets, which shows our effectiveness.\nMoreover, compared with other approximate nearest neighbor methods, ExchNet\nachieves the best speed-up and storage reduction, revealing its efficiency and\npracticality.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "cui2024exchnet", "year": "2024", "title":"Exchnet A Unified Hashing Network For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Retrieving content relevant images from a large-scale fine grained dataset could suffer from intolerably slow query speed and highly\nredundant storage cost, due to high-dimensional real-valued embeddings\nwhich aim to distinguish subtle visual differences of fine-grained objects.\nIn this paper, we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images, leveraging the search\nand storage efficiency of hash learning to alleviate the aforementioned\nproblems. Specifically, we propose a unified end-to-end trainable network,\ntermed as ExchNet. Based on attention mechanisms and proposed attention constraints, it can firstly obtain both local and global features\nto represent object parts and whole fine-grained objects, respectively.\nFurthermore, to ensure the discriminative ability and semantic meaning’s\nconsistency of these part-level features across images, we design a local\nfeature alignment approach by performing a feature exchanging operation. Later, an alternative learning algorithm is employed to optimize\nthe whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments, our proposal consistently outperforms\nstate-of-the-art generic hashing methods on five fine-grained datasets,\nwhich shows our effectiveness. Moreover, compared with other approximate nearest neighbor methods, ExchNet achieves the best speed-up and\nstorage reduction, revealing its efficiency and practicality.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "curtin2012fast", "year": "2012", "title":"Fast Exact Max-kernel Search", "abstract": "<p>The wide applicability of kernels makes the problem of max-kernel search\nubiquitous and more general than the usual similarity search in metric spaces.\nWe focus on solving this problem efficiently. We begin by characterizing the\ninherent hardness of the max-kernel search problem with a novel notion of\ndirectional concentration. Following that, we present a method to use an \\(O(n\nlog n)\\) algorithm to index any set of objects (points in \\(\\Real^\\dims\\) or\nabstract objects) directly in the Hilbert space without any explicit feature\nrepresentations of the objects in this space. We present the first provably\n\\(O(log n)\\) algorithm for exact max-kernel search using this index. Empirical\nresults for a variety of data sets as well as abstract objects demonstrate up\nto 4 orders of magnitude speedup in some cases. Extensions for approximate\nmax-kernel search are also presented.</p>\n", "tags": ["ARXIV"] },
{"key": "curtin2016fast", "year": "2016", "title":"Fast Approximate Furthest Neighbors With Data-dependent Hashing", "abstract": "<p>We present a novel hashing strategy for approximate furthest neighbor search\nthat selects projection bases using the data distribution. This strategy leads\nto an algorithm, which we call DrusillaHash, that is able to outperform\nexisting approximate furthest neighbor strategies. Our strategy is motivated by\nan empirical study of the behavior of the furthest neighbor search problem,\nwhich lends intuition for where our algorithm is most useful. We also present a\nvariant of the algorithm that gives an absolute approximation guarantee; to our\nknowledge, this is the first such approximate furthest neighbor hashing\napproach to give such a guarantee. Performance studies indicate that\nDrusillaHash can achieve comparable levels of approximation to other algorithms\nwhile giving up to an order of magnitude speedup. An implementation is\navailable in the mlpack machine learning library (found at\nhttp://www.mlpack.org).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "curtó2017segmentation", "year": "2017", "title":"Segmentation Of Objects By Hashing", "abstract": "<p>We propose a novel approach to address the problem of Simultaneous Detection\nand Segmentation introduced in [Hariharan et al 2014]. Using the hierarchical\nstructures first presented in [Arbel'aez et al 2011] we use an efficient and\naccurate procedure that exploits the feature information of the hierarchy using\nLocality Sensitive Hashing. We build on recent work that utilizes convolutional\nneural networks to detect bounding boxes in an image [Ren et al 2015] and then\nuse the top similar hierarchical region that best fits each bounding box after\nhashing, we call this approach C&amp;Z Segmentation. We then refine our final\nsegmentation results by automatic hierarchical pruning. C&amp;Z Segmentation\nintroduces a train-free alternative to Hypercolumns [Hariharan et al 2015]. We\nconduct extensive experiments on PASCAL VOC 2012 segmentation dataset, showing\nthat C&amp;Z gives competitive state-of-the-art segmentations of objects.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "dadaneh2020pairwise", "year": "2020", "title":"Pairwise Supervised Hashing With Bernoulli Variational Auto-encoder And Self-control Gradient Estimator", "abstract": "<p>Semantic hashing has become a crucial component of fast similarity search in\nmany large-scale information retrieval systems, in particular, for text data.\nVariational auto-encoders (VAEs) with binary latent variables as hashing codes\nprovide state-of-the-art performance in terms of precision for document\nretrieval. We propose a pairwise loss function with discrete latent VAE to\nreward within-class similarity and between-class dissimilarity for supervised\nhashing. Instead of solving the optimization relying on existing biased\ngradient estimators, an unbiased low-variance gradient estimator is adopted to\noptimize the hashing function by evaluating the non-differentiable loss\nfunction over two correlated sets of binary hashing codes to control the\nvariance of gradient estimates. This new semantic hashing framework achieves\nsuperior performance compared to the state-of-the-arts, as demonstrated by our\ncomprehensive experiments.</p>\n", "tags": ["Supervised"] },
{"key": "dahlgaard2014approximately", "year": "2014", "title":"Approximately Minwise Independence With Twisted Tabulation", "abstract": "<p>A random hash function \\(h\\) is \\(\\epsilon\\)-minwise if for any set \\(S\\),\n\\(|S|=n\\), and element \\(x\\in S\\), \\(\\Pr[h(x)=\\min h(S)]=(1\\pm\\epsilon)/n\\).\nMinwise hash functions with low bias \\(\\epsilon\\) have widespread applications\nwithin similarity estimation.\n  Hashing from a universe \\([u]\\), the twisted tabulation hashing of\nP\\v{a}tra\\c{s}cu and Thorup [SODA’13] makes \\(c=O(1)\\) lookups in tables of size\n\\(u^{1/c}\\). Twisted tabulation was invented to get good concentration for\nhashing based sampling. Here we show that twisted tabulation yields \\(\\tilde\nO(1/u^{1/c})\\)-minwise hashing.\n  In the classic independence paradigm of Wegman and Carter [FOCS’79] \\(\\tilde\nO(1/u^{1/c})\\)-minwise hashing requires \\(Ω(log u)\\)-independence [Indyk\nSODA’99]. P\\v{a}tra\\c{s}cu and Thorup [STOC’11] had shown that simple\ntabulation, using same space and lookups yields \\(\\tilde O(1/n^{1/c})\\)-minwise\nindependence, which is good for large sets, but useless for small sets. Our\nanalysis uses some of the same methods, but is much cleaner bypassing a\ncomplicated induction argument.</p>\n", "tags": ["ARXIV","FOCS","Independent"] },
{"key": "dahlgaard2014hashing", "year": "2014", "title":"Hashing For Statistics Over K-partitions", "abstract": "<p>In this paper we analyze a hash function for \\(k\\)-partitioning a set into\nbins, obtaining strong concentration bounds for standard algorithms combining\nstatistics from each bin.\n  This generic method was originally introduced by Flajolet and\nMartin~[FOCS’83] in order to save a factor \\(Ω(k)\\) of time per element over\n\\(k\\) independent samples when estimating the number of distinct elements in a\ndata stream. It was also used in the widely used HyperLogLog algorithm of\nFlajolet et al.~[AOFA’97] and in large-scale machine learning by Li et\nal.~[NIPS’12] for minwise estimation of set similarity.\n  The main issue of \\(k\\)-partition, is that the contents of different bins may\nbe highly correlated when using popular hash functions. This means that methods\nof analyzing the marginal distribution for a single bin do not apply. Here we\nshow that a tabulation based hash function, mixed tabulation, does yield strong\nconcentration bounds on the most popular applications of \\(k\\)-partitioning\nsimilar to those we would get using a truly random hash function. The analysis\nis very involved and implies several new results of independent interest for\nboth simple and double tabulation, e.g. a simple and efficient construction for\ninvertible bloom filters and uniform hashing on a given set.</p>\n", "tags": ["ARXIV","FOCS","Independent"] },
{"key": "dahlgaard2017fast", "year": "2017", "title":"Fast Similarity Sketching", "abstract": "<p>We consider the \\(\\textit{Similarity Sketching}\\) problem: Given a universe\n\\([u] = \\{0,\\ldots, u-1\\}\\) we want a random function \\(S\\) mapping subsets\n\\(A\\subseteq [u]\\) into vectors \\(S(A)\\) of size \\(t\\), such that the Jaccard\nsimilarity \\(J(A,B) = |A\\cap B|/|A\\cup B|\\) between sets \\(A\\) and \\(B\\) is\npreserved. More precisely, define \\(X_i = [S(A)[i] =\n  S(B)[i]]\\) and \\(X = \\sum_{i\\in [t]} X_i\\). We want \\(E[X_i]=J(A,B)\\), and we want\n\\(X\\) to be strongly concentrated around \\(E[X] = t \\cdot J(A,B)\\) (i.e.\nChernoff-style bounds). This is a fundamental problem which has found numerous\napplications in data mining, large-scale classification, computer vision,\nsimilarity search, etc. via the classic MinHash algorithm. The vectors \\(S(A)\\)\nare also called \\(\\textit{sketches}\\). Strong concentration is critical, for\noften we want to sketch many sets \\(B_1,\\ldots,B_n\\) so that we later, for a\nquery set \\(A\\), can find (one of) the most similar \\(B_i\\). It is then critical\nthat no \\(B_i\\) looks much more similar to \\(A\\) due to errors in the sketch.\n  The seminal \\(t\\times\\textit{MinHash}\\) algorithm uses \\(t\\) random hash\nfunctions \\(h_1,\\ldots, h_t\\), and stores \\(\\left ( \\min_{a\\in A} h_1(A),\\ldots,\n\\min_{a\\in A} h_t(A) \\right )\\) as the sketch of \\(A\\). The main drawback of\nMinHash is, however, its \\(O(t\\cdot |A|)\\) running time, and finding a sketch\nwith similar properties and faster running time has been the subject of several\npapers. (continued…)</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "dahlgaard2017practical", "year": "2017", "title":"Practical Hash Functions For Similarity Estimation And Dimensionality Reduction", "abstract": "<p>Hashing is a basic tool for dimensionality reduction employed in several aspects of machine learning. However, the perfomance analysis is often carried out under the abstract assumption that a truly random unit cost hash function is used, without concern for which concrete hash function is employed. The concrete hash function may work fine on sufficiently random input. The question is if it can be trusted in the real world when faced with more structured input.  In this paper we focus on two prominent applications of hashing, namely similarity estimation with the one permutation hashing (OPH) scheme of Li et al. [NIPS’12] and feature hashing (FH) of Weinberger et al. [ICML’09], both of which have found numerous applications, i.e. in approximate near-neighbour search with LSH and large-scale classification with SVM.      We consider the recent mixed tabulation hash function of Dahlgaard et al. [FOCS’15] which was proved theoretically to perform like a truly random hash function in many applications, including the above OPH. Here we first show improved concentration bounds for FH with truly random hashing and then argue that mixed tabulation performs similar when the input vectors are sparse. Our main contribution, however, is an experimental comparison of different hashing schemes when used inside FH, OPH, and LSH.  We find that mixed tabulation hashing is almost as fast as the classic multiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work well on sufficiently random data, but we demonstrate that in the above applications, it can lead to bias and poor concentration on both real-world and synthetic data. We also compare with the very popular MurmurHash3, which has no proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to truly random hashing in our experiments. However, mixed tabulation was 40% faster than MurmurHash3, and it has the proven guarantee of good performance on all possible input making it more reliable.</p>\n", "tags": ["FOCS","ICML","LSH","NEURIPS","Unsupervised"] },
{"key": "dai2017stochastic", "year": "2017", "title":"Stochastic Generative Hashing", "abstract": "<p>Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "dai2020convolutional", "year": "2020", "title":"Convolutional Embedding For Edit Distance", "abstract": "<p>Edit-distance-based string similarity search has many applications such as\nspell correction, data de-duplication, and sequence alignment. However,\ncomputing edit distance is known to have high complexity, which makes string\nsimilarity search challenging for large datasets. In this paper, we propose a\ndeep learning pipeline (called CNN-ED) that embeds edit distance into Euclidean\ndistance for fast approximate similarity search. A convolutional neural network\n(CNN) is used to generate fixed-length vector embeddings for a dataset of\nstrings and the loss function is a combination of the triplet loss and the\napproximation error. To justify our choice of using CNN instead of other\nstructures (e.g., RNN) as the model, theoretical analysis is conducted to show\nthat some basic operations in our CNN model preserve edit distance.\nExperimental results show that CNN-ED outperforms data-independent CGK\nembedding and RNN-based GRU embedding in terms of both accuracy and efficiency\nby a large margin. We also show that string similarity search can be\nsignificantly accelerated using CNN-based embeddings, sometimes by orders of\nmagnitude.</p>\n", "tags": ["ARXIV","CNN","Deep Learning","Supervised"] },
{"key": "dalal2023two", "year": "2023", "title":"Two-way Linear Probing Revisited", "abstract": "<p>We introduce linear probing hashing schemes that construct a hash table of\nsize \\(n\\), with constant load factor \\(\\alpha\\), on which the worst-case\nunsuccessful search time is asymptotically almost surely \\(O(log log n)\\). The\nschemes employ two linear probe sequences to find empty cells for the keys.\nMatching lower bounds on the maximum cluster size produced by any algorithm\nthat uses two linear probe sequences are obtained as well.</p>\n", "tags": ["ARXIV"] },
{"key": "daras2020smyrf", "year": "2020", "title":"SMYRF - Efficient Attention Using Asymmetric Clustering", "abstract": "<p>We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from \\(O(N^2)\\) to \\(O(N log N)\\), where N is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. tight queries and keys) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using 50% less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions.  Using a single TPU, we train BigGAN on Celeba-HQ, with attention at resolution 128x128 and 256x256, capable of generating realistic human faces.</p>\n", "tags": ["LSH","NEURIPS","Unsupervised"] },
{"key": "dasgupta2010sparse", "year": "2010", "title":"A Sparse Johnson--lindenstrauss Transform", "abstract": "<p>Dimension reduction is a key algorithmic tool with many applications\nincluding nearest-neighbor search, compressed sensing and linear algebra in the\nstreaming model. In this work we obtain a {\\em sparse} version of the\nfundamental tool in dimension reduction — the Johnson–Lindenstrauss\ntransform. Using hashing and local densification, we construct a sparse\nprojection matrix with just \\(\\tilde{O}(\\frac{1}{\\epsilon})\\) non-zero entries\nper column. We also show a matching lower bound on the sparsity for a large\nclass of projection matrices. Our bounds are somewhat surprising, given the\nknown lower bounds of \\(Ω(\\frac{1}{\\epsilon^2})\\) both on the number of rows\nof any projection matrix and on the sparsity of projection matrices generated\nby natural constructions.\n  Using this, we achieve an \\(\\tilde{O}(\\frac{1}{\\epsilon})\\) update time per\nnon-zero element for a \\((1\\pm\\epsilon)\\)-approximate projection, thereby\nsubstantially outperforming the \\(\\tilde{O}(\\frac{1}{\\epsilon^2})\\) update time\nrequired by prior approaches. A variant of our method offers the same\nguarantees for sparse vectors, yet its \\(\\tilde{O}(d)\\) worst case running time\nmatches the best approach of Ailon and Liberty.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "dasgupta2023review", "year": "2023", "title":"Review Of Extreme Multilabel Classification", "abstract": "<p>Extreme multilabel classification or XML, is an active area of interest in\nmachine learning. Compared to traditional multilabel classification, here the\nnumber of labels is extremely large, hence, the name extreme multilabel\nclassification. Using classical one versus all classification wont scale in\nthis case due to large number of labels, same is true for any other\nclassifiers. Embedding of labels as well as features into smaller label space\nis an essential first step. Moreover, other issues include existence of head\nand tail labels, where tail labels are labels which exist in relatively smaller\nnumber of given samples. The existence of tail labels creates issues during\nembedding. This area has invited application of wide range of approaches\nranging from bit compression motivated from compressed sensing, tree based\nembeddings, deep learning based latent space embedding including using\nattention weights, linear algebra based embeddings such as SVD, clustering,\nhashing, to name a few. The community has come up with a useful set of metrics\nto identify correctly the prediction for head or tail labels.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "datar2024locality", "year": "2024", "title":"Locality-sensitive Hashing Scheme Based On P-stable Distributions", "abstract": "<p>We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p&lt;1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain “bounded growth” condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "davoodi2019forestdsh", "year": "2019", "title":"Forestdsh A Universal Hash Design For Discrete Probability Distributions", "abstract": "<p>In this paper, we consider the problem of classification of \\(M\\) high\ndimensional queries \\(y^1,\\cdots,y^M\\in B^S\\) to \\(N\\) high dimensional classes\n\\(x^1,\\cdots,x^N\\in A^S\\) where \\(A\\) and \\(B\\) are discrete alphabets and the\nprobabilistic model that relates data to the classes \\(P(x,y)\\) is known. This\nproblem has applications in various fields including the database search\nproblem in mass spectrometry. The problem is analogous to the nearest neighbor\nsearch problem, where the goal is to find the data point in a database that is\nthe most similar to a query point. The state of the art method for solving an\napproximate version of the nearest neighbor search problem in high dimensions\nis locality sensitive hashing (LSH). LSH is based on designing hash functions\nthat map near points to the same buckets with a probability higher than random\n(far) points. To solve our high dimensional classification problem, we\nintroduce distribution sensitive hashes that map jointly generated pairs\n\\((x,y)\\sim P\\) to the same bucket with probability higher than random pairs\n\\(x\\sim P^A\\) and \\(y\\sim P^B\\), where \\(P^A\\) and \\(P^B\\) are the marginal probability\ndistributions of \\(P\\). We design distribution sensitive hashes using a forest of\ndecision trees and we show that the complexity of search grows with\n\\(O(N^{\\lambda^<em>(P)})\\) where \\(\\lambda^</em>(P)\\) is expressed in an analytical form.\nWe further show that the proposed hashes perform faster than state of the art\napproximate nearest neighbor search methods for a range of probability\ndistributions, in both theory and simulations. Finally, we apply our method to\nthe spectral library search problem in mass spectrometry, and show that it is\nan order of magnitude faster than the state of the art methods.</p>\n", "tags": ["LSH","Supervised"] },
{"key": "defranca2014iterative", "year": "2014", "title":"Iterative Universal Hash Function Generator For Minhashing", "abstract": "<p>Minhashing is a technique used to estimate the Jaccard Index between two sets\nby exploiting the probability of collision in a random permutation. In order to\nspeed up the computation, a random permutation can be approximated by using an\nuniversal hash function such as the \\(h_{a,b}\\) function proposed by Carter and\nWegman. A better estimate of the Jaccard Index can be achieved by using many of\nthese hash functions, created at random. In this paper a new iterative\nprocedure to generate a set of \\(h_{a,b}\\) functions is devised that eliminates\nthe need for a list of random values and avoid the multiplication operation\nduring the calculation. The properties of the generated hash functions remains\nthat of an universal hash function family. This is possible due to the random\nnature of features occurrence on sparse datasets. Results show that the\nuniformity of hashing the features is maintaned while obtaining a speed up of\nup to \\(1.38\\) compared to the traditional approach.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "defrança2014hash", "year": "2014", "title":"A Hash-based Co-clustering Algorithm For Categorical Data", "abstract": "<p>Many real-life data are described by categorical attributes without a\npre-classification. A common data mining method used to extract information\nfrom this type of data is clustering. This method group together the samples\nfrom the data that are more similar than all other samples. But, categorical\ndata pose a challenge when extracting information because: the calculation of\ntwo objects similarity is usually done by measuring the number of common\nfeatures, but ignore a possible importance weighting; if the data may be\ndivided differently according to different subsets of the features, the\nalgorithm may find clusters with different meanings from each other,\ndifficulting the post analysis. Data Co-Clustering of categorical data is the\ntechnique that tries to find subsets of samples that share a subset of features\nin common. By doing so, not only a sample may belong to more than one cluster\nbut, the feature selection of each cluster describe its own characteristics. In\nthis paper a novel Co-Clustering technique for categorical data is proposed by\nusing Locality Sensitive Hashing technique in order to preprocess a list of\nCo-Clusters seeds based on a previous research. Results indicate this technique\nis capable of finding high quality Co-Clusters in many different categorical\ndata sets and scales linearly with the data set size.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "deherve2014perceptual", "year": "2014", "title":"A Perceptual Hash Function To Store And Retrieve Large Scale DNA Sequences", "abstract": "<p>This paper proposes a novel approach for storing and retrieving massive DNA\nsequences.. The method is based on a perceptual hash function, commonly used to\ndetermine the similarity between digital images, that we adapted for DNA\nsequences. Perceptual hash function presented here is based on a Discrete\nCosine Transform Sign Only (DCT-SO). Each nucleotide is encoded as a fixed gray\nlevel intensity pixel and the hash is calculated from its significant frequency\ncharacteristics. This results to a drastic data reduction between the sequence\nand the perceptual hash. Unlike cryptographic hash functions, perceptual hashes\nare not affected by “avalanche effect” and thus can be compared. The similarity\ndistance between two hashes is estimated with the Hamming Distance, which is\nused to retrieve DNA sequences. Experiments that we conducted show that our\napproach is relevant for storing massive DNA sequences, and retrieving them.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "dellafiore2020improved", "year": "2020", "title":"Improved Bounds For (bk)-hashing", "abstract": "<p>For fixed integers \\(b\\geq k\\), a problem of relevant interest in computer\nscience and combinatorics is that of determining the asymptotic growth, with\n\\(n\\), of the largest set for which a \\((b, k)\\)-hash family of \\(n\\) functions\nexists. Equivalently, determining the asymptotic growth of a largest subset of\n\\(\\{1,2,\\ldots,b\\}^n\\) such that, for any \\(k\\) distinct elements in the set, there\nis a coordinate where they all differ.\n  An important asymptotic upper bound for general \\(b, k\\), was derived by\nFredman and Koml'os in the ’80s and improved for certain \\(b\\neq k\\) by K\"orner\nand Marton and by Arikan. Only very recently better bounds were derived for the\ngeneral \\(b,k\\) case by Guruswami and Riazanov while stronger results for small\nvalues of \\(b=k\\) were obtained by Arikan, by Dalai, Guruswami and Radhakrishnan\nand by Costa and Dalai. In this paper, we both show how some of the latter\nresults extend to \\(b\\neq k\\) and further strengthen the bounds for some specific\nsmall values of \\(b\\) and \\(k\\). The method we use, which depends on the reduction\nof an optimization problem to a finite number of cases, shows that further\nresults might be obtained by refined arguments at the expense of higher\ncomplexity which could be reduced by using more sophisticated and optimized\nalgorithmic approaches.</p>\n", "tags": ["ARXIV"] },
{"key": "demaine2005de", "year": "2005", "title":"De Dictionariis Dynamicis Pauco Spatio Utentibus", "abstract": "<p>We develop dynamic dictionaries on the word RAM that use asymptotically\noptimal space, up to constant factors, subject to insertions and deletions, and\nsubject to supporting perfect-hashing queries and/or membership queries, each\noperation in constant time with high probability. When supporting only\nmembership queries, we attain the optimal space bound of Theta(n lg(u/n)) bits,\nwhere n and u are the sizes of the dictionary and the universe, respectively.\nPrevious dictionaries either did not achieve this space bound or had time\nbounds that were only expected and amortized. When supporting perfect-hashing\nqueries, the optimal space bound depends on the range {1,2,…,n+t} of\nhashcodes allowed as output. We prove that the optimal space bound is Theta(n\nlglg(u/n) + n lg(n/(t+1))) bits when supporting only perfect-hashing queries,\nand it is Theta(n lg(u/n) + n lg(n/(t+1))) bits when also supporting membership\nqueries. All upper bounds are new, as is the Omega(n lg(n/(t+1))) lower bound.</p>\n", "tags": ["ARXIV"] },
{"key": "deng2019triplet", "year": "2019", "title":"Triplet-based Deep Hashing Network For Cross-modal Retrieval", "abstract": "<p>Given the benefits of its low storage requirements and high retrieval\nefficiency, hashing has recently received increasing attention. In\nparticular,cross-modal hashing has been widely and successfully used in\nmultimedia similarity search applications. However, almost all existing methods\nemploying cross-modal hashing cannot obtain powerful hash codes due to their\nignoring the relative similarity between heterogeneous data that contains\nricher semantic information, leading to unsatisfactory retrieval performance.\nIn this paper, we propose a triplet-based deep hashing (TDH) network for\ncross-modal retrieval. First, we utilize the triplet labels, which describes\nthe relative relationships among three instances as supervision in order to\ncapture more general semantic correlations between cross-modal instances. We\nthen establish a loss function from the inter-modal view and the intra-modal\nview to boost the discriminative abilities of the hash codes. Finally, graph\nregularization is introduced into our proposed TDH method to preserve the\noriginal semantic similarity between hash codes in Hamming space. Experimental\nresults show that our proposed method outperforms several state-of-the-art\napproaches on two popular cross-modal datasets.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Independent"] },
{"key": "deng2024imagenet", "year": "2024", "title":"Imagenet A Large-scale Hierarchical Image Database", "abstract": "<p>The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "deng2024two", "year": "2024", "title":"Two-stream Deep Hashing With Class-specific Centers For Supervised Image Search", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. Recent supervised hashing research has shown that deep learning-based methods can significantly outperform nondeep methods. Most existing supervised deep hashing methods exploit supervisory signals to generate similar and dissimilar image pairs for training. However, natural images can have large intraclass and small interclass variations, which may degrade the accuracy of hash codes. To address this problem, we propose a novel two-stream ConvNet architecture, which learns hash codes with class-specific representation centers. Our basic idea is that if we can learn a unified binary representation for each class as a center and encourage hash codes of images to be close to the corresponding centers, the intraclass variation will be greatly reduced. Accordingly, we design a neural network that leverages label information and outputs a unified binary representation for each class. Moreover, we also design an image network to learn hash codes from images and force these hash codes to be close to the corresponding class-specific centers. These two neural networks are then seamlessly incorporated to create a unified, end-to-end trainable framework. Extensive experiments on three popular benchmarks corroborate that our proposed method outperforms current state-of-the-art methods.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "depalma2017distributed", "year": "2017", "title":"Distributed Stratified Locality Sensitive Hashing For Critical Event Prediction In The Cloud", "abstract": "<p>The availability of massive healthcare data repositories calls for efficient\ntools for data-driven medicine. We introduce a distributed system for\nStratified Locality Sensitive Hashing to perform fast similarity-based\nprediction on large medical waveform datasets. Our implementation, for an ICU\nuse case, prioritizes latency over throughput and is targeted at a cloud\nenvironment. We demonstrate our system on Acute Hypotensive Episode prediction\nfrom Arterial Blood Pressure waveforms. On a dataset of \\(1.37\\) million points,\nwe show scaling up to \\(40\\) processors and a \\(21\\times\\) speedup in number of\ncomparisons to parallel exhaustive search at the price of a \\(10\\%\\) Matthews\ncorrelation coefficient (MCC) loss. Furthermore, if additional MCC loss can be\ntolerated, our system achieves speedups up to two orders of magnitude.</p>\n", "tags": ["ARXIV"] },
{"key": "dereich2004probabilities", "year": "2004", "title":"Probabilities Of Randomly Centered Small Balls And Quantization In Banach Spaces", "abstract": "<p>We investigate the Gaussian small ball probabilities with random centers,\nfind their deterministic a.s.-equivalents and establish a relation to\ninfinite-dimensional high-resolution quantization.</p>\n", "tags": ["Independent","Quantisation"] },
{"key": "desai2021semantically", "year": "2021", "title":"Semantically Constrained Memory Allocation (SCMA) For Embedding In Efficient Recommendation Systems", "abstract": "<p>Deep learning-based models are utilized to achieve state-of-the-art\nperformance for recommendation systems. A key challenge for these models is to\nwork with millions of categorical classes or tokens. The standard approach is\nto learn end-to-end, dense latent representations or embeddings for each token.\nThe resulting embeddings require large amounts of memory that blow up with the\nnumber of tokens. Training and inference with these models create storage, and\nmemory bandwidth bottlenecks leading to significant computing and energy\nconsumption when deployed in practice. To this end, we present the problem of\n\\textit{Memory Allocation} under budget for embeddings and propose a novel\nformulation of memory shared embedding, where memory is shared in proportion to\nthe overlap in semantic information. Our formulation admits a practical and\nefficient randomized solution with Locality sensitive hashing based Memory\nAllocation (LMA). We demonstrate a significant reduction in the memory\nfootprint while maintaining performance. In particular, our LMA embeddings\nachieve the same performance compared to standard embeddings with a 16\\(\\times\\)\nreduction in memory footprint. Moreover, LMA achieves an average improvement of\nover 0.003 AUC across different memory regimes than standard DLRM models on\nCriteo and Avazu datasets</p>\n", "tags": ["ARXIV","Deep Learning","Independent"] },
{"key": "desai2024identity", "year": "2024", "title":"Identity With Locality An Ideal Hash For Gene Sequence Search", "abstract": "<p>Gene sequence search is a fundamental operation in computational genomics.\nDue to the petabyte scale of genome archives, most gene search systems now use\nhashing-based data structures such as Bloom Filters (BF). The state-of-the-art\nsystems such as Compact bit-slicing signature index (COBS) and Repeated And\nMerged Bloom filters (RAMBO) use BF with Random Hash (RH) functions for gene\nrepresentation and identification. The standard recipe is to cast the gene\nsearch problem as a sequence of membership problems testing if each subsequent\ngene substring (called kmer) of Q is present in the set of kmers of the entire\ngene database D. We observe that RH functions, which are crucial to the memory\nand the computational advantage of BF, are also detrimental to the system\nperformance of gene-search systems. While subsequent kmers being queried are\nlikely very similar, RH, oblivious to any similarity, uniformly distributes the\nkmers to different parts of potentially large BF, thus triggering excessive\ncache misses and causing system slowdown. We propose a novel hash function\ncalled the Identity with Locality (IDL) hash family, which co-locates the keys\nclose in input space without causing collisions. This approach ensures both\ncache locality and key preservation. IDL functions can be a drop-in replacement\nfor RH functions and help improve the performance of information retrieval\nsystems. We give a simple but practical construction of IDL function families\nand show that replacing the RH with IDL functions reduces cache misses by a\nfactor of 5x, thus improving query and indexing times of SOTA methods such as\nCOBS and RAMBO by factors up to 2x without compromising their quality. We also\nprovide a theoretical analysis of the false positive rate of BF with IDL\nfunctions. Our hash function is the first study that bridges Locality Sensitive\nHash (LSH) and RH to obtain cache efficiency.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "determe2020mac", "year": "2020", "title":"MAC Address Anonymization For Crowd Counting", "abstract": "<p>Research has shown that counting WiFi packets called probe requests (PRs)\nimplicitly provides a proxy for the number of people in an area. In this paper,\nwe discuss a crowd counting system involving WiFi sensors detecting PRs over\nthe air, then extracting and anonymizing their media access control (MAC)\naddresses using a hash-based approach. This paper discusses an anonymization\nprocedure and shows time-synchronization inaccuracies among sensors and hashing\ncollision rates to be low enough to prevent anonymization from interfering with\ncounting algorithms. In particular, we derive an approximation of the collision\nrate of uniformly distributed identifiers, with analytical error bounds.</p>\n", "tags": [] },
{"key": "dev2015improving", "year": "2015", "title":"Improving Style Similarity Metrics Of 3D Shapes", "abstract": "<p>The idea of style similarity metrics has been recently developed for various\nmedia types such as 2D clip art and 3D shapes. We explore this style metric\nproblem and improve existing style similarity metrics of 3D shapes in four\nnovel ways. First, we consider the color and texture of 3D shapes which are\nimportant properties that have not been previously considered. Second, we\nexplore the effect of clustering a dataset of 3D models by comparing between\nstyle metrics for a single object type and style metrics that combine clusters\nof object types. Third, we explore the idea of user-guided learning for this\nproblem. Fourth, we introduce an iterative approach that can learn a metric\nfrom a general set of 3D models. We demonstrate these contributions with\nvarious classes of 3D shapes and with applications such as style-based\nsimilarity search and scene composition.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "dewan2021scaling", "year": "2021", "title":"Scaling Shared-memory Data Structures As Distributed Global-view Data Structures In The Partitioned Global Address Space Model", "abstract": "<p>The Partitioned Global Address Space (PGAS), a memory model in which the\nglobal address space is explicitly partitioned across compute nodes in a\ncluster, strives to bridge the gap between shared-memory and distributed-memory\nprogramming. To further bridge this gap, there has been an adoption of\nglobal-view distributed data structures, such as ‘global arrays’ or\n‘distributed arrays’. This work demonstrates how shared-memory data structures\ncan be modified to scale in distributed memory. Presented in this work is the\nDistributed Interlocked Hash Table (DIHT), a global-view distributed map data\nstructure inpired by the Interlocked Hash Table (IHT). At 64 nodes with 44\ncores per node, DIHT provides upto 110x the performance of the Chapel\nstandard-library HashedDist.</p>\n", "tags": ["ARXIV"] },
{"key": "dey2009hf", "year": "2009", "title":"Hf-hash Hash Functions Using Restricted HFE Challenge-1", "abstract": "<p>Vulnerability of dedicated hash functions to various attacks has made the\ntask of designing hash function much more challenging. This provides us a\nstrong motivation to design a new cryptographic hash function viz. HF-hash.\nThis is a hash function, whose compression function is designed by using first\n32 polynomials of HFE Challenge-1 with 64 variables by forcing remaining 16\nvariables as zero. HF-hash gives 256 bits message digest and is as efficient as\nSHA-256. It is secure against the differential attack proposed by Chabaud and\nJoux as well as by Wang et. al. applied to SHA-0 and SHA-1.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "dey2010gb", "year": "2010", "title":"Gb-hash Hash Functions Using Groebner Basis", "abstract": "<p>In this paper we present an improved version of HF-hash, viz., GB-hash : Hash\nFunctions Using Groebner Basis. In case of HF-hash, the compression function\nconsists of 32 polynomials with 64 variables which were taken from the first 32\npolynomials of hidden field equations challenge-1 by forcing last 16 variables\nas 0. In GB-hash we have designed the compression function in such way that\nthese 32 polynomials with 64 variables form a minimal Groebner basis of the\nideal generated by them with respect to graded lexicographical (grlex) ordering\nas well as with respect to graded reverse lexicographical (grevlex) ordering.\nIn this paper we will prove that GB-hash is more secure than HF-hash as well as\nmore secure than SHA-256. We have also compared the efficiency of our GB-hash\nwith SHA-256 and HF-hash.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "dhar2022linear", "year": "2022", "title":"Linear Hashing With ell_infty Guarantees And Two-sided Kakeya Bounds", "abstract": "<p>We show that a randomly chosen linear map over a finite field gives a good\nhash function in the \\(\\ell_\\infty\\) sense. More concretely, consider a set \\(S\n\\subset \\mathbb{F}<em>q^n\\) and a randomly chosen linear map \\(L : \\mathbb{F}_q^n\n\\to \\mathbb{F}_q^t\\) with \\(q^t\\) taken to be sufficiently smaller than \\( |S|\\).\nLet \\(U_S\\) denote a random variable distributed uniformly on \\(S\\). Our main\ntheorem shows that, with high probability over the choice of \\(L\\), the random\nvariable \\(L(U_S)\\) is close to uniform in the \\(\\ell</em>\\infty\\) norm. In other\nwords, {\\em every} element in the range \\(\\mathbb{F}_q^t\\) has about the same\nnumber of elements in \\(S\\) mapped to it. This complements the widely-used\nLeftover Hash Lemma (LHL) which proves the analog statement under the\nstatistical, or \\(\\ell_1\\), distance (for a richer class of functions) as well as\nprior work on the expected largest ‘bucket size’ in linear hash functions\n[ADMPT99]. By known bounds from the load balancing literature [RS98], our\nresults are tight and show that linear functions hash as well as trully random\nfunction up to a constant factor in the entropy loss. Our proof leverages a\nconnection between linear hashing and the finite field Kakeya problem and\nextends some of the tools developed in this area, in particular the polynomial\nmethod.</p>\n", "tags": ["Independent"] },
{"key": "dhulipala2024muvera", "year": "2024", "title":"MUVERA Multi-vector Retrieval Via Fixed Dimensional Encodings", "abstract": "<p>Neural embedding models have become a fundamental component of modern\ninformation retrieval (IR) pipelines. These models produce a single embedding\n\\(x \\in \\mathbb{R}^d\\) per data-point, allowing for fast retrieval via highly\noptimized maximum inner product search (MIPS) algorithms. Recently, beginning\nwith the landmark ColBERT paper, multi-vector models, which produce a set of\nembedding per data point, have achieved markedly superior performance for IR\ntasks. Unfortunately, using these models for IR is computationally expensive\ndue to the increased complexity of multi-vector retrieval and scoring.\n  In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a\nretrieval mechanism which reduces multi-vector similarity search to\nsingle-vector similarity search. This enables the usage of off-the-shelf MIPS\nsolvers for multi-vector retrieval. MUVERA asymmetrically generates Fixed\nDimensional Encodings (FDEs) of queries and documents, which are vectors whose\ninner product approximates multi-vector similarity. We prove that FDEs give\nhigh-quality \\(\\epsilon\\)-approximations, thus providing the first single-vector\nproxy for multi-vector similarity with theoretical guarantees. Empirically, we\nfind that FDEs achieve the same recall as prior state-of-the-art heuristics\nwhile retrieving 2-5\\(\\times\\) fewer candidates. Compared to prior state of the\nart implementations, MUVERA achieves consistently good end-to-end recall and\nlatency across a diverse set of the BEIR retrieval datasets, achieving an\naverage of 10\\(\\%\\) improved recall with \\(90\\%\\) lower latency.</p>\n", "tags": ["ARXIV"] },
{"key": "dias2022pattern", "year": "2022", "title":"Pattern Spotting And Image Retrieval In Historical Documents Using Deep Hashing", "abstract": "<p>This paper presents a deep learning approach for image retrieval and pattern\nspotting in digital collections of historical documents. First, a region\nproposal algorithm detects object candidates in the document page images. Next,\ndeep learning models are used for feature extraction, considering two distinct\nvariants, which provide either real-valued or binary code representations.\nFinally, candidate images are ranked by computing the feature similarity with a\ngiven input query. A robust experimental protocol evaluates the proposed\napproach considering each representation scheme (real-valued and binary code)\non the DocExplore image database. The experimental results show that the\nproposed deep models compare favorably to the state-of-the-art image retrieval\napproaches for images of historical documents, outperforming other deep models\nby 2.56 percentage points using the same techniques for pattern spotting.\nBesides, the proposed approach also reduces the search time by up to 200x and\nthe storage cost up to 6,000x when compared to related works based on\nreal-valued representations.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval"] },
{"key": "dickens2024key", "year": "2024", "title":"Key Compression Limits For k-minimum Value Sketches", "abstract": "<p>The \\(k\\)-Minimum Values (\\kmv) data sketch algorithm stores the \\(k\\) least hash\nkeys generated by hashing the items in a dataset. We show that compression\nbased on ordering the keys and encoding successive differences can offer\n\\(O(log n)\\) bits per key in expected storage savings, where \\(n\\) is the number\nof unique values in the data set. We also show that \\(O(log n)\\) expected bits\nsaved per key is optimal for any form of compression for the \\(k\\) least of \\(n\\)\nrandom values – that the encoding method is near-optimal among all methods to\nencode a \\kmv sketch. We present a practical method to perform that\ncompression, show that it is computationally efficient, and demonstrate that\nits average savings in practice is within about five percent of the theoretical\nminimum based on entropy. We verify that our method outperforms off-the-shelf\ncompression methods, and we demonstrate that it is practical, using real and\nsynthetic data.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ding2018mean", "year": "2018", "title":"Mean Local Group Average Precision (mlgap) A New Performance Metric For Hashing-based Retrieval", "abstract": "<p>The research on hashing techniques for visual data is gaining increased\nattention in recent years due to the need for compact representations\nsupporting efficient search/retrieval in large-scale databases such as online\nimages. Among many possibilities, Mean Average Precision(mAP) has emerged as\nthe dominant performance metric for hashing-based retrieval. One glaring\nshortcoming of mAP is its inability in balancing retrieval accuracy and\nutilization of hash codes: pushing a system to attain higher mAP will\ninevitably lead to poorer utilization of the hash codes. Poor utilization of\nthe hash codes hinders good retrieval because of increased collision of samples\nin the hash space. This means that a model giving a higher mAP values does not\nnecessarily do a better job in retrieval. In this paper, we introduce a new\nmetric named Mean Local Group Average Precision (mLGAP) for better evaluation\nof the performance of hashing-based retrieval. The new metric provides a\nretrieval performance measure that also reconciles the utilization of hash\ncodes, leading to a more practically meaningful performance metric than\nconventional ones like mAP. To this end, we start by mathematical analysis of\nthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and\nshow why it is more appropriate for hashing-based retrieval. Experiments on\nimage retrieval are used to demonstrate the effectiveness of the proposed\nmetric.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "ding2019bilinear", "year": "2019", "title":"Bilinear Supervised Hashing Based On 2D Image Features", "abstract": "<p>Hashing has been recognized as an efficient representation learning method to\neffectively handle big data due to its low computational complexity and memory\ncost. Most of the existing hashing methods focus on learning the\nlow-dimensional vectorized binary features based on the high-dimensional raw\nvectorized features. However, studies on how to obtain preferable binary codes\nfrom the original 2D image features for retrieval is very limited. This paper\nproposes a bilinear supervised discrete hashing (BSDH) method based on 2D image\nfeatures which utilizes bilinear projections to binarize the image matrix\nfeatures such that the intrinsic characteristics in the 2D image space are\npreserved in the learned binary codes. Meanwhile, the bilinear projection\napproximation and vectorization binary codes regression are seamlessly\nintegrated together to formulate the final robust learning framework.\nFurthermore, a discrete optimization strategy is developed to alternatively\nupdate each variable for obtaining the high-quality binary codes. In addition,\ntwo 2D image features, traditional SURF-based FVLAD feature and CNN-based\nAlexConv5 feature are designed for further improving the performance of the\nproposed BSDH method. Results of extensive experiments conducted on four\nbenchmark datasets show that the proposed BSDH method almost outperforms all\ncompeting hashing methods with different input features by different evaluation\nprotocols.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "ding2021dynamic", "year": "2021", "title":"Dynamic Texture Recognition Using PDV Hashing And Dictionary Learning On Multi-scale Volume Local Binary Pattern", "abstract": "<p>Spatial-temporal local binary pattern (STLBP) has been widely used in dynamic\ntexture recognition. STLBP often encounters the high-dimension problem as its\ndimension increases exponentially, so that STLBP could only utilize a small\nneighborhood. To tackle this problem, we propose a method for dynamic texture\nrecognition using PDV hashing and dictionary learning on multi-scale volume\nlocal binary pattern (PHD-MVLBP). Instead of forming very high-dimensional LBP\nhistogram features, it first uses hash functions to map the pixel difference\nvectors (PDVs) to binary vectors, then forms a dictionary using the derived\nbinary vector, and encodes them using the derived dictionary. In such a way,\nthe PDVs are mapped to feature vectors of the size of dictionary, instead of\nLBP histograms of very high dimension. Such an encoding scheme could extract\nthe discriminant information from videos in a much larger neighborhood\neffectively. The experimental results on two widely-used dynamic textures\ndatasets, DynTex++ and UCLA, show the superiority performance of the proposed\napproach over the state-of-the-art methods.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ding2024collective", "year": "2024", "title":"Collective Matrix Factorization Hashing For Multimodal Data", "abstract": "<p>Nearest neighbor search methods based on hashing have\nattracted considerable attention for effective and efficient\nlarge-scale similarity search in computer vision and information\nretrieval community. In this paper, we study the\nproblems of learning hash functions in the context of multimodal\ndata for cross-view similarity search. We put forward\na novel hashing method, which is referred to Collective\nMatrix Factorization Hashing (CMFH). CMFH learns unified\nhash codes by collective matrix factorization with latent\nfactor model from different modalities of one instance,\nwhich can not only supports cross-view search but also increases\nthe search accuracy by merging multiple view information\nsources. We also prove that CMFH, a similaritypreserving\nhashing learning method, has upper and lower\nboundaries. Extensive experiments verify that CMFH significantly\noutperforms several state-of-the-art methods on\nthree different datasets.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "ding2024efficient", "year": "2024", "title":"Efficient Retrieval With Learned Similarities", "abstract": "<p>Retrieval plays a fundamental role in recommendation systems, search, and\nnatural language processing by efficiently finding relevant items from a large\ncorpus given a query. Dot products have been widely used as the similarity\nfunction in such retrieval tasks, thanks to Maximum Inner Product Search (MIPS)\nthat enabled efficient retrieval based on dot products. However,\nstate-of-the-art retrieval algorithms have migrated to learned similarities.\nSuch algorithms vary in form; the queries can be represented with multiple\nembeddings, complex neural networks can be deployed, the item ids can be\ndecoded directly from queries using beam search, and multiple approaches can be\ncombined in hybrid solutions. Unfortunately, we lack efficient solutions for\nretrieval in these state-of-the-art setups. Our work investigates techniques\nfor approximate nearest neighbor search with learned similarity functions. We\nfirst prove that Mixture-of-Logits (MoL) is a universal approximator, and can\nexpress all learned similarity functions. We next propose techniques to\nretrieve the approximate top K results using MoL with a tight bound. We finally\ncompare our techniques with existing approaches, showing that MoL sets new\nstate-of-the-art results on recommendation retrieval tasks, and our approximate\ntop-k retrieval with learned similarities outperforms baselines by up to two\norders of magnitude in latency, while achieving &gt; .99 recall rate of exact\nalgorithms.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "ding2024knn", "year": "2024", "title":"Knn Hashing With Factorized Neighborhood Representation", "abstract": "<p>Hashing is very effective for many tasks in reducing the\nprocessing time and in compressing massive databases. Although lots of approaches have been developed to learn\ndata-dependent hash functions in recent years, how to learn\nhash functions to yield good performance with acceptable\ncomputational and memory cost is still a challenging problem. Based on the observation that retrieval precision is\nhighly related to the kNN classification accuracy, this paper\nproposes a novel kNN-based supervised hashing method,\nwhich learns hash functions by directly maximizing the kNN\naccuracy of the Hamming-embedded training data. To make\nit scalable well to large problem, we propose a factorized\nneighborhood representation to parsimoniously model the\nneighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable,\nwe further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn\naccurate hashing functions with tolerable computation and\nstorage cost. Experiments on four benchmarks demonstrate\nthat our method outperforms the state-of-the-arts.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "dirksen2016fast", "year": "2016", "title":"Fast Binary Embeddings With Gaussian Circulant Matrices Improved Bounds", "abstract": "<p>We consider the problem of encoding a finite set of vectors into a small\nnumber of bits while approximately retaining information on the angular\ndistances between the vectors. By deriving improved variance bounds related to\nbinary Gaussian circulant embeddings, we largely fix a gap in the proof of the\nbest known fast binary embedding method. Our bounds also show that\nwell-spreadness assumptions on the data vectors, which were needed in earlier\nwork on variance bounds, are unnecessary. In addition, we propose a new binary\nembedding with a faster running time on sparse data.</p>\n", "tags": ["ARXIV"] },
{"key": "do2015discrete", "year": "2015", "title":"Discrete Hashing With Deep Neural Network", "abstract": "<p>This paper addresses the problem of learning binary hash codes for large\nscale image search by proposing a novel hashing method based on deep neural\nnetwork. The advantage of our deep model over previous deep model used in\nhashing is that our model contains necessary criteria for producing good codes\nsuch as similarity preserving, balance and independence. Another advantage of\nour method is that instead of relaxing the binary constraint of codes during\nthe learning process as most previous works, in this paper, by introducing the\nauxiliary variable, we reformulate the optimization into two sub-optimization\nsteps allowing us to efficiently solve binary constraints without any\nrelaxation.\n  The proposed method is also extended to the supervised hashing by leveraging\nthe label information such that the learned binary codes preserve the pairwise\nlabel of inputs.\n  The experimental results on three benchmark datasets show the proposed\nmethods outperform state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "do2016binary", "year": "2016", "title":"Binary Hashing With Semidefinite Relaxation And Augmented Lagrangian", "abstract": "<p>This paper proposes two approaches for inferencing binary codes in two-step\n(supervised, unsupervised) hashing. We first introduce an unified formulation\nfor both supervised and unsupervised hashing. Then, we cast the learning of one\nbit as a Binary Quadratic Problem (BQP). We propose two approaches to solve\nBQP. In the first approach, we relax BQP as a semidefinite programming problem\nwhich its global optimum can be achieved. We theoretically prove that the\nobjective value of the binary solution achieved by this approach is well\nbounded. In the second approach, we propose an augmented Lagrangian based\napproach to solve BQP directly without relaxing the binary constraint.\nExperimental results on three benchmark datasets show that our proposed methods\ncompare favorably with the state of the art.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "do2016learning", "year": "2016", "title":"Learning To Hash With Binary Deep Neural Network", "abstract": "<p>This work proposes deep network models and learning algorithms for\nunsupervised and supervised binary hashing. Our novel network design constrains\none hidden layer to directly output the binary codes. This addresses a\nchallenging issue in some previous works: optimizing non-smooth objective\nfunctions due to binarization. Moreover, we incorporate independence and\nbalance properties in the direct and strict forms in the learning. Furthermore,\nwe include similarity preserving property in our objective function. Our\nresulting optimization with these binary, independence, and balance constraints\nis difficult to solve. We propose to attack it with alternating optimization\nand careful relaxation. Experimental results on three benchmark datasets show\nthat our proposed methods compare favorably with the state of the art.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "do2017compact", "year": "2017", "title":"Compact Hash Code Learning With Binary Deep Neural Network", "abstract": "<p>Learning compact binary codes for image retrieval problem using deep neural\nnetworks has recently attracted increasing attention. However, training deep\nhashing networks is challenging due to the binary constraints on the hash\ncodes. In this paper, we propose deep network models and learning algorithms\nfor learning binary hash codes given image representations under both\nunsupervised and supervised manners. The novelty of our network design is that\nwe constrain one hidden layer to directly output the binary codes. This design\nhas overcome a challenging problem in some previous works: optimizing\nnon-smooth objective functions because of binarization. In addition, we propose\nto incorporate independence and balance properties in the direct and strict\nforms into the learning schemes. We also include a similarity preserving\nproperty in our objective functions. The resulting optimizations involving\nthese binary, independence, and balance constraints are difficult to solve. To\ntackle this difficulty, we propose to learn the networks with alternating\noptimization and careful relaxation. Furthermore, by leveraging the powerful\ncapacity of convolutional neural networks, we propose an end-to-end\narchitecture that jointly learns to extract visual features and produce binary\nhash codes. Experimental results for the benchmark datasets show that the\nproposed methods compare favorably or outperform the state of the art.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "do2017simultaneous", "year": "2017", "title":"Simultaneous Feature Aggregating And Hashing For Large-scale Image Search", "abstract": "<p>In most state-of-the-art hashing-based visual search systems, local image\ndescriptors of an image are first aggregated as a single feature vector. This\nfeature vector is then subjected to a hashing function that produces a binary\nhash code. In previous work, the aggregating and the hashing processes are\ndesigned independently. In this paper, we propose a novel framework where\nfeature aggregating and hashing are designed simultaneously and optimized\njointly. Specifically, our joint optimization produces aggregated\nrepresentations that can be better reconstructed by some binary codes. This\nleads to more discriminative binary hash codes and improved retrieval accuracy.\nIn addition, we also propose a fast version of the recently-proposed Binary\nAutoencoder to be used in our proposed framework. We perform extensive\nretrieval experiments on several benchmark datasets with both SIFT and\nconvolutional features. Our results suggest that the proposed framework\nachieves significant improvements over the state of the art.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "do2018binary", "year": "2018", "title":"Binary Constrained Deep Hashing Network For Image Retrieval Without Manual Annotation", "abstract": "<p>Learning compact binary codes for image retrieval task using deep neural\nnetworks has attracted increasing attention recently. However, training deep\nhashing networks for the task is challenging due to the binary constraints on\nthe hash codes, the similarity preserving property, and the requirement for a\nvast amount of labelled images. To the best of our knowledge, none of the\nexisting methods has tackled all of these challenges completely in a unified\nframework. In this work, we propose a novel end-to-end deep learning approach\nfor the task, in which the network is trained to produce binary codes directly\nfrom image pixels without the need of manual annotation. In particular, to deal\nwith the non-smoothness of binary constraints, we propose a novel pairwise\nconstrained loss function, which simultaneously encodes the distances between\npairs of hash codes, and the binary quantization error. In order to train the\nnetwork with the proposed loss function, we propose an efficient parameter\nlearning algorithm. In addition, to provide similar / dissimilar training\nimages to train the network, we exploit 3D models reconstructed from unlabelled\nimages for automatic generation of enormous training image pairs. The extensive\nexperiments on image retrieval benchmark datasets demonstrate the improvements\nof the proposed method over the state-of-the-art compact representation methods\non the image retrieval problem.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Independent","Quantisation"] },
{"key": "do2018from", "year": "2018", "title":"From Selective Deep Convolutional Features To Compact Binary Representations For Image Retrieval", "abstract": "<p>In the large-scale image retrieval task, the two most important requirements\nare the discriminability of image representations and the efficiency in\ncomputation and storage of representations. Regarding the former requirement,\nConvolutional Neural Network (CNN) is proven to be a very powerful tool to\nextract highly discriminative local descriptors for effective image search.\nAdditionally, in order to further improve the discriminative power of the\ndescriptors, recent works adopt fine-tuned strategies. In this paper, taking a\ndifferent approach, we propose a novel, computationally efficient, and\ncompetitive framework. Specifically, we firstly propose various strategies to\ncompute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and eliminate redundant\nfeatures. Our in-depth analyses demonstrate that proposed masking schemes are\neffective to address the burstiness drawback and improve retrieval accuracy.\nSecondly, we propose to employ recent embedding and aggregating methods which\ncan significantly boost the feature discriminability. Regarding the computation\nand storage efficiency, we include a hashing module to produce very compact\nbinary image representations. Extensive experiments on six image retrieval\nbenchmarks demonstrate that our proposed framework achieves the\nstate-of-the-art retrieval performances.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "do2019simultaneous", "year": "2019", "title":"Simultaneous Feature Aggregating And Hashing For Compact Binary Code Learning", "abstract": "<p>Representing images by compact hash codes is an attractive approach for\nlarge-scale content-based image retrieval. In most state-of-the-art\nhashing-based image retrieval systems, for each image, local descriptors are\nfirst aggregated as a global representation vector. This global vector is then\nsubjected to a hashing function to generate a binary hash code. In previous\nworks, the aggregating and the hashing processes are designed independently.\nHence these frameworks may generate suboptimal hash codes. In this paper, we\nfirst propose a novel unsupervised hashing framework in which feature\naggregating and hashing are designed simultaneously and optimized jointly.\nSpecifically, our joint optimization generates aggregated representations that\ncan be better reconstructed by some binary codes. This leads to more\ndiscriminative binary hash codes and improved retrieval accuracy. In addition,\nthe proposed method is flexible. It can be extended for supervised hashing.\nWhen the data label is available, the framework can be adapted to learn binary\ncodes which minimize the reconstruction loss w.r.t. label vectors. Furthermore,\nwe also propose a fast version of the state-of-the-art hashing method Binary\nAutoencoder to be used in our proposed frameworks. Extensive experiments on\nbenchmark datasets under various settings show that the proposed methods\noutperform state-of-the-art unsupervised and supervised hashing methods.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Supervised"] },
{"key": "doan2020hidden", "year": "2020", "title":"HM4 Hidden Markov Model With Memory Management For Visual Place Recognition", "abstract": "<p>Visual place recognition needs to be robust against appearance variability\ndue to natural and man-made causes. Training data collection should thus be an\nongoing process to allow continuous appearance changes to be recorded. However,\nthis creates an unboundedly-growing database that poses time and memory\nscalability challenges for place recognition methods. To tackle the scalability\nissue for visual place recognition in autonomous driving, we develop a Hidden\nMarkov Model approach with a two-tiered memory management. Our algorithm,\ndubbed HM\\(^4\\), exploits temporal look-ahead to transfer promising candidate\nimages between passive storage and active memory when needed. The inference\nprocess takes into account both promising images and a coarse representations\nof the full database. We show that this allows constant time and space\ninference for a fixed coverage area. The coarse representations can also be\nupdated incrementally to absorb new data. To further reduce the memory\nrequirements, we derive a compact image representation inspired by Locality\nSensitive Hashing (LSH). Through experiments on real world data, we demonstrate\nthe excellent scalability and accuracy of the approach under appearance changes\nand provide comparisons against state-of-the-art techniques.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "doan2020image", "year": "2020", "title":"Image Hashing By Minimizing Discrete Component-wise Wasserstein Distance", "abstract": "<p>Image hashing is one of the fundamental problems that demand both efficient\nand effective solutions for various practical scenarios. Adversarial\nautoencoders are shown to be able to implicitly learn a robust,\nlocality-preserving hash function that generates balanced and high-quality hash\ncodes. However, the existing adversarial hashing methods are inefficient to be\nemployed for large-scale image retrieval applications. Specifically, they\nrequire an exponential number of samples to be able to generate optimal hash\ncodes and a significantly high computational cost to train. In this paper, we\nshow that the high sample-complexity requirement often results in sub-optimal\nretrieval performance of the adversarial hashing methods. To address this\nchallenge, we propose a new adversarial-autoencoder hashing approach that has a\nmuch lower sample requirement and computational cost. Specifically, by\nexploiting the desired properties of the hash function in the low-dimensional,\ndiscrete space, our method efficiently estimates a better variant of\nWasserstein distance by averaging a set of easy-to-compute one-dimensional\nWasserstein distances. The resulting hashing approach has an order-of-magnitude\nbetter sample complexity, thus better generalization property, compared to the\nother adversarial hashing methods. In addition, the computational cost is\nsignificantly reduced using our approach. We conduct experiments on several\nreal-world datasets and show that the proposed method outperforms the competing\nhashing methods, achieving up to 10% improvement over the current\nstate-of-the-art image hashing methods. The code accompanying this paper is\navailable on Github (https://github.com/khoadoan/adversarial-hashing).</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Unsupervised"] },
{"key": "doan2022asymmetric", "year": "2022", "title":"Asymmetric Hashing For Fast Ranking Via Neural Network Measures", "abstract": "<p>Fast item ranking is an important task in recommender systems. In previous\nworks, graph-based Approximate Nearest Neighbor (ANN) approaches have\ndemonstrated good performance on item ranking tasks with generic\nsearching/matching measures (including complex measures such as neural network\nmeasures). However, since these ANN approaches must go through the neural\nmeasures several times during ranking, the computation is not practical if the\nneural measure is a large network. On the other hand, fast item ranking using\nexisting hashing-based approaches, such as Locality Sensitive Hashing (LSH),\nonly works with a limited set of measures. Previous learning-to-hash approaches\nare also not suitable to solve the fast item ranking problem since they can\ntake a significant amount of time and computation to train the hash functions.\nHashing approaches, however, are attractive because they provide a principle\nand efficient way to retrieve candidate items. In this paper, we propose a\nsimple and effective learning-to-hash approach for the fast item ranking\nproblem that can be used for any type of measure, including neural network\nmeasures. Specifically, we solve this problem with an asymmetric hashing\nframework based on discrete inner product fitting. We learn a pair of related\nhash functions that map heterogeneous objects (e.g., users and items) into a\ncommon discrete space where the inner product of their binary codes reveals\ntheir true similarity defined via the original searching measure. The fast\nranking problem is reduced to an ANN search via this asymmetric hashing scheme.\nThen, we propose a sampling strategy to efficiently select relevant and\ncontrastive samples to train the hashing model. We empirically validate the\nproposed method against the existing state-of-the-art fast item ranking methods\nin several combinations of non-linear searching functions and prominent\ndatasets.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","LSH","Supervised"] },
{"key": "doan2022coophash", "year": "2022", "title":"Coophash Cooperative Learning Of Multipurpose Descriptor And Contrastive Pair Generator Via Variational MCMC Teaching For Supervised Image Hashing", "abstract": "<p>Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10\\% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.</p>\n", "tags": ["ARXIV","GAN","Supervised"] },
{"key": "doan2022one", "year": "2022", "title":"One Loss For Quantization Deep Hashing With Discrete Wasserstein Distributional Matching", "abstract": "<p>Image hashing is a principled approximate nearest neighbor approach to find\nsimilar items to a query in a large collection of images. Hashing aims to learn\na binary-output function that maps an image to a binary vector. For optimal\nretrieval performance, producing balanced hash codes with low-quantization\nerror to bridge the gap between the learning stage’s continuous relaxation and\nthe inference stage’s discrete quantization is important. However, in the\nexisting deep supervised hashing methods, coding balance and low-quantization\nerror are difficult to achieve and involve several losses. We argue that this\nis because the existing quantization approaches in these methods are\nheuristically constructed and not effective to achieve these objectives. This\npaper considers an alternative approach to learning the quantization\nconstraints. The task of learning balanced codes with low quantization error is\nre-formulated as matching the learned distribution of the continuous codes to a\npre-defined discrete, uniform distribution. This is equivalent to minimizing\nthe distance between two distributions. We then propose a computationally\nefficient distributional distance by leveraging the discrete property of the\nhash functions. This distributional distance is a valid distance and enjoys\nlower time and sample complexities. The proposed single-loss quantization\nobjective can be integrated into any existing supervised hashing method to\nimprove code balance and quantization error. Experiments confirm that the\nproposed approach substantially improves the performance of several\nrepresentative hashing~methods.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "dolatshah2015ball", "year": "2015", "title":"Ball-tree Efficient Spatial Indexing For Constrained Nearest-neighbor Search In Metric Spaces", "abstract": "<p>Emerging location-based systems and data analysis frameworks requires\nefficient management of spatial data for approximate and exact search. Exact\nsimilarity search can be done using space partitioning data structures, such as\nKd-tree, R<em>-tree, and Ball-tree. In this paper, we focus on Ball-tree, an\nefficient search tree that is specific for spatial queries which use euclidean\ndistance. Each node of a Ball-tree defines a ball, i.e. a hypersphere that\ncontains a subset of the points to be searched.\n  In this paper, we propose Ball</em>-tree, an improved Ball-tree that is more\nefficient for spatial queries. Ball<em>-tree enjoys a modified space partitioning\nalgorithm that considers the distribution of the data points in order to find\nan efficient splitting hyperplane. Also, we propose a new algorithm for KNN\nqueries with restricted range using Ball</em>-tree, which performs better than both\nKNN and range search for such queries. Results show that Ball*-tree performs\n39%-57% faster than the original Ball-tree algorithm.</p>\n", "tags": ["ARXIV"] },
{"key": "dolhansky2020adversarial", "year": "2020", "title":"Adversarial Collision Attacks On Image Hashing Functions", "abstract": "<p>Hashing images with a perceptual algorithm is a common approach to solving\nduplicate image detection problems. However, perceptual image hashing\nalgorithms are differentiable, and are thus vulnerable to gradient-based\nadversarial attacks. We demonstrate that not only is it possible to modify an\nimage to produce an unrelated hash, but an exact image hash collision between a\nsource and target image can be produced via minuscule adversarial\nperturbations. In a white box setting, these collisions can be replicated\nacross nearly every image pair and hash type (including both deep and\nnon-learned hashes). Furthermore, by attacking points other than the output of\na hashing function, an attacker can avoid having to know the details of a\nparticular algorithm, resulting in collisions that transfer across different\nhash sizes or model architectures. Using these techniques, an adversary can\npoison the image lookup table of a duplicate image detection service, resulting\nin undefined or unwanted behavior. Finally, we offer several potential\nmitigations to gradient-based image hash attacks.</p>\n", "tags": ["ARXIV"] },
{"key": "domnita2020genetic", "year": "2020", "title":"A Genetic Algorithm For Obtaining Memory Constrained Near-perfect Hashing", "abstract": "<p>The problem of fast items retrieval from a fixed collection is often\nencountered in most computer science areas, from operating system components to\ndatabases and user interfaces. We present an approach based on hash tables that\nfocuses on both minimizing the number of comparisons performed during the\nsearch and minimizing the total collection size. The standard open-addressing\ndouble-hashing approach is improved with a non-linear transformation that can\nbe parametrized in order to ensure a uniform distribution of the data in the\nhash table. The optimal parameter is determined using a genetic algorithm. The\npaper results show that near-perfect hashing is faster than binary search, yet\nuses less memory than perfect hashing, being a good choice for\nmemory-constrained applications where search time is also critical.</p>\n", "tags": ["ARXIV"] },
{"key": "dong2017video", "year": "2017", "title":"Video Retrieval Based On Deep Convolutional Neural Network", "abstract": "<p>Recently, with the enormous growth of online videos, fast video retrieval\nresearch has received increasing attention. As an extension of image hashing\ntechniques, traditional video hashing methods mainly depend on hand-crafted\nfeatures and transform the real-valued features into binary hash codes. As\nvideos provide far more diverse and complex visual information than images,\nextracting features from videos is much more challenging than that from images.\nTherefore, high-level semantic features to represent videos are needed rather\nthan low-level hand-crafted methods. In this paper, a deep convolutional neural\nnetwork is proposed to extract high-level semantic features and a binary hash\nfunction is then integrated into this framework to achieve an end-to-end\noptimization. Particularly, our approach also combines triplet loss function\nwhich preserves the relative similarity and difference of videos and\nclassification loss function as the optimization objective. Experiments have\nbeen performed on two public datasets and the results demonstrate the\nsuperiority of our proposed method compared with other state-of-the-art video\nretrieval methods.</p>\n", "tags": ["ARXIV","Supervised","Video Retrieval"] },
{"key": "dong2019document", "year": "2019", "title":"Document Hashing With Mixture-prior Generative Models", "abstract": "<p>Hashing is promising for large-scale information retrieval tasks thanks to\nthe efficiency of distance evaluation between binary codes. Generative hashing\nis often used to generate hashing codes in an unsupervised way. However,\nexisting generative hashing methods only considered the use of simple priors,\nlike Gaussian and Bernoulli priors, which limits these methods to further\nimprove their performance. In this paper, two mixture-prior generative models\nare proposed, under the objective to produce high-quality hashing codes for\ndocuments. Specifically, a Gaussian mixture prior is first imposed onto the\nvariational auto-encoder (VAE), followed by a separate step to cast the\ncontinuous latent representation of VAE into binary code. To avoid the\nperformance loss caused by the separate casting, a model using a Bernoulli\nmixture prior is further developed, in which an end-to-end training is admitted\nby resorting to the straight-through (ST) discrete gradient estimator.\nExperimental results on several benchmark datasets demonstrate that the\nproposed methods, especially the one using Bernoulli mixture priors,\nconsistently outperform existing ones by a substantial margin.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "dong2019learning", "year": "2019", "title":"Learning Space Partitions For Nearest Neighbor Search", "abstract": "<p>Space partitions of \\(\\mathbb{R}^d\\) underlie a vast and important class of\nfast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for building space\npartitions reducing the problem to balanced graph partitioning followed by\nsupervised classification. We instantiate this general approach with the KaHIP\ngraph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,\nto obtain a new partitioning procedure called Neural Locality-Sensitive Hashing\n(Neural LSH). On several standard benchmarks for NNS, our experiments show that\nthe partitions obtained by Neural LSH consistently outperform partitions found\nby quantization-based and tree-based methods as well as classic, data-oblivious\nLSH.</p>\n", "tags": ["ARXIV","FOCS","Graph","LSH","Quantisation","Supervised"] },
{"key": "dong2021ash", "year": "2021", "title":"ASH A Modern Framework For Parallel Spatial Hashing In 3D Perception", "abstract": "<p>We present ASH, a modern and high-performance framework for parallel spatial\nhashing on GPU. Compared to existing GPU hash map implementations, ASH achieves\nhigher performance, supports richer functionality, and requires fewer lines of\ncode (LoC) when used for implementing spatially varying operations from\nvolumetric geometry reconstruction to differentiable appearance reconstruction.\nUnlike existing GPU hash maps, the ASH framework provides a versatile tensor\ninterface, hiding low-level details from the users. In addition, by decoupling\nthe internal hashing data structures and key-value data in buffers, we offer\ndirect access to spatially varying data via indices, enabling seamless\nintegration to modern libraries such as PyTorch. To achieve this, we 1) detach\nstored key-value data from the low-level hash map implementation; 2) bridge the\npointer-first low level data structures to index-first high-level tensor\ninterfaces via an index heap; 3) adapt both generic and non-generic\ninteger-only hash map implementations as backends to operate on\nmulti-dimensional keys. We first profile our hash map against state-of-the-art\nhash maps on synthetic data to show the performance gain from this\narchitecture. We then show that ASH can consistently achieve higher performance\non various large-scale 3D perception tasks with fewer LoC by showcasing several\napplications, including 1) point cloud voxelization, 2) retargetable volumetric\nscene reconstruction, 3) non-rigid point cloud registration and volumetric\ndeformation, and 4) spatially varying geometry and appearance refinement. ASH\nand its example applications are open sourced in Open3D\n(http://www.open3d.org).</p>\n", "tags": ["ARXIV","Has Code"] },
{"key": "dong2021dxhash", "year": "2021", "title":"Dxhash A Scalable Consistent Hash Based On The Pseudo-random Sequence", "abstract": "<p>Consistent hashing (CH) has been pivotal as a data router and load balancer\nin diverse fields, including distributed databases, cloud infrastructure, and\npeer-to-peer networks. However, existing CH algorithms often fall short in\nsimultaneously meeting various critical requirements, such as load balance,\nminimal disruption, statelessness, high lookup rate, small memory footprint,\nand low update overhead. To address these limitations, we introduce DxHash, a\nscalable consistent hashing algorithm based on pseudo-random sequences. To\nadjust workloads on heterogeneous nodes and enhance flexibility, we propose\nweighted DxHash. Through comprehensive evaluations, DxHash demonstrates\nsubstantial improvements across all six requirements compared to\nstate-of-the-art alternatives. Notably, even when confronted with a 50% failure\nratio in a cluster of one million nodes, DxHash maintains remarkable processing\ncapabilities, handling up to 13.3 million queries per second.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "dong2022generalized", "year": "2022", "title":"A Generalized Approach For Cancellable Template And Its Realization For Minutia Cylinder-code", "abstract": "<p>Hashing technology gains much attention in protecting the biometric template\nlately. For instance, Index-of-Max (IoM), a recent reported hashing technique,\nis a ranking-based locality sensitive hashing technique, which illustrates the\nfeasibility to protect the ordered and fixed-length biometric template.\nHowever, biometric templates are not always in the form of ordered and\nfixed-length, rather it may be an unordered and variable size point set e.g.\nfingerprint minutiae, which restricts the usage of the traditional hashing\ntechnology. In this paper, we proposed a generalized version of IoM hashing\nnamely gIoM, and therefore the unordered and variable size biometric template\ncan be used. We demonstrate a realization using a well-known variable size\nfeature vector, fingerprint Minutia Cylinder-Code (MCC). The gIoM transforms\nMCC into index domain to form indexing-based feature representation.\nConsequently, the inversion of MCC from the transformed representation is\ncomputational infeasible, thus to achieve non-invertibility while the\nperformance is preserved. Public fingerprint databases FVC2002 and FVC2004 are\nemployed for experiment as benchmark to demonstrate a fair comparison with\nother methods. Moreover, the security and privacy analysis suggest that gIoM\nmeets the criteria of template protection: non-invertibility, revocability, and\nnon-linkability.</p>\n", "tags": ["ARXIV"] },
{"key": "dong2024learning", "year": "2024", "title":"Learning Space Partitions For Nearest Neighbor Search", "abstract": "<p>Space partitions of underlie a vast and important\nclass of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification.\nWe instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.</p>\n", "tags": ["ARXIV","Graph","LSH","Quantisation","Supervised"] },
{"key": "dong2024simisketch", "year": "2024", "title":"Simisketch Efficiently Estimating Similarity Of Streaming Multisets", "abstract": "<p>The challenge of estimating similarity between sets has been a significant\nconcern in data science, finding diverse applications across various domains.\nHowever, previous approaches, such as MinHash, have predominantly centered\naround hashing techniques, which are well-suited for sets but less naturally\nadaptable to multisets, a common occurrence in scenarios like network streams\nand text data. Moreover, with the increasing prevalence of data arriving in\nstreaming patterns, many existing methods struggle to handle cases where set\nitems are presented in a continuous stream. Consequently, our focus in this\npaper is on the challenging scenario of multisets with item streams. To address\nthis, we propose SimiSketch, a sketching algorithm designed to tackle this\nspecific problem. The paper begins by presenting two simpler versions that\nemploy intuitive sketches for similarity estimation. Subsequently, we formally\nintroduce SimiSketch and leverage SALSA to enhance accuracy. To validate our\nalgorithms, we conduct extensive testing on synthetic datasets, real-world\nnetwork traffic, and text articles. Our experiment shows that compared with the\nstate-of-the-art, SimiSketch can improve the accuracy by up to 42 times, and\nincrease the throughput by up to 360 times. The complete source code is\nopen-sourced and available on GitHub for reference.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "doshi2020lanns", "year": "2020", "title":"LANNS A Web-scale Approximate Nearest Neighbor Lookup System", "abstract": "<p>Nearest neighbor search (NNS) has a wide range of applications in information\nretrieval, computer vision, machine learning, databases, and other areas.\nExisting state-of-the-art algorithm for nearest neighbor search, Hierarchical\nNavigable Small World Networks(HNSW), is unable to scale to large datasets of\n100M records in high dimensions. In this paper, we propose LANNS, an end-to-end\nplatform for Approximate Nearest Neighbor Search, which scales for web-scale\ndatasets. Library for Large Scale Approximate Nearest Neighbor Search (LANNS)\nis deployed in multiple production systems for identifying topK (\\(100 \\leq topK\n\\leq 200\\)) approximate nearest neighbors with a latency of a few milliseconds\nper query, high throughput of 2.5k Queries Per Second (QPS) on a single node,\non large (\\(\\sim\\)180M data points) high dimensional (50-2048 dimensional)\ndatasets.</p>\n", "tags": ["ARXIV"] },
{"key": "douze2016polysemous", "year": "2016", "title":"Polysemous Codes", "abstract": "<p>This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90’s to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine.</p>\n", "tags": ["ARXIV","CNN","Graph","Quantisation"] },
{"key": "douze2018link", "year": "2018", "title":"Link And Code Fast Indexing With Graphs And Compact Regression Codes", "abstract": "<p>Similarity search approaches based on graph walks have recently attained\noutstanding speed-accuracy trade-offs, taking aside the memory requirements. In\nthis paper, we revisit these approaches by considering, additionally, the\nmemory constraint required to index billions of images on a single server. This\nleads us to propose a method based both on graph traversal and compact\nrepresentations. We encode the indexed vectors using quantization and exploit\nthe graph structure to refine the similarity estimation.\n  In essence, our method takes the best of these two worlds: the search\nstrategy is based on nested graphs, thereby providing high precision with a\nrelatively small set of comparisons. At the same time it offers a significant\nmemory compression. As a result, our approach outperforms the state of the art\non operating points considering 64-128 bytes per vector, as demonstrated by our\nresults on two billion-scale public benchmarks.</p>\n", "tags": ["ARXIV","Graph","Quantisation","Supervised"] },
{"key": "douze2024faiss", "year": "2024", "title":"The Faiss Library", "abstract": "<p>Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.</p>\n", "tags": ["ARXIV"] },
{"key": "driemel2017locality", "year": "2017", "title":"Locality-sensitive Hashing Of Curves", "abstract": "<p>We study data structures for storing a set of polygonal curves in \\({\\rm R}^d\\)\nsuch that, given a query curve, we can efficiently retrieve similar curves from\nthe set, where similarity is measured using the discrete Fr'echet distance or\nthe dynamic time warping distance. To this end we devise the first\nlocality-sensitive hashing schemes for these distance measures. A major\nchallenge is posed by the fact that these distance measures internally optimize\nthe alignment between the curves. We give solutions for different types of\nalignments including constrained and unconstrained versions. For unconstrained\nalignments, we improve over a result by Indyk from 2002 for short curves. Let\n\\(n\\) be the number of input curves and let \\(m\\) be the maximum complexity of a\ncurve in the input. In the particular case where \\(m \\leq \\frac{\\alpha}{4d} log\nn\\), for some fixed \\(\\alpha&gt;0\\), our solutions imply an approximate near-neighbor\ndata structure for the discrete Fr'echet distance that uses space in\n\\(O(n^{1+\\alpha}log n)\\) and achieves query time in \\(O(n^{\\alpha}log^2 n)\\) and\nconstant approximation factor. Furthermore, our solutions provide a trade-off\nbetween approximation quality and computational performance: for any parameter\n\\(k \\in [m]\\), we can give a data structure that uses space in \\(O(2^{2k}m^{k-1} n\nlog n + nm)\\), answers queries in \\(O( 2^{2k} m^{k}log n)\\) time and achieves\napproximation factor in \\(O(m/k)\\).</p>\n", "tags": ["ARXIV"] },
{"key": "du2014inner", "year": "2014", "title":"Inner Product Similarity Search Using Compositional Codes", "abstract": "<p>This paper addresses the nearest neighbor search problem under inner product\nsimilarity and introduces a compact code-based approach. The idea is to\napproximate a vector using the composition of several elements selected from a\nsource dictionary and to represent this vector by a short code composed of the\nindices of the selected elements. The inner product between a query vector and\na database vector is efficiently estimated from the query vector and the short\ncode of the database vector. We show the superior performance of the proposed\ngroup \\(M\\)-selection algorithm that selects \\(M\\) elements from \\(M\\) source\ndictionaries for vector approximation in terms of search accuracy and\nefficiency for compact codes of the same length via theoretical and empirical\nanalysis. Experimental results on large-scale datasets (\\(1M\\) and \\(1B\\) SIFT\nfeatures, \\(1M\\) linear models and Netflix) demonstrate the superiority of the\nproposed approach.</p>\n", "tags": ["ARXIV"] },
{"key": "dubey2021vision", "year": "2021", "title":"Vision Transformer Hashing For Image Retrieval", "abstract": "<p>Deep learning has shown a tremendous growth in hashing techniques for image\nretrieval. Recently, Transformer has emerged as a new architecture by utilizing\nself-attention without convolution. Transformer is also extended to Vision\nTransformer (ViT) for the visual recognition with a promising performance on\nImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)\nfor image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone\nnetwork and add the hashing head. The proposed VTS model is fine tuned for\nhashing under six different image retrieval frameworks, including Deep\nSupervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network\n(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)\nwith their objective functions. We perform the extensive experiments on\nCIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image\nretrieval outperforms the recent state-of-the-art hashing techniques with a\ngreat margin. We also find the proposed VTS model as the backbone network is\nbetter than the existing networks, such as AlexNet and ResNet. The code is\nreleased at \\url{https://github.com/shivram1987/VisionTransformerHashing}.</p>\n", "tags": ["ARXIV","Deep Learning","Has Code","Image Retrieval","Quantisation","Supervised"] },
{"key": "dubey2024transformer", "year": "2024", "title":"Transformer-based Clipped Contrastive Quantization Learning For Unsupervised Image Retrieval", "abstract": "<p>Unsupervised image retrieval aims to learn the important visual\ncharacteristics without any given level to retrieve the similar images for a\ngiven query image. The Convolutional Neural Network (CNN)-based approaches have\nbeen extensively exploited with self-supervised contrastive learning for image\nhashing. However, the existing approaches suffer due to lack of effective\nutilization of global features by CNNs and biased-ness created by false\nnegative pairs in the contrastive learning. In this paper, we propose a\nTransClippedCLR model by encoding the global context of an image using\nTransformer having local context through patch based processing, by generating\nthe hash codes through product quantization and by avoiding the potential false\nnegative pairs through clipped contrastive learning. The proposed model is\ntested with superior performance for unsupervised image retrieval on benchmark\ndatasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent\nstate-of-the-art deep models. The results using the proposed clipped\ncontrastive learning are greatly improved on all datasets as compared to same\nbackbone network with vanilla contrastive learning.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "duda2016distortion", "year": "2016", "title":"Distortion-resistant Hashing For Rapid Search Of Similar DNA Subsequence", "abstract": "<p>One of the basic tasks in bioinformatics is localizing a short subsequence\n\\(S\\), read while sequencing, in a long reference sequence \\(R\\), like the human\ngeneome. A natural rapid approach would be finding a hash value for \\(S\\) and\ncompare it with a prepared database of hash values for each of length \\(|S|\\)\nsubsequences of \\(R\\). The problem with such approach is that it would only spot\na perfect match, while in reality there are lots of small changes:\nsubstitutions, deletions and insertions.\n  This issue could be repaired if having a hash function designed to tolerate\nsome small distortion accordingly to an alignment metric (like\nNeedleman-Wunch): designed to make that two similar sequences should most\nlikely give the same hash value. This paper discusses construction of\nDistortion-Resistant Hashing (DRH) to generate such fingerprints for rapid\nsearch of similar subsequences. The proposed approach is based on the rate\ndistortion theory: in a nearly uniform subset of length \\(|S|\\) sequences, the\nhash value represents the closest sequence to \\(S\\). This gives some control of\nthe distance of collisions: sequences having the same hash value.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "dutta2017stochastic", "year": "2017", "title":"Stochastic Graphlet Embedding", "abstract": "<p>Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of –\nexplicit/implicit – graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.</p>\n", "tags": ["Graph","Supervised"] },
{"key": "dutta2018graph", "year": "2018", "title":"Graph Kernels Based On High Order Graphlet Parsing And Hashing", "abstract": "<p>Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of –\nexplicit/implicit – graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "dutta2018when", "year": "2018", "title":"When Hashing Met Matching Efficient Spatio-temporal Search For Ridesharing", "abstract": "<p>Carpooling, or sharing a ride with other passengers, holds immense potential\nfor urban transportation. Ridesharing platforms enable such sharing of rides\nusing real-time data. Finding ride matches in real-time at urban scale is a\ndifficult combinatorial optimization task and mostly heuristic approaches are\napplied. In this work, we mathematically model the problem as that of finding\nnear-neighbors and devise a novel efficient spatio-temporal search algorithm\nbased on the theory of locality sensitive hashing for Maximum Inner Product\nSearch (MIPS). The proposed algorithm can find \\(k\\) near-optimal potential\nmatches for every ride from a pool of \\(n\\) rides in time \\(O(n^{1 + \\rho} (k +\nlog n) log k)\\) and space \\(O(n^{1 + \\rho} log k)\\) for a small \\(\\rho &lt; 1\\). Our\nalgorithm can be extended in several useful and interesting ways increasing its\npractical appeal. Experiments with large NY yellow taxi trip datasets show that\nour algorithm consistently outperforms state-of-the-art heuristic methods\nthereby proving its practical applicability.</p>\n", "tags": ["ARXIV"] },
{"key": "e2022wavelet", "year": "2022", "title":"Wavelet Feature Maps Compression For Image-to-image Cnns", "abstract": "<p>Convolutional Neural Networks (CNNs) are known for requiring extensive computational resources, and quantization is among the best and most common methods for compressing them. While aggressive quantization (i.e., less than 4-bits) performs well for classification, it may cause severe performance degradation in image-to-image tasks such as semantic segmentation and depth estimation. In this paper, we propose Wavelet Compressed Convolution (WCC)—a novel approach for high-resolution activation maps compression integrated with point-wise convolutions, which are the main computational cost of modern architectures. To this end, we use an efficient and hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. We experiment with various tasks that benefit from high-resolution input. By combining WCC with light quantization, we achieve compression rates equivalent to 1-4bit activation quantization with relatively small and much more graceful degradation in performance. Our code is available at https://github.com/BGUCompSci/WaveletCompressedConvolution.</p>\n", "tags": ["Has Code","NEURIPS","Quantisation","Supervised"] },
{"key": "efremenko2019fast", "year": "2019", "title":"Fast And Bayes-consistent Nearest Neighbors", "abstract": "<p>Research on nearest-neighbor methods tends to focus somewhat dichotomously\neither on the statistical or the computational aspects – either on, say, Bayes\nconsistency and rates of convergence or on techniques for speeding up the\nproximity search. This paper aims at bridging these realms: to reap the\nadvantages of fast evaluation time while maintaining Bayes consistency, and\nfurther without sacrificing too much in the risk decay rate. We combine the\nlocality-sensitive hashing (LSH) technique with a novel missing-mass argument\nto obtain a fast and Bayes-consistent classifier. Our algorithm’s prediction\nruntime compares favorably against state of the art approximate NN methods,\nwhile maintaining Bayes-consistency and attaining rates comparable to minimax.\nOn samples of size \\(n\\) in \\(\\R^d\\), our pre-processing phase has runtime \\(O(d n\nlog n)\\), while the evaluation phase has runtime \\(O(dlog n)\\) per query point.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "eghbali2016fast", "year": "2016", "title":"Fast Cosine Similarity Search In Binary Space With Angular Multi-index Hashing", "abstract": "<p>Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find \\(K\\) codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular \\(K\\) nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.</p>\n", "tags": ["ARXIV"] },
{"key": "eghbali2019deep", "year": "2019", "title":"Deep Spherical Quantization For Image Search", "abstract": "<p>Hashing methods, which encode high-dimensional images with compact discrete\ncodes, have been widely applied to enhance large-scale image retrieval. In this\npaper, we put forward Deep Spherical Quantization (DSQ), a novel method to make\ndeep convolutional neural networks generate supervised and compact binary codes\nfor efficient image search. Our approach simultaneously learns a mapping that\ntransforms the input images into a low-dimensional discriminative space, and\nquantizes the transformed data points using multi-codebook quantization. To\neliminate the negative effect of norm variance on codebook learning, we force\nthe network to L_2 normalize the extracted features and then quantize the\nresulting vectors using a new supervised quantization technique specifically\ndesigned for points lying on a unit hypersphere. Furthermore, we introduce an\neasy-to-implement extension of our quantization technique that enforces\nsparsity on the codebooks. Extensive experiments demonstrate that DSQ and its\nsparse variant can generate semantically separable compact binary codes\noutperforming many state-of-the-art image retrieval methods on three\nbenchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "elkishky2022knn", "year": "2022", "title":"Knn-embed Locally Smoothed Embedding Mixtures For Multi-interest Candidate Retrieval", "abstract": "<p>Candidate retrieval is the first stage in recommendation systems, where a\nlight-weight system is used to retrieve potentially relevant items for an input\nuser. These candidate items are then ranked and pruned in later stages of\nrecommender systems using a more complex ranking model. As the top of the\nrecommendation funnel, it is important to retrieve a high-recall candidate set\nto feed into downstream ranking models. A common approach is to leverage\napproximate nearest neighbor (ANN) search from a single dense query embedding;\nhowever, this approach this can yield a low-diversity result set with many near\nduplicates. As users often have multiple interests, candidate retrieval should\nideally return a diverse set of candidates reflective of the user’s multiple\ninterests. To this end, we introduce kNN-Embed, a general approach to improving\ndiversity in dense ANN-based retrieval. kNN-Embed represents each user as a\nsmoothed mixture over learned item clusters that represent distinct “interests”\nof the user. By querying each of a user’s mixture component in proportion to\ntheir mixture weights, we retrieve a high-diversity set of candidates\nreflecting elements from each of a user’s interests. We experimentally compare\nkNN-Embed to standard ANN candidate retrieval, and show significant\nimprovements in overall recall and improved diversity across three datasets.\nAccompanying this work, we open source a large Twitter follow-graph dataset\n(https://huggingface.co/datasets/Twitter/TwitterFollowGraph), to spur further\nresearch in graph-mining and representation learning for recommender systems.</p>\n", "tags": ["ARXIV","Graph","Text Retrieval"] },
{"key": "engels2021practical", "year": "2021", "title":"Practical Near Neighbor Search Via Group Testing", "abstract": "<p>We present a new algorithm for the approximate near neighbor problem that combines classical ideas from group testing with locality-sensitive hashing (LSH). We reduce the near neighbor search problem to a group testing problem by designating neighbors as “positives,” non-neighbors as “negatives,” and approximate membership queries as group tests. We instantiate this framework using distance-sensitive Bloom Filters to Identify Near-Neighbor Groups (FLINNG). We prove that FLINNG has sub-linear query time and show that our algorithm comes with a variety of practical advantages. For example, FLINNG can be constructed in a single pass through the data, consists entirely of efficient integer operations, and does not require any distance computations. We conduct large-scale experiments on high-dimensional search tasks such as genome search, URL similarity search, and embedding search over the massive YFCC100M dataset. In our comparison with leading algorithms such as HNSW and FAISS, we find that FLINNG can provide up to a 10x query speedup with substantially smaller indexing time and memory.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "engels2024approximate", "year": "2024", "title":"Approximate Nearest Neighbor Search With Window Filters", "abstract": "<p>We define and investigate the problem of \\(\\textit{c-approximate window\nsearch}\\): approximate nearest neighbor search where each point in the dataset\nhas a numeric label, and the goal is to find nearest neighbors to queries\nwithin arbitrary label ranges. Many semantic search problems, such as image and\ndocument search with timestamp filters, or product search with cost filters,\nare natural examples of this problem. We propose and theoretically analyze a\nmodular tree-based framework for transforming an index that solves the\ntraditional c-approximate nearest neighbor problem into a data structure that\nsolves window search. On standard nearest neighbor benchmark datasets equipped\nwith random label values, adversarially constructed embeddings, and image\nsearch embeddings with real timestamps, we obtain up to a \\(75\\times\\) speedup\nover existing solutions at the same level of recall.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "eppstein2014wear", "year": "2014", "title":"Wear Minimization For Cuckoo Hashing How Not To Throw A Lot Of Eggs Into One Basket", "abstract": "<p>We study wear-leveling techniques for cuckoo hashing, showing that it is\npossible to achieve a memory wear bound of \\(loglog n+O(1)\\) after the\ninsertion of \\(n\\) items into a table of size \\(Cn\\) for a suitable constant \\(C\\)\nusing cuckoo hashing. Moreover, we study our cuckoo hashing method empirically,\nshowing that it significantly improves on the memory wear performance for\nclassic cuckoo hashing and linear probing in practice.</p>\n", "tags": ["ARXIV"] },
{"key": "erbert2016gerbil", "year": "2016", "title":"Gerbil A Fast And Memory-efficient k-mer Counter With Gpu-support", "abstract": "<p>A basic task in bioinformatics is the counting of \\(k\\)-mers in genome strings.\nThe \\(k\\)-mer counting problem is to build a histogram of all substrings of\nlength \\(k\\) in a given genome sequence. We present the open source \\(k\\)-mer\ncounting software Gerbil that has been designed for the efficient counting of\n\\(k\\)-mers for \\(k\\geq32\\). Given the technology trend towards long reads of\nnext-generation sequencers, support for large \\(k\\) becomes increasingly\nimportant. While existing \\(k\\)-mer counting tools suffer from excessive memory\nresource consumption or degrading performance for large \\(k\\), Gerbil is able to\nefficiently support large \\(k\\) without much loss of performance. Our software\nimplements a two-disk approach. In the first step, DNA reads are loaded from\ndisk and distributed to temporary files that are stored at a working disk. In a\nsecond step, the temporary files are read again, split into \\(k\\)-mers and\ncounted via a hash table approach. In addition, Gerbil can optionally use GPUs\nto accelerate the counting step. For large \\(k\\), we outperform state-of-the-art\nopen source \\(k\\)-mer counting tools for large genome data sets.</p>\n", "tags": ["ARXIV"] },
{"key": "ercoli2016compact", "year": "2016", "title":"Compact Hash Codes For Efficient Visual Descriptors Retrieval In Large Scale Databases", "abstract": "<p>In this paper we present an efficient method for visual descriptors retrieval\nbased on compact hash codes computed using a multiple k-means assignment. The\nmethod has been applied to the problem of approximate nearest neighbor (ANN)\nsearch of local and global visual content descriptors, and it has been tested\non different datasets: three large scale public datasets of up to one billion\ndescriptors (BIGANN) and, supported by recent progress in convolutional neural\nnetworks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental results\nshow that, despite its simplicity, the proposed method obtains a very high\nperformance that makes it superior to more complex state-of-the-art methods.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ermon2013embed", "year": "2013", "title":"Embed And Project Discrete Sampling With Universal Hashing", "abstract": "<p>We consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set, specified for instance by a graphical model. We propose a sampling algorithm, called PAWS, based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods. Our scheme can leverage fast combinatorial optimization tools as a blackbox and, unlike MCMC methods, samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution. We demonstrate that by using state-of-the-art combinatorial search tools, PAWS can efficiently sample from Ising grids with strong interactions and from software verification instances, while MCMC and variational methods fail in both cases.</p>\n", "tags": ["Graph","Independent","NEURIPS"] },
{"key": "ertl2017superminhash", "year": "2017", "title":"Superminhash - A New Minwise Hashing Algorithm For Jaccard Similarity Estimation", "abstract": "<p>This paper presents a new algorithm for calculating hash signatures of sets\nwhich can be directly used for Jaccard similarity estimation. The new approach\nis an improvement over the MinHash algorithm, because it has a better runtime\nbehavior and the resulting signatures allow a more precise estimation of the\nJaccard index.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ertl2018bagminhash", "year": "2018", "title":"Bagminhash - Minwise Hashing Algorithm For Weighted Sets", "abstract": "<p>Minwise hashing has become a standard tool to calculate signatures which\nallow direct estimation of Jaccard similarities. While very efficient\nalgorithms already exist for the unweighted case, the calculation of signatures\nfor weighted sets is still a time consuming task. BagMinHash is a new algorithm\nthat can be orders of magnitude faster than current state of the art without\nany particular restrictions or assumptions on weights or data dimensionality.\nApplied to the special case of unweighted sets, it represents the first\nefficient algorithm producing independent signature components. A series of\ntests finally verifies the new algorithm and also reveals limitations of other\napproaches published in the recent past.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ertl2019probminhash", "year": "2019", "title":"Probminhash -- A Class Of Locality-sensitive Hash Algorithms For The (probability) Jaccard Similarity", "abstract": "<p>The probability Jaccard similarity was recently proposed as a natural\ngeneralization of the Jaccard similarity to measure the proximity of sets whose\nelements are associated with relative frequencies or probabilities. In\ncombination with a hash algorithm that maps those weighted sets to compact\nsignatures which allow fast estimation of pairwise similarities, it constitutes\na valuable method for big data applications such as near-duplicate detection,\nnearest neighbor search, or clustering. This paper introduces a class of\none-pass locality-sensitive hash algorithms that are orders of magnitude faster\nthan the original approach. The performance gain is achieved by calculating\nsignature components not independently, but collectively. Four different\nalgorithms are proposed based on this idea. Two of them are statistically\nequivalent to the original approach and can be used as drop-in replacements.\nThe other two may even improve the estimation error by introducing statistical\ndependence between signature components. Moreover, the presented techniques can\nbe specialized for the conventional Jaccard similarity, resulting in highly\nefficient algorithms that outperform traditional minwise hashing and that are\nable to compete with the state of the art.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ertl2021setsketch", "year": "2021", "title":"Setsketch Filling The Gap Between Minhash And Hyperloglog", "abstract": "<p>MinHash and HyperLogLog are sketching algorithms that have become\nindispensable for set summaries in big data applications. While HyperLogLog\nallows counting different elements with very little space, MinHash is suitable\nfor the fast comparison of sets as it allows estimating the Jaccard similarity\nand other joint quantities. This work presents a new data structure called\nSetSketch that is able to continuously fill the gap between both use cases. Its\ncommutative and idempotent insert operation and its mergeable state make it\nsuitable for distributed environments. Fast, robust, and easy-to-implement\nestimators for cardinality and joint quantities, as well as the ability to use\nSetSketch for similarity search, enable versatile applications. The presented\njoint estimator can also be applied to other data structures such as MinHash,\nHyperLogLog, or HyperMinHash, where it even performs better than the\ncorresponding state-of-the-art estimators in many cases.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ertl2024jumpbackhash", "year": "2024", "title":"Jumpbackhash Say Goodbye To The Modulo Operation To Distribute Keys Uniformly To Buckets", "abstract": "<p>The distribution of keys to a given number of buckets is a fundamental task\nin distributed data processing and storage. A simple, fast, and therefore\npopular approach is to map the hash values of keys to buckets based on the\nremainder after dividing by the number of buckets. Unfortunately, these\nmappings are not stable when the number of buckets changes, which can lead to\nsevere spikes in system resource utilization, such as network or database\nrequests. Consistent hash algorithms can minimize remappings, but are either\nsignificantly slower than the modulo-based approach, require floating-point\narithmetic, or are based on a family of hash functions rarely available in\nstandard libraries. This paper introduces JumpBackHash, which uses only integer\narithmetic and a standard pseudorandom generator. Due to its speed and simple\nimplementation, it can safely replace the modulo-based approach to improve\nassignment and system stability. A production-ready Java implementation of\nJumpBackHash has been released as part of the Hash4j open source library.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "esen2016large", "year": "2016", "title":"Large-scale Video Search With Efficient Temporal Voting Structure", "abstract": "<p>In this work, we propose a fast content-based video querying system for\nlarge-scale video search. The proposed system is distinguished from similar\nworks with two major contributions. First contribution is superiority of joint\nusage of repeated content representation and efficient hashing mechanisms.\nRepeated content representation is utilized with a simple yet robust feature,\nwhich is based on edge energy of frames. Each of the representation is\nconverted into hash code with Hamming Embedding method for further queries.\nSecond contribution is novel queue-based voting scheme that leads to modest\nmemory requirements with gradual memory allocation capability, contrary to\ncomplete brute-force temporal voting schemes. This aspect enables us to make\nqueries on large video databases conveniently, even on commodity computers with\nlimited memory capacity. Our results show that the system can respond to video\nqueries on a large video database with fast query times, high recall rate and\nvery low memory and disk requirements.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "esposito2019recsplit", "year": "2019", "title":"Recsplit Minimal Perfect Hashing Via Recursive Splitting", "abstract": "<p>A minimal perfect hash function bijectively maps a key set \\(S\\) out of a\nuniverse \\(U\\) into the first \\(|S|\\) natural numbers. Minimal perfect hash\nfunctions are used, for example, to map irregularly-shaped keys, such as\nstring, in a compact space so that metadata can then be simply stored in an\narray. While it is known that just \\(1.44\\) bits per key are necessary to store a\nminimal perfect function, no published technique can go below \\(2\\) bits per key\nin practice. We propose a new technique for storing minimal perfect hash\nfunctions with expected linear construction time and expected constant lookup\ntime that makes it possible to build for the first time, for example,\nstructures which need \\(1.56\\) bits per key, that is, within \\(8.3\\)% of the lower\nbound, in less than \\(2\\) ms per key. We show that instances of our construction\nare able to simultaneously beat the construction time, space usage and lookup\ntime of the state-of-the-art data structure reaching \\(2\\) bits per key.\nMoreover, we provide parameter choices giving structures which are competitive\nwith alternative, larger-size data structures in terms of space and lookup\ntime. The construction of our data structures can be easily parallelized or\nmapped on distributed computational units (e.g., within the MapReduce\nframework), and structures larger than the available RAM can be directly built\nin mass storage.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "esser2021faster", "year": "2021", "title":"A Faster Algorithm For Finding Closest Pairs In Hamming Metric", "abstract": "<p>We study the Closest Pair Problem in Hamming metric, which asks to find the\npair with the smallest Hamming distance in a collection of binary vectors. We\ngive a new randomized algorithm for the problem on uniformly random input\noutperforming previous approaches whenever the dimension of input points is\nsmall compared to the dataset size. For moderate to large dimensions, our\nalgorithm matches the time complexity of the previously best-known locality\nsensitive hashing based algorithms. Technically our algorithm follows similar\ndesign principles as Dubiner (IEEE Trans. Inf. Theory 2010) and May-Ozerov\n(Eurocrypt 2015). Besides improving the time complexity in the aforementioned\nareas, we significantly simplify the analysis of these previous works. We give\na modular analysis, which allows us to investigate the performance of the\nalgorithm also on non-uniform input distributions. Furthermore, we give a proof\nof concept implementation of our algorithm which performs well in comparison to\na quadratic search baseline. This is the first step towards answering an open\nquestion raised by May and Ozerov regarding the practicability of algorithms\nfollowing these design principles.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "facebookmeta2024facebook", "year": "2024", "title":"Facebook Simsearchnet++", "abstract": "<p>Facebook SimSearchNet++ is a new dataset released by Facebook for this competition. It consists of features used for image copy detection for integrity purposes. The features are generated by Facebook SimSearchNet++ model.</p>\n", "tags": ["ARXIV"] },
{"key": "facebooksimsearchnet", "year": "2021", "title":"Facebook SimSearchNet++", "abstract": "<p>Facebook SimSearchNet++ is a new dataset released by Facebook for this competition. It consists of features used for image copy detection for integrity purposes. The features are generated by Facebook SimSearchNet++ model.</p>\n", "tags": ["Dataset"] },
{"key": "fan2024deep", "year": "2024", "title":"Deep Polarized Network For Supervised Learning Of Accurate Binary Hashing Codes", "abstract": "<p>This paper proposes a novel deep polarized network (DPN) for learning to hash, in which each channel in the network outputs is pushed far away\nfrom zero by employing a differentiable bit-wise hinge-like loss which is dubbed as polarization loss. Reformulated within a generic Hamming Distance Metric Learning framework [Norouzi et al.,\n2012], the proposed polarization loss bypasses the requirement to prepare pairwise labels for (dis-)similar items and, yet, the proposed loss strictly bounds from above the pairwise Hamming Distance based losses. The intrinsic connection between pairwise and pointwise label information, as\ndisclosed in this paper, brings about the following methodological improvements: (a) we may directly employ the proposed differentiable polarization loss with no large deviations incurred from\nthe target Hamming distance based loss; and (b) the subtask of assigning binary codes becomes extremely simple — even random codes assigned to each class suffice to result in state-of-the-art performances, as demonstrated in CIFAR10, NUS-WIDE and ImageNet100 datasets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "fan2024supervised", "year": "2024", "title":"Supervised Binary Hash Code Learning With Jensen Shannon Divergence", "abstract": "<p>This paper proposes to learn binary hash codes within\na statistical learning framework, in which an upper bound\nof the probability of Bayes decision errors is derived for\ndifferent forms of hash functions and a rigorous proof of\nthe convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent\nperformance improvements of existing hash code learning\nalgorithms, regardless of whether original algorithms are\nunsupervised or supervised. This paper also illustrates a\nfast hash coding method that exploits simple binary tests to\nachieve orders of magnitude improvement in coding speed\nas compared to projection based methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "fang2020attention", "year": "2020", "title":"Attention-based Saliency Hashing For Ophthalmic Image Retrieval", "abstract": "<p>Deep hashing methods have been proved to be effective for the large-scale\nmedical image search assisting reference-based diagnosis for clinicians.\nHowever, when the salient region plays a maximal discriminative role in\nophthalmic image, existing deep hashing methods do not fully exploit the\nlearning ability of the deep network to capture the features of salient regions\npointedly. The different grades or classes of ophthalmic images may be share\nsimilar overall performance but have subtle differences that can be\ndifferentiated by mining salient regions. To address this issue, we propose a\nnovel end-to-end network, named Attention-based Saliency Hashing (ASH), for\nlearning compact hash-code to represent ophthalmic images. ASH embeds a\nspatial-attention module to focus more on the representation of salient regions\nand highlights their essential role in differentiating ophthalmic images.\nBenefiting from the spatial-attention module, the information of salient\nregions can be mapped into the hash-code for similarity calculation. In the\ntraining stage, we input the image pairs to share the weights of the network,\nand a pairwise loss is designed to maximize the discriminability of the\nhash-code. In the retrieval stage, ASH obtains the hash-code by inputting an\nimage with an end-to-end manner, then the hash-code is used to similarity\ncalculation to return the most similar images. Extensive experiments on two\ndifferent modalities of ophthalmic image datasets demonstrate that the proposed\nASH can further improve the retrieval performance compared to the\nstate-of-the-art deep hashing methods due to the huge contributions of the\nspatial-attention module.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "fang2021deep", "year": "2021", "title":"Deep Triplet Hashing Network For Case-based Medical Image Retrieval", "abstract": "<p>Deep hashing methods have been shown to be the most efficient approximate\nnearest neighbor search techniques for large-scale image retrieval. However,\nexisting deep hashing methods have a poor small-sample ranking performance for\ncase-based medical image retrieval. The top-ranked images in the returned query\nresults may be as a different class than the query image. This ranking problem\nis caused by classification, regions of interest (ROI), and small-sample\ninformation loss in the hashing space. To address the ranking problem, we\npropose an end-to-end framework, called Attention-based Triplet Hashing (ATH)\nnetwork, to learn low-dimensional hash codes that preserve the classification,\nROI, and small-sample information. We embed a spatial-attention module into the\nnetwork structure of our ATH to focus on ROI information. The spatial-attention\nmodule aggregates the spatial information of feature maps by utilizing\nmax-pooling, element-wise maximum, and element-wise mean operations jointly\nalong the channel axis. The triplet cross-entropy loss can help to map the\nclassification information of images and similarity between images into the\nhash codes. Extensive experiments on two case-based medical datasets\ndemonstrate that our proposed ATH can further improve the retrieval performance\ncompared to the state-of-the-art deep hashing methods and boost the ranking\nperformance for small samples. Compared to the other loss methods, the triplet\ncross-entropy loss can enhance the classification performance and hash\ncode-discriminability</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "feinberg2020chromatic", "year": "2020", "title":"Chromatic Learning For Sparse Datasets", "abstract": "<p>Learning over sparse, high-dimensional data frequently necessitates the use\nof specialized methods such as the hashing trick. In this work, we design a\nhighly scalable alternative approach that leverages the low degree of feature\nco-occurrences present in many practical settings. This approach, which we call\nChromatic Learning (CL), obtains a low-dimensional dense feature representation\nby performing graph coloring over the co-occurrence graph of features—an\napproach previously used as a runtime performance optimization for GBDT\ntraining. This color-based dense representation can be combined with additional\ndense categorical encoding approaches, e.g., submodular feature compression, to\nfurther reduce dimensionality. CL exhibits linear parallelizability and\nconsumes memory linear in the size of the co-occurrence graph. By leveraging\nthe structural properties of the co-occurrence graph, CL can compress sparse\ndatasets, such as KDD Cup 2012, that contain over 50M features down to 1024,\nusing an order of magnitude fewer features than frequency-based truncation and\nthe hashing trick while maintaining the same test error for linear models. This\ncompression further enables the use of deep networks in this wide, sparse\nsetting, where CL similarly has favorable performance compared to existing\nbaselines for budgeted input dimension.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "feng2014learning", "year": "2014", "title":"Learning To Rank Binary Codes", "abstract": "<p>Binary codes have been widely used in vision problems as a compact feature\nrepresentation to achieve both space and time advantages. Various methods have\nbeen proposed to learn data-dependent hash functions which map a feature vector\nto a binary code. However, considerable data information is inevitably lost\nduring the binarization step which also causes ambiguity in measuring sample\nsimilarity using Hamming distance. Besides, the learned hash functions cannot\nbe changed after training, which makes them incapable of adapting to new data\noutside the training data set. To address both issues, in this paper we propose\na flexible bitwise weight learning framework based on the binary codes obtained\nby state-of-the-art hashing methods, and incorporate the learned weights into\nthe weighted Hamming distance computation. We then formulate the proposed\nframework as a ranking problem and leverage the Ranking SVM model to offline\ntackle the weight learning. The framework is further extended to an online mode\nwhich updates the weights at each time new data comes, thereby making it\nscalable to large and dynamic data sets. Extensive experimental results\ndemonstrate significant performance gains of using binary codes with bitwise\nweighting in image retrieval tasks. It is appealing that the online weight\nlearning leads to comparable accuracy with its offline counterpart, which thus\nmakes our approach practical for realistic applications.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "feng2016deep", "year": "2016", "title":"Deep Image Set Hashing", "abstract": "<p>In applications involving matching of image sets, the information from\nmultiple images must be effectively exploited to represent each set.\nState-of-the-art methods use probabilistic distribution or subspace to model a\nset and use specific distance measure to compare two sets. These methods are\nslow to compute and not compact to use in a large scale scenario.\nLearning-based hashing is often used in large scale image retrieval as they\nprovide a compact representation of each sample and the Hamming distance can be\nused to efficiently compare two samples. However, most hashing methods encode\neach image separately and discard knowledge that multiple images in the same\nset represent the same object or person. We investigate the set hashing problem\nby combining both set representation and hashing in a single deep neural\nnetwork. An image set is first passed to a CNN module to extract image\nfeatures, then these features are aggregated using two types of set feature to\ncapture both set specific and database-wide distribution information. The\ncomputed set feature is then fed into a multilayer perceptron to learn a\ncompact binary embedding. Triplet loss is used to train the network by forming\nset similarity relations using class labels. We extensively evaluate our\napproach on datasets used for image matching and show highly competitive\nperformance compared to state-of-the-art methods.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval"] },
{"key": "feng2023towards", "year": "2023", "title":"Towards Efficient Deep Hashing Retrieval Condensing Your Data Via Feature-embedding Matching", "abstract": "<p>The expenses involved in training state-of-the-art deep hashing retrieval\nmodels have witnessed an increase due to the adoption of more sophisticated\nmodels and large-scale datasets. Dataset Distillation (DD) or Dataset\nCondensation(DC) focuses on generating smaller synthetic dataset that retains\nthe original information. Nevertheless, existing DD methods face challenges in\nmaintaining a trade-off between accuracy and efficiency. And the\nstate-of-the-art dataset distillation methods can not expand to all deep\nhashing retrieval methods. In this paper, we propose an efficient condensation\nframework that addresses these limitations by matching the feature-embedding\nbetween synthetic set and real set. Furthermore, we enhance the diversity of\nfeatures by incorporating the strategies of early-stage augmented models and\nmulti-formation. Extensive experiments provide compelling evidence of the\nremarkable superiority of our approach, both in terms of performance and\nefficiency, compared to state-of-the-art baseline methods.</p>\n", "tags": ["ARXIV"] },
{"key": "ferdowsi2017multi", "year": "2017", "title":"A Multi-layer Network Based On Sparse Ternary Codes For Universal Vector Compression", "abstract": "<p>We present the multi-layer extension of the Sparse Ternary Codes (STC) for\nfast similarity search where we focus on the reconstruction of the database\nvectors from the ternary codes. To consider the trade-offs between the\ncompactness of the STC and the quality of the reconstructed vectors, we study\nthe rate-distortion behavior of these codes under different setups. We show\nthat a single-layer code cannot achieve satisfactory results at high rates.\nTherefore, we extend the concept of STC to multiple layers and design the\nML-STC, a codebook-free system that successively refines the reconstruction of\nthe residuals of previous layers. While the ML-STC keeps the sparse ternary\nstructure of the single-layer STC and hence is suitable for fast similarity\nsearch in large-scale databases, we show its superior rate-distortion\nperformance on both model-based synthetic data and public large-scale\ndatabases, as compared to several binary hashing methods.</p>\n", "tags": ["ARXIV"] },
{"key": "ferdowsi2017sparse", "year": "2017", "title":"Sparse Ternary Codes For Similarity Search Have Higher Coding Gain Than Dense Binary Codes", "abstract": "<p>This paper addresses the problem of Approximate Nearest Neighbor (ANN) search\nin pattern recognition where feature vectors in a database are encoded as\ncompact codes in order to speed-up the similarity search in large-scale\ndatabases. Considering the ANN problem from an information-theoretic\nperspective, we interpret it as an encoding, which maps the original feature\nvectors to a less entropic sparse representation while requiring them to be as\ninformative as possible. We then define the coding gain for ANN search using\ninformation-theoretic measures. We next show that the classical approach to\nthis problem, which consists of binarization of the projected vectors is\nsub-optimal. Instead, a properly designed ternary encoding achieves higher\ncoding gains and lower complexity.</p>\n", "tags": ["ARXIV"] },
{"key": "fernandes2020locality", "year": "2020", "title":"Locality Sensitive Hashing With Extended Differential Privacy", "abstract": "<p>Extended differential privacy, a generalization of standard differential\nprivacy (DP) using a general metric, has been widely studied to provide\nrigorous privacy guarantees while keeping high utility. However, existing works\non extended DP are limited to few metrics, such as the Euclidean metric.\nConsequently, they have only a small number of applications, such as\nlocation-based services and document processing. In this paper, we propose a\ncouple of mechanisms providing extended DP with a different metric: angular\ndistance (or cosine distance). Our mechanisms are based on locality sensitive\nhashing (LSH), which can be applied to the angular distance and work well for\npersonal data in a high-dimensional space. We theoretically analyze the privacy\nproperties of our mechanisms, and prove extended DP for input data by taking\ninto account that LSH preserves the original metric only approximately. We\napply our mechanisms to friend matching based on high-dimensional personal data\nwith angular distance in the local model, and evaluate our mechanisms using two\nreal datasets. We show that LDP requires a very large privacy budget and that\nRAPPOR does not work in this application. Then we show that our mechanisms\nenable friend matching with high utility and rigorous privacy guarantees based\non extended DP.</p>\n", "tags": ["AAAI","Independent","LSH"] },
{"key": "fernandez2022active", "year": "2022", "title":"Active Image Indexing", "abstract": "<p>Image copy detection and retrieval from large databases leverage two\ncomponents. First, a neural network maps an image to a vector representation,\nthat is relatively robust to various transformations of the image. Second, an\nefficient but approximate similarity search algorithm trades scalability (size\nand speed) against quality of the search, thereby introducing a source of\nerror. This paper improves the robustness of image copy detection with active\nindexing, that optimizes the interplay of these two components. We reduce the\nquantization loss of a given image representation by making imperceptible\nchanges to the image before its release. The loss is back-propagated through\nthe deep neural network back to the image, under perceptual constraints. These\nmodifications make the image more retrievable. Our experiments show that the\nretrieval and copy detection of activated images is significantly improved. For\ninstance, activation improves by \\(+40\\%\\) the Recall1@1 on various image\ntransformations, and for several popular indexing structures based on product\nquantization and locality sensitivity hashing.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "ferragina2023learned", "year": "2023", "title":"Learned Monotone Minimal Perfect Hashing", "abstract": "<p>A Monotone Minimal Perfect Hash Function (MMPHF) constructed on a set S of\nkeys is a function that maps each key in S to its rank. On keys not in S, the\nfunction returns an arbitrary value. Applications range from databases, search\nengines, data encryption, to pattern-matching algorithms.\n  In this paper, we describe LeMonHash, a new technique for constructing MMPHFs\nfor integers. The core idea of LeMonHash is surprisingly simple and effective:\nwe learn a monotone mapping from keys to their rank via an error-bounded\npiecewise linear model (the PGM-index), and then we solve the collisions that\nmight arise among keys mapping to the same rank estimate by associating small\nintegers with them in a retrieval data structure (BuRR). On synthetic random\ndatasets, LeMonHash needs 34% less space than the next larger competitor, while\nachieving about 16 times faster queries. On real-world datasets, the space\nusage is very close to or much better than the best competitors, while\nachieving up to 19 times faster queries than the next larger competitor. As far\nas the construction of LeMonHash is concerned, we get an improvement by a\nfactor of up to 2, compared to the competitor with the next best space usage.\n  We also investigate the case of keys being variable-length strings,\nintroducing the so-called LeMonHash-VL: it needs space within 13% of the best\ncompetitors while achieving up to 3 times faster queries than the next larger\ncompetitor.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "feydy2020fast", "year": "2020", "title":"Fast Geometric Learning With Symbolic Matrices", "abstract": "<p>Geometric methods rely on tensors that can be encoded using a symbolic formula and data arrays, such as kernel and distance matrices. We present an extension for standard machine learning frameworks that provides comprehensive support for this abstraction on CPUs and GPUs: our toolbox combines a versatile, transparent user interface with fast runtimes and low memory usage. Unlike general purpose acceleration frameworks such as XLA, our library turns generic Python code into binaries whose performances are competitive with state-of-the-art geometric libraries - such as FAISS for nearest neighbor search - with the added benefit of flexibility. We perform an extensive evaluation on a broad class of problems: Gaussian modelling, K-nearest neighbors search, geometric deep learning, non-Euclidean embeddings and optimal transport theory. In practice, for geometric problems that involve 1k to 1M samples in dimension 1 to 100, our library speeds up baseline GPU implementations by up to two orders of magnitude.</p>\n", "tags": ["Deep Learning","NEURIPS","Theory"] },
{"key": "fleischhacker2023invertible", "year": "2023", "title":"Invertible Bloom Lookup Tables With Less Memory And Randomness", "abstract": "<p>In this work we study Invertible Bloom Lookup Tables (IBLTs) with small\nfailure probabilities. IBLTs are highly versatile data structures that have\nfound applications in set reconciliation protocols, error-correcting codes, and\neven the design of advanced cryptographic primitives. For storing \\(n\\) elements\nand ensuring correctness with probability at least \\(1 - \\delta\\), existing IBLT\nconstructions require \\(Ω(n(\\frac{log(1/\\delta)}{log(n)}+1))\\) space and\nthey crucially rely on fully random hash functions.\n  We present new constructions of IBLTs that are simultaneously more space\nefficient and require less randomness. For storing \\(n\\) elements with a failure\nprobability of at most \\(\\delta\\), our data structure only requires\n\\(\\mathcal{O}(n + log(1/\\delta)loglog(1/\\delta))\\) space and\n\\(\\mathcal{O}(log(log(n)/\\delta))\\)-wise independent hash functions.\n  As a key technical ingredient we show that hashing \\(n\\) keys with any \\(k\\)-wise\nindependent hash function \\(h:U \\to [Cn]\\) for some sufficiently large constant\n\\(C\\) guarantees with probability \\(1 - 2^{-Ω(k)}\\) that at least \\(n/2\\) keys\nwill have a unique hash value. Proving this is highly non-trivial as \\(k\\)\napproaches \\(n\\). We believe that the techniques used to prove this statement may\nbe of independent interest.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "fountoulakis2009sharp", "year": "2009", "title":"Sharp Load Thresholds For Cuckoo Hashing", "abstract": "<p>The paradigm of many choices has influenced significantly the design of\nefficient data structures and, most notably, hash tables. Cuckoo hashing is a\ntechnique that extends this concept. There,we are given a table with \\(n\\)\nlocations, and we assume that each location can hold one item. Each item to be\ninserted chooses randomly k&gt;1 locations and has to be placed in any one of\nthem. How much load can cuckoo hashing handle before collisions prevent the\nsuccessful assignment of the available items to the chosen locations? Practical\nevaluations of this method have shown that one can allocate a number of\nelements that is a large proportion of the size of the table, being very close\nto 1 even for small values of k such as 4 or 5.\n  In this paper we show that there is a critical value for this proportion:\nwith high probability, when the amount of available items is below this value,\nthen these can be allocated successfully, but when it exceeds this value, the\nallocation becomes impossible. We give explicitly for each k&gt;1 this critical\nvalue. This answers an open question posed by Mitzenmacher (ESA ‘09) and\nunderpins theoretically the experimental results. Our proofs are based on the\ntranslation of the question into a hypergraph setting, and the study of the\nrelated typical properties of random k-uniform hypergraphs.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "fountoulakis2010insertion", "year": "2010", "title":"On The Insertion Time Of Cuckoo Hashing", "abstract": "<p>Cuckoo hashing is an efficient technique for creating large hash tables with\nhigh space utilization and guaranteed constant access times. There, each item\ncan be placed in a location given by any one out of k different hash functions.\nIn this paper we investigate further the random walk heuristic for inserting in\nan online fashion new items into the hash table. Provided that k &gt; 2 and that\nthe number of items in the table is below (but arbitrarily close) to the\ntheoretically achievable load threshold, we show a polylogarithmic bound for\nthe maximum insertion time that holds with high probability.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "frei2023bounds", "year": "2023", "title":"Bounds For C-ideal Hashing", "abstract": "<p>In this paper, we analyze hashing from a worst-case perspective. To this end,\nwe study a new property of hash families that is strongly related to d-perfect\nhashing, namely c-ideality. On the one hand, this notion generalizes the\ndefinition of perfect hashing, which has been studied extensively; on the other\nhand, it provides a direct link to the notion of c-approximativity. We focus on\nthe usually neglected case where the average load \\alpha is at least 1 and\nprove upper and lower parametrized bounds on the minimal size of c-ideal hash\nfamilies.\n  As an aside, we show how c-ideality helps to analyze the advice complexity of\nhashing. The concept of advice, introduced a decade ago, lets us measure the\ninformation content of an online problem. We prove hashing’s advice complexity\nto be linear in the hash table size.</p>\n", "tags": ["ARXIV"] },
{"key": "freksen2018fully", "year": "2018", "title":"Fully Understanding The Hashing Trick", "abstract": "<p>Feature hashing, also known as {\\em the hashing trick}, introduced by\nWeinberger et al. (2009), is one of the key techniques used in scaling-up\nmachine learning algorithms. Loosely speaking, feature hashing uses a random\nsparse projection matrix \\(A : \\mathbb{R}^n \\to \\mathbb{R}^m\\) (where \\(m \\ll n\\))\nin order to reduce the dimension of the data from \\(n\\) to \\(m\\) while\napproximately preserving the Euclidean norm. Every column of \\(A\\) contains\nexactly one non-zero entry, equals to either \\(-1\\) or \\(1\\).\n  Weinberger et al. showed tail bounds on \\(|Ax|<em>2^2\\). Specifically they\nshowed that for every \\(\\epsilon, \\delta\\), if \\(|x|</em>{\\infty} / |x|<em>2\\) is\nsufficiently small, and \\(m\\) is sufficiently large, then $\\(\\Pr[ \\; |\n\\;|Ax|_2^2 - |x|_2^2\\; | &lt; \\epsilon |x|_2^2 \\;] \\ge 1 - \\delta \\;.\\)\\(\nThese bounds were later extended by Dasgupta \\etal (2010) and most recently\nrefined by Dahlgaard et al. (2017), however, the true nature of the performance\nof this key technique, and specifically the correct tradeoff between the\npivotal parameters \\)|x|</em>{\\infty} / |x|_2, m, \\epsilon, \\delta\\( remained\nan open question.\n  We settle this question by giving tight asymptotic bounds on the exact\ntradeoff between the central parameters, thus providing a complete\nunderstanding of the performance of feature hashing. We complement the\nasymptotic bound with empirical data, which shows that the constants “hiding”\nin the asymptotic notation are, in fact, very close to \\)1$, thus further\nillustrating the tightness of the presented bounds in practice.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "frieze2016insertion", "year": "2016", "title":"On The Insertion Time Of Random Walk Cuckoo Hashing", "abstract": "<p>Cuckoo Hashing is a hashing scheme invented by Pagh and Rodler. It uses\n\\(d\\geq 2\\) distinct hash functions to insert items into the hash table. It has\nbeen an open question for some time as to the expected time for Random Walk\nInsertion to add items. We show that if the number of hash functions \\(d=O(1)\\)\nis sufficiently large, then the expected insertion time is \\(O(1)\\) per item.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "fu2016auto", "year": "2016", "title":"Auto-jacobin Auto-encoder Jacobian Binary Hashing", "abstract": "<p>Binary codes can be used to speed up nearest neighbor search tasks in large\nscale data sets as they are efficient for both storage and retrieval. In this\npaper, we propose a robust auto-encoder model that preserves the geometric\nrelationships of high-dimensional data sets in Hamming space. This is done by\nconsidering a noise-removing function in a region surrounding the manifold\nwhere the training data points lie. This function is defined with the property\nthat it projects the data points near the manifold into the manifold wisely,\nand we approximate this function by its first order approximation. Experimental\nresults show that the proposed method achieves better than state-of-the-art\nresults on three large scale high dimensional data sets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "fu2016efanna", "year": "2016", "title":"EFANNA An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based On Knn Graph", "abstract": "<p>Approximate nearest neighbor (ANN) search is a fundamental problem in many\nareas of data mining, machine learning and computer vision. The performance of\ntraditional hierarchical structure (tree) based methods decreases as the\ndimensionality of data grows, while hashing based methods usually lack\nefficiency in practice. Recently, the graph based methods have drawn\nconsiderable attention. The main idea is that <em>a neighbor of a neighbor is\nalso likely to be a neighbor</em>, which we refer as <em>NN-expansion</em>. These\nmethods construct a \\(k\\)-nearest neighbor (\\(k\\)NN) graph offline. And at online\nsearch stage, these methods find candidate neighbors of a query point in some\nway (\\eg, random selection), and then check the neighbors of these candidate\nneighbors for closer ones iteratively. Despite some promising results, there\nare mainly two problems with these approaches: 1) These approaches tend to\nconverge to local optima. 2) Constructing a \\(k\\)NN graph is time consuming. We\nfind that these two problems can be nicely solved when we provide a good\ninitialization for NN-expansion. In this paper, we propose EFANNA, an extremely\nfast approximate nearest neighbor search algorithm based on \\(k\\)NN Graph. Efanna\nnicely combines the advantages of hierarchical structure based methods and\nnearest-neighbor-graph based methods. Extensive experiments have shown that\nEFANNA outperforms the state-of-art algorithms both on approximate nearest\nneighbor search and approximate nearest neighbor graph construction. To the\nbest of our knowledge, EFANNA is the fastest algorithm so far both on\napproximate nearest neighbor graph construction and approximate nearest\nneighbor search. A library EFANNA based on this research is released on Github.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "fu2018neurons", "year": "2018", "title":"Neurons Merging Layer Towards Progressive Redundancy Reduction For Deep Supervised Hashing", "abstract": "<p>Deep supervised hashing has become an active topic in information retrieval.\nIt generates hashing bits by the output neurons of a deep hashing network.\nDuring binary discretization, there often exists much redundancy between\nhashing bits that degenerates retrieval performance in terms of both storage\nand accuracy. This paper proposes a simple yet effective Neurons Merging Layer\n(NMLayer) for deep supervised hashing. A graph is constructed to represent the\nredundancy relationship between hashing bits that is used to guide the learning\nof a hashing network. Specifically, it is dynamically learned by a novel\nmechanism defined in our active and frozen phases. According to the learned\nrelationship, the NMLayer merges the redundant neurons together to balance the\nimportance of each output neuron. Moreover, multiple NMLayers are progressively\ntrained for a deep hashing network to learn a more compact hashing code from a\nlong redundant code. Extensive experiments on four datasets demonstrate that\nour proposed method outperforms state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "fu2020deep", "year": "2020", "title":"Deep Momentum Uncertainty Hashing", "abstract": "<p>Combinatorial optimization (CO) has been a hot research topic because of its\ntheoretic and practical importance. As a classic CO problem, deep hashing aims\nto find an optimal code for each data from finite discrete possibilities, while\nthe discrete nature brings a big challenge to the optimization process.\nPrevious methods usually mitigate this challenge by binary approximation,\nsubstituting binary codes for real-values via activation functions or\nregularizations. However, such approximation leads to uncertainty between\nreal-values and binary ones, degrading retrieval performance. In this paper, we\npropose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly\nestimates the uncertainty during training and leverages the uncertainty\ninformation to guide the approximation process. Specifically, we model\nbit-level uncertainty via measuring the discrepancy between the output of a\nhashing network and that of a momentum-updated network. The discrepancy of each\nbit indicates the uncertainty of the hashing network to the approximate output\nof that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can\nbe regarded as image-level uncertainty. It embodies the uncertainty of the\nhashing network to the corresponding input image. The hashing bit and image\nwith higher uncertainty are paid more attention during optimization. To the\nbest of our knowledge, this is the first work to study the uncertainty in\nhashing bits. Extensive experiments are conducted on four datasets to verify\nthe superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a\nmillion-scale dataset Clothing1M. Our method achieves the best performance on\nall of the datasets and surpasses existing state-of-the-art methods by a large\nmargin.</p>\n", "tags": ["ARXIV"] },
{"key": "fuad2007extended", "year": "2007", "title":"The Extended Edit Distance Metric", "abstract": "<p>Similarity search is an important problem in information retrieval. This\nsimilarity is based on a distance. Symbolic representation of time series has\nattracted many researchers recently, since it reduces the dimensionality of\nthese high dimensional data objects. We propose a new distance metric that is\napplied to symbolic data objects and we test it on time series data bases in a\nclassification task. We compare it to other distances that are well known in\nthe literature for symbolic data objects. We also prove, mathematically, that\nour distance is metric.</p>\n", "tags": ["Supervised"] },
{"key": "fuad2013abc", "year": "2013", "title":"ABC-SG A New Artificial Bee Colony Algorithm-based Distance Of Sequential Data Using Sigma Grams", "abstract": "<p>The problem of similarity search is one of the main problems in computer\nscience. This problem has many applications in text-retrieval, web search,\ncomputational biology, bioinformatics and others. Similarity between two data\nobjects can be depicted using a similarity measure or a distance metric. There\nare numerous distance metrics in the literature, some are used for a particular\ndata type, and others are more general. In this paper we present a new distance\nmetric for sequential data which is based on the sum of n-grams. The novelty of\nour distance is that these n-grams are weighted using artificial bee colony; a\nrecent optimization algorithm based on the collective intelligence of a swarm\nof bees on their search for nectar. This algorithm has been used in optimizing\na large number of numerical problems. We validate the new distance\nexperimentally.</p>\n", "tags": ["ARXIV","Text Retrieval"] },
{"key": "fuentes2021sketch", "year": "2021", "title":"Sketch-qnet A Quadruplet Convnet For Color Sketch-based Image Retrieval", "abstract": "<p>Architectures based on siamese networks with triplet loss have shown\noutstanding performance on the image-based similarity search problem. This\napproach attempts to discriminate between positive (relevant) and negative\n(irrelevant) items. However, it undergoes a critical weakness. Given a query,\nit cannot discriminate weakly relevant items, for instance, items of the same\ntype but different color or texture as the given query, which could be a\nserious limitation for many real-world search applications. Therefore, in this\nwork, we present a quadruplet-based architecture that overcomes the\naforementioned weakness. Moreover, we present an instance of this quadruplet\nnetwork, which we call Sketch-QNet, to deal with the color sketch-based image\nretrieval (CSBIR) problem, achieving new state-of-the-art results.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "fuentespineda2015sampled", "year": "2015", "title":"Sampled Weighted Min-hashing For Large-scale Topic Mining", "abstract": "<p>We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to\nautomatically mine topics from large-scale corpora. SWMH generates multiple\nrandom partitions of the corpus vocabulary based on term co-occurrence and\nagglomerates highly overlapping inter-partition cells to produce the mined\ntopics. While other approaches define a topic as a probabilistic distribution\nover a vocabulary, SWMH topics are ordered subsets of such vocabulary.\nInterestingly, the topics mined by SWMH underlie themes from the corpus at\ndifferent levels of granularity. We extensively evaluate the meaningfulness of\nthe mined topics both qualitatively and quantitatively on the NIPS (1.7 K\ndocuments), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora.\nAdditionally, we compare the quality of SWMH with Online LDA topics for\ndocument representation in classification.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "fusy2023count", "year": "2023", "title":"Count-min Sketch With Variable Number Of Hash Functions An Experimental Study", "abstract": "<p>Conservative Count-Min, an improved version of Count-Min sketch [Cormode,\nMuthukrishnan 2005], is an online-maintained hashing-based data structure\nsummarizing element frequency information without storing elements themselves.\nAlthough several works attempted to analyze the error that can be made by\nCount-Min, the behavior of this data structure remains poorly understood. In\n[Fusy, Kucherov 2022], we demonstrated that under the uniform distribution of\ninput elements, the error of conservative Count-Min follows two distinct\nregimes depending on its load factor.\n  In this work, we provide a series of experimental results providing new\ninsights into the behavior of conservative Count-Min. Our contributions can be\nseen as twofold. On one hand, we provide a detailed experimental analysis of\nthe behavior of Count-Min sketch in different regimes and under several\nrepresentative probability distributions of input elements. On the other hand,\nwe demonstrate improvements that can be made by assigning a variable number of\nhash functions to different elements. This includes, in particular, reduced\nspace of the data structure while still supporting a small error.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "gagie2013compressed", "year": "2013", "title":"Compressed Spaced Suffix Arrays", "abstract": "<p>Spaced seeds are important tools for similarity search in bioinformatics, and\nusing several seeds together often significantly improves their performance.\nWith existing approaches, however, for each seed we keep a separate linear-size\ndata structure, either a hash table or a spaced suffix array (SSA). In this\npaper we show how to compress SSAs relative to normal suffix arrays (SAs) and\nstill support fast random access to them. We first prove a theoretical upper\nbound on the space needed to store an SSA when we already have the SA. We then\npresent experiments indicating that our approach works even better in practice.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "gajic2019bag", "year": "2019", "title":"Bag Of Negatives For Siamese Architectures", "abstract": "<p>Training a Siamese architecture for re-identification with a large number of\nidentities is a challenging task due to the difficulty of finding relevant\nnegative samples efficiently. In this work we present Bag of Negatives (BoN), a\nmethod for accelerated and improved training of Siamese networks that scales\nwell on datasets with a very large number of identities. BoN is an efficient\nand loss-independent method, able to select a bag of high quality negatives,\nbased on a novel online hashing strategy.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "gan2014image", "year": "2014", "title":"Image Classification With A Deep Network Model Based On Compressive Sensing", "abstract": "<p>To simplify the parameter of the deep learning network, a cascaded\ncompressive sensing model “CSNet” is implemented for image classification.\nFirstly, we use cascaded compressive sensing network to learn feature from the\ndata. Secondly, CSNet generates the feature by binary hashing and block-wise\nhistograms. Finally, a linear SVM classifier is used to classify these\nfeatures. The experiments on the MNIST dataset indicate that higher\nclassification accuracy can be obtained by this algorithm.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "gan2023binary", "year": "2023", "title":"Binary Embedding-based Retrieval At Tencent", "abstract": "<p>Large-scale embedding-based retrieval (EBR) is the cornerstone of\nsearch-related industrial applications. Given a user query, the system of EBR\naims to identify relevant information from a large corpus of documents that may\nbe tens or hundreds of billions in size. The storage and computation turn out\nto be expensive and inefficient with massive documents and high concurrent\nqueries, making it difficult to further scale up. To tackle the challenge, we\npropose a binary embedding-based retrieval (BEBR) engine equipped with a\nrecurrent binarization algorithm that enables customized bits per dimension.\nSpecifically, we compress the full-precision query and document embeddings,\nformulated as float vectors in general, into a composition of multiple binary\nvectors using a lightweight transformation model with residual multilayer\nperception (MLP) blocks. We can therefore tailor the number of bits for\ndifferent applications to trade off accuracy loss and cost savings.\nImportantly, we enable task-agnostic efficient training of the binarization\nmodel using a new embedding-to-embedding strategy. We also exploit the\ncompatible training of binary embeddings so that the BEBR engine can support\nindexing among multiple embedding versions within a unified system. To further\nrealize efficient search, we propose Symmetric Distance Calculation (SDC) to\nachieve lower response time than Hamming codes. We successfully employed the\nintroduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World,\netc. The binarization algorithm can be seamlessly generalized to various tasks\nwith multiple modalities. Extensive experiments on offline benchmarks and\nonline A/B tests demonstrate the efficiency and effectiveness of our method,\nsignificantly saving 30%~50% index costs with almost no loss of accuracy at the\nsystem level.</p>\n", "tags": ["ARXIV"] },
{"key": "gao2019beyond", "year": "2019", "title":"Beyond Product Quantization Deep Progressive Quantization For Image Retrieval", "abstract": "<p>Product Quantization (PQ) has long been a mainstream for generating an\nexponentially large codebook at very low memory/time cost. Despite its success,\nPQ is still tricky for the decomposition of high-dimensional vector space, and\nthe retraining of model is usually unavoidable when the code length changes. In\nthis work, we propose a deep progressive quantization (DPQ) model, as an\nalternative to PQ, for large scale image retrieval. DPQ learns the quantization\ncodes sequentially and approximates the original feature space progressively.\nTherefore, we can train the quantization codes with different code lengths\nsimultaneously. Specifically, we first utilize the label information for\nguiding the learning of visual features, and then apply several quantization\nblocks to progressively approach the visual features. Each quantization block\nis designed to be a layer of a convolutional neural network, and the whole\nframework can be trained in an end-to-end manner. Experimental results on the\nbenchmark datasets show that our model significantly outperforms the\nstate-of-the-art for image retrieval. Our model is trained once for different\ncode lengths and therefore requires less computation time. Additional ablation\nstudy demonstrates the effect of each component of our proposed model. Our code\nis released at https://github.com/cfm-uestc/DPQ.</p>\n", "tags": ["Has Code","Image Retrieval","Quantisation","Supervised"] },
{"key": "gao2021backdoor", "year": "2021", "title":"Backdoor Attack On Hash-based Image Retrieval Via Clean-label Data Poisoning", "abstract": "<p>A backdoored deep hashing model is expected to behave normally on original\nquery images and return the images with the target label when a specific\ntrigger pattern presents. To this end, we propose the confusing\nperturbations-induced backdoor attack (CIBA). It injects a small number of\npoisoned images with the correct label into the training data, which makes the\nattack hard to be detected. To craft the poisoned images, we first propose the\nconfusing perturbations to disturb the hashing code learning. As such, the\nhashing model can learn more about the trigger. The confusing perturbations are\nimperceptible and generated by optimizing the intra-class dispersion and\ninter-class shift in the Hamming space. We then employ the targeted adversarial\npatch as the backdoor trigger to improve the attack performance. We have\nconducted extensive experiments to verify the effectiveness of our proposed\nCIBA. Our code is available at https://github.com/KuofengGao/CIBA.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Supervised"] },
{"key": "gao2022long", "year": "2022", "title":"Long-tail Cross Modal Hashing", "abstract": "<p>Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced\ndata, while imbalanced data with long-tail distribution is more general in\nreal-world. Several long-tail hashing methods have been proposed but they can\nnot adapt for multi-modal data, due to the complex interplay between labels and\nindividuality and commonality information of multi-modal data. Furthermore, CMH\nmethods mostly mine the commonality of multi-modal data to learn hash codes,\nwhich may override tail labels encoded by the individuality of respective\nmodalities. In this paper, we propose LtCMH (Long-tail CMH) to handle\nimbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the\nindividuality and commonality of different modalities by minimizing the\ndependency between the individuality of respective modalities and by enhancing\nthe commonality of these modalities. Then it dynamically combines the\nindividuality and commonality with direct features extracted from respective\nmodalities to create meta features that enrich the representation of tail\nlabels, and binaries meta features to generate hash codes. LtCMH significantly\noutperforms state-of-the-art baselines on long-tail datasets and holds a better\n(or comparable) performance on datasets with balanced labels.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "gao2023high", "year": "2023", "title":"High-dimensional Approximate Nearest Neighbor Search With Reliable And Efficient Distance Comparison Operations", "abstract": "<p>Approximate K nearest neighbor (AKNN) search is a fundamental and challenging\nproblem. We observe that in high-dimensional space, the time consumption of\nnearly all AKNN algorithms is dominated by that of the distance comparison\noperations (DCOs). For each operation, it scans full dimensions of an object\nand thus, runs in linear time wrt the dimensionality. To speed it up, we\npropose a randomized algorithm named ADSampling which runs in logarithmic time\nwrt to the dimensionality for the majority of DCOs and succeeds with high\nprobability. In addition, based on ADSampling we develop one general and two\nalgorithm-specific techniques as plugins to enhance existing AKNN algorithms.\nBoth theoretical and empirical studies confirm that: (1) our techniques\nintroduce nearly no accuracy loss and (2) they consistently improve the\nefficiency.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "gao2024practical", "year": "2024", "title":"Practical And Asymptotically Optimal Quantization Of High-dimensional Vectors In Euclidean Space For Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor (ANN) query in high-dimensional Euclidean space\nis a key operator in database systems. For this query, quantization is a\npopular family of methods developed for compressing vectors and reducing memory\nconsumption. Recently, a method called RaBitQ achieves the state-of-the-art\nperformance among these methods. It produces better empirical performance in\nboth accuracy and efficiency when using the same compression rate and provides\nrigorous theoretical guarantees. However, the method is only designed for\ncompressing vectors at high compression rates (32x) and lacks support for\nachieving higher accuracy by using more space. In this paper, we introduce a\nnew quantization method to address this limitation by extending RaBitQ. The new\nmethod inherits the theoretical guarantees of RaBitQ and achieves the\nasymptotic optimality in terms of the trade-off between space and error bounds\nas to be proven in this study. Additionally, we present efficient\nimplementations of the method, enabling its application to ANN queries to\nreduce both space and time consumption. Extensive experiments on real-world\ndatasets confirm that our method consistently outperforms the state-of-the-art\nbaselines in both accuracy and efficiency when using the same amount of memory.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "garciamorato2024general", "year": "2024", "title":"A General Framework For Distributed Approximate Similarity Search With Arbitrary Distances", "abstract": "<p>Similarity search is a central problem in domains such as information\nmanagement and retrieval or data analysis. Many similarity search algorithms\nare designed or specifically adapted to metric distances. Thus, they are\nunsuitable for alternatives like the cosine distance, which has become quite\ncommon, for example, with embeddings and in text mining. This paper presents\nGDASC (General Distributed Approximate Similarity search with Clustering), a\ngeneral framework for distributed approximate similarity search that accepts\narbitrary distances. This framework can build a multilevel index structure, by\nselecting a clustering algorithm, the number of prototypes in each cluster and\nany arbitrary distance function. As a result, this framework effectively\novercomes the limitation of using metric distances and can address situations\ninvolving cosine similarity or other non-standard similarity measures.\nExperimental results using k-medoids clustering in GDASC with real datasets\nconfirm the applicability of this approach for approximate similarity search,\nimproving the performance of extant algorithms for this purpose.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "garcíamarco2022free", "year": "2022", "title":"Free Resolutions And Generalized Hamming Weights Of Binary Linear Codes", "abstract": "<p>In this work, we explore the relationship between free resolution of some\nmonomial ideals and Generalized Hamming Weights (GHWs) of binary codes. More\nprecisely, we look for a structure smaller than the set of codewords of minimal\nsupport that provides us some information about the GHWs. We prove that the\nfirst and second generalized Hamming weight of a binary linear code can be\ncomputed (by means of a graded free resolution) from a set of monomials\nassociated to a binomial ideal related with the code. Moreover, the remaining\nweights are bounded by the Betti numbers for that set.</p>\n", "tags": ["ARXIV"] },
{"key": "garg2017kernelized", "year": "2017", "title":"Kernelized Hashcode Representations For Relation Extraction", "abstract": "<p>Kernel methods have produced state-of-the-art results for a number of NLP\ntasks such as relation extraction, but suffer from poor scalability due to the\nhigh cost of computing kernel similarities between natural language structures.\nA recently proposed technique, kernelized locality-sensitive hashing (KLSH),\ncan significantly reduce the computational cost, but is only applicable to\nclassifiers operating on kNN graphs. Here we propose to use random subspaces of\nKLSH codes for efficiently constructing an explicit representation of NLP\nstructures suitable for general classification methods. Further, we propose an\napproach for optimizing the KLSH model for classification problems by\nmaximizing an approximation of mutual information between the KLSH codes\n(feature vectors) and the class labels. We evaluate the proposed approach on\nbiomedical relation extraction datasets, and observe significant and robust\nimprovements in accuracy w.r.t. state-of-the-art classifiers, along with\ndrastic (orders-of-magnitude) speedup compared to conventional kernel methods.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "garg2019nearly", "year": "2019", "title":"Nearly-unsupervised Hashcode Representations For Relation Extraction", "abstract": "<p>Recently, kernelized locality sensitive hashcodes have been successfully\nemployed as representations of natural language text, especially showing high\nrelevance to biomedical relation extraction tasks. In this paper, we propose to\noptimize the hashcode representations in a nearly unsupervised manner, in which\nwe only use data points, but not their class labels, for learning. The\noptimized hashcode representations are then fed to a supervised classifier\nfollowing the prior work. This nearly unsupervised approach allows fine-grained\noptimization of each hash function, which is particularly suitable for building\nhashcode representations generalizing from a training set to a test set. We\nempirically evaluate the proposed approach for biomedical relation extraction\ntasks, obtaining significant accuracy improvements w.r.t. state-of-the-art\nsupervised and semi-supervised approaches.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "garg2020fast", "year": "2020", "title":"Fast Compact And Highly Scalable Visual Place Recognition Through Sequence-based Matching Of Overloaded Representations", "abstract": "<p>Visual place recognition algorithms trade off three key characteristics:\ntheir storage footprint, their computational requirements, and their resultant\nperformance, often expressed in terms of recall rate. Significant prior work\nhas investigated highly compact place representations, sub-linear computational\nscaling and sub-linear storage scaling techniques, but have always involved a\nsignificant compromise in one or more of these regards, and have only been\ndemonstrated on relatively small datasets. In this paper we present a novel\nplace recognition system which enables for the first time the combination of\nultra-compact place representations, near sub-linear storage scaling and\nextremely lightweight compute requirements. Our approach exploits the\ninherently sequential nature of much spatial data in the robotics domain and\ninverts the typical target criteria, through intentionally coarse scalar\nquantization-based hashing that leads to more collisions but is resolved by\nsequence-based matching. For the first time, we show how effective place\nrecognition rates can be achieved on a new very large 10 million place dataset,\nrequiring only 8 bytes of storage per place and 37K unitary operations to\nachieve over 50% recall for matching a sequence of 100 frames, where a\nconventional state-of-the-art approach both consumes 1300 times more compute\nand fails catastrophically. We present analysis investigating the effectiveness\nof our hashing overload approach under varying sizes of quantized vector\nlength, comparison of near miss matches with the actual match selections and\ncharacterise the effect of variance re-scaling of data on quantization.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "gaskill2019bitwise", "year": "2019", "title":"The Bitwise Hashing Trick For Personalized Search", "abstract": "<p>Many real world problems require fast and efficient lexical comparison of\nlarge numbers of short text strings. Search personalization is one such domain.\nWe introduce the use of feature bit vectors using the hashing trick for\nimproving relevance in personalized search and other personalization\napplications. We present results of several lexical hashing and comparison\nmethods. These methods are applied to a user’s historical behavior and are used\nto predict future behavior. Using a single bit per dimension instead of\nfloating point results in an order of magnitude decrease in data structure\nsize, while preserving or even improving quality. We use real data to simulate\na search personalization task. A simple method for combining bit vectors\ndemonstrates an order of magnitude improvement in compute time on the task with\nonly a small decrease in accuracy.</p>\n", "tags": [] },
{"key": "gattupalli2018weakly", "year": "2018", "title":"Weakly Supervised Deep Image Hashing Through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised\nlearning problems that utilize images and label information to learn the binary\nhash codes. However, large-scale labeled image data is expensive to obtain,\nthus imposing a restriction on the usage of such algorithms. On the other hand,\nunlabelled image data is abundant due to the existence of many Web image\nrepositories. Such Web images may often come with images tags that contain\nuseful information, although raw tags, in general, do not readily lead to\nsemantic labels. Motivated by this scenario, we formulate the problem of\nsemantic image hashing as a weakly-supervised learning problem. We utilize the\ninformation contained in the user-generated tags associated with the images to\nlearn the hash codes. More specifically, we extract the word2vec semantic\nembeddings of the tags and use the information contained in them for\nconstraining the learning. Accordingly, we name our model Weakly Supervised\nDeep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of\nsemantic image retrieval and is compared against several state-of-art models.\nResults show that our approach sets a new state-of-art in the area of weekly\nsupervised image hashing.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised","Weakly Supervised"] },
{"key": "gattupalli2024weakly", "year": "2024", "title":"Weakly Supervised Deep Image Hashing Through Tag Embeddings", "abstract": "<p>Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels.\nMotivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning.\nAccordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised","Weakly Supervised"] },
{"key": "gawrychowski2011pattern", "year": "2011", "title":"Pattern Matching In Lempel-ziv Compressed Strings Fast Simple And Deterministic", "abstract": "<p>Countless variants of the Lempel-Ziv compression are widely used in many\nreal-life applications. This paper is concerned with a natural modification of\nthe classical pattern matching problem inspired by the popularity of such\ncompression methods: given an uncompressed pattern s[1..m] and a Lempel-Ziv\nrepresentation of a string t[1..N], does s occur in t? Farach and Thorup gave a\nrandomized O(nlog^2(N/n)+m) time solution for this problem, where n is the size\nof the compressed representation of t. We improve their result by developing a\nfaster and fully deterministic O(nlog(N/n)+m) time algorithm with the same\nspace complexity. Note that for highly compressible texts, log(N/n) might be of\norder n, so for such inputs the improvement is very significant. A (tiny)\nfragment of our method can be used to give an asymptotically optimal solution\nfor the substring hashing problem considered by Farach and Muthukrishnan.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ge2024graph", "year": "2024", "title":"Graph Cuts For Supervised Binary Coding", "abstract": "<p>Learning short binary codes is challenged by the inherent discrete\nnature of the problem. The graph cuts algorithm is a well-studied\ndiscrete label assignment solution in computer vision, but has not yet\nbeen applied to solve the binary coding problems. This is partially because\nit was unclear how to use it to learn the encoding (hashing) functions\nfor out-of-sample generalization. In this paper, we formulate supervised\nbinary coding as a single optimization problem that involves both\nthe encoding functions and the binary label assignment. Then we apply\nthe graph cuts algorithm to address the discrete optimization problem\ninvolved, with no continuous relaxation. This method, named as Graph\nCuts Coding (GCC), shows competitive results in various datasets.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "gebre2024pfeed", "year": "2024", "title":"Pfeed Generating Near Real-time Personalized Feeds Using Precomputed Embedding Similarities", "abstract": "<p>In personalized recommender systems, embeddings are often used to encode\ncustomer actions and items, and retrieval is then performed in the embedding\nspace using approximate nearest neighbor search. However, this approach can\nlead to two challenges: 1) user embeddings can restrict the diversity of\ninterests captured and 2) the need to keep them up-to-date requires an\nexpensive, real-time infrastructure. In this paper, we propose a method that\novercomes these challenges in a practical, industrial setting. The method\ndynamically updates customer profiles and composes a feed every two minutes,\nemploying precomputed embeddings and their respective similarities. We tested\nand deployed this method to personalise promotional items at Bol, one of the\nlargest e-commerce platforms of the Netherlands and Belgium. The method\nenhanced customer engagement and experience, leading to a significant 4.9%\nuplift in conversions.</p>\n", "tags": ["ARXIV"] },
{"key": "gelbhart2020discrete", "year": "2020", "title":"Discrete Few-shot Learning For Pan Privacy", "abstract": "<p>In this paper we present the first baseline results for the task of few-shot\nlearning of discrete embedding vectors for image recognition. Few-shot learning\nis a highly researched task, commonly leveraged by recognition systems that are\nresource constrained to train on a small number of images per class. Few-shot\nsystems typically store a continuous embedding vector of each class, posing a\nrisk to privacy where system breaches or insider threats are a concern. Using\ndiscrete embedding vectors, we devise a simple cryptographic protocol, which\nuses one-way hash functions in order to build recognition systems that do not\nstore their users’ embedding vectors directly, thus providing the guarantee of\ncomputational pan privacy in a practical and wide-spread setting.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "geng2018regularizing", "year": "2018", "title":"Regularizing Deep Hashing Networks Using GAN Generated Fake Images", "abstract": "<p>Recently, deep-networks-based hashing (deep hashing) has become a leading\napproach for large-scale image retrieval. It aims to learn a compact bitwise\nrepresentation for images via deep networks, so that similar images are mapped\nto nearby hash codes. Since a deep network model usually has a large number of\nparameters, it may probably be too complicated for the training data we have,\nleading to model over-fitting. To address this issue, in this paper, we propose\na simple two-stage pipeline to learn deep hashing models, by regularizing the\ndeep hashing networks using fake images. The first stage is to generate fake\nimages from the original training set without extra data, via a generative\nadversarial network (GAN). In the second stage, we propose a deep architec-\nture to learn hash functions, in which we use a maximum-entropy based loss to\nincorporate the newly created fake images by the GAN. We show that this loss\nacts as a strong regularizer of the deep architecture, by penalizing\nlow-entropy output hash codes. This loss can also be interpreted as a model\nensemble by simultaneously training many network models with massive weight\nsharing but over different training sets. Empirical evaluation results on\nseveral benchmark datasets show that the proposed method has superior\nperformance gains over state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","GAN","Image Retrieval","Supervised"] },
{"key": "geva2012topsig", "year": "2012", "title":"Topsig Topology Preserving Document Signatures", "abstract": "<p>Performance comparisons between File Signatures and Inverted Files for text\nretrieval have previously shown several significant shortcomings of file\nsignatures relative to inverted files. The inverted file approach underpins\nmost state-of-the-art search engine algorithms, such as Language and\nProbabilistic models. It has been widely accepted that traditional file\nsignatures are inferior alternatives to inverted files. This paper describes\nTopSig, a new approach to the construction of file signatures. Many advances in\nsemantic hashing and dimensionality reduction have been made in recent times,\nbut these were not so far linked to general purpose, signature file based,\nsearch engines. This paper introduces a different signature file approach that\nbuilds upon and extends these recent advances. We are able to demonstrate\nsignificant improvements in the performance of signature file based indexing\nand retrieval, performance that is comparable to that of state of the art\ninverted file based systems, including Language models and BM25. These findings\nsuggest that file signatures offer a viable alternative to inverted files in\nsuitable settings and from the theoretical perspective it positions the file\nsignatures model in the class of Vector Space retrieval models.</p>\n", "tags": ["ARXIV","Text Retrieval","Unsupervised"] },
{"key": "ghaemmaghami2022learning", "year": "2022", "title":"Learning To Collide Recommendation System Model Compression With Learned Hash Functions", "abstract": "<p>A key characteristic of deep recommendation models is the immense memory\nrequirements of their embedding tables. These embedding tables can often reach\nhundreds of gigabytes which increases hardware requirements and training cost.\nA common technique to reduce model size is to hash all of the categorical\nvariable identifiers (ids) into a smaller space. This hashing reduces the\nnumber of unique representations that must be stored in the embedding table;\nthus decreasing its size. However, this approach introduces collisions between\nsemantically dissimilar ids that degrade model quality. We introduce an\nalternative approach, Learned Hash Functions, which instead learns a new\nmapping function that encourages collisions between semantically similar ids.\nWe derive this learned mapping from historical data and embedding access\npatterns. We experiment with this technique on a production model and find that\na mapping informed by the combination of access frequency and a learned low\ndimension embedding is the most effective. We demonstrate a small improvement\nrelative to the hashing trick and other collision related compression\ntechniques. This is ongoing work that explores the impact of categorical id\ncollisions on recommendation model quality and how those collisions may be\ncontrolled to improve model performance.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ghashami2015binary", "year": "2015", "title":"Binary Coding In Stream", "abstract": "<p>Big data is becoming ever more ubiquitous, ranging over massive video\nrepositories, document corpuses, image sets and Internet routing history.\nProximity search and clustering are two algorithmic primitives fundamental to\ndata analysis, but suffer from the “curse of dimensionality” on these gigantic\ndatasets. A popular attack for this problem is to convert object\nrepresentations into short binary codewords, while approximately preserving\nnear neighbor structure. However, there has been limited research on\nconstructing codewords in the “streaming” or “online” settings often applicable\nto this scale of data, where one may only make a single pass over data too\nmassive to fit in local memory.\n  In this paper, we apply recent advances in matrix sketching techniques to\nconstruct binary codewords in both streaming and online setting. Our\nexperimental results compete outperform several of the most popularly used\nalgorithms, and we prove theoretical guarantees on performance in the streaming\nsetting under mild assumptions on the data and randomness of the training set.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "giakkoupis2020cluster", "year": "2020", "title":"Cluster-and-conquer When Randomness Meets Graph Locality", "abstract": "<p>K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining\nand machine-learning applications. Some of the most efficient KNN graph\nalgorithms are incremental and local: they start from a random graph, which\nthey incrementally improve by traversing neighbors-of-neighbors links.\nParadoxically, this random start is also one of the key weaknesses of these\nalgorithms: nodes are initially connected to dissimilar neighbors, that lie far\naway according to the similarity metric. As a result, incremental algorithms\nmust first laboriously explore spurious potential neighbors before they can\nidentify similar nodes, and start converging. In this paper, we remove this\ndrawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts\nthe starting configuration of greedy algorithms thanks to a novel lightweight\nclustering mechanism, dubbed FastRandomHash. FastRandomHash leverages\nrandom-ness and recursion to pre-cluster similar nodes at a very low cost. Our\nextensive evaluation on real datasets shows that Cluster-and-Conquer\nsignificantly outperforms existing approaches, including LSH, yielding\nspeed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN\nquality.</p>\n", "tags": ["ARXIV","Graph","Independent","LSH"] },
{"key": "gillard2023unified", "year": "2023", "title":"Unified Functional Hashing In Automatic Machine Learning", "abstract": "<p>The field of Automatic Machine Learning (AutoML) has recently attained\nimpressive results, including the discovery of state-of-the-art machine\nlearning solutions, such as neural image classifiers. This is often done by\napplying an evolutionary search method, which samples multiple candidate\nsolutions from a large space and evaluates the quality of each candidate\nthrough a long training process. As a result, the search tends to be slow. In\nthis paper, we show that large efficiency gains can be obtained by employing a\nfast unified functional hash, especially through the functional equivalence\ncaching technique, which we also present. The central idea is to detect by\nhashing when the search method produces equivalent candidates, which occurs\nvery frequently, and this way avoid their costly re-evaluation. Our hash is\n“functional” in that it identifies equivalent candidates even if they were\nrepresented or coded differently, and it is “unified” in that the same\nalgorithm can hash arbitrary representations; e.g. compute graphs, imperative\ncode, or lambda functions. As evidence, we show dramatic improvements on\nmultiple AutoML domains, including neural architecture search and algorithm\ndiscovery. Finally, we consider the effect of hash collisions, evaluation\nnoise, and search distribution through empirical analysis. Altogether, we hope\nthis paper may serve as a guide to hashing techniques in AutoML.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "gillick2018end", "year": "2018", "title":"End-to-end Retrieval In Continuous Space", "abstract": "<p>Most text-based information retrieval (IR) systems index objects by words or\nphrases. These discrete systems have been augmented by models that use\nembeddings to measure similarity in continuous space. But continuous-space\nmodels are typically used just to re-rank the top candidates. We consider the\nproblem of end-to-end continuous retrieval, where standard approximate nearest\nneighbor (ANN) search replaces the usual discrete inverted index, and rely\nentirely on distances between learned embeddings. By training simple models\nspecifically for retrieval, with an appropriate model architecture, we improve\non a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval\ntasks. We also discuss the problem of evaluation for retrieval systems, and\nshow how to modify existing pairwise similarity datasets for this purpose.</p>\n", "tags": ["ARXIV"] },
{"key": "gilreath2004hash", "year": "2004", "title":"Hash Sort A Linear Time Complexity Multiple-dimensional Sort Algorithm", "abstract": "<p>Sorting and hashing are two completely different concepts in computer\nscience, and appear mutually exclusive to one another. Hashing is a search\nmethod using the data as a key to map to the location within memory, and is\nused for rapid storage and retrieval. Sorting is a process of organizing data\nfrom a random permutation into an ordered arrangement, and is a common activity\nperformed frequently in a variety of applications.\n  Almost all conventional sorting algorithms work by comparison, and in doing\nso have a linearithmic greatest lower bound on the algorithmic time complexity.\nAny improvement in the theoretical time complexity of a sorting algorithm can\nresult in overall larger gains in implementation performance.. A gain in\nalgorithmic performance leads to much larger gains in speed for the application\nthat uses the sort algorithm. Such a sort algorithm needs to use an alternative\nmethod for ordering the data than comparison, to exceed the linearithmic time\ncomplexity boundary on algorithmic performance.\n  The hash sort is a general purpose non-comparison based sorting algorithm by\nhashing, which has some interesting features not found in conventional sorting\nalgorithms. The hash sort asymptotically outperforms the fastest traditional\nsorting algorithm, the quick sort. The hash sort algorithm has a linear time\ncomplexity factor – even in the worst case. The hash sort opens an area for\nfurther work and investigation into alternative means of sorting.</p>\n", "tags": ["Independent"] },
{"key": "gionis2024similarity", "year": "2024", "title":"Similarity Search In High Dimensions Via Hashing", "abstract": "<p>The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately,\nall known techniques for solving this problem fall prey to the curse of dimensionality. That is, the data structures scale poorly with data dimensionality;\nin fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should suffice for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our\nmethod gives significant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition.\nExperimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).</p>\n", "tags": ["ARXIV"] },
{"key": "girish2023shacira", "year": "2023", "title":"SHACIRA Scalable Hash-grid Compression For Implicit Neural Representations", "abstract": "<p>Implicit Neural Representations (INR) or neural fields have emerged as a\npopular framework to encode multimedia signals such as images and radiance\nfields while retaining high-quality. Recently, learnable feature grids proposed\nby Instant-NGP have allowed significant speed-up in the training as well as the\nsampling of INRs by replacing a large neural network with a multi-resolution\nlook-up table of feature vectors and a much smaller neural network. However,\nthese feature grids come at the expense of large memory consumption which can\nbe a bottleneck for storage and streaming applications. In this work, we\npropose SHACIRA, a simple yet effective task-agnostic framework for compressing\nsuch feature grids with no additional post-hoc pruning/quantization stages. We\nreparameterize feature grids with quantized latent weights and apply entropy\nregularization in the latent space to achieve high levels of compression across\nvarious domains. Quantitative and qualitative results on diverse datasets\nconsisting of images, videos, and radiance fields, show that our approach\noutperforms existing INR approaches without the need for any large datasets or\ndomain-specific heuristics. Our project page is available at\nhttp://shacira.github.io .</p>\n", "tags": ["ARXIV","Has Code","Quantisation","Supervised"] },
{"key": "gomesgonçalves2018geometry", "year": "2018", "title":"Geometry And Clustering With Metrics Derived From Separable Bregman Divergences", "abstract": "<p>Separable Bregman divergences induce Riemannian metric spaces that are\nisometric to the Euclidean space after monotone embeddings. We investigate\nfixed rate quantization and its codebook Voronoi diagrams, and report on\nexperimental performances of partition-based, hierarchical, and soft clustering\nalgorithms with respect to these Riemann-Bregman distances.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "goncalves2023geometric", "year": "2023", "title":"Geometric Covering Using Random Fields", "abstract": "<p>A set of vectors \\(S \\subseteq \\mathbb{R}^d\\) is\n\\((k_1,\\epsilon)\\)-clusterable if there are \\(k_1\\) balls of radius\n\\(\\epsilon\\) that cover \\(S\\). A set of vectors \\(S \\subseteq \\mathbb{R}^d\\) is\n\\((k_2,\\delta)\\)-far from being clusterable if there are at least \\(k_2\\) vectors\nin \\(S\\), with all pairwise distances at least \\(\\delta\\). We propose a\nprobabilistic algorithm to distinguish between these two cases. Our algorithm\nreaches a decision by only looking at the extreme values of a scalar valued\nhash function, defined by a random field, on \\(S\\); hence, it is especially\nsuitable in distributed and online settings. An important feature of our method\nis that the algorithm is oblivious to the number of vectors: in the online\nsetting, for example, the algorithm stores only a constant number of scalars,\nwhich is independent of the stream length.\n  We introduce random field hash functions, which are a key ingredient in our\nparadigm. Random field hash functions generalize locality-sensitive hashing\n(LSH). In addition to the LSH requirement that <code class=\"language-plaintext highlighter-rouge\">nearby vectors are hashed to\nsimilar values\", our hash function also guarantees that the</code>hash values are\n(nearly) independent random variables for distant vectors”. We formulate\nnecessary conditions for the kernels which define the random fields applied to\nour problem, as well as a measure of kernel optimality, for which we provide a\nbound. Then, we propose a method to construct kernels which approximate the\noptimal one.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "gong2012angular", "year": "2012", "title":"Angular Quantization-based Binary Codes For Fast Similarity Search", "abstract": "<p>This paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data. Such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.  Also, cosine distance is commonly used as a measure of dissimilarity between such vectors. In this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties. The number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.  We propose a very efficient method for computing the binary embedding using such large number of landmarks. Further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.  Experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods.</p>\n", "tags": ["NEURIPS","Quantisation","Text Retrieval"] },
{"key": "gong2022unsupervised", "year": "2022", "title":"Vit2hash Unsupervised Information-preserving Hashing", "abstract": "<p>Unsupervised image hashing, which maps images into binary codes without\nsupervision, is a compressor with a high compression rate. Hence, how to\npreserving meaningful information of the original data is a critical problem.\nInspired by the large-scale vision pre-training model, known as ViT, which has\nshown significant progress for learning visual representations, in this paper,\nwe propose a simple information-preserving compressor to finetune the ViT model\nfor the target unsupervised hashing task. Specifically, from pixels to\ncontinuous features, we first propose a feature-preserving module, using the\ncorrupted image as input to reconstruct the original feature from the\npre-trained ViT model and the complete image, so that the feature extractor can\nfocus on preserving the meaningful information of original data. Secondly, from\ncontinuous features to hash codes, we propose a hashing-preserving module,\nwhich aims to keep the semantic information from the pre-trained ViT model by\nusing the proposed Kullback-Leibler divergence loss. Besides, the quantization\nloss and the similarity loss are added to minimize the quantization error. Our\nmethod is very simple and achieves a significantly higher degree of MAP on\nthree benchmark image datasets.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "gong2024iterative", "year": "2024", "title":"Iterative Quantization A Procrustean Approach To Learning Binary Codes", "abstract": "<p>This paper addresses the problem of learning similarity preserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zerocentered data so as to minimize the quantization error of\nmapping this data to the vertices of a zero-centered binary\nhypercube. This method, dubbed iterative quantization\n(ITQ), has connections to multi-class spectral clustering\nand to the orthogonal Procrustes problem, and it can be\nused both with unsupervised data embeddings such as PCA\nand supervised embeddings such as canonical correlation\nanalysis (CCA). Our experiments show that the resulting\nbinary coding schemes decisively outperform several other\nstate-of-the-art methods.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "gong2024learning", "year": "2024", "title":"Learning Binary Codes For High-dimensional Data Using Bilinear Projections", "abstract": "<p>Recent advances in visual recognition indicate that to\nachieve good retrieval and classification accuracy on largescale\ndatasets like ImageNet, extremely high-dimensional\nvisual descriptors, e.g., Fisher Vectors, are needed. We\npresent a novel method for converting such descriptors to\ncompact similarity-preserving binary codes that exploits\ntheir natural matrix structure to reduce their dimensionality\nusing compact bilinear projections instead of a single\nlarge projection matrix. This method achieves comparable\nretrieval and classification accuracy to the original descriptors\nand to the state-of-the-art Product Quantization\napproach while having orders of magnitude faster code generation\ntime and smaller memory footprint.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "goodrich2009efficient", "year": "2009", "title":"Efficient Authenticated Data Structures For Graph Connectivity And Geometric Search Problems", "abstract": "<p>Authenticated data structures provide cryptographic proofs that their answers\nare as accurate as the author intended, even if the data structure is being\ncontrolled by a remote untrusted host. We present efficient techniques for\nauthenticating data structures that represent graphs and collections of\ngeometric objects. We introduce the path hash accumulator, a new primitive\nbased on cryptographic hashing for efficiently authenticating various\nproperties of structured data represented as paths, including any decomposable\nquery over sequences of elements. We show how to employ our primitive to\nauthenticate queries about properties of paths in graphs and search queries on\nmulti-catalogs. This allows the design of new, efficient authenticated data\nstructures for fundamental problems on networks, such as path and connectivity\nqueries over graphs, and complex queries on two-dimensional geometric objects,\nsuch as intersection and containment queries.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "gordo2016end", "year": "2016", "title":"End-to-end Learning Of Deep Visual Representations For Image Retrieval", "abstract": "<p>While deep learning has become a key ingredient in the top performing methods\nfor many computer vision tasks, it has failed so far to bring similar\nimprovements to instance-level image retrieval. In this article, we argue that\nreasons for the underwhelming results of deep methods on image retrieval are\nthreefold: i) noisy training data, ii) inappropriate deep architecture, and\niii) suboptimal training procedure. We address all three issues.\n  First, we leverage a large-scale but noisy landmark dataset and develop an\nautomatic cleaning method that produces a suitable training set for deep\nretrieval. Second, we build on the recent R-MAC descriptor, show that it can be\ninterpreted as a deep and differentiable architecture, and present improvements\nto enhance it. Last, we train this network with a siamese architecture that\ncombines three streams with a triplet loss. At the end of the training process,\nthe proposed architecture produces a global image representation in a single\nforward pass that is well suited for image retrieval. Extensive experiments\nshow that our approach significantly outperforms previous retrieval approaches,\nincluding state-of-the-art methods based on costly local descriptor indexing\nand spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively\nreport 94.7, 96.6, and 94.8 mean average precision. Our representations can\nalso be heavily compressed using product quantization with little loss in\naccuracy. For additional material, please see\nwww.xrce.xerox.com/Deep-Image-Retrieval.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation","Supervised"] },
{"key": "gordon2008optimal", "year": "2008", "title":"Optimal Hash Functions For Approximate Closest Pairs On The N-cube", "abstract": "<p>One way to find closest pairs in large datasets is to use hash functions. In\nrecent years locality-sensitive hash functions for various metrics have been\ngiven: projecting an n-cube onto k bits is simple hash function that performs\nwell. In this paper we investigate alternatives to projection. For various\nparameters hash functions given by complete decoding algorithms for codes work\nbetter, and asymptotically random codes perform better than projection.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "govindarajan2024lagrangian", "year": "2024", "title":"Lagrangian Hashing For Compressed Neural Field Representations", "abstract": "<p>We present Lagrangian Hashing, a representation for neural fields combining\nthe characteristics of fast training NeRF methods that rely on Eulerian grids\n(i.e.~InstantNGP), with those that employ points equipped with features as a\nway to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We\nachieve this by incorporating a point-based representation into the\nhigh-resolution layers of the hierarchical hash tables of an InstantNGP\nrepresentation. As our points are equipped with a field of influence, our\nrepresentation can be interpreted as a mixture of Gaussians stored within the\nhash table. We propose a loss that encourages the movement of our Gaussians\ntowards regions that require more representation budget to be sufficiently well\nrepresented. Our main finding is that our representation allows the\nreconstruction of signals using a more compact representation without\ncompromising quality.</p>\n", "tags": ["ARXIV"] },
{"key": "grabowski2014two", "year": "2014", "title":"Two Simple Full-text Indexes Based On The Suffix Array", "abstract": "<p>We propose two suffix array inspired full-text indexes. One, called SA-hash,\naugments the suffix array with a hash table to speed up pattern searches due to\nsignificantly narrowed search interval before the binary search phase. The\nother, called FBCSA, is a compact data structure, similar to M{\"a}kinen’s\ncompact suffix array, but working on fixed sized blocks. Experiments on the\nPizza~\\&amp;~Chili 200\\,MB datasets show that SA-hash is about 2–3 times faster in\npattern searches (counts) than the standard suffix array, for the price of\nrequiring \\(0.2n-1.1n\\) bytes of extra space, where \\(n\\) is the text length, and\nsetting a minimum pattern length. FBCSA is relatively fast in single cell\naccesses (a few times faster than related indexes at about the same or better\ncompression), but not competitive if many consecutive cells are to be\nextracted. Still, for the task of extracting, e.g., 10 successive cells its\ntime-space relation remains attractive.</p>\n", "tags": ["ARXIV"] },
{"key": "grauman2024learning", "year": "2024", "title":"Learning Binary Hash Codes For Large-scale Image Search", "abstract": "<p>Algorithms to rapidly search massive image or video collections are critical for many vision applications, including visual search, content-based retrieval, and non-parametric models for object recognition. Recent work shows that learned binary projections are a powerful way to index large collections according to their content. The basic idea is to formulate the projections so as to approximately preserve a given similarity function of interest. Having done so, one can then search the data efficiently using hash tables, or by exploring the Hamming ball volume around a novel query. Both enable sub-linear time retrieval with respect to the database size. Further, depending on the design of the projections, in some cases it is possible to bound the number of database examples that must be searched in order to achieve a given level of accuracy.</p>\n\n<p>This chapter overviews data structures for fast search with binary codes, and then describes several supervised and unsupervised strategies for generating the codes. In particular, we review supervised methods that integrate metric learning, boosting, and neural networks into the hash key construction, and unsupervised methods based on spectral analysis or kernelized random projections that compute affinity-preserving binary codes.Whether learning from explicit semantic supervision or exploiting the structure among unlabeled data, these methods make scalable retrieval possible for a variety of robust visual similarity measures.We focus on defining the algorithms, and illustrate the main points with results using millions of images.</p>\n", "tags": ["ARXIV","Supervised","Survey Paper"] },
{"key": "green2019hashgraph", "year": "2019", "title":"Hashgraph -- Scalable Hash Tables Using A Sparse Graph Data Structure", "abstract": "<p>Hash tables are ubiquitous and used in a wide range of applications for\nefficient probing of large and unsorted data. If designed properly, hash-tables\ncan enable efficients look ups in a constant number of operations or commonly\nreferred to as O(1) operations. As data sizes continue to grow and data becomes\nless structured (as is common for big-data applications), the need for\nefficient and scalable hash table also grows. In this paper we introduce\nHashGraph, a new scalable approach for building hash tables that uses concepts\ntaken from sparse graph representations–hence the name HashGraph. We show two\ndifferent variants of HashGraph, a simple algorithm that outlines the method to\ncreate the hash-table and an advanced method that creates the hash table in a\nmore efficient manner (with an improved memory access pattern). HashGraph shows\na new way to deal with hash-collisions that does not use “open-addressing” or\n“chaining”, yet has all the benefits of both these approaches. HashGraph\ncurrently works for static inputs, though recent progress with dynamic graph\ndata structures suggest that HashGraph might be extended to dynamic inputs as\nwell. We show that HashGraph can deal with a large number of hash-values per\nentry without loss of performance as most open-addressing and chaining\napproaches have. Further, we show that HashGraph is indifferent to the\nload-factor. Lastly, we show a new probing algorithm for the second phase of\nvalue lookups. Given the above, HashGraph is extremely fast and outperforms\nseveral state of the art hash-table implementations. The implementation of\nHashGraph in this paper is for NVIDIA GPUs, though HashGraph is not\narchitecture dependent. Using a NVIDIA GV100 GPU, HashGraph is anywhere from\n2X-8X faster than cuDPP, WarpDrive, and cuDF. HashGraph is able to build a\nhash-table at a rate of 2.5 billion keys per second and can probe at nearly the\nsame rate.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "gripon2016associative", "year": "2016", "title":"Associative Memories To Accelerate Approximate Nearest Neighbor Search", "abstract": "<p>Nearest neighbor search is a very active field in machine learning for it\nappears in many application cases, including classification and object\nretrieval. In its canonical version, the complexity of the search is linear\nwith both the dimension and the cardinal of the collection of vectors the\nsearch is performed in. Recently many works have focused on reducing the\ndimension of vectors using quantization techniques or hashing, while providing\nan approximate result. In this paper we focus instead on tackling the cardinal\nof the collection of vectors. Namely, we introduce a technique that partitions\nthe collection of vectors and stores each part in its own associative memory.\nWhen a query vector is given to the system, associative memories are polled to\nidentify which one contain the closest match. Then an exhaustive search is\nconducted only on the part of vectors stored in the selected associative\nmemory. We study the effectiveness of the system when messages to store are\ngenerated from i.i.d. uniform \\(\\pm\\)1 random variables or 0-1 sparse i.i.d.\nrandom variables. We also conduct experiment on both synthetic data and real\ndata and show it is possible to achieve interesting trade-offs between\ncomplexity and accuracy.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "grossi2011fast", "year": "2011", "title":"Fast Compressed Tries Through Path Decompositions", "abstract": "<p>Tries are popular data structures for storing a set of strings, where common\nprefixes are represented by common root-to-node paths. Over fifty years of\nusage have produced many variants and implementations to overcome some of their\nlimitations. We explore new succinct representations of path-decomposed tries\nand experimentally evaluate the corresponding reduction in space usage and\nmemory latency, comparing with the state of the art. We study two cases of\napplications: (1) a compressed dictionary for (compressed) strings, and (2) a\nmonotone minimal perfect hash for strings that preserves their lexicographic\norder.\n  For (1), we obtain data structures that outperform other state-of-the-art\ncompressed dictionaries in space efficiency, while obtaining predictable query\ntimes that are competitive with data structures preferred by the practitioners.\nIn (2), our tries perform several times faster than other trie-based monotone\nperfect hash functions, while occupying nearly the same space.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "grossi2018round", "year": "2018", "title":"Round-hashing For Data Storage Distributed Servers And External-memory Tables", "abstract": "<p>This paper proposes round-hashing, which is suitable for data storage on\ndistributed servers and for implementing external-memory tables in which each\nlookup retrieves at most a single block of external memory, using a stash. For\ndata storage, round-hashing is like consistent hashing as it avoids a full\nrehashing of the keys when new servers are added. Experiments show that the\nspeed to serve requests is tenfold or more than the state of the art. In\ndistributed data storage, this guarantees better throughput for serving\nrequests and, moreover, greatly reduces decision times for which data should\nmove to new servers as rescanning data is much faster.</p>\n", "tags": ["ARXIV"] },
{"key": "gu2015cross", "year": "2015", "title":"Cross-modality Hashing With Partial Correspondence", "abstract": "<p>Learning a hashing function for cross-media search is very desirable due to\nits low storage cost and fast query speed. However, the data crawled from\nInternet cannot always guarantee good correspondence among different modalities\nwhich affects the learning for hashing function. In this paper, we focus on\ncross-modal hashing with partially corresponded data. The data without full\ncorrespondence are made in use to enhance the hashing performance. The\nexperiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method\noutperforms some state-of-the-art hashing approaches with fewer correspondence\ninformation.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "gu2022accelerating", "year": "2022", "title":"Accelerating Code Search With Deep Hashing And Code Classification", "abstract": "<p>Code search is to search reusable code snippets from source code corpus based\non natural languages queries. Deep learning-based methods of code search have\nshown promising results. However, previous methods focus on retrieval accuracy\nbut lacked attention to the efficiency of the retrieval process. We propose a\nnovel method CoSHC to accelerate code search with deep hashing and code\nclassification, aiming to perform an efficient code search without sacrificing\ntoo much accuracy. To evaluate the effectiveness of CoSHC, we apply our method\nto five code search models. Extensive experimental results indicate that\ncompared with previous code search baselines, CoSHC can save more than 90% of\nretrieval time meanwhile preserving at least 99% of retrieval accuracy.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "guan2019post", "year": "2019", "title":"Post-training 4-bit Quantization On Embedding Tables", "abstract": "<p>Continuous representations have been widely adopted in recommender systems\nwhere a large number of entities are represented using embedding vectors. As\nthe cardinality of the entities increases, the embedding components can easily\ncontain millions of parameters and become the bottleneck in both storage and\ninference due to large memory consumption. This work focuses on post-training\n4-bit quantization on the continuous embeddings. We propose row-wise uniform\nquantization with greedy search and codebook-based quantization that\nconsistently outperforms state-of-the-art quantization approaches on reducing\naccuracy degradation. We deploy our uniform quantization technique on a\nproduction model in Facebook and demonstrate that it can reduce the model size\nto only 13.89% of the single-precision version while the model quality stays\nneutral.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "gui2019fast", "year": "2019", "title":"Fast Supervised Discrete Hashing", "abstract": "<p>Learning-based hashing algorithms are <code class=\"language-plaintext highlighter-rouge\">hot topics\" because they can greatly\nincrease the scale at which existing methods operate. In this paper, we propose\na new learning-based hashing method called</code>fast supervised discrete hashing”\n(FSDH) based on ``supervised discrete hashing” (SDH). Regressing the training\nexamples (or hash code) to the corresponding class labels is widely used in\nordinary least squares regression. Rather than adopting this method, FSDH uses\na very simple yet effective regression of the class labels of training examples\nto the corresponding hash code to accelerate the algorithm. To the best of our\nknowledge, this strategy has not previously been used for hashing. Traditional\nSDH decomposes the optimization into three sub-problems, with the most critical\nsub-problem - discrete optimization for binary hash codes - solved using\niterative discrete cyclic coordinate descent (DCC), which is time-consuming.\nHowever, FSDH has a closed-form solution and only requires a single rather than\niterative hash code-solving step, which is highly efficient. Furthermore, FSDH\nis usually faster than SDH for solving the projection matrix for least squares\nregression, making FSDH generally faster than SDH. For example, our results\nshow that FSDH is about 12-times faster than SDH when the number of hashing\nbits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than\nFastHash when the number of hashing bits is 64 on the MNIST data-base. Our\nexperimental results show that FSDH is not only fast, but also outperforms\nother comparative methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "gui2019supervised", "year": "2019", "title":"Supervised Discrete Hashing With Relaxation", "abstract": "<p>Data-dependent hashing has recently attracted attention due to being able to\nsupport efficient retrieval and storage of high-dimensional data such as\ndocuments, images, and videos. In this paper, we propose a novel learning-based\nhashing method called “Supervised Discrete Hashing with Relaxation” (SDHR)\nbased on “Supervised Discrete Hashing” (SDH). SDH uses ordinary least squares\nregression and traditional zero-one matrix encoding of class label information\nas the regression target (code words), thus fixing the regression target. In\nSDHR, the regression target is instead optimized. The optimized regression\ntarget matrix satisfies a large margin constraint for correct classification of\neach example. Compared with SDH, which uses the traditional zero-one matrix,\nSDHR utilizes the learned regression target matrix and, therefore, more\naccurately measures the classification error of the regression model and is\nmore flexible. As expected, SDHR generally outperforms SDH. Experimental\nresults on two large-scale image datasets (CIFAR-10 and MNIST) and a\nlarge-scale and challenging face dataset (FRGC) demonstrate the effectiveness\nand efficiency of SDHR.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "guo2014tight", "year": "2014", "title":"On Tight Bounds For Binary Frameproof Codes", "abstract": "<p>In this paper, we study \\(w\\)-frameproof codes, which are equivalent to\n\\(\\{1,w\\}\\)-separating hash families. Our main results concern binary codes,\nwhich are defined over an alphabet of two symbols. For all \\(w \\geq 3\\), and for\n\\(w+1 \\leq N \\leq 3w\\), we show that an \\(SHF(N; n,2, \\{1,w \\})\\) exists only if \\(n\n\\leq N\\), and an \\(SHF(N; N,2, \\{1,w \\})\\) must be a permutation matrix of degree\n\\(N\\).</p>\n", "tags": ["ARXIV"] },
{"key": "guo2015cnn", "year": "2015", "title":"CNN Based Hashing For Image Retrieval", "abstract": "<p>Along with data on the web increasing dramatically, hashing is becoming more\nand more popular as a method of approximate nearest neighbor search. Previous\nsupervised hashing methods utilized similarity/dissimilarity matrix to get\nsemantic information. But the matrix is not easy to construct for a new\ndataset. Rather than to reconstruct the matrix, we proposed a straightforward\nCNN-based hashing method, i.e. binarilizing the activations of a fully\nconnected layer with threshold 0 and taking the binary result as hash codes.\nThis method achieved the best performance on CIFAR-10 and was comparable with\nthe state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that\nthe signs of activations may carry more information than the relative values of\nactivations between samples, and that the co-adaption between feature extractor\nand hash functions is important for hashing.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "guo2015quantization", "year": "2015", "title":"Quantization Based Fast Inner Product Search", "abstract": "<p>We propose a quantization based approach for fast approximate Maximum Inner\nProduct Search (MIPS). Each database vector is quantized in multiple subspaces\nvia a set of codebooks, learned directly by minimizing the inner product\nquantization error. Then, the inner product of a query to a database vector is\napproximated as the sum of inner products with the subspace quantizers.\nDifferent from recently proposed LSH approaches to MIPS, the database vectors\nand queries do not need to be augmented in a higher dimensional feature space.\nWe also provide a theoretical analysis of the proposed approach, consisting of\nthe concentration results under mild assumptions. Furthermore, if a small\nsample of example queries is given at the training time, we propose a modified\ncodebook learning procedure which further improves the accuracy. Experimental\nresults on a variety of datasets including those arising from deep neural\nnetworks show that the proposed approach significantly outperforms the existing\nstate-of-the-art.</p>\n", "tags": ["ARXIV","Independent","LSH","Quantisation"] },
{"key": "guo2019deep", "year": "2019", "title":"Deep Hashing For Signed Social Network Embedding", "abstract": "<p>Network embedding is a promising way of network representation, facilitating\nmany signed social network processing and analysis tasks such as link\nprediction and node classification. Recently, feature hashing has been adopted\nin several existing embedding algorithms to improve the efficiency, which has\nobtained a great success. However, the existing feature hashing based embedding\nalgorithms only consider the positive links in signed social networks.\nIntuitively, negative links can also help improve the performance. Thus, in\nthis paper, we propose a novel deep hashing method for signed social network\nembedding by considering simultaneously positive and negative links. Extensive\nexperiments show that the proposed method performs better than several\nstate-of-the-art baselines through link prediction task over two real-world\nsigned social networks.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "guo2020deep", "year": "2020", "title":"Deep Kernel Supervised Hashing For Node Classification In Structural Networks", "abstract": "<p>Node classification in structural networks has been proven to be useful in\nmany real world applications. With the development of network embedding, the\nperformance of node classification has been greatly improved. However, nearly\nall the existing network embedding based methods are hard to capture the actual\ncategory features of a node because of the linearly inseparable problem in\nlow-dimensional space; meanwhile they cannot incorporate simultaneously network\nstructure information and node label information into network embedding. To\naddress the above problems, in this paper, we propose a novel Deep Kernel\nSupervised Hashing (DKSH) method to learn the hashing representations of nodes\nfor node classification. Specifically, a deep multiple kernel learning is first\nproposed to map nodes into suitable Hilbert space to deal with linearly\ninseparable problem. Then, instead of only considering structural similarity\nbetween two nodes, a novel similarity matrix is designed to merge both network\nstructure information and node label information. Supervised by the similarity\nmatrix, the learned hashing representations of nodes simultaneously preserve\nthe two kinds of information well from the learned Hilbert space. Extensive\nexperiments show that the proposed method significantly outperforms the\nstate-of-the-art baselines over three real world benchmark datasets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "gupta2010hashing", "year": "2010", "title":"Hashing Image Patches For Zooming", "abstract": "<p>In this paper we present a Bayesian image zooming/super-resolution algorithm\nbased on a patch based representation. We work on a patch based model with\noverlap and employ a Locally Linear Embedding (LLE) based approach as our data\nfidelity term in the Bayesian inference. The image prior imposes continuity\nconstraints across the overlapping patches. We apply an error back-projection\ntechnique, with an approximate cross bilateral filter. The problem of nearest\nneighbor search is handled by a variant of the locality sensitive hashing (LSH)\nscheme. The novelty of our work lies in the speed up achieved by the hashing\nscheme and the robustness and inherent modularity and parallel structure\nachieved by the LLE setup. The ill-posedness of the image reconstruction\nproblem is handled by the introduction of regularization priors which encode\nthe knowledge present in vast collections of natural images. We present\ncomparative results for both run-time as well as visual image quality based\nmeasurements.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "gupta2021irli", "year": "2021", "title":"IRLI Iterative Re-partitioning For Learning To Index", "abstract": "<p>Neural models have transformed the fundamental information retrieval problem\nof mapping a query to a giant set of items. However, the need for efficient and\nlow latency inference forces the community to reconsider efficient approximate\nnear-neighbor search in the item space. To this end, learning to index is\ngaining much interest in recent times. Methods have to trade between obtaining\nhigh accuracy while maintaining load balance and scalability in distributed\nsettings. We propose a novel approach called IRLI (pronounced `early’), which\niteratively partitions the items by learning the relevant buckets directly from\nthe query-item relevance data. Furthermore, IRLI employs a superior\npower-of-\\(k\\)-choices based load balancing strategy. We mathematically show that\nIRLI retrieves the correct item with high probability under very natural\nassumptions and provides superior load balancing. IRLI surpasses the best\nbaseline’s precision on multi-label classification while being \\(5x\\) faster on\ninference. For near-neighbor search tasks, the same method outperforms the\nstate-of-the-art Learned Hashing approach NeuralLSH by requiring only ~\n{1/6}^th of the candidates for the same recall. IRLI is both data and model\nparallel, making it ideal for distributed GPU implementation. We demonstrate\nthis advantage by indexing 100 million dense vectors and surpassing the popular\nFAISS library by &gt;10% on recall.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "gupta2023efficient", "year": "2023", "title":"Efficient High-resolution Template Matching With Vector Quantized Nearest Neighbour Fields", "abstract": "<p>Template matching is a fundamental problem in computer vision with\napplications in fields including object detection, image registration, and\nobject tracking. Current methods rely on nearest-neighbour (NN) matching, where\nthe query feature space is converted to NN space by representing each query\npixel with its NN in the template. NN-based methods have been shown to perform\nbetter in occlusions, appearance changes, and non-rigid transformations;\nhowever, they scale poorly with high-resolution data and high feature\ndimensions. We present an NN-based method which efficiently reduces the NN\ncomputations and introduces filtering in the NN fields (NNFs). A vector\nquantization step is introduced before the NN calculation to represent the\ntemplate with \\(k\\) features, and the filter response over the NNFs is used to\ncompare the template and query distributions over the features. We show that\nstate-of-the-art performance is achieved in low-resolution data, and our method\noutperforms previous methods at higher resolution.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "guruswami2018beating", "year": "2018", "title":"Beating Fredman-komlos For Perfect k-hashing", "abstract": "<p>We say a subset \\(C \\subseteq \\{1,2,\\dots,k\\}^n\\) is a \\(k\\)-hash code (also\ncalled \\(k\\)-separated) if for every subset of \\(k\\) codewords from \\(C\\), there\nexists a coordinate where all these codewords have distinct values.\nUnderstanding the largest possible rate (in bits), defined as \\((log_2 |C|)/n\\),\nof a \\(k\\)-hash code is a classical problem. It arises in two equivalent\ncontexts: (i) the smallest size possible for a perfect hash family that maps a\nuniverse of \\(N\\) elements into \\(\\{1,2,\\dots,k\\}\\), and (ii) the zero-error\ncapacity for decoding with lists of size less than \\(k\\) for a certain\ncombinatorial channel.\n  A general upper bound of \\(k!/k^{k-1}\\) on the rate of a \\(k\\)-hash code (in the\nlimit of large \\(n\\)) was obtained by Fredman and Koml'{o}s in 1984 for any \\(k\n\\geq 4\\). While better bounds have been obtained for \\(k=4\\), their original bound\nhas remained the best known for each \\(k \\ge 5\\). In this work, we obtain the\nfirst improvement to the Fredman-Koml'{o}s bound for every \\(k \\ge 5\\). While we\nget explicit (numerical) bounds for \\(k=5,6\\), for larger \\(k\\) we only show that\nthe FK bound can be improved by a positive, but unspecified, amount. Under a\nconjecture on the optimum value of a certain polynomial optimization problem\nover the simplex, our methods allow an effective bound to be computed for every\n\\(k\\).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "guskov2012properties", "year": "2012", "title":"Properties Of Perfect Transitive Binary Codes Of Length 15 And Extended Perfect Transitive Binary Codes Of Length 16", "abstract": "<p>Some properties of perfect transitive binary codes of length 15 and extended\nperfect transitive binary codes of length 16 are presented for reference\npurposes.</p>\n", "tags": ["ARXIV"] },
{"key": "géraud2019quotient", "year": "2019", "title":"Quotient Hash Tables - Efficiently Detecting Duplicates In Streaming Data", "abstract": "<p>This article presents the Quotient Hash Table (QHT) a new data structure for\nduplicate detection in unbounded streams. QHTs stem from a corrected analysis\nof streaming quotient filters (SQFs), resulting in a 33\\% reduction in memory\nusage for equal performance. We provide a new and thorough analysis of both\nalgorithms, with results of interest to other existing constructions.\n  We also introduce an optimised version of our new data structure dubbed\nQueued QHT with Duplicates (QQHTD).\n  Finally we discuss the effect of adversarial inputs for hash-based duplicate\nfilters similar to QHT.</p>\n", "tags": ["ARXIV","Streaming Data"] },
{"key": "habi2020hmq", "year": "2020", "title":"HMQ Hardware Friendly Mixed Precision Quantization Block For Cnns", "abstract": "<p>Recent work in network quantization produced state-of-the-art results using\nmixed precision quantization. An imperative requirement for many efficient edge\ndevice hardware implementations is that their quantizers are uniform and with\npower-of-two thresholds. In this work, we introduce the Hardware Friendly Mixed\nPrecision Quantization Block (HMQ) in order to meet this requirement. The HMQ\nis a mixed precision quantization block that repurposes the Gumbel-Softmax\nestimator into a smooth estimator of a pair of quantization parameters, namely,\nbit-width and threshold. HMQs use this to search over a finite space of\nquantization schemes. Empirically, we apply HMQs to quantize classification\nmodels trained on CIFAR10 and ImageNet. For ImageNet, we quantize four\ndifferent architectures and show that, in spite of the added restrictions to\nour quantization scheme, we achieve competitive and, in some cases,\nstate-of-the-art results.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "hacene2018quantized", "year": "2018", "title":"Quantized Guided Pruning For Efficient Hardware Implementations Of Convolutional Neural Networks", "abstract": "<p>Convolutional Neural Networks (CNNs) are state-of-the-art in numerous\ncomputer vision tasks such as object classification and detection. However, the\nlarge amount of parameters they contain leads to a high computational\ncomplexity and strongly limits their usability in budget-constrained devices\nsuch as embedded devices. In this paper, we propose a combination of a new\npruning technique and a quantization scheme that effectively reduce the\ncomplexity and memory usage of convolutional layers of CNNs, and replace the\ncomplex convolutional operation by a low-cost multiplexer. We perform\nexperiments on the CIFAR10, CIFAR100 and SVHN and show that the proposed method\nachieves almost state-of-the-art accuracy, while drastically reducing the\ncomputational and memory footprints. We also propose an efficient hardware\narchitecture to accelerate CNN operations. The proposed hardware architecture\nis a pipeline and accommodates multiple layers working at the same time to\nspeed up the inference process.</p>\n", "tags": ["ARXIV","CNN","Quantisation","Supervised"] },
{"key": "hajebi2013efficient", "year": "2013", "title":"An Efficient Index For Visual Search In Appearance-based SLAM", "abstract": "<p>Vector-quantization can be a computationally expensive step in visual\nbag-of-words (BoW) search when the vocabulary is large. A BoW-based appearance\nSLAM needs to tackle this problem for an efficient real-time operation. We\npropose an effective method to speed up the vector-quantization process in\nBoW-based visual SLAM. We employ a graph-based nearest neighbor search (GNNS)\nalgorithm to this aim, and experimentally show that it can outperform the\nstate-of-the-art. The graph-based search structure used in GNNS can efficiently\nbe integrated into the BoW model and the SLAM framework. The graph-based index,\nwhich is a k-NN graph, is built over the vocabulary words and can be extracted\nfrom the BoW’s vocabulary construction procedure, by adding one iteration to\nthe k-means clustering, which adds small extra cost. Moreover, exploiting the\nfact that images acquired for appearance-based SLAM are sequential, GNNS search\ncan be initiated judiciously which helps increase the speedup of the\nquantization process considerably.</p>\n", "tags": ["ARXIV","Graph","Quantisation","Unsupervised"] },
{"key": "hajebi2013stopping", "year": "2013", "title":"Stopping Rules For Bag-of-words Image Search And Its Application In Appearance-based Localization", "abstract": "<p>We propose a technique to improve the search efficiency of the bag-of-words\n(BoW) method for image retrieval. We introduce a notion of difficulty for the\nimage matching problems and propose methods that reduce the amount of\ncomputations required for the feature vector-quantization task in BoW by\nexploiting the fact that easier queries need less computational resources.\nMeasuring the difficulty of a query and stopping the search accordingly is\nformulated as a stopping problem. We introduce stopping rules that terminate\nthe image search depending on the difficulty of each query, thereby\nsignificantly reducing the computational cost. Our experimental results show\nthe effectiveness of our approach when it is applied to appearance-based\nlocalization problem.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "hamilton2020mosaic", "year": "2020", "title":"Mosaic Finding Artistic Connections Across Culture With Conditional Image Retrieval", "abstract": "<p>We introduce MosAIc, an interactive web app that allows users to find pairs\nof semantically related artworks that span different cultures, media, and\nmillennia. To create this application, we introduce Conditional Image Retrieval\n(CIR) which combines visual similarity search with user supplied filters or\n“conditions”. This technique allows one to find pairs of similar images that\nspan distinct subsets of the image corpus. We provide a generic way to adapt\nexisting image retrieval data-structures to this new domain and provide\ntheoretical bounds on our approach’s efficiency. To quantify the performance of\nCIR systems, we introduce new datasets for evaluating CIR methods and show that\nCIR performs non-parametric style transfer. Finally, we demonstrate that our\nCIR data-structures can identify “blind spots” in Generative Adversarial\nNetworks (GAN) where they fail to properly model the true data distribution.</p>\n", "tags": ["ARXIV","GAN","Image Retrieval"] },
{"key": "hamlin2019quantum", "year": "2019", "title":"Quantum Security Of Hash Functions And Property-preservation Of Iterated Hashing", "abstract": "<p>This work contains two major parts: comprehensively studying the security\nnotions of cryptographic hash functions against quantum attacks and the\nrelationships between them; and revisiting whether Merkle-Damgard and related\niterated hash constructions preserve the security properties of the compression\nfunction in the quantum setting. Specifically, we adapt the seven notions in\nRogaway and Shrimpton (FSE’04) to the quantum setting and prove that the\nseemingly stronger attack model where an adversary accesses a challenger in\nquantum superposition does not make a difference. We confirm the implications\nand separations between the seven properties in the quantum setting, and in\naddition we construct explicit examples separating an inherently quantum notion\ncalled collapsing from several proposed properties. Finally, we pin down the\nproperties that are preserved under several iterated hash schemes. In\nparticular, we prove that the ROX construction in Andreeva et al.\n(Asiacrypt’07) preserves the seven properties in the quantum random oracle\nmodel.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "hamster2023rediscovering", "year": "2023", "title":"Rediscovering Hashed Random Projections For Efficient Quantization Of Contextualized Sentence Embeddings", "abstract": "<p>Training and inference on edge devices often requires an efficient setup due\nto computational limitations. While pre-computing data representations and\ncaching them on a server can mitigate extensive edge device computation, this\nleads to two challenges. First, the amount of storage required on the server\nthat scales linearly with the number of instances. Second, the bandwidth\nrequired to send extensively large amounts of data to an edge device. To reduce\nthe memory footprint of pre-computed data representations, we propose a simple,\nyet effective approach that uses randomly initialized hyperplane projections.\nTo further reduce their size by up to 98.96%, we quantize the resulting\nfloating-point representations into binary vectors. Despite the greatly reduced\nsize, we show that the embeddings remain effective for training models across\nvarious English and German sentence classification tasks that retain 94%–99%\nof their floating-point.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "han2017beyond", "year": "2017", "title":"Beyond SIFT Using Binary Features For Loop Closure Detection", "abstract": "<p>In this paper a binary feature based Loop Closure Detection (LCD) method is\nproposed, which for the first time achieves higher precision-recall (PR)\nperformance compared with state-of-the-art SIFT feature based approaches. The\nproposed system originates from our previous work Multi-Index hashing for Loop\nclosure Detection (MILD), which employs Multi-Index Hashing\n(MIH)~\\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of\nbinary features. As the accuracy of MILD is limited by repeating textures and\ninaccurate image similarity measurement, burstiness handling is introduced to\nsolve this problem and achieves considerable accuracy improvement.\nAdditionally, a comprehensive theoretical analysis on MIH used in MILD is\nconducted to further explore the potentials of hashing methods for ANN search\nof binary features from probabilistic perspective. This analysis provides more\nfreedom on best parameter choosing in MIH for different application scenarios.\nExperiments on popular public datasets show that the proposed approach achieved\nthe highest accuracy compared with state-of-the-art while running at 30Hz for\ndatabases containing thousands of images.</p>\n", "tags": ["ARXIV"] },
{"key": "han2017mild", "year": "2017", "title":"MILD Multi-index Hashing For Loop Closure Detection", "abstract": "<p>Loop Closure Detection (LCD) has been proved to be extremely useful in global\nconsistent visual Simultaneously Localization and Mapping (SLAM) and\nappearance-based robot relocalization. Methods exploiting binary features in\nbag of words representation have recently gained a lot of popularity for their\nefficiency, but suffer from low recall due to the inherent drawback that high\ndimensional binary feature descriptors lack well-defined centroids. In this\npaper, we propose a realtime LCD approach called MILD (Multi-Index Hashing for\nLoop closure Detection), in which image similarity is measured by feature\nmatching directly to achieve high recall without introducing extra\ncomputational complexity with the aid of Multi-Index Hashing (MIH). A\ntheoretical analysis of the approximate image similarity measurement using MIH\nis presented, which reveals the trade-off between efficiency and accuracy from\na probabilistic perspective. Extensive comparisons with state-of-the-art LCD\nmethods demonstrate the superiority of MILD in both efficiency and accuracy.</p>\n", "tags": ["ARXIV"] },
{"key": "han2023comprehensive", "year": "2023", "title":"A Comprehensive Survey On Vector Database Storage And Retrieval Technique Challenge", "abstract": "<p>A vector database is used to store high-dimensional data that cannot be\ncharacterized by traditional DBMS. Although there are not many articles\ndescribing existing or introducing new vector database architectures, the\napproximate nearest neighbor search problem behind vector databases has been\nstudied for a long time, and considerable related algorithmic articles can be\nfound in the literature. This article attempts to comprehensively review\nrelevant algorithms to provide a general understanding of this booming research\narea. The basis of our framework categorises these studies by the approach of\nsolving ANNS problem, respectively hash-based, tree-based, graph-based and\nquantization-based approaches. Then we present an overview of existing\nchallenges for vector databases. Lastly, we sketch how vector databases can be\ncombined with large language models and provide new possibilities.</p>\n", "tags": ["ARXIV","Graph","Quantisation","Survey Paper"] },
{"key": "han2024hashing", "year": "2024", "title":"Hashing Based Contrastive Learning For Virtual Screening", "abstract": "<p>Virtual screening (VS) is a critical step in computer-aided drug discovery,\naiming to identify molecules that bind to a specific target receptor like\nprotein. Traditional VS methods, such as docking, are often too time-consuming\nfor screening large-scale molecular databases. Recent advances in deep learning\nhave demonstrated that learning vector representations for both proteins and\nmolecules using contrastive learning can outperform traditional docking\nmethods. However, given that target databases often contain billions of\nmolecules, real-valued vector representations adopted by existing methods can\nstill incur significant memory and time costs in VS. To address this problem,\nin this paper we propose a hashing-based contrastive learning method, called\nDrugHash, for VS. DrugHash treats VS as a retrieval task that uses efficient\nbinary hash codes for retrieval. In particular, DrugHash designs a simple yet\neffective hashing strategy to enable end-to-end learning of binary hash codes\nfor both protein and molecule modalities, which can dramatically reduce the\nmemory and time costs with higher accuracy compared with existing methods.\nExperimental results show that DrugHash can outperform existing methods to\nachieve state-of-the-art accuracy, with a memory saving of 32\\(\\times\\) and a\nspeed improvement of 3.5\\(\\times\\).</p>\n", "tags": ["ARXIV","Deep Learning","Self Supervised"] },
{"key": "hansen2019unsupervised", "year": "2019", "title":"Unsupervised Neural Generative Semantic Hashing", "abstract": "<p>Fast similarity search is a key component in large-scale information\nretrieval, where semantic hashing has become a popular strategy for\nrepresenting documents as binary hash codes. Recent advances in this area have\nbeen obtained through neural network based models: generative models trained by\nlearning to reconstruct the original documents. We present a novel unsupervised\ngenerative semantic hashing approach, \\textit{Ranking based Semantic Hashing}\n(RBSH) that consists of both a variational and a ranking based component.\nSimilarly to variational autoencoders, the variational component is trained to\nreconstruct the original document conditioned on its generated hash code, and\nas in prior work, it only considers documents individually. The ranking\ncomponent solves this limitation by incorporating inter-document similarity\ninto the hash code generation, modelling document ranking through a hinge loss.\nTo circumvent the need for labelled data to compute the hinge loss, we use a\nweak labeller and thus keep the approach fully unsupervised.\n  Extensive experimental evaluation on four publicly available datasets against\ntraditional baselines and recent state-of-the-art methods for semantic hashing\nshows that RBSH significantly outperforms all other methods across all\nevaluated hash code lengths. In fact, RBSH hash codes are able to perform\nsimilarly to state-of-the-art hash codes while using 2-4x fewer bits.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "hansen2020content", "year": "2020", "title":"Content-aware Neural Hashing For Cold-start Recommendation", "abstract": "<p>Content-aware recommendation approaches are essential for providing\nmeaningful recommendations for \\textit{new} (i.e., \\textit{cold-start}) items\nin a recommender system. We present a content-aware neural hashing-based\ncollaborative filtering approach (NeuHash-CF), which generates binary hash\ncodes for users and items, such that the highly efficient Hamming distance can\nbe used for estimating user-item relevance. NeuHash-CF is modelled as an\nautoencoder architecture, consisting of two joint hashing components for\ngenerating user and item hash codes. Inspired from semantic hashing, the item\nhashing component generates a hash code directly from an item’s content\ninformation (i.e., it generates cold-start and seen item hash codes in the same\nmanner). This contrasts existing state-of-the-art models, which treat the two\nitem cases separately. The user hash codes are generated directly based on user\nid, through learning a user embedding matrix. We show experimentally that\nNeuHash-CF significantly outperforms state-of-the-art baselines by up to 12\\%\nNDCG and 13\\% MRR in cold-start recommendation settings, and up to 4\\% in both\nNDCG and MRR in standard settings where all items are present while training.\nOur approach uses 2-4x shorter hash codes, while obtaining the same or better\nperformance compared to the state of the art, thus consequently also enabling a\nnotable storage reduction.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "hansen2020unsupervised", "year": "2020", "title":"Unsupervised Semantic Hashing With Pairwise Reconstruction", "abstract": "<p>Semantic Hashing is a popular family of methods for efficient similarity\nsearch in large-scale datasets. In Semantic Hashing, documents are encoded as\nshort binary vectors (i.e., hash codes), such that semantic similarity can be\nefficiently computed using the Hamming distance. Recent state-of-the-art\napproaches have utilized weak supervision to train better performing hashing\nmodels. Inspired by this, we present Semantic Hashing with Pairwise\nReconstruction (PairRec), which is a discrete variational autoencoder based\nhashing model. PairRec first encodes weakly supervised training pairs (a query\ndocument and a semantically similar document) into two hash codes, and then\nlearns to reconstruct the same query document from both of these hash codes\n(i.e., pairwise reconstruction). This pairwise reconstruction enables our model\nto encode local neighbourhood structures within the hash code directly through\nthe decoder. We experimentally compare PairRec to traditional and\nstate-of-the-art approaches, and obtain significant performance improvements in\nthe task of document similarity search.</p>\n", "tags": ["ARXIV","Unsupervised","Weakly Supervised"] },
{"key": "hansen2021projected", "year": "2021", "title":"Projected Hamming Dissimilarity For Bit-level Importance Coding In Collaborative Filtering", "abstract": "<p>When reasoning about tasks that involve large amounts of data, a common\napproach is to represent data items as objects in the Hamming space where\noperations can be done efficiently and effectively. Object similarity can then\nbe computed by learning binary representations (hash codes) of the objects and\ncomputing their Hamming distance. While this is highly efficient, each bit\ndimension is equally weighted, which means that potentially discriminative\ninformation of the data is lost. A more expressive alternative is to use\nreal-valued vector representations and compute their inner product; this allows\nvarying the weight of each dimension but is many magnitudes slower. To fix\nthis, we derive a new way of measuring the dissimilarity between two objects in\nthe Hamming space with binary weighting of each dimension (i.e., disabling\nbits): we consider a field-agnostic dissimilarity that projects the vector of\none object onto the vector of the other. When working in the Hamming space,\nthis results in a novel projected Hamming dissimilarity, which by choice of\nprojection, effectively allows a binary importance weighting of the hash code\nof one object through the hash code of the other. We propose a variational\nhashing model for learning hash codes optimized for this projected Hamming\ndissimilarity, and experimentally evaluate it in collaborative filtering\nexperiments. The resultant hash codes lead to effectiveness gains of up to +7%\nin NDCG and +14% in MRR compared to state-of-the-art hashing-based\ncollaborative filtering baselines, while requiring no additional storage and no\ncomputational overhead compared to using the Hamming distance.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "hansen2021representation", "year": "2021", "title":"Representation Learning For Efficient And Effective Similarity Search And Recommendation", "abstract": "<p>How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "hansen2021unsupervised", "year": "2021", "title":"Unsupervised Multi-index Semantic Hashing", "abstract": "<p>Semantic hashing represents documents as compact binary vectors (hash codes)\nand allows both efficient and effective similarity search in large-scale\ninformation retrieval. The state of the art has primarily focused on learning\nhash codes that improve similarity search effectiveness, while assuming a\nbrute-force linear scan strategy for searching over all the hash codes, even\nthough much faster alternatives exist. One such alternative is multi-index\nhashing, an approach that constructs a smaller candidate set to search over,\nwhich depending on the distribution of the hash codes can lead to sub-linear\nsearch time. In this work, we propose Multi-Index Semantic Hashing (MISH), an\nunsupervised hashing model that learns hash codes that are both effective and\nhighly efficient by being optimized for multi-index hashing. We derive novel\ntraining objectives, which enable to learn hash codes that reduce the candidate\nsets produced by multi-index hashing, while being end-to-end trainable. In\nfact, our proposed training objectives are model agnostic, i.e., not tied to\nhow the hash codes are generated specifically in MISH, and are straight-forward\nto include in existing and future semantic hashing models. We experimentally\ncompare MISH to state-of-the-art semantic hashing baselines in the task of\ndocument similarity search. We find that even though multi-index hashing also\nimproves the efficiency of the baselines compared to a linear scan, they are\nstill upwards of 33% slower than MISH, while MISH is still able to obtain\nstate-of-the-art effectiveness.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "hansen2024content", "year": "2024", "title":"Content-aware Neural Hashing For Cold-start Recommendation", "abstract": "<p>Content-aware recommendation approaches are essential for providing meaningful recommendations for new (i.e., cold-start) items in a recommender system. We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), which generates binary hash codes for users and items, such that the highly efficient Hamming distance can be used for estimating user-item relevance. NeuHash-CF is modelled as an autoencoder architecture, consisting of two joint hashing components for generating user and item hash codes. Inspired from semantic hashing, the item hashing component generates a hash code directly from an item’s content information (i.e., it generates cold-start and seen item hash codes in the same manner). This contrasts existing state-of-the-art models, which treat the two item cases separately. The user hash codes are generated directly based on user id, through learning a user embedding matrix. We show experimentally that NeuHash-CF significantly outperforms state-of-the-art baselines by up to 12% NDCG and 13% MRR in cold-start recommendation settings, and up to 4% in both NDCG and MRR in standard settings where all items are present while training. Our approach uses 2-4x shorter hash codes, while obtaining the same or better performance compared to the state of the art, thus consequently also enabling a notable storage reduction.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "hansen2024unsupervised", "year": "2024", "title":"Unsupervised Semantic Hashing With Pairwise Reconstruction", "abstract": "<p>Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing, documents are encoded as short binary vectors (i.e., hash codes), such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this, we present Semantic Hashing with Pairwise Reconstruction (PairRec), which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes, and then learns to reconstruct the same query document from both of these hash codes (i.e., pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches, and obtain significant performance improvements in the task of document similarity search.</p>\n", "tags": ["ARXIV","Unsupervised","Weakly Supervised"] },
{"key": "haq2019survey", "year": "2019", "title":"A Survey Of Binary Code Similarity", "abstract": "<p>Binary code similarity approaches compare two or more pieces of binary code\nto identify their similarities and differences. The ability to compare binary\ncode enables many real-world applications on scenarios where source code may\nnot be available such as patch analysis, bug search, and malware detection and\nanalysis. Over the past 20 years numerous binary code similarity approaches\nhave been proposed, but the research area has not yet been systematically\nanalyzed. This paper presents a first survey of binary code similarity. It\nanalyzes 61 binary code similarity approaches, which are systematized on four\naspects: (1) the applications they enable, (2) their approach characteristics,\n(3) how the approaches are implemented, and (4) the benchmarks and\nmethodologies used to evaluate them. In addition, the survey discusses the\nscope and origins of the area, its evolution over the past two decades, and the\nchallenges that lie ahead.</p>\n", "tags": ["ARXIV","Survey Paper"] },
{"key": "harandi2014expanding", "year": "2014", "title":"Expanding The Family Of Grassmannian Kernels An Embedding Perspective", "abstract": "<p>Modeling videos and image-sets as linear subspaces has proven beneficial for\nmany visual recognition tasks. However, it also incurs challenges arising from\nthe fact that linear subspaces do not obey Euclidean geometry, but lie on a\nspecial type of Riemannian manifolds known as Grassmannian. To leverage the\ntechniques developed for Euclidean spaces (e.g, support vector machines) with\nsubspaces, several recent studies have proposed to embed the Grassmannian into\na Hilbert space by making use of a positive definite kernel. Unfortunately,\nonly two Grassmannian kernels are known, none of which -as we will show- is\nuniversal, which limits their ability to approximate a target function\narbitrarily well. Here, we introduce several positive definite Grassmannian\nkernels, including universal ones, and demonstrate their superiority over\npreviously-known kernels in various tasks, such as classification, clustering,\nsparse coding and hashing.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "harpeled2010approximate", "year": "2010", "title":"Approximate Nearest Neighbor Search For Low Dimensional Queries", "abstract": "<p>We study the Approximate Nearest Neighbor problem for metric spaces where the\nquery points are constrained to lie on a subspace of low doubling dimension,\nwhile the data is high-dimensional. We show that this problem can be solved\nefficiently despite the high dimensionality of the data.</p>\n", "tags": ["ARXIV"] },
{"key": "harpeled2019near", "year": "2019", "title":"Near Neighbor Who Is The Fairest Of Them All", "abstract": "<p>In this work we study a “fair” variant of the near neighbor problem. Namely, given a set of \\(n\\) points \\(P\\) and a parameter \\(r\\), the goal is to preprocess the points, such that given a query point \\(q\\), any point in the \\(r\\)-neighborhood of the query, i.e., \\(B(q,r)\\), have the same probability of being reported as the near neighbor.</p>\n\n<p>We show that LSH based algorithms can be made fair, without a significant loss in efficiency. Specifically, we show an algorithm that reports a point \\(p\\) in the \\(r\\)-neighborhood of a query \\(q\\) with almost uniform probability.  The time to report such a point is proportional to \\(O(\\dns(q.r) Q(n,c))\\), and its space is \\(O(S(n,c))\\), where \\(Q(n,c)\\) and \\(S(n,c)\\) are the query time and space of an LSH algorithm for \\(c\\)-approximate near neighbor, and \\(\\dns(q,r)\\) is a function of the local density around \\(q\\).</p>\n\n<p>Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. Finally, we run experiments to show performance of our approach on real data.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "harvey2024explicit", "year": "2024", "title":"Explicit Orthogonal Arrays And Universal Hashing With Arbitrary Parameters", "abstract": "<p>Orthogonal arrays are a type of combinatorial design that were developed in\nthe 1940s in the design of statistical experiments. In 1947, Rao proved a lower\nbound on the size of any orthogonal array, and raised the problem of\nconstructing arrays of minimum size. Kuperberg, Lovett and Peled (2017) gave a\nnon-constructive existence proof of orthogonal arrays whose size is\nnear-optimal (i.e., within a polynomial of Rao’s lower bound), leaving open the\nquestion of an algorithmic construction. We give the first explicit,\ndeterministic, algorithmic construction of orthogonal arrays achieving\nnear-optimal size for all parameters. Our construction uses algebraic geometry\ncodes.\n  In pseudorandomness, the notions of \\(t\\)-independent generators or\n\\(t\\)-independent hash functions are equivalent to orthogonal arrays. Classical\nconstructions of \\(t\\)-independent hash functions are known when the size of the\ncodomain is a prime power, but very few constructions are known for an\narbitrary codomain. Our construction yields algorithmically efficient\n\\(t\\)-independent hash functions for arbitrary domain and codomain.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "hayashi2013more", "year": "2013", "title":"More Efficient Privacy Amplification With Less Random Seeds Via Dual Universal Hash Function", "abstract": "<p>We explicitly construct random hash functions for privacy amplification\n(extractors) that require smaller random seed lengths than the previous\nliterature, and still allow efficient implementations with complexity \\(O(nlog\nn)\\) for input length \\(n\\). The key idea is the concept of dual universal\\(_2\\)\nhash function introduced recently. We also use a new method for constructing\nextractors by concatenating \\(\\delta\\)-almost dual universal\\(_2\\) hash functions\nwith other extractors. Besides minimizing seed lengths, we also introduce\nmethods that allow one to use non-uniform random seeds for extractors. These\nmethods can be applied to a wide class of extractors, including dual\nuniversal\\(_2\\) hash function, as well as to conventional universal\\(_2\\) hash\nfunctions.</p>\n", "tags": ["Independent"] },
{"key": "hayashi2013security", "year": "2013", "title":"Security Analysis Of Epsilon-almost Dual Universal2 Hash Functions Smoothing Of Min Entropy Vs. Smoothing Of Renyi Entropy Of Order 2", "abstract": "<p>Recently, \\(\\epsilon\\)-almost dual universal\\(_2\\) hash functions has been\nproposed as a new and wider class of hash functions. Using this class of hash\nfunctions, several efficient hash functions were proposed. This paper evaluates\nthe security performance when we apply this kind of hash functions. We evaluate\nthe security in several kinds of setting based on the \\(L_1\\) distinguishability\ncriterion and the modified mutual information criterion. The obtained\nevaluation is based on smoothing of R'{e}nyi entropy of order 2 and/or min\nentropy. We clarify the difference between these two methods.</p>\n", "tags": ["Independent"] },
{"key": "he2012difficulty", "year": "2012", "title":"On The Difficulty Of Nearest Neighbor Search", "abstract": "<p>Fast approximate nearest neighbor (NN) search in large databases is becoming\npopular. Several powerful learning-based formulations have been proposed\nrecently. However, not much attention has been paid to a more fundamental\nquestion: how difficult is (approximate) nearest neighbor search in a given\ndata set? And which data properties affect the difficulty of nearest neighbor\nsearch and how? This paper introduces the first concrete measure called\nRelative Contrast that can be used to evaluate the influence of several crucial\ndata characteristics such as dimensionality, sparsity, and database size\nsimultaneously in arbitrary normed metric spaces. Moreover, we present a\ntheoretical analysis to prove how the difficulty measure (relative contrast)\ndetermines/affects the complexity of Local Sensitive Hashing, a popular\napproximate NN search method. Relative contrast also provides an explanation\nfor a family of heuristic hashing algorithms with good practical performance\nbased on PCA. Finally, we show that most of the previous works in measuring NN\nsearch meaningfulness/difficulty can be derived as special asymptotic cases for\ndense vectors of the proposed measure.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "he2017hashing", "year": "2017", "title":"Hashing As Tie-aware Learning To Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest\nneighbor retrieval. In this paper, we develop learning to rank formulations for\nhashing, aimed at directly optimizing ranking-based evaluation metrics such as\nAverage Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We\nfirst observe that the integer-valued Hamming distance often leads to tied\nrankings, and propose to use tie-aware versions of AP and NDCG to evaluate\nhashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive\ntheir continuous relaxations, and perform gradient-based optimization with deep\nneural networks. Our results establish the new state-of-the-art for image\nretrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "he2019one", "year": "2019", "title":"One Network For Multi-domains Domain Adaptive Hashing With Intersectant Generative Adversarial Network", "abstract": "<p>With the recent explosive increase of digital data, image recognition and\nretrieval become a critical practical application. Hashing is an effective\nsolution to this problem, due to its low storage requirement and high query\nspeed. However, most of past works focus on hashing in a single (source)\ndomain. Thus, the learned hash function may not adapt well in a new (target)\ndomain that has a large distributional difference with the source domain. In\nthis paper, we explore an end-to-end domain adaptive learning framework that\nsimultaneously and precisely generates discriminative hash codes and classifies\ntarget domain images. Our method encodes two domains images into a semantic\ncommon space, followed by two independent generative adversarial networks\narming at crosswise reconstructing two domains’ images, reducing domain\ndisparity and improving alignment in the shared space. We evaluate our\nframework on {four} public benchmark datasets, all of which show that our\nmethod is superior to the other state-of-the-art methods on the tasks of object\nrecognition and image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "he2021self", "year": "2021", "title":"Self-supervised Video Retrieval Transformer Network", "abstract": "<p>Content-based video retrieval aims to find videos from a large video database\nthat are similar to or even near-duplicate of a given query video. Video\nrepresentation and similarity search algorithms are crucial to any video\nretrieval system. To derive effective video representation, most video\nretrieval systems require a large amount of manually annotated data for\ntraining, making it costly inefficient. In addition, most retrieval systems are\nbased on frame-level features for video similarity searching, making it\nexpensive both storage wise and search wise. We propose a novel video retrieval\nsystem, termed SVRTN, that effectively addresses the above shortcomings. It\nfirst applies self-supervised training to effectively learn video\nrepresentation from unlabeled data to avoid the expensive cost of manual\nannotation. Then, it exploits transformer structure to aggregate frame-level\nfeatures into clip-level to reduce both storage space and search complexity. It\ncan learn the complementary and discriminative information from the\ninteractions among clip frames, as well as acquire the frame permutation and\nmissing invariant ability to support more flexible retrieval manners.\nComprehensive experiments on two challenging video retrieval datasets, namely\nFIVR-200K and SVD, verify the effectiveness of our proposed SVRTN method, which\nachieves the best performance of video retrieval on accuracy and efficiency.</p>\n", "tags": ["ARXIV","Supervised","Video Retrieval"] },
{"key": "he2021unsupervised", "year": "2021", "title":"Unsupervised Domain-adaptive Hash For Networks", "abstract": "<p>Abundant real-world data can be naturally represented by large-scale\nnetworks, which demands efficient and effective learning algorithms. At the\nsame time, labels may only be available for some networks, which demands these\nalgorithms to be able to adapt to unlabeled networks. Domain-adaptive hash\nlearning has enjoyed considerable success in the computer vision community in\nmany practical tasks due to its lower cost in both retrieval time and storage\nfootprint. However, it has not been applied to multiple-domain networks. In\nthis work, we bridge this gap by developing an unsupervised domain-adaptive\nhash learning method for networks, dubbed UDAH. Specifically, we develop four\n{task-specific yet correlated} components: (1) network structure preservation\nvia a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,\n(3) cross-domain intersected discriminators, and (4) semantic center alignment.\nWe conduct a wide range of experiments to evaluate the effectiveness and\nefficiency of our method on a range of tasks including link prediction, node\nclassification, and neighbor recommendation. Our evaluation results demonstrate\nthat our model achieves better performance than the state-of-the-art\nconventional discrete embedding methods over all the tasks.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "he2024bit", "year": "2024", "title":"Bit-mask Robust Contrastive Knowledge Distillation For Unsupervised Semantic Hashing", "abstract": "<p>Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.</p>\n", "tags": ["ARXIV","Has Code","Unsupervised"] },
{"key": "he2024hashing", "year": "2024", "title":"Hashing As Tie-aware Learning To Rank", "abstract": "<p>Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "he2024hybridhash", "year": "2024", "title":"Hybridhash Hybrid Convolutional And Self-attention Deep Hashing For Image Retrieval", "abstract": "<p>Deep image hashing aims to map input images into simple binary hash codes via\ndeep neural networks and thus enable effective large-scale image retrieval.\nRecently, hybrid networks that combine convolution and Transformer have\nachieved superior performance on various computer tasks and have attracted\nextensive attention from researchers. Nevertheless, the potential benefits of\nsuch hybrid networks in image retrieval still need to be verified. To this end,\nwe propose a hybrid convolutional and self-attention deep hashing method known\nas HybridHash. Specifically, we propose a backbone network with stage-wise\narchitecture in which the block aggregation function is introduced to achieve\nthe effect of local self-attention and reduce the computational complexity. The\ninteraction module has been elaborately designed to promote the communication\nof information between image blocks and to enhance the visual representations.\nWe have conducted comprehensive experiments on three widely used datasets:\nCIFAR-10, NUS-WIDE and IMAGENET. The experimental results demonstrate that the\nmethod proposed in this paper has superior performance with respect to\nstate-of-the-art deep hashing methods. Source code is available\nhttps://github.com/shuaichaochao/HybridHash.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Supervised"] },
{"key": "he2024k", "year": "2024", "title":"K-nearest Neighbors Hashing", "abstract": "<p>Hashing based approximate nearest neighbor search embeds high dimensional data to compact binary codes, which\nenables efficient similarity search and storage. However,\nthe non-isometry sign(·) function makes it hard to project\nthe nearest neighbors in continuous data space into the\nclosest codewords in discrete Hamming space. In this work,\nwe revisit the sign(·) function from the perspective of space partitioning.\nIn specific, we bridge the gap between\nk-nearest neighbors and binary hashing codes with Shannon entropy. We further propose a novel K-Nearest Neighbors Hashing (KNNH) method to learn binary representations from KNN within the subspaces generated by sign(·).\nTheoretical and experimental results show that the KNN relation is of central importance to neighbor preserving embeddings, and the proposed method outperforms the state-of-the-arts on benchmark datasets.</p>\n", "tags": ["ARXIV"] },
{"key": "heddes2022hyperdimensional", "year": "2022", "title":"Hyperdimensional Hashing A Robust And Efficient Dynamic Hash Table", "abstract": "<p>Most cloud services and distributed applications rely on hashing algorithms\nthat allow dynamic scaling of a robust and efficient hash table. Examples\ninclude AWS, Google Cloud and BitTorrent. Consistent and rendezvous hashing are\nalgorithms that minimize key remapping as the hash table resizes. While memory\nerrors in large-scale cloud deployments are common, neither algorithm offers\nboth efficiency and robustness. Hyperdimensional Computing is an emerging\ncomputational model that has inherent efficiency, robustness and is well suited\nfor vector or hardware acceleration. We propose Hyperdimensional (HD) hashing\nand show that it has the efficiency to be deployed in large systems. Moreover,\na realistic level of memory errors causes more than 20% mismatches for\nconsistent hashing while HD hashing remains unaffected.</p>\n", "tags": ["ARXIV"] },
{"key": "hegeman2024compact", "year": "2024", "title":"Compact Parallel Hash Tables On The GPU", "abstract": "<p>On the GPU, hash table operation speed is determined in large part by cache\nline efficiency, and state-of-the-art hashing schemes thus divide tables into\ncache line-sized buckets. This raises the question whether performance can be\nfurther improved by increasing the number of entries that fit in such buckets.\nKnown compact hashing techniques have not yet been adapted to the massively\nparallel setting, nor have they been evaluated on the GPU. We consider a\ncompact version of bucketed cuckoo hashing, and a version of compact iceberg\nhashing suitable for the GPU. We discuss the tables from a theoretical\nperspective, and provide an open source implementation of both schemes in CUDA\nfor comparative benchmarking. In terms of performance, the state-of-the-art\ncuckoo hashing benefits from compactness on lookups and insertions (most\nexperiments show at least 10-20% increase in throughput), and the iceberg table\nbenefits significantly, to the point of being comparable to compact cuckoo\nhashing–while supporting performant dynamic operation.</p>\n", "tags": ["ARXIV"] },
{"key": "helbling2020directed", "year": "2020", "title":"Directed Graph Hashing", "abstract": "<p>This paper presents several algorithms for hashing directed graphs. The\nalgorithms given are capable of hashing entire graphs as well as assigning hash\nvalues to specific nodes in a given graph. The notion of node symmetry is made\nprecise via computation of vertex orbits and the graph automorphism group, and\nnodes that are symmetrically identical are assigned equal hashes. We also\npresent a novel Merkle-style hashing algorithm that seeks to fulfill the\nrecursive principle that a hash of a node should depend only on the hash of its\nneighbors. This algorithm works even in the presence of cycles, which would not\nbe possible with a naive approach. Structurally hashing trees has seen\nwidespread use in blockchain, source code version control, and web\napplications. Despite the popularity of tree hashing, directed graph hashing\nremains unstudied in the literature. Our algorithms open new possibilities to\nhashing both directed graphs and more complex data structures that can be\nreduced to directed graphs such as hypergraphs.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "hemati2020non", "year": "2020", "title":"A Non-alternating Graph Hashing Algorithm For Large Scale Image Search", "abstract": "<p>In the era of big data, methods for improving memory and computational\nefficiency have become crucial for successful deployment of technologies.\nHashing is one of the most effective approaches to deal with computational\nlimitations that come with big data. One natural way for formulating this\nproblem is spectral hashing that directly incorporates affinity to learn binary\ncodes. However, due to binary constraints, the optimization becomes\nintractable. To mitigate this challenge, different relaxation approaches have\nbeen proposed to reduce the computational load of obtaining binary codes and\nstill attain a good solution. The problem with all existing relaxation methods\nis resorting to one or more additional auxiliary variables to attain high\nquality binary codes while relaxing the problem. The existence of auxiliary\nvariables leads to coordinate descent approach which increases the\ncomputational complexity. We argue that introducing these variables is\nunnecessary. To this end, we propose a novel relaxed formulation for spectral\nhashing that adds no additional variables to the problem. Furthermore, instead\nof solving the problem in original space where number of variables is equal to\nthe data points, we solve the problem in a much smaller space and retrieve the\nbinary codes from this solution. This trick reduces both the memory and\ncomputational complexity at the same time. We apply two optimization\ntechniques, namely projected gradient and optimization on manifold, to obtain\nthe solution. Using comprehensive experiments on four public datasets, we show\nthat the proposed efficient spectral hashing (ESH) algorithm achieves highly\ncompetitive retrieval performance compared with state of the art at low\ncomplexity.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "hemati2021beyond", "year": "2021", "title":"Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing", "abstract": "<p>An effective unsupervised hashing algorithm leads to compact binary codes\npreserving the neighborhood structure of data as much as possible. One of the\nmost established schemes for unsupervised hashing is to reduce the\ndimensionality of data and then find a rigid (neighbourhood-preserving)\ntransformation that reduces the quantization error. Although employing rigid\ntransformations is effective, we may not reduce quantization loss to the\nultimate limits. As well, reducing dimensionality and quantization loss in two\nseparate steps seems to be sub-optimal. Motivated by these shortcomings, we\npropose to employ both rigid and non-rigid transformations to reduce\nquantization error and dimensionality simultaneously. We relax the\northogonality constraint on the projection in a PCA-formulation and regularize\nthis by a quantization term. We show that both the non-rigid projection matrix\nand rotation matrix contribute towards minimizing quantization loss but in\ndifferent ways. A scalable nested coordinate descent approach is proposed to\noptimize this mixed-integer optimization problem. We evaluate the proposed\nmethod on five public benchmark datasets providing almost half a million\nimages. Comparative results indicate that the proposed method mostly\noutperforms state-of-art linear methods and competes with end-to-end deep\nsolutions.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "hemati2024beyond", "year": "2024", "title":"Beyond Neighbourhood-preserving Transformations For Quantization-based Unsupervised Hashing", "abstract": "<p>An effective unsupervised hashing algorithm leads to compact binary codes preserving the neighborhood structure of data as much as possible. One of the most established schemes for unsupervised hashing is to reduce the dimensionality of data and then find a rigid (neighbourhood-preserving) transformation that reduces the quantization error. Although employing rigid transformations is effective, we may not reduce quantization loss to the ultimate limits. As well, reducing dimensionality and quantization loss in two separate steps seems to be sub-optimal. Motivated by these shortcomings, we propose to employ both rigid and non-rigid transformations to reduce quantization error and dimensionality simultaneously. We relax the orthogonality constraint on the projection in a PCA-formulation and regularize this by a quantization term. We show that both the non-rigid projection matrix and rotation matrix contribute towards minimizing quantization loss but in different ways. A scalable nested coordinate descent approach is proposed to optimize this mixed-integer optimization problem. We evaluate the proposed method on five public benchmark datasets providing almost half a million images. Comparative results indicate that the proposed method mostly outperforms state-of-art linear methods and competes with end-to-end deep solutions.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "hemati2024non", "year": "2024", "title":"A Non-alternating Graph Hashing Algorithm For Large Scale Image Search", "abstract": "<p>In the era of big data, methods for improving memory and computational efficiency have become crucial for successful deployment of technologies. Hashing is one of the most effective approaches to deal with computational limitations that come with big data. One natural way for formulating this problem is spectral hashing that directly incorporates affinity to learn binary codes. However, due to binary constraints, the optimization becomes intractable. To mitigate this challenge, different relaxation approaches have been proposed to reduce the computational load of obtaining binary codes and still attain a good solution. The problem with all existing relaxation methods is resorting to one or more additional auxiliary variables to attain high quality binary codes while relaxing the problem. The existence of auxiliary variables leads to coordinate descent approach which increases the computational complexity. We argue that introducing these variables is unnecessary. To this end, we propose a novel relaxed formulation for spectral hashing that adds no additional variables to the problem. Furthermore, instead of solving the problem in original space where number of variables is equal to the data points, we solve the problem in a much smaller space and retrieve the binary codes from this solution. This trick reduces both the memory and computational complexity at the same time. We apply two optimization techniques, namely projected gradient and optimization on manifold, to obtain the solution. Using comprehensive experiments on four public datasets, we show that the proposed efficient spectral hashing (ESH) algorithm achieves highly competitive retrieval performance compared with state of the art at low complexity.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "heng2020optimal", "year": "2020", "title":"Optimal Binary Linear Codes From Maximal Arcs", "abstract": "<p>The binary Hamming codes with parameters \\([2^m-1, 2^m-1-m, 3]\\) are perfect.\nTheir extended codes have parameters \\([2^m, 2^m-1-m, 4]\\) and are\ndistance-optimal. The first objective of this paper is to construct a class of\nbinary linear codes with parameters \\([2^{m+s}+2^s-2^m,2^{m+s}+2^s-2^m-2m-2,4]\\),\nwhich have better information rates than the class of extended binary Hamming\ncodes, and are also distance-optimal. The second objective is to construct a\nclass of distance-optimal binary codes with parameters \\([2^m+2, 2^m-2m, 6]\\).\nBoth classes of binary linear codes have new parameters.</p>\n", "tags": ["ARXIV"] },
{"key": "heo2024spherical", "year": "2024", "title":"Spherical Hashing", "abstract": "<p>Many binary code encoding schemes based on hashing\nhave been actively studied recently, since they can provide\nefficient similarity search, especially nearest neighbor\nsearch, and compact data representations suitable for handling\nlarge scale image databases in many computer vision\nproblems. Existing hashing techniques encode highdimensional\ndata points by using hyperplane-based hashing\nfunctions. In this paper we propose a novel hyperspherebased\nhashing function, spherical hashing, to map more\nspatially coherent data points into a binary code compared\nto hyperplane-based hashing functions. Furthermore, we\npropose a new binary code distance function, spherical\nHamming distance, that is tailored to our hyperspherebased\nbinary coding scheme, and design an efficient iterative\noptimization process to achieve balanced partitioning\nof data points for each hash function and independence between\nhashing functions. Our extensive experiments show\nthat our spherical hashing technique significantly outperforms\nsix state-of-the-art hashing techniques based on hyperplanes\nacross various image benchmarks of sizes ranging\nfrom one to 75 million of GIST descriptors. The performance\ngains are consistent and large, up to 100% improvements.\nThe excellent results confirm the unique merits of\nthe proposed idea in using hyperspheres to encode proximity\nregions in high-dimensional spaces. Finally, our method\nis intuitive and easy to implement.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "hermann2024phobic", "year": "2024", "title":"PHOBIC Perfect Hashing With Optimized Bucket Sizes And Interleaved Coding", "abstract": "<p>A minimal perfect hash function (MPHF) maps a set of n keys to {1, …, n}\nwithout collisions. Such functions find widespread application e.g. in\nbioinformatics and databases. In this paper we revisit PTHash - a construction\ntechnique particularly designed for fast queries. PTHash distributes the input\nkeys into small buckets and, for each bucket, it searches for a hash function\nseed that places its keys in the output domain without collisions. The\ncollection of all seeds is then stored in a compressed way. Since the first\nbuckets are easier to place, buckets are considered in non-increasing order of\nsize. Additionally, PTHash heuristically produces an imbalanced distribution of\nbucket sizes by distributing 60% of the keys into 30% of the buckets. Our main\ncontribution is to characterize, up to lower order terms, an optimal\ndistribution of expected bucket sizes. We arrive at a simple, closed form\nsolution which improves construction throughput for space efficient\nconfigurations in practice. Our second contribution is a novel encoding scheme\nfor the seeds. We split the keys into partitions. Within each partition, we run\nthe bucket distribution and search step. We then store the seeds in an\ninterleaved way by consecutively placing the seeds for the i-th buckets from\nall partitions. The seeds for the i-th bucket of each partition follow the same\nstatistical distribution. This allows us to tune a compressor for each bucket.\nHence, we call our technique PHOBIC - Perfect Hashing with Optimized Bucket\nsizes and Interleaved Coding. Compared to PTHash, PHOBIC is 0.17 bits/key more\nspace efficient for same query time and construction throughput. We also\ncontribute a GPU implementation to further accelerate MPHF construction. For a\nconfiguration with fast queries, PHOBIC-GPU can construct a perfect hash\nfunction at 2.17 bits/key in 28 ns per key, which can be queried in 37 ns on\nthe CPU.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "hoang2017enhance", "year": "2017", "title":"Enhance Feature Discrimination For Unsupervised Hashing", "abstract": "<p>We introduce a novel approach to improve unsupervised hashing. Specifically,\nwe propose a very efficient embedding method: Gaussian Mixture Model embedding\n(Gemb). The proposed method, using Gaussian Mixture Model, embeds feature\nvector into a low-dimensional vector and, simultaneously, enhances the\ndiscriminative property of features before passing them into hashing. Our\nexperiment shows that the proposed method boosts the hashing performance of\nmany state-of-the-art, e.g. Binary Autoencoder (BA) [1], Iterative Quantization\n(ITQ) [2], in standard evaluation metrics for the three main benchmark\ndatasets.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "hoang2018simultaneous", "year": "2018", "title":"Simultaneous Compression And Quantization A Joint Approach For Efficient Unsupervised Hashing", "abstract": "<p>For unsupervised data-dependent hashing, the two most important requirements\nare to preserve similarity in the low-dimensional feature space and to minimize\nthe binary quantization loss. A well-established hashing approach is Iterative\nQuantization (ITQ), which addresses these two requirements in separate steps.\nIn this paper, we revisit the ITQ approach and propose novel formulations and\nalgorithms to the problem. Specifically, we propose a novel approach, named\nSimultaneous Compression and Quantization (SCQ), to jointly learn to compress\n(reduce dimensionality) and binarize input data in a single formulation under\nstrict orthogonal constraint. With this approach, we introduce a loss function\nand its relaxed version, termed Orthonormal Encoder (OnE) and Orthogonal\nEncoder (OgE) respectively, which involve challenging binary and orthogonal\nconstraints. We propose to attack the optimization using novel algorithms based\non recent advances in cyclic coordinate descent approach. Comprehensive\nexperiments on unsupervised image retrieval demonstrate that our proposed\nmethods consistently outperform other state-of-the-art hashing methods.\nNotably, our proposed methods outperform recent deep neural networks and GAN\nbased hashing in accuracy, while being very computationally-efficient.</p>\n", "tags": ["ARXIV","GAN","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "hoang2020unsupervised", "year": "2020", "title":"Unsupervised Deep Cross-modality Spectral Hashing", "abstract": "<p>This paper presents a novel framework, namely Deep Cross-modality Spectral\nHashing (DCSH), to tackle the unsupervised learning problem of binary hash\ncodes for efficient cross-modal retrieval. The framework is a two-step hashing\napproach which decouples the optimization into (1) binary optimization and (2)\nhashing function learning. In the first step, we propose a novel spectral\nembedding-based algorithm to simultaneously learn single-modality and binary\ncross-modality representations. While the former is capable of well preserving\nthe local structure of each modality, the latter reveals the hidden patterns\nfrom all modalities. In the second step, to learn mapping functions from\ninformative data inputs (images and word embeddings) to binary codes obtained\nfrom the first step, we leverage the powerful CNN for images and propose a\nCNN-based deep architecture to learn text modality. Quantitative evaluations on\nthree standard benchmark datasets demonstrate that the proposed DCSH method\nconsistently outperforms other state-of-the-art methods.</p>\n", "tags": ["ARXIV","CNN","Cross Modal","Unsupervised"] },
{"key": "hoang2021multi", "year": "2021", "title":"Multi-modal Mutual Information Maximization A Novel Approach For Unsupervised Deep Cross-modal Hashing", "abstract": "<p>In this paper, we adopt the maximizing mutual information (MI) approach to\ntackle the problem of unsupervised learning of binary hash codes for efficient\ncross-modal retrieval. We proposed a novel method, dubbed Cross-Modal Info-Max\nHashing (CMIMH). First, to learn informative representations that can preserve\nboth intra- and inter-modal similarities, we leverage the recent advances in\nestimating variational lower-bound of MI to maximize the MI between the binary\nrepresentations and input features and between binary representations of\ndifferent modalities. By jointly maximizing these MIs under the assumption that\nthe binary representations are modelled by multivariate Bernoulli\ndistributions, we can learn binary representations, which can preserve both\nintra- and inter-modal similarities, effectively in a mini-batch manner with\ngradient descent. Furthermore, we find out that trying to minimize the modality\ngap by learning similar binary representations for the same instance from\ndifferent modalities could result in less informative representations. Hence,\nbalancing between reducing the modality gap and losing modality-private\ninformation is important for the cross-modal retrieval tasks. Quantitative\nevaluations on standard benchmark datasets demonstrate that the proposed method\nconsistently outperforms other state-of-the-art cross-modal retrieval methods.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "hoe2021one", "year": "2021", "title":"One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives: to make the\nlearned binary hash codes discriminative and to minimize a quantization error.\nWith further constraints such as bit balance and code orthogonality, it is not\nuncommon for existing models to employ a large number (&gt;4) of losses. This\nleads to difficulties in model training and subsequently impedes their\neffectiveness. In this work, we propose a novel deep hashing model with only a\nsingle learning objective. Specifically, we show that maximizing the cosine\nsimilarity between the continuous codes and their corresponding binary\northogonal codes can ensure both hash code discriminativeness and quantization\nerror minimization. Further, with this learning objective, code balancing can\nbe achieved by simply using a Batch Normalization (BN) layer and multi-label\nclassification is also straightforward with label smoothing. The result is an\none-loss deep hashing model that removes all the hassles of tuning the weights\nof various losses. Importantly, extensive experiments show that our model is\nhighly effective, outperforming the state-of-the-art multi-loss hashing models\non three large-scale instance retrieval benchmarks, often by significant\nmargins. Code is available at https://github.com/kamwoh/orthohash</p>\n", "tags": ["ARXIV","Has Code","Quantisation","Supervised"] },
{"key": "hoe2024one", "year": "2024", "title":"One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives: to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality, it is not uncommon for existing models to employ a large number (&gt;4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work, we propose a novel deep hashing model with only a single learning objective. Specifically, we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further, with this learning objective, code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly, extensive experiments show that our model is highly effective, outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "holden2023identifying", "year": "2023", "title":"Identifying Reducible K-tuples Of Vectors With Subspace-proximity Sensitive Hashing/filtering", "abstract": "<p>We introduce and analyse a family of hash and predicate functions that are\nmore likely to produce collisions for small reducible configurations of\nvectors. These may offer practical improvements to lattice sieving for short\nvectors. In particular, in one asymptotic regime the family exhibits\nsignificantly different convergent behaviour than existing hash functions and\npredicates.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "horiuchi2022similarity", "year": "2022", "title":"Similarity Search On Computational Notebooks", "abstract": "<p>Computational notebook software such as Jupyter Notebook is popular for data\nscience tasks. Numerous computational notebooks are available on the Web and\nreusable; however, searching for computational notebooks manually is a tedious\ntask, and so far, there are no tools to search for computational notebooks\neffectively and efficiently. In this paper, we propose a similarity search on\ncomputational notebooks and develop a new framework for the similarity search.\nGiven contents (i.e., source codes, tabular data, libraries, and outputs\nformats) in computational notebooks as a query, the similarity search problem\naims to find top-k computational notebooks with the most similar contents. We\ndefine two similarity measures; set-based and graph-based similarities.\nSet-based similarity handles each content independently, while graph-based\nsimilarity captures the relationships between contents. Our framework can\neffectively prune the candidates of computational notebooks that should not be\nin the top-k results. Furthermore, we develop optimization techniques such as\ncaching and indexing to accelerate the search. Experiments using Kaggle\nnotebooks show that our method, in particular graph-based similarity, can\nachieve high accuracy and high efficiency.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "hou2023semstamp", "year": "2023", "title":"Semstamp A Semantic Watermark With Paraphrastic Robustness For Text Generation", "abstract": "<p>Existing watermarking algorithms are vulnerable to paraphrase attacks because\nof their token-level design. To address this issue, we propose SemStamp, a\nrobust sentence-level semantic watermarking algorithm based on\nlocality-sensitive hashing (LSH), which partitions the semantic space of\nsentences. The algorithm encodes and LSH-hashes a candidate sentence generated\nby an LLM, and conducts sentence-level rejection sampling until the sampled\nsentence falls in watermarked partitions in the semantic embedding space. A\nmargin-based constraint is used to enhance its robustness. To show the\nadvantages of our algorithm, we propose a “bigram” paraphrase attack using the\nparaphrase that has the fewest bigram overlaps with the original sentence. This\nattack is shown to be effective against the existing token-level watermarking\nmethod. Experimental results show that our novel semantic watermark algorithm\nis not only more robust than the previous state-of-the-art method on both\ncommon and bigram paraphrase attacks, but also is better at preserving the\nquality of generation.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "hou2024k", "year": "2024", "title":"K-semstamp A Clustering-based Semantic Watermark For Detection Of Machine-generated Text", "abstract": "<p>Recent watermarked generation algorithms inject detectable signatures during\nlanguage generation to facilitate post-hoc detection. While token-level\nwatermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023)\napplies watermark on the semantic representation of sentences and demonstrates\npromising robustness. SemStamp employs locality-sensitive hashing (LSH) to\npartition the semantic space with arbitrary hyperplanes, which results in a\nsuboptimal tradeoff between robustness and speed. We propose k-SemStamp, a\nsimple yet effective enhancement of SemStamp, utilizing k-means clustering as\nan alternative of LSH to partition the embedding space with awareness of\ninherent semantic structure. Experimental results indicate that k-SemStamp\nsaliently improves its robustness and sampling efficiency while preserving the\ngeneration quality, advancing a more effective tool for machine-generated text\ndetection.</p>\n", "tags": ["ARXIV","LSH","Unsupervised"] },
{"key": "houen2022understanding", "year": "2022", "title":"Understanding The Moments Of Tabulation Hashing Via Chaoses", "abstract": "<p>Simple tabulation hashing dates back to Zobrist in 1970 and is defined as\nfollows: Each key is viewed as \\(c\\) characters from some alphabet \\(\\Sigma\\), we\nhave \\(c\\) fully random hash functions \\(h_0, \\ldots, h_{c - 1} \\colon \\Sigma \\to\n\\{0, \\ldots, 2^l - 1\\}\\), and a key \\(x = (x_0, \\ldots, x_{c - 1})\\) is hashed to\n\\(h(x) = h_0(x_0) \\oplus \\ldots \\oplus h_{c - 1}(x_{c - 1})\\) where \\(\\oplus\\) is\nthe bitwise XOR operation. The previous results on tabulation hashing by P{\\v\na}tra{\\c s}cu and Thorup~[J.ACM’11] and by Aamand et al.~[STOC’20] focused on\nproving Chernoff-style tail bounds on hash-based sums, e.g., the number keys\nhashing to a given value, for simple tabulation hashing, but their bounds do\nnot cover the entire tail.\n  Chaoses are random variables of the form \\(\\sum a_{i_0, \\ldots, i_{c - 1}}\nX_{i_0} \\cdot \\ldots \\cdot X_{i_{c - 1}}\\) where \\(X_i\\) are independent random\nvariables. Chaoses are a well-studied concept from probability theory, and\ntight analysis has been proven in several instances, e.g., when the independent\nrandom variables are standard Gaussian variables and when the independent\nrandom variables have logarithmically convex tails. We notice that hash-based\nsums of simple tabulation hashing can be seen as a sum of chaoses that are not\nindependent. This motivates us to use techniques from the theory of chaoses to\nanalyze hash-based sums of simple tabulation hashing.\n  In this paper, we obtain bounds for all the moments of hash-based sums for\nsimple tabulation hashing which are tight up to constants depending only on\n\\(c\\). In contrast with the previous attempts, our approach will mostly be\nanalytical and does not employ intricate combinatorial arguments. The improved\nanalysis of simple tabulation hashing allows us to obtain bounds for the\nmoments of hash-based sums for the mixed tabulation hashing introduced by\nDahlgaard et al.~[FOCS’15].</p>\n", "tags": ["ARXIV","FOCS","Independent"] },
{"key": "houen2023sparse", "year": "2023", "title":"A Sparse Johnson-lindenstrauss Transform Using Fast Hashing", "abstract": "<p>The <em>Sparse Johnson-Lindenstrauss Transform</em> of Kane and Nelson (SODA\n2012) provides a linear dimensionality-reducing map \\(A \\in \\mathbb{R}^{m \\times\nu}\\) in \\(ℓ₂\\) that preserves distances up to distortion of \\(1 + \\epsilon\\)\nwith probability \\(1 - \\delta\\), where \\(m = O(\\epsilon^{-2} log 1/\\delta)\\)\nand each column of \\(A\\) has \\(O(\\epsilon m)\\) non-zero entries. The previous\nanalyses of the Sparse Johnson-Lindenstrauss Transform all assumed access to a\n\\(Ω(log 1/\\delta)\\)-wise independent hash function. The main contribution\nof this paper is a more general analysis of the Sparse Johnson-Lindenstrauss\nTransform with less assumptions on the hash function. We also show that the\n<em>Mixed Tabulation hash function</em> of Dahlgaard, Knudsen, Rotenberg, and\nThorup (FOCS 2015) satisfies the conditions of our analysis, thus giving us the\nfirst analysis of a Sparse Johnson-Lindenstrauss Transform that works with a\npractical hash function.</p>\n", "tags": ["ARXIV","FOCS","Independent"] },
{"key": "hsieh2016fast", "year": "2016", "title":"Fast Binary Embedding Via Circulant Downsampled Matrix -- A Data-independent Approach", "abstract": "<p>Binary embedding of high-dimensional data aims to produce low-dimensional\nbinary codes while preserving discriminative power. State-of-the-art methods\noften suffer from high computation and storage costs. We present a simple and\nfast embedding scheme by first downsampling N-dimensional data into\nM-dimensional data and then multiplying the data with an MxM circulant matrix.\nOur method requires O(N +M log M) computation and O(N) storage costs. We prove\nif data have sparsity, our scheme can achieve similarity-preserving well.\nExperiments further demonstrate that though our method is cost-effective and\nfast, it still achieves comparable performance in image applications.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "hu2012combined", "year": "2012", "title":"Combined Descriptors In Spatial Pyramid Domain For Image Classification", "abstract": "<p>Recently spatial pyramid matching (SPM) with scale invariant feature\ntransform (SIFT) descriptor has been successfully used in image classification.\nUnfortunately, the codebook generation and feature quantization procedures\nusing SIFT feature have the high complexity both in time and space. To address\nthis problem, in this paper, we propose an approach which combines local binary\npatterns (LBP) and three-patch local binary patterns (TPLBP) in spatial pyramid\ndomain. The proposed method does not need to learn the codebook and feature\nquantization processing, hence it becomes very efficient. Experiments on two\npopular benchmark datasets demonstrate that the proposed method always\nsignificantly outperforms the very popular SPM based SIFT descriptor method\nboth in time and classification accuracy.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "hu2017supervised", "year": "2017", "title":"Supervised Hashing Based On Energy Minimization", "abstract": "<p>Recently, supervised hashing methods have attracted much attention since they\ncan optimize retrieval speed and storage cost while preserving semantic\ninformation. Because hashing codes learning is NP-hard, many methods resort to\nsome form of relaxation technique. But the performance of these methods can\neasily deteriorate due to the relaxation. Luckily, many supervised hashing\nformulations can be viewed as energy functions, hence solving hashing codes is\nequivalent to learning marginals in the corresponding conditional random field\n(CRF). By minimizing the KL divergence between a fully factorized distribution\nand the Gibbs distribution of this CRF, a set of consistency equations can be\nobtained, but updating them in parallel may not yield a local optimum since the\nvariational lower bound is not guaranteed to increase. In this paper, we use a\nlinear approximation of the sigmoid function to convert these consistency\nequations to linear systems, which have a closed-form solution. By applying\nthis novel technique to two classical hashing formulations KSH and SPLH, we\nobtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH.\nExperimental results on three datasets show the superiority of our methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "hu2017vc", "year": "2017", "title":"On The Vc-dimension Of Binary Codes", "abstract": "<p>We investigate the asymptotic rates of length-\\(n\\) binary codes with\nVC-dimension at most \\(dn\\) and minimum distance at least \\(\\delta n\\). Two upper\nbounds are obtained, one as a simple corollary of a result by Haussler and the\nother via a shortening approach combining Sauer-Shelah lemma and the linear\nprogramming bound. Two lower bounds are given using Gilbert-Varshamov type\narguments over constant-weight and Markov-type sets.</p>\n", "tags": [] },
{"key": "hu2018deep", "year": "2018", "title":"Deep LDA Hashing", "abstract": "<p>The conventional supervised hashing methods based on classification do not\nentirely meet the requirements of hashing technique, but Linear Discriminant\nAnalysis (LDA) does. In this paper, we propose to perform a revised LDA\nobjective over deep networks to learn efficient hashing codes in a truly\nend-to-end fashion. However, the complicated eigenvalue decomposition within\neach mini-batch in every epoch has to be faced with when simply optimizing the\ndeep network w.r.t. the LDA objective. In this work, the revised LDA objective\nis transformed into a simple least square problem, which naturally overcomes\nthe intractable problems and can be easily solved by the off-the-shelf\noptimizer. Such deep extension can also overcome the weakness of LDA Hashing in\nthe limited linear projection and feature learning. Amounts of experiments are\nconducted on three benchmark datasets. The proposed Deep LDA Hashing shows\nnearly 70 points improvement over the conventional one on the CIFAR-10 dataset.\nIt also beats several state-of-the-art methods on various metrics.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "hu2018from", "year": "2018", "title":"From Hashing To Cnns Training Binaryweight Networks Via Hashing", "abstract": "<p>Deep convolutional neural networks (CNNs) have shown appealing performance on\nvarious computer vision tasks in recent years. This motivates people to deploy\nCNNs to realworld applications. However, most of state-of-art CNNs require\nlarge memory and computational resources, which hinders the deployment on\nmobile devices. Recent studies show that low-bit weight representation can\nreduce much storage and memory demand, and also can achieve efficient network\ninference. To achieve this goal, we propose a novel approach named BWNH to\ntrain Binary Weight Networks via Hashing. In this paper, we first reveal the\nstrong connection between inner-product preserving hashing and binary weight\nnetworks, and show that training binary weight networks can be intrinsically\nregarded as a hashing problem. Based on this perspective, we propose an\nalternating optimization method to learn the hash codes instead of directly\nlearning binary weights. Extensive experiments on CIFAR10, CIFAR100 and\nImageNet demonstrate that our proposed BWNH outperforms current state-of-art by\na large margin.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "hu2020creating", "year": "2020", "title":"Creating Something From Nothing Unsupervised Knowledge Distillation For Cross-modal Hashing", "abstract": "<p>In recent years, cross-modal hashing (CMH) has attracted increasing\nattentions, mainly because its potential ability of mapping contents from\ndifferent modalities, especially in vision and language, into the same space,\nso that it becomes efficient in cross-modal data retrieval. There are two main\nframeworks for CMH, differing from each other in whether semantic supervision\nis required. Compared to the unsupervised methods, the supervised methods often\nenjoy more accurate results, but require much heavier labors in data\nannotation. In this paper, we propose a novel approach that enables guiding a\nsupervised method using outputs produced by an unsupervised method.\nSpecifically, we make use of teacher-student optimization for propagating\nknowledge. Experiments are performed on two popular CMH benchmarks, i.e., the\nMIRFlickr and NUS-WIDE datasets. Our approach outperforms all existing\nunsupervised methods by a large margin.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "hu2020efficient", "year": "2020", "title":"Efficient Approximate Nearest Neighbor Search For Multiple Weighted l_pleq2 Distance Functions", "abstract": "<p>Nearest neighbor search is fundamental to a wide range of applications. Since\nthe exact nearest neighbor search suffers from the “curse of dimensionality”,\napproximate approaches, such as Locality-Sensitive Hashing (LSH), are widely\nused to trade a little query accuracy for a much higher query efficiency. In\nmany scenarios, it is necessary to perform nearest neighbor search under\nmultiple weighted distance functions in high-dimensional spaces. This paper\nconsiders the important problem of supporting efficient approximate nearest\nneighbor search for multiple weighted distance functions in high-dimensional\nspaces. To the best of our knowledge, prior work can only solve the problem for\nthe \\(l_2\\) distance. However, numerous studies have shown that the \\(l_p\\)\ndistance with \\(p\\in(0,2)\\) could be more effective than the \\(l_2\\) distance in\nhigh-dimensional spaces. We propose a novel method, WLSH, to address the\nproblem for the \\(l_p\\) distance for \\(p\\in(0,2]\\). WLSH takes the LSH approach and\ncan theoretically guarantee both the efficiency of processing queries and the\naccuracy of query results while minimizing the required total number of hash\ntables. We conduct extensive experiments on synthetic and real data sets, and\nthe results show that WLSH achieves high performance in terms of query\nefficiency, query accuracy and space consumption.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "hu2022badhash", "year": "2022", "title":"Badhash Invisible Backdoor Attacks Against Deep Hashing With Clean Label", "abstract": "<p>Due to its powerful feature learning capability and high efficiency, deep\nhashing has achieved great success in large-scale image retrieval. Meanwhile,\nextensive works have demonstrated that deep neural networks (DNNs) are\nsusceptible to adversarial examples, and exploring adversarial attack against\ndeep hashing has attracted many research efforts. Nevertheless, backdoor\nattack, another famous threat to DNNs, has not been studied for deep hashing\nyet. Although various backdoor attacks have been proposed in the field of image\nclassification, existing approaches failed to realize a truly imperceptive\nbackdoor attack that enjoys invisible triggers and clean label setting\nsimultaneously, and they also cannot meet the intrinsic demand of image\nretrieval backdoor. In this paper, we propose BadHash, the first\ngenerative-based imperceptible backdoor attack against deep hashing, which can\neffectively generate invisible and input-specific poisoned images with clean\nlabel. Specifically, we first propose a new conditional generative adversarial\nnetwork (cGAN) pipeline to effectively generate poisoned samples. For any given\nbenign image, it seeks to generate a natural-looking poisoned counterpart with\na unique invisible trigger. In order to improve the attack effectiveness, we\nintroduce a label-based contrastive learning network LabCLN to exploit the\nsemantic characteristics of different labels, which are subsequently used for\nconfusing and misleading the target model to learn the embedded trigger. We\nfinally explore the mechanism of backdoor attacks on image retrieval in the\nhash space. Extensive experiments on multiple benchmark datasets verify that\nBadHash can generate imperceptible poisoned samples with strong attack ability\nand transferability over state-of-the-art deep hashing schemes.</p>\n", "tags": ["ARXIV","Image Retrieval","Self Supervised"] },
{"key": "hu2024creating", "year": "2024", "title":"Creating Something From Nothing Unsupervised Knowledge Distillation For Cross-modal Hashing", "abstract": "<p>In recent years, cross-modal hashing (CMH) has attracted increasing attentions, mainly because its potential\nability of mapping contents from different modalities, especially in vision and language, into the same space, so that\nit becomes efficient in cross-modal data retrieval. There are\ntwo main frameworks for CMH, differing from each other in\nwhether semantic supervision is required. Compared to the\nunsupervised methods, the supervised methods often enjoy\nmore accurate results, but require much heavier labors in\ndata annotation. In this paper, we propose a novel approach\nthat enables guiding a supervised method using outputs produced by an unsupervised method. Specifically, we make\nuse of teacher-student optimization for propagating knowledge. Experiments are performed on two popular CMH\nbenchmarks, i.e., the MIRFlickr and NUS-WIDE datasets.\nOur approach outperforms all existing unsupervised methods by a large margin</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "hu2024separated", "year": "2024", "title":"Separated Variational Hashing Networks For Cross-modal Retrieval", "abstract": "<p>Cross-modal hashing, due to its low storage cost and high query speed, has been successfully used for similarity search in multimedia retrieval applications. It projects high-dimensional data into a shared isomorphic Hamming space with similar binary codes for semantically-similar data. In some applications, all modalities may not be obtained or trained simultaneously for some reasons, such as privacy, secret, storage limitation, and computational resource limitation. However, most existing cross-modal hashing methods need all modalities to jointly learn the common Hamming space, thus hindering them from handling these problems. In this paper, we propose a novel approach called Separated Variational Hashing Networks (SVHNs) to overcome the above challenge. Firstly, it adopts a label network (LabNet) to exploit available and nonspecific label annotations to learn a latent common Hamming space by projecting each semantic label into a common binary representation. Then, each modality-specific network can separately map the samples of the corresponding modality into their binary semantic codes learned by LabNet. We achieve it by conducting variational inference to match the aggregated posterior of the hashing code of LabNet with an arbitrary prior distribution. The effectiveness and efficiency of our SVHNs are verified by extensive experiments carried out on four widely-used multimedia databases, in comparison with 11 state-of-the-art approaches.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "huang2015hash", "year": "2015", "title":"Hash Function Learning Via Codewords", "abstract": "<p>In this paper we introduce a novel hash learning framework that has two main\ndistinguishing features, when compared to past approaches. First, it utilizes\ncodewords in the Hamming space as ancillary means to accomplish its hash\nlearning task. These codewords, which are inferred from the data, attempt to\ncapture similarity aspects of the data’s hash codes. Secondly and more\nimportantly, the same framework is capable of addressing supervised,\nunsupervised and, even, semi-supervised hash learning tasks in a natural\nmanner. A series of comparative experiments focused on content-based image\nretrieval highlights its performance advantages.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "huang2017online", "year": "2017", "title":"Online Hashing", "abstract": "<p>Although hash function learning algorithms have achieved great success in\nrecent years, most existing hash models are off-line, which are not suitable\nfor processing sequential or online data. To address this problem, this work\nproposes an online hash model to accommodate data coming in stream for online\nlearning. Specifically, a new loss function is proposed to measure the\nsimilarity loss between a pair of data samples in hamming space. Then, a\nstructured hash model is derived and optimized in a passive-aggressive way.\nTheoretical analysis on the upper bound of the cumulative loss for the proposed\nonline hash model is provided. Furthermore, we extend our online hashing from a\nsingle-model to a multi-model online hashing that trains multiple models so as\nto retain diverse online hashing models in order to avoid biased update. The\ncompetitive efficiency and effectiveness of the proposed online hash models are\nverified through extensive experiments on several large-scale datasets as\ncompared to related hashing methods.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "huang2017unsupervised", "year": "2017", "title":"Unsupervised Triplet Hashing For Fast Image Retrieval", "abstract": "<p>Hashing has played a pivotal role in large-scale image retrieval. With the\ndevelopment of Convolutional Neural Network (CNN), hashing learning has shown\ngreat promise. But existing methods are mostly tuned for classification, which\nare not optimized for retrieval tasks, especially for instance-level retrieval.\nIn this study, we propose a novel hashing method for large-scale image\nretrieval. Considering the difficulty in obtaining labeled datasets for image\nretrieval task in large scale, we propose a novel CNN-based unsupervised\nhashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised\nhashing network is designed under the following three principles: 1) more\ndiscriminative representations for image retrieval; 2) minimum quantization\nloss between the original real-valued feature descriptors and the learned hash\ncodes; 3) maximum information entropy for the learned hash codes. Extensive\nexperiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH\noutperforms several state-of-the-art unsupervised hashing methods in terms of\nretrieval accuracy.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "huang2019learning", "year": "2019", "title":"Learning Hash Function Through Codewords", "abstract": "<p>In this paper, we propose a novel hash learning approach that has the\nfollowing main distinguishing features, when compared to past frameworks.\nFirst, the codewords are utilized in the Hamming space as ancillary techniques\nto accomplish its hash learning task. These codewords, which are inferred from\nthe data, attempt to capture grouping aspects of the data’s hash codes.\nFurthermore, the proposed framework is capable of addressing supervised,\nunsupervised and, even, semi-supervised hash learning scenarios. Additionally,\nthe framework adopts a regularization term over the codewords, which\nautomatically chooses the codewords for the problem. To efficiently solve the\nproblem, one Block Coordinate Descent algorithm is showcased in the paper. We\nalso show that one step of the algorithms can be casted into several Support\nVector Machine problems which enables our algorithms to utilize efficient\nsoftware package. For the regularization term, a closed form solution of the\nproximal operator is provided in the paper. A series of comparative experiments\nfocused on content-based image retrieval highlights its performance advantages.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "huang2022sah", "year": "2022", "title":"SAH Shifting-aware Asymmetric Hashing For Reverse k-maximum Inner Product Search", "abstract": "<p>This paper investigates a new yet challenging problem called Reverse\n\\(k\\)-Maximum Inner Product Search (R\\(k\\)MIPS). Given a query (item) vector, a set\nof item vectors, and a set of user vectors, the problem of R\\(k\\)MIPS aims to\nfind a set of user vectors whose inner products with the query vector are one\nof the \\(k\\) largest among the query and item vectors. We propose the first\nsubquadratic-time algorithm, i.e., Shifting-aware Asymmetric Hashing (SAH), to\ntackle the R\\(k\\)MIPS problem. To speed up the Maximum Inner Product Search\n(MIPS) on item vectors, we design a shifting-invariant asymmetric\ntransformation and develop a novel sublinear-time Shifting-Aware Asymmetric\nLocality Sensitive Hashing (SA-ALSH) scheme. Furthermore, we devise a new\nblocking strategy based on the Cone-Tree to effectively prune user vectors (in\na batch). We prove that SAH achieves a theoretical guarantee for solving the\nRMIPS problem. Experimental results on five real-world datasets show that SAH\nruns 4\\(\\sim\\)8\\(\\times\\) faster than the state-of-the-art methods for R\\(k\\)MIPS\nwhile achieving F1-scores of over 90\\%. The code is available at\n\\url{https://github.com/HuangQiang/SAH}.</p>\n", "tags": ["ARXIV","Has Code","Independent"] },
{"key": "huang2023lightweight", "year": "2023", "title":"Lightweight-yet-efficient Revitalizing Ball-tree For Point-to-hyperplane Nearest Neighbor Search", "abstract": "<p>Finding the nearest neighbor to a hyperplane (or Point-to-Hyperplane Nearest\nNeighbor Search, simply P2HNNS) is a new and challenging problem with\napplications in many research domains. While existing state-of-the-art hashing\nschemes (e.g., NH and FH) are able to achieve sublinear time complexity without\nthe assumption of the data being in a unit hypersphere, they require an\nasymmetric transformation, which increases the data dimension from \\(d\\) to\n\\(Ω(d^2)\\). This leads to considerable overhead for indexing and incurs\nsignificant distortion errors.\n  In this paper, we investigate a tree-based approach for solving P2HNNS using\nthe classical Ball-Tree index. Compared to hashing-based methods, tree-based\nmethods usually require roughly linear costs for construction, and they provide\ndifferent kinds of approximations with excellent flexibility. A simple\nbranch-and-bound algorithm with a novel lower bound is first developed on\nBall-Tree for performing P2HNNS. Then, a new tree structure named BC-Tree,\nwhich maintains the Ball and Cone structures in the leaf nodes of Ball-Tree, is\ndescribed together with two effective strategies, i.e., point-level pruning and\ncollaborative inner product computing. BC-Tree inherits both the low\nconstruction cost and lightweight property of Ball-Tree while providing a\nsimilar or more efficient search. Experimental results over 16 real-world data\nsets show that Ball-Tree and BC-Tree are around 1.1\\(\\sim\\)10\\(\\times\\) faster than\nNH and FH, and they can reduce the index size and indexing time by about\n1\\(\\sim\\)3 orders of magnitudes on average. The code is available at\n\\url{https://github.com/HuangQiang/BC-Tree}.</p>\n", "tags": ["ARXIV","Has Code"] },
{"key": "huang2024accelerate", "year": "2024", "title":"Accelerate Learning Of Deep Hashing With Gradient Attention", "abstract": "<p>Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Independent"] },
{"key": "huijben2024residual", "year": "2024", "title":"Residual Quantization With Implicit Neural Codebooks", "abstract": "<p>Vector quantization is a fundamental operation for data compression and\nvector search. To obtain high accuracy, multi-codebook methods represent each\nvector using codewords across several codebooks. Residual quantization (RQ) is\none such method, which iteratively quantizes the error of the previous step.\nWhile the error distribution is dependent on previously-selected codewords,\nthis dependency is not accounted for in conventional RQ as it uses a fixed\ncodebook per quantization step. In this paper, we propose QINCo, a neural RQ\nvariant that constructs specialized codebooks per step that depend on the\napproximation of the vector from previous steps. Experiments show that QINCo\noutperforms state-of-the-art methods by a large margin on several datasets and\ncode sizes. For example, QINCo achieves better nearest-neighbor search accuracy\nusing 12-byte codes than the state-of-the-art UNQ using 16 bytes on the\nBigANN1M and Deep1M datasets.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "huiskes2024mir", "year": "2024", "title":"The MIR Flickr Retrieval Evaluation.", "abstract": "<p>In most well known image retrieval test sets, the imagery\ntypically cannot be freely distributed or is not representative of a\nlarge community of users. In this paper we present a collection\nfor the MIR community comprising 25000 images from the Flickr\nwebsite which are redistributable for research purposes and\nrepresent a real community of users both in the image content and\nimage tags. We have extracted the tags and EXIF image metadata,\nand also make all of these publicly available. In addition we\ndiscuss several challenges for benchmarking retrieval and\nclassification methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "huynh2018fast", "year": "2018", "title":"Fast Binary Embeddings And Quantized Compressed Sensing With Structured Matrices", "abstract": "<p>This paper deals with two related problems, namely distance-preserving binary\nembeddings and quantization for compressed sensing . First, we propose fast\nmethods to replace points from a subset \\(\\mathcal{X} \\subset \\mathbb{R}^n\\),\nassociated with the Euclidean metric, with points in the cube \\(\\{\\pm 1\\}^m\\) and\nwe associate the cube with a pseudo-metric that approximates Euclidean distance\namong points in \\(\\mathcal{X}\\). Our methods rely on quantizing fast\nJohnson-Lindenstrauss embeddings based on bounded orthonormal systems and\npartial circulant ensembles, both of which admit fast transforms. Our\nquantization methods utilize noise-shaping, and include Sigma-Delta schemes and\ndistributed noise-shaping schemes. The resulting approximation errors decay\npolynomially and exponentially fast in \\(m\\), depending on the embedding method.\nThis dramatically outperforms the current decay rates associated with binary\nembeddings and Hamming distances. Additionally, it is the first such binary\nembedding result that applies to fast Johnson-Lindenstrauss maps while\npreserving \\(ℓ₂\\) norms.\n  Second, we again consider noise-shaping schemes, albeit this time to quantize\ncompressed sensing measurements arising from bounded orthonormal ensembles and\npartial circulant matrices. We show that these methods yield a reconstruction\nerror that again decays with the number of measurements (and bits), when using\nconvex optimization for reconstruction. Specifically, for Sigma-Delta schemes,\nthe error decays polynomially in the number of measurements, and it decays\nexponentially for distributed noise-shaping schemes based on beta encoding.\nThese results are near optimal and the first of their kind dealing with bounded\northonormal systems.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "hyvönen2015fast", "year": "2015", "title":"Fast K-nn Search", "abstract": "<p>Efficient index structures for fast approximate nearest neighbor queries are\nrequired in many applications such as recommendation systems. In\nhigh-dimensional spaces, many conventional methods suffer from excessive usage\nof memory and slow response times. We propose a method where multiple random\nprojection trees are combined by a novel voting scheme. The key idea is to\nexploit the redundancy in a large number of candidate sets obtained by\nindependently generated random projections in order to reduce the number of\nexpensive exact distance evaluations. The method is straightforward to\nimplement using sparse projections which leads to a reduced memory footprint\nand fast index construction. Furthermore, it enables grouping of the required\ncomputations into big matrix multiplications, which leads to additional savings\ndue to cache effects and low-level parallelization. We demonstrate by extensive\nexperiments on a wide variety of data sets that the method is faster than\nexisting partitioning tree or hashing based approaches, making it the fastest\navailable technique on high accuracy levels.</p>\n", "tags": ["ICML","Independent"] },
{"key": "hyvönen2022multilabel", "year": "2022", "title":"A Multilabel Classification Framework For Approximate Nearest Neighbor Search", "abstract": "<p>Both supervised and unsupervised machine learning algorithms have been used to learn partition-based index structures for approximate nearest neighbor (ANN) search. Existing supervised algorithms formulate the learning task as finding a partition in which the nearest neighbors of a training set point belong to the same partition element as the point itself, so that the nearest neighbor candidates can be retrieved by naive lookup or backtracking search. We formulate candidate set selection in ANN search directly as a multilabel classification problem where the labels correspond to the nearest neighbors of the query point, and interpret the partitions as partitioning classifiers for solving this task. Empirical results suggest that the natural classifier based on this interpretation leads to strictly improved performance when combined with any unsupervised or supervised partitioning strategy. We also prove a sufficient condition for consistency of a partitioning classifier for ANN search, and illustrate the result by verifying this condition for chronological \\(k\\)-d trees.</p>\n", "tags": ["NEURIPS","Supervised"] },
{"key": "iida2018robust", "year": "2018", "title":"Robust Image Identification For Double-compressed JPEG Images", "abstract": "<p>It is known that JPEG images uploaded to social networks (SNs) are mostly\nre-compressed by the social network providers. Because of such a situation, a\nnew image identification scheme for double-compressed JPEG images is proposed\nin this paper. The aim is to detect single-compressed images that have the same\noriginal image as that of a double-compressed one. In the proposed scheme, the\nsigns of only DC coefficients in DCT coefficients and one threshold value are\nused for the identification. The use of them allows us to robustly avoid errors\ncaused by double-compression, which are not considered in conventional schemes.\nThe proposed scheme has applications not only to find uploaded images\ncorresponding to double-compressed ones, but also to detect some image\nintegrity. The simulation results demonstrate that the proposed one outperforms\nconventional ones including state-of-art image hashing one in terms of the\nquerying performance.</p>\n", "tags": ["ARXIV"] },
{"key": "imagenet2009using", "year": "2009", "title":"ImageNet: A large-scale hierarchical image database", "abstract": "<p>The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</p>\n", "tags": ["Dataset"] },
{"key": "indyk2017practical", "year": "2017", "title":"Practical Data-dependent Metric Compression With Provable Guarantees", "abstract": "<p>We introduce a new distance-preserving compact representation of multi-dimensional point-sets. Given n points in a d-dimensional space where each coordinate is represented using B bits (i.e., dB bits per point), it produces  a representation of size O( d log(d B/epsilon) +log n) bits per point from which one can approximate the distances up to a factor of 1 + epsilon. Our algorithm almost matches the recent bound of Indyk et al, 2017} while being much simpler. We compare our algorithm to Product Quantization (PQ) (Jegou et al, 2011) a state of the art heuristic metric compression method. We evaluate both algorithms on several data sets: SIFT, MNIST, New York City taxi time series and a synthetic one-dimensional data set embedded in a high-dimensional space. Our algorithm produces representations that are comparable to or better than those produced by PQ, while having provable guarantees on its performance.</p>\n", "tags": ["NEURIPS","Quantisation"] },
{"key": "indyk2018approximate", "year": "2018", "title":"Approximate Nearest Neighbors In Limited Space", "abstract": "<p>We consider the \\((1+\\epsilon)\\)-approximate nearest neighbor search problem:\ngiven a set \\(X\\) of \\(n\\) points in a \\(d\\)-dimensional space, build a data\nstructure that, given any query point \\(y\\), finds a point \\(x \\in X\\) whose\ndistance to \\(y\\) is at most \\((1+\\epsilon) \\min_{x \\in X} |x-y|\\) for an\naccuracy parameter \\(\\epsilon \\in (0,1)\\). Our main result is a data structure\nthat occupies only \\(O(\\epsilon^{-2} n log(n) log(1/\\epsilon))\\) bits of space,\nassuming all point coordinates are integers in the range \\(\\{-n^{O(1)} \\ldots\nn^{O(1)}\\}\\), i.e., the coordinates have \\(O(log n)\\) bits of precision. This\nimproves over the best previously known space bound of \\(O(\\epsilon^{-2} n\nlog(n)^2)\\), obtained via the randomized dimensionality reduction method of\nJohnson and Lindenstrauss (1984). We also consider the more general problem of\nestimating all distances from a collection of query points to all data points\n\\(X\\), and provide almost tight upper and lower bounds for the space complexity\nof this problem.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "indyk2023worst", "year": "2023", "title":"Worst-case Performance Of Popular Approximate Nearest Neighbor Search Implementations Guarantees And Limitations", "abstract": "<p>Graph-based approaches to nearest neighbor search are popular and powerful\ntools for handling large datasets in practice, but they have limited\ntheoretical guarantees. We study the worst-case performance of recent\ngraph-based approximate nearest neighbor search algorithms, such as HNSW, NSG\nand DiskANN. For DiskANN, we show that its “slow preprocessing” version\nprovably supports approximate nearest neighbor search query with constant\napproximation ratio and poly-logarithmic query time, on data sets with bounded\n“intrinsic” dimension. For the other data structure variants studied, including\nDiskANN with “fast preprocessing”, HNSW and NSG, we present a family of\ninstances on which the empirical query time required to achieve a “reasonable”\naccuracy is linear in instance size. For example, for DiskANN, we show that the\nquery procedure can take at least \\(0.1 n\\) steps on instances of size \\(n\\) before\nit encounters any of the \\(5\\) nearest neighbors of the query.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "irie2024locally", "year": "2024", "title":"Locally Linear Hashing For Extracting Non-linear Manifolds", "abstract": "<p>Previous efforts in hashing intend to preserve data variance\nor pairwise affinity, but neither is adequate in capturing\nthe manifold structures hidden in most visual data. In\nthis paper, we tackle this problem by reconstructing the locally\nlinear structures of manifolds in the binary Hamming\nspace, which can be learned by locality-sensitive sparse\ncoding. We cast the problem as a joint minimization of\nreconstruction error and quantization loss, and show that,\ndespite its NP-hardness, a local optimum can be obtained\nefficiently via alternative optimization. Our method distinguishes\nitself from existing methods in its remarkable ability\nto extract the nearest neighbors of the query from the\nsame manifold, instead of from the ambient space. On extensive\nexperiments on various image benchmarks, our results\nimprove previous state-of-the-art by 28-74% typically,\nand 627% on the Yale face data.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "iscen2014memory", "year": "2014", "title":"Memory Vectors For Similarity Search In High-dimensional Spaces", "abstract": "<p>We study an indexing architecture to store and search in a database of\nhigh-dimensional vectors from the perspective of statistical signal processing\nand decision theory. This architecture is composed of several memory units,\neach of which summarizes a fraction of the database by a single representative\nvector. The potential similarity of the query to one of the vectors stored in\nthe memory unit is gauged by a simple correlation with the memory unit’s\nrepresentative vector. This representative optimizes the test of the following\nhypothesis: the query is independent from any vector in the memory unit vs. the\nquery is a simple perturbation of one of the stored vectors.\n  Compared to exhaustive search, our approach finds the most similar database\nvectors significantly faster without a noticeable reduction in search quality.\nInterestingly, the reduction of complexity is provably better in\nhigh-dimensional spaces. We empirically demonstrate its practical interest in a\nlarge-scale image search scenario with off-the-shelf state-of-the-art\ndescriptors.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "iscen2017fast", "year": "2017", "title":"Fast Spectral Ranking For Similarity Search", "abstract": "<p>Despite the success of deep learning on representing images for particular\nobject retrieval, recent studies show that the learned representations still\nlie on manifolds in a high dimensional space. This makes the Euclidean nearest\nneighbor search biased for this task. Exploring the manifolds online remains\nexpensive even if a nearest neighbor graph has been computed offline. This work\nintroduces an explicit embedding reducing manifold search to Euclidean search\nfollowed by dot product similarity search. This is equivalent to linear graph\nfiltering of a sparse signal in the frequency domain. To speed up online\nsearch, we compute an approximate Fourier basis of the graph offline. We\nimprove the state of art on particular object retrieval datasets including the\nchallenging Instre dataset containing small objects. At a scale of 10^5 images,\nthe offline cost is only a few hours, while query time is comparable to\nstandard similarity search.</p>\n", "tags": ["ARXIV","Deep Learning","Graph"] },
{"key": "iscen2018local", "year": "2018", "title":"Local Orthogonal-group Testing", "abstract": "<p>This work addresses approximate nearest neighbor search applied in the domain\nof large-scale image retrieval. Within the group testing framework we propose\nan efficient off-line construction of the search structures. The linear-time\ncomplexity orthogonal grouping increases the probability that at most one\nelement from each group is matching to a given query. Non-maxima suppression\nwith each group efficiently reduces the number of false positive results at no\nextra cost. Unlike in other well-performing approaches, all processing is\nlocal, fast, and suitable to process data in batches and in parallel. We\nexperimentally show that the proposed method achieves search accuracy of the\nexhaustive search with significant reduction in the search complexity. The\nmethod can be naturally combined with existing embedding methods.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "ishaq2019clustered", "year": "2019", "title":"Clustered Hierarchical Entropy-scaling Search Of Astronomical And Biological Data", "abstract": "<p>Both astronomy and biology are experiencing explosive growth of data,\nresulting in a “big data” problem that stands in the way of a “big data”\nopportunity for discovery. One common question asked of such data is that of\napproximate search (\\(\\rho-\\)nearest neighbors search). We present a hierarchical\nsearch algorithm for such data sets that takes advantage of particular\ngeometric properties apparent in both astronomical and biological data sets,\nnamely the metric entropy and fractal dimensionality of the data. We present\nCHESS (Clustered Hierarchical Entropy-Scaling Search), a search tool with\nvirtually no loss in specificity or sensitivity, demonstrating a \\(13.6\\times\\)\nspeedup over linear search on the Sloan Digital Sky Survey’s APOGEE data set\nand a \\(68\\times\\) speedup on the GreenGenes 16S metagenomic data set, as well as\nasymptotically fewer distance comparisons on APOGEE when compared to the\nFALCONN locality-sensitive hashing library. CHESS demonstrates an asymptotic\ncomplexity not directly dependent on data set size, and is in practice at least\nan order of magnitude faster than linear search by performing fewer distance\ncomparisons. Unlike locality-sensitive hashing approaches, CHESS can work with\nany user-defined distance function. CHESS also allows for implicit data\ncompression, which we demonstrate on the APOGEE data set. We also discuss an\nextension allowing for efficient k-nearest neighbors search.</p>\n", "tags": ["ARXIV","Survey Paper"] },
{"key": "ishikawa2015pairwise", "year": "2015", "title":"Pairwise Rotation Hashing For High-dimensional Features", "abstract": "<p>Binary Hashing is widely used for effective approximate nearest neighbors\nsearch. Even though various binary hashing methods have been proposed, very few\nmethods are feasible for extremely high-dimensional features often used in\nvisual tasks today. We propose a novel highly sparse linear hashing method\nbased on pairwise rotations. The encoding cost of the proposed algorithm is\n\\(\\mathrm{O}(n log n)\\) for n-dimensional features, whereas that of the existing\nstate-of-the-art method is typically \\(\\mathrm{O}(n^2)\\). The proposed method is\nalso remarkably faster in the learning phase. Along with the efficiency, the\nretrieval accuracy is comparable to or slightly outperforming the\nstate-of-the-art. Pairwise rotations used in our method are formulated from an\nanalytical study of the trade-off relationship between quantization error and\nentropy of binary codes. Although these hashing criteria are widely used in\nprevious researches, its analytical behavior is rarely studied. All building\nblocks of our algorithm are based on the analytical solution, and it thus\nprovides a fairly simple and efficient procedure.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "islam2024spatially", "year": "2024", "title":"Spatially Optimized Compact Deep Metric Learning Model For Similarity Search", "abstract": "<p>Spatial optimization is often overlooked in many computer vision tasks.\nFilters should be able to recognize the features of an object regardless of\nwhere it is in the image. Similarity search is a crucial task where spatial\nfeatures decide an important output. The capacity of convolution to capture\nvisual patterns across various locations is limited. In contrast to\nconvolution, the involution kernel is dynamically created at each pixel based\non the pixel value and parameters that have been learned. This study\ndemonstrates that utilizing a single layer of involution feature extractor\nalongside a compact convolution model significantly enhances the performance of\nsimilarity search. Additionally, we improve predictions by using the GELU\nactivation function rather than the ReLU. The negligible amount of weight\nparameters in involution with a compact model with better performance makes the\nmodel very useful in real-world implementations. Our proposed model is below 1\nmegabyte in size. We have experimented with our proposed methodology and other\nmodels on CIFAR-10, FashionMNIST, and MNIST datasets. Our proposed method\noutperforms across all three datasets.</p>\n", "tags": ["ARXIV"] },
{"key": "ivanchykhin2016regular", "year": "2016", "title":"Regular And Almost Universal Hashing An Efficient Implementation", "abstract": "<p>Random hashing can provide guarantees regarding the performance of data\nstructures such as hash tables—even in an adversarial setting. Many existing\nfamilies of hash functions are universal: given two data objects, the\nprobability that they have the same hash value is low given that we pick hash\nfunctions at random. However, universality fails to ensure that all hash\nfunctions are well behaved. We further require regularity: when picking data\nobjects at random they should have a low probability of having the same hash\nvalue, for any fixed hash function. We present the efficient implementation of\na family of non-cryptographic hash functions (PM+) offering good running times,\ngood memory usage as well as distinguishing theoretical guarantees: almost\nuniversality and component-wise regularity. On a variety of platforms, our\nimplementations are comparable to the state of the art in performance. On\nrecent Intel processors, PM+ achieves a speed of 4.7 bytes per cycle for 32-bit\noutputs and 3.3 bytes per cycle for 64-bit outputs. We review vectorization\nthrough SIMD instructions (e.g., AVX2) and optimizations for superscalar\nexecution.</p>\n", "tags": ["Graph","Independent","Survey Paper"] },
{"key": "izadinia2018viser", "year": "2018", "title":"VISER Visual Self-regularization", "abstract": "<p>In this work, we propose the use of large set of unlabeled images as a source\nof regularization data for learning robust visual representation. Given a\nvisual model trained by a labeled dataset in a supervised fashion, we augment\nour training samples by incorporating large number of unlabeled data and train\na semi-supervised model. We demonstrate that our proposed learning approach\nleverages an abundance of unlabeled images and boosts the visual recognition\nperformance which alleviates the need to rely on large labeled datasets for\nlearning robust representation. To increment the number of image instances\nneeded to learn robust visual models in our approach, each labeled image\npropagates its label to its nearest unlabeled image instances. These retrieved\nunlabeled images serve as local perturbations of each labeled image to perform\nVisual Self-Regularization (VISER). To retrieve such visual self regularizers,\nwe compute the cosine similarity in a semantic space defined by the penultimate\nlayer in a fully convolutional neural network. We use the publicly available\nYahoo Flickr Creative Commons 100M dataset as the source of our unlabeled image\nset and propose a distributed approximate nearest neighbor algorithm to make\nretrieval practical at that scale. Using the labeled instances and their\nregularizer samples we show that we significantly improve object categorization\nand localization performance on the MS COCO and Visual Genome datasets where\nobjects appear in context.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "jacques2013quantized", "year": "2013", "title":"A Quantized Johnson Lindenstrauss Lemma The Finding Of Buffons Needle", "abstract": "<p>In 1733, Georges-Louis Leclerc, Comte de Buffon in France, set the ground of\ngeometric probability theory by defining an enlightening problem: What is the\nprobability that a needle thrown randomly on a ground made of equispaced\nparallel strips lies on two of them? In this work, we show that the solution to\nthis problem, and its generalization to \\(N\\) dimensions, allows us to discover a\nquantized form of the Johnson-Lindenstrauss (JL) Lemma, i.e., one that combines\na linear dimensionality reduction procedure with a uniform quantization of\nprecision \\(\\delta&gt;0\\). In particular, given a finite set \\(\\mathcal S \\subset\n\\mathbb R^N\\) of \\(S\\) points and a distortion level \\(\\epsilon&gt;0\\), as soon as \\(M &gt;\nM_0 = O(\\epsilon^{-2} log S)\\), we can (randomly) construct a mapping from\n\\((\\mathcal S, ℓ₂)\\) to \\((\\delta\\mathbb Z^M, \\ell_1)\\) that approximately\npreserves the pairwise distances between the points of \\(\\mathcal S\\).\nInterestingly, compared to the common JL Lemma, the mapping is quasi-isometric\nand we observe both an additive and a multiplicative distortions on the\nembedded distances. These two distortions, however, decay as \\(O(\\sqrt{(log\nS)/M})\\) when \\(M\\) increases. Moreover, for coarse quantization, i.e., for high\n\\(\\delta\\) compared to the set radius, the distortion is mainly additive, while\nfor small \\(\\delta\\) we tend to a Lipschitz isometric embedding. Finally, we\nprove the existence of a “nearly” quasi-isometric embedding of \\((\\mathcal S,\nℓ₂)\\) into \\((\\delta\\mathbb Z^M, ℓ₂)\\). This one involves a non-linear\ndistortion of the \\(ℓ₂\\)-distance in \\(\\mathcal S\\) that vanishes for distant\npoints in this set. Noticeably, the additive distortion in this case is slower,\nand decays as \\(O(\\sqrt[4]{(log S)/M})\\).</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "jacques2016time", "year": "2016", "title":"Time For Dithering Fast And Quantized Random Embeddings Via The Restricted Isometry Property", "abstract": "<p>Recently, many works have focused on the characterization of non-linear\ndimensionality reduction methods obtained by quantizing linear embeddings,\ne.g., to reach fast processing time, efficient data compression procedures,\nnovel geometry-preserving embeddings or to estimate the information/bits stored\nin this reduced data representation. In this work, we prove that many linear\nmaps known to respect the restricted isometry property (RIP) can induce a\nquantized random embedding with controllable multiplicative and additive\ndistortions with respect to the pairwise distances of the data points beings\nconsidered. In other words, linear matrices having fast matrix-vector\nmultiplication algorithms (e.g., based on partial Fourier ensembles or on the\nadjacency matrix of unbalanced expanders) can be readily used in the definition\nof fast quantized embeddings with small distortions. This implication is made\npossible by applying right after the linear map an additive and random “dither”\nthat stabilizes the impact of the uniform scalar quantization operator applied\nafterwards. For different categories of RIP matrices, i.e., for different\nlinear embeddings of a metric space \\((\\mathcal K \\subset \\mathbb R^n, \\ell_q)\\)\nin \\((\\mathbb R^m, \\ell_p)\\) with \\(p,q \\geq 1\\), we derive upper bounds on the\nadditive distortion induced by quantization, showing that it decays either when\nthe embedding dimension \\(m\\) increases or when the distance of a pair of\nembedded vectors in \\(\\mathcal K\\) decreases. Finally, we develop a novel\n“bi-dithered” quantization scheme, which allows for a reduced distortion that\ndecreases when the embedding dimension grows and independently of the\nconsidered pair of vectors.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "jafari2019efficient", "year": "2019", "title":"Efficient Bitmap-based Indexing And Retrieval Of Similarity Search Image Queries", "abstract": "<p>Finding similar images is a necessary operation in many multimedia\napplications. Images are often represented and stored as a set of\nhigh-dimensional features, which are extracted using localized feature\nextraction algorithms. Locality Sensitive Hashing is one of the most popular\napproximate processing techniques for finding similar points in\nhigh-dimensional spaces. Locality Sensitive Hashing (LSH) and its variants are\ndesigned to find similar points, but they are not designed to find objects\n(such as images, which are made up of a collection of points) efficiently. In\nthis paper, we propose an index structure, Bitmap-Image LSH (bImageLSH), for\nefficient processing of high-dimensional images. Using a real dataset, we\nexperimentally show the performance benefit of our novel design while keeping\nthe accuracy of the image results high.</p>\n", "tags": ["Independent","LSH"] },
{"key": "jafari2022experimental", "year": "2022", "title":"Experimental Analysis Of Machine Learning Techniques For Finding Search Radius In Locality Sensitive Hashing", "abstract": "<p>Finding similar data in high-dimensional spaces is one of the important tasks\nin multimedia applications. Approaches introduced to find exact searching\ntechniques often use tree-based index structures which are known to suffer from\nthe curse of the dimensionality problem that limits their performance.\nApproximate searching techniques prefer performance over accuracy and they\nreturn good enough results while achieving a better performance. Locality\nSensitive Hashing (LSH) is one of the most popular approximate nearest neighbor\nsearch techniques for high-dimensional spaces. One of the most time-consuming\nprocesses in LSH is to find the neighboring points in the projected spaces. An\nimproved LSH-based index structure, called radius-optimized Locality Sensitive\nHashing (roLSH) has been proposed to utilize Machine Learning and efficiently\nfind these neighboring points; thus, further improve the overall performance of\nLSH. In this paper, we extend roLSH by experimentally studying the effect of\ndifferent types of famous Machine Learning techniques on overall performance.\nWe compare ten regression techniques on four real-world datasets and show that\nNeural Network-based techniques are the best fit to be used in roLSH as their\naccuracy and performance trade-off are the best compared to the other\ntechniques.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "jagadeesan2019understanding", "year": "2019", "title":"Understanding Sparse JL For Feature Hashing", "abstract": "<p>Feature hashing and other random projection schemes are commonly used to reduce the dimensionality of feature vectors. The goal is to efficiently project a high-dimensional feature vector living in R^n into a much lower-dimensional space R^m, while approximately preserving Euclidean norm. These schemes can be constructed using sparse random projections, for example using a sparse Johnson-Lindenstrauss (JL) transform. A line of work introduced by Weinberger et. al (ICML ‘09) analyzes the accuracy of sparse JL with sparsity 1 on feature vectors with small linfinity-to-l2 norm ratio. Recently, Freksen, Kamma, and Larsen (NeurIPS ‘18) closed this line of work by proving a tight tradeoff between linfinity-to-l2 norm ratio and accuracy for sparse JL with sparsity 1. In this paper, we demonstrate the benefits of using sparsity s greater than 1 in sparse JL on feature vectors. Our main result is a tight tradeoff between linfinity-to-l2 norm ratio and accuracy for a general sparsity s, that significantly generalizes the result of Freksen et. al. Our result theoretically demonstrates that sparse JL with s &gt; 1 can have significantly better norm-preservation properties on feature vectors than sparse JL with s = 1; we also empirically demonstrate this finding.</p>\n", "tags": ["ICML","Independent","NEURIPS"] },
{"key": "jain2008online", "year": "2008", "title":"Online Metric Learning And Fast Similarity Search", "abstract": "<p>Metric learning algorithms can provide useful distance functions for a variety of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However, in many real applications, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned metric. Existing online algorithms offer bounds on worst-case performance, but typically do not perform well in practice as compared to their offline counterparts. We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds, and empirically compare the proposed method against existing online metric learning algorithms. To further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efficient updates for approximate similarity search data structures. We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines.</p>\n", "tags": ["NEURIPS"] },
{"key": "jain2010hashing", "year": "2010", "title":"Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning", "abstract": "<p>We consider the problem of retrieving the database points nearest to a given {\\em hyperplane} query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method’s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points.</p>\n", "tags": ["NEURIPS"] },
{"key": "jain2016approximate", "year": "2016", "title":"Approximate Search With Quantized Sparse Representations", "abstract": "<p>This paper tackles the task of storing a large collection of vectors, such as\nvisual descriptors, and of searching in it. To this end, we propose to\napproximate database vectors by constrained sparse coding, where possible atom\nweights are restricted to belong to a finite subset. This formulation\nencompasses, as particular cases, previous state-of-the-art methods such as\nproduct or residual quantization. As opposed to traditional sparse coding\nmethods, quantized sparse coding includes memory usage as a design constraint,\nthereby allowing us to index a large collection such as the BIGANN\nbillion-sized benchmark. Our experiments, carried out on standard benchmarks,\nshow that our formulation leads to competitive solutions when considering\ndifferent trade-offs between learning/coding time, index size and search\nquality.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "jain2017compact", "year": "2017", "title":"Compact Environment-invariant Codes For Robust Visual Place Recognition", "abstract": "<p>Robust visual place recognition (VPR) requires scene representations that are\ninvariant to various environmental challenges such as seasonal changes and\nvariations due to ambient lighting conditions during day and night. Moreover, a\npractical VPR system necessitates compact representations of environmental\nfeatures. To satisfy these requirements, in this paper we suggest a\nmodification to the existing pipeline of VPR systems to incorporate supervised\nhashing. The modified system learns (in a supervised setting) compact binary\ncodes from image feature descriptors. These binary codes imbibe robustness to\nthe visual variations exposed to it during the training phase, thereby, making\nthe system adaptive to severe environmental changes. Also, incorporating\nsupervised hashing makes VPR computationally more efficient and easy to\nimplement on simple hardware. This is because binary embeddings can be learned\nover simple-to-compute features and the distance computation is also in the\nlow-dimensional hamming space of binary codes. We have performed experiments on\nseveral challenging data sets covering seasonal, illumination and viewpoint\nvariations. We also compare two widely used supervised hashing methods of\nCCAITQ and MLH and show that this new pipeline out-performs or closely matches\nthe state-of-the-art deep learning VPR methods that are based on\nhigh-dimensional features extracted from pre-trained deep convolutional neural\nnetworks.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "jain2017subic", "year": "2017", "title":"SUBIC A Supervised Structured Binary Code For Image Search", "abstract": "<p>For large-scale visual search, highly compressed yet meaningful\nrepresentations of images are essential. Structured vector quantizers based on\nproduct quantization and its variants are usually employed to achieve such\ncompression while minimizing the loss of accuracy. Yet, unlike binary hashing\nschemes, these unsupervised methods have not yet benefited from the\nsupervision, end-to-end learning and novel architectures ushered in by the deep\nlearning revolution. We hence propose herein a novel method to make deep\nconvolutional neural networks produce supervised, compact, structured binary\ncodes for visual search. Our method makes use of a novel block-softmax\nnon-linearity and of batch-based entropy losses that together induce structure\nin the learned encodings. We show that our method outperforms state-of-the-art\ncompact representations based on deep hashing or structured quantization in\nsingle and cross-domain category retrieval, instance retrieval and\nclassification. We make our code and models publicly available online.</p>\n", "tags": ["ARXIV","Cross Modal","Deep Learning","Quantisation","Supervised"] },
{"key": "jain2024fast", "year": "2024", "title":"Fast Similarity Search For Learned Metrics", "abstract": "<p>We propose a method to efficiently index into a large database of examples according to a learned metric.\nGiven a collection of examples, we learn a Mahalanobis distance using an information-theoretic metric\nlearning technique that adapts prior knowledge about pairwise distances to incorporate similarity and dissimilarity\nconstraints. To enable sub-linear time similarity search under the learned metric, we show how\nto encode a learned Mahalanobis parameterization into randomized locality-sensitive hash functions. We\nfurther formulate an indirect solution that enables metric learning and hashing for sparse input vector spaces\nwhose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions.\nWe demonstrate the approach applied to systems and image datasets, and show that our learned metrics\nimprove accuracy relative to commonly-used metric baselines, while our hashing construction permits effi-\ncient indexing with a learned distance and very large databases.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "jain2024hashing", "year": "2024", "title":"Hashing Hyperplane Queries To Near Points With Applications To Large-scale Active Learning", "abstract": "<p>We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the \ndatabase. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method’s preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions \nof unlabeled points.</p>\n", "tags": ["ARXIV"] },
{"key": "jaiswal2022ood", "year": "2022", "title":"Ood-diskann Efficient And Scalable Graph ANNS For Out-of-distribution Queries", "abstract": "<p>State-of-the-art algorithms for Approximate Nearest Neighbor Search (ANNS)\nsuch as DiskANN, FAISS-IVF, and HNSW build data dependent indices that offer\nsubstantially better accuracy and search efficiency over data-agnostic indices\nby overfitting to the index data distribution. When the query data is drawn\nfrom a different distribution - e.g., when index represents image embeddings\nand query represents textual embeddings - such algorithms lose much of this\nperformance advantage. On a variety of datasets, for a fixed recall target,\nlatency is worse by an order of magnitude or more for Out-Of-Distribution (OOD)\nqueries as compared to In-Distribution (ID) queries. The question we address in\nthis work is whether ANNS algorithms can be made efficient for OOD queries if\nthe index construction is given access to a small sample set of these queries.\nWe answer positively by presenting OOD-DiskANN, which uses a sparing sample (1%\nof index set size) of OOD queries, and provides up to 40% improvement in mean\nquery latency over SoTA algorithms of a similar memory footprint. OOD-DiskANN\nis scalable and has the efficiency of graph-based ANNS indices. Some of our\ncontributions can improve query efficiency for ID queries as well.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "james2019deephashing", "year": "2019", "title":"Deephashing Using Tripletloss", "abstract": "<p>Hashing is one of the most efficient techniques for approximate nearest\nneighbour search for large scale image retrieval. Most of the techniques are\nbased on hand-engineered features and do not give optimal results all the time.\nDeep Convolutional Neural Networks have proven to generate very effective\nrepresentation of images that are used for various computer vision tasks and\ninspired by this there have been several Deep Hashing models like Wang et al.\n(2016) have been proposed. These models train on the triplet loss function\nwhich can be used to train models with superior representation capabilities.\nTaking the latest advancements in training using the triplet loss I propose new\ntechniques that help the Deep Hash-ing models train more faster and\nefficiently. Experiment result1show that using the more efficient techniques\nfor training on the triplet loss, we have obtained a 5%percent improvement in\nour model compared to the original work of Wang et al.(2016). Using a larger\nmodel and more training data we can drastically improve the performance using\nthe techniques we propose</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "jang2011linear", "year": "2011", "title":"A Linear-time Approximation Of The Earth Movers Distance", "abstract": "<p>Color descriptors are one of the important features used in content-based\nimage retrieval. The Dominant Color Descriptor (DCD) represents a few\nperceptually dominant colors in an image through color quantization. For image\nretrieval based on DCD, the earth mover’s distance and the optimal color\ncomposition distance are proposed to measure the dissimilarity between two\nimages. Although providing good retrieval results, both methods are too\ntime-consuming to be used in a large image database. To solve the problem, we\npropose a new distance function that calculates an approximate earth mover’s\ndistance in linear time. To calculate the dissimilarity in linear time, the\nproposed approach employs the space-filling curve for multidimensional color\nspace. To improve the accuracy, the proposed approach uses multiple curves and\nadjusts the color positions. As a result, our approach achieves\norder-of-magnitude time improvement but incurs small errors. We have performed\nextensive experiments to show the effectiveness and efficiency of the proposed\napproach. The results reveal that our approach achieves almost the same results\nwith the EMD in linear time.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "jang2020generalized", "year": "2020", "title":"Generalized Product Quantization Network For Semi-supervised Image Retrieval", "abstract": "<p>Image retrieval methods that employ hashing or vector quantization have\nachieved great success by taking advantage of deep learning. However, these\napproaches do not meet expectations unless expensive label information is\nsufficient. To resolve this issue, we propose the first quantization-based\nsemi-supervised image retrieval scheme: Generalized Product Quantization (GPQ)\nnetwork. We design a novel metric learning strategy that preserves semantic\nsimilarity between labeled data, and employ entropy regularization term to\nfully exploit inherent potentials of unlabeled data. Our solution increases the\ngeneralization capacity of the quantization network, which allows overcoming\nprevious limitations in the retrieval community. Extensive experimental results\ndemonstrate that GPQ yields state-of-the-art performance on large-scale real\nimage benchmark datasets.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation","Supervised"] },
{"key": "jang2021deep", "year": "2021", "title":"Deep Hash Distillation For Image Retrieval", "abstract": "<p>In hash-based image retrieval systems, degraded or transformed inputs usually\ngenerate different codes from the original, deteriorating the retrieval\naccuracy. To mitigate this issue, data augmentation can be applied during\ntraining. However, even if augmented samples of an image are similar in real\nfeature space, the quantization can scatter them far away in Hamming space.\nThis results in representation discrepancies that can impede training and\ndegrade performance. In this work, we propose a novel self-distilled hashing\nscheme to minimize the discrepancy while exploiting the potential of augmented\ndata. By transferring the hash knowledge of the weakly-transformed samples to\nthe strong ones, we make the hash code insensitive to various transformations.\nWe also introduce hash proxy-based similarity learning and binary cross\nentropy-based quantization loss to provide fine quality hash codes. Ultimately,\nwe construct a deep hashing framework that not only improves the existing deep\nhashing approaches, but also achieves the state-of-the-art retrieval results.\nExtensive experiments are conducted and confirm the effectiveness of our work.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Quantisation"] },
{"key": "jang2021self", "year": "2021", "title":"Self-supervised Product Quantization For Deep Unsupervised Image Retrieval", "abstract": "<p>Supervised deep learning-based hash and vector quantization are enabling fast\nand large-scale image retrieval systems. By fully exploiting label annotations,\nthey are achieving outstanding retrieval performances compared to the\nconventional methods. However, it is painstaking to assign labels precisely for\na vast amount of training data, and also, the annotation process is\nerror-prone. To tackle these issues, we propose the first deep unsupervised\nimage retrieval method dubbed Self-supervised Product Quantization (SPQ)\nnetwork, which is label-free and trained in a self-supervised manner. We design\na Cross Quantized Contrastive learning strategy that jointly learns codewords\nand deep visual descriptors by comparing individually transformed images\n(views). Our method analyzes the image contents to extract descriptive\nfeatures, allowing us to understand image representations for accurate\nretrieval. By conducting extensive experiments on benchmarks, we demonstrate\nthat the proposed method yields state-of-the-art results even without\nsupervised pretraining.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation","Supervised"] },
{"key": "jang2021similarity", "year": "2021", "title":"Similarity Guided Deep Face Image Retrieval", "abstract": "<p>Face image retrieval, which searches for images of the same identity from the\nquery input face image, is drawing more attention as the size of the image\ndatabase increases rapidly. In order to conduct fast and accurate retrieval, a\ncompact hash code-based methods have been proposed, and recently, deep face\nimage hashing methods with supervised classification training have shown\noutstanding performance. However, classification-based scheme has a\ndisadvantage in that it cannot reveal complex similarities between face images\ninto the hash code learning. In this paper, we attempt to improve the face\nimage retrieval quality by proposing a Similarity Guided Hashing (SGH) method,\nwhich gently considers self and pairwise-similarity simultaneously. SGH employs\nvarious data augmentations designed to explore elaborate similarities between\nface images, solving both intra and inter identity-wise difficulties. Extensive\nexperimental results on the protocols with existing benchmarks and an\nadditionally proposed large scale higher resolution face image dataset\ndemonstrate that our SGH delivers state-of-the-art retrieval performance.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "jang2024distilling", "year": "2024", "title":"Distilling Vision-language Pretraining For Efficient Cross-modal Retrieval", "abstract": "<p>``Learning to hash’’ is a practical solution for efficient retrieval,\noffering fast search speed and low storage cost. It is widely applied in\nvarious applications, such as image-text cross-modal search. In this paper, we\nexplore the potential of enhancing the performance of learning to hash with the\nproliferation of powerful large pre-trained models, such as Vision-Language\nPre-training (VLP) models. We introduce a novel method named Distillation for\nCross-Modal Quantization (DCMQ), which leverages the rich semantic knowledge of\nVLP models to improve hash representation learning. Specifically, we use the\nVLP as a <code class=\"language-plaintext highlighter-rouge\">teacher' to distill knowledge into a </code>student’ hashing model equipped\nwith codebooks. This process involves the replacement of supervised labels,\nwhich are composed of multi-hot vectors and lack semantics, with the rich\nsemantics of VLP. In the end, we apply a transformation termed Normalization\nwith Paired Consistency (NPC) to achieve a discriminative target for\ndistillation. Further, we introduce a new quantization method, Product\nQuantization with Gumbel (PQG) that promotes balanced codebook learning,\nthereby improving the retrieval performance. Extensive benchmark testing\ndemonstrates that DCMQ consistently outperforms existing supervised cross-modal\nhashing approaches, showcasing its significant potential.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "janson2005individual", "year": "2005", "title":"Individual Displacements In Hashing With Coalesced Chains", "abstract": "<p>We study the asymptotic distribution of the displacements in hashing with\ncoalesced chains, for both late-insertion and early-insertion. Asymptotic\nformulas for means and variances follow. The method uses Poissonization and\nsome stochastic calculus.</p>\n", "tags": ["ARXIV"] },
{"key": "javadi2021many", "year": "2021", "title":"The Many Faces Of Anger A Multicultural Video Dataset Of Negative Emotions In The Wild (mfa-wild)", "abstract": "<p>The portrayal of negative emotions such as anger can vary widely between\ncultures and contexts, depending on the acceptability of expressing full-blown\nemotions rather than suppression to maintain harmony. The majority of emotional\ndatasets collect data under the broad label ``anger”, but social signals can\nrange from annoyed, contemptuous, angry, furious, hateful, and more. In this\nwork, we curated the first in-the-wild multicultural video dataset of emotions,\nand deeply explored anger-related emotional expressions by asking\nculture-fluent annotators to label the videos with 6 labels and 13 emojis in a\nmulti-label framework. We provide a baseline multi-label classifier on our\ndataset, and show how emojis can be effectively used as a language-agnostic\ntool for annotation.</p>\n", "tags": ["ARXIV"] },
{"key": "jayaram2019diskann", "year": "2019", "title":"Diskann Fast Accurate Billion-point Nearest Neighbor Search On A Single Node", "abstract": "<p>Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms\ngenerate indices that must be stored in main memory for fast high-recall search.\nThis makes them expensive and limits the size of the dataset. We present a\nnew graph-based indexing and search system called DiskANN that can index,\nstore, and search a billion point database on a single workstation with just 64GB\nRAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom,\nwe demonstrate that the SSD-based indices built by DiskANN can meet all three\ndesiderata for large-scale ANNS: high-recall, low query latency and high density\n(points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN\nserves &gt; 5000 queries a second with &lt; 3ms mean latency and 95%+ 1-recall@1\non a 16 core machine, where state-of-the-art billion-point ANNS algorithms with\nsimilar memory footprint like FAISS and IVFOADC+G+P plateau at\naround 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can\nindex and serve 5 − 10x more points per node compared to state-of-the-art graph-\nbased methods such as HNSW and NSG. Finally, as part of our overall\nDiskANN system, we introduce Vamana, a new graph-based ANNS index that is\nmore versatile than the graph indices even for in-memory indices.</p>\n", "tags": ["Graph","NEURIPS"] },
{"key": "jayaram2024data", "year": "2024", "title":"Data-dependent LSH For The Earth Movers Distance", "abstract": "<p>We give new data-dependent locality sensitive hashing schemes (LSH) for the\nEarth Mover’s Distance (\\(\\mathsf{EMD}\\)), and as a result, improve the best\napproximation for nearest neighbor search under \\(\\mathsf{EMD}\\) by a quadratic\nfactor. Here, the metric \\(\\mathsf{EMD}_s(\\mathbb{R}^d,\\ell_p)\\) consists of sets\nof \\(s\\) vectors in \\(\\mathbb{R}^d\\), and for any two sets \\(x,y\\) of \\(s\\) vectors the\ndistance \\(\\mathsf{EMD}(x,y)\\) is the minimum cost of a perfect matching between\n\\(x,y\\), where the cost of matching two vectors is their \\(\\ell_p\\) distance.\nPreviously, Andoni, Indyk, and Krauthgamer gave a (data-independent)\nlocality-sensitive hashing scheme for \\(\\mathsf{EMD}_s(\\mathbb{R}^d,\\ell_p)\\)\nwhen \\(p \\in [1,2]\\) with approximation \\(O(log^2 s)\\). By being data-dependent,\nwe improve the approximation to \\(\\tilde{O}(log s)\\).\n  Our main technical contribution is to show that for any distribution \\(\\mu\\)\nsupported on the metric \\(\\mathsf{EMD}_s(\\mathbb{R}^d, \\ell_p)\\), there exists a\ndata-dependent LSH for dense regions of \\(\\mu\\) which achieves approximation\n\\(\\tilde{O}(log s)\\), and that the data-independent LSH actually achieves a\n\\(\\tilde{O}(log s)\\)-approximation outside of those dense regions. Finally, we\nshow how to “glue” together these two hashing schemes without any additional\nloss in the approximation.\n  Beyond nearest neighbor search, our data-dependent LSH also gives optimal\n(distributional) sketches for the Earth Mover’s Distance. By known sketching\nlower bounds, this implies that our LSH is optimal (up to \\(\\mathrm{poly}(log\nlog s)\\) factors) among those that collide close points with constant\nprobability.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "jegou2024datasets", "year": "2024", "title":"Datasets For Approximate Nearest Neighbor Search", "abstract": "<p>BIGANN consists of SIFT descriptors applied to images from extracted from a large image dataset.</p>\n", "tags": ["ARXIV"] },
{"key": "jegou2024microsoft", "year": "2024", "title":"Microsoft Turing-anns-1b", "abstract": "<p>Microsoft Turing-ANNS-1B is a new dataset being released by the Microsoft Turing team for this competition. It consists of Bing queries encoded by Turing AGI v5 that trains Transformers to capture similarity of intent in web search queries. An early version of the RNN-based AGI Encoder is described in a SIGIR’19 paper and a blogpost.</p>\n", "tags": ["ARXIV","SIGIR"] },
{"key": "jegou2024searching", "year": "2024", "title":"Searching With Quantization Approximate Nearest Neighbor Search Using Short Codes And Distance Estimators", "abstract": "<p>We propose an approximate nearest neighbor search method based\non quantization. It uses, in particular, product quantizer to produce short codes\nand corresponding distance estimators approximating the Euclidean distance\nbetween the orginal vectors. The method is advantageously used in an asymmetric\nmanner, by computing the distance between a vector and code, unlike\ncompeting techniques such as spectral hashing that only compare codes.\nOur approach approximates the Euclidean distance based on memory efficient codes and, thus, permits efficient nearest neighbor search. Experiments\nperformed on SIFT and GIST image descriptors show excellent search accuracy.\nThe method is shown to outperform two state-of-the-art approaches of the literature.\nTimings measured when searching a vector set of 2 billion vectors are\nshown to be excellent given the high accuracy of the method.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "jeong2018efficient", "year": "2018", "title":"Efficient End-to-end Learning For Quantizable Representations", "abstract": "<p>Embedding representation learning via neural networks is at the core\nfoundation of modern similarity based search. While much effort has been put in\ndeveloping algorithms for learning binary hamming code representations for\nsearch efficiency, this still requires a linear scan of the entire dataset per\neach query and trades off the search accuracy through binarization. To this\nend, we consider the problem of directly learning a quantizable embedding\nrepresentation and the sparse binary hash code end-to-end which can be used to\nconstruct an efficient hash table not only providing significant search\nreduction in the number of data but also achieving the state of the art search\naccuracy outperforming previous state of the art deep metric learning methods.\nWe also show that finding the optimal sparse binary hash code in a mini-batch\ncan be computed exactly in polynomial time by solving a minimum cost flow\nproblem. Our results on Cifar-100 and on ImageNet datasets show the state of\nthe art search accuracy in precision@k and NMI metrics while providing up to\n98X and 478X search speedup respectively over exhaustive linear search. The\nsource code is available at\nhttps://github.com/maestrojeong/Deep-Hash-Table-ICML18</p>\n", "tags": ["ARXIV","Has Code","Supervised"] },
{"key": "jeong2019end", "year": "2019", "title":"End-to-end Efficient Representation Learning Via Cascading Combinatorial Optimization", "abstract": "<p>We develop hierarchically quantized efficient embedding representations for\nsimilarity-based search and show that this representation provides not only the\nstate of the art performance on the search accuracy but also provides several\norders of speed up during inference. The idea is to hierarchically quantize the\nrepresentation so that the quantization granularity is greatly increased while\nmaintaining the accuracy and keeping the computational complexity low. We also\nshow that the problem of finding the optimal sparse compound hash code\nrespecting the hierarchical structure can be optimized in polynomial time via\nminimum cost flow in an equivalent flow network. This allows us to train the\nmethod end-to-end in a mini-batch stochastic gradient descent setting. Our\nexperiments on Cifar100 and ImageNet datasets show the state of the art search\naccuracy while providing several orders of magnitude search speedup\nrespectively over exhaustive linear search over the dataset.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "jha2023mem", "year": "2023", "title":"Mem-rec Memory Efficient Recommendation System Using Alternative Representation", "abstract": "<p>Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI\nmodels to provide high-quality personalized recommendations. Training data used\nfor modern recommendation systems commonly includes categorical features taking\non tens-of-millions of possible distinct values. These categorical tokens are\ntypically assigned learned vector representations, that are stored in large\nembedding tables, on the order of 100s of GB. Storing and accessing these\ntables represent a substantial burden in commercial deployments. Our work\nproposes MEM-REC, a novel alternative representation approach for embedding\ntables. MEM-REC leverages bloom filters and hashing methods to encode\ncategorical features using two cache-friendly embedding tables. The first table\n(token embedding) contains raw embeddings (i.e. learned vector representation),\nand the second table (weight embedding), which is much smaller, contains\nweights to scale these raw embeddings to provide better discriminative\ncapability to each data point. We provide a detailed architecture, design and\nanalysis of MEM-REC addressing trade-offs in accuracy and computation\nrequirements, in comparison with state-of-the-art techniques. We show that\nMEM-REC can not only maintain the recommendation quality and significantly\nreduce the memory footprint for commercial scale recommendation models but can\nalso improve the embedding latency. In particular, based on our results,\nMEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by 2900x and\nperforms up to 3.4x faster embeddings while achieving the same AUC as that of\nthe full uncompressed model.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "jhuo2017set", "year": "2017", "title":"Set-to-set Hashing With Applications In Visual Recognition", "abstract": "<p>Visual data, such as an image or a sequence of video frames, is often\nnaturally represented as a point set. In this paper, we consider the\nfundamental problem of finding a nearest set from a collection of sets, to a\nquery set. This problem has obvious applications in large-scale visual\nretrieval and recognition, and also in applied fields beyond computer vision.\nOne challenge stands out in solving the problem—set representation and\nmeasure of similarity. Particularly, the query set and the sets in dataset\ncollection can have varying cardinalities. The training collection is large\nenough such that linear scan is impractical. We propose a simple representation\nscheme that encodes both statistical and structural information of the sets.\nThe derived representations are integrated in a kernel framework for flexible\nsimilarity measurement. For the query set process, we adopt a learning-to-hash\npipeline that turns the kernel representations into hash bits based on simple\nlearners, using multiple kernel learning. Experiments on two visual retrieval\ndatasets show unambiguously that our set-to-set hashing framework outperforms\nprior methods that do not take the set-to-set search setting.</p>\n", "tags": ["ARXIV"] },
{"key": "ji2012super", "year": "2012", "title":"Super-bit Locality-sensitive Hashing", "abstract": "<p>Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within \\((0,\\pi/2]\\). The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "ji2018attribute", "year": "2018", "title":"Attribute-guided Network For Cross-modal Zero-shot Hashing", "abstract": "<p>Zero-Shot Hashing aims at learning a hashing model that is trained only by\ninstances from seen categories but can generate well to those of unseen\ncategories. Typically, it is achieved by utilizing a semantic embedding space\nto transfer knowledge from seen domain to unseen domain. Existing efforts\nmainly focus on single-modal retrieval task, especially Image-Based Image\nRetrieval (IBIR). However, as a highlighted research topic in the field of\nhashing, cross-modal retrieval is more common in real world applications. To\naddress the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task, we propose a\nnovel Attribute-Guided Network (AgNet), which can perform not only IBIR, but\nalso Text-Based Image Retrieval (TBIR). In particular, AgNet aligns different\nmodal data into a semantically rich attribute space, which bridges the gap\ncaused by modality heterogeneity and zero-shot setting. We also design an\neffective strategy that exploits the attribute to guide the generation of hash\ncodes for image and text within the same network. Extensive experimental\nresults on three benchmark datasets (AwA, SUN, and ImageNet) demonstrate the\nsuperiority of AgNet on both cross-modal and single-modal zero-shot image\nretrieval tasks.</p>\n", "tags": ["ARXIV","Cross Modal","Image Retrieval"] },
{"key": "jia2021joint", "year": "2021", "title":"Joint Representation Learning And Novel Category Discovery On Single- And Multi-modal Data", "abstract": "<p>This paper studies the problem of novel category discovery on single- and\nmulti-modal data with labels from different but relevant categories. We present\na generic, end-to-end framework to jointly learn a reliable representation and\nassign clusters to unlabelled data. To avoid over-fitting the learnt embedding\nto labelled data, we take inspiration from self-supervised representation\nlearning by noise-contrastive estimation and extend it to jointly handle\nlabelled and unlabelled data. In particular, we propose using category\ndiscrimination on labelled data and cross-modal discrimination on multi-modal\ndata to augment instance discrimination used in conventional contrastive\nlearning approaches. We further employ Winner-Take-All (WTA) hashing algorithm\non the shared representation space to generate pairwise pseudo labels for\nunlabelled data to better predict cluster assignments. We thoroughly evaluate\nour framework on large-scale multi-modal video benchmarks Kinetics-400 and\nVGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining\nstate-of-the-art results.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "jia2022fast", "year": "2022", "title":"Fast Online Hashing With Multi-label Projection", "abstract": "<p>Hashing has been widely researched to solve the large-scale approximate\nnearest neighbor search problem owing to its time and storage superiority. In\nrecent years, a number of online hashing methods have emerged, which can update\nthe hash functions to adapt to the new stream data and realize dynamic\nretrieval. However, existing online hashing methods are required to update the\nwhole database with the latest hash functions when a query arrives, which leads\nto low retrieval efficiency with the continuous increase of the stream data. On\nthe other hand, these methods ignore the supervision relationship among the\nexamples, especially in the multi-label case. In this paper, we propose a novel\nFast Online Hashing (FOH) method which only updates the binary codes of a small\npart of the database. To be specific, we first build a query pool in which the\nnearest neighbors of each central point are recorded. When a new query arrives,\nonly the binary codes of the corresponding potential neighbors are updated. In\naddition, we create a similarity matrix which takes the multi-label supervision\ninformation into account and bring in the multi-label projection loss to\nfurther preserve the similarity among the multi-label data. The experimental\nresults on two common benchmarks show that the proposed FOH can achieve\ndramatic superiority on query time up to 6.28 seconds less than\nstate-of-the-art baselines with competitive retrieval accuracy.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "jia2024fast", "year": "2024", "title":"Fast Online Hashing With Multi-label Projection", "abstract": "<p>Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years, a number of online hashing methods have emerged, which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However, existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives, which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand, these methods ignore the supervision relationship among the examples, especially in the multi-label case. In this paper, we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific, we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives, only the binary codes of the corresponding potential neighbors are updated. In addition, we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "jian2020fast", "year": "2020", "title":"Fast Top-k Cosine Similarity Search Through Xor-friendly Binary Quantization On Gpus", "abstract": "<p>We explore the use of GPU for accelerating large scale nearest neighbor\nsearch and we propose a fast vector-quantization-based exhaustive nearest\nneighbor search algorithm that can achieve high accuracy without any indexing\nconstruction specifically designed for cosine similarity. This algorithm uses a\nnovel XOR-friendly binary quantization method to encode floating-point numbers\nsuch that high-complexity multiplications can be optimized as low-complexity\nbitwise operations. Experiments show that, our quantization method takes short\npreprocessing time, and helps make the search speed of our exhaustive search\nmethod much more faster than that of popular approximate nearest neighbor\nalgorithms when high accuracy is needed.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "jiang2004asymptotic", "year": "2004", "title":"Asymptotic Improvement Of The Gilbert-varshamov Bound On The Size Of Binary Codes", "abstract": "<p>Given positive integers \\(n\\) and \\(d\\), let \\(A_2(n,d)\\) denote the maximum size\nof a binary code of length \\(n\\) and minimum distance \\(d\\). The well-known\nGilbert-Varshamov bound asserts that \\(A_2(n,d) \\geq 2^n/V(n,d-1)\\), where\n\\(V(n,d) = \\sum_{i=0}^{d} {n \\choose i}\\) is the volume of a Hamming sphere of\nradius \\(d\\). We show that, in fact, there exists a positive constant \\(c\\) such\nthat $\\( A_2(n,d) \\geq c \\frac{2^n}{V(n,d-1)} log_2 V(n,d-1) \\)\\( whenever \\)d/n\n\\le 0.499$. The result follows by recasting the Gilbert- Varshamov bound into a\ngraph-theoretic framework and using the fact that the corresponding graph is\nlocally sparse. Generalizations and extensions of this result are briefly\ndiscussed.</p>\n", "tags": ["Graph","Theory"] },
{"key": "jiang2014revisiting", "year": "2014", "title":"Revisiting Kernelized Locality-sensitive Hashing For Improved Large-scale Image Retrieval", "abstract": "<p>We present a simple but powerful reinterpretation of kernelized\nlocality-sensitive hashing (KLSH), a general and popular method developed in\nthe vision community for performing approximate nearest-neighbor searches in an\narbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based\non viewing the steps of the KLSH algorithm in an appropriately projected space,\nand has several key theoretical and practical benefits. First, it eliminates\nthe problematic conceptual difficulties that are present in the existing\nmotivation of KLSH. Second, it yields the first formal retrieval performance\nbounds for KLSH. Third, our analysis reveals two techniques for boosting the\nempirical performance of KLSH. We evaluate these extensions on several\nlarge-scale benchmark image retrieval data sets, and show that our analysis\nleads to improved recall performance of at least 12%, and sometimes much\nhigher, over the standard KLSH method.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "jiang2016deep", "year": "2016", "title":"Deep Cross-modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, cross-modal hashing (CMH)\nhas been widely used for similarity search in multimedia retrieval\napplications. However, almost all existing CMH methods are based on\nhand-crafted features which might not be optimally compatible with the\nhash-code learning procedure. As a result, existing CMH methods with\nhandcrafted features may not achieve satisfactory performance. In this paper,\nwe propose a novel cross-modal hashing method, called deep crossmodal hashing\n(DCMH), by integrating feature learning and hash-code learning into the same\nframework. DCMH is an end-to-end learning framework with deep neural networks,\none for each modality, to perform feature learning from scratch. Experiments on\ntwo real datasets with text-image modalities show that DCMH can outperform\nother baselines to achieve the state-of-the-art performance in cross-modal\nretrieval applications.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "jiang2017asymmetric", "year": "2017", "title":"Asymmetric Deep Supervised Hashing", "abstract": "<p>Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "jiang2017deep", "year": "2017", "title":"Deep Discrete Supervised Hashing", "abstract": "<p>Hashing has been widely used for large-scale search due to its low storage\ncost and fast query speed. By using supervised information, supervised hashing\ncan significantly outperform unsupervised hashing. Recently, discrete\nsupervised hashing and deep hashing are two representative progresses in\nsupervised hashing. On one hand, hashing is essentially a discrete optimization\nproblem. Hence, utilizing supervised information to directly guide discrete\n(binary) coding procedure can avoid sub-optimal solution and improve the\naccuracy. On the other hand, deep hashing, which integrates deep feature\nlearning and hash-code learning into an end-to-end architecture, can enhance\nthe feedback between feature learning and hash-code learning. The key in\ndiscrete supervised hashing is to adopt supervised information to directly\nguide the discrete coding procedure in hashing. The key in deep hashing is to\nadopt the supervised information to directly guide the deep feature learning\nprocedure. However, there have not existed works which can use the supervised\ninformation to directly guide both discrete coding procedure and deep feature\nlearning procedure in the same framework. In this paper, we propose a novel\ndeep hashing method, called deep discrete supervised hashing (DDSH), to address\nthis problem. DDSH is the first deep hashing method which can utilize\nsupervised information to directly guide both discrete coding procedure and\ndeep feature learning procedure, and thus enhance the feedback between these\ntwo important procedures. Experiments on three real datasets show that DDSH can\noutperform other state-of-the-art baselines, including both discrete hashing\nand deep hashing baselines, for image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "jiang2017discrete", "year": "2017", "title":"Discrete Latent Factor Model For Cross-modal Hashing", "abstract": "<p>Due to its storage and retrieval efficiency, cross-modal hashing~(CMH) has\nbeen widely used for cross-modal similarity search in multimedia applications.\nAccording to the training strategy, existing CMH methods can be mainly divided\ninto two categories: relaxation-based continuous methods and discrete methods.\nIn general, the training of relaxation-based continuous methods is faster than\ndiscrete methods, but the accuracy of relaxation-based continuous methods is\nnot satisfactory. On the contrary, the accuracy of discrete methods is\ntypically better than relaxation-based continuous methods, but the training of\ndiscrete methods is time-consuming. In this paper, we propose a novel CMH\nmethod, called discrete latent factor model based cross-modal hashing~(DLFH),\nfor cross modal similarity search. DLFH is a discrete method which can directly\nlearn the binary hash codes for CMH. At the same time, the training of DLFH is\nefficient. Experiments on real datasets show that DLFH can achieve\nsignificantly better accuracy than existing methods, and the training time of\nDLFH is comparable to that of relaxation-based continuous methods which are\nmuch faster than existing discrete methods.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "jiang2019evaluation", "year": "2019", "title":"On The Evaluation Metric For Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, hashing has been widely\nused for large-scale approximate nearest neighbor (ANN) search. Bucket search,\nalso called hash lookup, can achieve fast query speed with a sub-linear time\ncost based on the inverted index table constructed from hash codes. Many\nmetrics have been adopted to evaluate hashing algorithms. However, all existing\nmetrics are improper to evaluate the hash codes for bucket search. On one hand,\nall existing metrics ignore the retrieval time cost which is an important\nfactor reflecting the performance of search. On the other hand, some of them,\nsuch as mean average precision (MAP), suffer from the uncertainty problem as\nthe ranked list is based on integer-valued Hamming distance, and are\ninsensitive to Hamming radius as these metrics only depend on relative Hamming\ndistance. Other metrics, such as precision at Hamming radius R, fail to\nevaluate global performance as these metrics only depend on one specific\nHamming radius. In this paper, we first point out the problems of existing\nmetrics which have been ignored by the hashing community, and then propose a\nnovel evaluation metric called radius aware mean average precision (RAMAP) to\nevaluate hash codes for bucket search. Furthermore, two coding strategies are\nalso proposed to qualitatively show the problems of existing metrics.\nExperiments demonstrate that our proposed RAMAP can provide more proper\nevaluation than existing metrics.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "jiang2019graph", "year": "2019", "title":"Graph-based Multi-view Binary Learning For Image Clustering", "abstract": "<p>Hashing techniques, also known as binary code learning, have recently gained\nincreasing attention in large-scale data analysis and storage. Generally, most\nexisting hash clustering methods are single-view ones, which lack complete\nstructure or complementary information from multiple views. For cluster tasks,\nabundant prior researches mainly focus on learning discrete hash code while few\nworks take original data structure into consideration. To address these\nproblems, we propose a novel binary code algorithm for clustering, which adopts\ngraph embedding to preserve the original data structure, called (Graph-based\nMulti-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding\nthe information of multiple views into a compact binary code, which explores\ncomplementary information from multiple views. In particular, in order to\nmaintain the graph-based structure of the original data, we adopt a Laplacian\nmatrix to preserve the local linear relationship of the data and map it to the\nHamming space. Considering different views have distinctive contributions to\nthe final clustering results, GMBL adopts a strategy of automatically assign\nweights for each view to better guide the clustering. Finally, An alternating\niterative optimization method is adopted to optimize discrete binary codes\ndirectly instead of relaxing the binary constraint in two steps. Experiments on\nfive public datasets demonstrate the superiority of our proposed method\ncompared with previous approaches in terms of clustering performance.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Unsupervised"] },
{"key": "jiang2021fast", "year": "2021", "title":"A Fast Randomized Algorithm For Massive Text Normalization", "abstract": "<p>Many popular machine learning techniques in natural language processing and\ndata mining rely heavily on high-quality text sources. However real-world text\ndatasets contain a significant amount of spelling errors and improperly\npunctuated variants where the performance of these models would quickly\ndeteriorate. Moreover, real-world, web-scale datasets contain hundreds of\nmillions or even billions of lines of text, where the existing text cleaning\ntools are prohibitively expensive to execute over and may require an overhead\nto learn the corrections. In this paper, we present FLAN, a scalable randomized\nalgorithm to clean and canonicalize massive text data. Our algorithm relies on\nthe Jaccard similarity between words to suggest correction results. We\nefficiently handle the pairwise word-to-word comparisons via Locality Sensitive\nHashing (LSH). We also propose a novel stabilization process to address the\nissue of hash collisions between dissimilar words, which is a consequence of\nthe randomized nature of LSH and is exacerbated by the massive scale of\nreal-world datasets. Compared with existing approaches, our method is more\nefficient, both asymptotically and in empirical evaluations, and does not rely\non additional features, such as lexical/phonetic similarity or word embedding\nfeatures. In addition, FLAN does not require any annotated data or supervised\nlearning. We further theoretically show the robustness of our algorithm with\nupper bounds on the false positive and false negative rates of corrections. Our\nexperimental results on real-world datasets demonstrate the efficiency and\nefficacy of FLAN.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "jiang2022givens", "year": "2022", "title":"Givens Coordinate Descent Methods For Rotation Matrix Learning In Trainable Embedding Indexes", "abstract": "<p>Product quantization (PQ) coupled with a space rotation, is widely used in\nmodern approximate nearest neighbor (ANN) search systems to significantly\ncompress the disk storage for embeddings and speed up the inner product\ncomputation. Existing rotation learning methods, however, minimize quantization\ndistortion for fixed embeddings, which are not applicable to an end-to-end\ntraining scenario where embeddings are updated constantly. In this paper, based\non geometric intuitions from Lie group theory, in particular the special\northogonal group \\(SO(n)\\), we propose a family of block Givens coordinate\ndescent algorithms to learn rotation matrix that are provably convergent on any\nconvex objectives. Compared to the state-of-the-art SVD method, the Givens\nalgorithms are much more parallelizable, reducing runtime by orders of\nmagnitude on modern GPUs, and converge more stably according to experimental\nstudies. They further improve upon vanilla product quantization significantly\nin an end-to-end training scenario.</p>\n", "tags": ["ICML","Quantisation"] },
{"key": "jiang2024deep", "year": "2024", "title":"Deep Cross-modal Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, crossmodal hashing (CMH) has been widely used for similarity\nsearch in multimedia retrieval applications. However, most\nexisting CMH methods are based on hand-crafted features\nwhich might not be optimally compatible with the hash-code\nlearning procedure. As a result, existing CMH methods\nwith hand-crafted features may not achieve satisfactory\nperformance. In this paper, we propose a novel CMH\nmethod, called deep cross-modal hashing (DCMH), by\nintegrating feature learning and hash-code learning into\nthe same framework. DCMH is an end-to-end learning\nframework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments\non three real datasets with image-text modalities show\nthat DCMH can outperform other baselines to achieve\nthe state-of-the-art performance in cross-modal retrieval\napplications.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "jiang2024scalable", "year": "2024", "title":"Scalable Graph Hashing With Feature Transformation", "abstract": "<p>Hashing has been widely used for approximate nearest\nneighbor (ANN) search in big data applications\nbecause of its low storage cost and fast retrieval\nspeed. The goal of hashing is to map the data\npoints from the original space into a binary-code\nspace where the similarity (neighborhood structure)\nin the original space is preserved. By directly\nexploiting the similarity to guide the hashing\ncode learning procedure, graph hashing has attracted\nmuch attention. However, most existing graph\nhashing methods cannot achieve satisfactory performance\nin real applications due to the high complexity\nfor graph modeling. In this paper, we propose\na novel method, called scalable graph hashing\nwith feature transformation (SGH), for large-scale\ngraph hashing. Through feature transformation, we\ncan effectively approximate the whole graph without\nexplicitly computing the similarity graph matrix,\nbased on which a sequential learning method\nis proposed to learn the hash functions in a bit-wise\nmanner. Experiments on two datasets with one million\ndata points show that our SGH method can\noutperform the state-of-the-art methods in terms of\nboth accuracy and scalability.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "jie2013novel", "year": "2013", "title":"A Novel Block-dct And PCA Based Image Perceptual Hashing Algorithm", "abstract": "<p>Image perceptual hashing finds applications in content indexing, large-scale\nimage database management, certification and authentication and digital\nwatermarking. We propose a Block-DCT and PCA based image perceptual hash in\nthis article and explore the algorithm in the application of tamper detection.\nThe main idea of the algorithm is to integrate color histogram and DCT\ncoefficients of image blocks as perceptual feature, then to compress perceptual\nfeatures as inter-feature with PCA, and to threshold to create a robust hash.\nThe robustness and discrimination properties of the proposed algorithm are\nevaluated in detail. Our algorithms first construct a secondary image, derived\nfrom input image by pseudo-randomly extracting features that approximately\ncapture semi-global geometric characteristics. From the secondary image (which\ndoes not perceptually resemble the input), we further extract the final\nfeatures which can be used as a hash value (and can be further suitably\nquantized). In this paper, we use spectral matrix invariants as embodied by\nSingular Value Decomposition. Surprisingly, formation of the secondary image\nturns out be quite important since it not only introduces further robustness,\nbut also enhances the security properties. Indeed, our experiments reveal that\nour hashing algorithms extract most of the geometric information from the\nimages and hence are robust to severe perturbations (e.g. up to %50 cropping by\narea with 20 degree rotations) on images while avoiding misclassification.\nExperimental results show that the proposed image perceptual hash algorithm can\neffectively address the tamper detection problem with advantageous robustness\nand discrimination.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "jin2017ranking", "year": "2017", "title":"Ranking Based Locality Sensitive Hashing Enabled Cancelable Biometrics Index-of-max Hashing", "abstract": "<p>In this paper, we propose a ranking based locality sensitive hashing inspired\ntwo-factor cancelable biometrics, dubbed “Index-of-Max” (IoM) hashing for\nbiometric template protection. With externally generated random parameters, IoM\nhashing transforms a real-valued biometric feature vector into discrete index\n(max ranked) hashed code. We demonstrate two realizations from IoM hashing\nnotion, namely Gaussian Random Projection based and Uniformly Random\nPermutation based hashing schemes. The discrete indices representation nature\nof IoM hashed codes enjoy serveral merits. Firstly, IoM hashing empowers strong\nconcealment to the biometric information. This contributes to the solid ground\nof non-invertibility guarantee. Secondly, IoM hashing is insensitive to the\nfeatures magnitude, hence is more robust against biometric features variation.\nThirdly, the magnitude-independence trait of IoM hashing makes the hash codes\nbeing scale-invariant, which is critical for matching and feature alignment.\nThe experimental results demonstrate favorable accuracy performance on\nbenchmark FVC2002 and FVC2004 fingerprint databases. The analyses justify its\nresilience to the existing and newly introduced security and privacy attacks as\nwell as satisfy the revocability and unlinkability criteria of cancelable\nbiometrics.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "jin2018deep", "year": "2018", "title":"Deep Saliency Hashing", "abstract": "<p>In recent years, hashing methods have been proved to be effective and\nefficient for the large-scale Web media search. However, the existing general\nhashing methods have limited discriminative power for describing fine-grained\nobjects that share similar overall appearance but have subtle difference. To\nsolve this problem, we for the first time introduce the attention mechanism to\nthe learning of fine-grained hashing codes. Specifically, we propose a novel\ndeep hashing model, named deep saliency hashing (DSaH), which automatically\nmines salient regions and learns semantic-preserving hashing codes\nsimultaneously. DSaH is a two-step end-to-end model consisting of an attention\nnetwork and a hashing network. Our loss function contains three basic\ncomponents, including the semantic loss, the saliency loss, and the\nquantization loss. As the core of DSaH, the saliency loss guides the attention\nnetwork to mine discriminative regions from pairs of images. We conduct\nextensive experiments on both fine-grained and general retrieval datasets for\nperformance evaluation. Experimental results on fine-grained datasets,\nincluding Oxford Flowers-17, Stanford Dogs-120, and CUB Bird demonstrate that\nour DSaH performs the best for fine-grained retrieval task and beats the\nstrongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and\nCUB Bird. DSaH is also comparable to several state-of-the-art hashing methods\non general datasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "jin2018unsupervised", "year": "2018", "title":"Unsupervised Semantic Deep Hashing", "abstract": "<p>In recent years, deep hashing methods have been proved to be efficient since\nit employs convolutional neural network to learn features and hashing codes\nsimultaneously. However, these methods are mostly supervised. In real-world\napplication, it is a time-consuming and overloaded task for annotating a large\nnumber of images. In this paper, we propose a novel unsupervised deep hashing\nmethod for large-scale image retrieval. Our method, namely unsupervised\nsemantic deep hashing (\\textbf{USDH}), uses semantic information preserved in\nthe CNN feature layer to guide the training of network. We enforce four\ncriteria on hashing codes learning based on VGG-19 model: 1) preserving\nrelevant information of feature space in hashing space; 2) minimizing\nquantization loss between binary-like codes and hashing codes; 3) improving the\nusage of each bit in hashing codes by using maximum information entropy, and 4)\ninvariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have\ndemonstrated that \\textbf{USDH} outperforms several state-of-the-art\nunsupervised hashing methods for image retrieval. We also conduct experiments\non Oxford 17 datasets for fine-grained classification to verify its efficiency\nfor other computer vision tasks.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "jin2019deep", "year": "2019", "title":"Deep Semantic Multimodal Hashing Network For Scalable Image-text And Video-text Retrievals", "abstract": "<p>Hashing has been widely applied to multimodal retrieval on large-scale\nmultimedia data due to its efficiency in computation and storage. In this\narticle, we propose a novel deep semantic multimodal hashing network (DSMHN)\nfor scalable image-text and video-text retrieval. The proposed deep hashing\nframework leverages 2-D convolutional neural networks (CNN) as the backbone\nnetwork to capture the spatial information for image-text retrieval, while the\n3-D CNN as the backbone network to capture the spatial and temporal information\nfor video-text retrieval. In the DSMHN, two sets of modality-specific hash\nfunctions are jointly learned by explicitly preserving both intermodality\nsimilarities and intramodality semantic labels. Specifically, with the\nassumption that the learned hash codes should be optimal for the classification\ntask, two stream networks are jointly trained to learn the hash functions by\nembedding the semantic labels on the resultant hash codes. Moreover, a unified\ndeep multimodal hashing framework is proposed to learn compact and high-quality\nhash codes by exploiting the feature representation learning, intermodality\nsimilarity-preserving learning, semantic label-preserving learning, and hash\nfunction learning with different types of loss functions simultaneously. The\nproposed DSMHN method is a generic and scalable deep hashing framework for both\nimage-text and video-text retrievals, which can be flexibly integrated with\ndifferent types of loss functions. We conduct extensive experiments for both\nsingle modal- and cross-modal-retrieval tasks on four widely used\nmultimodal-retrieval data sets. Experimental results on both image-text- and\nvideo-text-retrieval tasks demonstrate that the DSMHN significantly outperforms\nthe state-of-the-art methods.</p>\n", "tags": ["ARXIV","CNN","Cross Modal","Supervised","Text Retrieval"] },
{"key": "jin2019ssah", "year": "2019", "title":"SSAH Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation", "abstract": "<p>Deep hashing methods have been proved to be effective and efficient for\nlarge-scale Web media search. The success of these data-driven methods largely\ndepends on collecting sufficient labeled data, which is usually a crucial\nlimitation in practical cases. The current solutions to this issue utilize\nGenerative Adversarial Network (GAN) to augment data in semi-supervised\nlearning. However, existing GAN-based methods treat image generations and\nhashing learning as two isolated processes, leading to generation\nineffectiveness. Besides, most works fail to exploit the semantic information\nin unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace\nAdversarial Hashing method, named SSAH to solve the above problems in a unified\nframework. The SSAH method consists of an adversarial network (A-Net) and a\nhashing network (H-Net). To improve the quality of generative images, first,\nthe A-Net learns hard samples with multi-scale occlusions and multi-angle\nrotated deformations which compete against the learning of accurate hashing\ncodes. Second, we design a novel self-paced hard generation policy to gradually\nincrease the hashing difficulty of generated samples. To make use of the\nsemantic information in unlabeled ones, we propose a semi-supervised consistent\nloss. The experimental results show that our method can significantly improve\nstate-of-the-art models on both the widely-used hashing datasets and\nfine-grained datasets.</p>\n", "tags": ["ARXIV","GAN","Supervised"] },
{"key": "jin2024complementary", "year": "2024", "title":"Complementary Projection Hashing", "abstract": "<p>Recently, hashing techniques have been widely applied\nto solve the approximate nearest neighbors search problem\nin many vision applications. Generally, these hashing\napproaches generate 2^c buckets, where c is the length\nof the hash code. A good hashing method should satisfy\nthe following two requirements: 1) mapping the nearby\ndata points into the same bucket or nearby (measured by\nthe Hamming distance) buckets. 2) all the data points are\nevenly distributed among all the buckets. In this paper,\nwe propose a novel algorithm named Complementary Projection\nHashing (CPH) to find the optimal hashing functions\nwhich explicitly considers the above two requirements.\nSpecifically, CPH aims at sequentially finding a series of hyperplanes\n(hashing functions) which cross the sparse region\nof the data. At the same time, the data points are evenly distributed\nin the hypercubes generated by these hyperplanes.\nThe experiments comparing with the state-of-the-art hashing\nmethods demonstrate the effectiveness of the proposed\nmethod.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "jin2024deep", "year": "2024", "title":"Deep Saliency Hashing For Fine-grained Retrieval", "abstract": "<p>In recent years, hashing methods have been proved to be\neffective and efficient for the large-scale Web media search.\nHowever, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle\ndifference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained\nhashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which\nautomatically mines salient regions and learns semanticpreserving hashing codes simultaneously. DSaH is a twostep end-to-end model consisting of an attention network\nand a hashing network. Our loss function contains three\nbasic components, including the semantic loss, the saliency\nloss, and the quantization loss. As the core of DSaH, the\nsaliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval\ndatasets for performance evaluation. Experimental results\non fine grained dataset, including Oxford Flowers-17, Stanford Dogs-120 and CUB Bird demonstrate that our DSaH\nperforms the best for fine-grained retrieval task and beats\nstrongest competitor (DTQ) by approximately 10% on both\nStanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general\ndatasets, including CIFAR-10 and NUS-WIDE.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "jin2024ssah", "year": "2024", "title":"SSAH Semi-supervised Adversarial Deep Hashing With Self-paced Hard Sample Generation", "abstract": "<p>Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.</p>\n", "tags": ["ARXIV","GAN","Supervised"] },
{"key": "jin2024unsupervised", "year": "2024", "title":"Unsupervised Discrete Hashing With Affinity Similarity", "abstract": "<p>In recent years, supervised hashing has been validated to greatly boost the performance of image retrieval. However, the label-hungry property requires massive label collection, making it intractable in practical scenarios. To liberate the model training procedure from laborious manual annotations, some unsupervised methods are proposed. However, the following two factors make unsupervised algorithms inferior to their supervised counterparts: (1) Without manually-defined labels, it is difficult to capture the semantic information across data, which is of crucial importance to guide robust binary code learning. (2) The widely adopted relaxation on binary constraints results in quantization error accumulation in the optimization procedure. To address the above-mentioned problems, in this paper, we propose a novel Unsupervised Discrete Hashing method (UDH). Specifically, to capture the semantic information, we propose a balanced graph-based semantic loss which explores the affinity priors in the original feature space. Then, we propose a novel self-supervised loss, termed orthogonal consistent loss, which can leverage semantic loss of instance and impose independence of codes. Moreover, by integrating the discrete optimization into the proposed unsupervised framework, the binary constraints are consistently preserved, alleviating the influence of quantization errors. Extensive experiments demonstrate that UDH outperforms state-of-the-art unsupervised methods for image retrieval.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "jinnai2017hash", "year": "2017", "title":"On Hash-based Work Distribution Methods For Parallel Best-first Search", "abstract": "<p>Parallel best-first search algorithms such as Hash Distributed A* (HDA<em>)\ndistribute work among the processes using a global hash function. We analyze\nthe search and communication overheads of state-of-the-art hash-based parallel\nbest-first search algorithms, and show that although Zobrist hashing, the\nstandard hash function used by HDA</em>, achieves good load balance for many\ndomains, it incurs significant communication overhead since almost all\ngenerated nodes are transferred to a different processor than their parents. We\npropose Abstract Zobrist hashing, a new work distribution method for parallel\nsearch which, instead of computing a hash value based on the raw features of a\nstate, uses a feature projection function to generate a set of abstract\nfeatures which results in a higher locality, resulting in reduced\ncommunications overhead. We show that Abstract Zobrist hashing outperforms\nprevious methods on search domains using hand-coded, domain specific feature\nprojection functions. We then propose GRAZHDA<em>, a graph-partitioning based\napproach to automatically generating feature projection functions. GRAZHDA</em>\nseeks to approximate the partitioning of the actual search space graph by\npartitioning the domain transition graph, an abstraction of the state space\ngraph. We show that GRAZHDA* outperforms previous methods on domain-independent\nplanning.</p>\n", "tags": ["Graph","Independent"] },
{"key": "johnson2017billion", "year": "2017", "title":"Billion-scale Similarity Search With Gpus", "abstract": "<p>Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.</p>\n", "tags": ["ARXIV","Graph","Quantisation"] },
{"key": "joly2024random", "year": "2024", "title":"Random Maximum Margin Hashing", "abstract": "<p>Following the success of hashing methods for multidimensional\nindexing, more and more works are interested\nin embedding visual feature space in compact hash codes.\nSuch approaches are not an alternative to using index structures\nbut a complementary way to reduce both the memory\nusage and the distance computation cost. Several data\ndependent hash functions have notably been proposed to\nclosely fit data distribution and provide better selectivity\nthan usual random projections such as LSH. However, improvements\noccur only for relatively small hash code sizes\nup to 64 or 128 bits. As discussed in the paper, this is mainly\ndue to the lack of independence between the produced hash\nfunctions. We introduce a new hash function family that\nattempts to solve this issue in any kernel space. Rather\nthan boosting the collision probability of close points, our\nmethod focus on data scattering. By training purely random\nsplits of the data, regardless the closeness of the training\nsamples, it is indeed possible to generate consistently\nmore independent hash functions. On the other side, the\nuse of large margin classifiers allows to maintain good generalization\nperformances. Experiments show that our new\nRandom Maximum Margin Hashing scheme (RMMH) outperforms\nfour state-of-the-art hashing methods, notably in\nkernel spaces.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "jose2020optimized", "year": "2020", "title":"Optimized Feature Space Learning For Generating Efficient Binary Codes For Image Retrieval", "abstract": "<p>In this paper we propose an approach for learning low dimensional optimized\nfeature space with minimum intra-class variance and maximum inter-class\nvariance. We address the problem of high-dimensionality of feature vectors\nextracted from neural networks by taking care of the global statistics of\nfeature space. Classical approach of Linear Discriminant Analysis (LDA) is\ngenerally used for generating an optimized low dimensional feature space for\nsingle-labeled images. Since, image retrieval involves both multi-labeled and\nsingle-labeled images, we utilize the equivalence between LDA and Canonical\nCorrelation Analysis (CCA) to generate an optimized feature space for\nsingle-labeled images and use CCA to generate an optimized feature space for\nmulti-labeled images. Our approach correlates the projections of feature\nvectors with label vectors in our CCA based network architecture. The neural\nnetwork minimize a loss function which maximizes the correlation coefficients.\nWe binarize our generated feature vectors with the popular Iterative\nQuantization (ITQ) approach and also propose an ensemble network to generate\nbinary codes of desired bit length for image retrieval. Our measurement of mean\naverage precision shows competitive results on other state-of-the-art\nsingle-labeled and multi-labeled image retrieval datasets.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "joslyn2018deep", "year": "2018", "title":"Deep Segment Hash Learning For Music Generation", "abstract": "<p>Music generation research has grown in popularity over the past decade,\nthanks to the deep learning revolution that has redefined the landscape of\nartificial intelligence. In this paper, we propose a novel approach to music\ngeneration inspired by musical segment concatenation methods and hash learning\nalgorithms. Given a segment of music, we use a deep recurrent neural network\nand ranking-based hash learning to assign a forward hash code to the segment to\nretrieve candidate segments for continuation with matching backward hash codes.\nThe proposed method is thus called Deep Segment Hash Learning (DSHL). To the\nbest of our knowledge, DSHL is the first end-to-end segment hash learning\nmethod for music generation, and the first to use pair-wise training with\nsegments of music. We demonstrate that this method is capable of generating\nmusic which is both original and enjoyable, and that DSHL offers a promising\nnew direction for music generation research.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "joulin2016compressing", "year": "2016", "title":"Fasttext.zip Compressing Text Classification Models", "abstract": "<p>We consider the problem of producing compact architectures for text\nclassification, such that the full model fits in a limited amount of memory.\nAfter considering different solutions inspired by the hashing literature, we\npropose a method built upon product quantization to store word embeddings.\nWhile the original technique leads to a loss in accuracy, we adapt this method\nto circumvent quantization artefacts. Our experiments carried out on several\nbenchmarks show that our approach typically requires two orders of magnitude\nless memory than fastText while being only slightly inferior with respect to\naccuracy. As a result, it outperforms the state of the art by a good margin in\nterms of the compromise between memory usage and accuracy.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "joyce2023feature", "year": "2023", "title":"Avscan2vec Feature Learning On Antivirus Scan Data For Production-scale Malware Corpora", "abstract": "<p>When investigating a malicious file, searching for related files is a common\ntask that malware analysts must perform. Given that production malware corpora\nmay contain over a billion files and consume petabytes of storage, many feature\nextraction and similarity search approaches are computationally infeasible. Our\nwork explores the potential of antivirus (AV) scan data as a scalable source of\nfeatures for malware. This is possible because AV scan reports are widely\navailable through services such as VirusTotal and are ~100x smaller than the\naverage malware sample. The information within an AV scan report is abundant\nwith information and can indicate a malicious file’s family, behavior, target\noperating system, and many other characteristics. We introduce AVScan2Vec, a\nlanguage model trained to comprehend the semantics of AV scan data. AVScan2Vec\ningests AV scan data for a malicious file and outputs a meaningful vector\nrepresentation. AVScan2Vec vectors are ~3 to 85x smaller than popular\nalternatives in use today, enabling faster vector comparisons and lower memory\nusage. By incorporating Dynamic Continuous Indexing, we show that\nnearest-neighbor queries on AVScan2Vec vectors can scale to even the largest\nmalware production datasets. We also demonstrate that AVScan2Vec vectors are\nsuperior to other leading malware feature vector representations across nearly\nall classification, clustering, and nearest-neighbor lookup algorithms that we\nevaluated.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "jung2021picarrange", "year": "2021", "title":"Picarrange -- Visually Sort Search And Explore Private Images On A Mac Computer", "abstract": "<p>The native macOS application PicArrange integrates state-of-the-art image\nsorting and similarity search to enable users to get a better overview of their\nimages. Many file and image management features have been added to make it a\ntool that addresses a full image management workflow. A modification of the\nSelf Sorting Map algorithm enables a list-like image arrangement without\nloosing the visual sorting. Efficient calculation and storage of visual\nfeatures as well as the use of many macOS APIs result in an application that is\nfluid to use.</p>\n", "tags": ["ARXIV"] },
{"key": "junussov2019note", "year": "2019", "title":"Note On Distance Matrix Hashing", "abstract": "<p>Hashing algorithm of dynamical set of distances is described. Proposed\nhashing function is residual. Data structure which implementation accelerates\ncomputations is presented</p>\n", "tags": ["ARXIV"] },
{"key": "jégou2011anti", "year": "2011", "title":"Anti-sparse Coding For Approximate Nearest Neighbor Search", "abstract": "<p>This paper proposes a binarization scheme for vectors of high dimension based\non the recent concept of anti-sparse coding, and shows its excellent\nperformance for approximate nearest neighbor search. Unlike other binarization\nschemes, this framework allows, up to a scaling factor, the explicit\nreconstruction from the binary representation of the original vector. The paper\nalso shows that random projections which are used in Locality Sensitive Hashing\nalgorithms, are significantly outperformed by regular frames for both synthetic\nand real data if the number of bits exceeds the vector dimensionality, i.e.,\nwhen high precision is required.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "jégou2011searching", "year": "2011", "title":"Searching In One Billion Vectors Re-rank With Source Coding", "abstract": "<p>Recent indexing techniques inspired by source coding have been shown\nsuccessful to index billions of high-dimensional vectors in memory. In this\npaper, we propose an approach that re-ranks the neighbor hypotheses obtained by\nthese compressed-domain indexing methods. In contrast to the usual\npost-verification scheme, which performs exact distance calculation on the\nshort-list of hypotheses, the estimated distances are refined based on short\nquantization codes, to avoid reading the full vectors from disk. We have\nreleased a new public dataset of one billion 128-dimensional vectors and\nproposed an experimental setup to evaluate high dimensional indexing algorithms\non a realistic scale. Experiments show that our method accurately and\nefficiently re-ranks the neighbor hypotheses using little memory compared to\nthe full vectors representation.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "kaga2019pdh", "year": "2019", "title":"PDH Probabilistic Deep Hashing Based On MAP Estimation Of Hamming Distance", "abstract": "<p>With the growth of image on the web, research on hashing which enables\nhigh-speed image retrieval has been actively studied. In recent years, various\nhashing methods based on deep neural networks have been proposed and achieved\nhigher precision than the other hashing methods. In these methods, multiple\nlosses for hash codes and the parameters of neural networks are defined. They\ngenerate hash codes that minimize the weighted sum of the losses. Therefore, an\nexpert has to tune the weights for the losses heuristically, and the\nprobabilistic optimality of the loss function cannot be explained. In order to\ngenerate explainable hash codes without weight tuning, we theoretically derive\na single loss function with no hyperparameters for the hash code from the\nprobability distribution of the images. By generating hash codes that minimize\nthis loss function, highly accurate image retrieval with probabilistic\noptimality is performed. We evaluate the performance of hashing using MNIST,\nCIFAR-10, SVHN and show that the proposed method outperforms the\nstate-of-the-art hashing methods.</p>\n", "tags": ["Image Retrieval","Supervised"] },
{"key": "kalantidis2016loh", "year": "2016", "title":"LOH And Behold Web-scale Visual Search Recommendation And Clustering Using Locally Optimized Hashing", "abstract": "<p>We propose a novel hashing-based matching scheme, called Locally Optimized\nHashing (LOH), based on a state-of-the-art quantization algorithm that can be\nused for efficient, large-scale search, recommendation, clustering, and\ndeduplication. We show that matching with LOH only requires set intersections\nand summations to compute and so is easily implemented in generic distributed\ncomputing systems. We further show application of LOH to: a) large-scale search\ntasks where performance is on par with other state-of-the-art hashing\napproaches; b) large-scale recommendation where queries consisting of thousands\nof images can be used to generate accurate recommendations from collections of\nhundreds of millions of images; and c) efficient clustering with a graph-based\nalgorithm that can be scaled to massive collections in a distributed\nenvironment or can be used for deduplication for small collections, like search\nresults, performing better than traditional hashing approaches while only\nrequiring a few milliseconds to run. In this paper we experiment on datasets of\nup to 100 million images, but in practice our system can scale to larger\ncollections and can be used for other types of data that have a vector\nrepresentation in a Euclidean space.</p>\n", "tags": ["ARXIV","Graph","Quantisation","Unsupervised"] },
{"key": "kanda2019b", "year": "2019", "title":"b-bit Sketch Trie Scalable Similarity Search On Integer Sketches", "abstract": "<p>Recently, randomly mapping vectorial data to strings of discrete symbols\n(i.e., sketches) for fast and space-efficient similarity searches has become\npopular. Such random mapping is called similarity-preserving hashing and\napproximates a similarity metric by using the Hamming distance. Although many\nefficient similarity searches have been proposed, most of them are designed for\nbinary sketches. Similarity searches on integer sketches are in their infancy.\nIn this paper, we present a novel space-efficient trie named \\(b\\)-bit sketch\ntrie on integer sketches for scalable similarity searches by leveraging the\nidea behind succinct data structures (i.e., space-efficient data structures\nwhile supporting various data operations in the compressed format) and a\nfavorable property of integer sketches as fixed-length strings. Our\nexperimental results obtained using real-world datasets show that a trie-based\nindex is built from integer sketches and efficiently performs similarity\nsearches on the index by pruning useless portions of the search space, which\ngreatly improves the search time and space-efficiency of the similarity search.\nThe experimental results show that our similarity search is at most one order\nof magnitude faster than state-of-the-art similarity searches. Besides, our\nmethod needs only 10 GiB of memory on a billion-scale database, while\nstate-of-the-art similarity searches need 29 GiB of memory.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "kanda2019sketch", "year": "2019", "title":"(b)-bit Sketch Trie Scalable Similarity Search On Integer Sketches", "abstract": "<p>Recently, randomly mapping vectorial data to strings of discrete symbols (i.e., sketches) for fast and space-efficient similarity searches has become popular. Such random mapping is called similarity-preserving hashing and approximates a similarity metric by using the Hamming distance. Although many efficient similarity searches have been proposed, most of them are designed for binary sketches. Similarity searches on integer sketches are in their infancy. In this paper, we present a novel space-efficient trie named \\(b\\)-bit sketch trie on integer sketches for scalable similarity searches by leveraging the idea behind succinct data structures (i.e., space-efficient data structures while supporting various data operations in the compressed format) and a favorable property of integer sketches as fixed-length strings. Our experimental results obtained using real-world datasets show that a trie-based index is built from integer sketches and efficiently performs similarity searches on the index by pruning useless portions of the search space, which greatly improves the search time and space-efficiency of the similarity search. The experimental results show that our similarity search is at most one order of magnitude faster than state-of-the-art similarity searches. Besides, our method needs only 10 GiB of memory on a billion-scale database, while state-of-the-art similarity searches need 29 GiB of memory.</p>\n", "tags": ["ARXIV"] },
{"key": "kanda2020dynamic", "year": "2020", "title":"Dynamic Similarity Search On Integer Sketches", "abstract": "<p>Similarity-preserving hashing is a core technique for fast similarity\nsearches, and it randomly maps data points in a metric space to strings of\ndiscrete symbols (i.e., sketches) in the Hamming space. While traditional\nhashing techniques produce binary sketches, recent ones produce integer\nsketches for preserving various similarity measures. However, most similarity\nsearch methods are designed for binary sketches and inefficient for integer\nsketches. Moreover, most methods are either inapplicable or inefficient for\ndynamic datasets, although modern real-world datasets are updated over time. We\npropose dynamic filter trie (DyFT), a dynamic similarity search method for both\nbinary and integer sketches. An extensive experimental analysis using large\nreal-world datasets shows that DyFT performs superiorly with respect to\nscalability, time performance, and memory efficiency. For example, on a huge\ndataset of 216 million data points, DyFT performs a similarity search 6,000\ntimes faster than a state-of-the-art method while reducing to one-thirteenth in\nmemory.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "kanda2020succinct", "year": "2020", "title":"Succinct Trit-array Trie For Scalable Trajectory Similarity Search", "abstract": "<p>Massive datasets of spatial trajectories representing the mobility of a\ndiversity of moving objects are ubiquitous in research and industry. Similarity\nsearch of a large collection of trajectories is indispensable for turning these\ndatasets into knowledge. Locality sensitive hashing (LSH) is a powerful\ntechnique for fast similarity searches. Recent methods employ LSH and attempt\nto realize an efficient similarity search of trajectories; however, those\nmethods are inefficient in terms of search time and memory when applied to\nmassive datasets. To address this problem, we present the trajectory-indexing\nsuccinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for\ntrajectory similarity searches. tSTAT quickly performs the search on a tree\ndata structure called trie. We also present two novel techniques that enable to\ndramatically enhance the memory efficiency of tSTAT. One is a node reduction\ntechnique that substantially omits redundant trie nodes while maintaining the\ntime performance. The other is a space-efficient representation that leverages\nthe idea behind succinct data structures (i.e., a compressed data structure\nsupporting fast data operations). We experimentally test tSTAT on its ability\nto retrieve similar trajectories for a query from large collections of\ntrajectories and show that tSTAT performs superiorly in comparison to\nstate-of-the-art similarity search methods.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "kane2010derandomized", "year": "2010", "title":"A Derandomized Sparse Johnson-lindenstrauss Transform", "abstract": "<p>Recent work of [Dasgupta-Kumar-Sarlos, STOC 2010] gave a sparse\nJohnson-Lindenstrauss transform and left as a main open question whether their\nconstruction could be efficiently derandomized. We answer their question\naffirmatively by giving an alternative proof of their result requiring only\nbounded independence hash functions. Furthermore, the sparsity bound obtained\nin our proof is improved. The main ingredient in our proof is a spectral moment\nbound for quadratic forms that was recently used in [Diakonikolas-Kane-Nelson,\nFOCS 2010].</p>\n", "tags": ["ARXIV","FOCS","Independent"] },
{"key": "kang2019candidate", "year": "2019", "title":"Candidate Generation With Binary Codes For Large-scale Top-n Recommendation", "abstract": "<p>Generating the Top-N recommendations from a large corpus is computationally\nexpensive to perform at scale. Candidate generation and re-ranking based\napproaches are often adopted in industrial settings to alleviate efficiency\nproblems. However it remains to be fully studied how well such schemes\napproximate complete rankings (or how many candidates are required to achieve a\ngood approximation), or to develop systematic approaches to generate\nhigh-quality candidates efficiently. In this paper, we seek to investigate\nthese questions via proposing a candidate generation and re-ranking based\nframework (CIGAR), which first learns a preference-preserving binary embedding\nfor building a hash table to retrieve candidates, and then learns to re-rank\nthe candidates using real-valued ranking models with a candidate-oriented\nobjective. We perform a comprehensive study on several large-scale real-world\ndatasets consisting of millions of users/items and hundreds of millions of\ninteractions. Our results show that CIGAR significantly boosts the Top-N\naccuracy against state-of-the-art recommendation models, while reducing the\nquery time by orders of magnitude. We hope that this work could draw more\nattention to the candidate generation problem in recommender systems.</p>\n", "tags": ["ARXIV","Survey Paper"] },
{"key": "kang2020learning", "year": "2020", "title":"Learning To Embed Categorical Features Without Embedding Tables For Recommendation", "abstract": "<p>Embedding learning of categorical features (e.g. user/item IDs) is at the\ncore of various recommendation models including matrix factorization and neural\ncollaborative filtering. The standard approach creates an embedding table where\neach row represents a dedicated embedding vector for every unique feature\nvalue. However, this method fails to efficiently handle high-cardinality\nfeatures and unseen feature values (e.g. new video ID) that are prevalent in\nreal-world recommendation systems. In this paper, we propose an alternative\nembedding framework Deep Hash Embedding (DHE), replacing embedding tables by a\ndeep embedding network to compute embeddings on the fly. DHE first encodes the\nfeature value to a unique identifier vector with multiple hashing functions and\ntransformations, and then applies a DNN to convert the identifier vector to an\nembedding. The encoding module is deterministic, non-learnable, and free of\nstorage, while the embedding network is updated during the training time to\nlearn embedding generation. Empirical results show that DHE achieves comparable\nAUC against the standard one-hot full embedding, with smaller model sizes. Our\nwork sheds light on the design of DNN-based alternative embedding schemes for\ncategorical features without using embedding table lookup.</p>\n", "tags": ["ARXIV"] },
{"key": "kang2024column", "year": "2024", "title":"Column Sampling Based Discrete Supervised Hashing", "abstract": "<p>By leveraging semantic (label) information, supervised hashing has demonstrated better accuracy than unsupervised hashing in many real applications. Because the hashing-code learning problem is essentially a discrete optimization problem which is hard to solve, most existing supervised hashing methods try to solve a relaxed continuous optimization problem by dropping the discrete constraints.\nHowever, these methods typically suffer from poor performance due to the errors caused by the relaxation. Some other methods try to directly solve the discrete optimization problem. However, they are typically time-consuming and unscalable. In this paper, we propose a novel method, called column sampling based discrete supervised hashing (COSDISH), to directly learn the discrete hashing code from semantic information.\nCOSDISH is an iterative method, in each iteration of which several columns are sampled from the semantic similarity matrix and then the hashing code is decomposed into two parts which can be alternately optimized in a discrete way. Theoretical analysis shows that the learning (optimization) algorithm of COSDISH has a constant-approximation bound in each step of the alternating optimization procedure. Empirical results on datasets with semantic labels illustrate that COSDISH can outperform the state-of-the-art methods in real applications like image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "kang2024maximum", "year": "2024", "title":"Maximum-margin Hamming Hashing", "abstract": "<p>Deep hashing enables computation and memory efficient\nimage search through end-to-end learning of feature representations and binary codes. While linear scan over binary\nhash codes is more efficient than over the high-dimensional\nrepresentations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for\neach query, there is a Hamming ball centered at the query\nand the data points within the ball are returned as relevant.\nSince inside the Hamming ball implies retrievable while\noutside irretrievable, it is crucial to explicitly characterize\nthe Hamming ball. The main idea of this work is to directly\nembody the Hamming radius into the loss functions, leading\nto Maximum-Margin Hamming Hashing (MMHH), a new\nmodel specifically optimized for Hamming space retrieval.\nWe introduce a max-margin t-distribution loss, where the\nt-distribution concentrates more similar data points to be\nwithin the Hamming ball, and the margin characterizes the\nHamming radius such that less penalization is applied to\nsimilar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems.\nThe model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data.\nOur method yields state-of-the-art results on four datasets\nand shows superior performance on noisy data.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "kanizo2010maximum", "year": "2010", "title":"Maximum Bipartite Matching Size And Application To Cuckoo Hashing", "abstract": "<p>Cuckoo hashing with a stash is a robust multiple choice hashing scheme with\nhigh memory utilization that can be used in many network device applications.\nUnfortunately, for memory loads beyond 0.5, little is known on its performance.\n  In this paper, we analyze its average performance over such loads. We tackle\nthis problem by recasting the problem as an analysis of the expected maximum\nmatching size of a given random bipartite graph. We provide exact results for\nany finite system, and also deduce asymptotic results as the memory size\nincreases. We further consider other variants of this problem, and finally\nevaluate the performance of our models on Internet backbone traces. More\ngenerally, our results give a tight lower bound on the size of the stash needed\nfor any multiple-choice hashing scheme.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "kaplan2020locality", "year": "2020", "title":"Locality Sensitive Hashing For Set-queries Motivated By Group Recommendations", "abstract": "<p>Locality Sensitive Hashing (LSH) is an effective method to index a set of\npoints such that we can efficiently find the nearest neighbors of a query\npoint. We extend this method to our novel Set-query LSH (SLSH), such that it\ncan find the nearest neighbors of a set of points, given as a query.\n  Let \\( s(x,y) \\) be the similarity between two points \\( x \\) and \\( y \\). We\ndefine a similarity between a set \\( Q\\) and a point \\( x \\) by aggregating the\nsimilarities \\( s(p,x) \\) for all \\( p\\in Q \\). For example, we can take \\( s(p,x) \\)\nto be the angular similarity between \\( p \\) and \\( x \\) (i.e., \\(1-{\\angle\n(x,p)}/{\\pi}\\)), and aggregate by arithmetic or geometric averaging, or taking\nthe lowest similarity.\n  We develop locality sensitive hash families and data structures for a large\nset of such arithmetic and geometric averaging similarities, and analyze their\ncollision probabilities. We also establish an analogous framework and hash\nfamilies for distance functions. Specifically, we give a structure for the\neuclidean distance aggregated by either averaging or taking the maximum.\n  We leverage SLSH to solve a geometric extension of the approximate near\nneighbors problem. In this version, we consider a metric for which the unit\nball is an ellipsoid and its orientation is specified with the query.\n  An important application that motivates our work is group recommendation\nsystems. Such a system embeds movies and users in the same feature space, and\nthe task of recommending a movie for a group to watch together, translates to a\nset-query \\( Q \\) using an appropriate similarity.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "kaplan2021locality", "year": "2021", "title":"Locality Sensitive Hashing For Efficient Similar Polygon Retrieval", "abstract": "<p>Locality Sensitive Hashing (LSH) is an effective method of indexing a set of\nitems to support efficient nearest neighbors queries in high-dimensional\nspaces. The basic idea of LSH is that similar items should produce hash\ncollisions with higher probability than dissimilar items.\n  We study LSH for (not necessarily convex) polygons, and use it to give\nefficient data structures for similar shape retrieval. Arkin et al. represent\npolygons by their “turning function” - a function which follows the angle\nbetween the polygon’s tangent and the \\( x \\)-axis while traversing the perimeter\nof the polygon. They define the distance between polygons to be variations of\nthe \\( L_p \\) (for \\(p=1,2\\)) distance between their turning functions. This metric\nis invariant under translation, rotation and scaling (and the selection of the\ninitial point on the perimeter) and therefore models well the intuitive notion\nof shape resemblance.\n  We develop and analyze LSH near neighbor data structures for several\nvariations of the \\( L_p \\) distance for functions (for \\(p=1,2\\)). By applying our\nschemes to the turning functions of a collection of polygons we obtain\nefficient near neighbor LSH-based structures for polygons. To tune our\nstructures to turning functions of polygons, we prove some new properties of\nthese turning functions that may be of independent interest.\n  As part of our analysis, we address the following problem which is of\nindependent interest. Find the vertical translation of a function \\( f \\) that is\nclosest in \\( L_1 \\) distance to a function \\( g \\). We prove tight bounds on the\napproximation guarantee obtained by the translation which is equal to the\ndifference between the averages of \\( g \\) and \\( f \\).</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "kapralov2020scaling", "year": "2020", "title":"Scaling Up Kernel Ridge Regression Via Locality Sensitive Hashing", "abstract": "<p>Random binning features, introduced in the seminal paper of Rahimi and Recht\n(2007), are an efficient method for approximating a kernel matrix using\nlocality sensitive hashing. Random binning features provide a very simple and\nefficient way of approximating the Laplace kernel but unfortunately do not\napply to many important classes of kernels, notably ones that generate smooth\nGaussian processes, such as the Gaussian kernel and Matern kernel. In this\npaper, we introduce a simple weighted version of random binning features and\nshow that the corresponding kernel function generates Gaussian processes of any\ndesired smoothness. We show that our weighted random binning features provide a\nspectral approximation to the corresponding kernel matrix, leading to efficient\nalgorithms for kernel ridge regression. Experiments on large scale regression\ndatasets show that our method outperforms the accuracy of random Fourier\nfeatures method.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "kapralov2024adversarial", "year": "2024", "title":"On The Adversarial Robustness Of Locality-sensitive Hashing In Hamming Space", "abstract": "<p>Locality-sensitive hashing~[Indyk,Motwani’98] is a classical data structure\nfor approximate nearest neighbor search. It allows, after a close to linear\ntime preprocessing of the input dataset, to find an approximately nearest\nneighbor of any fixed query in sublinear time in the dataset size. The\nresulting data structure is randomized and succeeds with high probability for\nevery fixed query.\n  In many modern applications of nearest neighbor search the queries are chosen\nadaptively. In this paper, we study the robustness of the locality-sensitive\nhashing to adaptive queries in Hamming space. We present a simple adversary\nthat can, under mild assumptions on the initial point set, provably find a\nquery to the approximate near neighbor search data structure that the data\nstructure fails on. Crucially, our adaptive algorithm finds the hard query\nexponentially faster than random sampling.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "karaman2019unsupervised", "year": "2019", "title":"Unsupervised Rank-preserving Hashing For Large-scale Image Retrieval", "abstract": "<p>We propose an unsupervised hashing method which aims to produce binary codes\nthat preserve the ranking induced by a real-valued representation. Such compact\nhash codes enable the complete elimination of real-valued feature storage and\nallow for significant reduction of the computation complexity and storage cost\nof large-scale image retrieval applications. Specifically, we learn a neural\nnetwork-based model, which transforms the input representation into a binary\nrepresentation. We formalize the training objective of the network in an\nintuitive and effective way, considering each training sample as a query and\naiming to obtain the same retrieval results using the produced hash codes as\nthose obtained with the original features. This training formulation directly\noptimizes the hashing model for the target usage of the hash codes it produces.\nWe further explore the addition of a decoder trained to obtain an approximated\nreconstruction of the original features. At test time, we retrieved the most\npromising database samples with an efficient graph-based search procedure using\nonly our hash codes and perform re-ranking using the reconstructed features,\nthus without needing to access the original features at all. Experiments\nconducted on multiple publicly available large-scale datasets show that our\nmethod consistently outperforms all compared state-of-the-art unsupervised\nhashing methods and that the reconstruction procedure can effectively boost the\nsearch accuracy with a minimal constant additional cost.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Unsupervised"] },
{"key": "karan2018fast", "year": "2018", "title":"Fast Counting In Machine Learning Applications", "abstract": "<p>We propose scalable methods to execute counting queries in machine learning\napplications. To achieve memory and computational efficiency, we abstract\ncounting queries and their context such that the counts can be aggregated as a\nstream. We demonstrate performance and scalability of the resulting approach on\nrandom queries, and through extensive experimentation using Bayesian networks\nlearning and association rule mining. Our methods significantly outperform\ncommonly used ADtrees and hash tables, and are practical alternatives for\nprocessing large-scale data.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "karch2010improved", "year": "2010", "title":"Improved Fast Similarity Search In Dictionaries", "abstract": "<p>We engineer an algorithm to solve the approximate dictionary matching\nproblem. Given a list of words \\(\\mathcal{W}\\), maximum distance \\(d\\) fixed at\npreprocessing time and a query word \\(q\\), we would like to retrieve all words\nfrom \\(\\mathcal{W}\\) that can be transformed into \\(q\\) with \\(d\\) or less edit\noperations. We present data structures that support fault tolerant queries by\ngenerating an index. On top of that, we present a generalization of the method\nthat eases memory consumption and preprocessing time significantly. At the same\ntime, running times of queries are virtually unaffected. We are able to match\nin lists of hundreds of thousands of words and beyond within microseconds for\nreasonable distances.</p>\n", "tags": ["ARXIV"] },
{"key": "karjalainen2024fast", "year": "2024", "title":"Fast Redescription Mining Using Locality-sensitive Hashing", "abstract": "<p>Redescription mining is a data analysis technique that has found applications\nin diverse fields. The most used redescription mining approaches involve two\nphases: finding matching pairs among data attributes and extending the pairs.\nThis process is relatively efficient when the number of attributes remains\nlimited and when the attributes are Boolean, but becomes almost intractable\nwhen the data consist of many numerical attributes. In this paper, we present\nnew algorithms that perform the matching and extension orders of magnitude\nfaster than the existing approaches. Our algorithms are based on\nlocality-sensitive hashing with a tailored approach to handle the\ndiscretisation of numerical attributes as used in redescription mining.</p>\n", "tags": ["ARXIV"] },
{"key": "karppa2021deann", "year": "2021", "title":"DEANN Speeding Up Kernel-density Estimation Using Approximate Nearest Neighbor Search", "abstract": "<p>Kernel Density Estimation (KDE) is a nonparametric method for estimating the\nshape of a density function, given a set of samples from the distribution.\nRecently, locality-sensitive hashing, originally proposed as a tool for nearest\nneighbor search, has been shown to enable fast KDE data structures. However,\nthese approaches do not take advantage of the many other advances that have\nbeen made in algorithms for nearest neighbor algorithms. We present an\nalgorithm called Density Estimation from Approximate Nearest Neighbors (DEANN)\nwhere we apply Approximate Nearest Neighbor (ANN) algorithms as a black box\nsubroutine to compute an unbiased KDE. The idea is to find points that have a\nlarge contribution to the KDE using ANN, compute their contribution exactly,\nand approximate the remainder with Random Sampling (RS). We present a\ntheoretical argument that supports the idea that an ANN subroutine can speed up\nthe evaluation. Furthermore, we provide a C++ implementation with a Python\ninterface that can make use of an arbitrary ANN implementation as a subroutine\nfor kernel density estimation. We show empirically that our implementation\noutperforms state of the art implementations in all high dimensional datasets\nwe considered, and matches the performance of RS in cases where the ANN yield\nno gains in performance.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "karunanayake2020multi", "year": "2020", "title":"A Multi-modal Neural Embeddings Approach For Detecting Mobile Counterfeit Apps A Case Study On Google Play Store", "abstract": "<p>Counterfeit apps impersonate existing popular apps in attempts to misguide\nusers to install them for various reasons such as collecting personal\ninformation or spreading malware. Many counterfeits can be identified once\ninstalled, however even a tech-savvy user may struggle to detect them before\ninstallation. To this end, this paper proposes to leverage the recent advances\nin deep learning methods to create image and text embeddings so that\ncounterfeit apps can be efficiently identified when they are submitted for\npublication. We show that a novel approach of combining content embeddings and\nstyle embeddings outperforms the baseline methods for image similarity such as\nSIFT, SURF, and various image hashing methods. We first evaluate the\nperformance of the proposed method on two well-known datasets for evaluating\nimage similarity methods and show that content, style, and combined embeddings\nincrease precision@k and recall@k by 10%-15% and 12%-25%, respectively when\nretrieving five nearest neighbours. Second, specifically for the app\ncounterfeit detection problem, combined content and style embeddings achieve\n12% and 14% increase in precision@k and recall@k, respectively compared to the\nbaseline methods. Third, we present an analysis of approximately 1.2 million\napps from Google Play Store and identify a set of potential counterfeits for\ntop-10,000 popular apps. Under a conservative assumption, we were able to find\n2,040 potential counterfeits that contain malware in a set of 49,608 apps that\nshowed high similarity to one of the top-10,000 popular apps in Google Play\nStore. We also find 1,565 potential counterfeits asking for at least five\nadditional dangerous permissions than the original app and 1,407 potential\ncounterfeits having at least five extra third party advertisement libraries.</p>\n", "tags": ["ARXIV","Case Study","Deep Learning"] },
{"key": "kaser2012strongly", "year": "2012", "title":"Strongly Universal String Hashing Is Fast", "abstract": "<p>We present fast strongly universal string hashing families: they can process\ndata at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that\nthese families—though they require a large buffer of random numbers—are\noften faster than popular hash functions with weaker theoretical guarantees.\nMoreover, conventional wisdom is that hash functions with fewer multiplications\nare faster. Yet we find that they may fail to be faster due to operation\npipelining. We present experimental results on several processors including\nlow-powered processors. Our tests include hash functions designed for\nprocessors with the Carry-Less Multiplication (CLMUL) instruction set. We also\nprove, using accessible proofs, the strong universality of our families.</p>\n", "tags": ["Independent"] },
{"key": "kasyap2024private", "year": "2024", "title":"Private And Secure Fuzzy Name Matching", "abstract": "<p>Modern financial institutions rely on data for many operations, including a\nneed to drive efficiency, enhance services and prevent financial crime. Data\nsharing across an organisation or between institutions can facilitate rapid,\nevidence-based decision making, including identifying money laundering and\nfraud. However, data privacy regulations impose restrictions on data sharing.\nPrivacy-enhancing technologies are being increasingly employed to allow\norganisations to derive shared intelligence while ensuring regulatory\ncompliance. This paper examines the case in which regulatory restrictions mean\na party cannot share data on accounts of interest with another (internal or\nexternal) party to identify people that hold an account in each dataset. We\nobserve that the names of account holders may be recorded differently in each\ndata set. We introduce a novel privacy-preserving approach for fuzzy name\nmatching across institutions, employing fully homomorphic encryption with\nlocality-sensitive hashing. The efficiency of the approach is enhanced using a\nclustering mechanism. The practicality and effectiveness of the proposed\napproach are evaluated using different datasets. Experimental results\ndemonstrate it takes around 100 and 1000 seconds to search 1000 names from 10k\nand 100k names, respectively. Moreover, the proposed approach exhibits\nsignificant improvement in reducing communication overhead by 30-300 times,\nusing clustering.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "kehl2016hashmod", "year": "2016", "title":"Hashmod A Hashing Method For Scalable 3D Object Detection", "abstract": "<p>We present a scalable method for detecting objects and estimating their 3D\nposes in RGB-D data. To this end, we rely on an efficient representation of\nobject views and employ hashing techniques to match these views against the\ninput frame in a scalable way. While a similar approach already exists for 2D\ndetection, we show how to extend it to estimate the 3D pose of the detected\nobjects. In particular, we explore different hashing strategies and identify\nthe one which is more suitable to our problem. We show empirically that the\ncomplexity of our method is sublinear with the number of objects and we enable\ndetection and pose estimation of many 3D objects with high accuracy while\noutperforming the state-of-the-art in terms of runtime.</p>\n", "tags": ["ARXIV"] },
{"key": "kelly2019lock", "year": "2019", "title":"Lock-free Hopscotch Hashing", "abstract": "<p>In this paper we present a lock-free version of Hopscotch Hashing. Hopscotch\nHashing is an open addressing algorithm originally proposed by Herlihy, Shavit,\nand Tzafrir, which is known for fast performance and excellent cache locality.\nThe algorithm allows users of the table to skip or jump over irrelevant\nentries, allowing quick search, insertion, and removal of entries. Unlike\ntraditional linear probing, Hopscotch Hashing is capable of operating under a\nhigh load factor, as probe counts remain small. Our lock-free version improves\non both speed, cache locality, and progress guarantees of the original, being a\nchimera of two concurrent hash tables. We compare our data structure to various\nother lock-free and blocking hashing algorithms and show that its performance\nis in many cases superior to existing strategies. The proposed lock-free\nversion overcomes some of the drawbacks associated with the original blocking\nversion, leading to a substantial boost in scalability while maintaining\nattractive features like physical deletion or probe-chain compression.</p>\n", "tags": ["ARXIV"] },
{"key": "kennedy2016fast", "year": "2016", "title":"Fast Cross-polytope Locality-sensitive Hashing", "abstract": "<p>We provide a variant of cross-polytope locality sensitive hashing with\nrespect to angular distance which is provably optimal in asymptotic sensitivity\nand enjoys \\(\\mathcal{O}(d \\ln d )\\) hash computation time. Building on a recent\nresult (by Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015), we show that\noptimal asymptotic sensitivity for cross-polytope LSH is retained even when the\ndense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform\nfollowed by discrete pseudo-rotation, reducing the hash computation time from\n\\(\\mathcal{O}(d^2)\\) to \\(\\mathcal{O}(d \\ln d )\\). Moreover, our scheme achieves\nthe optimal rate of convergence for sensitivity. By incorporating a\nlow-randomness Johnson-Lindenstrauss transform, our scheme can be modified to\nrequire only \\(\\mathcal{O}(\\ln^9(d))\\) random bits</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "khandelwal2020nearest", "year": "2020", "title":"Nearest Neighbor Machine Translation", "abstract": "<p>We introduce \\(k\\)-nearest-neighbor machine translation (\\(k\\)NN-MT), which\npredicts tokens with a nearest neighbor classifier over a large datastore of\ncached examples, using representations from a neural translation model for\nsimilarity search. This approach requires no additional training and scales to\ngive the decoder direct access to billions of examples at test time, resulting\nin a highly expressive model that consistently improves performance across many\nsettings. Simply adding nearest neighbor search improves a state-of-the-art\nGerman-English translation model by 1.5 BLEU. \\(k\\)NN-MT allows a single model to\nbe adapted to diverse domains by using a domain-specific datastore, improving\nresults by an average of 9.2 BLEU over zero-shot transfer, and achieving new\nstate-of-the-art results – without training on these domains. A massively\nmultilingual model can also be specialized for particular language pairs, with\nimprovements of 3 BLEU for translating from English into German and Chinese.\nQualitatively, \\(k\\)NN-MT is easily interpretable; it combines source and target\ncontext to retrieve highly relevant examples.</p>\n", "tags": ["ARXIV"] },
{"key": "khoram2019interleaved", "year": "2019", "title":"Interleaved Composite Quantization For High-dimensional Similarity Search", "abstract": "<p>Similarity search retrieves the nearest neighbors of a query vector from a\ndataset of high-dimensional vectors. As the size of the dataset grows, the cost\nof performing the distance computations needed to implement a query can become\nprohibitive. A method often used to reduce this computational cost is\nquantization of the vector space and location-based encoding of the dataset\nvectors. These encodings can be used during query processing to find\napproximate nearest neighbors of the query point quickly. Search speed can be\nimproved by using shorter codes, but shorter codes have higher quantization\nerror, leading to degraded precision. In this work, we propose the Interleaved\nComposite Quantization (ICQ) which achieves fast similarity search without\nusing shorter codes. In ICQ, a small subset of the code is used to approximate\nthe distances, with complete codes being used only when necessary. Our method\neffectively reduces both code length and quantization error. Furthermore, ICQ\nis compatible with several recently proposed techniques for reducing\nquantization error and can be used in conjunction with these other techniques\nto improve results. We confirm these claims and show strong empirical\nperformance of ICQ using several synthetic and real-word datasets.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "khosla2016faster", "year": "2016", "title":"A Faster Algorithm For Cuckoo Insertion And Bipartite Matching In Large Graphs", "abstract": "<p>Hash tables are ubiquitous in computer science for efficient access to large\ndatasets. However, there is always a need for approaches that offer compact\nmemory utilisation without substantial degradation of lookup performance.\nCuckoo hashing is an efficient technique of creating hash tables with high\nspace utilisation and offer a guaranteed constant access time. We are given \\(n\\)\nlocations and \\(m\\) items. Each item has to be placed in one of the \\(k\\ge2\\)\nlocations chosen by \\(k\\) random hash functions. By allowing more than one choice\nfor a single item, cuckoo hashing resembles multiple choice allocations\nschemes. In addition it supports dynamically changing the location of an item\namong its possible locations. We propose and analyse an insertion algorithm for\ncuckoo hashing that runs in <em>linear time</em> with high probability and in\nexpectation. Previous work on total allocation time has analysed breadth first\nsearch, and it was shown to be linear only in <em>expectation</em>. Our algorithm\nfinds an assignment (with probability 1) whenever it exists. In contrast, the\nother known insertion method, known as <em>random walk insertion</em>, may run\nindefinitely even for a solvable instance. We also present experimental results\ncomparing the performance of our algorithm with the random walk method, also\nfor the case when each location can hold more than one item.\n  As a corollary we obtain a linear time algorithm (with high probability and\nin expectation) for finding perfect matchings in a special class of sparse\nrandom bipartite graphs. We support this by performing experiments on a real\nworld large dataset for finding maximum matchings in general large bipartite\ngraphs. We report an order of magnitude improvement in the running time as\ncompared to the <em>Hopkraft-Karp</em> matching algorithm.</p>\n", "tags": ["Graph","Independent"] },
{"key": "kiktenko2019proof", "year": "2019", "title":"Proof-of-forgery For Hash-based Signatures", "abstract": "<p>In the present work, a peculiar property of hash-based signatures allowing\ndetection of their forgery event is explored. This property relies on the fact\nthat a successful forgery of a hash-based signature most likely results in a\ncollision with respect to the employed hash function, while the demonstration\nof this collision could serve as convincing evidence of the forgery. Here we\nprove that with properly adjusted parameters Lamport and Winternitz one-time\nsignatures schemes could exhibit a forgery detection availability property.\nThis property is of significant importance in the framework of crypto-agility\nparadigm since the considered forgery detection serves as an alarm that the\nemployed cryptographic hash function becomes insecure to use and the\ncorresponding scheme has to be replaced.</p>\n", "tags": ["AAAI","Graph","Independent"] },
{"key": "kim2015bilinear", "year": "2015", "title":"Bilinear Random Projections For Locality-sensitive Binary Codes", "abstract": "<p>Locality-sensitive hashing (LSH) is a popular data-independent indexing\nmethod for approximate similarity search, where random projections followed by\nquantization hash the points from the database so as to ensure that the\nprobability of collision is much higher for objects that are close to each\nother than for those that are far apart. Most of high-dimensional visual\ndescriptors for images exhibit a natural matrix structure. When visual\ndescriptors are represented by high-dimensional feature vectors and long binary\ncodes are assigned, a random projection matrix requires expensive complexities\nin both space and time. In this paper we analyze a bilinear random projection\nmethod where feature matrices are transformed to binary codes by two smaller\nrandom projection matrices. We base our theoretical analysis on extending\nRaginsky and Lazebnik’s result where random Fourier features are composed with\nrandom binary quantizers to form locality sensitive binary codes. To this end,\nwe answer the following two questions: (1) whether a bilinear random projection\nalso yields similarity-preserving binary codes; (2) whether a bilinear random\nprojection yields performance gain or loss, compared to a large linear\nprojection. Regarding the first question, we present upper and lower bounds on\nthe expected Hamming distance between binary codes produced by bilinear random\nprojections. In regards to the second question, we analyze the upper and lower\nbounds on covariance between two bits of binary codes, showing that the\ncorrelation between two bits is small. Numerical experiments on MNIST and\nFlickr45K datasets confirm the validity of our method.</p>\n", "tags": ["ARXIV","Independent","LSH","Quantisation"] },
{"key": "kim2019nearest", "year": "2019", "title":"Nearest Neighbor Search-based Bitwise Source Separation Using Discriminant Winner-take-all Hashing", "abstract": "<p>We propose an iteration-free source separation algorithm based on\nWinner-Take-All (WTA) hash codes, which is a faster, yet accurate alternative\nto a complex machine learning model for single-channel source separation in a\nresource-constrained environment. We first generate random permutations with\nWTA hashing to encode the shape of the multidimensional audio spectrum to a\nreduced bitstring representation. A nearest neighbor search on the hash codes\nof an incoming noisy spectrum as the query string results in the closest\nmatches among the hashed mixture spectra. Using the indices of the matching\nframes, we obtain the corresponding ideal binary mask vectors for denoising.\nSince both the training data and the search operation are bitwise, the\nprocedure can be done efficiently in hardware implementations. Experimental\nresults show that the WTA hash codes are discriminant and provide an affordable\ndictionary search mechanism that leads to a competent performance compared to a\ncomprehensive model and oracle masking.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "kirchoff2024utilizing", "year": "2024", "title":"Utilizing Low-dimensional Molecular Embeddings For Rapid Chemical Similarity Search", "abstract": "<p>Nearest neighbor-based similarity searching is a common task in chemistry,\nwith notable use cases in drug discovery. Yet, some of the most commonly used\napproaches for this task still leverage a brute-force approach. In practice\nthis can be computationally costly and overly time-consuming, due in part to\nthe sheer size of modern chemical databases. Previous computational\nadvancements for this task have generally relied on improvements to hardware or\ndataset-specific tricks that lack generalizability. Approaches that leverage\nlower-complexity searching algorithms remain relatively underexplored. However,\nmany of these algorithms are approximate solutions and/or struggle with typical\nhigh-dimensional chemical embeddings. Here we evaluate whether a combination of\nlow-dimensional chemical embeddings and a k-d tree data structure can achieve\nfast nearest neighbor queries while maintaining performance on standard\nchemical similarity search benchmarks. We examine different dimensionality\nreductions of standard chemical embeddings as well as a learned,\nstructurally-aware embedding – SmallSA – for this task. With this framework,\nsearches on over one billion chemicals execute in less than a second on a\nsingle CPU core, five orders of magnitude faster than the brute-force approach.\nWe also demonstrate that SmallSA achieves competitive performance on chemical\nsimilarity benchmarks.</p>\n", "tags": ["ARXIV"] },
{"key": "kishimoto2012evaluation", "year": "2012", "title":"Evaluation Of A Simple Scalable Parallel Best-first Search Strategy", "abstract": "<p>Large-scale, parallel clusters composed of commodity processors are\nincreasingly available, enabling the use of vast processing capabilities and\ndistributed RAM to solve hard search problems. We investigate Hash-Distributed\nA* (HDA<em>), a simple approach to parallel best-first search that asynchronously\ndistributes and schedules work among processors based on a hash function of the\nsearch state. We use this approach to parallelize the A</em> algorithm in an\noptimal sequential version of the Fast Downward planner, as well as a 24-puzzle\nsolver. The scaling behavior of HDA* is evaluated experimentally on a shared\nmemory, multicore machine with 8 cores, a cluster of commodity machines using\nup to 64 cores, and large-scale high-performance clusters, using up to 2400\nprocessors. We show that this approach scales well, allowing the effective\nutilization of large amounts of distributed memory to optimally solve problems\nwhich require terabytes of RAM. We also compare HDA* to Transposition-table\nDriven Scheduling (TDS), a hash-based parallelization of IDA<em>, and show that,\nin planning, HDA</em> significantly outperforms TDS. A simple hybrid which combines\nHDA* and TDS to exploit strengths of both algorithms is proposed and evaluated.</p>\n", "tags": ["AAAI","Independent"] },
{"key": "klassen2011independence", "year": "2011", "title":"Independence Of Tabulation-based Hash Classes", "abstract": "<p>A tabulation-based hash function maps a key into d derived characters\nindexing random values in tables that are then combined with bitwise xor\noperations to give the hash. Thorup and Zhang (2004) presented d-wise\nindependent tabulation-based hash classes that use linear maps over finite\nfields to map a key, considered as a vector (a,b), to derived characters. We\nshow that a variant where the derived characters are a+b*i for i=0,…, q-1\n(using integer arithmetic) yielding (2d-1)-wise independence. Our analysis is\nbased on an algebraic property that characterizes k-wise independence of\ntabulation-based hashing schemes, and combines this characterization with a\ngeometric argument. We also prove a non-trivial lower bound on the number of\nderived characters necessary for k-wise independence with our and related hash\nclasses.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "klein2015conditional", "year": "2015", "title":"A Conditional Berry-esseen Bound And A Conditional Large Deviation Result Without Laplace Transform. Application To Hashing With Linear Probing", "abstract": "<p>\\noindent We study the asymptotic behavior of a sum of independent and\nidentically distributed random variables conditioned by a sum of independent\nand identically distributed integer-valued random variables. We prove a\nBerry-Esseen bound in a general setting and a large deviation result when the\nLaplace transform of the underlying distribution is not defined in a\nneighborhood of zero. Then we present several combinatorial applications. In\nparticular, we prove a large deviation result for the model of hashing with\nlinear probing.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "klein2016deviation", "year": "2016", "title":"Deviation Results For Sparse Tables In Hashing With Linear Probing", "abstract": "<p>We consider the model of hashing with linear probing and we establish the\nmoderate and large deviations for the total displacement in sparse tables. In\nthis context, Weibull-like-tailed random variables appear. Deviations for sums\nof such heavy-tailed random variables are studied in\n\\cite{Nagaev69-1,Nagaev69-2}. Here we adapt the proofs therein to deal with\nconditioned sums of such variables and solve the open question in \\cite{TFC12}.\nBy the way, we establish the deviations of the total displacement in full\ntables, which can be derived from the deviations of empirical processes of\ni.i.d.\\ random variables established in \\cite{Wu94}..</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "klein2017end", "year": "2017", "title":"End-to-end Supervised Product Quantization For Image Search And Retrieval", "abstract": "<p>Product Quantization, a dictionary based hashing method, is one of the\nleading unsupervised hashing techniques. While it ignores the labels, it\nharnesses the features to construct look up tables that can approximate the\nfeature space. In recent years, several works have achieved state of the art\nresults on hashing benchmarks by learning binary representations in a\nsupervised manner. This work presents Deep Product Quantization (DPQ), a\ntechnique that leads to more accurate retrieval and classification than the\nlatest state of the art methods, while having similar computational complexity\nand memory footprint as the Product Quantization method. To our knowledge, this\nis the first work to introduce a dictionary-based representation that is\ninspired by Product Quantization and which is learned end-to-end, and thus\nbenefits from the supervised signal. DPQ explicitly learns soft and hard\nrepresentations to enable an efficient and accurate asymmetric search, by using\na straight-through estimator. Our method obtains state of the art results on an\nextensive array of retrieval and classification experiments.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "knudsen2015quicksort", "year": "2015", "title":"Quicksort Largest Bucket And Min-wise Hashing With Limited Independence", "abstract": "<p>Randomized algorithms and data structures are often analyzed under the\nassumption of access to a perfect source of randomness. The most fundamental\nmetric used to measure how “random” a hash function or a random number\ngenerator is, is its independence: a sequence of random variables is said to be\n\\(k\\)-independent if every variable is uniform and every size \\(k\\) subset is\nindependent. In this paper we consider three classic algorithms under limited\nindependence. We provide new bounds for randomized quicksort, min-wise hashing\nand largest bucket size under limited independence. Our results can be\nsummarized as follows.\n  -Randomized quicksort. When pivot elements are computed using a\n\\(5\\)-independent hash function, Karloff and Raghavan, J.ACM’93 showed \\(O ( n\nlog n)\\) expected worst-case running time for a special version of quicksort.\nWe improve upon this, showing that the same running time is achieved with only\n\\(4\\)-independence.\n  -Min-wise hashing. For a set \\(A\\), consider the probability of a particular\nelement being mapped to the smallest hash value. It is known that\n\\(5\\)-independence implies the optimal probability \\(O (1 /n)\\). Broder et al.,\nSTOC’98 showed that \\(2\\)-independence implies it is \\(O(1 / \\sqrt{|A|})\\). We show\na matching lower bound as well as new tight bounds for \\(3\\)- and \\(4\\)-independent\nhash functions.\n  -Largest bucket. We consider the case where \\(n\\) balls are distributed to \\(n\\)\nbuckets using a \\(k\\)-independent hash function and analyze the largest bucket\nsize. Alon et. al, STOC’97 showed that there exists a \\(2\\)-independent hash\nfunction implying a bucket of size \\(Ω ( n^{1/2})\\). We generalize the\nbound, providing a \\(k\\)-independent family of functions that imply size \\(Ω\n( n^{1/k})\\).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "knudsen2017linear", "year": "2017", "title":"Linear Hashing Is Awesome", "abstract": "<p>We consider the hash function \\(h(x) = ((ax+b) \\bmod p) \\bmod n\\) where \\(a,b\\)\nare chosen uniformly at random from \\(\\{0,1,\\ldots,p-1\\}\\). We prove that when we\nuse \\(h(x)\\) in hashing with chaining to insert \\(n\\) elements into a table of size\n\\(n\\) the expected length of the longest chain is\n\\(\\tilde{O}!\\left(n^{1/3}\\right)\\). The proof also generalises to give the same\nbound when we use the multiply-shift hash function by Dietzfelbinger et al.\n[Journal of Algorithms 1997].</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "knuth1998linear", "year": "1998", "title":"Linear Probing And Graphs", "abstract": "<p>Mallows and Riordan showed in 1968 that labeled trees with a small number of\ninversions are related to labeled graphs that are connected and sparse. Wright\nenumerated sparse connected graphs in 1977, and Kreweras related the inversions\nof trees to the so-called ``parking problem’’ in 1980. A~combination of these\nthree results leads to a surprisingly simple analysis of the behavior of\nhashing by linear probing, including higher moments of the cost of successful\nsearch.</p>\n", "tags": ["Graph"] },
{"key": "ko2021low", "year": "2021", "title":"Low-precision Quantization For Efficient Nearest Neighbor Search", "abstract": "<p>Fast k-Nearest Neighbor search over real-valued vector spaces (KNN) is an\nimportant algorithmic task for information retrieval and recommendation\nsystems. We present a method for using reduced precision to represent vectors\nthrough quantized integer values, enabling both a reduction in the memory\noverhead of indexing these vectors and faster distance computations at query\ntime. While most traditional quantization techniques focus on minimizing the\nreconstruction error between a point and its uncompressed counterpart, we focus\ninstead on preserving the behavior of the underlying distance metric.\nFurthermore, our quantization approach is applied at the implementation level\nand can be combined with existing KNN algorithms. Our experiments on both open\nsource and proprietary datasets across multiple popular KNN frameworks validate\nthat quantized distance metrics can reduce memory by 60% and improve query\nthroughput by 30%, while incurring only a 2% reduction in recall.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "kokkala2015coloring", "year": "2015", "title":"A Coloring Of The Square Of The 8-cube With 13 Colors", "abstract": "<p>Let \\(\\chi_{\\bar{k}}(n)\\) be the number of colors required to color the\n\\(n\\)-dimensional hypercube such that no two vertices with the same color are at\na distance at most \\(k\\). In other words, \\(\\chi_{\\bar{k}}(n)\\) is the minimum\nnumber of binary codes with minimum distance at least \\(k+1\\) required to\npartition the \\(n\\)-dimensional Hamming space. By giving an explicit coloring, it\nis shown that \\(\\chi_{\\bar{2}}(8)=13\\).</p>\n", "tags": ["ARXIV"] },
{"key": "kolosovskiy2009data", "year": "2009", "title":"Data Structure For Representing A Graph Combination Of Linked List And Hash Table", "abstract": "<p>In this article we discuss a data structure, which combines advantages of two\ndifferent ways for representing graphs: adjacency matrix and collection of\nadjacency lists. This data structure can fast add and search edges (advantages\nof adjacency matrix), use linear amount of memory, let to obtain adjacency list\nfor certain vertex (advantages of collection of adjacency lists). Basic\nknowledge of linked lists and hash tables is required to understand this\narticle. The article contains examples of implementation on Java.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "komorowski2017evaluation", "year": "2017", "title":"Evaluation Of Hashing Methods Performance On Binary Feature Descriptors", "abstract": "<p>In this paper we evaluate performance of data-dependent hashing methods on\nbinary data. The goal is to find a hashing method that can effectively produce\nlower dimensional binary representation of 512-bit FREAK descriptors. A\nrepresentative sample of recent unsupervised, semi-supervised and supervised\nhashing methods was experimentally evaluated on large datasets of labelled\nbinary FREAK feature descriptors.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "komorowski2017random", "year": "2017", "title":"Random Binary Trees For Approximate Nearest Neighbour Search In Binary Space", "abstract": "<p>Approximate nearest neighbour (ANN) search is one of the most important\nproblems in computer science fields such as data mining or computer vision. In\nthis paper, we focus on ANN for high-dimensional binary vectors and we propose\na simple yet powerful search method that uses Random Binary Search Trees\n(RBST). We apply our method to a dataset of 1.25M binary local feature\ndescriptors obtained from a real-life image-based localisation system provided\nby Google as a part of Project Tango. An extensive evaluation of our method\nagainst the state-of-the-art variations of Locality Sensitive Hashing (LSH),\nnamely Uniform LSH and Multi-probe LSH, shows the superiority of our method in\nterms of retrieval precision with performance boost of over 20%</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "kong2012isotropic", "year": "2012", "title":"Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": ["NEURIPS","Theory","Unsupervised"] },
{"key": "kong2024double", "year": "2024", "title":"Double-bit Quantisation For Hashing", "abstract": "<p>Hashing, which tries to learn similarity-preserving binary\ncodes for data representation, has been widely\nused for efficient nearest neighbor search in massive\ndatabases due to its fast query speed and low storage\ncost. Because it is NP hard to directly compute the best\nbinary codes for a given data set, mainstream hashing\nmethods typically adopt a two-stage strategy. In the\nfirst stage, several projected dimensions of real values\nare generated. Then in the second stage, the real values\nwill be quantized into binary codes by thresholding.\nCurrently, most existing methods use one single bit to\nquantize each projected dimension. One problem with\nthis single-bit quantization (SBQ) is that the threshold\ntypically lies in the region of the highest point density\nand consequently a lot of neighboring points close to\nthe threshold will be hashed to totally different bits,\nwhich is unexpected according to the principle of hashing.\nIn this paper, we propose a novel quantization strategy,\ncalled double-bit quantization (DBQ), to solve the\nproblem of SBQ. The basic idea of DBQ is to quantize\neach projected dimension into double bits with adaptively\nlearned thresholds. Extensive experiments on two\nreal data sets show that our DBQ strategy can signifi-\ncantly outperform traditional SBQ strategy for hashing.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "kong2024isotropic", "year": "2024", "title":"Isotropic Hashing", "abstract": "<p>Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</p>\n", "tags": ["ARXIV","Theory","Unsupervised"] },
{"key": "kong2024manhattan", "year": "2024", "title":"Manhattan Hashing For Large-scale Image Retrieval", "abstract": "<p>Hashing is used to learn binary-code representation for data with\nexpectation of preserving the neighborhood structure in the original\nfeature space. Due to its fast query speed and reduced storage\ncost, hashing has been widely used for efficient nearest neighbor\nsearch in a large variety of applications like text and image retrieval.\nMost existing hashing methods adopt Hamming distance to\nmeasure the similarity (neighborhood) between points in the hashcode\nspace. However, one problem with Hamming distance is that\nit may destroy the neighborhood structure in the original feature\nspace, which violates the essential goal of hashing. In this paper,\nManhattan hashing (MH), which is based on Manhattan distance, is\nproposed to solve the problem of Hamming distance based hashing.\nThe basic idea of MH is to encode each projected dimension with\nmultiple bits of natural binary code (NBC), based on which the\nManhattan distance between points in the hashcode space is calculated\nfor nearest neighbor search. MH can effectively preserve the\nneighborhood structure in the data to achieve the goal of hashing.\nTo the best of our knowledge, this is the first work to adopt Manhattan\ndistance with NBC for hashing. Experiments on several largescale\nimage data sets containing up to one million points show that\nour MH method can significantly outperform other state-of-the-art\nmethods.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "konoshima2012hyperplane", "year": "2012", "title":"Hyperplane Arrangements And Locality-sensitive Hashing With Lift", "abstract": "<p>Locality-sensitive hashing converts high-dimensional feature vectors, such as\nimage and speech, into bit arrays and allows high-speed similarity calculation\nwith the Hamming distance. There is a hashing scheme that maps feature vectors\nto bit arrays depending on the signs of the inner products between feature\nvectors and the normal vectors of hyperplanes placed in the feature space. This\nhashing can be seen as a discretization of the feature space by hyperplanes. If\nlabels for data are given, one can determine the hyperplanes by using learning\nalgorithms. However, many proposed learning methods do not consider the\nhyperplanes’ offsets. Not doing so decreases the number of partitioned regions,\nand the correlation between Hamming distances and Euclidean distances becomes\nsmall. In this paper, we propose a lift map that converts learning algorithms\nwithout the offsets to the ones that take into account the offsets. With this\nmethod, the learning methods without the offsets give the discretizations of\nspaces as if it takes into account the offsets. For the proposed method, we\ninput several high-dimensional feature data sets and studied the relationship\nbetween the statistical characteristics of data, the number of hyperplanes, and\nthe effect of the proposed method.</p>\n", "tags": ["ARXIV"] },
{"key": "konoshima2012locality", "year": "2012", "title":"Locality-sensitive Hashing With Margin Based Feature Selection", "abstract": "<p>We propose a learning method with feature selection for Locality-Sensitive\nHashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.\nThese bit arrays can be used to perform similarity searches and personal\nauthentication. The proposed method uses bit arrays longer than those used in\nthe end for similarity and other searches and by learning selects the bits that\nwill be used. We demonstrated this method can effectively perform optimization\nfor cases such as fingerprint images with a large number of labels and\nextremely few data that share the same labels, as well as verifying that it is\nalso effective for natural images, handwritten digits, and speech features.</p>\n", "tags": ["ARXIV"] },
{"key": "koo2021semantic", "year": "2021", "title":"Semantic-aware Binary Code Representation With BERT", "abstract": "<p>A wide range of binary analysis applications, such as bug discovery, malware\nanalysis and code clone detection, require recovery of contextual meanings on a\nbinary code. Recently, binary analysis techniques based on machine learning\nhave been proposed to automatically reconstruct the code representation of a\nbinary instead of manually crafting specifics of the analysis algorithm.\nHowever, the existing approaches utilizing machine learning are still\nspecialized to solve one domain of problems, rendering recreation of models for\ndifferent types of binary analysis. In this paper, we propose DeepSemantic\nutilizing BERT in producing the semantic-aware code representation of a binary\ncode.\n  To this end, we introduce well-balanced instruction normalization that holds\nrich information for each of instructions yet minimizing an out-of-vocabulary\n(OOV) problem. DeepSemantic has been carefully designed based on our study with\nlarge swaths of binaries. Besides, DeepSemantic leverages the essence of the\nBERT architecture into re-purposing a pre-trained generic model that is readily\navailable as a one-time processing, followed by quickly applying specific\ndownstream tasks with a fine-tuning process. We demonstrate DeepSemantic with\ntwo downstream tasks, namely, binary similarity comparison and compiler\nprovenance (i.e., compiler and optimization level) prediction. Our experimental\nresults show that the binary similarity model outperforms two state-of-the-art\nbinary similarity tools, DeepBinDiff and SAFE, 49.84% and 15.83% on average,\nrespectively.</p>\n", "tags": ["ARXIV"] },
{"key": "korfhage2023elastichash", "year": "2023", "title":"Elastichash Semantic Image Similarity Search By Deep Hashing With Elasticsearch", "abstract": "<p>We present ElasticHash, a novel approach for high-quality, efficient, and\nlarge-scale semantic image similarity search. It is based on a deep hashing\nmodel to learn hash codes for fine-grained image similarity search in natural\nimages and a two-stage method for efficiently searching binary hash codes using\nElasticsearch (ES). In the first stage, a coarse search based on short hash\ncodes is performed using multi-index hashing and ES terms lookup of neighboring\nhash codes. In the second stage, the list of results is re-ranked by computing\nthe Hamming distance on long hash codes. We evaluate the retrieval performance\nof \\textit{ElasticHash} for more than 120,000 query images on about 6.9 million\ndatabase images of the OpenImages data set. The results show that our approach\nachieves high-quality retrieval results and low search latencies.</p>\n", "tags": ["AAAI","Independent"] },
{"key": "koutaki2016fast", "year": "2016", "title":"Fast Supervised Discrete Hashing And Its Analysis", "abstract": "<p>In this paper, we propose a learning-based supervised discrete hashing\nmethod. Binary hashing is widely used for large-scale image retrieval as well\nas video and document searches because the compact representation of binary\ncode is essential for data storage and reasonable for query searches using\nbit-operations. The recently proposed Supervised Discrete Hashing (SDH)\nefficiently solves mixed-integer programming problems by alternating\noptimization and the Discrete Cyclic Coordinate descent (DCC) method. We show\nthat the SDH model can be simplified without performance degradation based on\nsome preliminary experiments; we call the approximate model for this the “Fast\nSDH” (FSDH) model. We analyze the FSDH model and provide a mathematically exact\nsolution for it. In contrast to SDH, our model does not require an alternating\noptimization algorithm and does not depend on initial values. FSDH is also\neasier to implement than Iterative Quantization (ITQ). Experimental results\ninvolving a large-scale database showed that FSDH outperforms conventional SDH\nin terms of precision, recall, and computation time.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "kraus2015nearbucket", "year": "2015", "title":"Nearbucket-lsh Efficient Similarity Search In P2P Networks", "abstract": "<p>We present NearBucket-LSH, an effective algorithm for similarity search in\nlarge-scale distributed online social networks organized as peer-to-peer\noverlays. As communication is a dominant consideration in distributed systems,\nwe focus on minimizing the network cost while guaranteeing good search quality.\nOur algorithm is based on Locality Sensitive Hashing (LSH), which limits the\nsearch to collections of objects, called buckets, that have a high probability\nto be similar to the query. More specifically, NearBucket-LSH employs an LSH\nextension that searches in near buckets, and improves search quality but also\nsignificantly increases the network cost. We decrease the network cost by\nconsidering the internals of both LSH and the P2P overlay, and harnessing their\nproperties to our needs. We show that our NearBucket-LSH increases search\nquality for a given network cost compared to previous art. In many cases, the\nsearch quality increases by more than 50%.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "krishna2019video", "year": "2019", "title":"Video Segment Copy Detection Using Memory Constrained Hierarchical Batch-normalized LSTM Autoencoder", "abstract": "<p>In this report, we introduce a video hashing method for scalable video\nsegment copy detection. The objective of video segment copy detection is to\nfind the video (s) present in a large database, one of whose segments (cropped\nin time) is a (transformed) copy of the given query video. This transformation\nmay be temporal (for example frame dropping, change in frame rate) or spatial\n(brightness and contrast change, addition of noise etc.) in nature although the\nprimary focus of this report is detecting temporal attacks. The video hashing\nmethod proposed by us uses a deep learning neural network to learn variable\nlength binary hash codes for the entire video considering both temporal and\nspatial features into account. This is in contrast to most existing video\nhashing methods, as they use conventional image hashing techniques to obtain\nhash codes for a video after extracting features for every frame or certain key\nframes, in which case the temporal information present in the video is not\nexploited. Our hashing method is specifically resilient to time cropping making\nit extremely useful in video segment copy detection. Experimental results\nobtained on the large augmented dataset consisting of around 25,000 videos with\nsegment copies demonstrate the efficacy of our proposed video hashing method.</p>\n", "tags": ["ARXIV","Deep Learning","Unsupervised"] },
{"key": "krishnan2021projective", "year": "2021", "title":"Projective Clustering Product Quantization", "abstract": "<p>This paper suggests the use of projective clustering based product\nquantization for improving nearest neighbor and max-inner-product vector search\n(MIPS) algorithms. We provide anisotropic and quantized variants of projective\nclustering which outperform previous clustering methods used for this problem\nsuch as ScaNN. We show that even with comparable running time complexity, in\nterms of lookup-multiply-adds, projective clustering produces more quantization\ncenters resulting in more accurate dot-product estimates. We provide thorough\nexperimentation to support our claims.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "krizhevsky2024learning", "year": "2024", "title":"Learning Multiple Layers Of Features From Tiny Images", "abstract": "<p>Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It\nis, in principle, an excellent dataset for unsupervised training of deep generative models, but previous\nresearchers who have tried this have found it difficult to learn a good set of\nfilters from the images.\nWe show how to train a multi-layer generative model that learns to extract meaningful features which\nresemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute\nthe work among multiple machines connected on a network, we show how training such a model can be\ndone in reasonable time.\nA second problematic aspect of the tiny images dataset is that there are no reliable class labels\nwhich makes it hard to use for object recognition experiments. We created two sets of reliable labels.\nThe CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of\neach of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly\nimproved by pre-training a layer of features on a large set of unlabeled tiny images.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "kulis2009learning", "year": "2009", "title":"Learning To Hash With Binary Reconstructive Embeddings", "abstract": "<p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.  In this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the Hamming distances of the corresponding binary embeddings.  We develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings.  Unlike existing methods such as semantic hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data.  We present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "kulis2024kernelized", "year": "2024", "title":"Kernelized Locality-sensitive Hashing For Scalable Image Search", "abstract": "<p>Fast retrieval methods are critical for large-scale and\ndata-driven vision applications. Recent work has explored\nways to embed high-dimensional features or complex distance\nfunctions into a low-dimensional Hamming space\nwhere items can be efficiently searched. However, existing\nmethods do not apply for high-dimensional kernelized\ndata when the underlying feature embedding for the kernel\nis unknown. We show how to generalize locality-sensitive\nhashing to accommodate arbitrary kernel functions, making\nit possible to preserve the algorithm’s sub-linear time similarity\nsearch guarantees for a wide class of useful similarity\nfunctions. Since a number of successful image-based kernels\nhave unknown or incomputable embeddings, this is especially\nvaluable for image retrieval tasks. We validate our\ntechnique on several large-scale datasets, and show that it\nenables accurate and fast performance for example-based\nobject classification, feature matching, and content-based\nretrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "kulis2024learning", "year": "2024", "title":"Learning To Hash With Binary Reconstructive Embeddings", "abstract": "<p>Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been\nseveral recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.\nIn this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the\nreconstruction error between the original distances and the Hamming distances of the corresponding binary\nembeddings. We develop a scalable coordinate-descent algorithm for our proposed hashing objective that\nis able to efficiently learn hash functions in a variety of settings. Unlike existing methods such as semantic\nhashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions\nabout the underlying distribution of the data. We present results over several domains to demonstrate that\nour method outperforms existing state-of-the-art techniques.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "kumar2012new", "year": "2012", "title":"A New Hybrid Jpeg Image Compression Scheme Using Symbol Reduction Technique", "abstract": "<p>Lossy JPEG compression is a widely used compression technique. Normally the\nJPEG standard technique uses three process mapping reduces interpixel\nredundancy, quantization, which is lossy process and entropy encoding, which is\nconsidered lossless process. In this paper, a new technique has been proposed\nby combining the JPEG algorithm and Symbol Reduction Huffman technique for\nachieving more compression ratio. The symbols reduction technique reduces the\nnumber of symbols by combining together to form a new symbol. As a result of\nthis technique the number of Huffman code to be generated also reduced. It is\nsimple fast and easy to implement. The result shows that the performance of\nstandard JPEG method can be improved by proposed method. This hybrid approach\nachieves about 20% more compression ratio than the Standard JPEG.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "kumar2024learning", "year": "2024", "title":"Learning Hash Functions For Cross-view Similarity Search", "abstract": "<p>Many applications in Multilingual and Multimodal\nInformation Access involve searching large\ndatabases of high dimensional data objects with\nmultiple (conditionally independent) views. In this\nwork we consider the problem of learning hash\nfunctions for similarity search across the views\nfor such applications. We propose a principled\nmethod for learning a hash function for each view\ngiven a set of multiview training data objects. The\nhash functions map similar objects to similar codes\nacross the views thus enabling cross-view similarity\nsearch. We present results from an extensive\nempirical study of the proposed approach\nwhich demonstrate its effectiveness on Japanese\nlanguage People Search and Multilingual People\nSearch problems.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "kuo2016de", "year": "2016", "title":"De-hashing Server-side Context-aware Feature Reconstruction For Mobile Visual Search", "abstract": "<p>Due to the prevalence of mobile devices, mobile search becomes a more\nconvenient way than desktop search. Different from the traditional desktop\nsearch, mobile visual search needs more consideration for the limited resources\non mobile devices (e.g., bandwidth, computing power, and memory consumption).\nThe state-of-the-art approaches show that bag-of-words (BoW) model is robust\nfor image and video retrieval; however, the large vocabulary tree might not be\nable to be loaded on the mobile device. We observe that recent works mainly\nfocus on designing compact feature representations on mobile devices for\nbandwidth-limited network (e.g., 3G) and directly adopt feature matching on\nremote servers (cloud). However, the compact (binary) representation might fail\nto retrieve target objects (images, videos). Based on the hashed binary codes,\nwe propose a de-hashing process that reconstructs BoW by leveraging the\ncomputing power of remote servers. To mitigate the information loss from binary\ncodes, we further utilize contextual information (e.g., GPS) to reconstruct a\ncontext-aware BoW for better retrieval results. Experiment results show that\nthe proposed method can achieve competitive retrieval accuracy as BoW while\nonly transmitting few bits from mobile devices.</p>\n", "tags": ["ARXIV","Video Retrieval"] },
{"key": "kurpicz2022pachash", "year": "2022", "title":"Pachash Packed And Compressed Hash Tables", "abstract": "<p>We introduce PaCHash, a hash table that stores its objects contiguously in an\narray without intervening space, even if the objects have variable size. In\nparticular, each object can be compressed using standard compression\ntechniques. A small search data structure allows locating the objects in\nconstant expected time. PaCHash is most naturally described as a static\nexternal hash table where it needs a constant number of bits of internal memory\nper block of external memory. Here, in some sense, PaCHash beats a lower bound\non the space consumption of k-perfect hashing. An implementation for fast SSDs\nneeds about 5 bits of internal memory per block of external memory, requires\nonly one disk access (of variable length) per search operation, and has small\ninternal search overhead compared to the disk access cost. Our experiments show\nthat it has lower space consumption than all previous approaches even when\nconsidering objects of identical size.</p>\n", "tags": ["ARXIV"] },
{"key": "kusupati2021llc", "year": "2021", "title":"LLC Accurate Multi-purpose Learnt Low-dimensional Binary Codes", "abstract": "<p>Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional neural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for \\(\\textbf{L}\\)earning \\(\\textbf{L}\\)ow-dimensional binary \\(\\textbf{C}\\)odes \\((\\textbf{LLC})\\) for instances as well as classes. Our method does \\({\\textit{not}}\\) require any side-information, like annotated attributes or label meta-data, and learns extremely low-dimensional binary codes (\\(\\approx 20\\) bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring \\(\\textit{nearly optimal}\\) classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further quantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 retrieval problem, our learnt binary codes outperform \\(16\\) bit HashNet using only \\(10\\) bits and also are as accurate as \\(10\\) dimensional real representations. Finally, our learnt binary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs \\(\\approx3000\\) samples to tune its threshold, while we require \\({\\textit{none}}\\). Code is open-sourced at https://github.com/RAIVNLab/LLC.</p>\n", "tags": ["Has Code","Image Retrieval","NEURIPS","Supervised"] },
{"key": "kusupati2024llc", "year": "2024", "title":"LLC Accurate Multi-purpose Learnt Low-dimensional Binary Codes", "abstract": "<p>Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional \nneural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for \nLearning Low-dimensional binary Codes (LLC) for instances as well as classes. Our method does not require any side-information, like annotated attributes or label meta-data, \nand learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring nearly optimal classification accuracy for \nResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further \nquantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 \nretrieval problem, our learnt binary codes outperform 16 bit HashNet using only 10 bits and also are as accurate as 10 dimensional real representations. Finally, our learnt \nbinary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune its threshold, while we require none.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "kuszmaul2022hash", "year": "2022", "title":"A Hash Table Without Hash Functions And How To Get The Most Out Of Your Random Bits", "abstract": "<p>This paper considers the basic question of how strong of a probabilistic\nguarantee can a hash table, storing \\(n\\) \\((1 + \\Theta(1)) log n\\)-bit key/value\npairs, offer? Past work on this question has been bottlenecked by limitations\nof the known families of hash functions: The only hash tables to achieve\nfailure probabilities less than \\(1 / 2^{\\polylog n}\\) require access to\nfully-random hash functions – if the same hash tables are implemented using\nthe known explicit families of hash functions, their failure probabilities\nbecome \\(1 / \\poly(n)\\).\n  To get around these obstacles, we show how to construct a randomized data\nstructure that has the same guarantees as a hash table, but that <em>avoids\nthe direct use of hash functions</em>. Building on this, we are able to construct a\nhash table using \\(O(n)\\) random bits that achieves failure probability \\(1 /\nn^{n^{1 - \\epsilon}}\\) for an arbitrary positive constant \\(\\epsilon\\).\n  In fact, we show that this guarantee can even be achieved by a <em>succinct\ndictionary</em>, that is, by a dictionary that uses space within a \\(1 + o(1)\\)\nfactor of the information-theoretic optimum.\n  Finally we also construct a succinct hash table whose probabilistic\nguarantees fall on a different extreme, offering a failure probability of \\(1 /\n\\poly(n)\\) while using only \\(\\tilde{O}(log n)\\) random bits. This latter result\nmatches (up to low-order terms) a guarantee previously achieved by\nDietzfelbinger et al., but with increased space efficiency and with several\nsurprising technical components.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "kutlu2017grayscale", "year": "2017", "title":"Grayscale Image Authentication Using Neural Hashing", "abstract": "<p>Many different approaches for neural network based hash functions have been\nproposed. Statistical analysis must correlate security of them. This paper\nproposes novel neural hashing approach for gray scale image authentication. The\nsuggested system is rapid, robust, useful and secure. Proposed hash function\ngenerates hash values using neural network one-way property and non-linear\ntechniques. As a result security and performance analysis are performed and\nsatisfying results are achieved. These features are dominant reasons for\npreferring against traditional ones.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "kutzkov2014consistent", "year": "2014", "title":"Consistent Subset Sampling", "abstract": "<p>Consistent sampling is a technique for specifying, in small space, a subset\n\\(S\\) of a potentially large universe \\(U\\) such that the elements in \\(S\\) satisfy a\nsuitably chosen sampling condition. Given a subset \\(\\mathcal{I}\\subseteq U\\) it\nshould be possible to quickly compute \\(\\mathcal{I}\\cap S\\), i.e., the elements\nin \\(\\mathcal{I}\\) satisfying the sampling condition. Consistent sampling has\nimportant applications in similarity estimation, and estimation of the number\nof distinct items in a data stream.\n  In this paper we generalize consistent sampling to the setting where we are\ninterested in sampling size-\\(k\\) subsets occurring in some set in a collection\nof sets of bounded size \\(b\\), where \\(k\\) is a small integer. This can be done by\napplying standard consistent sampling to the \\(k\\)-subsets of each set, but that\napproach requires time \\(\\Theta(b^k)\\). Using a carefully designed hash function,\nfor a given sampling probability \\(p \\in (0,1]\\), we show how to improve the time\ncomplexity to \\(\\Theta(b^{\\lceil k/2\\rceil}log log b + pb^k)\\) in expectation,\nwhile maintaining strong concentration bounds for the sample. The space usage\nof our method is \\(\\Theta(b^{\\lceil k/4\\rceil})\\).\n  We demonstrate the utility of our technique by applying it to several\nwell-studied data mining problems. We show how to efficiently estimate the\nnumber of frequent \\(k\\)-itemsets in a stream of transactions and the number of\nbipartite cliques in a graph given as incidence stream. Further, building upon\na recent work by Campagna et al., we show that our approach can be applied to\nfrequent itemset mining in a parallel or distributed setting. We also present\napplications in graph stream mining.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "kwok2024learning", "year": "2024", "title":"Learning To Hash With A Dimension Analysis-based Quantizer For Image Retrieval", "abstract": "<p>The last few years have witnessed the rise of the big data era in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results have demonstrated that hashing can achieve promising performance due to its appealing storage and search efficiency. Since complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash code learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by the Hamming distance is very efficient. However, the massive information loss produced by the quantization step should be reduced in applications where high search accuracy is required, such as in image retrieval. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis-based quantization (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than others, and then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by the Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Quantisation"] },
{"key": "köppl2019separate", "year": "2019", "title":"Separate Chaining Meets Compact Hashing", "abstract": "<p>While separate chaining is a common strategy for resolving collisions in a\nhash table taught in most textbooks, compact hashing is a less common technique\nfor saving space when hashing integers whose domain is relatively small with\nrespect to the problem size. It is widely believed that hash tables waste a\nconsiderable amount of memory, as they either leave allocated space untouched\n(open addressing) or store additional pointers (separate chaining). For the\nformer, Cleary introduced the compact hashing technique that stores only a part\nof a key to save space. However, as can be seen by the line of research\nfocusing on compact hash tables with open addressing, there is additional\ninformation, called displacement, required for restoring a key. There are\nseveral representations of this displacement information with different space\nand time trade-offs. In this article, we introduce a separate chaining hash\ntable that applies the compact hashing technique without the need for the\ndisplacement information. Practical evaluations reveal that insertions in this\nhash table are faster or use less space than all previously known compact hash\ntables on modern computer architectures when storing sufficiently large\nsatellite data.</p>\n", "tags": ["ARXIV"] },
{"key": "kúdela2018extracting", "year": "2018", "title":"Extracting Parallel Paragraphs From Common Crawl", "abstract": "<p>Most of the current methods for mining parallel texts from the web assume\nthat web pages of web sites share same structure across languages. We believe\nthat there still exists a non-negligible amount of parallel data spread across\nsources not satisfying this assumption. We propose an approach based on a\ncombination of bivec (a bilingual extension of word2vec) and locality-sensitive\nhashing which allows us to efficiently identify pairs of parallel segments\nlocated anywhere on pages of a given web domain, regardless their structure. We\nvalidate our method on realigning segments from a large parallel corpus.\nAnother experiment with real-world data provided by Common Crawl Foundation\nconfirms that our solution scales to hundreds of terabytes large set of\nweb-crawled data.</p>\n", "tags": ["Graph"] },
{"key": "laaksonen2016constructing", "year": "2016", "title":"Constructing Error-correcting Binary Codes Using Transitive Permutation Groups", "abstract": "<p>Let \\(A_2(n,d)\\) be the maximum size of a binary code of length \\(n\\) and minimum\ndistance \\(d\\). In this paper we present the following new lower bounds:\n\\(A_2(18,4) \\ge 5632\\), \\(A_2(21,4) \\ge 40960\\), \\(A_2(22,4) \\ge 81920\\), \\(A_2(23,4)\n\\ge 163840\\), \\(A_2(24,4) \\ge 327680\\), \\(A_2(24,10) \\ge 136\\), and \\(A_2(25,6) \\ge\n17920\\). The new lower bounds are a result of a systematic computer search over\ntransitive permutation groups.</p>\n", "tags": ["ARXIV"] },
{"key": "laarhoven2017faster", "year": "2017", "title":"Faster Tuple Lattice Sieving Using Spherical Locality-sensitive Filters", "abstract": "<p>To overcome the large memory requirement of classical lattice sieving\nalgorithms for solving hard lattice problems, Bai-Laarhoven-Stehl'{e} [ANTS\n2016] studied tuple lattice sieving, where tuples instead of pairs of lattice\nvectors are combined to form shorter vectors. Herold-Kirshanova [PKC 2017]\nrecently improved upon their results for arbitrary tuple sizes, for example\nshowing that a triple sieve can solve the shortest vector problem (SVP) in\ndimension \\(d\\) in time \\(2^{0.3717d + o(d)}\\), using a technique similar to\nlocality-sensitive hashing for finding nearest neighbors.\n  In this work, we generalize the spherical locality-sensitive filters of\nBecker-Ducas-Gama-Laarhoven [SODA 2016] to obtain space-time tradeoffs for near\nneighbor searching on dense data sets, and we apply these techniques to tuple\nlattice sieving to obtain even better time complexities. For instance, our\ntriple sieve heuristically solves SVP in time \\(2^{0.3588d + o(d)}\\). For\npractical sieves based on Micciancio-Voulgaris’ GaussSieve [SODA 2010], this\nshows that a triple sieve uses less space and less time than the current best\nnear-linear space double sieve.</p>\n", "tags": ["ARXIV"] },
{"key": "laarhoven2017graph", "year": "2017", "title":"Graph-based Time-space Trade-offs For Approximate Near Neighbors", "abstract": "<p>We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size \\(n = 2^{o(d)}\\) on\nthe \\(d\\)-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor \\(c &gt; 1\\) in query time \\(n^{\\rho_q + o(1)}\\) and space \\(n^{1 + \\rho_s +\no(1)}\\), for arbitrary \\(\\rho_q, \\rho_s \\geq 0\\) satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small \\(c\\) and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA’17]. We further\nstudy how the trade-offs scale when the data set is of size \\(n =\n2^{\\Theta(d)}\\), and analyze asymptotic complexities when applying these results\nto lattice sieving.</p>\n", "tags": ["Graph","Independent"] },
{"key": "laarhoven2017hypercube", "year": "2017", "title":"Hypercube LSH For Approximate Near Neighbors", "abstract": "<p>A celebrated technique for finding near neighbors for the angular distance\ninvolves using a set of \\textit{random} hyperplanes to partition the space into\nhash regions [Charikar, STOC 2002]. Experiments later showed that using a set\nof \\textit{orthogonal} hyperplanes, thereby partitioning the space into the\nVoronoi regions induced by a hypercube, leads to even better results [Terasawa\nand Tanaka, WADS 2007]. However, no theoretical explanation for this\nimprovement was ever given, and it remained unclear how the resulting hypercube\nhash method scales in high dimensions.\n  In this work, we provide explicit asymptotics for the collision probabilities\nwhen using hypercubes to partition the space. For instance, two near-orthogonal\nvectors are expected to collide with probability \\((\\frac{1}{\\pi})^{d + o(d)}\\)\nin dimension \\(d\\), compared to \\((\\frac{1}{2})^d\\) when using random hyperplanes.\nVectors at angle \\(\\frac{\\pi}{3}\\) collide with probability\n\\((\\frac{\\sqrt{3}}{\\pi})^{d + o(d)}\\), compared to \\((\\frac{2}{3})^d\\) for random\nhyperplanes, and near-parallel vectors collide with similar asymptotic\nprobabilities in both cases.\n  For \\(c\\)-approximate nearest neighbor searching, this translates to a decrease\nin the exponent \\(\\rho\\) of locality-sensitive hashing (LSH) methods of a factor\nup to \\(log_2(\\pi) \\approx 1.652\\) compared to hyperplane LSH. For \\(c = 2\\), we\nobtain \\(\\rho \\approx 0.302 + o(1)\\) for hypercube LSH, improving upon the \\(\\rho\n\\approx 0.377\\) for hyperplane LSH. We further describe how to use hypercube LSH\nin practice, and we consider an example application in the area of lattice\nalgorithms.</p>\n", "tags": ["Independent","LSH"] },
{"key": "laarhoven2019polytopes", "year": "2019", "title":"Polytopes Lattices And Spherical Codes For The Nearest Neighbor Problem", "abstract": "<p>We study locality-sensitive hash methods for the nearest neighbor problem for\nthe angular distance, focusing on the approach of first projecting down onto a\nlow-dimensional subspace, and then partitioning the projected vectors according\nto Voronoi cells induced by a suitable spherical code. This approach\ngeneralizes and interpolates between the fast but suboptimal hyperplane hashing\nof Charikar [STOC’02] and the asymptotically optimal but practically often\nslower hash families of Andoni-Indyk [FOCS’06], Andoni-Indyk-Nguyen-Razenshteyn\n[SODA’14] and Andoni-Indyk-Laarhoven-Razenshteyn-Schmidt [NIPS’15]. We set up a\nframework for analyzing the performance of any spherical code in this context,\nand we provide results for various codes from the literature, such as those\nrelated to regular polytopes and root lattices. Similar to hyperplane hashing,\nand unlike cross-polytope hashing, our analysis of collision probabilities and\nquery exponents is exact and does not hide order terms which vanish only for\nlarge \\(d\\), facilitating an easy parameter selection.\n  For the two-dimensional case, we derive closed-form expressions for arbitrary\nspherical codes, and we show that the equilateral triangle is optimal,\nachieving a better performance than the two-dimensional analogues of hyperplane\nand cross-polytope hashing. In three and four dimensions, we numerically find\nthat the tetrahedron, \\(5\\)-cell, and \\(16\\)-cell achieve the best query exponents,\nwhile in five or more dimensions orthoplices appear to outperform regular\nsimplices, as well as the root lattice families \\(A_k\\) and \\(D_k\\). We argue that\nin higher dimensions, larger spherical codes will likely exist which will\noutperform orthoplices in theory, and we argue why using the \\(D_k\\) root\nlattices will likely lead to better results in practice, due to a better\ntrade-off between the asymptotic query exponent and the concrete costs of\nhashing.</p>\n", "tags": ["AAAI","FOCS"] },
{"key": "labelme2007labelme", "year": "2007", "title":"LabelMe: a database and web-based tool for image annotation", "abstract": "<p>We seek to build a large collection of images with ground truth labels to be used for object\ndetection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation\nand instant sharing of such annotations. Using this annotation tool, we have collected a large\ndataset that spans many object categories, often containing multiple instances over a wide variety\nof images. We quantify the contents of the dataset and compare against existing state of the\nart datasets used for object recognition and detection. Also, we show how to extend the dataset\nto automatically enhance object labels with WordNet, discover object parts, recover a depth ordering\nof objects in a scene, and increase the number of labels using minimal user supervision\nand images from the web.</p>\n", "tags": ["Dataset","Supervised"] },
{"key": "lai2015simultaneous", "year": "2015", "title":"Simultaneous Feature Learning And Hash Coding With Deep Neural Networks", "abstract": "<p>Similarity-preserving hashing is a widely-used method for nearest neighbour\nsearch in large-scale image retrieval tasks. For most existing hashing methods,\nan image is first encoded as a vector of hand-engineering visual features,\nfollowed by another separate projection or quantization step that generates\nbinary codes. However, such visual feature vectors may not be optimally\ncompatible with the coding process, thus producing sub-optimal hashing codes.\nIn this paper, we propose a deep architecture for supervised hashing, in which\nimages are mapped into binary codes via carefully designed deep neural\nnetworks. The pipeline of the proposed deep architecture consists of three\nbuilding blocks: 1) a sub-network with a stack of convolution layers to produce\nthe effective intermediate image features; 2) a divide-and-encode module to\ndivide the intermediate image features into multiple branches, each encoded\ninto one hash bit; and 3) a triplet ranking loss designed to characterize that\none image is more similar to the second image than to the third one. Extensive\nevaluations on several benchmark image datasets show that the proposed\nsimultaneous feature learning and hash coding pipeline brings substantial\nimprovements over other state-of-the-art supervised or unsupervised hashing\nmethods.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "lai2016instance", "year": "2016", "title":"Instance-aware Hashing For Multi-label Image Retrieval", "abstract": "<p>Similarity-preserving hashing is a commonly used method for nearest neighbour\nsearch in large-scale image retrieval. For image retrieval, deep-networks-based\nhashing methods are appealing since they can simultaneously learn effective\nimage representations and compact hash codes. This paper focuses on\ndeep-networks-based hashing for multi-label images, each of which may contain\nobjects of multiple categories. In most existing hashing methods, each image is\nrepresented by one piece of hash code, which is referred to as semantic\nhashing. This setting may be suboptimal for multi-label image retrieval. To\nsolve this problem, we propose a deep architecture that learns\n\\textbf{instance-aware} image representations for multi-label image data, which\nare organized in multiple groups, with each group containing the features for\none category. The instance-aware representations not only bring advantages to\nsemantic hashing, but also can be used in category-aware hashing, in which an\nimage is represented by multiple pieces of hash codes and each piece of code\ncorresponds to a category. Extensive evaluations conducted on several benchmark\ndatasets demonstrate that, for both semantic hashing and category-aware\nhashing, the proposed method shows substantial improvement over the\nstate-of-the-art supervised and unsupervised hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "lai2017improved", "year": "2017", "title":"Improved Search In Hamming Space Using Deep Multi-index Hashing", "abstract": "<p>Similarity-preserving hashing is a widely-used method for nearest neighbour\nsearch in large-scale image retrieval tasks. There has been considerable\nresearch on generating efficient image representation via the\ndeep-network-based hashing methods. However, the issue of efficient searching\nin the deep representation space remains largely unsolved. To this end, we\npropose a simple yet efficient deep-network-based multi-index hashing method\nfor simultaneously learning the powerful image representation and the efficient\nsearching. To achieve these two goals, we introduce the multi-index hashing\n(MIH) mechanism into the proposed deep architecture, which divides the binary\ncodes into multiple substrings. Due to the non-uniformly distributed codes will\nresult in inefficiency searching, we add the two balanced constraints at\nfeature-level and instance-level, respectively. Extensive evaluations on\nseveral benchmark image retrieval datasets show that the learned balanced\nbinary codes bring dramatic speedups and achieve comparable performance over\nthe existing baselines.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "lai2017transductive", "year": "2017", "title":"Transductive Zero-shot Hashing Via Coarse-to-fine Similarity Mining", "abstract": "<p>Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes\nwithout training data, which is an important and challenging problem. Most\nexisting ZSH approaches exploit transfer learning via an intermediate shared\nsemantic representations between the seen/source classes and novel/target\nclasses. However, due to having disjoint, the hash functions learned from the\nsource dataset are biased when applied directly to the target classes. In this\npaper, we study the transductive ZSH, i.e., we have unlabeled data for novel\nclasses. We put forward a simple yet efficient joint learning approach via\ncoarse-to-fine similarity mining which transfers knowledges from source data to\ntarget data. It mainly consists of two building blocks in the proposed deep\narchitecture: 1) a shared two-streams network, which the first stream operates\non the source data and the second stream operates on the unlabeled data, to\nlearn the effective common image representations, and 2) a coarse-to-fine\nmodule, which begins with finding the most representative images from target\nclasses and then further detect similarities among these images, to transfer\nthe similarities of the source data to the target data in a greedy fashion.\nExtensive evaluation results on several benchmark datasets demonstrate that the\nproposed hashing method achieves significant improvement over the\nstate-of-the-art methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "lai2024simultaneous", "year": "2024", "title":"Simultaneous Feature Learning And Hash Coding With Deep Neural Networks", "abstract": "<p>Similarity-preserving hashing is a widely-used method\nfor nearest neighbour search in large-scale image retrieval\ntasks. For most existing hashing methods, an image is\nfirst encoded as a vector of hand-engineering visual features,\nfollowed by another separate projection or quantization\nstep that generates binary codes. However, such visual\nfeature vectors may not be optimally compatible with the\ncoding process, thus producing sub-optimal hashing codes.\nIn this paper, we propose a deep architecture for supervised\nhashing, in which images are mapped into binary codes via\ncarefully designed deep neural networks. The pipeline of\nthe proposed deep architecture consists of three building\nblocks: 1) a sub-network with a stack of convolution layers\nto produce the effective intermediate image features; 2)\na divide-and-encode module to divide the intermediate image\nfeatures into multiple branches, each encoded into one\nhash bit; and 3) a triplet ranking loss designed to characterize\nthat one image is more similar to the second image than\nto the third one. Extensive evaluations on several benchmark\nimage datasets show that the proposed simultaneous\nfeature learning and hash coding pipeline brings substantial\nimprovements over other state-of-the-art supervised or\nunsupervised hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "lam2018quantized", "year": "2018", "title":"Word2bits - Quantized Word Vectors", "abstract": "<p>Word vectors require significant amounts of memory and storage, posing issues\nto resource limited devices like mobile phones and GPUs. We show that high\nquality quantized word vectors using 1-2 bits per parameter can be learned by\nintroducing a quantization function into Word2Vec. We furthermore show that\ntraining with the quantization function acts as a regularizer. We train word\nvectors on English Wikipedia (2017) and evaluate them on standard word\nsimilarity and analogy tasks and on question answering (SQuAD). Our quantized\nword vectors not only take 8-16x less space than full precision (32 bit) word\nvectors but also outperform them on word similarity tasks and question\nanswering.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "lamberger2012memoryless", "year": "2012", "title":"Memoryless Near-collisions Revisited", "abstract": "<p>In this paper we discuss the problem of generically finding near-collisions\nfor cryptographic hash functions in a memoryless way. A common approach is to\ntruncate several output bits of the hash function and to look for collisions of\nthis modified function. In two recent papers, an enhancement to this approach\nwas introduced which is based on classical cycle-finding techniques and\ncovering codes. This paper investigates two aspects of the problem of\nmemoryless near-collisions. Firstly, we give a full treatment of the trade-off\nbetween the number of truncated bits and the success-probability of the\ntruncation based approach. Secondly, we demonstrate the limits of cycle-finding\nmethods for finding near-collisions by showing that, opposed to the collision\ncase, a memoryless variant cannot match the query-complexity of the\n“memory-full” birthday-like near-collision finding method.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "lassance2022composite", "year": "2022", "title":"Composite Code Sparse Autoencoders For First Stage Retrieval", "abstract": "<p>We propose a Composite Code Sparse Autoencoder (CCSA) approach for\nApproximate Nearest Neighbor (ANN) search of document representations based on\nSiamese-BERT models. In Information Retrieval (IR), the ranking pipeline is\ngenerally decomposed in two stages: the first stage focus on retrieving a\ncandidate set from the whole collection. The second stage re-ranks the\ncandidate set by relying on more complex models. Recently, Siamese-BERT models\nhave been used as first stage ranker to replace or complement the traditional\nbag-of-word models. However, indexing and searching a large document collection\nrequire efficient similarity search on dense vectors and this is why ANN\ntechniques come into play. Since composite codes are naturally sparse, we first\nshow how CCSA can learn efficient parallel inverted index thanks to an\nuniformity regularizer. Second, CCSA can be used as a binary quantization\nmethod and we propose to combine it with the recent graph based ANN techniques.\nOur experiments on MSMARCO dataset reveal that CCSA outperforms IVF with\nproduct quantization. Furthermore, CCSA binary quantization is beneficial for\nthe index size, and memory usage for the graph-based HNSW method, while\nmaintaining a good level of recall and MRR. Third, we compare with recent\nsupervised quantization methods for image retrieval and find that CCSA is able\nto outperform them.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Quantisation","Supervised"] },
{"key": "lau2017end", "year": "2017", "title":"End-to-end Network For Twitter Geolocation Prediction And Hashing", "abstract": "<p>We propose an end-to-end neural network to predict the geolocation of a\ntweet. The network takes as input a number of raw Twitter metadata such as the\ntweet message and associated user account information. Our model is language\nindependent, and despite minimal feature engineering, it is interpretable and\ncapable of learning location indicative words and timing patterns. Compared to\nstate-of-the-art systems, our model outperforms them by 2%-6%. Additionally, we\npropose extensions to the model to compress representation learnt by the\nnetwork into binary codes. Experiments show that it produces compact codes\ncompared to benchmark hashing algorithms. An implementation of the model is\nreleased publicly.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "lav2020proximity", "year": "2020", "title":"Proximity Preserving Binary Code Using Signed Graph-cut", "abstract": "<p>We introduce a binary embedding framework, called Proximity Preserving Code\n(PPC), which learns similarity and dissimilarity between data points to create\na compact and affinity-preserving binary code. This code can be used to apply\nfast and memory-efficient approximation to nearest-neighbor searches. Our\nframework is flexible, enabling different proximity definitions between data\npoints. In contrast to previous methods that extract binary codes based on\nunsigned graph partitioning, our system models the attractive and repulsive\nforces in the data by incorporating positive and negative graph weights. The\nproposed framework is shown to boil down to finding the minimal cut of a signed\ngraph, a problem known to be NP-hard. We offer an efficient approximation and\nachieve superior results by constructing the code bit after bit. We show that\nthe proposed approximation is superior to the commonly used spectral methods\nwith respect to both accuracy and complexity. Thus, it is useful for many other\nproblems that can be translated into signed graph cut.</p>\n", "tags": ["Graph"] },
{"key": "le2019btel", "year": "2019", "title":"BTEL A Binary Tree Encoding Approach For Visual Localization", "abstract": "<p>Visual localization algorithms have achieved significant improvements in\nperformance thanks to recent advances in camera technology and vision-based\ntechniques. However, there remains one critical caveat: all current approaches\nthat are based on image retrieval currently scale at best linearly with the\nsize of the environment with respect to both storage, and consequentially in\nmost approaches, query time. This limitation severely curtails the capability\nof autonomous systems in a wide range of compute, power, storage, size, weight\nor cost constrained applications such as drones. In this work, we present a\nnovel binary tree encoding approach for visual localization which can serve as\nan alternative for existing quantization and indexing techniques. The proposed\ntree structure allows us to derive a compressed training scheme that achieves\nsub-linearity in both required storage and inference time. The encoding memory\ncan be easily configured to satisfy different storage constraints. Moreover,\nour approach is amenable to an optional sequence filtering mechanism to further\nimprove the localization results, while maintaining the same amount of storage.\nOur system is entirely agnostic to the front-end descriptors, allowing it to be\nused on top of recent state-of-the-art image representations. Experimental\nresults show that the proposed method significantly outperforms\nstate-of-the-art approaches under limited storage constraints.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "lecroq2023optimal", "year": "2023", "title":"Optimal-hash Exact String Matching Algorithms", "abstract": "<p>String matching is the problem of finding all the occurrences of a pattern in\na text. We propose improved versions of the fast family of string matching\nalgorithms based on hashing \\(q\\)-grams. The improvement consists of considering\nminimal values \\(q\\) such that each \\(q\\)-grams of the pattern has a unique hash\nvalue. The new algorithms are fastest than algorithm of the HASH family for\nshort patterns on large size alphabets.</p>\n", "tags": ["ARXIV"] },
{"key": "lecun2024mnist", "year": "2024", "title":"The MNIST Database Of Handwritten Digits", "abstract": "<p>The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.</p>\n", "tags": ["ARXIV"] },
{"key": "lee2011similarity", "year": "2011", "title":"Similarity Join Size Estimation Using Locality Sensitive Hashing", "abstract": "<p>Similarity joins are important operations with a broad range of applications.\nIn this paper, we study the problem of vector similarity join size estimation\n(VSJ). It is a generalization of the previously studied set similarity join\nsize estimation (SSJ) problem and can handle more interesting cases such as\nTF-IDF vectors. One of the key challenges in similarity join size estimation is\nthat the join size can change dramatically depending on the input similarity\nthreshold.\n  We propose a sampling based algorithm that uses the\nLocality-Sensitive-Hashing (LSH) scheme. The proposed algorithm LSH-SS uses an\nLSH index to enable effective sampling even at high thresholds. We compare the\nproposed technique with random sampling and the state-of-the-art technique for\nSSJ (adapted to VSJ) and demonstrate LSH-SS offers more accurate estimates at\nboth high and low similarity thresholds and small variance using real-world\ndata sets.</p>\n", "tags": ["Independent","LSH"] },
{"key": "lee2020flexor", "year": "2020", "title":"Flexor Trainable Fractional Quantization", "abstract": "<p>Quantization based on the binary codes is gaining attention because each quantized bit can be directly utilized for computations without dequantization using look-up tables. Previous attempts, however, only allow for integer numbers of quantization bits, which ends up restricting the search space for compression ratio and accuracy. In this paper, we propose an encryption algorithm/architecture to compress quantized weights so as to achieve fractional numbers of bits per weight. Decryption during inference is implemented by digital XOR-gate networks added into the neural network model while XOR gates are described by utilizing \\(\\tanh(x)\\) for backward propagation to enable gradient calculations. We perform experiments using MNIST, CIFAR-10, and ImageNet to show that inserting XOR gates learns quantization/encrypted bit decisions through training and obtains high accuracy even for fractional sub 1-bit weights. As a result, our proposed method yields smaller size and higher model accuracy compared to binary neural networks.</p>\n", "tags": ["NEURIPS","Quantisation","Supervised"] },
{"key": "lee2022similarity", "year": "2022", "title":"Set2box Similarity Preserving Representation Learning Of Sets", "abstract": "<p>Sets have been used for modeling various types of objects (e.g., a document\nas the set of keywords in it and a customer as the set of the items that she\nhas purchased). Measuring similarity (e.g., Jaccard Index) between sets has\nbeen a key building block of a wide range of applications, including,\nplagiarism detection, recommendation, and graph compression. However, as sets\nhave grown in numbers and sizes, the computational cost and storage required\nfor set similarity computation have become substantial, and this has led to the\ndevelopment of hashing and sketching based solutions. In this work, we propose\nSet2Box, a learning-based approach for compressed representations of sets from\nwhich various similarity measures can be estimated accurately in constant time.\nThe key idea is to represent sets as boxes to precisely capture overlaps of\nsets. Additionally, based on the proposed box quantization scheme, we design\nSet2Box+, which yields more concise but more accurate box representations of\nsets. Through extensive experiments on 8 real-world datasets, we show that,\ncompared to baseline approaches, Set2Box+ is (a) Accurate: achieving up to\n40.8X smaller estimation error while requiring 60% fewer bits to encode sets,\n(b) Concise: yielding up to 96.8X more concise representations with similar\nestimation error, and (c) Versatile: enabling the estimation of four\nset-similarity measures from a single representation of each set.</p>\n", "tags": ["ARXIV","Graph","Quantisation"] },
{"key": "lehmann2022sichash", "year": "2022", "title":"Sichash -- Small Irregular Cuckoo Tables For Perfect Hashing", "abstract": "<p>A Perfect Hash Function (PHF) is a hash function that has no collisions on a\ngiven input set. PHFs can be used for space efficient storage of data in an\narray, or for determining a compact representative of each object in the set.\nIn this paper, we present the PHF construction algorithm SicHash - Small\nIrregular Cuckoo Tables for Perfect Hashing. At its core, SicHash uses a known\ntechnique: It places objects in a cuckoo hash table and then stores the final\nhash function choice of each object in a retrieval data structure. We combine\nthe idea with irregular cuckoo hashing, where each object has a different\nnumber of hash functions. Additionally, we use many small tables that we\noverload beyond their asymptotic maximum load factor. The most space efficient\ncompetitors often use brute force methods to determine the PHFs. SicHash\nprovides a more direct construction algorithm that only rarely needs to\nrecompute parts. Our implementation improves the state of the art in terms of\nspace usage versus construction time for a wide range of configurations. At the\nsame time, it provides very fast queries.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "lehmann2023shockhash", "year": "2023", "title":"Shockhash Near Optimal-space Minimal Perfect Hashing Beyond Brute-force", "abstract": "<p>A minimal perfect hash function (MPHF) maps a set S of n keys to the first n\nintegers without collisions. There is a lower bound of n*log(e)=1.44n bits\nneeded to represent an MPHF. This can be reached by a brute-force algorithm\nthat tries e^n hash function seeds in expectation and stores the first seed\nleading to an MPHF. The most space-efficient previous algorithms for\nconstructing MPHFs all use such a brute-force approach as a basic building\nblock.\n  In this paper, we introduce ShockHash - Small, heavily overloaded cuckoo hash\ntables for minimal perfect hashing. ShockHash uses two hash functions h_0 and\nh_1, hoping for the existence of a function f : S-&gt;{0, 1} such that x -&gt;\nh_{f(x)}(x) is an MPHF on S. It then uses a 1-bit retrieval data structure to\nstore f using n + o(n) bits.\n  In graph terminology, ShockHash generates n-edge random graphs until\nstumbling on a pseudoforest - where each component contains as many edges as\nnodes. Using cuckoo hashing, ShockHash then derives an MPHF from the\npseudoforest in linear time. We show that ShockHash needs to try only about\n(e/2)^n=1.359^n seeds in expectation. This reduces the space for storing the\nseed by roughly n bits (maintaining the asymptotically optimal space\nconsumption) and speeds up construction by almost a factor of 2^n compared to\nbrute-force. Bipartite ShockHash reduces the expected construction time again\nto 1.166^n by maintaining a pool of candidate hash functions and checking all\npossible pairs.\n  ShockHash as a building block within the RecSplit framework can be\nconstructed up to 3 orders of magnitude faster than competing approaches. It\ncan build an MPHF for 10 million keys with 1.489 bits per key in about half an\nhour. When instead using ShockHash after an efficient k-perfect hash function,\nit achieves space usage similar to the best competitors, while being\nsignificantly faster to construct and query.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "lehmann2023sliding", "year": "2023", "title":"Sliding Block Hashing (slick) -- Basic Algorithmic Ideas", "abstract": "<p>We present {\\bf Sli}ding Blo{\\bf ck} Hashing (Slick), a simple hash table\ndata structure that combines high performance with very good space efficiency.\nThis preliminary report outlines avenues for analysis and implementation that\nwe intend to pursue.</p>\n", "tags": ["ARXIV"] },
{"key": "lei2020locality", "year": "2020", "title":"Locality-sensitive Hashing Scheme Based On Longest Circular Co-substring", "abstract": "<p>Locality-Sensitive Hashing (LSH) is one of the most popular methods for\n\\(c\\)-Approximate Nearest Neighbor Search (\\(c\\)-ANNS) in high-dimensional spaces.\nIn this paper, we propose a novel LSH scheme based on the Longest Circular\nCo-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.\nWe introduce a novel concept of LCCS and a new data structure named Circular\nShift Array (CSA) for \\(k\\)-LCCS search. The insight of LCCS search framework is\nthat close data objects will have a longer LCCS than the far-apart ones with\nhigh probability. LCCS-LSH is <em>LSH-family-independent</em>, and it supports\n\\(c\\)-ANNS with different kinds of distance metrics. We also introduce a\nmulti-probe version of LCCS-LSH and conduct extensive experiments over five\nreal-life datasets. The experimental results demonstrate that LCCS-LSH\noutperforms state-of-the-art LSH schemes.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "lemire2006one", "year": "2006", "title":"One-pass One-hash N-gram Statistics Estimation", "abstract": "<p>In multimedia, text or bioinformatics databases, applications query sequences\nof n consecutive symbols called n-grams. Estimating the number of distinct\nn-grams is a view-size estimation problem. While view sizes can be estimated by\nsampling under statistical assumptions, we desire an unassuming algorithm with\nuniversally valid accuracy bounds. Most related work has focused on repeatedly\nhashing the data, which is prohibitive for large data sources. We prove that a\none-pass one-hash algorithm is sufficient for accurate estimates if the hashing\nis sufficiently independent. To reduce costs further, we investigate recursive\nrandom hashing algorithms and show that they are sufficiently independent in\npractice. We compare our running times with exact counts using suffix arrays\nand show that, while we use hardly any storage, we are an order of magnitude\nfaster. The approach further is extended to a one-pass/one-hash computation of\nn-gram entropy and iceberg counts. The experiments use a large collection of\nEnglish text from the Gutenberg Project as well as synthetic data.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "lemire2007recursive", "year": "2007", "title":"Recursive N-gram Hashing Is Pairwise Independent At Best", "abstract": "<p>Many applications use sequences of n consecutive symbols (n-grams). Hashing\nthese n-grams can be a performance bottleneck. For more speed, recursive hash\nfamilies compute hash values by updating previous values. We prove that\nrecursive hash families cannot be more than pairwise independent. While hashing\nby irreducible polynomials is pairwise independent, our implementations either\nrun in time O(n) or use an exponential amount of memory. As a more scalable\nalternative, we make hashing by cyclic polynomials pairwise independent by\nignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials\nis is twice as fast as hashing by irreducible polynomials. We also show that\nrandomized Karp-Rabin hash families are not pairwise independent.</p>\n", "tags": ["Independent"] },
{"key": "lemire2010universality", "year": "2010", "title":"The Universality Of Iterated Hashing Over Variable-length Strings", "abstract": "<p>Iterated hash functions process strings recursively, one character at a time.\nAt each iteration, they compute a new hash value from the preceding hash value\nand the next character. We prove that iterated hashing can be pairwise\nindependent, but never 3-wise independent. We show that it can be almost\nuniversal over strings much longer than the number of hash values; we bound the\nmaximal string length given the collision probability.</p>\n", "tags": ["Independent"] },
{"key": "lemire2015faster", "year": "2015", "title":"Faster 64-bit Universal Hashing Using Carry-less Multiplications", "abstract": "<p>Intel and AMD support the Carry-less Multiplication (CLMUL) instruction set\nin their x64 processors. We use CLMUL to implement an almost universal 64-bit\nhash family (CLHASH). We compare this new family with what might be the fastest\nalmost universal family on x64 processors (VHASH). We find that CLHASH is at\nleast 60% faster. We also compare CLHASH with a popular hash function designed\nfor speed (Google’s CityHash). We find that CLHASH is 40% faster than CityHash\non inputs larger than 64 bytes and just as fast otherwise.</p>\n", "tags": ["Independent"] },
{"key": "leng2024hashing", "year": "2024", "title":"Hashing For Distributed Data", "abstract": "<p>Recently, hashing based approximate nearest\nneighbors search has attracted much attention.\nExtensive centralized hashing algorithms have\nbeen proposed and achieved promising performance. However, due to the large scale of many\napplications, the data is often stored or even collected in a distributed manner. Learning hash\nfunctions by aggregating all the data into a fusion\ncenter is infeasible because of the prohibitively\nexpensive communication and computation overhead.\nIn this paper, we develop a novel hashing\nmodel to learn hash functions in a distributed setting. We cast a centralized hashing model as a\nset of subproblems with consensus constraints.\nWe find these subproblems can be analytically\nsolved in parallel on the distributed compute nodes. Since no training data is transmitted across\nthe nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large\nscale datasets containing up to 100 million samples demonstrate the efficacy of our method.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "lepage2023lrvs", "year": "2023", "title":"Lrvs-fashion Extending Visual Search With Referring Instructions", "abstract": "<p>This paper introduces a new challenge for image similarity search in the\ncontext of fashion, addressing the inherent ambiguity in this domain stemming\nfrom complex images. We present Referred Visual Search (RVS), a task allowing\nusers to define more precisely the desired similarity, following recent\ninterest in the industry. We release a new large public dataset, LRVS-Fashion,\nconsisting of 272k fashion products with 842k images extracted from fashion\ncatalogs, designed explicitly for this task. However, unlike traditional visual\nsearch methods in the industry, we demonstrate that superior performance can be\nachieved by bypassing explicit object detection and adopting weakly-supervised\nconditional contrastive learning on image tuples. Our method is lightweight and\ndemonstrates robustness, reaching Recall at one superior to strong\ndetection-based baselines against 2M distractors. The dataset is available at\nhttps://huggingface.co/datasets/Slep/LAION-RVS-Fashion .</p>\n", "tags": ["ARXIV","Supervised","Weakly Supervised"] },
{"key": "lessley2018data", "year": "2018", "title":"Data-parallel Hashing Techniques For GPU Architectures", "abstract": "<p>Hash tables are one of the most fundamental data structures for effectively\nstoring and accessing sparse data, with widespread usage in domains ranging\nfrom computer graphics to machine learning. This study surveys the\nstate-of-the-art research on data-parallel hashing techniques for emerging\nmassively-parallel, many-core GPU architectures. Key factors affecting the\nperformance of different hashing schemes are discovered and used to suggest\nbest practices and pinpoint areas for further research.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "leu2023fast", "year": "2023", "title":"Fast Consistent Hashing In Constant Time", "abstract": "<p>Consistent hashing is a technique that can minimize key remapping when the\nnumber of hash buckets changes. The paper proposes a fast consistent hash\nalgorithm (called power consistent hash) that has \\(O(1)\\) expected time for key\nlookup, independent of the number of buckets. Hash values are computed in real\ntime. No search data structure is constructed to store bucket ranges or key\nmappings. The algorithm has a lightweight design using \\(O(1)\\) space with\nsuperior scalability. In particular, it uses two auxiliary hash functions to\nachieve distribution uniformity and \\(O(1)\\) expected time for key lookup.\nFurthermore, it performs consistent hashing such that only a minimal number of\nkeys are remapped when the number of buckets changes. Consistent hashing has a\nwide range of use cases, including load balancing, distributed caching, and\ndistributed key-value stores. The proposed algorithm is faster than well-known\nconsistent hash algorithms with \\(O(log n)\\) lookup time.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "leybovich2021efficient", "year": "2021", "title":"Efficient Approximate Search For Sets Of Vectors", "abstract": "<p>We consider a similarity measure between two sets \\(A\\) and \\(B\\) of vectors,\nthat balances the average and maximum cosine distance between pairs of vectors,\none from set \\(A\\) and one from set \\(B\\). As a motivation for this measure, we\npresent lineage tracking in a database. To practically realize this measure, we\nneed an approximate search algorithm that given a set of vectors \\(A\\) and sets\nof vectors \\(B_1,…,B_n\\), the algorithm quickly locates the set \\(B_i\\) that\nmaximizes the similarity measure. For the case where all sets are singleton\nsets, essentially each is a single vector, there are known efficient\napproximate search algorithms, e.g., approximated versions of tree search\nalgorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and\nproximity graph algorithms. In this work, we present approximate search\nalgorithms for the general case. The underlying idea in these algorithms is\nencoding a set of vectors via a “long” single vector. The proposed approximate\napproach achieves significant performance gains over an optimized, exact search\non vector sets.</p>\n", "tags": ["ARXIV","Graph","Independent","LSH","Quantisation"] },
{"key": "li2006cascade", "year": "2006", "title":"Cascade Hash Tables A Series Of Multilevel Double Hashing Schemes With O(1) Worst Case Lookup Time", "abstract": "<p>In this paper, the author proposes a series of multilevel double hashing\nschemes called cascade hash tables. They use several levels of hash tables. In\neach table, we use the common double hashing scheme. Higher level hash tables\nwork as fail-safes of lower level hash tables. By this strategy, it could\neffectively reduce collisions in hash insertion. Thus it gains a constant worst\ncase lookup time with a relatively high load factor(70%-85%) in random\nexperiments. Different parameters of cascade hash tables are tested.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "li2009b", "year": "2009", "title":"B-bit Minwise Hashing", "abstract": "<p>This paper establishes the theoretical framework of b-bit minwise hashing.\nThe original minwise hashing method has become a standard technique for\nestimating set similarity (e.g., resemblance) with applications in information\nretrieval, data management, social networks and computational advertising.\n  By only storing the lowest \\(b\\) bits of each (minwise) hashed value (e.g., b=1\nor 2), one can gain substantial advantages in terms of computational efficiency\nand storage space. We prove the basic theoretical results and provide an\nunbiased estimator of the resemblance for any b. We demonstrate that, even in\nthe least favorable scenario, using b=1 may reduce the storage space at least\nby a factor of 21.3 (or 10.7) compared to using b=64 (or b=32), if one is\ninterested in resemblance &gt; 0.5.</p>\n", "tags": ["ARXIV"] },
{"key": "li2010b", "year": "2010", "title":"B-bit Minwise Hashing For Estimating Three-way Similarities", "abstract": "<p>Computing two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b&gt;= 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that \\(b\\)-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.</p>\n", "tags": ["NEURIPS"] },
{"key": "li2011accurate", "year": "2011", "title":"Accurate Estimators For Improving Minwise Hashing And B-bit Minwise Hashing", "abstract": "<p>Minwise hashing is the standard technique in the context of search and\ndatabases for efficiently estimating set (e.g., high-dimensional 0/1 vector)\nsimilarities. Recently, b-bit minwise hashing was proposed which significantly\nimproves upon the original minwise hashing in practice by storing only the\nlowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing\nis particularly effective in applications which mainly concern sets of high\nsimilarities (e.g., the resemblance &gt;0.5). However, there are other important\napplications in which not just pairs of high similarities matter. For example,\nmany learning algorithms require all pairwise similarities and it is expected\nthat only a small fraction of the pairs are similar. Furthermore, many\napplications care more about containment (e.g., how much one object is\ncontained by another object) than the resemblance. In this paper, we show that\nthe estimators for minwise hashing and b-bit minwise hashing used in the\ncurrent practice can be systematically improved and the improvements are most\nsignificant for set pairs of low resemblance and high containment.</p>\n", "tags": ["ARXIV"] },
{"key": "li2011b", "year": "2011", "title":"B-bit Minwise Hashing For Large-scale Linear SVM", "abstract": "<p>In this paper, we propose to (seamlessly) integrate b-bit minwise hashing\nwith linear SVM to substantially improve the training (and testing) efficiency\nusing much smaller memory, with essentially no loss of accuracy. Theoretically,\nwe prove that the resemblance matrix, the minwise hashing matrix, and the b-bit\nminwise hashing matrix are all positive definite matrices (kernels).\nInterestingly, our proof for the positive definiteness of the b-bit minwise\nhashing kernel naturally suggests a simple strategy to integrate b-bit hashing\nwith linear SVM. Our technique is particularly useful when the data can not fit\nin memory, which is an increasingly critical issue in large-scale machine\nlearning. Our preliminary experimental results on a publicly available webspam\ndataset (350K samples and 16 million dimensions) verified the effectiveness of\nour algorithm. For example, the training time was reduced to merely a few\nseconds. In addition, our technique can be easily extended to many other linear\nand nonlinear machine learning applications such as logistic regression.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2011hashing", "year": "2011", "title":"Hashing Algorithms For Large-scale Learning", "abstract": "<p>Minwise hashing is a standard technique in the context of search for efficiently computing set similarities. The recent development of b-bit minwise hashing provides a  substantial improvement by storing only the lowest b bits of each hashed value. In this paper, we demonstrate that  b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression, to solve large-scale and high-dimensional statistical learning tasks, especially when the data do not fit in memory.   We  compare \\(b\\)-bit minwise hashing with  the Count-Min (CM)  and  Vowpal Wabbit (VW) algorithms, which have essentially the same variances as random projections. Our theoretical and empirical comparisons illustrate that b-bit minwise hashing is significantly more accurate (at the same storage cost) than VW (and random projections) for binary data.</p>\n", "tags": ["NEURIPS","Supervised"] },
{"key": "li2011learning", "year": "2011", "title":"Learning To Search Efficiently In High Dimensions", "abstract": "<p>High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efficiency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efficiency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search. Our approach takes both search quality and computational cost into consideration. Specifically, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efficiently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efficiency. Experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).</p>\n", "tags": ["LSH","NEURIPS","Supervised"] },
{"key": "li2011training", "year": "2011", "title":"Training Logistic Regression And SVM On 200GB Data Using B-bit Minwise Hashing And Comparisons With Vowpal Wabbit (VW)", "abstract": "<p>We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit\nminwise hashing algorithms for training very large-scale logistic regression\nand SVM. The results confirm our prior work that, compared with the VW hashing\nalgorithm (which has the same variance as random projections), b-bit minwise\nhashing is substantially more accurate at the same storage. For example, with\nmerely 30 hashed values per data point, b-bit minwise hashing can achieve\nsimilar accuracies as VW with 2^14 hashed values per data point.\n  We demonstrate that the preprocessing cost of b-bit minwise hashing is\nroughly on the same order of magnitude as the data loading time. Furthermore,\nby using a GPU, the preprocessing cost can be reduced to a small fraction of\nthe data loading time.\n  Minwise hashing has been widely used in industry, at least in the context of\nsearch. One reason for its popularity is that one can efficiently simulate\npermutations by (e.g.,) universal hashing. In other words, there is no need to\nstore the permutation matrix. In this paper, we empirically verify this\npractice, by demonstrating that even using the simplest 2-universal hashing\ndoes not degrade the learning performance.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2012b", "year": "2012", "title":"B-bit Minwise Hashing In Practice Large-scale Batch And Online Learning And Using Gpus For Fast Preprocessing With Simple Hash Functions", "abstract": "<p>In this paper, we study several critical issues which must be tackled before\none can apply b-bit minwise hashing to the volumes of data often used\nindustrial applications, especially in the context of search.</p>\n<ol>\n  <li>(b-bit) Minwise hashing requires an expensive preprocessing step that\ncomputes k (e.g., 500) minimal values after applying the corresponding\npermutations for each data vector. We developed a parallelization scheme using\nGPUs and observed that the preprocessing time can be reduced by a factor of\n20-80 and becomes substantially smaller than the data loading time.</li>\n  <li>One major advantage of b-bit minwise hashing is that it can substantially\nreduce the amount of memory required for batch learning. However, as online\nalgorithms become increasingly popular for large-scale learning in the context\nof search, it is not clear if b-bit minwise yields significant improvements for\nthem. This paper demonstrates that \\(b\\)-bit minwise hashing provides an\neffective data size/dimension reduction scheme and hence it can dramatically\nreduce the data loading time for each epoch of the online training process.\nThis is significant because online learning often requires many (e.g., 10 to\n100) epochs to reach a sufficient accuracy.</li>\n  <li>Another critical issue is that for very large data sets it becomes\nimpossible to store a (fully) random permutation matrix, due to its space\nrequirements. Our paper is the first study to demonstrate that \\(b\\)-bit minwise\nhashing implemented using simple hash functions, e.g., the 2-universal (2U) and\n4-universal (4U) hash families, can produce very similar learning results as\nusing fully random permutations. Experiments on datasets of up to 200GB are\npresented.</li>\n</ol>\n", "tags": ["ARXIV","Independent"] },
{"key": "li2012one", "year": "2012", "title":"One Permutation Hashing", "abstract": "<p>While minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) \\(k=500\\) permutations on the data. The testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.  In this paper, we develop a simple \\textbf{one permutation hashing} scheme to address this important issue. While it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation. Also, reducing \\(k\\) permutations to just one  would be much more \\textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry. While the theoretical probability analysis is  interesting, our experiments on similarity estimation and   SVM \\&amp; logistic regression also confirm the theoretical results.</p>\n", "tags": ["NEURIPS","Supervised"] },
{"key": "li2013coding", "year": "2013", "title":"Coding For Random Projections", "abstract": "<p>The method of random projections has become very popular for large-scale\napplications in statistical learning, information retrieval, bio-informatics\nand other applications. Using a well-designed coding scheme for the projected\ndata, which determines the number of bits needed for each projected value and\nhow to allocate these bits, can significantly improve the effectiveness of the\nalgorithm, in storage cost as well as computational speed. In this paper, we\nstudy a number of simple coding schemes, focusing on the task of similarity\nestimation and on an application to training linear classifiers. We demonstrate\nthat uniform quantization outperforms the standard existing influential method\n(Datar et. al. 2004). Indeed, we argue that in many cases coding with just a\nsmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bit\ncoding scheme that generally performs well in practice, as confirmed by our\nexperiments on training linear support vector machines (SVM).</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "li2013learning", "year": "2013", "title":"Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming an increasingly important tool in\nsolving many large-scale problems. Recently a number of approaches to learning\ndata-dependent hash functions have been developed. In this work, we propose a\ncolumn generation based method for learning data-dependent hash functions on\nthe basis of proximity comparison information. Given a set of triplets that\nencode the pairwise proximity comparison information, our method learns hash\nfunctions that preserve the relative comparison relationships in the data as\nwell as possible within the large-margin learning framework. The learning\nprocedure is implemented using column generation and hence is named CGHash. At\neach iteration of the column generation procedure, the best hash function is\nselected. Unlike most other hashing methods, our method generalizes to new data\npoints naturally; and has a training objective which is convex, thus ensuring\nthat the global optimum can be identified. Experiments demonstrate that the\nproposed method learns compact binary codes and that its retrieval performance\ncompares favorably with state-of-the-art methods when tested on a few benchmark\ndatasets.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "li2014coding", "year": "2014", "title":"Coding For Random Projections And Approximate Near Neighbor Search", "abstract": "<p>This technical note compares two coding (quantization) schemes for random\nprojections in the context of sub-linear time approximate near neighbor search.\nThe first scheme is based on uniform quantization while the second scheme\nutilizes a uniform quantization plus a uniformly random offset (which has been\npopular in practice). The prior work compared the two schemes in the context of\nsimilarity estimation and training linear classifiers, with the conclusion that\nthe step of random offset is not necessary and may hurt the performance\n(depending on the similarity level). The task of near neighbor search is\nrelated to similarity estimation with importance distinctions and requires own\nstudy. In this paper, we demonstrate that in the context of near neighbor\nsearch, the step of random offset is not needed either and may hurt the\nperformance (sometimes significantly so, depending on the similarity and other\nparameters).</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "li2014core", "year": "2014", "title":"Core Kernels", "abstract": "<p>The term “CoRE kernel” stands for correlation-resemblance kernel. In many\napplications (e.g., vision), the data are often high-dimensional, sparse, and\nnon-binary. We propose two types of (nonlinear) CoRE kernels for non-binary\nsparse data and demonstrate the effectiveness of the new kernels through a\nclassification experiment. CoRE kernels are simple with no tuning parameters.\nHowever, training nonlinear kernel SVM can be (very) costly in time and memory\nand may not be suitable for truly large-scale industrial applications (e.g.\nsearch). In order to make the proposed CoRE kernels more practical, we develop\nbasic probabilistic hashing algorithms which transform nonlinear kernels into\nlinear kernels.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2015binary", "year": "2015", "title":"Binary Speaker Embedding", "abstract": "<p>The popular i-vector model represents speakers as low-dimensional continuous\nvectors (i-vectors), and hence it is a way of continuous speaker embedding. In\nthis paper, we investigate binary speaker embedding, which transforms i-vectors\nto binary vectors (codes) by a hash function. We start from locality sensitive\nhashing (LSH), a simple binarization approach where binary codes are derived\nfrom a set of random hash functions. A potential problem of LSH is that the\nrandomly sampled hash functions might be suboptimal. We therefore propose an\nimproved Hamming distance learning approach, where the hash function is learned\nby a variable-sized block training that projects each dimension of the original\ni-vectors to variable-sized binary codes independently. Our experiments show\nthat binary speaker embedding can deliver competitive or even better results on\nboth speaker verification and identification tasks, while the memory usage and\nthe computation cost are significantly reduced.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "li2015fast", "year": "2015", "title":"Fast K-nearest Neighbour Search Via Dynamic Continuous Indexing", "abstract": "<p>Existing methods for retrieving k-nearest neighbours suffer from the curse of\ndimensionality. We argue this is caused in part by inherent deficiencies of\nspace partitioning, which is the underlying strategy used by most existing\nmethods. We devise a new strategy that avoids partitioning the vector space and\npresent a novel randomized algorithm that runs in time linear in dimensionality\nof the space and sub-linear in the intrinsic dimensionality and the size of the\ndataset and takes space constant in dimensionality of the space and linear in\nthe size of the dataset. The proposed algorithm allows fine-grained control\nover accuracy and speed on a per-query basis, automatically adapts to\nvariations in data density, supports dynamic updates to the dataset and is\neasy-to-implement. We show appealing theoretical properties and demonstrate\nempirically that the proposed algorithm outperforms locality-sensitivity\nhashing (LSH) in terms of approximation quality, speed and space efficiency.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "li2015feature", "year": "2015", "title":"Feature Learning Based Deep Supervised Hashing With Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of hashing for large-scale image\nretrieval. However, most existing hashing methods are based on hand-crafted\nfeatures which might not be optimally compatible with the hashing procedure.\nRecently, deep hashing methods have been proposed to perform simultaneous\nfeature learning and hash-code learning with deep neural networks, which have\nshown better performance than traditional hashing methods with hand-crafted\nfeatures. Most of these deep hashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application\nscenario with pairwise labels, there have not existed methods for simultaneous\nfeature learning and hash-code learning. In this paper, we propose a novel deep\nhashing method, called deep pairwise-supervised hashing(DPSH), to perform\nsimultaneous feature learning and hash-code learning for applications with\npairwise labels. Experiments on real datasets show that our DPSH method can\noutperform other methods to achieve the state-of-the-art performance in image\nretrieval applications.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "li2015min", "year": "2015", "title":"Min-max Kernels", "abstract": "<p>The min-max kernel is a generalization of the popular resemblance kernel\n(which is designed for binary data). In this paper, we demonstrate, through an\nextensive classification study using kernel machines, that the min-max kernel\noften provides an effective measure of similarity for nonnegative data. As the\nmin-max kernel is nonlinear and might be difficult to be used for industrial\napplications with massive data, we show that the min-max kernel can be\nlinearized via hashing techniques. This allows practitioners to apply min-max\nkernel to large-scale applications using well matured linear algorithms such as\nlinear SVM or logistic regression.\n  The previous remarkable work on consistent weighted sampling (CWS) produces\nsamples in the form of (\\(i^<em>, t^</em>\\)) where the \\(i^<em>\\) records the location (and\nin fact also the weights) information analogous to the samples produced by\nclassical minwise hashing on binary data. Because the \\(t^</em>\\) is theoretically\nunbounded, it was not immediately clear how to effectively implement CWS for\nbuilding large-scale linear classifiers. In this paper, we provide a simple\nsolution by discarding \\(t^*\\) (which we refer to as the “0-bit” scheme). Via an\nextensive empirical study, we show that this 0-bit scheme does not lose\nessential information. We then apply the “0-bit” CWS for building linear\nclassifiers to approximate min-max kernel classifiers, as extensively validated\non a wide range of publicly available classification datasets. We expect this\nwork will generate interests among data mining practitioners who would like to\nefficiently utilize the nonlinear information of non-binary and nonnegative\ndata.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2015rank", "year": "2015", "title":"Rank Subspace Learning For Compact Hash Codes", "abstract": "<p>The era of Big Data has spawned unprecedented interests in developing hashing\nalgorithms for efficient storage and fast nearest neighbor search. Most\nexisting work learn hash functions that are numeric quantizations of feature\nvalues in projected feature space. In this work, we propose a novel hash\nlearning framework that encodes feature’s rank orders instead of numeric values\nin a number of optimal low-dimensional ranking subspaces. We formulate the\nranking subspace learning problem as the optimization of a piece-wise linear\nconvex-concave function and present two versions of our algorithm: one with\nindependent optimization of each hash bit and the other exploiting a sequential\nlearning framework. Our work is a generalization of the Winner-Take-All (WTA)\nhash family and naturally enjoys all the numeric stability benefits of rank\ncorrelation measures while being optimized to achieve high precision at very\nshort code length. We compare with several state-of-the-art hashing algorithms\nin both supervised and unsupervised domain, showing superior performance in a\nnumber of data sets.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "li2016generalized", "year": "2016", "title":"Generalized Intersection Kernel", "abstract": "<p>Following the very recent line of work on the <code class=\"language-plaintext highlighter-rouge\">generalized min-max'' (GMM)\nkernel, this study proposes the</code>generalized intersection’’ (GInt) kernel and\nthe related <code class=\"language-plaintext highlighter-rouge\">normalized generalized min-max'' (NGMM) kernel. In computer\nvision, the (histogram) intersection kernel has been popular, and the GInt\nkernel generalizes it to data which can have both negative and positive\nentries. Through an extensive empirical classification study on 40 datasets\nfrom the UCI repository, we are able to show that this (tuning-free) GInt\nkernel performs fairly well.\n  The empirical results also demonstrate that the NGMM kernel typically\noutperforms the GInt kernel. Interestingly, the NGMM kernel has another\ninterpretation --- it is the</code>asymmetrically transformed’’ version of the GInt\nkernel, based on the idea of ``asymmetric hashing’’. Just like the GMM kernel,\nthe NGMM kernel can be efficiently linearized through (e.g.,) generalized\nconsistent weighted sampling (GCWS), as empirically validated in our study.\nOwing to the discrete nature of hashed values, it also provides a scheme for\napproximate near neighbor search.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2016quantized", "year": "2016", "title":"Quantized Random Projections And Non-linear Estimation Of Cosine Similarity", "abstract": "<p>Random projections constitute a simple, yet effective technique for dimensionality reduction with applications in learning and search problems. In the present paper, we consider the problem of estimating cosine similarities when the projected data undergo scalar quantization to \\(b\\) bits. We here argue that the maximum likelihood estimator (MLE) is a principled approach to deal with the non-linearity resulting from quantization, and subsequently study its computational and statistical properties. A specific focus is on the on the trade-off between bit depth and the number of projections given a fixed budget of bits for storage or transmission. Along the way, we also touch upon the existence of a qualitative counterpart to the Johnson-Lindenstrauss lemma in the presence of quantization.</p>\n", "tags": ["Independent","NEURIPS","Quantisation"] },
{"key": "li2016random", "year": "2016", "title":"2-bit Random Projections Nonlinear Estimators And Approximate Near Neighbor Search", "abstract": "<p>The method of random projections has become a standard tool for machine\nlearning, data mining, and search with massive data at Web scale. The effective\nuse of random projections requires efficient coding schemes for quantizing\n(real-valued) projected data into integers. In this paper, we focus on a simple\n2-bit coding scheme. In particular, we develop accurate nonlinear estimators of\ndata similarity based on the 2-bit strategy. This work will have important\npractical applications. For example, in the task of near neighbor search, a\ncrucial step (often called re-ranking) is to compute or estimate data\nsimilarities once a set of candidate data points have been identified by hash\ntable techniques. This re-ranking step can take advantage of the proposed\ncoding scheme and estimator.\n  As a related task, in this paper, we also study a simple uniform quantization\nscheme for the purpose of building hash tables with projected data. Our\nanalysis shows that typically only a small number of bits are needed. For\nexample, when the target similarity level is high, 2 or 3 bits might be\nsufficient. When the target similarity level is not so high, it is preferable\nto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good\nchoice for the task of sublinear time approximate near neighbor search via hash\ntables.\n  Combining these results, we conclude that 2-bit random projections should be\nrecommended for approximate near neighbor search and similarity estimation.\nExtensive experimental results are provided.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "li2016theory", "year": "2016", "title":"Theory Of The GMM Kernel", "abstract": "<p>We develop some theoretical results for a robust similarity measure named\n“generalized min-max” (GMM). This similarity has direct applications in machine\nlearning as a positive definite kernel and can be efficiently computed via\nprobabilistic hashing. Owing to the discrete nature, the hashed values can also\nbe used for efficient near neighbor search. We prove the theoretical limit of\nGMM and the consistency result, assuming that the data follow an elliptical\ndistribution, which is a very general family of distributions and includes the\nmultivariate \\(t\\)-distribution as a special case. The consistency result holds\nas long as the data have bounded first moment (an assumption which essentially\nholds for datasets commonly encountered in practice). Furthermore, we establish\nthe asymptotic normality of GMM. Compared to the “cosine” similarity which is\nroutinely adopted in current practice in statistics and machine learning, the\nconsistency of GMM requires much weaker conditions. Interestingly, when the\ndata follow the \\(t\\)-distribution with \\(\\nu\\) degrees of freedom, GMM typically\nprovides a better measure of similarity than “cosine” roughly when \\(\\nu&lt;8\\)\n(which is already very close to normal). These theoretical results will help\nexplain the recent success of GMM in learning tasks.</p>\n", "tags": ["ARXIV"] },
{"key": "li2017deep", "year": "2017", "title":"Deep Supervised Discrete Hashing", "abstract": "<p>With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.</p>\n", "tags": ["Deep Learning","Image Retrieval","NEURIPS","Supervised"] },
{"key": "li2017fast", "year": "2017", "title":"Fast K-nearest Neighbour Search Via Prioritized DCI", "abstract": "<p>Most exact methods for k-nearest neighbour search suffer from the curse of\ndimensionality; that is, their query times exhibit exponential dependence on\neither the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing\n(DCI) offers a promising way of circumventing the curse and successfully\nreduces the dependence of query time on intrinsic dimensionality from\nexponential to sublinear. In this paper, we propose a variant of DCI, which we\ncall Prioritized DCI, and show a remarkable improvement in the dependence of\nquery time on intrinsic dimensionality. In particular, a linear increase in\nintrinsic dimensionality, or equivalently, an exponential increase in the\nnumber of points near a query, can be mostly counteracted with just a linear\nincrease in space. We also demonstrate empirically that Prioritized DCI\nsignificantly outperforms prior methods. In particular, relative to\nLocality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of\ndistance evaluations by a factor of 14 to 116 and the memory consumption by a\nfactor of 21.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "li2017simple", "year": "2017", "title":"Simple Strategies For Recovering Inner Products From Coarsely Quantized Random Projections", "abstract": "<p>Random projections have been increasingly adopted for a diverse set of tasks in machine learning involving dimensionality reduction. One specific line of research on this topic has investigated the use of quantization subsequent to projection with the aim of additional data compression. Motivated by applications in nearest neighbor search and linear learning, we revisit the problem of recovering inner products (respectively cosine similarities) in such setting. We show that even under coarse scalar quantization with 3 to 5 bits per projection, the loss in accuracy tends to range from negligible’’ tomoderate’’. One implication is that in most scenarios of practical interest, there is no need for a sophisticated recovery approach like maximum likelihood estimation as considered in previous work on the subject. What we propose herein also yields considerable improvements in terms of accuracy over the Hamming distance-based approach in Li et al. (ICML 2014) which is comparable in terms of simplicity</p>\n", "tags": ["ICML","Independent","NEURIPS","Quantisation"] },
{"key": "li2017song", "year": "2017", "title":"Image2song Song Retrieval Via Bridging Image Content And Lyric Words", "abstract": "<p>Image is usually taken for expressing some kinds of emotions or purposes,\nsuch as love, celebrating Christmas. There is another better way that combines\nthe image and relevant song to amplify the expression, which has drawn much\nattention in the social network recently. Hence, the automatic selection of\nsongs should be expected. In this paper, we propose to retrieve semantic\nrelevant songs just by an image query, which is named as the image2song\nproblem. Motivated by the requirements of establishing correlation in\nsemantic/content, we build a semantic-based song retrieval framework, which\nlearns the correlation between image content and lyric words. This model uses a\nconvolutional neural network to generate rich tags from image regions, a\nrecurrent neural network to model lyric, and then establishes correlation via a\nmulti-layer perceptron. To reduce the content gap between image and lyric, we\npropose to make the lyric modeling focus on the main image content via a tag\nattention. We collect a dataset from the social-sharing multimodal data to\nstudy the proposed problem, which consists of (image, music clip, lyric)\ntriplets. We demonstrate that our proposed model shows noticeable results in\nthe image2song retrieval task and provides suitable songs. Besides, the\nsong2image task is also performed.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "li2018dual", "year": "2018", "title":"Dual Asymmetric Deep Hashing Learning", "abstract": "<p>Due to the impressive learning power, deep learning has achieved a remarkable\nperformance in supervised hash function learning. In this paper, we propose a\nnovel asymmetric supervised deep hashing method to preserve the semantic\nstructure among different categories and generate the binary codes\nsimultaneously. Specifically, two asymmetric deep networks are constructed to\nreveal the similarity between each pair of images according to their semantic\nlabels. The deep hash functions are then learned through two networks by\nminimizing the gap between the learned features and discrete codes.\nFurthermore, since the binary codes in the Hamming space also should keep the\nsemantic affinity existing in the original space, another asymmetric pairwise\nloss is introduced to capture the similarity between the binary codes and\nreal-value features. This asymmetric loss not only improves the retrieval\nperformance, but also contributes to a quick convergence at the training phase.\nBy taking advantage of the two-stream deep structures and two types of\nasymmetric pairwise functions, an alternating algorithm is designed to optimize\nthe deep features and high-quality binary codes efficiently. Experimental\nresults on three real-world datasets substantiate the effectiveness and\nsuperiority of our approach as compared with state-of-the-art.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "li2018fast", "year": "2018", "title":"Fast Similarity Search Via Optimal Sparse Lifting", "abstract": "<p>Similarity search is a fundamental problem in computing science with various applications and has attracted significant research attention, especially in large-scale search with high dimensions. Motivated by the evidence in biological science, our work develops a novel approach for similarity search. Fundamentally different from existing methods that typically reduce the dimension of the data to lessen the computational complexity and speed up the search, our approach projects the data into an even higher-dimensional space while ensuring the sparsity of the data in the output space, with the objective of further improving precision and speed. Specifically, our approach has two key steps. Firstly, it computes the optimal sparse lifting for given input samples and increases the dimension of the data while approximately preserving their pairwise similarity. Secondly, it seeks the optimal lifting operator that best maps input samples to the optimal sparse lifting. Computationally, both steps are modeled as optimization problems that can be efficiently and effectively solved by the Frank-Wolfe algorithm. Simple as it is, our approach has reported significantly improved results in empirical evaluations, and exhibited its high potentials in solving practical problems.</p>\n", "tags": ["NEURIPS"] },
{"key": "li2018hashtran", "year": "2018", "title":"Hashtran-dnn A Framework For Enhancing Robustness Of Deep Neural Networks Against Adversarial Malware Samples", "abstract": "<p>Adversarial machine learning in the context of image processing and related\napplications has received a large amount of attention. However, adversarial\nmachine learning, especially adversarial deep learning, in the context of\nmalware detection has received much less attention despite its apparent\nimportance. In this paper, we present a framework for enhancing the robustness\nof Deep Neural Networks (DNNs) against adversarial malware samples, dubbed\nHashing Transformation Deep Neural Networks} (HashTran-DNN). The core idea is\nto use hash functions with a certain locality-preserving property to transform\nsamples to enhance the robustness of DNNs in malware classification. The\nframework further uses a Denoising Auto-Encoder (DAE) regularizer to\nreconstruct the hash representations of samples, making the resulting DNN\nclassifiers capable of attaining the locality information in the latent space.\nWe experiment with two concrete instantiations of the HashTran-DNN framework to\nclassify Android malware. Experimental results show that four known attacks can\nrender standard DNNs useless in classifying Android malware, that known\ndefenses can at most defend three of the four attacks, and that HashTran-DNN\ncan effectively defend against all of the four attacks.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "li2018self", "year": "2018", "title":"Self-supervised Adversarial Hashing Networks For Cross-modal Retrieval", "abstract": "<p>Thanks to the success of deep learning, cross-modal retrieval has made\nsignificant progress recently. However, there still remains a crucial\nbottleneck: how to bridge the modality gap to further enhance the retrieval\naccuracy. In this paper, we propose a self-supervised adversarial hashing\n(\\textbf{SSAH}) approach, which lies among the early attempts to incorporate\nadversarial learning into cross-modal hashing in a self-supervised fashion. The\nprimary contribution of this work is that two adversarial networks are\nleveraged to maximize the semantic correlation and consistency of the\nrepresentations between different modalities. In addition, we harness a\nself-supervised semantic network to discover high-level semantic information in\nthe form of multi-label annotations. Such information guides the feature\nlearning process and preserves the modality relationships in both the common\nsemantic space and the Hamming space. Extensive experiments carried out on\nthree benchmark datasets validate that the proposed SSAH surpasses the\nstate-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Deep Learning","Supervised"] },
{"key": "li2019coupled", "year": "2019", "title":"Coupled Cyclegan Unsupervised Hashing Network For Cross-modal Retrieval", "abstract": "<p>In recent years, hashing has attracted more and more attention owing to its\nsuperior capacity of low storage cost and high query efficiency in large-scale\ncross-modal retrieval. Benefiting from deep leaning, continuously compelling\nresults in cross-modal retrieval community have been achieved. However,\nexisting deep cross-modal hashing methods either rely on amounts of labeled\ninformation or have no ability to learn an accuracy correlation between\ndifferent modalities. In this paper, we proposed Unsupervised coupled Cycle\ngenerative adversarial Hashing networks (UCH), for cross-modal retrieval, where\nouter-cycle network is used to learn powerful common representation, and\ninner-cycle network is explained to generate reliable hash codes. Specifically,\nour proposed UCH seamlessly couples these two networks with generative\nadversarial mechanism, which can be optimized simultaneously to learn\nrepresentation and hash codes. Extensive experiments on three popular benchmark\ndatasets show that the proposed UCH outperforms the state-of-the-art\nunsupervised cross-modal hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Deep Learning","Unsupervised"] },
{"key": "li2019deep", "year": "2019", "title":"Deep Multi-index Hashing For Person Re-identification", "abstract": "<p>Traditional person re-identification (ReID) methods typically represent\nperson images as real-valued features, which makes ReID inefficient when the\ngallery set is extremely large. Recently, some hashing methods have been\nproposed to make ReID more efficient. However, these hashing methods will\ndeteriorate the accuracy in general, and the efficiency of them is still not\nhigh enough. In this paper, we propose a novel hashing method, called deep\nmulti-index hashing (DMIH), to improve both efficiency and accuracy for ReID.\nDMIH seamlessly integrates multi-index hashing and multi-branch based networks\ninto the same framework. Furthermore, a novel block-wise multi-index hashing\ntable construction approach and a search-aware multi-index (SAMI) loss are\nproposed in DMIH to improve the search efficiency. Experiments on three widely\nused datasets show that DMIH can outperform other state-of-the-art baselines,\nincluding both hashing methods and real-valued methods, in terms of both\nefficiency and accuracy.</p>\n", "tags": ["ARXIV"] },
{"key": "li2019push", "year": "2019", "title":"Push For Quantization Deep Fisher Hashing", "abstract": "<p>Current massive datasets demand light-weight access for analysis. Discrete\nhashing methods are thus beneficial because they map high-dimensional data to\ncompact binary codes that are efficient to store and process, while preserving\nsemantic similarity. To optimize powerful deep learning methods for image\nhashing, gradient-based methods are required. Binary codes, however, are\ndiscrete and thus have no continuous derivatives. Relaxing the problem by\nsolving it in a continuous space and then quantizing the solution is not\nguaranteed to yield separable binary codes. The quantization needs to be\nincluded in the optimization. In this paper we push for quantization: We\noptimize maximum class separability in the binary space. We introduce a margin\non distances between dissimilar image pairs as measured in the binary space. In\naddition to pair-wise distances, we draw inspiration from Fisher’s Linear\nDiscriminant Analysis (Fisher LDA) to maximize the binary distances between\nclasses and at the same time minimize the binary distance of images within the\nsame class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate\ncompact codes comparing favorably to the current state of the art.</p>\n", "tags": ["ARXIV","Deep Learning","Quantisation"] },
{"key": "li2019random", "year": "2019", "title":"Random Projections With Asymmetric Quantization", "abstract": "<p>The method of random projection has been a popular tool for data compression,\nsimilarity search, and machine learning. In many practical scenarios, applying\nquantization on randomly projected data could be very helpful to further reduce\nstorage cost and facilitate more efficient retrievals, while only suffering from\nlittle loss in accuracy. In real-world applications, however, data collected from\ndifferent sources may be quantized under different schemes, which calls for a need to study the asymmetric quantization problem. In this paper, we investigate the cosine similarity estimators derived in such setting under the Lloyd-Max (LM)\nquantization scheme. We thoroughly analyze the biases and variances of a series of estimators including the basic simple estimators, their normalized versions, and\ntheir debiased versions. Furthermore, by studying the monotonicity, we show that\nthe expectation of proposed estimators increases with the true cosine similarity,\non a broader family of stair-shaped quantizers. Experiments on nearest neighbor\nsearch justify the theory and illustrate the effectiveness of our proposed estimators.</p>\n", "tags": ["Independent","NEURIPS","Quantisation","Theory"] },
{"key": "li2019re", "year": "2019", "title":"Re-randomized Densification For One Permutation Hashing And Bin-wise Consistent Weighted Sampling", "abstract": "<p>Jaccard similarity is widely used as a distance measure in many machine learning\nand search applications. Typically, hashing methods are essential for the use of\nJaccard similarity to be practical in large-scale settings. For hashing binary (0/1)\ndata, the idea of one permutation hashing (OPH) with densification significantly\naccelerates traditional minwise hashing algorithms while providing unbiased and\naccurate estimates. In this paper, we propose a strategy named “re-randomization”\nin the process of densification that could achieve the smallest variance among all\ndensification schemes. The success of this idea naturally inspires us to generalize\none permutation hashing to weighted (non-binary) data, which results in the socalled “bin-wise consistent weighted sampling (BCWS)” algorithm. We analyze the\nbehavior of BCWS and compare it with a recent alternative. Extensive experiments\non various datasets illustrates the effectiveness of our proposed methods.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "li2020deep", "year": "2020", "title":"Deep Unsupervised Image Hashing By Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video\ncollections without having expensive annotations available. Hashing aims to\nlearn short binary codes for compact storage and efficient semantic retrieval.\nWe propose an unsupervised deep hashing layer called Bi-half Net that maximizes\nentropy of the binary codes. Entropy is maximal when both possible values of\nthe bit are uniformly (half-half) distributed. To maximize bit entropy, we do\nnot add a term to the loss function as this is difficult to optimize and tune.\nInstead, we design a new parameter-free network layer to explicitly force\ncontinuous image features to approximate the optimal half-half bit\ndistribution. This layer is shown to minimize a penalized term of the\nWasserstein distance between the learned continuous image features and the\noptimal half-half bit distribution. Experimental results on the image datasets\nFlickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and\nHmdb-51 show that our approach leads to compact codes and compares favorably to\nthe current state-of-the-art.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "li2020hamming", "year": "2020", "title":"Hamming OCR A Locality Sensitive Hashing Neural Network For Scene Text Recognition", "abstract": "<p>Recently, inspired by Transformer, self-attention-based scene text\nrecognition approaches have achieved outstanding performance. However, we find\nthat the size of model expands rapidly with the lexicon increasing.\nSpecifically, the number of parameters for softmax classification layer and\noutput embedding layer are proportional to the vocabulary size. It hinders the\ndevelopment of a lightweight text recognition model especially applied for\nChinese and multiple languages. Thus, we propose a lightweight scene text\nrecognition model named Hamming OCR. In this model, a novel Hamming classifier,\nwhich adopts locality sensitive hashing (LSH) algorithm to encode each\ncharacter, is proposed to replace the softmax regression and the generated LSH\ncode is directly employed to replace the output embedding. We also present a\nsimplified transformer decoder to reduce the number of parameters by removing\nthe feed-forward network and using cross-layer parameter sharing technique.\nCompared with traditional methods, the number of parameters in both\nclassification and embedding layers is independent on the size of vocabulary,\nwhich significantly reduces the storage requirement without loss of accuracy.\nExperimental results on several datasets, including four public benchmaks and a\nChinese text dataset synthesized by SynthText with more than 20,000 characters,\nshows that Hamming OCR achieves competitive results.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "li2020multiple", "year": "2020", "title":"Multiple Code Hashing For Efficient Image Retrieval", "abstract": "<p>Due to its low storage cost and fast query speed, hashing has been widely\nused in large-scale image retrieval tasks. Hash bucket search returns data\npoints within a given Hamming radius to each query, which can enable search at\na constant or sub-linear time cost. However, existing hashing methods cannot\nachieve satisfactory retrieval performance for hash bucket search in complex\nscenarios, since they learn only one hash code for each image. More\nspecifically, by using one hash code to represent one image, existing methods\nmight fail to put similar image pairs to the buckets with a small Hamming\ndistance to the query when the semantic information of images is complex. As a\nresult, a large number of hash buckets need to be visited for retrieving\nsimilar images, based on the learned codes. This will deteriorate the\nefficiency of hash bucket search. In this paper, we propose a novel hashing\nframework, called multiple code hashing (MCH), to improve the performance of\nhash bucket search. The main idea of MCH is to learn multiple hash codes for\neach image, with each code representing a different region of the image.\nFurthermore, we propose a deep reinforcement learning algorithm to learn the\nparameters in MCH. To the best of our knowledge, this is the first work that\nproposes to learn multiple hash codes for each image in image retrieval.\nExperiments demonstrate that MCH can achieve a significant improvement in hash\nbucket search, compared with existing methods that learn only one hash code for\neach image.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "li2020perceptual", "year": "2020", "title":"Perceptual Robust Hashing For Color Images With Canonical Correlation Analysis", "abstract": "<p>In this paper, a novel perceptual image hashing scheme for color images is\nproposed based on ring-ribbon quadtree and color vector angle. First, original\nimage is subjected to normalization and Gaussian low-pass filtering to produce\na secondary image, which is divided into a series of ring-ribbons with\ndifferent radii and the same number of pixels. Then, both textural and color\nfeatures are extracted locally and globally. Quadtree decomposition (QD) is\napplied on luminance values of the ring-ribbons to extract local textural\nfeatures, and the gray level co-occurrence matrix (GLCM) is used to extract\nglobal textural features. Local color features of significant corner points on\nouter boundaries of ring-ribbons are extracted through color vector angles\n(CVA), and color low-order moments (CLMs) is utilized to extract global color\nfeatures. Finally, two types of feature vectors are fused via canonical\ncorrelation analysis (CCA) to prodcue the final hash after scrambling. Compared\nwith direct concatenation, the CCA feature fusion method improves\nclassification performance, which better reflects overall correlation between\ntwo sets of feature vectors. Receiver operating characteristic (ROC) curve\nshows that our scheme has satisfactory performances with respect to robustness,\ndiscrimination and security, which can be effectively used in copy detection\nand content authentication.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2020task", "year": "2020", "title":"Task-adaptive Asymmetric Deep Cross-modal Hashing", "abstract": "<p>Supervised cross-modal hashing aims to embed the semantic correlations of\nheterogeneous modality data into the binary hash codes with discriminative\nsemantic labels. Because of its advantages on retrieval and storage efficiency,\nit is widely used for solving efficient cross-modal retrieval. However,\nexisting researches equally handle the different tasks of cross-modal\nretrieval, and simply learn the same couple of hash functions in a symmetric\nway for them. Under such circumstance, the uniqueness of different cross-modal\nretrieval tasks are ignored and sub-optimal performance may be brought.\nMotivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal\nHashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash\nfunctions for two sub-retrieval tasks via simultaneous modality representation\nand asymmetric hash learning. Unlike previous cross-modal hashing approaches,\nour learning framework jointly optimizes semantic preserving that transforms\ndeep features of multimedia data into binary hash codes, and the semantic\nregression which directly regresses query modality representation to explicit\nlabel. With our model, the binary codes can effectively preserve semantic\ncorrelations across different modalities, meanwhile, adaptively capture the\nquery semantics. The superiority of TA-ADCMH is proved on two standard datasets\nfrom many aspects.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "li2020topology", "year": "2020", "title":"Topology-aware Hashing For Effective Control Flow Graph Similarity Analysis", "abstract": "<p>Control Flow Graph (CFG) similarity analysis is an essential technique for a\nvariety of security analysis tasks, including malware detection and malware\nclustering. Even though various algorithms have been developed, existing CFG\nsimilarity analysis methods still suffer from limited efficiency, accuracy, and\nusability. In this paper, we propose a novel fuzzy hashing scheme called\ntopology-aware hashing (TAH) for effective and efficient CFG similarity\nanalysis. Given the CFGs constructed from program binaries, we extract blended\nn-gram graphical features of the CFGs, encode the graphical features into\nnumeric vectors (called graph signatures), and then measure the graph\nsimilarity by comparing the graph signatures. We further employ a fuzzy hashing\ntechnique to convert the numeric graph signatures into smaller fixed-size fuzzy\nhash signatures for efficient similarity calculation. Our comprehensive\nevaluation demonstrates that TAH is more effective and efficient compared to\nexisting CFG comparison techniques. To demonstrate the applicability of TAH to\nreal-world security analysis tasks, we develop a binary similarity analysis\ntool based on TAH, and show that it outperforms existing similarity analysis\ntools while conducting malware clustering.</p>\n", "tags": ["Graph","Unsupervised"] },
{"key": "li2021c", "year": "2021", "title":"C-OPH Improving The Accuracy Of One Permutation Hashing (OPH) With Circulant Permutations", "abstract": "<p>Minwise hashing (MinHash) is a classical method for efficiently estimating\nthe Jaccrad similarity in massive binary (0/1) data. To generate \\(K\\) hash\nvalues for each data vector, the standard theory of MinHash requires \\(K\\)\nindependent permutations. Interestingly, the recent work on “circulant MinHash”\n(C-MinHash) has shown that merely two permutations are needed. The first\npermutation breaks the structure of the data and the second permutation is\nre-used \\(K\\) time in a circulant manner. Surprisingly, the estimation accuracy\nof C-MinHash is proved to be strictly smaller than that of the original\nMinHash. The more recent work further demonstrates that practically only one\npermutation is needed. Note that C-MinHash is different from the well-known\nwork on “One Permutation Hashing (OPH)” published in NIPS’12. OPH and its\nvariants using different “densification” schemes are popular alternatives to\nthe standard MinHash. The densification step is necessary in order to deal with\nempty bins which exist in One Permutation Hashing.\n  In this paper, we propose to incorporate the essential ideas of C-MinHash to\nimprove the accuracy of One Permutation Hashing. Basically, we develop a new\ndensification method for OPH, which achieves the smallest estimation variance\ncompared to all existing densification schemes for OPH. Our proposed method is\nnamed C-OPH (Circulant OPH). After the initial permutation (which breaks the\nexisting structure of the data), C-OPH only needs a “shorter” permutation of\nlength \\(D/K\\) (instead of \\(D\\)), where \\(D\\) is the original data dimension and \\(K\\)\nis the total number of bins in OPH. This short permutation is re-used in \\(K\\)\nbins in a circulant shifting manner. It can be shown that the estimation\nvariance of the Jaccard similarity is strictly smaller than that of the\nexisting (densified) OPH methods.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "li2021ce", "year": "2021", "title":"Ce-dedup Cost-effective Convolutional Neural Nets Training Based On Image Deduplication", "abstract": "<p>Attributed to the ever-increasing large image datasets, Convolutional Neural\nNetworks (CNNs) have become popular for vision-based tasks. It is generally\nadmirable to have larger-sized datasets for higher network training accuracies.\nHowever, the impact of dataset quality has not to be involved. It is reasonable\nto assume the near-duplicate images exist in the datasets. For instance, the\nStreet View House Numbers (SVHN) dataset having cropped house plate digits from\n0 to 9 are likely to have repetitive digits from the same/similar house plates.\nRedundant images may take up a certain portion of the dataset without\nconsciousness. While contributing little to no accuracy improvement for the\nCNNs training, these duplicated images unnecessarily pose extra resource and\ncomputation consumption. To this end, this paper proposes a framework to assess\nthe impact of the near-duplicate images on CNN training performance, called\nCE-Dedup. Specifically, CE-Dedup associates a hashing-based image deduplication\napproach with downstream CNNs-based image classification tasks. CE-Dedup\nbalances the tradeoff between a large deduplication ratio and a stable accuracy\nby adjusting the deduplication threshold. The effectiveness of CE-Dedup is\nvalidated through extensive experiments on well-known CNN benchmarks. On one\nhand, while maintaining the same validation accuracy, CE-Dedup can reduce the\ndataset size by 23%. On the other hand, when allowing a small validation\naccuracy drop (by 5%), CE-Dedup can trim the dataset size by 75%.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "li2021extra", "year": "2021", "title":"EXTRA Explanation Ranking Datasets For Explainable Recommendation", "abstract": "<p>Recently, research on explainable recommender systems has drawn much\nattention from both academia and industry, resulting in a variety of\nexplainable models. As a consequence, their evaluation approaches vary from\nmodel to model, which makes it quite difficult to compare the explainability of\ndifferent models. To achieve a standard way of evaluating recommendation\nexplanations, we provide three benchmark datasets for EXplanaTion RAnking\n(denoted as EXTRA), on which explainability can be measured by ranking-oriented\nmetrics. Constructing such datasets, however, poses great challenges. First,\nuser-item-explanation triplet interactions are rare in existing recommender\nsystems, so how to find alternatives becomes a challenge. Our solution is to\nidentify nearly identical sentences from user reviews. This idea then leads to\nthe second challenge, i.e., how to efficiently categorize the sentences in a\ndataset into different groups, since it has quadratic runtime complexity to\nestimate the similarity between any two sentences. To mitigate this issue, we\nprovide a more efficient method based on Locality Sensitive Hashing (LSH) that\ncan detect near-duplicates in sub-linear time for a given query. Moreover, we\nmake our code publicly available to allow researchers in the community to\ncreate their own datasets.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "li2021more", "year": "2021", "title":"More Robust Dense Retrieval With Contrastive Dual Learning", "abstract": "<p>Dense retrieval conducts text retrieval in the embedding space and has shown\nmany advantages compared to sparse retrieval. Existing dense retrievers\noptimize representations of queries and documents with contrastive training and\nmap them to the embedding space. The embedding space is optimized by aligning\nthe matched query-document pairs and pushing the negative documents away from\nthe query. However, in such training paradigm, the queries are only optimized\nto align to the documents and are coarsely positioned, leading to an\nanisotropic query embedding space. In this paper, we analyze the embedding\nspace distributions and propose an effective training paradigm, Contrastive\nDual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained\nquery representations for dense retrieval. DANCE incorporates an additional\ndual training object of query retrieval, inspired by the classic information\nretrieval training axiom, query likelihood. With contrastive learning, the dual\ntraining object of DANCE learns more tailored representations for queries and\ndocuments to keep the embedding space smooth and uniform, thriving on the\nranking performance of DANCE on the MS MARCO document retrieval task. Different\nfrom ANCE that only optimized with the document retrieval task, DANCE\nconcentrates the query embeddings closer to document representations while\nmaking the document distribution more discriminative. Such concentrated query\nembedding distribution assigns more uniform negative sampling probabilities to\nqueries and helps to sufficiently optimize query representations in the query\nretrieval task. Our codes are released at https://github.com/thunlp/DANCE.</p>\n", "tags": ["ARXIV","Has Code","Self Supervised","Text Retrieval"] },
{"key": "li2022adaptive", "year": "2022", "title":"Adaptive Structural Similarity Preserving For Unsupervised Cross Modal Hashing", "abstract": "<p>Cross-modal hashing is an important approach for multimodal data management\nand application. Existing unsupervised cross-modal hashing algorithms mainly\nrely on data features in pre-trained models to mine their similarity\nrelationships. However, their optimization objectives are based on the static\nmetric between the original uni-modal features, without further exploring data\ncorrelations during the training. In addition, most of them mainly focus on\nassociation mining and alignment among pairwise instances in continuous space\nbut ignore the latent structural correlations contained in the semantic hashing\nspace. In this paper, we propose an unsupervised hash learning framework,\nnamely Adaptive Structural Similarity Preservation Hashing (ASSPH), to solve\nthe above problems. Firstly, we propose an adaptive learning scheme, with\nlimited data and training batches, to enrich semantic correlations of unlabeled\ninstances during the training process and meanwhile to ensure a smooth\nconvergence of the training process. Secondly, we present an asymmetric\nstructural semantic representation learning scheme. We introduce structural\nsemantic metrics based on graph adjacency relations during the semantic\nreconstruction and correlation mining stage and meanwhile align the structure\nsemantics in the hash space with an asymmetric binary optimization process.\nFinally, we conduct extensive experiments to validate the enhancements of our\nwork in comparison with existing works.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Unsupervised"] },
{"key": "li2022asymmetric", "year": "2022", "title":"Asymmetric Scalable Cross-modal Hashing", "abstract": "<p>Cross-modal hashing is a successful method to solve large-scale multimedia\nretrieval issue. A lot of matrix factorization-based hashing methods are\nproposed. However, the existing methods still struggle with a few problems,\nsuch as how to generate the binary codes efficiently rather than directly relax\nthem to continuity. In addition, most of the existing methods choose to use an\n\\(n\\times n\\) similarity matrix for optimization, which makes the memory and\ncomputation unaffordable. In this paper we propose a novel Asymmetric Scalable\nCross-Modal Hashing (ASCMH) to address these issues. It firstly introduces a\ncollective matrix factorization to learn a common latent space from the\nkernelized features of different modalities, and then transforms the similarity\nmatrix optimization to a distance-distance difference problem minimization with\nthe help of semantic labels and common latent space. Hence, the computational\ncomplexity of the \\(n\\times n\\) asymmetric optimization is relieved. In the\ngeneration of hash codes we also employ an orthogonal constraint of label\ninformation, which is indispensable for search accuracy. So the redundancy of\ncomputation can be much reduced. For efficient optimization and scalable to\nlarge-scale datasets, we adopt the two-step approach rather than optimizing\nsimultaneously. Extensive experiments on three benchmark datasets: Wiki,\nMIRFlickr-25K, and NUS-WIDE, demonstrate that our ASCMH outperforms the\nstate-of-the-art cross-modal hashing methods in terms of accuracy and\nefficiency.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "li2022efficient", "year": "2022", "title":"An Efficient Hashing-based Ensemble Method For Collaborative Outlier Detection", "abstract": "<p>In collaborative outlier detection, multiple participants exchange their\nlocal detectors trained on decentralized devices without exchanging their own\ndata. A key problem of collaborative outlier detection is efficiently\naggregating multiple local detectors to form a global detector without\nbreaching the privacy of participants’ data and degrading the detection\naccuracy. We study locality-sensitive hashing-based ensemble methods to detect\ncollaborative outliers since they are mergeable and compatible with\ndifferentially private mechanisms. Our proposed LSH iTables is simple and\noutperforms recent ensemble competitors on centralized and decentralized\nscenarios over many real-world data sets.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "li2023differentially", "year": "2023", "title":"Differentially Private One Permutation Hashing And Bin-wise Consistent Weighted Sampling", "abstract": "<p>Minwise hashing (MinHash) is a standard algorithm widely used in the\nindustry, for large-scale search and learning applications with the binary\n(0/1) Jaccard similarity. One common use of MinHash is for processing massive\nn-gram text representations so that practitioners do not have to materialize\nthe original data (which would be prohibitive). Another popular use of MinHash\nis for building hash tables to enable sub-linear time approximate near neighbor\n(ANN) search. MinHash has also been used as a tool for building large-scale\nmachine learning systems. The standard implementation of MinHash requires\napplying \\(K\\) random permutations. In comparison, the method of one permutation\nhashing (OPH), is an efficient alternative of MinHash which splits the data\nvectors into \\(K\\) bins and generates hash values within each bin. OPH is\nsubstantially more efficient and also more convenient to use.\n  In this paper, we combine the differential privacy (DP) with OPH (as well as\nMinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix,\nDP-OPH-re and DP-OPH-rand, depending on which densification strategy is adopted\nto deal with empty bins in OPH. A detailed roadmap to the algorithm design is\npresented along with the privacy analysis. An analytical comparison of our\nproposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided to\njustify the advantage of DP-OPH. Experiments on similarity search confirm the\nmerits of DP-OPH, and guide the choice of the proper variant in different\npractical scenarios. Our technique is also extended to bin-wise consistent\nweighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS for\nnon-binary data. Experiments on classification tasks demonstrate that DP-BCWS\nis able to achieve excellent utility at around \\(\\epsilon = 5\\sim 10\\), where\n\\(\\epsilon\\) is the standard parameter in the language of \\((\\epsilon,\n\\delta)\\)-DP.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2023dual", "year": "2023", "title":"Dual-stream Knowledge-preserving Hashing For Unsupervised Video Retrieval", "abstract": "<p>Unsupervised video hashing usually optimizes binary codes by learning to\nreconstruct input videos. Such reconstruction constraint spends much effort on\nframe-level temporal context changes without focusing on video-level global\nsemantics that are more useful for retrieval. Hence, we address this problem by\ndecomposing video information into reconstruction-dependent and\nsemantic-dependent information, which disentangles the semantic extraction from\nreconstruction constraint. Specifically, we first design a simple dual-stream\nstructure, including a temporal layer and a hash layer. Then, with the help of\nsemantic similarity knowledge obtained from self-supervision, the hash layer\nlearns to capture information for semantic retrieval, while the temporal layer\nlearns to capture the information for reconstruction. In this way, the model\nnaturally preserves the disentangled semantics into binary codes. Validated by\ncomprehensive experiments, our method consistently outperforms the\nstate-of-the-arts on three video benchmarks.</p>\n", "tags": ["ARXIV","Unsupervised","Video Retrieval"] },
{"key": "li2023locality", "year": "2023", "title":"Locality Preserving Multiview Graph Hashing For Large Scale Remote Sensing Image Search", "abstract": "<p>Hashing is very popular for remote sensing image search. This article\nproposes a multiview hashing with learnable parameters to retrieve the queried\nimages for a large-scale remote sensing dataset. Existing methods always\nneglect that real-world remote sensing data lies on a low-dimensional manifold\nembedded in high-dimensional ambient space. Unlike previous methods, this\narticle proposes to learn the consensus compact codes in a view-specific\nlow-dimensional subspace. Furthermore, we have added a hyperparameter learnable\nmodule to avoid complex parameter tuning. In order to prove the effectiveness\nof our method, we carried out experiments on three widely used remote sensing\ndata sets and compared them with seven state-of-the-art methods. Extensive\nexperiments show that the proposed method can achieve competitive results\ncompared to the other method.</p>\n", "tags": ["ARXIV","Cross Modal","Graph"] },
{"key": "li2023pb", "year": "2023", "title":"Pb-hash Partitioned B-bit Hashing", "abstract": "<p>Many hashing algorithms including minwise hashing (MinHash), one permutation\nhashing (OPH), and consistent weighted sampling (CWS) generate integers of \\(B\\)\nbits. With \\(k\\) hashes for each data vector, the storage would be \\(B\\times k\\)\nbits; and when used for large-scale learning, the model size would be\n\\(2^B\\times k\\), which can be expensive. A standard strategy is to use only the\nlowest \\(b\\) bits out of the \\(B\\) bits and somewhat increase \\(k\\), the number of\nhashes. In this study, we propose to re-use the hashes by partitioning the \\(B\\)\nbits into \\(m\\) chunks, e.g., \\(b\\times m =B\\). Correspondingly, the model size\nbecomes \\(m\\times 2^b \\times k\\), which can be substantially smaller than the\noriginal \\(2^B\\times k\\).\n  Our theoretical analysis reveals that by partitioning the hash values into\n\\(m\\) chunks, the accuracy would drop. In other words, using \\(m\\) chunks of \\(B/m\\)\nbits would not be as accurate as directly using \\(B\\) bits. This is due to the\ncorrelation from re-using the same hash. On the other hand, our analysis also\nshows that the accuracy would not drop much for (e.g.,) \\(m=2\\sim 4\\). In some\nregions, Pb-Hash still works well even for \\(m\\) much larger than 4. We expect\nPb-Hash would be a good addition to the family of hashing methods/applications\nand benefit industrial practitioners.\n  We verify the effectiveness of Pb-Hash in machine learning tasks, for linear\nSVM models as well as deep learning models. Since the hashed data are\nessentially categorical (ID) features, we follow the standard practice of using\nembedding tables for each hash. With Pb-Hash, we need to design an effective\nstrategy to combine \\(m\\) embeddings. Our study provides an empirical evaluation\non four pooling schemes: concatenation, max pooling, mean pooling, and product\npooling. There is no definite answer which pooling would be always better and\nwe leave that for future study.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "li2023practice", "year": "2023", "title":"Practice With Graph-based ANN Algorithms On Sparse Data Chi-square Two-tower Model HNSW Sign Cauchy Projections", "abstract": "<p>Sparse data are common. The traditional <code class=\"language-plaintext highlighter-rouge\">handcrafted'' features are often\nsparse. Embedding vectors from trained models can also be very sparse, for\nexample, embeddings trained via the</code>ReLu’’ activation function. In this\npaper, we report our exploration of efficient search in sparse data with\ngraph-based ANN algorithms (e.g., HNSW, or SONG which is the GPU version of\nHNSW), which are popular in industrial practice, e.g., search and ads\n(advertising).\n  We experiment with the proprietary ads targeting application, as well as\nbenchmark public datasets. For ads targeting, we train embeddings with the\nstandard <code class=\"language-plaintext highlighter-rouge\">cosine two-tower'' model and we also develop the</code>chi-square\ntwo-tower’’ model. Both models produce (highly) sparse embeddings when they are\nintegrated with the <code class=\"language-plaintext highlighter-rouge\">ReLu'' activation function. In EBR (embedding-based\nretrieval) applications, after we the embeddings are trained, the next crucial\ntask is the approximate near neighbor (ANN) search for serving. While there are\nmany ANN algorithms we can choose from, in this study, we focus on the\ngraph-based ANN algorithm (e.g., HNSW-type).\n  Sparse embeddings should help improve the efficiency of EBR. One benefit is\nthe reduced memory cost for the embeddings. The other obvious benefit is the\nreduced computational time for evaluating similarities, because, for\ngraph-based ANN algorithms such as HNSW, computing similarities is often the\ndominating cost. In addition to the effort on leveraging data sparsity for\nstorage and computation, we also integrate</code>sign cauchy random projections’’\n(SignCRP) to hash vectors to bits, to further reduce the memory cost and speed\nup the ANN search. In NIPS’13, SignCRP was proposed to hash the chi-square\nsimilarity, which is a well-adopted nonlinear kernel in NLP and computer\nvision. Therefore, the chi-square two-tower model, SignCRP, and HNSW are now\ntightly integrated.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "li2024bert", "year": "2024", "title":"BERT-LSH Reducing Absolute Compute For Attention", "abstract": "<p>This study introduces a novel BERT-LSH model that incorporates Locality\nSensitive Hashing (LSH) to approximate the attention mechanism in the BERT\narchitecture. We examine the computational efficiency and performance of this\nmodel compared to a standard baseline BERT model. Our findings reveal that\nBERT-LSH significantly reduces computational demand for the self-attention\nlayer while unexpectedly outperforming the baseline model in pretraining and\nfine-tuning tasks. These results suggest that the LSH-based attention mechanism\nnot only offers computational advantages but also may enhance the model’s\nability to generalize from its training data. For more information, visit our\nGitHub repository: https://github.com/leo4life2/algoml-final</p>\n", "tags": ["ARXIV","Has Code","Independent","LSH"] },
{"key": "li2024comae", "year": "2024", "title":"COMAE Comprehensive Attribute Exploration For Zero-shot Hashing", "abstract": "<p>Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency\nand generalization in large-scale retrieval scenarios. While considerable\nsuccess has been achieved, there still exist urgent limitations. Existing works\nignore the locality relationships of representations and attributes, which have\neffective transferability between seeable classes and unseeable classes. Also,\nthe continuous-value attributes are not fully harnessed. In response, we\nconduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which\ndepicts the relationships from seen classes to unseen ones through three\nmeticulously designed explorations, i.e., point-wise, pair-wise and class-wise\nconsistency constraints. By regressing attributes from the proposed attribute\nprototype network, COMAE learns the local features that are relevant to the\nvisual attributes. Then COMAE utilizes contrastive learning to comprehensively\ndepict the context of attributes, rather than instance-independent\noptimization. Finally, the class-wise constraint is designed to cohesively\nlearn the hash code, image representation, and visual attributes more\neffectively. Experimental results on the popular ZSH datasets demonstrate that\nCOMAE outperforms state-of-the-art hashing techniques, especially in scenarios\nwith a larger number of unseen label classes.</p>\n", "tags": ["ARXIV","Self Supervised"] },
{"key": "li2024consistent", "year": "2024", "title":"0-bit Consistent Weighted Sampling", "abstract": "<p>We develop 0-bit consistent weighted sampling (CWS) for efficiently estimating min-max kernel, which is a generalization of the resemblance kernel originally designed for binary data. Because the estimator of 0-bit CWS constitutes a positive definite kernel, this method can be naturally applied to large-scale data mining problems. Basically, if we feed the sampled data from 0-bit CWS to a highly efficient linear classifier (e.g., linear SVM), we effectively (and approximately) train a nonlinear classifier based on the min-max kernel. The accuracy improves as we increase the sample size.</p>\n\n<p>In this paper, we first demonstrate, through an extensive classification study using kernel machines, that the min-max kernel often provides an effective measure of similarity for nonnegative data. This helps justify the use of min-max kernel. However, as the min-max kernel is nonlinear and might be difficult to be used for industrial applications with massive data, we propose to linearize the min-max kernel via 0-bit CWS, a simplification of the original CWS method.</p>\n\n<p>The previous remarkable work on consistent weighted sampling (CWS) produces samples in the form of (i<em>, t</em>) where the i* records the location (and in fact also the weights) information analogous to the samples produced by classical minwise hashing on binary data. Because the t* is theoretically unbounded, it was not immediately clear how to effectively implement CWS for building large-scale linear classifiers. We provide a simple solution by discarding t* (which we refer to as the “0-bit” scheme). Via an extensive empirical study, we show that this 0-bit scheme does not lose essential information. We then apply 0-bit CWS for building linear classifiers to approximate min-max kernel classifiers, as extensively validated on a wide range of public datasets.</p>\n\n<p>We expect this work will generate interests among data mining practitioners who would like to efficiently utilize the nonlinear information of non-binary and nonnegative data.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "li2024deep", "year": "2024", "title":"Deep Unsupervised Image Hashing By Maximizing Bit Entropy", "abstract": "<p>Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy, we do not add a term to the loss function as this is difficult to optimize and tune. Instead, we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "li2024feature", "year": "2024", "title":"Feature Learning Based Deep Supervised Hashing With Pairwise Labels", "abstract": "<p>Recent years have witnessed wide application of\nhashing for large-scale image retrieval. However,\nmost existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep\nhashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown\nbetter performance than traditional hashing methods with hand-crafted features. Most of these deep\nhashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this\npaper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH),\nto perform simultaneous feature learning and hashcode learning for applications with pairwise labels.\nExperiments on real datasets show that our DPSH\nmethod can outperform other methods to achieve\nthe state-of-the-art performance in image retrieval\napplications.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "li2024learning", "year": "2024", "title":"Learning Hash Functions Using Column Generation", "abstract": "<p>Fast nearest neighbor searching is becoming\nan increasingly important tool in solving\nmany large-scale problems. Recently\na number of approaches to learning datadependent\nhash functions have been developed.\nIn this work, we propose a column\ngeneration based method for learning datadependent\nhash functions on the basis of\nproximity comparison information. Given a\nset of triplets that encode the pairwise proximity\ncomparison information, our method\nlearns hash functions that preserve the relative\ncomparison relationships in the data\nas well as possible within the large-margin\nlearning framework. The learning procedure\nis implemented using column generation and\nhence is named CGHash. At each iteration\nof the column generation procedure, the best\nhash function is selected. Unlike most other\nhashing methods, our method generalizes to\nnew data points naturally; and has a training\nobjective which is convex, thus ensuring\nthat the global optimum can be identi-\nfied. Experiments demonstrate that the proposed\nmethod learns compact binary codes\nand that its retrieval performance compares\nfavorably with state-of-the-art methods when\ntested on a few benchmark datasets.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "li2024mixed", "year": "2024", "title":"Mixed-precision Embeddings For Large-scale Recommendation Models", "abstract": "<p>Embedding techniques have become essential components of large databases in\nthe deep learning era. By encoding discrete entities, such as words, items, or\ngraph nodes, into continuous vector spaces, embeddings facilitate more\nefficient storage, retrieval, and processing in large databases. Especially in\nthe domain of recommender systems, millions of categorical features are encoded\nas unique embedding vectors, which facilitates the modeling of similarities and\ninteractions among features. However, numerous embedding vectors can result in\nsignificant storage overhead. In this paper, we aim to compress the embedding\ntable through quantization techniques. Given that features vary in importance\nlevels, we seek to identify an appropriate precision for each feature to\nbalance model accuracy and memory usage. To this end, we propose a novel\nembedding compression method, termed Mixed-Precision Embeddings (MPE).\nSpecifically, to reduce the size of the search space, we first group features\nby frequency and then search precision for each feature group. MPE further\nlearns the probability distribution over precision levels for each feature\ngroup, which can be used to identify the most suitable precision with a\nspecially designed sampling strategy. Extensive experiments on three public\ndatasets demonstrate that MPE significantly outperforms existing embedding\ncompression methods. Remarkably, MPE achieves about 200x compression on the\nCriteo dataset without comprising the prediction accuracy.</p>\n", "tags": ["ARXIV","Deep Learning","Graph","Quantisation"] },
{"key": "li2024neighborhood", "year": "2024", "title":"Neighborhood Preserving Hashing For Scalable Video Retrieval", "abstract": "<p>In this paper, we propose a Neighborhood Preserving\nHashing (NPH) method for scalable video retrieval in an\nunsupervised manner. Unlike most existing deep video\nhashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal\nneighborhood information into the encoding network such\nthat the neighborhood-relevant visual content of a video can\nbe preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses\non partial useful content of each input frame conditioned\non the neighborhood information. We then integrate the\nneighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the\nlearned hashing functions can map similar videos to similar\nbinary codes. Extensive experiments on three widely-used\nbenchmark datasets validate the effectiveness of our proposed approach.</p>\n", "tags": ["ARXIV","Unsupervised","Video Retrieval"] },
{"key": "li2024push", "year": "2024", "title":"Push For Quantization Deep Fisher Hashing", "abstract": "<p>Current massive datasets demand light-weight access for analysis. Discrete hashing methods are thus beneficial because they map high-dimensional data to compact binary codes that are efficient to store and process, while preserving semantic similarity. To optimize powerful deep learning methods for image hashing, gradient-based methods are required. Binary codes, however, are discrete and thus have no continuous derivatives. Relaxing the problem by solving it in a continuous space and then quantizing the solution is not guaranteed to yield separable binary codes. The quantization needs to be included in the optimization. In this paper we push for quantization: We optimize maximum class separability in the binary space. We introduce a margin on distances between dissimilar image pairs as measured in the binary space. In addition to pair-wise distances, we draw inspiration from Fisher’s Linear Discriminant Analysis (Fisher LDA) to maximize the binary distances between classes and at the same time minimize the binary distance of images within the same class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate compact codes comparing favorably to the current state of the art.</p>\n", "tags": ["ARXIV","Deep Learning","Quantisation"] },
{"key": "li2024self", "year": "2024", "title":"Self-supervised Video Hashing Via Bidirectional Transformers", "abstract": "<p>Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efficient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transformers (BTH). Based on the encoder-decoder structure of transformers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we further develop a similarity reconstruction task by establishing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Extensive experiments implemented on three public benchmark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach.</p>\n", "tags": ["ARXIV","Supervised","Video Retrieval"] },
{"key": "li2024two", "year": "2024", "title":"Two Birds One Stone Jointly Learning Binary Code For Large-scale Face Image Retrieval And Attributes Prediction", "abstract": "<p>We address the challenging large-scale content-based\nface image retrieval problem, intended as searching images\nbased on the presence of specific subject, given one face\nimage of him/her. To this end, one natural demand is a supervised binary code learning method. While the learned\ncodes might be discriminating, people often have a further\nexpectation that whether some semantic message (e.g., visual attributes) can be read from the human-incomprehensible\ncodes. For this purpose, we propose a novel binary code\nlearning framework by jointly encoding identity discriminability and a number of facial attributes into unified binary code. In this way, the learned binary codes can be applied to not only fine-grained face image retrieval, but also\nfacial attributes prediction, which is the very innovation of\nthis work, just like killing two birds with one stone. To evaluate the effectiveness of the proposed method, extensive experiments are conducted on a new purified large-scale web\ncelebrity database, named CFW 60K, with abundant manual identity and attributes annotation, and experimental results exhibit the superiority of our method over state-of-the-art.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "li2024very", "year": "2024", "title":"Very Sparse Random Projections", "abstract": "<p>There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "liang2023learning", "year": "2023", "title":"Learning Compact Compositional Embeddings Via Regularized Pruning For Recommendation", "abstract": "<p>Latent factor models are the dominant backbones of contemporary recommender\nsystems (RSs) given their performance advantages, where a unique vector\nembedding with a fixed dimensionality (e.g., 128) is required to represent each\nentity (commonly a user/item). Due to the large number of users and items on\ne-commerce sites, the embedding table is arguably the least memory-efficient\ncomponent of RSs. For any lightweight recommender that aims to efficiently\nscale with the growing size of users/items or to remain applicable in\nresource-constrained settings, existing solutions either reduce the number of\nembeddings needed via hashing, or sparsify the full embedding table to switch\noff selected embedding dimensions. However, as hash collision arises or\nembeddings become overly sparse, especially when adapting to a tighter memory\nbudget, those lightweight recommenders inevitably have to compromise their\naccuracy. To this end, we propose a novel compact embedding framework for RSs,\nnamely Compositional Embedding with Regularized Pruning (CERP). Specifically,\nCERP represents each entity by combining a pair of embeddings from two\nindependent, substantially smaller meta-embedding tables, which are then\njointly pruned via a learnable element-wise threshold. In addition, we\ninnovatively design a regularized pruning mechanism in CERP, such that the two\nsparsified meta-embedding tables are encouraged to encode information that is\nmutually complementary. Given the compatibility with agnostic latent factor\nmodels, we pair CERP with two popular recommendation models for extensive\nexperiments, where results on two real-world datasets under different memory\nbudgets demonstrate its superiority against state-of-the-art baselines. The\ncodebase of CERP is available in https://github.com/xurong-liang/CERP.</p>\n", "tags": ["ARXIV","Has Code","Independent"] },
{"key": "liao2020embedding", "year": "2020", "title":"Embedding Compression With Isotropic Iterative Quantization", "abstract": "<p>Continuous representation of words is a standard component in deep\nlearning-based NLP models. However, representing a large vocabulary requires\nsignificant memory, which can cause problems, particularly on\nresource-constrained platforms. Therefore, in this paper we propose an\nisotropic iterative quantization (IIQ) approach for compressing embedding\nvectors into binary ones, leveraging the iterative quantization technique well\nestablished for image retrieval, while satisfying the desired isotropic\nproperty of PMI based models. Experiments with pre-trained embeddings (i.e.,\nGloVe and HDC) demonstrate a more than thirty-fold compression ratio with\ncomparable and sometimes even improved performance over the original\nreal-valued embedding vectors.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation"] },
{"key": "liguori2016vector", "year": "2016", "title":"Vector Quantization For Machine Vision", "abstract": "<p>This paper shows how to reduce the computational cost for a variety of common\nmachine vision tasks by operating directly in the compressed domain,\nparticularly in the context of hardware acceleration. Pyramid Vector\nQuantization (PVQ) is the compression technique of choice and its properties\nare exploited to simplify Support Vector Machines (SVM), Convolutional Neural\nNetworks(CNNs), Histogram of Oriented Gradients (HOG) features, interest points\nmatching and other algorithms.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "lillis2017hierarchical", "year": "2017", "title":"Hierarchical Bloom Filter Trees For Approximate Matching", "abstract": "<p>Bytewise approximate matching algorithms have in recent years shown\nsignificant promise in de- tecting files that are similar at the byte level.\nThis is very useful for digital forensic investigators, who are regularly faced\nwith the problem of searching through a seized device for pertinent data. A\ncommon scenario is where an investigator is in possession of a collection of\n“known-illegal” files (e.g. a collection of child abuse material) and wishes to\nfind whether copies of these are stored on the seized device. Approximate\nmatching addresses shortcomings in traditional hashing, which can only find\nidentical files, by also being able to deal with cases of merged files,\nembedded files, partial files, or if a file has been changed in any way.\n  Most approximate matching algorithms work by comparing pairs of files, which\nis not a scalable approach when faced with large corpora. This paper\ndemonstrates the effectiveness of using a “Hierarchical Bloom Filter Tree”\n(HBFT) data structure to reduce the running time of\ncollection-against-collection matching, with a specific focus on the MRSH-v2\nalgorithm. Three experiments are discussed, which explore the effects of\ndifferent configurations of HBFTs. The proposed approach dramatically reduces\nthe number of pairwise comparisons required, and demonstrates substantial speed\ngains, while maintaining effectiveness.</p>\n", "tags": ["ARXIV"] },
{"key": "limasset2017fast", "year": "2017", "title":"Fast And Scalable Minimal Perfect Hashing For Massive Key Sets", "abstract": "<p>Minimal perfect hash functions provide space-efficient and collision-free\nhashing on static sets. Existing algorithms and implementations that build such\nfunctions have practical limitations on the number of input elements they can\nprocess, due to high construction time, RAM or external memory usage. We\nrevisit a simple algorithm and show that it is highly competitive with the\nstate of the art, especially in terms of construction time and memory usage. We\nprovide a parallel C++ implementation called BBhash. It is capable of creating\na minimal perfect hash function of \\(10^{10}\\) elements in less than 7 minutes\nusing 8 threads and 5 GB of memory, and the resulting function uses 3.7\nbits/element. To the best of our knowledge, this is also the first\nimplementation that has been successfully tested on an input of cardinality\n\\(10^{12}\\). Source code: https://github.com/rizkg/BBHash</p>\n", "tags": ["ARXIV","Has Code","Independent"] },
{"key": "lin2012density", "year": "2012", "title":"Density Sensitive Hashing", "abstract": "<p>Nearest neighbors search is a fundamental problem in various research fields\nlike machine learning, data mining and pattern recognition. Recently,\nhashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved to\nbe effective for scalable high dimensional nearest neighbors search. Many\nhashing algorithms found their theoretic root in random projection. Since these\nalgorithms generate the hash tables (projections) randomly, a large number of\nhash tables (i.e., long codewords) are required in order to achieve both high\nprecision and recall. To address this limitation, we propose a novel hashing\nalgorithm called {\\em Density Sensitive Hashing} (DSH) in this paper. DSH can\nbe regarded as an extension of LSH. By exploring the geometric structure of the\ndata, DSH avoids the purely random projections selection and uses those\nprojective functions which best agree with the distribution of the data.\nExtensive experimental results on real-world data sets have shown that the\nproposed method achieves better performance compared to the state-of-the-art\nhashing approaches.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "lin2013general", "year": "2013", "title":"A General Two-step Approach To Learning-based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function, and\nan optimization process which is typically deeply coupled to this specific\nform. This tight coupling restricts the flexibility of the method to respond to\nthe data, and can result in complex optimization problems that are difficult to\nsolve. Here we propose a flexible yet simple framework that is able to\naccommodate different types of loss functions and hash functions. This\nframework allows a number of existing approaches to hashing to be placed in\ncontext, and simplifies the development of new problem-specific hashing\nmethods. Our framework decomposes hashing learning problem into two steps: hash\nbit learning and hash function learning based on the learned bits. The first\nstep can typically be formulated as binary quadratic problems, and the second\nstep can be accomplished by training standard binary classifiers. Both problems\nhave been extensively studied in the literature. Our extensive experiments\ndemonstrate that the proposed framework is effective, flexible and outperforms\nthe state-of-the-art.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "lin2014fast", "year": "2014", "title":"Fast Supervised Hashing With Decision Trees For High-dimensional Data", "abstract": "<p>Supervised hashing aims to map the original features to compact binary codes\nthat are able to preserve label based similarity in the Hamming space.\nNon-linear hash functions have demonstrated the advantage over linear ones due\nto their powerful generalization capability. In the literature, kernel\nfunctions are typically used to achieve non-linearity in hashing, which achieve\nencouraging retrieval performance at the price of slow evaluation and training\ntime. Here we propose to use boosted decision trees for achieving non-linearity\nin hashing, which are fast to train and evaluate, hence more suitable for\nhashing with high dimensional data. In our approach, we first propose\nsub-modular formulations for the hashing binary code inference problem and an\nefficient GraphCut based block search method for solving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the\nbinary codes. Experiments demonstrate that our proposed method significantly\noutperforms most state-of-the-art methods in retrieval precision and training\ntime. Especially for high-dimensional data, our method is orders of magnitude\nfaster than many methods in terms of training time.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "lin2014optimizing", "year": "2014", "title":"Optimizing Ranking Measures For Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval.\nDespite much success, existing hashing methods optimize over simple objectives\nsuch as the reconstruction error or graph Laplacian related loss functions,\ninstead of the performance evaluation criteria of interest—multivariate\nperformance measures such as the AUC and NDCG. Here we present a general\nframework (termed StructHash) that allows one to directly optimize multivariate\nperformance measures. The resulting optimization problem can involve\nexponentially or infinitely many variables and constraints, which is more\nchallenging than standard structured output learning. To solve the StructHash\noptimization problem, we use a combination of column generation and\ncutting-plane techniques. We demonstrate the generality of StructHash by\napplying it to ranking prediction and image retrieval, and show that it\noutperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Deep Learning","Graph","Image Retrieval"] },
{"key": "lin2014supervised", "year": "2014", "title":"Supervised Hashing Using Graph Cuts And Boosted Decision Trees", "abstract": "<p>Embedding image features into a binary Hamming space can improve both the\nspeed and accuracy of large-scale query-by-example image retrieval systems.\nSupervised hashing aims to map the original features to compact binary codes in\na manner which preserves the label-based similarities of the original data.\nMost existing approaches apply a single form of hash function, and an\noptimization process which is typically deeply coupled to this specific form.\nThis tight coupling restricts the flexibility of those methods, and can result\nin complex optimization problems that are difficult to solve. In this work we\nproffer a flexible yet simple framework that is able to accommodate different\ntypes of loss functions and hash functions. The proposed framework allows a\nnumber of existing approaches to hashing to be placed in context, and\nsimplifies the development of new problem-specific hashing methods. Our\nframework decomposes the into two steps: binary code (hash bits) learning, and\nhash function learning. The first step can typically be formulated as a binary\nquadratic problem, and the second step can be accomplished by training standard\nbinary classifiers. For solving large-scale binary code inference, we show how\nto ensure that the binary quadratic problems are submodular such that an\nefficient graph cut approach can be used. To achieve efficiency as well as\nefficacy on large-scale high-dimensional data, we propose to use boosted\ndecision trees as the hash functions, which are nonlinear, highly descriptive,\nand very fast to train and evaluate. Experiments demonstrate that our proposed\nmethod significantly outperforms most state-of-the-art methods, especially on\nhigh-dimensional data.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Supervised"] },
{"key": "lin2015deephash", "year": "2015", "title":"Deephash Getting Regularization Depth And Fine-tuning Right", "abstract": "<p>This work focuses on representing very high-dimensional global image\ndescriptors using very compact 64-1024 bit binary hashes for instance\nretrieval. We propose DeepHash: a hashing scheme based on deep networks. Key to\nmaking DeepHash work at extremely low bitrates are three important\nconsiderations – regularization, depth and fine-tuning – each requiring\nsolutions specific to the hashing problem. In-depth evaluation shows that our\nscheme consistently outperforms state-of-the-art methods across all data sets\nfor both Fisher Vectors and Deep Convolutional Neural Network features, by up\nto 20 percent over other schemes. The retrieval performance with 256-bit hashes\nis close to that of the uncompressed floating point features – a remarkable\n512 times compression.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "lin2015implicit", "year": "2015", "title":"Implicit Sparse Code Hashing", "abstract": "<p>We address the problem of converting large-scale high-dimensional image data\ninto binary codes so that approximate nearest-neighbor search over them can be\nefficiently performed. Different from most of the existing unsupervised\napproaches for yielding binary codes, our method is based on a\ndimensionality-reduction criterion that its resulting mapping is designed to\npreserve the image relationships entailed by the inner products of sparse\ncodes, rather than those implied by the Euclidean distances in the ambient\nspace. While the proposed formulation does not require computing any sparse\ncodes, the underlying computation model still inevitably involves solving an\nunmanageable eigenproblem when extremely high-dimensional descriptors are used.\nTo overcome the difficulty, we consider the column-sampling technique and\npresume a special form of rotation matrix to facilitate subproblem\ndecomposition. We test our method on several challenging image datasets and\ndemonstrate its effectiveness by comparing with state-of-the-art binary coding\ntechniques.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "lin2015tiny", "year": "2015", "title":"Tiny Descriptors For Image Retrieval With Unsupervised Triplet Hashing", "abstract": "<p>A typical image retrieval pipeline starts with the comparison of global\ndescriptors from a large database to find a short list of candidate matches. A\ngood image descriptor is key to the retrieval pipeline and should reconcile two\ncontradictory requirements: providing recall rates as high as possible and\nbeing as compact as possible for fast matching. Following the recent successes\nof Deep Convolutional Neural Networks (DCNN) for large scale image\nclassification, descriptors extracted from DCNNs are increasingly used in place\nof the traditional hand crafted descriptors such as Fisher Vectors (FV) with\nbetter retrieval performances. Nevertheless, the dimensionality of a typical\nDCNN descriptor –extracted either from the visual feature pyramid or the\nfully-connected layers– remains quite high at several thousands of scalar\nvalues. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully\nunsupervised method to compute extremely compact binary hashes –in the 32-256\nbits range– from high-dimensional global descriptors. UTH consists of two\nsuccessive deep learning steps. First, Stacked Restricted Boltzmann Machines\n(SRBM), a type of unsupervised deep neural nets, are used to learn binary\nembedding functions able to bring the descriptor size down to the desired\nbitrate. SRBMs are typically able to ensure a very high compression rate at the\nexpense of loosing some desirable metric properties of the original DCNN\ndescriptor space. Then, triplet networks, a rank learning scheme based on\nweight sharing nets is used to fine-tune the binary embedding functions to\nretain as much as possible of the useful metric properties of the original\nspace. A thorough empirical evaluation conducted on multiple publicly available\ndataset using DCNN descriptors shows that our method is able to significantly\noutperform state-of-the-art unsupervised schemes in the target bit range.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Unsupervised"] },
{"key": "lin2016structured", "year": "2016", "title":"Structured Learning Of Binary Codes With Column Generation", "abstract": "<p>Hashing methods aim to learn a set of hash functions which map the original\nfeatures to compact binary codes with similarity preserving in the Hamming\nspace. Hashing has proven a valuable tool for large-scale information\nretrieval. We propose a column generation based binary code learning framework\nfor data-dependent hash function learning. Given a set of triplets that encode\nthe pairwise similarity comparison information, our column generation based\nmethod learns hash functions that preserve the relative comparison relations\nwithin the large-margin learning framework. Our method iteratively learns the\nbest hash functions during the column generation procedure. Existing hashing\nmethods optimize over simple objectives such as the reconstruction error or\ngraph Laplacian related loss functions, instead of the performance evaluation\ncriteria of interest—multivariate performance measures such as the AUC and\nNDCG. Our column generation based method can be further generalized from the\ntriplet loss to a general structured learning based framework that allows one\nto directly optimize multivariate performance measures. For optimizing general\nranking measures, the resulting optimization problem can involve exponentially\nor infinitely many variables and constraints, which is more challenging than\nstandard structured output learning. We use a combination of column generation\nand cutting-plane techniques to solve the optimization problem. To speed-up the\ntraining we further explore stage-wise training and propose to use a simplified\nNDCG loss for efficient inference. We demonstrate the generality of our method\nby applying it to ranking prediction and image retrieval, and show that it\noutperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Independent"] },
{"key": "lin2019hadamard", "year": "2019", "title":"Hadamard Matrix Guided Online Hashing", "abstract": "<p>Online image hashing has attracted increasing research attention recently,\nwhich receives large-scale data in a streaming manner to update the hash\nfunctions on-the-fly. Its key challenge lies in the difficulty of balancing the\nlearning timeliness and model accuracy. To this end, most works follow a\nsupervised setting, i.e., using class labels to boost the hashing performance,\nwhich defects in two aspects: First, strong constraints, e.g., orthogonal or\nsimilarity preserving, are used, which however are typically relaxed and lead\nto large accuracy drop. Second, large amounts of training batches are required\nto learn the up-to-date hash functions, which largely increase the learning\ncomplexity. To handle the above challenges, a novel supervised online hashing\nscheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this\npaper. Our key innovation lies in introducing Hadamard matrix, which is an\northogonal binary matrix built via Sylvester method. In particular, to release\nthe need of strong constraints, we regard each column of Hadamard matrix as the\ntarget code for each class label, which by nature satisfies several desired\nproperties of hashing codes. To accelerate the online training, LSH is first\nadopted to align the lengths of target code and to-be-learned binary code. We\nthen treat the learning of hash functions as a set of binary classification\nproblems to fit the assigned target code. Finally, extensive experiments\ndemonstrate the superior accuracy and efficiency of the proposed method over\nvarious state-of-the-art methods. Codes are available at\nhttps://github.com/lmbxmu/mycode.</p>\n", "tags": ["ARXIV","Has Code","LSH","Supervised"] },
{"key": "lin2019supervised", "year": "2019", "title":"Supervised Online Hashing Via Similarity Distribution Learning", "abstract": "<p>Online hashing has attracted extensive research attention when facing\nstreaming data. Most online hashing methods, learning binary codes based on\npairwise similarities of training instances, fail to capture the semantic\nrelationship, and suffer from a poor generalization in large-scale applications\ndue to large variations. In this paper, we propose to model the similarity\ndistributions between the input data and the hashing codes, upon which a novel\nsupervised online hashing method, dubbed as Similarity Distribution based\nOnline Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship\nin the produced Hamming space. Specifically, we first transform the discrete\nsimilarity matrix into a probability matrix via a Gaussian-based normalization\nto address the extremely imbalanced distribution issue. And then, we introduce\na scaling Student t-distribution to solve the challenging initialization\nproblem, and efficiently bridge the gap between the known and unknown\ndistributions. Lastly, we align the two distributions via minimizing the\nKullback-Leibler divergence (KL-diverence) with stochastic gradient descent\n(SGD), by which an intuitive similarity constraint is imposed to update hashing\nmodel on the new streaming data with a powerful generalizing ability to the\npast data. Extensive experiments on three widely-used benchmarks validate the\nsuperiority of the proposed SDOH over the state-of-the-art methods in the\nonline retrieval task.</p>\n", "tags": ["ARXIV","Streaming Data","Supervised"] },
{"key": "lin2019towards", "year": "2019", "title":"Towards Optimal Discrete Online Hashing With Balanced Similarity", "abstract": "<p>When facing large-scale image datasets, online hashing serves as a promising\nsolution for online retrieval and prediction tasks. It encodes the online\nstreaming data into compact binary codes, and simultaneously updates the hash\nfunctions to renew codes of the existing dataset. To this end, the existing\nmethods update hash functions solely based on the new data batch, without\ninvestigating the correlation between such new data and the existing dataset.\nIn addition, existing works update the hash functions using a relaxation\nprocess in its corresponding approximated continuous space. And it remains as\nan open problem to directly apply discrete optimizations in online hashing. In\nthis paper, we propose a novel supervised online hashing method, termed\nBalanced Similarity for Online Discrete Hashing (BSODH), to solve the above\nproblems in a unified framework. BSODH employs a well-designed hashing\nalgorithm to preserve the similarity between the streaming data and the\nexisting dataset via an asymmetric graph regularization. We further identify\nthe “data-imbalance” problem brought by the constructed asymmetric graph, which\nrestricts the application of discrete optimization in our problem. Therefore, a\nnovel balanced similarity is further proposed, which uses two equilibrium\nfactors to balance the similar and dissimilar weights and eventually enables\nthe usage of discrete optimizations. Extensive experiments conducted on three\nwidely-used benchmarks demonstrate the advantages of the proposed method over\nthe state-of-the-art methods.</p>\n", "tags": ["ARXIV","Graph","Streaming Data","Supervised"] },
{"key": "lin2020fast", "year": "2020", "title":"Fast Class-wise Updating For Online Hashing", "abstract": "<p>Online image hashing has received increasing research attention recently,\nwhich processes large-scale data in a streaming fashion to update the hash\nfunctions on-the-fly. To this end, most existing works exploit this problem\nunder a supervised setting, i.e., using class labels to boost the hashing\nperformance, which suffers from the defects in both adaptivity and efficiency:\nFirst, large amounts of training batches are required to learn up-to-date hash\nfunctions, which leads to poor online adaptivity. Second, the training is\ntime-consuming, which contradicts with the core need of online learning. In\nthis paper, a novel supervised online hashing scheme, termed Fast Class-wise\nUpdating for Online Hashing (FCOH), is proposed to address the above two\nchallenges by introducing a novel and efficient inner product operation. To\nachieve fast online adaptivity, a class-wise updating method is developed to\ndecompose the binary code learning and alternatively renew the hash functions\nin a class-wise fashion, which well addresses the burden on large amounts of\ntraining batches. Quantitatively, such a decomposition further leads to at\nleast 75% storage saving. To further achieve online efficiency, we propose a\nsemi-relaxation optimization, which accelerates the online training by treating\ndifferent binary constraints independently. Without additional constraints and\nvariables, the time complexity is significantly reduced. Such a scheme is also\nquantitatively shown to well preserve past information during updating hashing\nfunctions. We have quantitatively demonstrated that the collective effort of\nclass-wise updating and semi-relaxation optimization provides a superior\nperformance comparing to various state-of-the-art methods, which is verified\nthrough extensive experiments on three widely-used datasets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "lin2021deep", "year": "2021", "title":"Deep Self-adaptive Hashing For Image Retrieval", "abstract": "<p>Hashing technology has been widely used in image retrieval due to its\ncomputational and storage efficiency. Recently, deep unsupervised hashing\nmethods have attracted increasing attention due to the high cost of human\nannotations in the real world and the superiority of deep learning technology.\nHowever, most deep unsupervised hashing methods usually pre-compute a\nsimilarity matrix to model the pairwise relationship in the pre-trained feature\nspace. Then this similarity matrix would be used to guide hash learning, in\nwhich most of the data pairs are treated equivalently. The above process is\nconfronted with the following defects: 1) The pre-computed similarity matrix is\ninalterable and disconnected from the hash learning process, which cannot\nexplore the underlying semantic information. 2) The informative data pairs may\nbe buried by the large number of less-informative data pairs. To solve the\naforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model\nto adaptively capture the semantic information with two special designs:\nAdaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC).\nFirstly, we adopt the AND to initially construct a neighborhood-based\nsimilarity matrix, and then refine this initial similarity matrix with a novel\nupdate strategy to further investigate the semantic structure behind the\nlearned representation. Secondly, we measure the priorities of data pairs with\nPIC and assign adaptive weights to them, which is relies on the assumption that\nmore dissimilar data pairs contain more discriminative information for hash\nlearning. Extensive experiments on several datasets demonstrate that the above\ntwo technologies facilitate the deep hashing model to achieve superior\nperformance.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Unsupervised"] },
{"key": "lin2022deep", "year": "2022", "title":"Deep Unsupervised Hashing With Latent Semantic Components", "abstract": "<p>Deep unsupervised hashing has been appreciated in the regime of image\nretrieval. However, most prior arts failed to detect the semantic components\nand their relationships behind the images, which makes them lack discriminative\npower. To make up the defect, we propose a novel Deep Semantic Components\nHashing (DSCH), which involves a common sense that an image normally contains a\nbunch of semantic components with homology and co-occurrence relationships.\nBased on this prior, DSCH regards the semantic components as latent variables\nunder the Expectation-Maximization framework and designs a two-step iterative\nalgorithm with the objective of maximum likelihood of training data. Firstly,\nDSCH constructs a semantic component structure by uncovering the fine-grained\nsemantics components of images with a Gaussian Mixture Modal~(GMM), where an\nimage is represented as a mixture of multiple components, and the semantics\nco-occurrence are exploited. Besides, coarse-grained semantics components, are\ndiscovered by considering the homology relationships between fine-grained\ncomponents, and the hierarchy organization is then constructed. Secondly, DSCH\nmakes the images close to their semantic component centers at both fine-grained\nand coarse-grained levels, and also makes the images share similar semantic\ncomponents close to each other. Extensive experiments on three benchmark\ndatasets demonstrate that the proposed hierarchical semantic components indeed\nfacilitate the hashing model to achieve superior performance.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "lin2023rafic", "year": "2023", "title":"RAFIC Retrieval-augmented Few-shot Image Classification", "abstract": "<p>Few-shot image classification is the task of classifying unseen images to one\nof N mutually exclusive classes, using only a small number of training examples\nfor each class. The limited availability of these examples (denoted as K)\npresents a significant challenge to classification accuracy in some cases. To\naddress this, we have developed a method for augmenting the set of K with an\naddition set of A retrieved images. We call this system Retrieval-Augmented\nFew-shot Image Classification (RAFIC). Through a series of experiments, we\ndemonstrate that RAFIC markedly improves performance of few-shot image\nclassification across two challenging datasets. RAFIC consists of two main\ncomponents: (a) a retrieval component which uses CLIP, LAION-5B, and faiss, in\norder to efficiently retrieve images similar to the supplied images, and (b)\nretrieval meta-learning, which learns to judiciously utilize the retrieved\nimages. Code and data is available at github.com/amirziai/rafic.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "lin2024deep", "year": "2024", "title":"Deep Learning Of Binary Hash Codes For Fast Image Retrieval", "abstract": "<p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels.\nhe utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p>\n", "tags": ["ARXIV","CNN","Deep Learning","Image Retrieval","Supervised"] },
{"key": "lin2024fast", "year": "2024", "title":"Fast Supervised Hashing With Decision Trees For High-dimensional Data", "abstract": "<p>Supervised hashing aims to map the original features to\ncompact binary codes that are able to preserve label based\nsimilarity in the Hamming space. Non-linear hash functions\nhave demonstrated their advantage over linear ones due to\ntheir powerful generalization capability. In the literature,\nkernel functions are typically used to achieve non-linearity\nin hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time.\nHere we propose to use boosted decision trees for achieving\nnon-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional\ndata. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem\nand an efficient GraphCut based block search method for\nsolving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the binary\ncodes. Experiments demonstrate that our proposed method\nsignificantly outperforms most state-of-the-art methods in\nretrieval precision and training time. Especially for highdimensional data, our method is orders of magnitude faster\nthan many methods in terms of training time.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "lin2024general", "year": "2024", "title":"A General Two-step Approach To Learning-based Hashing", "abstract": "<p>Most existing approaches to hashing apply a single form of hash function, and an optimization process which\nis typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to\nrespond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose\na flexible yet simple framework that is able to accommodate different types of loss functions and hash functions.\nThis framework allows a number of existing approaches to hashing to be placed in context, and simplifies the\ndevelopment of new problem-specific hashing methods. Our framework decomposes hashing learning problem\ninto two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically\nbe formulated as binary quadratic problems, and the second step can be accomplished by training standard binary\nclassifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate\nthat the proposed framework is effective, flexible and outperforms the state-of-the-art.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "lin2024microsoft", "year": "2024", "title":"Microsoft COCO Common Objects In Context", "abstract": "<p>We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old.\nWith a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</p>\n", "tags": ["ARXIV"] },
{"key": "lin2024optimizing", "year": "2024", "title":"Optimizing Ranking Measures For Compact Binary Code Learning", "abstract": "<p>Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest—multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures.\nThe resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Deep Learning","Graph","Image Retrieval"] },
{"key": "lin2024semantics", "year": "2024", "title":"Semantics-preserving Hashing For Cross-view Retrieval", "abstract": "<p>With benefits of low storage costs and high query speeds,\nhashing methods are widely researched for efficiently retrieving large-scale data, which commonly contains multiple views, e.g. a news report with images, videos and texts.\nIn this paper, we study the problem of cross-view retrieval\nand propose an effective Semantics-Preserving Hashing\nmethod, termed SePH. Given semantic affinities of training data as supervised information, SePH transforms them\ninto a probability distribution and approximates it with tobe-learnt hash codes in Hamming space via minimizing the\nKullback-Leibler divergence. Then kernel logistic regression with a sampling strategy is utilized to learn the nonlinear projections from features in each view to the learnt\nhash codes. And for any unseen instance, predicted hash\ncodes and their corresponding output probabilities from observed views are utilized to determine its unified hash code,\nusing a novel probabilistic approach. Extensive experiments conducted on three benchmark datasets well demonstrate the effectiveness and reasonableness of SePH.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "lindgren2016leveraging", "year": "2016", "title":"Leveraging Sparsity For Efficient Submodular Data Summarization", "abstract": "<p>The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary—solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.</p>\n", "tags": ["Image Retrieval","NEURIPS","Unsupervised"] },
{"key": "lindgren2017leveraging", "year": "2017", "title":"Leveraging Sparsity For Efficient Submodular Data Summarization", "abstract": "<p>The facility location problem is widely used for summarizing large datasets\nand has additional applications in sensor placement, image retrieval, and\nclustering. One difficulty of this problem is that submodular optimization\nalgorithms require the calculation of pairwise benefits for all items in the\ndataset. This is infeasible for large problems, so recent work proposed to only\ncalculate nearest neighbor benefits. One limitation is that several strong\nassumptions were invoked to obtain provable approximation guarantees. In this\npaper we establish that these extra assumptions are not necessary—solving the\nsparsified problem will be almost optimal under the standard assumptions of the\nproblem. We then analyze a different method of sparsification that is a better\nmodel for methods such as Locality Sensitive Hashing to accelerate the nearest\nneighbor computations and extend the use of the problem to a broader family of\nsimilarities. We validate our approach by demonstrating that it rapidly\ngenerates interpretable summaries.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "linial2023elementary", "year": "2023", "title":"An Elementary Proof Of The First LP Bound On The Rate Of Binary Codes", "abstract": "<p>The asymptotic rate vs. distance problem is a long-standing fundamental\nproblem in coding theory. The best upper bound to date was given in 1977 and\nhas received since then numerous proofs and interpretations. Here we provide a\nnew, elementary proof of this bound based on counting walks in the Hamming\ncube.</p>\n", "tags": ["ARXIV"] },
{"key": "lins2003phorma", "year": "2003", "title":"PHORMA Perfectly Hashable Order Restricted Multidimensional Arrays", "abstract": "<p>In this paper we propose a simple and efficient data structure yielding a\nperfect hashing of quite general arrays. The data structure is named phorma,\nwhich is an acronym for perfectly hashable order restricted multidimensional\narray.\n  Keywords: Perfect hash function, Digraph, Implicit enumeration,\nNijenhuis-Wilf combinatorial family.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "liong2024cross", "year": "2024", "title":"Cross-modal Deep Variational Hashing", "abstract": "<p>In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval. Unlike existing cross-modal hashing methods\nwhich learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural\nnetwork to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained. We then design the modality-specific neural networks in a probabilistic manner where we model a latent\nvariable as close as possible from the inferred binary codes,\nwhich is approximated by a posterior distribution regularized by a known prior. Experimental results on three benchmark datasets show the efficacy of the proposed approach.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "liong2024deep", "year": "2024", "title":"Deep Variational And Structural Hashing", "abstract": "<p>In this paper, we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features, we develop a probabilistic framework to infer latent feature representation inside the network. Then, we design a struct layer rather than a bottleneck hash layer, to obtain binary codes through a simple encoding procedure. By doing these, we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval, we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically, we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "liu2004investigation", "year": "2004", "title":"An Investigation Of Practical Approximate Nearest Neighbor Algorithms", "abstract": "<p>This paper concerns approximate nearest neighbor searching algorithms,          which have become increasingly important, especially in high dimen-          sional perception areas such as computer vision, with dozens of publica-          tions in recent years. Much of this enthusiasm is due to a successful new          approximate nearest neighbor approach called Locality Sensitive Hash-          ing (LSH). In this paper we ask the question: can earlier spatial data          structure approaches to exact nearest neighbor, such as metric trees, be          altered to provide approximate answers to proximity queries and if so,          how? We introduce a new kind of metric tree that allows overlap: certain          datapoints may appear in both the children of a parent. We also intro-          duce new approximate k-NN search algorithms on this structure. We          show why these structures should be able to exploit the same random-          projection-based approximations that LSH enjoys, but with a simpler al-          gorithm and perhaps with greater efficiency. We then provide a detailed          empirical evaluation on five large, high dimensional datasets which show          up to 31-fold accelerations over LSH. This result holds true throughout          the spectrum of approximation levels.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "liu2012compact", "year": "2012", "title":"Compact Hyperplane Hashing With Bilinear Functions", "abstract": "<p>Hyperplane hashing aims at rapidly searching nearest points to a hyperplane,\nand has shown practical impact in scaling up active learning with SVMs.\nUnfortunately, the existing randomized methods need long hash codes to achieve\nreasonable search accuracy and thus suffer from reduced search speed and large\nmemory overhead. To this end, this paper proposes a novel hyperplane hashing\ntechnique which yields compact hash codes. The key idea is the bilinear form of\nthe proposed hash functions, which leads to higher collision probability than\nthe existing hyperplane hash functions when using random projections. To\nfurther increase the performance, we propose a learning based framework in\nwhich the bilinear functions are directly learned from the data. This results\nin short yet discriminative codes, and also boosts the search performance over\nthe random projection based solutions. Large-scale active learning experiments\ncarried out on two datasets with up to one million samples demonstrate the\noverall superiority of the proposed approach.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "liu2014discrete", "year": "2014", "title":"Discrete Graph Hashing", "abstract": "<p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes.</p>\n", "tags": ["Graph","NEURIPS","Unsupervised"] },
{"key": "liu2015accelerated", "year": "2015", "title":"Accelerated Distance Computation With Encoding Tree For High Dimensional Data", "abstract": "<p>We propose a novel distance to calculate distance between high dimensional\nvector pairs, utilizing vector quantization generated encodings. Vector\nquantization based methods are successful in handling large scale high\ndimensional data. These methods compress vectors into short encodings, and\nallow efficient distance computation between an uncompressed vector and\ncompressed dataset without decompressing explicitly. However for large\ndatasets, these distance computing methods perform excessive computations. We\navoid excessive computations by storing the encodings on an Encoding\nTree(E-Tree), interestingly the memory consumption is also lowered. We also\npropose Encoding Forest(E-Forest) to further lower the computation cost. E-Tree\nand E-Forest is compatible with various existing quantization-based methods. We\nshow by experiments our methods speed-up distance computing for high\ndimensional data drastically, and various existing algorithms can benefit from\nour methods.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "liu2015hclae", "year": "2015", "title":"HCLAE High Capacity Locally Aggregating Encodings For Approximate Nearest Neighbor Search", "abstract": "<p>Vector quantization-based approaches are successful to solve Approximate\nNearest Neighbor (ANN) problems which are critical to many applications. The\nidea is to generate effective encodings to allow fast distance approximation.\nWe propose quantization-based methods should partition the data space finely\nand exhibit locality of the dataset to allow efficient non-exhaustive search.\nIn this paper, we introduce the concept of High Capacity Locality Aggregating\nEncodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learn\nHCLAE by a simulated annealing procedure. The quantization error is lower than\nother state-of-the-art. The algorithms of DA can be easily extended to an\nonline learning scheme, allowing effective handle of large scale data. Further,\nwe propose Aggregating-Tree (A-Tree), a non-exhaustive search method using\nHCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-up\non ANN-Search tasks, compared to the state-of-the-art.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "liu2015improved", "year": "2015", "title":"Improved Residual Vector Quantization For High-dimensional Approximate Nearest Neighbor Search", "abstract": "<p>Quantization methods have been introduced to perform large scale approximate\nnearest search tasks. Residual Vector Quantization (RVQ) is one of the\neffective quantization methods. RVQ uses a multi-stage codebook learning scheme\nto lower the quantization error stage by stage. However, there are two major\nlimitations for RVQ when applied to on high-dimensional approximate nearest\nneighbor search: 1. The performance gain diminishes quickly with added stages.</p>\n<ol>\n  <li>Encoding a vector with RVQ is actually NP-hard. In this paper, we propose an\nimproved residual vector quantization (IRVQ) method, our IRVQ learns codebook\nwith a hybrid method of subspace clustering and warm-started k-means on each\nstage to prevent performance gain from dropping, and uses a multi-path encoding\nscheme to encode a vector with lower distortion. Experimental results on the\nbenchmark datasets show that our method gives substantially improves RVQ and\ndelivers better performance compared to the state-of-the-art.</li>\n</ol>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "liu2015indexing", "year": "2015", "title":"Indexing Of CNN Features For Large Scale Image Search", "abstract": "<p>The convolutional neural network (CNN) features can give a good description\nof image content, which usually represent images with unique global vectors.\nAlthough they are compact compared to local descriptors, they still cannot\nefficiently deal with large-scale image retrieval due to the cost of the linear\nincremental computation and storage. To address this issue, we build a simple\nbut effective indexing framework based on inverted table, which significantly\ndecreases both the search time and memory usage. In addition, several\nstrategies are fully investigated under an indexing framework to adapt it to\nCNN features and compensate for quantization errors. First, we use multiple\nassignment for the query and database images to increase the probability of\nrelevant images’ co-existing in the same Voronoi cells obtained via the\nclustering algorithm. Then, we introduce embedding codes to further improve\nprecision by removing false matches during a search. We demonstrate that by\nusing hashing schemes to calculate the embedding codes and by changing the\nranking rule, indexing framework speeds can be greatly improved. Extensive\nexperiments conducted on several unsupervised and supervised benchmarks support\nthese results and the superiority of the proposed indexing framework. We also\nprovide a fair comparison between the popular CNN features.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Quantisation","Supervised"] },
{"key": "liu2015learning", "year": "2015", "title":"Learning Better Encoding For Approximate Nearest Neighbor Search With Dictionary Annealing", "abstract": "<p>We introduce a novel dictionary optimization method for high-dimensional\nvector quantization employed in approximate nearest neighbor (ANN) search.\nVector quantization methods first seek a series of dictionaries, then\napproximate each vector by a sum of elements selected from these dictionaries.\nAn optimal series of dictionaries should be mutually independent, and each\ndictionary should generate a balanced encoding for the target dataset. Existing\nmethods did not explicitly consider this. To achieve these goals along with\nminimizing the quantization error (residue), we propose a novel dictionary\noptimization method called <em>Dictionary Annealing</em> that alternatively\n“heats up” a single dictionary by generating an intermediate dataset with\nresidual vectors, “cools down” the dictionary by fitting the intermediate\ndataset, then extracts the new residual vectors for the next iteration. Better\ncodes can be learned by DA for the ANN search tasks. DA is easily implemented\non GPU to utilize the latest computing technology, and can easily extended to\nan online dictionary learning scheme. We show by experiments that our optimized\ndictionaries substantially reduce the overall quantization error. Jointly used\nwith residual vector quantization, our optimized dictionaries lead to a better\napproximate nearest neighbor search performance compared to the\nstate-of-the-art methods.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "liu2015projection", "year": "2015", "title":"Projection Bank From High-dimensional Data To Medium-length Binary Codes", "abstract": "<p>Recently, very high-dimensional feature representations, e.g., Fisher Vector,\nhave achieved excellent performance for visual recognition and retrieval.\nHowever, these lengthy representations always cause extremely heavy\ncomputational and storage costs and even become unfeasible in some large-scale\napplications. A few existing techniques can transfer very high-dimensional data\ninto binary codes, but they still require the reduced code length to be\nrelatively long to maintain acceptable accuracies. To target a better balance\nbetween computational efficiency and accuracies, in this paper, we propose a\nnovel embedding method called Binary Projection Bank (BPB), which can\neffectively reduce the very high-dimensional representations to\nmedium-dimensional binary codes without sacrificing accuracies. Instead of\nusing conventional single linear or bilinear projections, the proposed method\nlearns a bank of small projections via the max-margin constraint to optimally\npreserve the intrinsic data similarity. We have systematically evaluated the\nproposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing\ncompetitive retrieval and recognition accuracies compared with state-of-the-art\napproaches, but with a significantly smaller memory footprint and lower coding\ncomplexity.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "liu2016accurate", "year": "2016", "title":"Accurate Deep Representation Quantization With Gradient Snapping Layer For Similarity Search", "abstract": "<p>Recent advance of large scale similarity search involves using deeply learned\nrepresentations to improve the search accuracy and use vector quantization\nmethods to increase the search speed. However, how to learn deep\nrepresentations that strongly preserve similarities between data pairs and can\nbe accurately quantized via vector quantization remains a challenging task.\nExisting methods simply leverage quantization loss and similarity loss, which\nresult in unexpectedly biased back-propagating gradients and affect the search\nperformances. To this end, we propose a novel gradient snapping layer (GSL) to\ndirectly regularize the back-propagating gradient towards a neighboring\ncodeword, the generated gradients are un-biased for reducing similarity loss\nand also propel the learned representations to be accurately quantized. Joint\ndeep representation and vector quantization learning can be easily performed by\nalternatively optimize the quantization codebook and the deep neural network.\nThe proposed framework is compatible with various existing vector quantization\napproaches. Experimental results demonstrate that the proposed framework is\neffective, flexible and outperforms the state-of-the-art large scale similarity\nsearch methods.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "liu2016dual", "year": "2016", "title":"Dual Purpose Hashing", "abstract": "<p>Recent years have seen more and more demand for a unified framework to\naddress multiple realistic image retrieval tasks concerning both category and\nattributes. Considering the scale of modern datasets, hashing is favorable for\nits low complexity. However, most existing hashing methods are designed to\npreserve one single kind of similarity, thus improper for dealing with the\ndifferent tasks simultaneously. To overcome this limitation, we propose a new\nhashing method, named Dual Purpose Hashing (DPH), which jointly preserves the\ncategory and attribute similarities by exploiting the Convolutional Neural\nNetwork (CNN) models to hierarchically capture the correlations between\ncategory and attributes. Since images with both category and attribute labels\nare scarce, our method is designed to take the abundant partially labelled\nimages on the Internet as training inputs. With such a framework, the binary\ncodes of new-coming images can be readily obtained by quantizing the network\noutputs of a binary-like layer, and the attributes can be recovered from the\ncodes easily. Experiments on two large-scale datasets show that our dual\npurpose hash codes can achieve comparable or even better performance than those\nstate-of-the-art methods specifically designed for each individual retrieval\ntask, while being more compact than the compared methods.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Independent"] },
{"key": "liu2016generalized", "year": "2016", "title":"Generalized Residual Vector Quantization For Large Scale Data", "abstract": "<p>Vector quantization is an essential tool for tasks involving large scale\ndata, for example, large scale similarity search, which is crucial for\ncontent-based information retrieval and analysis. In this paper, we propose a\nnovel vector quantization framework that iteratively minimizes quantization\nerror. First, we provide a detailed review on a relevant vector quantization\nmethod named \\textit{residual vector quantization} (RVQ). Next, we propose\n\\textit{generalized residual vector quantization} (GRVQ) to further improve\nover RVQ. Many vector quantization methods can be viewed as the special cases\nof our proposed framework. We evaluate GRVQ on several large scale benchmark\ndatasets for large scale search, classification and object retrieval. We\ncompared GRVQ with existing methods in detail. Extensive experiments\ndemonstrate our GRVQ framework substantially outperforms existing methods in\nterm of quantization accuracy and computation efficiency.</p>\n", "tags": ["ARXIV","Quantisation","Supervised","Survey Paper"] },
{"key": "liu2016ordinal", "year": "2016", "title":"Ordinal Constrained Binary Code Learning For Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed extensive attention in binary code learning,\na.k.a. hashing, for nearest neighbor search problems. It has been seen that\nhigh-dimensional data points can be quantized into binary codes to give an\nefficient similarity approximation via Hamming distance. Among existing\nschemes, ranking-based hashing is recent promising that targets at preserving\nordinal relations of ranking in the Hamming space to minimize retrieval loss.\nHowever, the size of the ranking tuples, which shows the ordinal relations, is\nquadratic or cubic to the size of training samples. By given a large-scale\ntraining data set, it is very expensive to embed such ranking tuples in binary\ncode learning. Besides, it remains a dificulty to build ranking tuples\nefficiently for most ranking-preserving hashing, which are deployed over an\nordinal graph-based setting. To handle these problems, we propose a novel\nranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH),\nwhich efficiently learns the optimal hashing functions with a graph-based\napproximation to embed the ordinal relations. The core idea is to reduce the\nsize of ordinal graph with ordinal constraint projection, which preserves the\nordinal relations through a small data set (such as clusters or random\nsamples). In particular, to learn such hash functions effectively, we further\nrelax the discrete constraints and design a specific stochastic gradient decent\nalgorithm for optimization. Experimental results on three large-scale visual\nsearch benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the\nproposed OCH method can achieve superior performance over the state-of-the-arts\napproaches.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "liu2016supervised", "year": "2016", "title":"Supervised Matrix Factorization For Cross-modality Hashing", "abstract": "<p>Matrix factorization has been recently utilized for the task of multi-modal\nhashing for cross-modality visual search, where basis functions are learned to\nmap data from different modalities to the same Hamming embedding. In this\npaper, we propose a novel cross-modality hashing algorithm termed Supervised\nMatrix Factorization Hashing (SMFH) which tackles the multi-modal hashing\nproblem with a collective non-matrix factorization across the different\nmodalities. In particular, SMFH employs a well-designed binary code learning\nalgorithm to preserve the similarities among multi-modal original features\nthrough a graph regularization. At the same time, semantic labels, when\navailable, are incorporated into the learning procedure. We conjecture that all\nthese would facilitate to preserve the most relevant information during the\nbinary quantization process, and hence improve the retrieval accuracy. We\ndemonstrate the superior performance of SMFH on three cross-modality visual\nsearch benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with\nquantitative comparison to various state-of-the-art methods</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Quantisation","Supervised"] },
{"key": "liu2017deep", "year": "2017", "title":"Deep Hashing With Category Mask For Fast Video Retrieval", "abstract": "<p>This paper proposes an end-to-end deep hashing framework with category mask\nfor fast video retrieval. We train our network in a supervised way by fully\nexploiting inter-class diversity and intra-class identity. Classification loss\nis optimized to maximize inter-class diversity, while intra-pair is introduced\nto learn representative intra-class identity. We investigate the binary bits\ndistribution related to categories and find out that the effectiveness of\nbinary bits is highly correlated with data categories, and some bits may\ndegrade classification performance of some categories. We then design hash code\ngeneration scheme with category mask to filter out bits with negative\ncontribution. Experimental results demonstrate the proposed method outperforms\nseveral state-of-the-arts under various evaluation metrics on public datasets.</p>\n", "tags": ["ARXIV","Supervised","Video Retrieval"] },
{"key": "liu2017end", "year": "2017", "title":"End-to-end Binary Representation Learning Via Direct Binary Embedding", "abstract": "<p>Learning binary representation is essential to large-scale computer vision\ntasks. Most existing algorithms require a separate quantization constraint to\nlearn effective hashing functions. In this work, we present Direct Binary\nEmbedding (DBE), a simple yet very effective algorithm to learn binary\nrepresentation in an end-to-end fashion. By appending an ingeniously designed\nDBE layer to the deep convolutional neural network (DCNN), DBE learns binary\ncode directly from the continuous DBE layer activation without quantization\nerror. By employing the deep residual network (ResNet) as DCNN component, DBE\ncaptures rich semantics from images. Furthermore, in the effort of handling\nmultilabel images, we design a joint cross entropy loss that includes both\nsoftmax cross entropy and weighted binary cross entropy in consideration of the\ncorrelation and independence of labels, respectively. Extensive experiments\ndemonstrate the significant superiority of DBE over state-of-the-art methods on\ntasks of natural object recognition, image retrieval and image annotation.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "liu2018discriminative", "year": "2018", "title":"Discriminative Cross-view Binary Representation Learning", "abstract": "<p>Learning compact representation is vital and challenging for large scale\nmultimedia data. Cross-view/cross-modal hashing for effective binary\nrepresentation learning has received significant attention with exponentially\ngrowing availability of multimedia content. Most existing cross-view hashing\nalgorithms emphasize the similarities in individual views, which are then\nconnected via cross-view similarities. In this work, we focus on the\nexploitation of the discriminative information from different views, and\npropose an end-to-end method to learn semantic-preserving and discriminative\nbinary representation, dubbed Discriminative Cross-View Hashing (DCVH), in\nlight of learning multitasking binary representation for various tasks\nincluding cross-view retrieval, image-to-image retrieval, and image\nannotation/tagging. The proposed DCVH has the following key components. First,\nit uses convolutional neural network (CNN) based nonlinear hashing functions\nand multilabel classification for both images and texts simultaneously. Such\nhashing functions achieve effective continuous relaxation during training\nwithout explicit quantization loss by using Direct Binary Embedding (DBE)\nlayers. Second, we propose an effective view alignment via Hamming distance\nminimization, which is efficiently accomplished by bit-wise XOR operation.\nExtensive experiments on two image-text benchmark datasets demonstrate that\nDCVH outperforms state-of-the-art cross-view hashing algorithms as well as\nsingle-view image hashing algorithms. In addition, DCVH can provide competitive\nperformance for image annotation/tagging.</p>\n", "tags": ["CNN","Cross Modal","Image Retrieval","Quantisation","Supervised"] },
{"key": "liu2018finding", "year": "2018", "title":"On Finding Quantum Multi-collisions", "abstract": "<p>A \\(k\\)-collision for a compressing hash function \\(H\\) is a set of \\(k\\) distinct\ninputs that all map to the same output. In this work, we show that for any\nconstant \\(k\\), \\(\\Theta\\left(N^{\\frac{1}{2}(1-\\frac{1}{2^k-1})}\\right)\\) quantum\nqueries are both necessary and sufficient to achieve a \\(k\\)-collision with\nconstant probability. This improves on both the best prior upper bound\n(Hosoyamada et al., ASIACRYPT 2017) and provides the first non-trivial lower\nbound, completely resolving the problem.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "liu2018fusion", "year": "2018", "title":"Fusion Hashing A General Framework For Self-improvement Of Hashing", "abstract": "<p>Hashing has been widely used for efficient similarity search based on its\nquery and storage efficiency. To obtain better precision, most studies focus on\ndesigning different objective functions with different constraints or penalty\nterms that consider neighborhood information. In this paper, in contrast to\nexisting hashing methods, we propose a novel generalized framework called\nfusion hashing (FH) to improve the precision of existing hashing methods\nwithout adding new constraints or penalty terms. In the proposed FH, given an\nexisting hashing method, we first execute it several times to get several\ndifferent hash codes for a set of training samples. We then propose two novel\nfusion strategies that combine these different hash codes into one set of final\nhash codes. Based on the final hash codes, we learn a simple linear hash\nfunction for the samples that can significantly improve model precision. In\ngeneral, the proposed FH can be adopted in existing hashing method and achieve\nmore precise and stable performance compared to the original hashing method\nwith little extra expenditure in terms of time and space. Extensive experiments\nwere performed based on three benchmark datasets and the results demonstrate\nthe superior performance of the proposed framework</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "liu2018mtfh", "year": "2018", "title":"MTFH A Matrix Tri-factorization Hashing Framework For Efficient Cross-modal Retrieval", "abstract": "<p>Hashing has recently sparked a great revolution in cross-modal retrieval\nbecause of its low storage cost and high query speed. Recent cross-modal\nhashing methods often learn unified or equal-length hash codes to represent the\nmulti-modal data and make them intuitively comparable. However, such unified or\nequal-length hash representations could inherently sacrifice their\nrepresentation scalability because the data from different modalities may not\nhave one-to-one correspondence and could be encoded more efficiently by\ndifferent hash codes of unequal lengths. To mitigate these problems, this paper\nexploits a related and relatively unexplored problem: encode the heterogeneous\ndata with varying hash lengths and generalize the cross-modal retrieval in\nvarious challenging scenarios. To this end, a generalized and flexible\ncross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH),\nis proposed to work seamlessly in various settings including paired or unpaired\nmulti-modal data, and equal or varying hash length encoding scenarios. More\nspecifically, MTFH exploits an efficient objective function to flexibly learn\nthe modality-specific hash codes with different length settings, while\nsynchronously learning two semantic correlation matrices to semantically\ncorrelate the different hash representations for heterogeneous data comparable.\nAs a result, the derived hash codes are more semantically meaningful for\nvarious challenging cross-modal retrieval tasks. Extensive experiments\nevaluated on public benchmark datasets highlight the superiority of MTFH under\nvarious retrieval scenarios and show its competitive performance with the\nstate-of-the-arts.</p>\n", "tags": ["Cross Modal","Independent"] },
{"key": "liu2019compositional", "year": "2019", "title":"Compositional Coding For Collaborative Filtering", "abstract": "<p>Efficiency is crucial to the online recommender systems. Representing users\nand items as binary vectors for Collaborative Filtering (CF) can achieve fast\nuser-item affinity computation in the Hamming space, in recent years, we have\nwitnessed an emerging research effort in exploiting binary hashing techniques\nfor CF methods. However, CF with binary codes naturally suffers from low\naccuracy due to limited representation capability in each bit, which impedes it\nfrom modeling complex structure of the data.\n  In this work, we attempt to improve the efficiency without hurting the model\nperformance by utilizing both the accuracy of real-valued vectors and the\nefficiency of binary codes to represent users/items. In particular, we propose\nthe Compositional Coding for Collaborative Filtering (CCCF) framework, which\nnot only gains better recommendation efficiency than the state-of-the-art\nbinarized CF approaches but also achieves even higher accuracy than the\nreal-valued CF method. Specifically, CCCF innovatively represents each\nuser/item with a set of binary vectors, which are associated with a sparse\nreal-value weight vector. Each value of the weight vector encodes the\nimportance of the corresponding binary vector to the user/item. The continuous\nweight vectors greatly enhances the representation capability of binary codes,\nand its sparsity guarantees the processing speed. Furthermore, an integer\nweight approximation scheme is proposed to further accelerate the speed. Based\non the CCCF framework, we design an efficient discrete optimization algorithm\nto learn its parameters. Extensive experiments on three real-world datasets\nshow that our method outperforms the state-of-the-art binarized CF methods\n(even achieves better performance than the real-valued CF method) by a large\nmargin in terms of both recommendation accuracy and efficiency.</p>\n", "tags": ["ARXIV"] },
{"key": "liu2019cross", "year": "2019", "title":"Cross-modal Zero-shot Hashing", "abstract": "<p>Hashing has been widely studied for big data retrieval due to its low storage\ncost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing\nmodel that is trained using only samples from seen categories, but can\ngeneralize well to samples of unseen categories. ZSH generally uses category\nattributes to seek a semantic embedding space to transfer knowledge from seen\ncategories to unseen ones. As a result, it may perform poorly when labeled data\nare insufficient. ZSH methods are mainly designed for single-modality data,\nwhich prevents their application to the widely spread multi-modal data. On the\nother hand, existing cross-modal hashing solutions assume that all the\nmodalities share the same category labels, while in practice the labels of\ndifferent data modalities may be different. To address these issues, we propose\na general Cross-modal Zero-shot Hashing (CZHash) solution to effectively\nleverage unlabeled and labeled multi-modality data with different label spaces.\nCZHash first quantifies the composite similarity between instances using label\nand feature information. It then defines an objective function to achieve deep\nfeature learning compatible with the composite similarity preserving, category\nattribute space learning, and hashing coding function learning. CZHash further\nintroduces an alternative optimization procedure to jointly optimize these\nlearning objectives. Experiments on benchmark multi-modal datasets show that\nCZHash significantly outperforms related representative hashing approaches both\non effectiveness and adaptability.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "liu2019deep", "year": "2019", "title":"Deep Triplet Quantization", "abstract": "<p>Deep hashing establishes efficient and effective image retrieval by\nend-to-end learning of deep representations and hash codes from similarity\ndata. We present a compact coding solution, focusing on deep learning to\nquantization approach that has shown superior performance over hashing\nsolutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ),\na novel approach to learning deep quantization models from the similarity\ntriplets. To enable more effective triplet training, we design a new triplet\nselection approach, Group Hard, that randomly selects hard triplets in each\nimage group. To generate compact binary codes, we further apply a triplet\nquantization with weak orthogonality during triplet training. The quantization\nloss reduces the codebook redundancy and enhances the quantizability of deep\nrepresentations through back-propagation. Extensive experiments demonstrate\nthat DTQ can generate high-quality and compact binary codes, which yields\nstate-of-the-art image retrieval performance on three benchmark datasets,\nNUS-WIDE, CIFAR-10, and MS-COCO.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Independent","Quantisation"] },
{"key": "liu2019mutual", "year": "2019", "title":"Mutual Linear Regression-based Discrete Hashing", "abstract": "<p>Label information is widely used in hashing methods because of its\neffectiveness of improving the precision. The existing hashing methods always\nuse two different projections to represent the mutual regression between hash\ncodes and class labels. In contrast to the existing methods, we propose a novel\nlearning-based hashing method termed stable supervised discrete hashing with\nmutual linear regression (S2DHMLR) in this study, where only one stable\nprojection is used to describe the linear correlation between hash codes and\ncorresponding labels. To the best of our knowledge, this strategy has not been\nused for hashing previously. In addition, we further use a boosting strategy to\nimprove the final performance of the proposed method without adding extra\nconstraints and with little extra expenditure in terms of time and space.\nExtensive experiments conducted on three image benchmarks demonstrate the\nsuperior performance of the proposed method.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "liu2019optimal", "year": "2019", "title":"Optimal Projection Guided Transfer Hashing For Image Retrieval", "abstract": "<p>Recently, learning to hash has been widely studied for image retrieval thanks\nto the computation and storage efficiency of binary codes. For most existing\nlearning to hash methods, sufficient training images are required and used to\nlearn precise hashing codes. However, in some real-world applications, there\nare not always sufficient training images in the domain of interest. In\naddition, some existing supervised approaches need a amount of labeled data,\nwhich is an expensive process in term of time, label and human expertise. To\nhandle such problems, inspired by transfer learning, we propose a simple yet\neffective unsupervised hashing method named Optimal Projection Guided Transfer\nHashing (GTH) where we borrow the images of other different but related domain\ni.e., source domain to help learn precise hashing codes for the domain of\ninterest i.e., target domain. Besides, we propose to seek for the maximum\nlikelihood estimation (MLE) solution of the hashing functions of target and\nsource domains due to the domain gap. Furthermore,an alternating optimization\nmethod is adopted to obtain the two projections of target and source domains\nsuch that the domain hashing disparity is reduced gradually. Extensive\nexperiments on various benchmark databases verify that our method outperforms\nmany state-of-the-art learning to hash methods. The implementation details are\navailable at https://github.com/liuji93/GTH.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Supervised"] },
{"key": "liu2019query", "year": "2019", "title":"Query-adaptive Hash Code Ranking For Large-scale Multi-view Visual Search", "abstract": "<p>Hash based nearest neighbor search has become attractive in many\napplications. However, the quantization in hashing usually degenerates the\ndiscriminative power when using Hamming distance ranking. Besides, for\nlarge-scale visual search, existing hashing methods cannot directly support the\nefficient search over the data with multiple sources, and while the literature\nhas shown that adaptively incorporating complementary information from diverse\nsources or views can significantly boost the search performance. To address the\nproblems, this paper proposes a novel and generic approach to building multiple\nhash tables with multiple views and generating fine-grained ranking results at\nbitwise and tablewise levels. For each hash table, a query-adaptive bitwise\nweighting is introduced to alleviate the quantization loss by simultaneously\nexploiting the quality of hash functions and their complement for nearest\nneighbor search. From the tablewise aspect, multiple hash tables are built for\ndifferent data views as a joint index, over which a query-specific rank fusion\nis proposed to rerank all results from the bitwise ranking by diffusing in a\ngraph. Comprehensive experiments on image search over three well-known\nbenchmarks show that the proposed method achieves up to 17.11% and 20.28%\nperformance gains on single and multiple table search over state-of-the-art\nmethods.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Independent","Quantisation"] },
{"key": "liu2019ranking", "year": "2019", "title":"Ranking-based Deep Cross-modal Hashing", "abstract": "<p>Cross-modal hashing has been receiving increasing interests for its low\nstorage cost and fast query speed in multi-modal data retrievals. However, most\nexisting hashing methods are based on hand-crafted or raw level features of\nobjects, which may not be optimally compatible with the coding process.\nBesides, these hashing methods are mainly designed to handle simple pairwise\nsimilarity. The complex multilevel ranking semantic structure of instances\nassociated with multiple labels has not been well explored yet. In this paper,\nwe propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH\nfirstly uses the feature and label information of data to derive a\nsemi-supervised semantic ranking list. Next, to expand the semantic\nrepresentation power of hand-crafted features, RDCMH integrates the semantic\nranking information into deep cross-modal hashing and jointly optimizes the\ncompatible parameters of deep feature representations and of hashing functions.\nExperiments on real multi-modal datasets show that RDCMH outperforms other\ncompetitive baselines and achieves the state-of-the-art performance in\ncross-modal retrieval applications.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "liu2019weakly", "year": "2019", "title":"Weakly-paired Cross-modal Hashing", "abstract": "<p>Hashing has been widely adopted for large-scale data retrieval in many\ndomains, due to its low storage cost and high retrieval speed. Existing\ncross-modal hashing methods optimistically assume that the correspondence\nbetween training samples across modalities are readily available. This\nassumption is unrealistic in practical applications. In addition, these methods\ngenerally require the same number of samples across different modalities, which\nrestricts their flexibility. We propose a flexible cross-modal hashing approach\n(Flex-CMH) to learn effective hashing codes from weakly-paired data, whose\ncorrespondence across modalities are partially (or even totally) unknown.\nFlexCMH first introduces a clustering-based matching strategy to explore the\nlocal structure of each cluster, and thus to find the potential correspondence\nbetween clusters (and samples therein) across modalities. To reduce the impact\nof an incomplete correspondence, it jointly optimizes in a unified objective\nfunction the potential correspondence, the cross-modal hashing functions\nderived from the correspondence, and a hashing quantitative loss. An\nalternative optimization technique is also proposed to coordinate the\ncorrespondence and hash functions, and to reinforce the reciprocal effects of\nthe two objectives. Experiments on publicly multi-modal datasets show that\nFlexCMH achieves significantly better results than state-of-the-art methods,\nand it indeed offers a high degree of flexibility for practical cross-modal\nhashing tasks.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "liu2020reinforcing", "year": "2020", "title":"Reinforcing Short-length Hashing", "abstract": "<p>Due to the compelling efficiency in retrieval and storage,\nsimilarity-preserving hashing has been widely applied to approximate nearest\nneighbor search in large-scale image retrieval. However, existing methods have\npoor performance in retrieval using an extremely short-length hash code due to\nweak ability of classification and poor distribution of hash bit. To address\nthis issue, in this study, we propose a novel reinforcing short-length hashing\n(RSLH). In this proposed RSLH, mutual reconstruction between the hash\nrepresentation and semantic labels is performed to preserve the semantic\ninformation. Furthermore, to enhance the accuracy of hash representation, a\npairwise similarity matrix is designed to make a balance between accuracy and\ntraining expenditure on memory. In addition, a parameter boosting strategy is\nintegrated to reinforce the precision with hash bits fusion. Extensive\nexperiments on three large-scale image benchmarks demonstrate the superior\nperformance of RSLH under various short-length hashing scenarios.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "liu2020shuffle", "year": "2020", "title":"Shuffle And Learn Minimizing Mutual Information For Unsupervised Hashing", "abstract": "<p>Unsupervised binary representation allows fast data retrieval without any\nannotations, enabling practical application like fast person re-identification\nand multimedia retrieval. It is argued that conflicts in binary space are one\nof the major barriers to high-performance unsupervised hashing as current\nmethods failed to capture the precise code conflicts in the full domain. A\nnovel relaxation method called Shuffle and Learn is proposed to tackle code\nconflicts in the unsupervised hash. Approximated derivatives for joint\nprobability and the gradients for the binary layer are introduced to bridge the\nupdate from the hash to the input. Proof on \\(\\epsilon\\)-Convergence of joint\nprobability with approximated derivatives is provided to guarantee the\npreciseness on update applied on the mutual information. The proposed algorithm\nis carried out with iterative global updates to minimize mutual information,\ndiverging the code before regular unsupervised optimization. Experiments\nsuggest that the proposed method can relax the code optimization from local\noptimum and help to generate binary representations that are more\ndiscriminative and informative without any annotations. Performance benchmarks\non image retrieval with the unsupervised binary code are conducted on three\nopen datasets, and the model achieves state-of-the-art accuracy on image\nretrieval task for all those datasets. Datasets and reproducible code are\nprovided.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "liu2021fddh", "year": "2021", "title":"FDDH Fast Discriminative Discrete Hashing For Large-scale Cross-modal Retrieval", "abstract": "<p>Cross-modal hashing, favored for its effectiveness and efficiency, has\nreceived wide attention to facilitating efficient retrieval across different\nmodalities. Nevertheless, most existing methods do not sufficiently exploit the\ndiscriminative power of semantic information when learning the hash codes,\nwhile often involving time-consuming training procedure for handling the\nlarge-scale dataset. To tackle these issues, we formulate the learning of\nsimilarity-preserving hash codes in terms of orthogonally rotating the semantic\ndata so as to minimize the quantization loss of mapping such data to hamming\nspace, and propose an efficient Fast Discriminative Discrete Hashing (FDDH)\napproach for large-scale cross-modal retrieval. More specifically, FDDH\nintroduces an orthogonal basis to regress the targeted hash codes of training\nexamples to their corresponding semantic labels, and utilizes “-dragging\ntechnique to provide provable large semantic margins. Accordingly, the\ndiscriminative power of semantic information can be explicitly captured and\nmaximized. Moreover, an orthogonal transformation scheme is further proposed to\nmap the nonlinear embedding data into the semantic subspace, which can well\nguarantee the semantic consistency between the data feature and its semantic\nrepresentation. Consequently, an efficient closed form solution is derived for\ndiscriminative hash code learning, which is very computationally efficient. In\naddition, an effective and stable online learning strategy is presented for\noptimizing modality-specific projection functions, featuring adaptivity to\ndifferent training sizes and streaming data. The proposed FDDH approach\ntheoretically approximates the bi-Lipschitz continuity, runs sufficiently fast,\nand also significantly improves the retrieval performance over the\nstate-of-the-art methods. The source code is released at:\nhttps://github.com/starxliu/FDDH.</p>\n", "tags": ["Cross Modal","Has Code","Independent","Quantisation","Streaming Data"] },
{"key": "liu2021ternary", "year": "2021", "title":"Ternary Hashing", "abstract": "<p>This paper proposes a novel ternary hash encoding for learning to hash\nmethods, which provides a principled more efficient coding scheme with\nperformances better than those of the state-of-the-art binary hashing\ncounterparts. Two kinds of axiomatic ternary logic, Kleene logic and\n{\\L}ukasiewicz logic are adopted to calculate the Ternary Hamming Distance\n(THD) for both the learning/encoding and testing/querying phases. Our work\ndemonstrates that, with an efficient implementation of ternary logic on\nstandard binary machines, the proposed ternary hashing is compared favorably to\nthe binary hashing methods with consistent improvements of retrieval mean\naverage precision (mAP) ranging from 1\\% to 5.9\\% as shown in CIFAR10, NUS-WIDE\nand ImageNet100 datasets.</p>\n", "tags": ["ARXIV"] },
{"key": "liu2022prototype", "year": "2022", "title":"Prototype-based Layered Federated Cross-modal Hashing", "abstract": "<p>Recently, deep cross-modal hashing has gained increasing attention. However,\nin many practical cases, data are distributed and cannot be collected due to\nprivacy concerns, which greatly reduces the cross-modal hashing performance on\neach client. And due to the problems of statistical heterogeneity, model\nheterogeneity, and forcing each client to accept the same parameters, applying\nfederated learning to cross-modal hash learning becomes very tricky. In this\npaper, we propose a novel method called prototype-based layered federated\ncross-modal hashing. Specifically, the prototype is introduced to learn the\nsimilarity between instances and classes on server, reducing the impact of\nstatistical heterogeneity (non-IID) on different clients. And we monitor the\ndistance between local and global prototypes to further improve the\nperformance. To realize personalized federated learning, a hypernetwork is\ndeployed on server to dynamically update different layers’ weights of local\nmodel. Experimental results on benchmark datasets show that our method\noutperforms state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "liu2023binary", "year": "2023", "title":"Binary Code Similarity Detection", "abstract": "<p>Binary code similarity detection is to detect the similarity of code at\nbinary (assembly) level without source code. Existing works have their\nlimitations when dealing with mutated binary code generated by different\ncompiling options. In this paper, we propose a novel approach to addressing\nthis problem. By inspecting the binary code, we found that generally, within a\nfunction, some instructions aim to calculate (prepare) values for other\ninstructions. The latter instructions are defined by us as key instructions.\nCurrently, we define four categories of key instructions: calling subfunctions,\ncomparing instruction, returning instruction, and memory-store instruction.\nThus if we symbolically execute similar binary codes, symbolic values at these\nkey instructions are expected to be similar. As such, we implement a prototype\ntool, which has three steps. First, it symbolically executes binary code;\nSecond, it extracts symbolic values at defined key instructions into a graph;\nLast, it compares the symbolic graph similarity. In our implementation, we also\naddress some problems, including path explosion and loop handling.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "liu2023can", "year": "2023", "title":"Can LSH (locality-sensitive Hashing) Be Replaced By Neural Network", "abstract": "<p>With the rapid development of GPU (Graphics Processing Unit) technologies and\nneural networks, we can explore more appropriate data structures and\nalgorithms. Recent progress shows that neural networks can partly replace\ntraditional data structures. In this paper, we proposed a novel DNN (Deep\nNeural Network)-based learned locality-sensitive hashing, called LLSH, to\nefficiently and flexibly map high-dimensional data to low-dimensional space.\nLLSH replaces the traditional LSH (Locality-sensitive Hashing) function\nfamilies with parallel multi-layer neural networks, which reduces the time and\nmemory consumption and guarantees query accuracy simultaneously. The proposed\nLLSH demonstrate the feasibility of replacing the hash index with\nlearning-based neural networks and open a new door for developers to design and\nconfigure data organization more accurately to improve information-searching\nperformance. Extensive experiments on different types of datasets show the\nsuperiority of the proposed method in query accuracy, time consumption, and\nmemory usage.</p>\n", "tags": ["ARXIV","Graph","LSH","Supervised"] },
{"key": "liu2023hs", "year": "2023", "title":"HS-GCN Hamming Spatial Graph Convolutional Networks For Recommendation", "abstract": "<p>An efficient solution to the large-scale recommender system is to represent\nusers and items as binary hash codes in the Hamming space. Towards this end,\nexisting methods tend to code users by modeling their Hamming similarities with\nthe items they historically interact with, which are termed as the first-order\nsimilarities in this work. Despite their efficiency, these methods suffer from\nthe suboptimal representative capacity, since they forgo the correlation\nestablished by connecting multiple first-order similarities, i.e., the relation\namong the indirect instances, which could be defined as the high-order\nsimilarity. To tackle this drawback, we propose to model both the first- and\nthe high-order similarities in the Hamming space through the user-item\nbipartite graph. Therefore, we develop a novel learning to hash framework,\nnamely Hamming Spatial Graph Convolutional Networks (HS-GCN), which explicitly\nmodels the Hamming similarity and embeds it into the codes of users and items.\nExtensive experiments on three public benchmark datasets demonstrate that our\nproposed model significantly outperforms several state-of-the-art hashing\nmodels, and obtains performance comparable with the real-valued recommendation\nmodels.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "liu2023learning", "year": "2023", "title":"Learning Category Trees For Id-based Recommendation Exploring The Power Of Differentiable Vector Quantization", "abstract": "<p>Category information plays a crucial role in enhancing the quality and\npersonalization of recommender systems. Nevertheless, the availability of item\ncategory information is not consistently present, particularly in the context\nof ID-based recommendations. In this work, we propose a novel approach to\nautomatically learn and generate entity (i.e., user or item) category trees for\nID-based recommendation. Specifically, we devise a differentiable vector\nquantization framework for automatic category tree generation, namely CAGE,\nwhich enables the simultaneous learning and refinement of categorical code\nrepresentations and entity embeddings in an end-to-end manner, starting from\nthe randomly initialized states. With its high adaptability, CAGE can be easily\nintegrated into both sequential and non-sequential recommender systems. We\nvalidate the effectiveness of CAGE on various recommendation tasks including\nlist completion, collaborative filtering, and click-through rate prediction,\nacross different recommendation models. We release the code and data for others\nto reproduce the reported results.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "liu2023sparse", "year": "2023", "title":"Sparse-inductive Generative Adversarial Hashing For Nearest Neighbor Search", "abstract": "<p>Unsupervised hashing has received extensive research focus on the past\ndecade, which typically aims at preserving a predefined metric (i.e. Euclidean\nmetric) in the Hamming space. To this end, the encoding functions of the\nexisting hashing are typically quasi-isometric, which devote to reducing the\nquantization loss from the target metric space to the discrete Hamming space.\nHowever, it is indeed problematic to directly minimize such error, since such\nmentioned two metric spaces are heterogeneous, and the quasi-isometric mapping\nis non-linear. The former leads to inconsistent feature distributions, while\nthe latter leads to problematic optimization issues. In this paper, we propose\na novel unsupervised hashing method, termed Sparsity-Induced Generative\nAdversarial Hashing (SiGAH), to encode large-scale high-dimensional features\ninto binary codes, which well solves the two problems through a generative\nadversarial training framework. Instead of minimizing the quantization loss,\nour key innovation lies in enforcing the learned Hamming space to have similar\ndata distribution to the target metric space via a generative model. In\nparticular, we formulate a ReLU-based neural network as a generator to output\nbinary codes and an MSE-loss based auto-encoder network as a discriminator,\nupon which a generative adversarial learning is carried out to train hash\nfunctions. Furthermore, to generate the synthetic features from the hash codes,\na compressed sensing procedure is introduced into the generative model, which\nenforces the reconstruction boundary of binary codes to be consistent with that\nof original features. Finally, such generative adversarial framework can be\ntrained via the Adam optimizer. Experimental results on four benchmarks, i.e.,\nTiny100K, GIST1M, Deep1M, and MNIST, have shown that the proposed SiGAH has\nsuperior performance over the state-of-the-art approaches.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Unsupervised"] },
{"key": "liu2024collaborative", "year": "2024", "title":"Collaborative Hashing", "abstract": "<p>Hashing technique has become a promising approach for\nfast similarity search. Most of existing hashing research\npursue the binary codes for the same type of entities by\npreserving their similarities. In practice, there are many\nscenarios involving nearest neighbor search on the data\ngiven in matrix form, where two different types of, yet\nnaturally associated entities respectively correspond to its\ntwo dimensions or views. To fully explore the duality\nbetween the two views, we propose a collaborative hashing\nscheme for the data in matrix form to enable fast search\nin various applications such as image search using bag of\nwords and recommendation using user-item ratings. By\nsimultaneously preserving both the entity similarities in\neach view and the interrelationship between views, our\ncollaborative hashing effectively learns the compact binary\ncodes and the explicit hash functions for out-of-sample\nextension in an alternating optimization way. Extensive\nevaluations are conducted on three well-known datasets\nfor search inside a single view and search across different\nviews, demonstrating that our proposed method outperforms\nstate-of-the-art baselines, with significant accuracy\ngains ranging from 7.67% to 45.87% relatively.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "liu2024discrete", "year": "2024", "title":"Discrete Graph Hashing", "abstract": "<p>Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic\ndatabases. In particular, learning based hashing has received considerable\nattention due to its appealing storage and search efficiency. However, the performance\nof most unsupervised learning based hashing methods deteriorates rapidly\nas the hash code length increases. We argue that the degraded performance is due\nto inferior optimization procedures used to achieve discrete binary codes. This\npaper presents a graph-based unsupervised hashing model to preserve the neighborhood\nstructure of massive data in a discrete code space. We cast the graph\nhashing problem into a discrete optimization framework which directly learns the\nbinary codes. A tractable alternating maximization algorithm is then proposed to\nexplicitly deal with the discrete constraints, yielding high-quality codes to well\ncapture the local neighborhoods. Extensive experiments performed on four large\ndatasets with up to one million samples show that our discrete optimization based\ngraph hashing method obtains superior search accuracy over state-of-the-art unsupervised\nhashing methods, especially for longer codes.</p>\n", "tags": ["ARXIV","Graph","Unsupervised"] },
{"key": "liu2024discretely", "year": "2024", "title":"Discretely Coding Semantic Rank Orders For Supervised Image Hashing", "abstract": "<p>Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "liu2024hash", "year": "2024", "title":"Hash Bit Selection A Unified Solution For Selection Problems In Hashing", "abstract": "<p>Hashing based methods recently have been shown promising for large-scale nearest neighbor search. However, good designs involve difficult decisions of many unknowns – data features, hashing algorithms, parameter settings, kernels, etc. In this paper, we provide a unified solution as hash bit selection, i.e., selecting the most informative hash bits from a pool of candidates that may have been generated under various conditions mentioned above. We represent the candidate bit pool as a vertex- and edge-weighted graph with the pooled bits as vertices. Then we formulate the bit selection problem as quadratic programming over the graph, and solve it efficiently by replicator dynamics. Extensive experiments show that our bit selection approach can achieve superior performance over both naive selection methods and state-of-the-art methods under each scenario, usually with significant accuracy gains from 10% to 50% relatively.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "liu2024hashing", "year": "2024", "title":"Hashing With Graphs", "abstract": "<p>Hashing is becoming increasingly popular for\nefficient nearest neighbor search in massive\ndatabases. However, learning short codes\nthat yield good search performance is still\na challenge. Moreover, in many cases realworld\ndata lives on a low-dimensional manifold,\nwhich should be taken into account\nto capture meaningful nearest neighbors. In\nthis paper, we propose a novel graph-based\nhashing method which automatically discovers\nthe neighborhood structure inherent in\nthe data to learn appropriate compact codes.\nTo make such an approach computationally\nfeasible, we utilize Anchor Graphs to obtain\ntractable low-rank adjacency matrices. Our\nformulation allows constant time hashing of a\nnew data point by extrapolating graph Laplacian\neigenvectors to eigenfunctions. Finally,\nwe describe a hierarchical threshold learning\nprocedure in which each eigenfunction yields\nmultiple bits, leading to higher search accuracy.\nExperimental comparison with the\nother state-of-the-art methods on two large\ndatasets demonstrates the efficacy of the proposed\nmethod.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "liu2024joint", "year": "2024", "title":"Joint-modal Distribution-based Similarity Hashing For Large-scale Unsupervised Deep Cross-modal Retrieval", "abstract": "<p>Hashing-based cross-modal search which aims to map multiple modality features into binary codes has attracted increasingly attention due to its storage and search efficiency especially in large-scale database retrieval. Recent unsupervised deep cross-modal hashing methods have shown promising results. However, existing approaches typically suffer from two limitations: (1) They usually learn cross-modal similarity information separately or in a redundant fusion manner, which may fail to capture semantic correlations among instances from different modalities sufficiently and effectively. (2) They seldom consider the sampling and weighting schemes for unsupervised cross-modal hashing, resulting in the lack of satisfactory discriminative ability in hash codes. To overcome these limitations, we propose a novel unsupervised deep cross-modal hashing method called Joint-modal Distribution-based Similarity Hashing (JDSH) for large-scale cross-modal retrieval. Firstly, we propose a novel cross-modal joint-training method by constructing a joint-modal similarity matrix to fully preserve the cross-modal semantic correlations among instances. Secondly, we propose a sampling and weighting scheme termed the Distribution-based Similarity Decision and Weighting (DSDW) method for unsupervised cross-modal hashing, which is able to generate more discriminative hash codes by pushing semantic similar instance pairs closer and pulling semantic dissimilar instance pairs apart. The experimental results demonstrate the superiority of JDSH compared with several unsupervised cross-modal hashing methods on two public datasets NUS-WIDE and MIRFlickr.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "liu2024moboost", "year": "2024", "title":"Moboost A Self-improvement Framework For Linear-based Hashing", "abstract": "<p>The linear model is commonly utilized in hashing methods owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider neighborhood information. In this study, we propose a novel generalized framework called Model Boost (MoBoost), which can achieve the self-improvement of the linear-based hashing. The proposed MoBoost is used to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, given a linear-based hashing method, we first execute the method several times to get several different hash codes for training samples, and then combine these different hash codes into one set utilizing one novel fusion strategy. Based on this set of hash codes, we learn some new parameters for the linear hash function that can significantly improve accuracy. The proposed MoBoost can be generally adopted in existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods while imposing negligible added expenditure in terms of time and space. Extensive experiments are performed based on three benchmark datasets, and the results demonstrate the superior performance of the proposed framework.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "liu2024model", "year": "2024", "title":"Model Optimization Boosting Framework For Linear Model Hash Learning", "abstract": "<p>Efficient hashing techniques have attracted extensive research interests in both storage and retrieval of high dimensional data, such as images and videos. In existing hashing methods, a linear model is commonly utilized owing to its efficiency. To obtain better accuracy, linear-based hashing methods focus on designing a generalized linear objective function with different constraints or penalty terms that consider the inherent characteristics and neighborhood information of samples. Differing from existing hashing methods, in this study, we propose a self-improvement framework called Model Boost (MoBoost) to improve model parameter optimization for linear-based hashing methods without adding new constraints or penalty terms. In the proposed MoBoost, for a linear-based hashing method, we first repeatedly execute the hashing method to obtain several hash codes to training samples. Then, utilizing two novel fusion strategies, these codes are fused into a single set. We also propose two new criteria to evaluate the goodness of hash bits during the fusion process. Based on the fused set of hash codes, we learn new parameters for the linear hash function that can significantly improve the accuracy. In general, the proposed MoBoost can be adopted by existing linear-based hashing methods, achieving more precise and stable performance compared to the original methods, and adopting the proposed MoBoost will incur negligible time and space costs. To evaluate the proposed MoBoost, we performed extensive experiments on four benchmark datasets, and the results demonstrate superior performance.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "liu2024multi", "year": "2024", "title":"Multi-view Complementary Hash Tables For Nearest Neighbor Search", "abstract": "<p>Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many\napplications (e.g., visual search, object detection, image\nmatching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views.\nHowever, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build\nmultiple complementary hash tables, much less to adaptively integrate information stemming from multiple views.\nIn\nthis paper we first present a novel multi-view complementary hash table method that learns complementary hash tables from the data with multiple views. For single multiview table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix,\nand learn discriminative hash functions in an efficient way.\nTo build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars\nshared by mis-separated neighbors. Extensive experiments\non three large-scale image datasets demonstrate that the\nproposed method significantly outperforms various naive\nsolutions and state-of-the-art multi-table methods.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "liu2024supervised", "year": "2024", "title":"Supervised Hashing With Kernels", "abstract": "<p>Recent years have witnessed the growing popularity of\nhashing in large-scale vision problems. It has been shown\nthat the hashing quality could be boosted by leveraging supervised\ninformation into hash function learning. However,\nthe existing supervised methods either lack adequate performance\nor often incur cumbersome model training. In this\npaper, we propose a novel kernel-based supervised hashing\nmodel which requires a limited amount of supervised information,\ni.e., similar and dissimilar data pairs, and a feasible\ntraining cost in achieving high quality hashing. The idea\nis to map the data to compact binary codes whose Hamming\ndistances are minimized on similar pairs and simultaneously\nmaximized on dissimilar pairs. Our approach is\ndistinct from prior works by utilizing the equivalence between\noptimizing the code inner products and the Hamming\ndistances. This enables us to sequentially and efficiently\ntrain the hash functions one bit at a time, yielding very\nshort yet discriminative codes. We carry out extensive experiments\non two image benchmarks with up to one million\nsamples, demonstrating that our approach significantly outperforms\nthe state-of-the-arts in searching both metric distance\nneighbors and semantically similar neighbors, with\naccuracy gains ranging from 13% to 46%.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "lockett2021assessing", "year": "2021", "title":"Assessing The Effectiveness Of YARA Rules For Signature-based Malware Detection And Classification", "abstract": "<p>Malware often uses obfuscation techniques or is modified slightly to evade\nsignature detection from antivirus software and malware analysis tools.\nTraditionally, to determine if a file is malicious and identify what type of\nmalware a sample is, a cryptographic hash of a file is calculated. A more\nrecent and flexible solution for malware detection is YARA, which enables the\ncreation of rules to identify and classify malware based on a file’s binary\npatterns. In this paper, the author will critically evaluate the effectiveness\nof YARA rules for signature-based detection and classification of malware in\ncomparison to alternative methods, which include cryptographic and fuzzy\nhashing.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "loncaric2018convolutional", "year": "2018", "title":"Convolutional Hashing For Automated Scene Matching", "abstract": "<p>We present a powerful new loss function and training scheme for learning\nbinary hash functions. In particular, we demonstrate our method by creating for\nthe first time a neural network that outperforms state-of-the-art Haar wavelets\nand color layout descriptors at the task of automated scene matching. By\naccurately relating distance on the manifold of network outputs to distance in\nHamming space, we achieve a 100-fold reduction in nontrivial false positive\nrate and significantly higher true positive rate. We expect our insights to\nprovide large wins for hashing models applied to other information retrieval\nhashing tasks as well.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "loncaric2018learning", "year": "2018", "title":"Learning Hash Codes Via Hamming Distance Targets", "abstract": "<p>We present a powerful new loss function and training scheme for learning\nbinary hash codes with any differentiable model and similarity function. Our\nloss function improves over prior methods by using log likelihood loss on top\nof an accurate approximation for the probability that two inputs fall within a\nHamming distance target. Our novel training scheme obtains a good estimate of\nthe true gradient by better sampling inputs and evaluating loss terms between\nall pairs of inputs in each minibatch. To fully leverage the resulting hashes,\nwe use multi-indexing. We demonstrate that these techniques provide large\nimprovements to a similarity search tasks. We report the best results to date\non competitive information retrieval tasks for ImageNet and SIFT 1M, improving\nMAP from 73% to 84% and reducing query cost by a factor of 2-8, respectively.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "long2015composite", "year": "2015", "title":"Composite Correlation Quantization For Efficient Multimodal Retrieval", "abstract": "<p>Efficient similarity retrieval from large-scale multimodal database is\npervasive in modern search engines and social networks. To support queries\nacross content modalities, the system should enable cross-modal correlation and\ncomputation-efficient indexing. While hashing methods have shown great\npotential in achieving this goal, current attempts generally fail to learn\nisomorphic hash codes in a seamless scheme, that is, they embed multiple\nmodalities in a continuous isomorphic space and separately threshold embeddings\ninto binary codes, which incurs substantial loss of retrieval accuracy. In this\npaper, we approach seamless multimodal hashing by proposing a novel Composite\nCorrelation Quantization (CCQ) model. Specifically, CCQ jointly finds\ncorrelation-maximal mappings that transform different modalities into\nisomorphic latent space, and learns composite quantizers that convert the\nisomorphic latent features into compact binary codes. An optimization framework\nis devised to preserve both intra-modal similarity and inter-modal correlation\nthrough minimizing both reconstruction and quantization errors, which can be\ntrained from both paired and partially paired data in linear time. A\ncomprehensive set of experiments clearly show the superior effectiveness and\nefficiency of CCQ against the state of the art hashing methods for both\nunimodal and cross-modal retrieval.</p>\n", "tags": ["ARXIV","Cross Modal","Independent","Quantisation"] },
{"key": "long2018filter", "year": "2018", "title":"A Filter Of Minhash For Image Similarity Measures", "abstract": "<p>Image similarity measures play an important role in nearest neighbor search\nand duplicate detection for large-scale image datasets. Recently, Minwise\nHashing (or Minhash) and its related hashing algorithms have achieved great\nperformances in large-scale image retrieval systems. However, there are a large\nnumber of comparisons for image pairs in these applications, which may spend a\nlot of computation time and affect the performance. In order to quickly obtain\nthe pairwise images that theirs similarities are higher than the specific\nthreshold T (e.g., 0.5), we propose a dynamic threshold filter of Minwise\nHashing for image similarity measures. It greatly reduces the calculation time\nby terminating the unnecessary comparisons in advance. We also find that the\nfilter can be extended to other hashing algorithms, on when the estimator\nsatisfies the binomial distribution, such as b-Bit Minwise Hashing, One\nPermutation Hashing, etc. In this pager, we use the Bag-of-Visual-Words (BoVW)\nmodel based on the Scale Invariant Feature Transform (SIFT) to represent the\nimage features. We have proved that the filter is correct and effective through\nthe experiment on real image datasets.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "long2022adaptive", "year": "2022", "title":"Adaptive Asymmetric Label-guided Hashing For Multimedia Search", "abstract": "<p>With the rapid growth of multimodal media data on the Web in recent years,\nhash learning methods as a way to achieve efficient and flexible cross-modal\nretrieval of massive multimedia data have received a lot of attention from the\ncurrent Web resource retrieval research community. Existing supervised hashing\nmethods simply transform label information into pairwise similarity information\nto guide hash learning, leading to a potential risk of semantic error in the\nface of multi-label data. In addition, most existing hash optimization methods\nsolve NP-hard optimization problems by employing approximate approximation\nstrategies based on relaxation strategies, leading to a large quantization\nerror. In order to address above obstacles, we present a simple yet efficient\nAdaptive Asymmetric Label-guided Hashing, named A2LH, for Multimedia Search.\nSpecifically, A2LH is a two-step hashing method. In the first step, we design\nan association representation model between the different modality\nrepresentations and semantic label representation separately, and use the\nsemantic label representation as an intermediate bridge to solve the semantic\ngap existing between different modalities. In addition, we present an efficient\ndiscrete optimization algorithm for solving the quantization error problem\ncaused by relaxation-based optimization algorithms. In the second step, we\nleverage the generated hash codes to learn the hash mapping functions. The\nexperimental results show that our proposed method achieves optimal performance\non all compared baseline methods.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "long2024deep", "year": "2024", "title":"Deep Domain Adaptation Hashing With Adversarial Learning", "abstract": "<p>The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios. Nevertheless, fine-tuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts. A valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain. The core problem is to bring the source and target distributions closer in the feature space. In the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift. Particularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data. Specifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components: a deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes. The whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions. Extensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks. Our DeDAHA framework achieves superior results when compared to the state-of-the-art techniques.</p>\n", "tags": ["ARXIV","CNN","Cross Modal","Supervised"] },
{"key": "lu2018fmhash", "year": "2018", "title":"Fmhash Deep Hashing Of In-air-handwriting For User Identification", "abstract": "<p>Many mobile systems and wearable devices, such as Virtual Reality (VR) or\nAugmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID\nand password for signing into a virtual website. However, they are usually\nequipped with gesture capture interfaces to allow the user to interact with the\nsystem directly with hand gestures. Although gesture-based authentication has\nbeen well-studied, less attention is paid to the gesture-based user\nidentification problem, which is essentially an input method of account ID and\nan efficient searching and indexing method of a database of gesture signals. In\nthis paper, we propose FMHash (i.e., Finger Motion Hash), a user identification\nframework that can generate a compact binary hash code from a piece of\nin-air-handwriting of an ID string. This hash code enables indexing and fast\nsearch of a large account database using the in-air-handwriting by a hash\ntable. To demonstrate the effectiveness of the framework, we implemented a\nprototype and achieved &gt;99.5% precision and &gt;92.6% recall with exact hash code\nmatch on a dataset of 200 accounts collected by us. The ability of hashing\nin-air-handwriting pattern to binary code can be used to achieve convenient\nsign-in and sign-up with in-air-handwriting gesture ID on future mobile and\nwearable systems connected to the Internet.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "lu2018instance", "year": "2018", "title":"Instance-level Sketch-based Retrieval By Deep Triplet Classification Siamese Network", "abstract": "<p>Sketch has been employed as an effective communicative tool to express the\nabstract and intuitive meanings of object. Recognizing the free-hand sketch\ndrawing is extremely useful in many real-world applications. While\ncontent-based sketch recognition has been studied for several decades, the\ninstance-level Sketch-Based Image Retrieval (SBIR) tasks have attracted\nsignificant research attention recently. The existing datasets such as\nQMUL-Chair and QMUL-Shoe, focus on the retrieval tasks of chairs and shoes.\nHowever, there are several key limitations in previous instance-level SBIR\nworks. The state-of-the-art works have to heavily rely on the pre-training\nprocess, quality of edge maps, multi-cropping testing strategy, and augmenting\nsketch images. To efficiently solve the instance-level SBIR, we propose a new\nDeep Triplet Classification Siamese Network (DeepTCNet) which employs\nDenseNet-169 as the basic feature extractor and is optimized by the triplet\nloss and classification loss. Critically, our proposed DeepTCNet can break the\nlimitations existed in previous works. The extensive experiments on five\nbenchmark sketch datasets validate the effectiveness of the proposed model.\nAdditionally, to study the tasks of sketch-based hairstyle retrieval, this\npaper contributes a new instance-level photo-sketch dataset - Hairstyle\nPhoto-Sketch dataset, which is composed of 3600 sketches and photos, and 2400\nsketch-photo pairs.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "lu2021deep", "year": "2021", "title":"Deep Asymmetric Hashing With Dual Semantic Regression And Class Structure Quantization", "abstract": "<p>Recently, deep hashing methods have been widely used in image retrieval task.\nMost existing deep hashing approaches adopt one-to-one quantization to reduce\ninformation loss. However, such class-unrelated quantization cannot give\ndiscriminative feedback for network training. In addition, these methods only\nutilize single label to integrate supervision information of data for hashing\nfunction learning, which may result in inferior network generalization\nperformance and relatively low-quality hash codes since the inter-class\ninformation of data is totally ignored. In this paper, we propose a dual\nsemantic asymmetric hashing (DSAH) method, which generates discriminative hash\ncodes under three-fold constraints. Firstly, DSAH utilizes class prior to\nconduct class structure quantization so as to transmit class information during\nthe quantization process. Secondly, a simple yet effective label mechanism is\ndesigned to characterize both the intra-class compactness and inter-class\nseparability of data, thereby achieving semantic-sensitive binary code\nlearning. Finally, a meaningful pairwise similarity preserving loss is devised\nto minimize the distances between class-related network outputs based on an\naffinity graph. With these three main components, high-quality hash codes can\nbe generated through network. Extensive experiments conducted on various\ndatasets demonstrate the superiority of DSAH in comparison with\nstate-of-the-art deep hashing methods.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Quantisation","Supervised"] },
{"key": "lu2021learnable", "year": "2021", "title":"Learnable Locality-sensitive Hashing For Video Anomaly Detection", "abstract": "<p>Video anomaly detection (VAD) mainly refers to identifying anomalous events\nthat have not occurred in the training set where only normal samples are\navailable. Existing works usually formulate VAD as a reconstruction or\nprediction problem. However, the adaptability and scalability of these methods\nare limited. In this paper, we propose a novel distance-based VAD method to\ntake advantage of all the available normal data efficiently and flexibly. In\nour method, the smaller the distance between a testing sample and normal\nsamples, the higher the probability that the testing sample is normal.\nSpecifically, we propose to use locality-sensitive hashing (LSH) to map samples\nwhose similarity exceeds a certain threshold into the same bucket in advance.\nIn this manner, the complexity of near neighbor search is cut down\nsignificantly. To make the samples that are semantically similar get closer and\nsamples not similar get further apart, we propose a novel learnable version of\nLSH that embeds LSH into a neural network and optimizes the hash functions with\ncontrastive learning strategy. The proposed method is robust to data imbalance\nand can handle the large intra-class variations in normal data flexibly.\nBesides, it has a good ability of scalability. Extensive experiments\ndemonstrate the superiority of our method, which achieves new state-of-the-art\nresults on VAD benchmarks.</p>\n", "tags": ["ARXIV","LSH","Self Supervised"] },
{"key": "lu2021slosh", "year": "2021", "title":"SLOSH Set Locality Sensitive Hashing Via Sliced-wasserstein Embeddings", "abstract": "<p>Learning from set-structured data is an essential problem with many\napplications in machine learning and computer vision. This paper focuses on\nnon-parametric and data-independent learning from set-structured data using\napproximate nearest neighbor (ANN) solutions, particularly locality-sensitive\nhashing. We consider the problem of set retrieval from an input set query. Such\nretrieval problem requires: 1) an efficient mechanism to calculate the\ndistances/dissimilarities between sets, and 2) an appropriate data structure\nfor fast nearest neighbor search. To that end, we propose Sliced-Wasserstein\nset embedding as a computationally efficient “set-2-vector” mechanism that\nenables downstream ANN, with theoretical guarantees. The set elements are\ntreated as samples from an unknown underlying distribution, and the\nSliced-Wasserstein distance is used to compare sets. We demonstrate the\neffectiveness of our algorithm, denoted as Set-LOcality Sensitive Hashing\n(SLOSH), on various set retrieval datasets and compare our proposed embedding\nwith standard set embedding approaches, including Generalized Mean (GeM)\nembedding/pooling, Featurewise Sort Pooling (FSPool), and Covariance Pooling\nand show consistent improvement in retrieval results. The code for replicating\nour results is available here:\n\\href{https://github.com/mint-vu/SLOSH}{https://github.com/mint-vu/SLOSH}.</p>\n", "tags": ["ARXIV","Has Code","Independent"] },
{"key": "lu2022asymmetric", "year": "2022", "title":"Asymmetric Transfer Hashing With Adaptive Bipartite Graph Learning", "abstract": "<p>Thanks to the efficient retrieval speed and low storage consumption, learning\nto hash has been widely used in visual retrieval tasks. However, existing\nhashing methods assume that the query and retrieval samples lie in homogeneous\nfeature space within the same domain. As a result, they cannot be directly\napplied to heterogeneous cross-domain retrieval. In this paper, we propose a\nGeneralized Image Transfer Retrieval (GITR) problem, which encounters two\ncrucial bottlenecks: 1) the query and retrieval samples may come from different\ndomains, leading to an inevitable {domain distribution gap}; 2) the features of\nthe two domains may be heterogeneous or misaligned, bringing up an additional\n{feature gap}. To address the GITR problem, we propose an Asymmetric Transfer\nHashing (ATH) framework with its unsupervised/semi-supervised/supervised\nrealizations. Specifically, ATH characterizes the domain distribution gap by\nthe discrepancy between two asymmetric hash functions, and minimizes the\nfeature gap with the help of a novel adaptive bipartite graph constructed on\ncross-domain data. By jointly optimizing asymmetric hash functions and the\nbipartite graph, not only can knowledge transfer be achieved but information\nloss caused by feature alignment can also be avoided. Meanwhile, to alleviate\nnegative transfer, the intrinsic geometrical structure of single-domain data is\npreserved by involving a domain affinity graph. Extensive experiments on both\nsingle-domain and cross-domain benchmarks under different GITR subtasks\nindicate the superiority of our ATH method in comparison with the\nstate-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Supervised"] },
{"key": "lu2023attributes", "year": "2023", "title":"Attributes Grouping And Mining Hashing For Fine-grained Image Retrieval", "abstract": "<p>In recent years, hashing methods have been popular in the large-scale media\nsearch for low storage and strong representation capabilities. To describe\nobjects with similar overall appearance but subtle differences, more and more\nstudies focus on hashing-based fine-grained image retrieval. Existing hashing\nnetworks usually generate both local and global features through attention\nguidance on the same deep activation tensor, which limits the diversity of\nfeature representations. To handle this limitation, we substitute convolutional\ndescriptors for attention-guided features and propose an Attributes Grouping\nand Mining Hashing (AGMH), which groups and embeds the category-specific visual\nattributes in multiple descriptors to generate a comprehensive feature\nrepresentation for efficient fine-grained image retrieval. Specifically, an\nAttention Dispersion Loss (ADL) is designed to force the descriptors to attend\nto various local regions and capture diverse subtle details. Moreover, we\npropose a Stepwise Interactive External Attention (SIEA) to mine critical\nattributes in each descriptor and construct correlations between fine-grained\nattributes and objects. The attention mechanism is dedicated to learning\ndiscrete attributes, which will not cost additional computations in hash codes\ngeneration. Finally, the compact binary codes are learned by preserving\npairwise similarities. Experimental results demonstrate that AGMH consistently\nyields the best performance against state-of-the-art methods on fine-grained\nbenchmark datasets.</p>\n", "tags": ["AAAI","Image Retrieval","Independent"] },
{"key": "lu2024label", "year": "2024", "title":"Label Self-adaption Hashing For Image Retrieval", "abstract": "<p>Hashing has attracted widespread attention in image retrieval because of its fast retrieval speed and low storage cost. Compared with supervised methods, unsupervised hashing methods are more reasonable and suitable for large-scale image retrieval since it is always difficult and expensive to collect true labels of the massive data. Without label information, however, unsupervised hashing methods can not guarantee the quality of learned binary codes. To resolve this dilemma, this paper proposes a novel unsupervised hashing method called Label Self-Adaption Hashing (LSAH), which contains effective hashing function learning part and self-adaption label generation part. In the first part, we utilize anchor graph to keep the local structure of the data and introduce joint sparsity into the model to extract effective features for high-quality binary code learning. In the second part, a self-adaptive cluster label matrix is learned from the data under the assumption that the nearest neighbor points should have a large probability to be in the same cluster. Therefore, the proposed LSAH can make full use of the potential discriminative information of the data to guide the learning of binary code. It is worth noting that LSAH can learn effective binary codes, hashing function and cluster labels simultaneously in a unified optimization framework. To solve the resulting optimization problem, an Augmented Lagrange Multiplier based iterative algorithm is elaborately designed. Extensive experiments on three large-scale data sets indicate the promising performance of the proposed LSAH.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Supervised"] },
{"key": "lu2024online", "year": "2024", "title":"Online Multi-modal Hashing With Dynamic Query-adaption", "abstract": "<p>Multi-modal hashing is an effective technique to support large-scale multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving binary codes. Although great progress has been achieved so far, existing methods still suffer from several problems, including: 1) All existing methods simply adopt fixed modality combination weights in online hashing process to generate the query hash codes. This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for unsupervised methods) or require high computation and storage cost (for the supervised methods, which rely on pair-wise semantic matrix). 3) They solve the hash codes with relaxed optimization strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation time. To address the above limitations, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ) method in a novel fashion. Specifically, a self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative capability, while avoiding the challenging symmetric similarity matrix factorization. Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity. Moreover, to accurately capture the query variations, at the online retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to the dynamic query contents. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach from various aspects.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "lunga2017hashed", "year": "2017", "title":"Hashed Binary Search Sampling For Convolutional Network Training With Large Overhead Image Patches", "abstract": "<p>Very large overhead imagery associated with ground truth maps has the\npotential to generate billions of training image patches for machine learning\nalgorithms. However, random sampling selection criteria often leads to\nredundant and noisy-image patches for model training. With minimal research\nefforts behind this challenge, the current status spells missed opportunities\nto develop supervised learning algorithms that generalize over wide\ngeographical scenes. In addition, much of the computational cycles for large\nscale machine learning are poorly spent crunching through noisy and redundant\nimage patches. We demonstrate a potential framework to address these challenges\nspecifically, while evaluating a human settlement detection task. A novel\nbinary search tree sampling scheme is fused with a kernel based hashing\nprocedure that maps image patches into hash-buckets using binary codes\ngenerated from image content. The framework exploits inherent redundancy within\nbillions of image patches to promote mostly high variance preserving samples\nfor accelerating algorithmic training and increasing model generalization.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "luo2016ssh", "year": "2016", "title":"SSH (sketch Shingle Hash) For Indexing Massive-scale Time Series", "abstract": "<p>Similarity search on time series is a frequent operation in large-scale\ndata-driven applications. Sophisticated similarity measures are standard for\ntime series matching, as they are usually misaligned. Dynamic Time Warping or\nDTW is the most widely used similarity measure for time series because it\ncombines alignment and matching at the same time. However, the alignment makes\nDTW slow. To speed up the expensive similarity search with DTW, branch and\nbound based pruning strategies are adopted. However, branch and bound based\npruning are only useful for very short queries (low dimensional time series),\nand the bounds are quite weak for longer queries. Due to the loose bounds\nbranch and bound pruning strategy boils down to a brute-force search.\n  To circumvent this issue, we design SSH (Sketch, Shingle, &amp; Hashing), an\nefficient and approximate hashing scheme which is much faster than the\nstate-of-the-art branch and bound searching technique: the UCR suite. SSH uses\na novel combination of sketching, shingling and hashing techniques to produce\n(probabilistic) indexes which align (near perfectly) with DTW similarity\nmeasure. The generated indexes are then used to create hash buckets for\nsub-linear search. Our results show that SSH is very effective for longer time\nsequence and prunes around 95% candidates, leading to the massive speedup in\nsearch with DTW. Empirical results on two large-scale benchmark time series\ndata show that our proposed method can be around 20 times faster than the\nstate-of-the-art package (UCR suite) without any significant loss in accuracy.</p>\n", "tags": ["ARXIV"] },
{"key": "luo2017arrays", "year": "2017", "title":"Arrays Of (locality-sensitive) Count Estimators (ACE) High-speed Anomaly Detection Via Cache Lookups", "abstract": "<p>Anomaly detection is one of the frequent and important subroutines deployed\nin large-scale data processing systems. Even being a well-studied topic,\nexisting techniques for unsupervised anomaly detection require storing\nsignificant amounts of data, which is prohibitive from memory and latency\nperspective. In the big-data world existing methods fail to address the new set\nof memory and latency constraints. In this paper, we propose ACE (Arrays of\n(locality-sensitive) Count Estimators) algorithm that can be 60x faster than\nthe ELKI package~\\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastest\nimplementation of the unsupervised anomaly detection algorithms. ACE algorithm\nrequires less than \\(4MB\\) memory, to dynamically compress the full data\ninformation into a set of count arrays. These tiny \\(4MB\\) arrays of counts are\nsufficient for unsupervised anomaly detection. At the core of the ACE\nalgorithm, there is a novel statistical estimator which is derived from the\nsampling view of Locality Sensitive Hashing(LSH). This view is significantly\ndifferent and efficient than the widely popular view of LSH for near-neighbor\nsearch. We show the superiority of ACE algorithm over 11 popular baselines on 3\nbenchmark datasets, including the KDD-Cup99 data which is the largest available\nbenchmark comprising of more than half a million entries with ground truth\nanomaly labels.</p>\n", "tags": ["ARXIV","LSH","Unsupervised"] },
{"key": "luo2018collaborative", "year": "2018", "title":"Collaborative Learning For Extremely Low Bit Asymmetric Hashing", "abstract": "<p>Hashing techniques are in great demand for a wide range of real-world\napplications such as image retrieval and network compression. Nevertheless,\nexisting approaches could hardly guarantee a satisfactory performance with the\nextremely low-bit (e.g., 4-bit) hash codes due to the severe information loss\nand the shrink of the discrete solution space. In this paper, we propose a\nnovel \\textit{Collaborative Learning} strategy that is tailored for generating\nhigh-quality low-bit hash codes. The core idea is to jointly distill\nbit-specific and informative representations for a group of pre-defined code\nlengths. The learning of short hash codes among the group can benefit from the\nmanifold shared with other long codes, where multiple views from different hash\ncodes provide the supplementary guidance and regularization, making the\nconvergence faster and more stable. To achieve that, an asymmetric hashing\nframework with two variants of multi-head embedding structures is derived,\ntermed as Multi-head Asymmetric Hashing (MAH), leading to great efficiency of\ntraining and querying. Extensive experiments on three benchmark datasets have\nbeen conducted to verify the superiority of the proposed MAH, and have shown\nthat the 8-bit hash codes generated by MAH achieve \\(94.3\\%\\) of the MAP (Mean\nAverage Precision (MAP)) score on the CIFAR-10 dataset, which significantly\nsurpasses the performance of the 48-bit codes by the state-of-the-arts in image\nretrieval tasks.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "luo2019snap", "year": "2019", "title":"Snap And Find Deep Discrete Cross-domain Garment Image Retrieval", "abstract": "<p>With the increasing number of online stores, there is a pressing need for\nintelligent search systems to understand the item photos snapped by customers\nand search against large-scale product databases to find their desired items.\nHowever, it is challenging for conventional retrieval systems to match up the\nitem photos captured by customers and the ones officially released by stores,\nespecially for garment images. To bridge the customer- and store- provided\ngarment photos, existing studies have been widely exploiting the clothing\nattributes (\\textit{e.g.,} black) and landmarks (\\textit{e.g.,} collar) to\nlearn a common embedding space for garment representations. Unfortunately they\nomit the sequential correlation of attributes and consume large quantity of\nhuman labors to label the landmarks. In this paper, we propose a deep\nmulti-task cross-domain hashing termed \\textit{DMCH}, in which cross-domain\nembedding and sequential attribute learning are modeled simultaneously.\nSequential attribute learning not only provides the semantic guidance for\nembedding, but also generates rich attention on discriminative local details\n(\\textit{e.g.,} black buttons) of clothing items without requiring extra\nlandmark labels. This leads to promising performance and 306\\(\\times\\) boost on\nefficiency when compared with the state-of-the-art models, which is\ndemonstrated through rigorous experiments on two public fashion datasets.</p>\n", "tags": ["ARXIV","Cross Modal","Image Retrieval"] },
{"key": "luo2020cimon", "year": "2020", "title":"CIMON Towards High-quality Hash Codes", "abstract": "<p>Recently, hashing is widely used in approximate nearest neighbor search for\nits storage and computational efficiency. Most of the unsupervised hashing\nmethods learn to map images into semantic similarity-preserving hash codes by\nconstructing local semantic similarity structure from the pre-trained model as\nthe guiding information, i.e., treating each point pair similar if their\ndistance is small in feature space. However, due to the inefficient\nrepresentation ability of the pre-trained model, many false positives and\nnegatives in local semantic similarity will be introduced and lead to error\npropagation during the hash code learning. Moreover, few of the methods\nconsider the robustness of models, which will cause instability of hash codes\nto disturbance. In this paper, we propose a new method named\n{\\textbf{C}}omprehensive s{\\textbf{I}}milarity {\\textbf{M}}ining and\nc{\\textbf{O}}nsistency lear{\\textbf{N}}ing (CIMON). First, we use global\nrefinement and similarity statistical distribution to obtain reliable and\nsmooth guidance. Second, both semantic and contrastive consistency learning are\nintroduced to derive both disturb-invariant and discriminative hash codes.\nExtensive experiments on several benchmark datasets show that the proposed\nmethod outperforms a wide range of state-of-the-art methods in both retrieval\nperformance and robustness.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "luo2020survey", "year": "2020", "title":"A Survey On Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims to obtain the samples in the database with the\nsmallest distances from them to the queries, which is a basic task in a range\nof fields, including computer vision and data mining. Hashing is one of the\nmost widely used methods for its computational and storage efficiency. With the\ndevelopment of deep learning, deep hashing methods show more advantages than\ntraditional methods. In this survey, we detailedly investigate current deep\nhashing algorithms including deep supervised hashing and deep unsupervised\nhashing. Specifically, we categorize deep supervised hashing methods into\npairwise methods, ranking-based methods, pointwise methods as well as\nquantization according to how measuring the similarities of the learned hash\ncodes. Moreover, deep unsupervised hashing is categorized into similarity\nreconstruction-based methods, pseudo-label-based methods and prediction-free\nself-supervised learning-based methods based on their semantic learning\nmanners. We also introduce three related important topics including\nsemi-supervised deep hashing, domain adaption deep hashing and multi-modal deep\nhashing. Meanwhile, we present some commonly used public datasets and the\nscheme to measure the performance of deep hashing algorithms. Finally, we\ndiscuss some potential research directions in conclusion.</p>\n", "tags": ["ARXIV","Deep Learning","Quantisation","Supervised","Survey Paper"] },
{"key": "luo2021deep", "year": "2021", "title":"Deep Unsupervised Hashing By Distilled Smooth Guidance", "abstract": "<p>Hashing has been widely used in approximate nearest neighbor search for its\nstorage and computational efficiency. Deep supervised hashing methods are not\nwidely used because of the lack of labeled data, especially when the domain is\ntransferred. Meanwhile, unsupervised deep hashing models can hardly achieve\nsatisfactory performance due to the lack of reliable similarity signals. To\ntackle this problem, we propose a novel deep unsupervised hashing method,\nnamely Distilled Smooth Guidance (DSG), which can learn a distilled dataset\nconsisting of similarity signals as well as smooth confidence signals. To be\nspecific, we obtain the similarity confidence weights based on the initial\nnoisy similarity signals learned from local structures and construct a priority\nloss function for smooth similarity-preserving learning. Besides, global\ninformation based on clustering is utilized to distill the image pairs by\nremoving contradictory similarity signals. Extensive experiments on three\nwidely used benchmark datasets show that the proposed DSG consistently\noutperforms the state-of-the-art search methods.</p>\n", "tags": ["Unsupervised"] },
{"key": "luo2024fast", "year": "2024", "title":"Fast Scalable Supervised Hashing", "abstract": "<p>Despite significant progress in supervised hashing, there are three\ncommon limitations of existing methods. First, most pioneer methods discretely learn hash codes bit by bit, making the learning\nprocedure rather time-consuming. Second, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply\nsampling strategies during training, which inevitably results in information loss and suboptimal performance; some recent methods\ntry to replace the large matrix with a smaller one, but the size is\nstill large. Third, among the methods that leverage the pairwise\nsimilarity matrix, most of them only encode the semantic label\ninformation in learning the hash codes, failing to fully capture\nthe characteristics of data. In this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing\n(FSSH), which circumvents the use of the large similarity matrix by\nintroducing a pre-computed intermediate term whose size is independent with the size of training data. Moreover, FSSH can learn\nthe hash codes with not only the semantic information but also\nthe features of data. Extensive experiments on three widely used\ndatasets demonstrate its superiority over several state-of-the-art\nmethods in both accuracy and scalability. Our experiment codes\nare available at: https://lcbwlx.wixsite.com/fssh.</p>\n", "tags": ["ARXIV","Has Code","Supervised"] },
{"key": "luo2024fine", "year": "2024", "title":"Fine-grained Embedding Dimension Optimization During Training For Recommender Systems", "abstract": "<p>Huge embedding tables in modern Deep Learning Recommender Models (DLRM)\nrequire prohibitively large memory during training and inference. Aiming to\nreduce the memory footprint of training, this paper proposes FIne-grained\nIn-Training Embedding Dimension optimization (FIITED). Given the observation\nthat embedding vectors are not equally important, FIITED adjusts the dimension\nof each individual embedding vector continuously during training, assigning\nlonger dimensions to more important embeddings while adapting to dynamic\nchanges in data. A novel embedding storage system based on virtually-hashed\nphysically-indexed hash tables is designed to efficiently implement the\nembedding dimension adjustment and effectively enable memory saving.\nExperiments on two industry models show that FIITED is able to reduce the size\nof embeddings by more than 65% while maintaining the trained model’s quality,\nsaving significantly more memory than a state-of-the-art in-training embedding\npruning method. On public click-through rate prediction datasets, FIITED is\nable to prune up to 93.75%-99.75% embeddings without significant accuracy loss.</p>\n", "tags": ["ARXIV","Deep Learning"] },
{"key": "luo2024survey", "year": "2024", "title":"A Survey On Deep Hashing Methods", "abstract": "<p>Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.</p>\n", "tags": ["ARXIV","Deep Learning","Quantisation","Supervised","Survey Paper"] },
{"key": "lv2024multi", "year": "2024", "title":"Multi-probe LSH Efficient Indexing For High-dimensional Similarity Search", "abstract": "<p>Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio, images, videos, and other sensor\ndata. Recently, locality sensitive hashing (LSH) and its\nvariations have been proposed as indexing techniques for\napproximate similarity search. A significant drawback of\nthese approaches is the requirement for a large number of\nhash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH\nthat overcomes this drawback. Multi-probe LSH is built on\nthe well-known LSH technique, but it intelligently probes\nmultiple buckets that are likely to contain query results in\na hash table. Our method is inspired by and improves upon\nrecent theoretical work on entropy-based LSH designed to\nreduce the space requirement of the basic LSH method. We\nhave implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional\ndatasets. Our evaluation shows that the multi-probe LSH\nmethod substantially improves upon previously proposed\nmethods in both space and time efficiency. To achieve the\nsame search quality, multi-probe LSH has a similar timeefficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison\nwith the entropy-based LSH method, to achieve the same\nsearch quality, multi-probe LSH uses less query time and 5\nto 8 times fewer number of hash tables.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "ma2019hierarchy", "year": "2019", "title":"Hierarchy Neighborhood Discriminative Hashing For An Unified View Of Single-label And Multi-label Image Retrieval", "abstract": "<p>Recently, deep supervised hashing methods have become popular for large-scale\nimage retrieval task. To preserve the semantic similarity notion between\nexamples, they typically utilize the pairwise supervision or the triplet\nsupervised information for hash learning. However, these methods usually ignore\nthe semantic class information which can help the improvement of the semantic\ndiscriminative ability of hash codes. In this paper, we propose a novel\nhierarchy neighborhood discriminative hashing method. Specifically, we\nconstruct a bipartite graph to build coarse semantic neighbourhood relationship\nbetween the sub-class feature centers and the embeddings features. Moreover, we\nutilize the pairwise supervised information to construct the fined semantic\nneighbourhood relationship between embeddings features. Finally, we propose a\nhierarchy neighborhood discriminative hashing loss to unify the single-label\nand multilabel image retrieval problem with a one-stream deep neural network\narchitecture. Experimental results on two largescale datasets demonstrate that\nthe proposed method can outperform the state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Supervised"] },
{"key": "ma2021rank", "year": "2021", "title":"Rank-consistency Deep Hashing For Scalable Multi-label Image Search", "abstract": "<p>As hashing becomes an increasingly appealing technique for large-scale image\nretrieval, multi-label hashing is also attracting more attention for the\nability to exploit multi-level semantic contents. In this paper, we propose a\nnovel deep hashing method for scalable multi-label image search. Unlike\nexisting approaches with conventional objectives such as contrast and triplet\nlosses, we employ a rank list, rather than pairs or triplets, to provide\nsufficient global supervision information for all the samples. Specifically, a\nnew rank-consistency objective is applied to align the similarity orders from\ntwo spaces, the original space and the hamming space. A powerful loss function\nis designed to penalize the samples whose semantic similarity and hamming\ndistance are mismatched in two spaces. Besides, a multi-label softmax\ncross-entropy loss is presented to enhance the discriminative power with a\nconcise formulation of the derivative function. In order to manipulate the\nneighborhood structure of the samples with different labels, we design a\nmulti-label clustering loss to cluster the hashing vectors of the samples with\nthe same labels by reducing the distances between the samples and their\nmultiple corresponding class centers. The state-of-the-art experimental results\nachieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and\nNUS-WIDE, demonstrate the effectiveness of the proposed method.</p>\n", "tags": ["Image Retrieval","Unsupervised"] },
{"key": "ma2022deep", "year": "2022", "title":"Deep Forest With Hashing Screening And Window Screening", "abstract": "<p>As a novel deep learning model, gcForest has been widely used in various\napplications. However, the current multi-grained scanning of gcForest produces\nmany redundant feature vectors, and this increases the time cost of the model.\nTo screen out redundant feature vectors, we introduce a hashing screening\nmechanism for multi-grained scanning and propose a model called HW-Forest which\nadopts two strategies, hashing screening and window screening. HW-Forest\nemploys perceptual hashing algorithm to calculate the similarity between\nfeature vectors in hashing screening strategy, which is used to remove the\nredundant feature vectors produced by multi-grained scanning and can\nsignificantly decrease the time cost and memory consumption. Furthermore, we\nadopt a self-adaptive instance screening strategy to improve the performance of\nour approach, called window screening, which can achieve higher accuracy\nwithout hyperparameter tuning on different datasets. Our experimental results\nshow that HW-Forest has higher accuracy than other models, and the time cost is\nalso reduced.</p>\n", "tags": ["ARXIV","Deep Learning"] },
{"key": "ma2023anserini", "year": "2023", "title":"Anserini Gets Dense Retrieval Integration Of Lucenes HNSW Indexes", "abstract": "<p>Anserini is a Lucene-based toolkit for reproducible information retrieval\nresearch in Java that has been gaining traction in the community. It provides\nretrieval capabilities for both “traditional” bag-of-words retrieval models\nsuch as BM25 as well as retrieval using learned sparse representations such as\nSPLADE. With Pyserini, which provides a Python interface to Anserini, users\ngain access to both sparse and dense retrieval models, as Pyserini implements\nbindings to the Faiss vector search library alongside Lucene inverted indexes\nin a uniform, consistent interface. Nevertheless, hybrid fusion techniques that\nintegrate sparse and dense retrieval models need to stitch together results\nfrom two completely different “software stacks”, which creates unnecessary\ncomplexities and inefficiencies. However, the introduction of HNSW indexes for\ndense vector search in Lucene promises the integration of both dense and sparse\nretrieval within a single software framework. We explore exactly this\nintegration in the context of Anserini. Experiments on the MS MARCO passage and\nBEIR datasets show that our Anserini HNSW integration supports (reasonably)\neffective and (reasonably) efficient approximate nearest neighbor search for\ndense retrieval models, using only Lucene.</p>\n", "tags": ["ARXIV"] },
{"key": "ma2023maximal", "year": "2023", "title":"On The Maximal Independent Sets Of k-mers With The Edit Distance", "abstract": "<p>In computational biology, \\(k\\)-mers and edit distance are fundamental\nconcepts. However, little is known about the metric space of all \\(k\\)-mers\nequipped with the edit distance. In this work, we explore the structure of the\n\\(k\\)-mer space by studying its maximal independent sets (MISs). An MIS is a\nsparse sketch of all \\(k\\)-mers with nice theoretical properties, and therefore\nadmits critical applications in clustering, indexing, hashing, and sketching\nlarge-scale sequencing data, particularly those with high error-rates. Finding\nan MIS is a challenging problem, as the size of a \\(k\\)-mer space grows\ngeometrically with respect to \\(k\\). We propose three algorithms for this\nproblem. The first and the most intuitive one uses a greedy strategy. The\nsecond method implements two techniques to avoid redundant comparisons by\ntaking advantage of the locality-property of the \\(k\\)-mer space and the\nestimated bounds on the edit distance. The last algorithm avoids expensive\ncalculations of the edit distance by translating the edit distance into the\nshortest path in a specifically designed graph. These algorithms are\nimplemented and the calculated MISs of \\(k\\)-mer spaces and their statistical\nproperties are reported and analyzed for \\(k\\) up to 15. Source code is freely\navailable at https://github.com/Shao-Group/kmerspace .</p>\n", "tags": ["ARXIV","Graph","Has Code","Independent"] },
{"key": "ma2024harr", "year": "2024", "title":"HARR Learning Discriminative And High-quality Hash Codes For Image Retrieval", "abstract": "<p>This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "ma2024progressive", "year": "2024", "title":"Progressive Generative Hashing For Image Retrieval", "abstract": "<p>Recent years have witnessed the success of the emerging hashing techniques in large-scale image\nretrieval. Owing to the great learning capacity,\ndeep hashing has become one of the most promising solutions, and achieved attractive performance\nin practice. However, without semantic label information, the unsupervised deep hashing still remains\nan open question. In this paper, we propose a novel\nprogressive generative hashing (PGH) framework\nto help learn a discriminative hashing network in an\nunsupervised way. Different from existing studies,\nit first treats the hash codes as a kind of semantic\ncondition for the similar image generation, and simultaneously feeds the original image and its codes\ninto the generative adversarial networks (GANs).\nThe real images together with the synthetic ones\ncan further help train a discriminative hashing network based on a triplet loss. By iteratively inputting\nthe learnt codes into the hash conditioned GANs, we can progressively enable the hashing network\nto discover the semantic relations. Extensive experiments on the widely-used image datasets demonstrate that PGH can significantly outperform stateof-the-art unsupervised hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "macgregor2023fast", "year": "2023", "title":"Fast Approximation Of Similarity Graphs With Kernel Density Estimation", "abstract": "<p>Constructing a similarity graph from a set \\(X\\) of data points in\n\\(\\mathbb{R}^d\\) is the first step of many modern clustering algorithms. However,\ntypical constructions of a similarity graph have high time complexity, and a\nquadratic space dependency with respect to \\(|X|\\). We address this limitation\nand present a new algorithmic framework that constructs a sparse approximation\nof the fully connected similarity graph while preserving its cluster structure.\nOur presented algorithm is based on the kernel density estimation problem, and\nis applicable for arbitrary kernel functions. We compare our designed algorithm\nwith the well-known implementations from the scikit-learn library and the FAISS\nlibrary, and find that our method significantly outperforms the implementation\nfrom both libraries on a variety of datasets.</p>\n", "tags": ["ARXIV","Graph","Unsupervised"] },
{"key": "macgregor2024spectral", "year": "2024", "title":"Spectral Toolkit Of Algorithms For Graphs Technical Report (2)", "abstract": "<p>Spectral Toolkit of Algorithms for Graphs (STAG) is an open-source library\nfor efficient graph algorithms. This technical report presents the newly\nimplemented component on locality sensitive hashing, kernel density estimation,\nand fast spectral clustering. The report includes a user’s guide to the newly\nimplemented algorithms, experiments and demonstrations of the new\nfunctionality, and several technical considerations behind our development.</p>\n", "tags": ["ARXIV","Graph","Unsupervised"] },
{"key": "magliani2018efficient", "year": "2018", "title":"Efficient Nearest Neighbors Search For Large-scale Landmark Recognition", "abstract": "<p>The problem of landmark recognition has achieved excellent results in\nsmall-scale datasets. When dealing with large-scale retrieval, issues that were\nirrelevant with small amount of data, quickly become fundamental for an\nefficient retrieval phase. In particular, computational time needs to be kept\nas low as possible, whilst the retrieval accuracy has to be preserved as much\nas possible. In this paper we propose a novel multi-index hashing method called\nBag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows\nto drastically reduce the query time and outperforms the accuracy results\ncompared to the state-of-the-art methods for large-scale landmark recognition.\nIt has been demonstrated that this family of algorithms can be applied on\ndifferent embedding techniques like VLAD and R-MAC obtaining excellent results\nin very short times on different public datasets: Holidays+Flickr1M, Oxford105k\nand Paris106k.</p>\n", "tags": ["ARXIV"] },
{"key": "magliani2019efficient", "year": "2019", "title":"An Efficient Approximate Knn Graph Method For Diffusion On Image Retrieval", "abstract": "<p>The application of the diffusion in many computer vision and artificial\nintelligence projects has been shown to give excellent improvements in\nperformance. One of the main bottlenecks of this technique is the quadratic\ngrowth of the kNN graph size due to the high-quantity of new connections\nbetween nodes in the graph, resulting in long computation times. Several\nstrategies have been proposed to address this, but none are effective and\nefficient. Our novel technique, based on LSH projections, obtains the same\nperformance as the exact kNN graph after diffusion, but in less time\n(approximately 18 times faster on a dataset of a hundred thousand images). The\nproposed method was validated and compared with other state-of-the-art on\nseveral public image datasets, including Oxford5k, Paris6k, and Oxford105k.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Independent","LSH"] },
{"key": "maier2016concurrent", "year": "2016", "title":"Concurrent Hash Tables Fast And General(!)", "abstract": "<p>Concurrent hash tables are one of the most important concurrent data\nstructures with numerous applications. Since hash table accesses can dominate\nthe execution time of the overall application, we need implementations that\nachieve good speedup. Unfortunately, currently available concurrent hashing\nlibraries turn out to be far away from this requirement in particular when\ncontention on some elements occurs.\n  Our starting point for better performing data structures is a fast and simple\nlock-free concurrent hash table based on linear probing that is limited to\nword-sized key-value types and does not support dynamic size adaptation. We\nexplain how to lift these limitations in a provably scalable way and\ndemonstrate that dynamic growing has a performance overhead comparable to the\nsame generalization in sequential hash tables.\n  We perform extensive experiments comparing the performance of our\nimplementations with six of the most widely used concurrent hash tables. Ours\nare considerably faster than the best algorithms with similar restrictions and\nan order of magnitude faster than the best more general tables. In some extreme\ncases, the difference even approaches four orders of magnitude.</p>\n", "tags": ["ARXIV"] },
{"key": "maier2017dynamic", "year": "2017", "title":"Dynamic Space Efficient Hashing", "abstract": "<p>We consider space efficient hash tables that can grow and shrink dynamically\nand are always highly space efficient, i.e., their space consumption is always\nclose to the lower bound even while growing and when taking into account\nstorage that is only needed temporarily. None of the traditionally used hash\ntables have this property. We show how known approaches like linear probing and\nbucket cuckoo hashing can be adapted to this scenario by subdividing them into\nmany subtables or using virtual memory overcommitting. However, these rather\nstraightforward solutions suffer from slow amortized insertion times due to\nfrequent reallocation in small increments.\n  Our main result is DySECT ({\\bf Dy}namic {\\bf S}pace {\\bf E}fficient {\\bf\nC}uckoo {\\bf T}able) which avoids these problems. DySECT consists of many\nsubtables which grow by doubling their size. The resulting inhomogeneity in\nsubtable sizes is equalized by the flexibility available in bucket cuckoo\nhashing where each element can go to several buckets each of which containing\nseveral cells. Experiments indicate that DySECT works well with load factors up\nto 98\\%. With up to 2.7 times better performance than the next best solution.</p>\n", "tags": ["ARXIV"] },
{"key": "maitinshepard2016elliptic", "year": "2016", "title":"Elliptic Curve Multiset Hash", "abstract": "<p>A homomorphic, or incremental, multiset hash function, associates a hash\nvalue to arbitrary collections of objects (with possible repetitions) in such a\nway that the hash of the union of two collections is easy to compute from the\nhashes of the two collections themselves: it is simply their sum under a\nsuitable group operation. In particular, hash values of large collections can\nbe computed incrementally and/or in parallel. Homomorphic hashing is thus a\nvery useful primitive with applications ranging from database integrity\nverification to streaming set/multiset comparison and network coding.\n  Unfortunately, constructions of homomorphic hash functions in the literature\nare hampered by two main drawbacks: they tend to be much longer than usual hash\nfunctions at the same security level (e.g. to achieve a collision resistance of\n2^128, they are several thousand bits long, as opposed to 256 bits for usual\nhash functions), and they are also quite slow.\n  In this paper, we introduce the Elliptic Curve Multiset Hash (ECMH), which\ncombines a usual bit string-valued hash function like BLAKE2 with an efficient\nencoding into binary elliptic curves to overcome both difficulties. On the one\nhand, the size of ECMH digests is essentially optimal: 2m-bit hash values\nprovide O(2^m) collision resistance. On the other hand, we demonstrate a\nhighly-efficient software implementation of ECMH, which our thorough empirical\nevaluation shows to be capable of processing over 3 million set elements per\nsecond on a 4 GHz Intel Haswell machine at the 128-bit security level—many\ntimes faster than previous practical methods.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "malakhov2015per", "year": "2015", "title":"Per-bucket Concurrent Rehashing Algorithms", "abstract": "<p>This paper describes a generic algorithm for concurrent resizing and\non-demand per-bucket rehashing for an extensible hash table. In contrast to\nknown lock-based hash table algorithms, the proposed algorithm separates the\nresizing and rehashing stages so that they neither invalidate existing buckets\nnor block any concurrent operations. Instead, the rehashing work is deferred\nand split across subsequent operations with the table. The rehashing operation\nuses bucket-level synchronization only and therefore allows a race condition\nbetween lookup and moving operations running in different threads. Instead of\nusing explicit synchronization, the algorithm detects the race condition and\nrestarts the lookup operation. In comparison with other lock-based algorithms,\nthe proposed algorithm reduces high-level synchronization on the hot path,\nimproving performance, concurrency, and scalability of the table. The response\ntime of the operations is also more predictable. The algorithm is compatible\nwith cache friendly data layouts for buckets and does not depend on any memory\nreclamation techniques thus potentially achieving additional performance gain\nwith corresponding implementations.</p>\n", "tags": ["ARXIV"] },
{"key": "malali2022learning", "year": "2022", "title":"Learning To Embed Semantic Similarity For Joint Image-text Retrieval", "abstract": "<p>We present a deep learning approach for learning the joint semantic\nembeddings of images and captions in a Euclidean space, such that the semantic\nsimilarity is approximated by the L2 distances in the embedding space. For\nthat, we introduce a metric learning scheme that utilizes multitask learning to\nlearn the embedding of identical semantic concepts using a center loss. By\nintroducing a differentiable quantization scheme into the end-to-end trainable\nnetwork, we derive a semantic embedding of semantically similar concepts in\nEuclidean space. We also propose a novel metric learning formulation using an\nadaptive margin hinge loss, that is refined during the training phase. The\nproposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets,\nand was shown to compare favorably with contemporary state-of-the-art\napproaches.</p>\n", "tags": ["ARXIV","Cross Modal","Deep Learning","Quantisation","Text Retrieval"] },
{"key": "mallik2022codes", "year": "2022", "title":"Codes From Incidence Matrices Of Hypergraphs", "abstract": "<p>Binary codes are constructed from incidence matrices of hypergraphs. A\ncombinatroial description is given for the minimum distances of such codes via\na combinatorial tool called ``eonv”. This combinatorial approach provides a\nfaster alternative method of finding the minimum distance, which is known to be\na hard problem. This is demonstrated on several classes of codes from\nhypergraphs. Moreover, self-duality and self-orthogonality conditions are also\nstudied through hypergraphs.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "malmasi2017open", "year": "2017", "title":"Open-set Language Identification", "abstract": "<p>We present the first open-set language identification experiments using\none-class classification. We first highlight the shortcomings of traditional\nfeature extraction methods and propose a hashing-based feature vectorization\napproach as a solution. Using a dataset of 10 languages from different writing\nsystems, we train a One- Class Support Vector Machine using only a monolingual\ncorpus for each language. Each model is evaluated against a test set of data\nfrom all 10 languages and we achieve an average F-score of 0.99, highlighting\nthe effectiveness of this approach for open-set language identification.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "mandal2020novel", "year": "2020", "title":"A Novel Incremental Cross-modal Hashing Approach", "abstract": "<p>Cross-modal retrieval deals with retrieving relevant items from one modality,\nwhen provided with a search query from another modality. Hashing techniques,\nwhere the data is represented as binary bits have specifically gained\nimportance due to the ease of storage, fast computations and high accuracy. In\nreal world, the number of data categories is continuously increasing, which\nrequires algorithms capable of handling this dynamic scenario. In this work, we\npropose a novel incremental cross-modal hashing algorithm termed “iCMH”, which\ncan adapt itself to handle incoming data of new categories. The proposed\napproach consists of two sequential stages, namely, learning the hash codes and\ntraining the hash functions. At every stage, a small amount of old category\ndata termed “exemplars” is is used so as not to forget the old data while\ntrying to learn for the new incoming data, i.e. to avoid catastrophic\nforgetting. In the first stage, the hash codes for the exemplars is used, and\nsimultaneously, hash codes for the new data is computed such that it maintains\nthe semantic relations with the existing data. For the second stage, we propose\nboth a non-deep and deep architectures to learn the hash functions effectively.\nExtensive experiments across a variety of cross-modal datasets and comparisons\nwith state-of-the-art cross-modal algorithms shows the usefulness of our\napproach.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "manohar2023parlayann", "year": "2023", "title":"Parlayann Scalable And Deterministic Parallel Graph-based Approximate Nearest Neighbor Search Algorithms", "abstract": "<p>Approximate nearest-neighbor search (ANNS) algorithms are a key part of the\nmodern deep learning stack due to enabling efficient similarity search over\nhigh-dimensional vector space representations (i.e., embeddings) of data. Among\nvarious ANNS algorithms, graph-based algorithms are known to achieve the best\nthroughput-recall tradeoffs. Despite the large scale of modern ANNS datasets,\nexisting parallel graph based implementations suffer from significant\nchallenges to scale to large datasets due to heavy use of locks and other\nsequential bottlenecks, which 1) prevents them from efficiently scaling to a\nlarge number of processors, and 2) results in nondeterminism that is\nundesirable in certain applications.\n  In this paper, we introduce ParlayANN, a library of deterministic and\nparallel graph-based approximate nearest neighbor search algorithms, along with\na set of useful tools for developing such algorithms. In this library, we\ndevelop novel parallel implementations for four state-of-the-art graph-based\nANNS algorithms that scale to billion-scale datasets. Our algorithms are\ndeterministic and achieve high scalability across a diverse set of challenging\ndatasets. In addition to the new algorithmic ideas, we also conduct a detailed\nexperimental study of our new algorithms as well as two existing non-graph\napproaches. Our experimental results both validate the effectiveness of our new\ntechniques, and lead to a comprehensive comparison among ANNS algorithms on\nlarge scale datasets with a list of interesting findings.</p>\n", "tags": ["ARXIV","Deep Learning","Graph"] },
{"key": "marchet2017resource", "year": "2017", "title":"A Resource-frugal Probabilistic Dictionary And Applications In Bioinformatics", "abstract": "<p>Indexing massive data sets is extremely expensive for large scale problems.\nIn many fields, huge amounts of data are currently generated, however\nextracting meaningful information from voluminous data sets, such as computing\nsimilarity between elements, is far from being trivial. It remains nonetheless\na fundamental need. This work proposes a probabilistic data structure based on\na minimal perfect hash function for indexing large sets of keys. Our structure\nout-compete the hash table for construction, query times and for memory usage,\nin the case of the indexation of a static set. To illustrate the impact of\nalgorithms performances, we provide two applications based on similarity\ncomputation between collections of sequences, and for which this calculation is\nan expensive but required operation. In particular, we show a practical case in\nwhich other bioinformatics tools fail to scale up the tested data set or\nprovide lower recall quality results.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "markchit2019effective", "year": "2019", "title":"Effective And Efficient Indexing In Cross-modal Hashing-based Datasets", "abstract": "<p>To overcome the barrier of storage and computation, the hashing technique has\nbeen widely used for nearest neighbor search in multimedia retrieval\napplications recently. Particularly, cross-modal retrieval that searches across\ndifferent modalities becomes an active but challenging problem. Although dozens\nof cross-modal hashing algorithms are proposed to yield compact binary codes,\nthe exhaustive search is impractical for the real-time purpose, and Hamming\ndistance computation suffers inaccurate results. In this paper, we propose a\nnovel search method that utilizes a probability-based index scheme over binary\nhash codes in cross-modal retrieval. The proposed hash code indexing scheme\nexploits a few binary bits of the hash code as the index code. We construct an\ninverted index table based on index codes and train a neural network to improve\nthe indexing accuracy and efficiency. Experiments are performed on two\nbenchmark datasets for retrieval across image and text modalities, where hash\ncodes are generated by three cross-modal hashing methods. Results show the\nproposed method effectively boost the performance on these hash methods.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "marshall2017exact", "year": "2017", "title":"Exact Clustering In Linear Time", "abstract": "<p>The time complexity of data clustering has been viewed as fundamentally\nquadratic, slowing with the number of data items, as each item is compared for\nsimilarity to preceding items. Clustering of large data sets has been\ninfeasible without resorting to probabilistic methods or to capping the number\nof clusters. Here we introduce MIMOSA, a novel class of algorithms which\nachieve linear time computational complexity on clustering tasks. MIMOSA\nalgorithms mark and match partial-signature keys in a hash table to obtain\nexact, error-free cluster retrieval. Benchmark measurements, on clustering a\ndata set of 10,000,000 news articles by news topic, found that a MIMOSA\nimplementation finished more than four orders of magnitude faster than a\nstandard centroid implementation.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "martinez2014stacked", "year": "2014", "title":"Stacked Quantizers For Compositional Vector Compression", "abstract": "<p>Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), a\ngeneralization of Product Quantization (PQ) where a non-independent set of\ncodebooks is used to compress vectors into small binary codes. Unfortunately,\nunder this scheme encoding cannot be done independently in each codebook, and\noptimal encoding is an NP-hard problem. In this paper, we observe that PQ and\nAQ are both compositional quantizers that lie on the extremes of the codebook\ndependence-independence assumption, and explore an intermediate approach that\nexploits a hierarchical structure in the codebooks. This results in a method\nthat achieves quantization error on par with or lower than AQ, while being\nseveral orders of magnitude faster. We perform a complexity analysis of PQ, AQ\nand our method, and evaluate our approach on standard benchmarks of SIFT and\nGIST descriptors, as well as on new datasets of features obtained from\nstate-of-the-art convolutional neural networks.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "martínfernández2022analysis", "year": "2022", "title":"Analysis Of The New Standard Hash Function", "abstract": "<p>On 2\\(^{nd}\\) October 2012 the NIST (National Institute of Standards and\nTechnology) in the United States of America announced the new hashing algorithm\nwhich will be adopted as standard from now on. Among a total of 73 candidates,\nthe winner was Keccak, designed by a group of cryptographers from Belgium and\nItaly. The public selection of a new standard of cryptographic hash function\nSHA (Secure Hash Algorithm) took five years. Its object is to generate a hash a\nfixed size from a pattern with arbitrary length. The first selection on behalf\nof NIST on a standard of this family took place in 1993 when SHA-1 was chosen,\nwhich later on was replaced by SHA-2. This paper is focused on the analysis\nboth from the point of view of security and the implementation of the Keccak\nfunction, which is the base of the new SHA-3 standard. In particular, an\nimplementation in the mobile platform Android is presented here, providing the\nfirst known external library in this mobile operating system so that any\ndeveloper could use the new standard hashing. Finally, the new standard in\napplications in the Internet of Things is analysed.</p>\n", "tags": ["Graph","Independent"] },
{"key": "masci2011descriptor", "year": "2011", "title":"Descriptor Learning For Omnidirectional Image Matching", "abstract": "<p>Feature matching in omnidirectional vision systems is a challenging problem,\nmainly because complicated optical systems make the theoretical modelling of\ninvariance and construction of invariant feature descriptors hard or even\nimpossible. In this paper, we propose learning invariant descriptors using a\ntraining set of similar and dissimilar descriptor pairs. We use the\nsimilarity-preserving hashing framework, in which we are trying to map the\ndescriptor data to the Hamming space preserving the descriptor similarity on\nthe training set. A neural network is used to solve the underlying optimization\nproblem. Our approach outperforms not only straightforward descriptor matching,\nbut also state-of-the-art similarity-preserving hashing methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "masci2012multimodal", "year": "2012", "title":"Multimodal Similarity-preserving Hashing", "abstract": "<p>We introduce an efficient computational framework for hashing data belonging\nto multiple modalities into a single representation space where they become\nmutually comparable. The proposed approach is based on a novel coupled siamese\nneural network architecture and allows unified treatment of intra- and\ninter-modality similarity learning. Unlike existing cross-modality similarity\nlearning approaches, our hashing functions are not limited to binarized linear\nprojections and can assume arbitrarily complex forms. We show experimentally\nthat our method significantly outperforms state-of-the-art hashing approaches\non multimedia retrieval tasks.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "masci2013sparse", "year": "2013", "title":"Sparse Similarity-preserving Hashing", "abstract": "<p>In recent years, a lot of attention has been devoted to efficient nearest\nneighbor search by means of similarity-preserving hashing. One of the plights\nof existing hashing techniques is the intrinsic trade-off between performance\nand computational complexity: while longer hash codes allow for lower false\npositive rates, it is very difficult to increase the embedding dimensionality\nwithout incurring in very high false negatives rates or prohibiting\ncomputational costs. In this paper, we propose a way to overcome this\nlimitation by enforcing the hash codes to be sparse. Sparse high-dimensional\ncodes enjoy from the low false positive rates typical of long hashes, while\nkeeping the false negative rates similar to those of a shorter dense hashing\nscheme with equal number of degrees of freedom. We use a tailored feed-forward\nneural network for the hashing function. Extensive experimental evaluation\ninvolving visual and multi-modal data shows the benefits of the proposed\nmethod.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "mashnoor2024locality", "year": "2024", "title":"Locality Sensitive Hashing For Network Traffic Fingerprinting", "abstract": "<p>The advent of the Internet of Things (IoT) has brought forth additional\nintricacies and difficulties to computer networks. These gadgets are\nparticularly susceptible to cyber-attacks because of their simplistic design.\nTherefore, it is crucial to recognise these devices inside a network for the\npurpose of network administration and to identify any harmful actions. Network\ntraffic fingerprinting is a crucial technique for identifying devices and\ndetecting anomalies. Currently, the predominant methods for this depend heavily\non machine learning (ML). Nevertheless, machine learning (ML) methods need the\nselection of features, adjustment of hyperparameters, and retraining of models\nto attain optimal outcomes and provide resilience to concept drifts detected in\na network. In this research, we suggest using locality-sensitive hashing (LSH)\nfor network traffic fingerprinting as a solution to these difficulties. Our\nstudy focuses on examining several design options for the Nilsimsa LSH\nfunction. We then use this function to create unique fingerprints for network\ndata, which may be used to identify devices. We also compared it with ML-based\ntraffic fingerprinting and observed that our method increases the accuracy of\nstate-of-the-art by 12% achieving around 94% accuracy in identifying devices in\na network.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "masson2024fliphash", "year": "2024", "title":"Fliphash A Constant-time Consistent Range-hashing Algorithm", "abstract": "<p>Consistent range-hashing is a technique used in distributed systems, either\ndirectly or as a subroutine for consistent hashing, commonly to realize an even\nand stable data distribution over a variable number of resources. We introduce\nFlipHash, a consistent range-hashing algorithm with constant time complexity\nand low memory requirements. Like Jump Consistent Hash, FlipHash is intended\nfor applications where resources can be indexed sequentially. Under this\ncondition, it ensures that keys are hashed evenly across resources and that\nchanging the number of resources only causes keys to be remapped from a removed\nresource or to an added one, but never shuffled across persisted ones. FlipHash\ndifferentiates itself with its low computational cost, achieving constant-time\ncomplexity. We show that FlipHash beats Jump Consistent Hash’s cost, which is\nlogarithmic in the number of resources, both theoretically and in experiments\nover practical settings.</p>\n", "tags": ["ARXIV"] },
{"key": "mastikhina2024improvement", "year": "2024", "title":"An Improvement Of Degree-based Hashing (DBH) Graph Partition Method Using A Novel Metric", "abstract": "<p>This paper examines the graph partition problem and introduces a new metric,\nMSIDS (maximal sum of inner degrees squared). We establish its connection to\nthe replication factor (RF) optimization, which has been the main focus of\ntheoretical work in this field. Additionally, we propose a new partition\nalgorithm, DBH-X, based on the DBH partitioner. We demonstrate that DBH-X\nsignificantly improves both the RF and MSIDS, compared to the baseline DBH\nalgorithm. In addition, we provide test results that show the runtime\nacceleration of GraphX-based PageRank and Label propagation algorithms.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "matsui2015sketch", "year": "2015", "title":"Sketch-based Manga Retrieval Using Manga109 Dataset", "abstract": "<p>Manga (Japanese comics) are popular worldwide. However, current e-manga\narchives offer very limited search support, including keyword-based search by\ntitle or author, or tag-based categorization. To make the manga search\nexperience more intuitive, efficient, and enjoyable, we propose a content-based\nmanga retrieval system. First, we propose a manga-specific image-describing\nframework. It consists of efficient margin labeling, edge orientation histogram\nfeature description, and approximate nearest-neighbor search using product\nquantization. Second, we propose a sketch-based interface as a natural way to\ninteract with manga content. The interface provides sketch-based querying,\nrelevance feedback, and query retouch. For evaluation, we built a novel dataset\nof manga images, Manga109, which consists of 109 comic books of 21,142 pages\ndrawn by professional manga artists. To the best of our knowledge, Manga109 is\ncurrently the biggest dataset of manga images available for research. We\nconducted a comparative study, a localization evaluation, and a large-scale\nqualitative study. From the experiments, we verified that: (1) the retrieval\naccuracy of the proposed method is higher than those of previous methods; (2)\nthe proposed method can localize an object instance with reasonable runtime and\naccuracy; and (3) sketch querying is useful for manga search.</p>\n", "tags": ["Quantisation"] },
{"key": "matsui2017pqtable", "year": "2017", "title":"Pqtable Non-exhaustive Fast Search For Product-quantized Codes Using Hash Tables", "abstract": "<p>In this paper, we propose a product quantization table (PQTable); a fast\nsearch method for product-quantized codes via hash-tables. An identifier of\neach database vector is associated with the slot of a hash table by using its\nPQ-code as a key. For querying, an input vector is PQ-encoded and hashed, and\nthe items associated with that code are then retrieved. The proposed PQTable\nproduces the same results as a linear PQ scan, and is 10^2 to 10^5 times\nfaster. Although state-of-the-art performance can be achieved by previous\ninverted-indexing-based approaches, such methods require manually-designed\nparameter setting and significant training; our PQTable is free of these\nlimitations, and therefore offers a practical and effective solution for\nreal-world problems. Specifically, when the vectors are highly compressed, our\nPQTable achieves one of the fastest search performances on a single CPU to date\nwith significantly efficient memory usage (0.059 ms per query over 10^9 data\npoints with just 5.5 GB memory consumption). Finally, we show that our proposed\nPQTable can naturally handle the codes of an optimized product quantization\n(OPQTable).</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "maziarz2021hashing", "year": "2021", "title":"Hashing Modulo Alpha-equivalence", "abstract": "<p>In many applications one wants to identify identical subtrees of a program\nsyntax tree. This identification should ideally be robust to alpha-renaming of\nthe program, but no existing technique has been shown to achieve this with good\nefficiency (better than \\(\\mathcal{O}(n^2)\\) in expression size). We present a\nnew, asymptotically efficient way to hash modulo alpha-equivalence. A key\ninsight of our method is to use a weak (commutative) hash combiner at exactly\none point in the construction, which admits an algorithm with \\(\\mathcal{O}(n\n(log n)^2)\\) time complexity. We prove that the use of the commutative combiner\nnevertheless yields a strong hash with low collision probability. Numerical\nbenchmarks attest to the asymptotic behaviour of the method.</p>\n", "tags": ["ARXIV"] },
{"key": "mccarter2022look", "year": "2022", "title":"Look-ups Are Not (yet) All You Need For Deep Learning Inference", "abstract": "<p>Fast approximations to matrix multiplication have the potential to\ndramatically reduce the cost of neural network inference. Recent work on\napproximate matrix multiplication proposed to replace costly multiplications\nwith table-lookups by fitting a fast hash function from training data. In this\nwork, we propose improvements to this previous work, targeted to the deep\nlearning inference setting, where one has access to both training data and\nfixed (already learned) model weight matrices. We further propose a fine-tuning\nprocedure for accelerating entire neural networks while minimizing loss in\naccuracy. Finally, we analyze the proposed method on a simple image\nclassification task. While we show improvements to prior work, overall\nclassification accuracy remains substantially diminished compared to exact\nmatrix multiplication. Our work, despite this negative result, points the way\ntowards future efforts to accelerate inner products with fast nonlinear hashing\nmethods.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "mccauley2018adaptive", "year": "2018", "title":"Adaptive Mapreduce Similarity Joins", "abstract": "<p>Similarity joins are a fundamental database operation. Given data sets S and\nR, the goal of a similarity join is to find all points x in S and y in R with\ndistance at most r. Recent research has investigated how locality-sensitive\nhashing (LSH) can be used for similarity join, and in particular two recent\nlines of work have made exciting progress on LSH-based join performance. Hu,\nTao, and Yi (PODS 17) investigated joins in a massively parallel setting,\nshowing strong results that adapt to the size of the output. Meanwhile, Ahle,\nAum\"uller, and Pagh (SODA 17) showed a sequential algorithm that adapts to the\nstructure of the data, matching classic bounds in the worst case but improving\nthem significantly on more structured data. We show that this adaptive strategy\ncan be adapted to the parallel setting, combining the advantages of these\napproaches. In particular, we show that a simple modification to Hu et al.’s\nalgorithm achieves bounds that depend on the density of points in the dataset\nas well as the total outsize of the output. Our algorithm uses no extra\nparameters over other LSH approaches (in particular, its execution does not\ndepend on the structure of the dataset), and is likely to be efficient in\npractice.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "mccauley2019approximate", "year": "2019", "title":"Approximate Similarity Search Under Edit Distance Using Locality-sensitive Hashing", "abstract": "<p>Edit distance similarity search, also called approximate pattern matching, is\na fundamental problem with widespread database applications. The goal of the\nproblem is to preprocess \\(n\\) strings of length \\(d\\), to quickly answer queries\n\\(q\\) of the form: if there is a database string within edit distance \\(r\\) of \\(q\\),\nreturn a database string within edit distance \\(cr\\) of \\(q\\). Previous approaches\nto this problem either rely on very large (superconstant) approximation ratios\n\\(c\\), or very small search radii \\(r\\). Outside of a narrow parameter range, these\nsolutions are not competitive with trivially searching through all \\(n\\) strings.\n  In this work give a simple and easy-to-implement hash function that can\nquickly answer queries for a wide range of parameters. Specifically, our\nstrategy can answer queries in time \\(\\tilde{O}(d3^rn^{1/c})\\). The best known\npractical results require \\(c \\gg r\\) to achieve any correctness guarantee;\nmeanwhile, the best known theoretical results are very involved and difficult\nto implement, and require query time at least \\(24^r\\). Our results significantly\nbroaden the range of parameters for which we can achieve nontrivial bounds,\nwhile retaining the practicality of a locality-sensitive hash function.\n  We also show how to apply our ideas to the closely-related Approximate\nNearest Neighbor problem for edit distance, obtaining similar time bounds.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "mccauley2024improved", "year": "2024", "title":"Improved Space-efficient Approximate Nearest Neighbor Search Using Function Inversion", "abstract": "<p>Approximate nearest neighbor search (ANN) data structures have widespread\napplications in machine learning, computational biology, and text processing.\nThe goal of ANN is to preprocess a set S so that, given a query q, we can find\na point y whose distance from q approximates the smallest distance from q to\nany point in S. For most distance functions, the best-known ANN bounds for\nhigh-dimensional point sets are obtained using techniques based on\nlocality-sensitive hashing (LSH).\n  Unfortunately, space efficiency is a major challenge for LSH-based data\nstructures. Classic LSH techniques require a very large amount of space,\noftentimes polynomial in |S|. A long line of work has developed intricate\ntechniques to reduce this space usage, but these techniques suffer from\ndownsides: they must be hand tailored to each specific LSH, are often\ncomplicated, and their space reduction comes at the cost of significantly\nincreased query times.\n  In this paper we explore a new way to improve the space efficiency of LSH\nusing function inversion techniques, originally developed in (Fiat and Naor\n2000).\n  We begin by describing how function inversion can be used to improve LSH data\nstructures. This gives a fairly simple, black box method to reduce LSH space\nusage.\n  Then, we give a data structure that leverages function inversion to improve\nthe query time of the best known near-linear space data structure for\napproximate nearest neighbor search under Euclidean distance: the ALRW data\nstructure of (Andoni, Laarhoven, Razenshteyn, and Waingarten 2017). ALRW was\npreviously shown to be optimal among “list-of-points” data structures for both\nEuclidean and Manhattan ANN; thus, in addition to giving improved bounds, our\nresults imply that list-of-points data structures are not optimal for Euclidean\nor Manhattan ANN.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "mckeown2022hamming", "year": "2022", "title":"Hamming Distributions Of Popular Perceptual Hashing Techniques", "abstract": "<p>Content-based file matching has been widely deployed for decades, largely for\nthe detection of sources of copyright infringement, extremist materials, and\nabusive sexual media. Perceptual hashes, such as Microsoft’s PhotoDNA, are one\nautomated mechanism for facilitating detection, allowing for machines to\napproximately match visual features of an image or video in a robust manner.\nHowever, there does not appear to be much public evaluation of such approaches,\nparticularly when it comes to how effective they are against content-preserving\nmodifications to media files. In this paper, we present a million-image scale\nevaluation of several perceptual hashing archetypes for popular algorithms\n(including Facebook’s PDQ, Apple’s Neuralhash, and the popular pHash library)\nagainst seven image variants. The focal point is the distribution of Hamming\ndistance scores between both unrelated images and image variants to better\nunderstand the problems faced by each approach.</p>\n", "tags": [] },
{"key": "meel2015constrained", "year": "2015", "title":"Constrained Sampling And Counting Universal Hashing Meets SAT Solving", "abstract": "<p>Constrained sampling and counting are two fundamental problems in artificial\nintelligence with a diverse range of applications, spanning probabilistic\nreasoning and planning to constrained-random verification. While the theory of\nthese problems was thoroughly investigated in the 1980s, prior work either did\nnot scale to industrial size instances or gave up correctness guarantees to\nachieve scalability. Recently, we proposed a novel approach that combines\nuniversal hashing and SAT solving and scales to formulas with hundreds of\nthousands of variables without giving up correctness guarantees. This paper\nprovides an overview of the key ingredients of the approach and discusses\nchallenges that need to be overcome to handle larger real-world instances.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "meel2020sparse", "year": "2020", "title":"Sparse Hashing For Scalable Approximate Model Counting Theory And Practice", "abstract": "<p>Given a CNF formula F on n variables, the problem of model counting or #SAT\nis to compute the number of satisfying assignments of F . Model counting is a\nfundamental but hard problem in computer science with varied applications.\nRecent years have witnessed a surge of effort towards developing efficient\nalgorithmic techniques that combine the classical 2-universal hashing with the\nremarkable progress in SAT solving over the past decade. These techniques\naugment the CNF formula F with random XOR constraints and invoke an NP oracle\nrepeatedly on the resultant CNF-XOR formulas. In practice, calls to the NP\noracle calls are replaced a SAT solver whose runtime performance is adversely\naffected by size of XOR constraints. The standard construction of 2-universal\nhash functions chooses every variable with probability p = 1/2 leading to XOR\nconstraints of size n/2 in expectation. Consequently, the challenge is to\ndesign sparse hash functions where variables can be chosen with smaller\nprobability and lead to smaller sized XOR constraints.\n  In this paper, we address this challenge from theoretical and practical\nperspectives. First, we formalize a relaxation of universal hashing, called\nconcentrated hashing and establish a novel and beautiful connection between\nconcentration measures of these hash functions and isoperimetric inequalities\non boolean hypercubes. This allows us to obtain (log m) tight bounds on\nvariance and dispersion index and show that p = O( log(m)/m ) suffices for\ndesign of sparse hash functions from {0, 1}^n to {0, 1}^m. We then use sparse\nhash functions belonging to this concentrated hash family to develop new\napproximate counting algorithms. A comprehensive experimental evaluation of our\nalgorithm on 1893 benchmarks demonstrates that usage of sparse hash functions\ncan lead to significant speedups.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "meiner2023instant", "year": "2023", "title":"Instant Complexity Reduction In Cnns Using Locality-sensitive Hashing", "abstract": "<p>To reduce the computational cost of convolutional neural networks (CNNs) for\nusage on resource-constrained devices, structured pruning approaches have shown\npromising results, drastically reducing floating-point operations (FLOPs)\nwithout substantial drops in accuracy. However, most recent methods require\nfine-tuning or specific training procedures to achieve a reasonable trade-off\nbetween retained accuracy and reduction in FLOPs. This introduces additional\ncost in the form of computational overhead and requires training data to be\navailable. To this end, we propose HASTE (Hashing for Tractable Efficiency), a\nparameter-free and data-free module that acts as a plug-and-play replacement\nfor any regular convolution module. It instantly reduces the network’s\ntest-time inference cost without requiring any training or fine-tuning. We are\nable to drastically compress latent feature maps without sacrificing much\naccuracy by using locality-sensitive hashing (LSH) to detect redundancies in\nthe channel dimension. Similar channels are aggregated to reduce the input and\nfilter depth simultaneously, allowing for cheaper convolutions. We demonstrate\nour approach on the popular vision benchmarks CIFAR-10 and ImageNet. In\nparticular, we are able to instantly drop 46.72% of FLOPs while only losing\n1.25% accuracy by just swapping the convolution modules in a ResNet34 on\nCIFAR-10 for our HASTE module.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "meisburger2020distributed", "year": "2020", "title":"Distributed Tera-scale Similarity Search With MPI Provably Efficient Similarity Search Over Billions Without A Single Distance Computation", "abstract": "<p>We present SLASH (Sketched LocAlity Sensitive Hashing), an MPI (Message\nPassing Interface) based distributed system for approximate similarity search\nover terabyte scale datasets. SLASH provides a multi-node implementation of the\npopular LSH (locality sensitive hashing) algorithm, which is generally\nimplemented on a single machine. We show how we can append the LSH algorithm\nwith heavy hitters sketches to provably solve the (high) similarity search\nproblem without a single distance computation. Overall, we mathematically show\nthat, under realistic data assumptions, we can identify the near-neighbor of a\ngiven query \\(q\\) in sub-linear (\\( \\ll O(n)\\)) number of simple sketch aggregation\noperations only. To make such a system practical, we offer a novel design and\nsketching solution to reduce the inter-machine communication overheads\nexponentially. In a direct comparison on comparable hardware, SLASH is more\nthan 10000x faster than the popular LSH package in PySpark. PySpark is a\nwidely-adopted distributed implementation of the LSH algorithm for large\ndatasets and is deployed in commercial platforms. In the end, we show how our\nsystem scale to Tera-scale Criteo dataset with more than 4 billion samples.\nSLASH can index this 2.3 terabyte data over 20 nodes in under an hour, with\nquery times in a fraction of milliseconds. To the best of our knowledge, there\nis no open-source system that can index and perform a similarity search on\nCriteo with a commodity cluster.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "meloacosta2022instance", "year": "2022", "title":"An Instance Selection Algorithm For Big Data In High Imbalanced Datasets Based On LSH", "abstract": "<p>Training of Machine Learning (ML) models in real contexts often deals with\nbig data sets and high-class imbalance samples where the class of interest is\nunrepresented (minority class). Practical solutions using classical ML models\naddress the problem of large data sets using parallel/distributed\nimplementations of training algorithms, approximate model-based solutions, or\napplying instance selection (IS) algorithms to eliminate redundant information.\nHowever, the combined problem of big and high imbalanced datasets has been less\naddressed. This work proposes three new methods for IS to be able to deal with\nlarge and imbalanced data sets. The proposed methods use Locality Sensitive\nHashing (LSH) as a base clustering technique, and then three different sampling\nmethods are applied on top of the clusters (or buckets) generated by LSH. The\nalgorithms were developed in the Apache Spark framework, guaranteeing their\nscalability. The experiments carried out in three different datasets suggest\nthat the proposed IS methods can improve the performance of a base ML model\nbetween 5% and 19% in terms of the geometric mean.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "mendelson2018anchorhash", "year": "2018", "title":"Anchorhash A Scalable Consistent Hash", "abstract": "<p>Consistent hashing (CH) is a central building block in many networking\napplications, from datacenter load-balancing to distributed storage.\nUnfortunately, state-of-the-art CH solutions cannot ensure full consistency\nunder arbitrary changes and/or cannot scale while maintaining reasonable memory\nfootprints and update times. We present AnchorHash, a scalable and\nfully-consistent hashing algorithm. AnchorHash achieves high key lookup rates,\na low memory footprint, and low update times. We formally establish its strong\ntheoretical guarantees, and present advanced implementations with a memory\nfootprint of only a few bytes per resource. Moreover, extensive evaluations\nindicate that it outperforms state-of-the-art algorithms, and that it can scale\non a single core to 100 million resources while still achieving a key lookup\nrate of more than 15 million keys per second.</p>\n", "tags": ["ARXIV"] },
{"key": "miao2024locality", "year": "2024", "title":"Locality-sensitive Hashing-based Efficient Point Transformer With Applications In High-energy Physics", "abstract": "<p>This study introduces a novel transformer model optimized for large-scale\npoint cloud processing in scientific domains such as high-energy physics (HEP)\nand astrophysics. Addressing the limitations of graph neural networks and\nstandard transformers, our model integrates local inductive bias and achieves\nnear-linear complexity with hardware-friendly regular operations. One\ncontribution of this work is the quantitative analysis of the error-complexity\ntradeoff of various sparsification techniques for building efficient\ntransformers. Our findings highlight the superiority of using\nlocality-sensitive hashing (LSH), especially OR &amp; AND-construction LSH, in\nkernel approximation for large-scale point cloud data with local inductive\nbias. Based on this finding, we propose LSH-based Efficient Point Transformer\n(HEPT), which combines E\\(^2\\)LSH with OR &amp; AND constructions and is built upon\nregular computations. HEPT demonstrates remarkable performance on two critical\nyet time-consuming HEP tasks, significantly outperforming existing GNNs and\ntransformers in accuracy and computational speed, marking a significant\nadvancement in geometric deep learning and large-scale scientific data\nprocessing. Our code is available at https://github.com/Graph-COM/HEPT.</p>\n", "tags": ["ARXIV","Deep Learning","Graph","Has Code","LSH","Supervised"] },
{"key": "microsoft2024microsoft", "year": "2024", "title":"Microsoft SPACEV-1B", "abstract": "<p>Microsoft SPACEV-1B is a new web search related dataset released by Microsoft Bing for this competition. It consists of document and query vectors encoded by Microsoft SpaceV Superior model to capture generic intent representation.</p>\n", "tags": ["ARXIV"] },
{"key": "microsoftspacev1B", "year": "2021", "title":"Microsoft SPACEV-1B", "abstract": "<p>Microsoft SPACEV-1B is a new web search related dataset released by Microsoft Bing for this competition. It consists of document and query vectors encoded by Microsoft SpaceV Superior model to capture generic intent representation.</p>\n", "tags": ["Dataset"] },
{"key": "microsoftturinganns1B", "year": "2021", "title":"Microsoft Turing-ANNS-1B", "abstract": "<p>Microsoft Turing-ANNS-1B is a new dataset being released by the Microsoft Turing team for this competition. It consists of Bing queries encoded by Turing AGI v5 that trains Transformers to capture similarity of intent in web search queries. An early version of the RNN-based AGI Encoder is described in a SIGIR’19 paper and a blogpost.</p>\n", "tags": ["Dataset"] },
{"key": "mieno2019space", "year": "2019", "title":"Space-efficient Algorithms For Computing Minimal/shortest Unique Substrings", "abstract": "<p>Given a string \\(T\\) of length \\(n\\), a substring \\(u = T[i..j]\\) of \\(T\\) is called\na shortest unique substring (SUS) for an interval \\([s,t]\\) if (a) \\(u\\) occurs\nexactly once in \\(T\\), (b) \\(u\\) contains the interval \\([s,t]\\) (i.e. \\(i \\leq s \\leq\nt \\leq j\\)), and (c) every substring \\(v\\) of \\(T\\) with \\(|v| &lt; |u|\\) containing\n\\([s,t]\\) occurs at least twice in \\(T\\). Given a query interval \\([s, t] \\subset\n[1, n]\\), the interval SUS problem is to output all the SUSs for the interval\n\\([s,t]\\). In this article, we propose a \\(4n + o(n)\\) bits data structure\nanswering an interval SUS query in output-sensitive \\(O(\\mathit{occ})\\) time,\nwhere \\(\\mathit{occ}\\) is the number of returned SUSs. Additionally, we focus on\nthe point SUS problem, which is the interval SUS problem for \\(s = t\\). Here, we\npropose a \\(\\lceil (log_2{3} + 1)n \\rceil + o(n)\\) bits data structure answering\na point SUS query in the same output-sensitive time. We also propose\nspace-efficient algorithms for computing the minimal unique substrings of \\(T\\).</p>\n", "tags": ["ARXIV"] },
{"key": "mikriukov2022deep", "year": "2022", "title":"Deep Unsupervised Contrastive Hashing For Large-scale Cross-modal Text-image Retrieval In Remote Sensing", "abstract": "<p>Due to the availability of large-scale multi-modal data (e.g., satellite\nimages acquired by different sensors, text sentences, etc) archives, the\ndevelopment of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in RS. In this paper, we focus our\nattention on cross-modal text-image retrieval, where queries from one modality\n(e.g., text) can be matched to archive entries from another (e.g., image). Most\nof the existing cross-modal text-image retrieval systems require a high number\nof labeled training samples and also do not allow fast and memory-efficient\nretrieval due to their intrinsic characteristics. These issues limit the\napplicability of the existing cross-modal retrieval systems for large-scale\napplications in RS. To address this problem, in this paper we introduce a novel\ndeep unsupervised cross-modal contrastive hashing (DUCH) method for RS\ntext-image retrieval. The proposed DUCH is made up of two main modules: 1)\nfeature extraction module (which extracts deep representations of the\ntext-image modalities); and 2) hashing module (which learns to generate\ncross-modal binary hash codes from the extracted representations). Within the\nhashing module, we introduce a novel multi-objective loss function including:\ni) contrastive objectives that enable similarity preservation in both intra-\nand inter-modal similarities; ii) an adversarial objective that is enforced\nacross two modalities for cross-modal representation consistency; iii)\nbinarization objectives for generating representative hash codes. Experimental\nresults show that the proposed DUCH outperforms state-of-the-art unsupervised\ncross-modal hashing methods on two multi-modal (image and text) benchmark\narchives in RS. Our code is publicly available at\nhttps://git.tu-berlin.de/rsim/duch.</p>\n", "tags": ["ARXIV","Cross Modal","Has Code","Image Retrieval","Unsupervised"] },
{"key": "mikriukov2022unsupervised", "year": "2022", "title":"Unsupervised Contrastive Hashing For Cross-modal Retrieval In Remote Sensing", "abstract": "<p>The development of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in remote sensing (RS). In this paper,\nwe focus our attention on cross-modal text-image retrieval, where queries from\none modality (e.g., text) can be matched to archive entries from another (e.g.,\nimage). Most of the existing cross-modal text-image retrieval systems in RS\nrequire a high number of labeled training samples and also do not allow fast\nand memory-efficient retrieval. These issues limit the applicability of the\nexisting cross-modal retrieval systems for large-scale applications in RS. To\naddress this problem, in this paper we introduce a novel unsupervised\ncross-modal contrastive hashing (DUCH) method for text-image retrieval in RS.\nTo this end, the proposed DUCH is made up of two main modules: 1) feature\nextraction module, which extracts deep representations of two modalities; 2)\nhashing module that learns to generate cross-modal binary hash codes from the\nextracted representations. We introduce a novel multi-objective loss function\nincluding: i) contrastive objectives that enable similarity preservation in\nintra- and inter-modal similarities; ii) an adversarial objective that is\nenforced across two modalities for cross-modal representation consistency; and\niii) binarization objectives for generating hash codes. Experimental results\nshow that the proposed DUCH outperforms state-of-the-art methods. Our code is\npublicly available at https://git.tu-berlin.de/rsim/duch.</p>\n", "tags": ["ARXIV","Cross Modal","Has Code","Image Retrieval","Unsupervised"] },
{"key": "miranda2022multi", "year": "2022", "title":"Multi Hash Embeddings In Spacy", "abstract": "<p>The distributed representation of symbols is one of the key technologies in\nmachine learning systems today, playing a pivotal role in modern natural\nlanguage processing. Traditional word embeddings associate a separate vector\nwith each word. While this approach is simple and leads to good performance, it\nrequires a lot of memory for representing a large vocabulary. To reduce the\nmemory footprint, the default embedding layer in spaCy is a hash embeddings\nlayer. It is a stochastic approximation of traditional embeddings that provides\nunique vectors for a large number of words without explicitly storing a\nseparate vector for each of them. To be able to compute meaningful\nrepresentations for both known and unknown words, hash embeddings represent\neach word as a summary of the normalized word form, subword information and\nword shape. Together, these features produce a multi-embedding of a word. In\nthis technical report we lay out a bit of history and introduce the embedding\nmethods in spaCy in detail. Second, we critically evaluate the hash embedding\narchitecture with multi-embeddings on Named Entity Recognition datasets from a\nvariety of domains and languages. The experiments validate most key design\nchoices behind spaCy’s embedders, but we also uncover a few surprising results.</p>\n", "tags": ["ARXIV"] },
{"key": "mirflickr2008new", "year": "2008", "title":"The MIR Flickr Retrieval Evaluation.", "abstract": "<p>In most well known image retrieval test sets, the imagery\ntypically cannot be freely distributed or is not representative of a\nlarge community of users. In this paper we present a collection\nfor the MIR community comprising 25000 images from the Flickr\nwebsite which are redistributable for research purposes and\nrepresent a real community of users both in the image content and\nimage tags. We have extracted the tags and EXIF image metadata,\nand also make all of these publicly available. In addition we\ndiscuss several challenges for benchmarking retrieval and\nclassification methods.</p>\n", "tags": ["Dataset"] },
{"key": "misra2018bernoulli", "year": "2018", "title":"Bernoulli Embeddings For Graphs", "abstract": "<p>Just as semantic hashing can accelerate information retrieval, binary valued\nembeddings can significantly reduce latency in the retrieval of graphical data.\nWe introduce a simple but effective model for learning such binary vectors for\nnodes in a graph. By imagining the embeddings as independent coin flips of\nvarying bias, continuous optimization techniques can be applied to the\napproximate expected loss. Embeddings optimized in this fashion consistently\noutperform the quantization of both spectral graph embeddings and various\nlearned real-valued embeddings, on both ranking and pre-ranking tasks for a\nvariety of datasets.</p>\n", "tags": ["ARXIV","Graph","Independent","Quantisation"] },
{"key": "mitzenmacher2012balanced", "year": "2012", "title":"Balanced Allocations And Double Hashing", "abstract": "<p>Double hashing has recently found more common usage in schemes that use\nmultiple hash functions. In double hashing, for an item \\(x\\), one generates two\nhash values \\(f(x)\\) and \\(g(x)\\), and then uses combinations \\((f(x) +k g(x)) \\bmod\nn\\) for \\(k=0,1,2,…\\) to generate multiple hash values from the initial two. We\nfirst perform an empirical study showing that, surprisingly, the performance\ndifference between double hashing and fully random hashing appears negligible\nin the standard balanced allocation paradigm, where each item is placed in the\nleast loaded of \\(d\\) choices, as well as several related variants. We then\nprovide theoretical results that explain the behavior of double hashing in this\ncontext.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "mitzenmacher2014new", "year": "2014", "title":"A New Approach To Analyzing Robin Hood Hashing", "abstract": "<p>Robin Hood hashing is a variation on open addressing hashing designed to\nreduce the maximum search time as well as the variance in the search time for\nelements in the hash table. While the case of insertions only using Robin Hood\nhashing is well understood, the behavior with deletions has remained open. Here\nwe show that Robin Hood hashing can be analyzed under the framework of\nfinite-level finite-dimensional jump Markov chains. This framework allows us to\nre-derive some past results for the insertion-only case with some new insight,\nas well as provide a new analysis for a standard deletion model, where we\nalternate between deleting a random old key and inserting a new one. In\nparticular, we show that a simple but apparently unstudied approach for\nhandling deletions with Robin Hood hashing offers good performance even under\nhigh loads.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "mitzenmacher2015more", "year": "2015", "title":"More Analysis Of Double Hashing For Balanced Allocations", "abstract": "<p>With double hashing, for a key \\(x\\), one generates two hash values \\(f(x)\\) and\n\\(g(x)\\), and then uses combinations \\((f(x) +i g(x)) \\bmod n\\) for \\(i=0,1,2,…\\)\nto generate multiple hash values in the range \\([0,n-1]\\) from the initial two.\nFor balanced allocations, keys are hashed into a hash table where each bucket\ncan hold multiple keys, and each key is placed in the least loaded of \\(d\\)\nchoices. It has been shown previously that asymptotically the performance of\ndouble hashing and fully random hashing is the same in the balanced allocation\nparadigm using fluid limit methods. Here we extend a coupling argument used by\nLueker and Molodowitch to show that double hashing and ideal uniform hashing\nare asymptotically equivalent in the setting of open address hash tables to the\nbalanced allocation setting, providing further insight into this phenomenon. We\nalso discuss the potential for and bottlenecks limiting the use this approach\nfor other multiple choice hashing schemes.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "mitzenmacher2018robust", "year": "2018", "title":"Robust Set Reconciliation Via Locality Sensitive Hashing", "abstract": "<p>We consider variations of set reconciliation problems where two parties,\nAlice and Bob, each hold a set of points in a metric space, and the goal is for\nBob to conclude with a set of points that is close to Alice’s set of points in\na well-defined way. This setting has been referred to as robust set\nreconciliation. More specifically, in one variation we examine the goal is for\nBob to end with a set of points that is close to Alice’s in earth mover’s\ndistance, and in another the goal is for Bob to have a point that is close to\neach of Alice’s. The first problem has been studied before; our results scale\nbetter with the dimension of the space. The second problem appears new.\n  Our primary novelty is utilizing Invertible Bloom Lookup Tables in\ncombination with locality sensitive hashing. This combination allows us to cope\nwith the geometric setting in a communication-efficient manner.</p>\n", "tags": ["ARXIV"] },
{"key": "miyaguchi2024tile", "year": "2024", "title":"Tile Compression And Embeddings For Multi-label Classification In Geolifeclef 2024", "abstract": "<p>We explore methods to solve the multi-label classification task posed by the\nGeoLifeCLEF 2024 competition with the DS@GT team, which aims to predict the\npresence and absence of plant species at specific locations using spatial and\ntemporal remote sensing data. Our approach uses frequency-domain coefficients\nvia the Discrete Cosine Transform (DCT) to compress and pre-compute the raw\ninput data for convolutional neural networks. We also investigate nearest\nneighborhood models via locality-sensitive hashing (LSH) for prediction and to\naid in the self-supervised contrastive learning of embeddings through tile2vec.\nOur best competition model utilized geolocation features with a leaderboard\nscore of 0.152 and a best post-competition score of 0.161. Source code and\nmodels are available at https://github.com/dsgt-kaggle-clef/geolifeclef-2024.</p>\n", "tags": ["ARXIV","Has Code","LSH","Supervised"] },
{"key": "mnist1999mnist", "year": "1999", "title":"The MNIST Database of Handwritten Digits", "abstract": "<p>The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.</p>\n", "tags": ["Dataset"] },
{"key": "mohamed2012thscalable", "year": "2012", "title":"Thscalable Distributed Trie Hashing", "abstract": "<p>In today’s world of computers, dealing with huge amounts of data is not\nunusual. The need to distribute this data in order to increase its availability\nand increase the performance of accessing it is more urgent than ever. For\nthese reasons it is necessary to develop scalable distributed data structures.\nIn this paper we propose a TH* distributed variant of the Trie Hashing data\nstructure. First we propose Thsw new version of TH without node Nil in digital\ntree (trie), then this version will be adapted to multicomputer environment.\nThe simulation results reveal that TH* is scalable in the sense that it grows\ngracefully, one bucket at a time, to a large number of servers, also TH* offers\na good storage space utilization and high query efficiency special for ordering\noperations.</p>\n", "tags": ["ARXIV"] },
{"key": "moran2024enhancing", "year": "2024", "title":"Enhancing First Story Detection Using Word Embeddings", "abstract": "<p>In this paper we show how word embeddings can be used to increase the effectiveness of a state-of-the art Locality Sensitive Hashing (LSH) based first story detection (FSD) system over a standard tweet corpus. Vocabulary mismatch, in which related tweets use different words, is a serious hindrance to the effectiveness of a modern FSD system. In this case, a tweet could be flagged as a first story even if a related tweet, which uses different but synonymous words, was already returned as a first story. In this work, we propose a novel approach to mitigate this problem of lexical variation, based on tweet expansion. In particular, we propose to expand tweets with semantically related paraphrases identified via automatically mined word embeddings over a background tweet corpus. Through experimentation on a large data stream comprised of 50 million tweets, we show that FSD effectiveness can be improved by 9.5% over a state-of-the-art FSD system.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "moran2024graph", "year": "2024", "title":"Graph Regularised Hashing", "abstract": "<p>In this paper we propose a two-step iterative scheme, Graph Regularised Hashing (GRH), for incrementally adjusting the positioning of the hashing hypersurfaces to better conform to the supervisory signal: in the first step the binary bits are regularised using a data similarity graph so that similar data points receive similar bits. In the second step the regularised hashcodes form targets for a set of binary classifiers which shift the position of each hypersurface so as to separate opposite bits with maximum margin. GRH exhibits superior retrieval accuracy to competing hashing methods.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "moran2024learning", "year": "2024", "title":"Learning To Project And Binarise For Hashing-based Approximate Nearest Neighbour Search", "abstract": "<p>In this paper we focus on improving the effectiveness of hashing-based approximate nearest neighbour search. Generating similarity preserving hashcodes for images has been shown to be an effective and efficient method for searching through large datasets. Hashcode generation generally involves two steps: bucketing the input feature space with a set of hyperplanes, followed by quantising the projection of the data-points onto the normal vectors to those hyperplanes. This procedure results in the makeup of the hashcodes depending on the positions of the data-points with respect to the hyperplanes in the feature space, allowing a degree of locality to be encoded into the hashcodes. In this paper we study the effect of learning both the hyperplanes and the thresholds as part of the same model. Most previous research either learn the hyperplanes assuming a fixed set of thresholds, or vice-versa. In our experiments over two standard image datasets we find statistically significant increases in retrieval effectiveness versus a host of state-of-the-art data-dependent and independent hashing models.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "moran2024neighbourhood", "year": "2024", "title":"Neighbourhood Preserving Quantisation For LSH", "abstract": "<p>We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.</p>\n", "tags": ["ARXIV","Independent","LSH","Quantisation"] },
{"key": "moran2024regularised", "year": "2024", "title":"Regularised Cross-modal Hashing", "abstract": "<p>In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing scheme that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the intra-modality similarity of data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyperplanes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regularisation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently improve retrieval effectiveness over state-of-the-art baselines.</p>\n", "tags": ["ARXIV","Cross Modal","Graph"] },
{"key": "moran2024variable", "year": "2024", "title":"Variable Bit Quantisation For LSH", "abstract": "<p>We introduce a scheme for optimally allocating\na variable number of bits per\nLSH hyperplane. Previous approaches assign\na constant number of bits per hyperplane.\nThis neglects the fact that a subset\nof hyperplanes may be more informative\nthan others. Our method, dubbed Variable\nBit Quantisation (VBQ), provides a datadriven\nnon-uniform bit allocation across\nhyperplanes. Despite only using a fraction\nof the available hyperplanes, VBQ outperforms\nuniform quantisation by up to 168%\nfor retrieval across standard text and image\ndatasets.</p>\n", "tags": ["ARXIV","Independent","LSH","Quantisation"] },
{"key": "mordido2021evaluating", "year": "2021", "title":"Evaluating Post-training Compression In Gans Using Locality-sensitive Hashing", "abstract": "<p>The analysis of the compression effects in generative adversarial networks\n(GANs) after training, i.e. without any fine-tuning, remains an unstudied,\nalbeit important, topic with the increasing trend of their computation and\nmemory requirements. While existing works discuss the difficulty of compressing\nGANs during training, requiring novel methods designed with the instability of\nGANs training in mind, we show that existing compression methods (namely\nclipping and quantization) may be directly applied to compress GANs\npost-training, without any additional changes. High compression levels may\ndistort the generated set, likely leading to an increase of outliers that may\nnegatively affect the overall assessment of existing k-nearest neighbor (KNN)\nbased metrics. We propose two new precision and recall metrics based on\nlocality-sensitive hashing (LSH), which, on top of increasing the outlier\nrobustness, decrease the complexity of assessing an evaluation sample against\n\\(n\\) reference samples from \\(O(n)\\) to \\(O(log(n))\\), if using LSH and KNN, and to\n\\(O(1)\\), if only applying LSH. We show that low-bit compression of several\npre-trained GANs on multiple datasets induces a trade-off between precision and\nrecall, retaining sample quality while sacrificing sample diversity.</p>\n", "tags": ["ARXIV","Independent","LSH","Quantisation"] },
{"key": "morgado2020deep", "year": "2020", "title":"Deep Hashing With Hash-consistent Large Margin Proxy Embeddings", "abstract": "<p>Image hash codes are produced by binarizing the embeddings of convolutional\nneural networks (CNN) trained for either classification or retrieval. While\nproxy embeddings achieve good performance on both tasks, they are non-trivial\nto binarize, due to a rotational ambiguity that encourages non-binary\nembeddings. The use of a fixed set of proxies (weights of the CNN\nclassification layer) is proposed to eliminate this ambiguity, and a procedure\nto design proxy sets that are nearly optimal for both classification and\nhashing is introduced. The resulting hash-consistent large margin (HCLM)\nproxies are shown to encourage saturation of hashing units, thus guaranteeing a\nsmall binarization error, while producing highly discriminative hash-codes. A\nsemantic extension (sHCLM), aimed to improve hashing performance in a transfer\nscenario, is also proposed. Extensive experiments show that sHCLM embeddings\nachieve significant improvements over state-of-the-art hashing procedures on\nseveral small and large datasets, both within and beyond the set of training\nclasses.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "morgado2024deep", "year": "2024", "title":"Deep Hashing With Hash-consistent Large Margin Proxy Embeddings", "abstract": "<p>Image hash codes are produced by binarizing\nthe embeddings of convolutional neural networks (CNN)\ntrained for either classification or retrieval. While proxy\nembeddings achieve good performance on both tasks,\nthey are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use\nof a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and\na procedure to design proxy sets that are nearly optimal\nfor both classification and hashing is introduced. The\nresulting hash-consistent large margin (HCLM) proxies\nare shown to encourage saturation of hashing units, thus\nguaranteeing a small binarization error, while producing\nhighly discriminative hash-codes. A semantic extension\n(sHCLM), aimed to improve hashing performance in\na transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant\nimprovements over state-of-the-art hashing procedures\non several small and large datasets, both within and\nbeyond the set of training classes.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "morozov2019unsupervised", "year": "2019", "title":"Unsupervised Neural Quantization For Compressed-domain Similarity Search", "abstract": "<p>We tackle the problem of unsupervised visual descriptors compression, which\nis a key ingredient of large-scale image retrieval systems. While the deep\nlearning machinery has benefited literally all computer vision pipelines, the\nexisting state-of-the-art compression methods employ shallow architectures, and\nwe aim to close this gap by our paper. In more detail, we introduce a DNN\narchitecture for the unsupervised compressed-domain retrieval, based on\nmulti-codebook quantization. The proposed architecture is designed to\nincorporate both fast data encoding and efficient distances computation via\nlookup tables. We demonstrate the exceptional advantage of our scheme over\nexisting quantization approaches on several datasets of visual descriptors via\noutperforming the previous state-of-the-art by a large margin.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "morris2016faster", "year": "2016", "title":"Faster Kernels For Graphs With Continuous Attributes Via Hashing", "abstract": "<p>While state-of-the-art kernels for graphs with discrete labels scale well to\ngraphs with thousands of nodes, the few existing kernels for graphs with\ncontinuous attributes, unfortunately, do not scale well. To overcome this\nlimitation, we present hash graph kernels, a general framework to derive\nkernels for graphs with continuous attributes from discrete ones. The idea is\nto iteratively turn continuous attributes into discrete labels using randomized\nhash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman\nsubtree kernel and for the shortest-path kernel. The resulting novel graph\nkernels are shown to be, both, able to handle graphs with continuous attributes\nand scalable to large graphs and data sets. This is supported by our\ntheoretical analysis and demonstrated by an extensive experimental evaluation.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "morvan2017streaming", "year": "2017", "title":"Streaming Binary Sketching Based On Subspace Tracking And Diagonal Uniformization", "abstract": "<p>In this paper, we address the problem of learning compact\nsimilarity-preserving embeddings for massive high-dimensional streams of data\nin order to perform efficient similarity search. We present a new online method\nfor computing binary compressed representations -sketches- of high-dimensional\nreal feature vectors. Given an expected code length \\(c\\) and high-dimensional\ninput data points, our algorithm provides a \\(c\\)-bits binary code for preserving\nthe distance between the points from the original high-dimensional space. Our\nalgorithm does not require neither the storage of the whole dataset nor a\nchunk, thus it is fully adaptable to the streaming setting. It also provides\nlow time complexity and convergence guarantees. We demonstrate the quality of\nour binary sketches through experiments on real data for the nearest neighbors\nsearch task in the online setting.</p>\n", "tags": ["ARXIV"] },
{"key": "morvan2018needs", "year": "2018", "title":"On The Needs For Rotations In Hypercubic Quantization Hashing", "abstract": "<p>The aim of this paper is to endow the well-known family of hypercubic\nquantization hashing methods with theoretical guarantees. In hypercubic\nquantization, applying a suitable (random or learned) rotation after\ndimensionality reduction has been experimentally shown to improve the results\naccuracy in the nearest neighbors search problem. We prove in this paper that\nthe use of these rotations is optimal under some mild assumptions: getting\noptimal binary sketches is equivalent to applying a rotation uniformizing the\ndiagonal of the covariance matrix between data points. Moreover, for two closed\npoints, the probability to have dissimilar binary sketches is upper bounded by\na factor of the initial distance between the data points. Relaxing these\nassumptions, we obtain a general concentration result for random matrices. We\nalso provide some experiments illustrating these theoretical points and compare\na set of algorithms in both the batch and online settings.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "morère2016group", "year": "2016", "title":"Group Invariant Deep Representations For Image Instance Retrieval", "abstract": "<p>Most image instance retrieval pipelines are based on comparison of vectors\nknown as global image descriptors between a query image and the database\nimages. Due to their success in large scale image classification,\nrepresentations extracted from Convolutional Neural Networks (CNN) are quickly\ngaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors\nfor image instance retrieval. While CNN-based descriptors are generally\nremarked for good retrieval performance at lower bitrates, they nevertheless\npresent a number of drawbacks including the lack of robustness to common object\ntransformations such as rotations compared with their interest point based FV\ncounterparts.\n  In this paper, we propose a method for computing invariant global descriptors\nfrom CNNs. Our method implements a recently proposed mathematical theory for\ninvariance in a sensory cortex modeled as a feedforward neural network. The\nresulting global descriptors can be made invariant to multiple arbitrary\ntransformation groups while retaining good discriminativeness.\n  Based on a thorough empirical evaluation using several publicly available\ndatasets, we show that our method is able to significantly and consistently\nimprove retrieval results every time a new type of invariance is incorporated.\nWe also show that our method which has few parameters is not prone to\noverfitting: improvements generalize well across datasets with different\nproperties with regard to invariances. Finally, we show that our descriptors\nare able to compare favourably to other state-of-the-art compact descriptors in\nsimilar bitranges, exceeding the highest retrieval results reported in the\nliterature on some datasets. A dedicated dimensionality reduction step\n–quantization or hashing– may be able to further improve the competitiveness\nof the descriptors.</p>\n", "tags": ["ARXIV","CNN","Quantisation","Supervised"] },
{"key": "morère2016nested", "year": "2016", "title":"Nested Invariance Pooling And RBM Hashing For Image Instance Retrieval", "abstract": "<p>The goal of this work is the computation of very compact binary hashes for\nimage instance retrieval. Our approach has two novel contributions. The first\none is Nested Invariance Pooling (NIP), a method inspired from i-theory, a\nmathematical theory for computing group invariant transformations with\nfeed-forward neural networks. NIP is able to produce compact and\nwell-performing descriptors with visual representations extracted from\nconvolutional neural networks. We specifically incorporate scale, translation\nand rotation invariances but the scheme can be extended to any arbitrary sets\nof transformations. We also show that using moments of increasing order\nthroughout nesting is important. The NIP descriptors are then hashed to the\ntarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novel\nbatch-level regularization scheme specifically designed for the purpose of\nhashing (RBMH). A thorough empirical evaluation with state-of-the-art shows\nthat the results obtained both with the NIP descriptors and the NIP+RBMH hashes\nare consistently outstanding across a wide range of datasets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "mou2016refined", "year": "2016", "title":"A Refined Analysis Of LSH For Well-dispersed Data Points", "abstract": "<p>Near neighbor problems are fundamental in algorithms for high-dimensional\nEuclidean spaces. While classical approaches suffer from the curse of\ndimensionality, locality sensitive hashing (LSH) can effectively solve\na-approximate r-near neighbor problem, and has been proven to be optimal in the\nworst case. However, for real-world data sets, LSH can naturally benefit from\nwell-dispersed data and low doubling dimension, leading to significantly\nimproved performance. In this paper, we address this issue and propose a\nrefined analyses for running time of approximating near neighbors queries via\nLSH. We characterize dispersion of data using N_b, the number of b*r-near pairs\namong the data points. Combined with optimal data-oblivious LSH scheme, we get\na new query time bound depending on N_b and doubling dimension. For many\nnatural scenarios where points are well-dispersed or lying in a\nlow-doubling-dimension space, our result leads to sharper performance than\nexisting worst-case analysis. This paper not only present first rigorous proof\non how LSHs make use of the structure of data points, but also provide\nimportant insights into parameter setting in the practice of LSH beyond worst\ncase. Besides, the techniques in our analysis involve a generalized version of\nsphere packing problem, which might be of some independent interest.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "mounits2005new", "year": "2005", "title":"New Upper Bounds On A(nd)", "abstract": "<p>Upper bounds on the maximum number of codewords in a binary code of a given\nlength and minimum Hamming distance are considered. New bounds are derived by a\ncombination of linear programming and counting arguments. Some of these bounds\nimprove on the best known analytic bounds. Several new record bounds are\nobtained for codes with small lengths.</p>\n", "tags": ["ARXIV"] },
{"key": "mounits2007lower", "year": "2007", "title":"Lower Bounds On The Minimum Average Distance Of Binary Codes", "abstract": "<p>New lower bounds on the minimum average Hamming distance of binary codes are\nderived. The bounds are obtained using linear programming approach.</p>\n", "tags": ["ARXIV"] },
{"key": "mu2016deep", "year": "2016", "title":"Deep Hashing A Joint Approach For Image Signature Learning", "abstract": "<p>Similarity-based image hashing represents crucial technique for visual data\nstorage reduction and expedited image search. Conventional hashing schemes\ntypically feed hand-crafted features into hash functions, which separates the\nprocedures of feature extraction and hash function learning. In this paper, we\npropose a novel algorithm that concurrently performs feature engineering and\nnon-linear supervised hashing function learning. Our technical contributions in\nthis paper are two-folds: 1) deep network optimization is often achieved by\ngradient propagation, which critically requires a smooth objective function.\nThe discrete nature of hash codes makes them not amenable for gradient-based\noptimization. To address this issue, we propose an exponentiated hashing loss\nfunction and its bilinear smooth approximation. Effective gradient calculation\nand propagation are thereby enabled; 2) pre-training is an important trick in\nsupervised deep learning. The impact of pre-training on the hash code quality\nhas never been discussed in current deep hashing literature. We propose a\npre-training scheme inspired by recent advance in deep network based image\nclassification, and experimentally demonstrate its effectiveness. Comprehensive\nquantitative evaluations are conducted on several widely-used image benchmarks.\nOn all benchmarks, our proposed deep hashing algorithm outperforms all\nstate-of-the-art competitors by significant margins. In particular, our\nalgorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy\nwith only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In\ncomparison, the best accuracies obtained on CIFAR10 by existing hashing\nalgorithms without or with deep networks are known to be 0.36 and 0.58\nrespectively.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "mu2019empirical", "year": "2019", "title":"An Empirical Comparison Of FAISS And FENSHSES For Nearest Neighbor Search In Hamming Space", "abstract": "<p>In this paper, we compare the performances of FAISS and FENSHSES on nearest\nneighbor search in Hamming space–a fundamental task with ubiquitous\napplications in nowadays eCommerce. Comprehensive evaluations are made in terms\nof indexing speed, search latency and RAM consumption. This comparison is\nconducted towards a better understanding on trade-offs between nearest neighbor\nsearch systems implemented in main memory and the ones implemented in secondary\nmemory, which is largely unaddressed in literature.</p>\n", "tags": ["ARXIV"] },
{"key": "mu2019fast", "year": "2019", "title":"Fast And Exact Nearest Neighbor Search In Hamming Space On Full-text Search Engines", "abstract": "<p>A growing interest has been witnessed recently from both academia and\nindustry in building nearest neighbor search (NNS) solutions on top of\nfull-text search engines. Compared with other NNS systems, such solutions are\ncapable of effectively reducing main memory consumption, coherently supporting\nmulti-model search and being immediately ready for production deployment. In\nthis paper, we continue the journey to explore specifically how to empower\nfull-text search engines with fast and exact NNS in Hamming space (i.e., the\nset of binary codes). By revisiting three techniques (bit operation, subs-code\nfiltering and data preprocessing with permutation) in information retrieval\nliterature, we develop a novel engineering solution for full-text search\nengines to efficiently accomplish this special but important NNS task. In the\nexperiment, we show that our proposed approach enables full-text search engines\nto achieve significant speed-ups over its state-of-the-art term match approach\nfor NNS within binary codes.</p>\n", "tags": ["ARXIV"] },
{"key": "mukherjee2012efficient", "year": "2012", "title":"An Efficient Cryptographic Hash Algorithm (BSA)", "abstract": "<p>Recent cryptanalytic attacks have exposed the vulnerabilities of some widely\nused cryptographic hash functions like MD5 and SHA-1. Attacks in the line of\ndifferential attacks have been used to expose the weaknesses of several other\nhash functions like RIPEMD, HAVAL. In this paper we propose a new efficient\nhash algorithm that provides a near random hash output and overcomes some of\nthe earlier weaknesses. Extensive simulations and comparisons with some\nexisting hash functions have been done to prove the effectiveness of the BSA,\nwhich is an acronym for the name of the 3 authors.</p>\n", "tags": ["COLT","Graph","Independent"] },
{"key": "mukherjee2024nmf", "year": "2024", "title":"An NMF Perspective On Binary Hashing", "abstract": "<p>The pervasiveness of massive data repositories has led\nto much interest in efficient methods for indexing, search,\nand retrieval. For image data, a rapidly developing body of\nwork for these applications shows impressive performance\nwith methods that broadly fall under the umbrella term of\nBinary Hashing. Given a distance matrix, a binary hashing\nalgorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the\noriginal distances. The formulation is non-convex — so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective\nthat is numerically more tractable. In this paper, we first\nderive an Augmented Lagrangian approach to optimize the\nstandard binary Hashing objective (i.e., maintain fidelity\nwith a given distance matrix). With appropriate step sizes,\nwe find that this scheme already yields results that match or\nsubstantially outperform state of the art methods on most\nbenchmarks used in the literature. Then, to allow the model\nto scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non-negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm — whose parallelization properties\nare exploited to obtain a fast GPU based implementation.\nWe give a probabilistic analysis of our initialization scheme\nand present a range of experiments to show that the method\nis simple to implement and competes favorably with available methods (both for optimization and generalization).</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "mulzer2014approximate", "year": "2014", "title":"Approximate K-flat Nearest Neighbor Search", "abstract": "<p>Let \\(k\\) be a nonnegative integer. In the approximate \\(k\\)-flat nearest\nneighbor (\\(k\\)-ANN) problem, we are given a set \\(P \\subset \\mathbb{R}^d\\) of \\(n\\)\npoints in \\(d\\)-dimensional space and a fixed approximation factor \\(c &gt; 1\\). Our\ngoal is to preprocess \\(P\\) so that we can efficiently answer approximate\n\\(k\\)-flat nearest neighbor queries: given a \\(k\\)-flat \\(F\\), find a point in \\(P\\)\nwhose distance to \\(F\\) is within a factor \\(c\\) of the distance between \\(F\\) and\nthe closest point in \\(P\\). The case \\(k = 0\\) corresponds to the well-studied\napproximate nearest neighbor problem, for which a plethora of results are\nknown, both in low and high dimensions. The case \\(k = 1\\) is called approximate\nline nearest neighbor. In this case, we are aware of only one provably\nefficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For \\(k\n\\geq 2\\), we know of no previous results.\n  We present the first efficient data structure that can handle approximate\nnearest neighbor queries for arbitrary \\(k\\). We use a data structure for\n\\(0\\)-ANN-queries as a black box, and the performance depends on the parameters\nof the \\(0\\)-ANN solution: suppose we have an \\(0\\)-ANN structure with query time\n\\(O(n^{\\rho})\\) and space requirement \\(O(n^{1+\\sigma})\\), for \\(\\rho, \\sigma &gt; 0\\).\nThen we can answer \\(k\\)-ANN queries in time \\(O(n^{k/(k + 1 - \\rho) + t})\\) and\nspace \\(O(n^{1+\\sigma k/(k + 1 - \\rho)} + nlog^{O(1/t)} n)\\). Here, \\(t &gt; 0\\) is\nan arbitrary constant and the \\(O\\)-notation hides exponential factors in \\(k\\),\n\\(1/t\\), and \\(c\\) and polynomials in \\(d\\). Our new data structures also give an\nimprovement in the space requirement over the previous result for \\(1\\)-ANN: we\ncan achieve near-linear space and sublinear query time, a further step towards\npractical applications where space constitutes the bottleneck.</p>\n", "tags": ["ARXIV"] },
{"key": "murtagh2011fast", "year": "2011", "title":"Fast Linear Time M-adic Hierarchical Clustering For Search And Retrieval Using The Baire Metric With Linkages To Generalized Ultrametrics Hashing Formal Concept Analysis And Precision Of Data Measurement", "abstract": "<p>We describe many vantage points on the Baire metric and its use in clustering\ndata, or its use in preprocessing and structuring data in order to support\nsearch and retrieval operations. In some cases, we proceed directly to clusters\nand do not directly determine the distances. We show how a hierarchical\nclustering can be read directly from one pass through the data. We offer\ninsights also on practical implications of precision of data measurement. As a\nmechanism for treating multidimensional data, including very high dimensional\ndata, we use random projections.</p>\n", "tags": ["Unsupervised"] },
{"key": "musser2008fast", "year": "2008", "title":"A Fast Generic Sequence Matching Algorithm", "abstract": "<p>A string matching – and more generally, sequence matching – algorithm is\npresented that has a linear worst-case computing time bound, a low worst-case\nbound on the number of comparisons (2n), and sublinear average-case behavior\nthat is better than that of the fastest versions of the Boyer-Moore algorithm.\nThe algorithm retains its efficiency advantages in a wide variety of sequence\nmatching problems of practical interest, including traditional string matching;\nlarge-alphabet problems (as in Unicode strings); and small-alphabet,\nlong-pattern problems (as in DNA searches). Since it is expressed as a generic\nalgorithm for searching in sequences over an arbitrary type T, it is well\nsuited for use in generic software libraries such as the C++ Standard Template\nLibrary. The algorithm was obtained by adding to the Knuth-Morris-Pratt\nalgorithm one of the pattern-shifting techniques from the Boyer-Moore\nalgorithm, with provision for use of hashing in this technique. In situations\nin which a hash function or random access to the sequences is not available,\nthe algorithm falls back to an optimized version of the Knuth-Morris-Pratt\nalgorithm.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "müller2022instant", "year": "2022", "title":"Instant Neural Graphics Primitives With A Multiresolution Hash Encoding", "abstract": "<p>Neural graphics primitives, parameterized by fully connected neural networks,\ncan be costly to train and evaluate. We reduce this cost with a versatile new\ninput encoding that permits the use of a smaller network without sacrificing\nquality, thus significantly reducing the number of floating point and memory\naccess operations: a small neural network is augmented by a multiresolution\nhash table of trainable feature vectors whose values are optimized through\nstochastic gradient descent. The multiresolution structure allows the network\nto disambiguate hash collisions, making for a simple architecture that is\ntrivial to parallelize on modern GPUs. We leverage this parallelism by\nimplementing the whole system using fully-fused CUDA kernels with a focus on\nminimizing wasted bandwidth and compute operations. We achieve a combined\nspeedup of several orders of magnitude, enabling training of high-quality\nneural graphics primitives in a matter of seconds, and rendering in tens of\nmilliseconds at a resolution of \\({1920!\\times!1080}\\).</p>\n", "tags": ["Graph","Supervised"] },
{"key": "naidan2015permutation", "year": "2015", "title":"Permutation Search Methods Are Efficient Yet Faster Search Is Possible", "abstract": "<p>We survey permutation-based methods for approximate k-nearest neighbor\nsearch. In these methods, every data point is represented by a ranked list of\npivots sorted by the distance to this point. Such ranked lists are called\npermutations. The underpinning assumption is that, for both metric and\nnon-metric spaces, the distance between permutations is a good proxy for the\ndistance between original points. Thus, it should be possible to efficiently\nretrieve most true nearest neighbors by examining only a tiny subset of data\npoints whose permutations are similar to the permutation of a query. We further\ntest this assumption by carrying out an extensive experimental evaluation where\npermutation methods are pitted against state-of-the art benchmarks (the\nmulti-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety\nof realistically large data set from the image and textual domain. The focus is\non the high-accuracy retrieval methods for generic spaces. Additionally, we\nassume that both data and indices are stored in main memory. We find\npermutation methods to be reasonably efficient and describe a setup where these\nmethods are most useful. To ease reproducibility, we make our software and data\nsets publicly available.</p>\n", "tags": ["ARXIV","Graph","Independent","LSH","Survey Paper"] },
{"key": "najibi2015large", "year": "2015", "title":"On Large-scale Retrieval Binary Or N-ary Coding", "abstract": "<p>The growing amount of data available in modern-day datasets makes the need to\nefficiently search and retrieve information. To make large-scale search\nfeasible, Distance Estimation and Subset Indexing are the main approaches.\nAlthough binary coding has been popular for implementing both techniques, n-ary\ncoding (known as Product Quantization) is also very effective for Distance\nEstimation. However, their relative performance has not been studied for Subset\nIndexing. We investigate whether binary or n-ary coding works better under\ndifferent retrieval strategies. This leads to the design of a new n-ary coding\nmethod, “Linear Subspace Quantization (LSQ)” which, unlike other n-ary\nencoders, can be used as a similarity-preserving embedding. Experiments on\nimage retrieval show that when Distance Estimation is used, n-ary LSQ\noutperforms other methods. However, when Subset Indexing is applied,\ninterestingly, binary codings are more effective and binary LSQ achieves the\nbest accuracy.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "nardini2024efficient", "year": "2024", "title":"Efficient Multi-vector Dense Retrieval Using Bit Vectors", "abstract": "<p>Dense retrieval techniques employ pre-trained large language models to build\na high-dimensional representation of queries and passages. These\nrepresentations compute the relevance of a passage w.r.t. to a query using\nefficient similarity measures. In this line, multi-vector representations show\nimproved effectiveness at the expense of a one-order-of-magnitude increase in\nmemory footprint and query latency by encoding queries and documents on a\nper-token level. Recently, PLAID has tackled these problems by introducing a\ncentroid-based term representation to reduce the memory impact of multi-vector\nsystems. By exploiting a centroid interaction mechanism, PLAID filters out\nnon-relevant documents, thus reducing the cost of the successive ranking\nstages. This paper proposes ``Efficient Multi-Vector dense retrieval with Bit\nvectors’’ (EMVB), a novel framework for efficient query processing in\nmulti-vector dense retrieval. First, EMVB employs a highly efficient\npre-filtering step of passages using optimized bit vectors. Second, the\ncomputation of the centroid interaction happens column-wise, exploiting SIMD\ninstructions, thus reducing its latency. Third, EMVB leverages Product\nQuantization (PQ) to reduce the memory footprint of storing vector\nrepresentations while jointly allowing for fast late interaction. Fourth, we\nintroduce a per-document term filtering method that further improves the\nefficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVB\nis up to 2.8x faster while reducing the memory footprint by 1.8x with no loss\nin retrieval accuracy compared to PLAID.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "ndoundam2011collision", "year": "2011", "title":"Collision-resistant Hash Function Based On Composition Of Functions", "abstract": "<p>cryptographic hash function is a deterministic procedure that compresses an\narbitrary block of numerical data and returns a fixed-size bit string. There\nexist many hash functions: MD5, HAVAL, SHA, … It was reported that these hash\nfunctions are not longer secure. Our work is focused in the construction of a\nnew hash function based on composition of functions. The construction used the\nNP-completeness of Three-dimensional contingency tables and the relaxation of\nthe constraint that a hash function should also be a compression function.</p>\n", "tags": ["Graph","Independent"] },
{"key": "ndungu2023deep", "year": "2023", "title":"Deep Supervised Hashing For Fast Retrieval Of Radio Image Cubes", "abstract": "<p>The shear number of sources that will be detected by next-generation radio\nsurveys will be astronomical, which will result in serendipitous discoveries.\nData-dependent deep hashing algorithms have been shown to be efficient at image\nretrieval tasks in the fields of computer vision and multimedia. However, there\nare limited applications of these methodologies in the field of astronomy. In\nthis work, we utilize deep hashing to rapidly search for similar images in a\nlarge database. The experiment uses a balanced dataset of 2708 samples\nconsisting of four classes: Compact, FRI, FRII, and Bent. The performance of\nthe method was evaluated using the mean average precision (mAP) metric where a\nprecision of 88.5\\% was achieved. The experimental results demonstrate the\ncapability to search and retrieve similar radio images efficiently and at\nscale. The retrieval is based on the Hamming distance between the binary hash\nof the query image and those of the reference images in the database.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "netay2024hashing", "year": "2024", "title":"Hashing Geographical Point Data Using The Space-filling H-curve", "abstract": "<p>We construct geohashing procedure based on using of space-filling H-curve.\nThis curve provides a way to construct geohash with less computations than the\nconstruction based on usage of Hilbert curve. At the same time, H-curve has\nbetter clustering properties.</p>\n", "tags": ["ARXIV","Graph","Unsupervised"] },
{"key": "neyshabur2013power", "year": "2013", "title":"The Power Of Asymmetry In Binary Hashing", "abstract": "<p>When approximating binary similarity using the hamming distance between short binary hashes, we shown that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e.~by approximating the similarity between \\(x\\) and \\(x’\\) as the hamming distance between \\(f(x)\\) and \\(g(x’)\\), for two distinct binary codes \\(f,g\\), rather than as the hamming distance between \\(f(x)\\) and \\(f(x’)\\).</p>\n", "tags": ["NEURIPS"] },
{"key": "neyshabur2014clustering", "year": "2014", "title":"Clustering Hamming Embedding Generalized LSH And The Max Norm", "abstract": "<p>We study the convex relaxation of clustering and hamming embedding, focusing\non the asymmetric case (co-clustering and asymmetric hamming embedding),\nunderstanding their relationship to LSH as studied by (Charikar 2002) and to\nthe max-norm ball, and the differences between their symmetric and asymmetric\nversions.</p>\n", "tags": ["ARXIV","LSH","Unsupervised"] },
{"key": "neyshabur2014symmetric", "year": "2014", "title":"On Symmetric And Asymmetric Lshs For Inner Product Search", "abstract": "<p>We consider the problem of designing locality sensitive hashes (LSH) for\ninner product similarity, and of the power of asymmetric hashes in this\ncontext. Shrivastava and Li argue that there is no symmetric LSH for the\nproblem and propose an asymmetric LSH based on different mappings for query and\ndatabase points. However, we show there does exist a simple symmetric LSH that\nenjoys stronger guarantees and better empirical performance than the asymmetric\nLSH they suggest. We also show a variant of the settings where asymmetry is\nin-fact needed, but there a different asymmetric LSH is required.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "neyshabur2024power", "year": "2024", "title":"The Power Of Asymmetry In Binary Hashing", "abstract": "<p>When approximating binary similarity using the hamming distance between short\nbinary hashes, we show that even if the similarity is symmetric, we can have\nshorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\n0\nas the hamming distance between f(x)\nand g(x0), for two distinct binary codes f, g, rather than as the hamming distance\nbetween f(x) and f(x0).</p>\n", "tags": ["ARXIV"] },
{"key": "ng2023unsupervised", "year": "2023", "title":"Unsupervised Hashing With Similarity Distribution Calibration", "abstract": "<p>Unsupervised hashing methods typically aim to preserve the similarity between\ndata points in a feature space by mapping them to binary hash codes. However,\nthese methods often overlook the fact that the similarity between data points\nin the continuous feature space may not be preserved in the discrete hash code\nspace, due to the limited similarity range of hash codes. The similarity range\nis bounded by the code length and can lead to a problem known as similarity\ncollapse. That is, the positive and negative pairs of data points become less\ndistinguishable from each other in the hash space. To alleviate this problem,\nin this paper a novel Similarity Distribution Calibration (SDC) method is\nintroduced. SDC aligns the hash code similarity distribution towards a\ncalibration distribution (e.g., beta distribution) with sufficient spread\nacross the entire similarity range, thus alleviating the similarity collapse\nproblem. Extensive experiments show that our SDC outperforms significantly the\nstate-of-the-art alternatives on coarse category-level and instance-level image\nretrieval. Code is available at https://github.com/kamwoh/sdc.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Unsupervised"] },
{"key": "ng2024concepthash", "year": "2024", "title":"Concepthash Interpretable Fine-grained Hashing Via Concept Discovery", "abstract": "<p>Existing fine-grained hashing methods typically lack code interpretability as\nthey compute hash code bits holistically using both global and local features.\nTo address this limitation, we propose ConceptHash, a novel method that\nachieves sub-code level interpretability. In ConceptHash, each sub-code\ncorresponds to a human-understandable concept, such as an object part, and\nthese concepts are automatically discovered without human annotations.\nSpecifically, we leverage a Vision Transformer architecture and introduce\nconcept tokens as visual prompts, along with image patch tokens as model\ninputs. Each concept is then mapped to a specific sub-code at the model output,\nproviding natural sub-code interpretability. To capture subtle visual\ndifferences among highly similar sub-categories (e.g., bird species), we\nincorporate language guidance to ensure that the learned hash codes are\ndistinguishable within fine-grained object classes while maintaining semantic\nalignment. This approach allows us to develop hash codes that exhibit\nsimilarity within families of species while remaining distinct from species in\nother families. Extensive experiments on four fine-grained image retrieval\nbenchmarks demonstrate that ConceptHash outperforms previous methods by a\nsignificant margin, offering unique sub-code interpretability as an additional\nbenefit. Code at: https://github.com/kamwoh/concepthash.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Independent"] },
{"key": "ng2024unsupervised", "year": "2024", "title":"Unsupervised Hashing With Similarity Distribution Calibration", "abstract": "<p>Unsupervised hashing methods typically aim to preserve the similarity between data points in a feature space by mapping them to binary hash codes. However, these methods often overlook the fact that the similarity between data points in the continuous feature space may not be preserved in the discrete hash code space, due to the limited similarity range of hash codes. The similarity range is bounded by the code length and can lead to a problem known as similarity collapse. That is, the positive and negative pairs of data points become less distinguishable from each other in the hash space. To alleviate this problem, in this paper a novel Simialrity Distribution Calibration (SDC) method is introduced. SDC aligns the hash code similarity distribution towards a calibration distribution (e.g., beta distribution) with sufficient spread across the entire similarity range, thus alleviating the similarity collapse problem. Extensive experiments show that our SDC outperforms significantly the state-of-the-art alternatives on coarse category-level and instance-level image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "nguyen2013approximate", "year": "2013", "title":"Approximate Nearest Neighbor Search In ell_p", "abstract": "<p>We present a new locality sensitive hashing (LSH) algorithm for\n\\(c\\)-approximate nearest neighbor search in \\(\\ell_p\\) with \\(1&lt;p&lt;2\\). For a\ndatabase of \\(n\\) points in \\(\\ell_p\\), we achieve \\(O(dn^{\\rho})\\) query time and\n\\(O(dn+n^{1+\\rho})\\) space, where \\(\\rho \\le O((\\ln c)^2/c^p)\\). This improves upon\nthe previous best upper bound \\(\\rho\\le 1/c\\) by Datar et al. (SOCG 2004), and is\nclose to the lower bound \\(\\rho \\ge 1/c^p\\) by O’Donnell, Wu and Zhou (ITCS\n2011). The proof is a simple generalization of the LSH scheme for \\(ℓ₂\\) by\nAndoni and Indyk (FOCS 2006).</p>\n", "tags": ["ARXIV","FOCS","Independent","LSH"] },
{"key": "nguyen2021oscar", "year": "2021", "title":"Oscar-net Object-centric Scene Graph Attention For Image Attribution", "abstract": "<p>Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject’s visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.</p>\n", "tags": ["ARXIV","Graph","Self Supervised"] },
{"key": "nikhal2023hashreid", "year": "2023", "title":"Hashreid Dynamic Network With Binary Codes For Efficient Person Re-identification", "abstract": "<p>Biometric applications, such as person re-identification (ReID), are often\ndeployed on energy constrained devices. While recent ReID methods prioritize\nhigh retrieval performance, they often come with large computational costs and\nhigh search time, rendering them less practical in real-world settings. In this\nwork, we propose an input-adaptive network with multiple exit blocks, that can\nterminate computation early if the retrieval is straightforward or noisy,\nsaving a lot of computation. To assess the complexity of the input, we\nintroduce a temporal-based classifier driven by a new training strategy.\nFurthermore, we adopt a binary hash code generation approach instead of relying\non continuous-valued features, which significantly improves the search process\nby a factor of 20. To ensure similarity preservation, we utilize a new ranking\nregularizer that bridges the gap between continuous and binary features.\nExtensive analysis of our proposed method is conducted on three datasets:\nMarket1501, MSMT17 (Multi-Scene Multi-Time), and the BGC1 (BRIAR Government\nCollection). Using our approach, more than 70% of the samples with compact hash\ncodes exit early on the Market1501 dataset, saving 80% of the networks\ncomputational cost and improving over other hash-based methods by 60%. These\nresults demonstrate a significant improvement over dynamic networks and\nshowcase comparable accuracy performance to conventional ReID methods. Code\nwill be made available.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "nikolić2020bitpruning", "year": "2020", "title":"Bitpruning Learning Bitlengths For Aggressive And Accurate Quantization", "abstract": "<p>Neural networks have demonstrably achieved state-of-the art accuracy using\nlow-bitlength integer quantization, yielding both execution time and energy\nbenefits on existing hardware designs that support short bitlengths. However,\nthe question of finding the minimum bitlength for a desired accuracy remains\nopen. We introduce a training method for minimizing inference bitlength at any\ngranularity while maintaining accuracy. Namely, we propose a regularizer that\npenalizes large bitlength representations throughout the architecture and show\nhow it can be modified to minimize other quantifiable criteria, such as number\nof operations or memory footprint. We demonstrate that our method learns\nthrifty representations while maintaining accuracy. With ImageNet, the method\nproduces an average per layer bitlength of 4.13, 3.76 and 4.36 bits on AlexNet,\nResNet18 and MobileNet V2 respectively, remaining within 2.0%, 0.5% and 0.5% of\nthe base TOP-1 accuracy.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "ning2016scalable", "year": "2016", "title":"Scalable Image Retrieval By Sparse Product Quantization", "abstract": "<p>Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional\nfeature indexing and retrieval is the crux of large-scale image retrieval. A\nrecent promising technique is Product Quantization, which attempts to index\nhigh-dimensional image features by decomposing the feature space into a\nCartesian product of low dimensional subspaces and quantizing each of them\nseparately. Despite the promising results reported, their quantization approach\nfollows the typical hard assignment of traditional quantization methods, which\nmay result in large quantization errors and thus inferior search performance.\nUnlike the existing approaches, in this paper, we propose a novel approach\ncalled Sparse Product Quantization (SPQ) to encoding the high-dimensional\nfeature vectors into sparse representation. We optimize the sparse\nrepresentations of the feature vectors by minimizing their quantization errors,\nmaking the resulting representation is essentially close to the original data\nin practice. Experiments show that the proposed SPQ technique is not only able\nto compress data, but also an effective encoding technique. We obtain\nstate-of-the-art results for ANN search on four public image datasets and the\npromising results of content-based image retrieval further validate the\nefficacy of our proposed method.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "niwa2022detection", "year": "2022", "title":"A Detection Method Of Temporally Operated Videos Using Robust Hashing", "abstract": "<p>SNS providers are known to carry out the recompression and resizing of\nuploaded videos/images, but most conventional methods for detecting tampered\nvideos/images are not robust enough against such operations. In addition,\nvideos are temporally operated such as the insertion of new frames and the\npermutation of frames, of which operations are difficult to be detected by\nusing conventional methods. Accordingly, in this paper, we propose a novel\nmethod with a robust hashing algorithm for detecting temporally operated videos\neven when applying resizing and compression to the videos.</p>\n", "tags": ["ARXIV"] },
{"key": "noma2013markov", "year": "2013", "title":"Markov Chain Monte Carlo For Arrangement Of Hyperplanes In Locality-sensitive Hashing", "abstract": "<p>Since Hamming distances can be calculated by bitwise computations, they can\nbe calculated with less computational load than L2 distances. Similarity\nsearches can therefore be performed faster in Hamming distance space. The\nelements of Hamming distance space are bit strings. On the other hand, the\narrangement of hyperplanes induce the transformation from the feature vectors\ninto feature bit strings. This transformation method is a type of\nlocality-sensitive hashing that has been attracting attention as a way of\nperforming approximate similarity searches at high speed. Supervised learning\nof hyperplane arrangements allows us to obtain a method that transforms them\ninto feature bit strings reflecting the information of labels applied to\nhigher-dimensional feature vectors. In this p aper, we propose a supervised\nlearning method for hyperplane arrangements in feature space that uses a Markov\nchain Monte Carlo (MCMC) method. We consider the probability density functions\nused during learning, and evaluate their performance. We also consider the\nsampling method for learning data pairs needed in learning, and we evaluate its\nperformance. We confirm that the accuracy of this learning method when using a\nsuitable probability density function and sampling method is greater than the\naccuracy of existing learning methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "noma2014eclipse", "year": "2014", "title":"Eclipse Hashing Alexandrov Compactification And Hashing With Hyperspheres For Fast Similarity Search", "abstract": "<p>The similarity searches that use high-dimensional feature vectors consisting\nof a vast amount of data have a wide range of application. One way of\nconducting a fast similarity search is to transform the feature vectors into\nbinary vectors and perform the similarity search by using the Hamming distance.\nSuch a transformation is a hashing method, and the choice of hashing function\nis important. Hashing methods using hyperplanes or hyperspheres are proposed.\nOne study reported here is inspired by Spherical LSH, and we use hypersperes to\nhash the feature vectors. Our method, called Eclipse-hashing, performs a\ncompactification of R^n by using the inverse stereographic projection, which is\na kind of Alexandrov compactification. By using Eclipse-hashing, one can obtain\nthe hypersphere-hash function without explicitly using hyperspheres. Hence, the\nnumber of nonlinear operations is reduced and the processing time of hashing\nbecomes shorter. Furthermore, we also show that as a result of improving the\napproximation accuracy, Eclipse-hashing is more accurate than\nhyperplane-hashing.</p>\n", "tags": ["ARXIV","Graph","Independent","LSH"] },
{"key": "norouzi2012hamming", "year": "2012", "title":"Hamming Distance Metric Learning", "abstract": "<p>Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs.  We develop a new loss-augmented inference algorithm that is quadratic in the code length.  We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes.</p>\n", "tags": ["NEURIPS","Supervised"] },
{"key": "norouzi2013fast", "year": "2013", "title":"Fast Exact Search In Hamming Space With Multi-index Hashing", "abstract": "<p>There is growing interest in representing image data and feature descriptors\nusing compact binary codes for fast near neighbor search. Although binary codes\nare motivated by their use as direct indices (addresses) into a hash table,\ncodes longer than 32 bits are not being used as such, as it was thought to be\nineffective. We introduce a rigorous way to build multiple hash tables on\nbinary code substrings that enables exact k-nearest neighbor search in Hamming\nspace. The approach is storage efficient and straightforward to implement.\nTheoretical analysis shows that the algorithm exhibits sub-linear run-time\nbehavior for uniformly distributed codes. Empirical results show dramatic\nspeedups over a linear scan baseline for datasets of up to one billion codes of\n64, 128, or 256 bits.</p>\n", "tags": ["ARXIV"] },
{"key": "norouzi2024hamming", "year": "2024", "title":"Hamming Distance Metric Learning", "abstract": "<p>Motivated by large-scale multimedia applications we propose to learn mappings\nfrom high-dimensional data to binary codes that preserve semantic similarity.\nBinary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable\nto broad families of mappings, and uses a flexible form of triplet ranking loss.\nWe overcome discontinuous optimization of the discrete mappings by minimizing\na piecewise-smooth upper bound on empirical loss, inspired by latent structural\nSVMs. We develop a new loss-augmented inference algorithm that is quadratic in\nthe code length. We show strong retrieval performance on CIFAR-10 and MNIST,\nwith promising classification results using no more than kNN on the binary codes.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "norouzi2024minimal", "year": "2024", "title":"Minimal Loss Hashing", "abstract": "<p>We propose a method for learning similaritypreserving\nhash functions that map highdimensional\ndata onto binary codes. The\nformulation is based on structured prediction\nwith latent variables and a hinge-like\nloss function. It is efficient to train for large\ndatasets, scales well to large code lengths,\nand outperforms state-of-the-art methods.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "nuswide2009nuswide", "year": "2009", "title":"NUS-WIDE: a real-world web image database from National University of Singapore", "abstract": "<p>This paper introduces a web image dataset created by NUS’s Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval.</p>\n", "tags": ["Dataset"] },
{"key": "oberst2024sliding", "year": "2024", "title":"Sliding Block (slick) Hashing An Implementation Benchmarks", "abstract": "<p>With hash tables being one of the most used data structures, Lehmann, Sanders\nand Walzer propose a novel, light-weight hash table, referred to as Slick Hash.\nTheir idea is to hit a sweet spot between space consumption and speed. Building\non the theoretical ideas by the authors, an implementation and experiments are\nrequired to evaluate the practical performance of Slick Hash. This work\ncontributes to fulfilling this requirement by providing a basic implementation\nof Slick Hash, an analysis of its performance, and an evaluation of the entry\ndeletion, focusing on the impact of backyard cleaning. The findings are\ndiscussed, and a conclusion is drawn.</p>\n", "tags": ["ARXIV"] },
{"key": "odonnell2009optimal", "year": "2009", "title":"Optimal Lower Bounds For Locality Sensitive Hashing (except When Q Is Tiny)", "abstract": "<p>We study lower bounds for Locality Sensitive Hashing (LSH) in the strongest\nsetting: point sets in {0,1}^d under the Hamming distance. Recall that here H\nis said to be an (r, cr, p, q)-sensitive hash family if all pairs x, y in\n{0,1}^d with dist(x,y) at most r have probability at least p of collision under\na randomly chosen h in H, whereas all pairs x, y in {0,1}^d with dist(x,y) at\nleast cr have probability at most q of collision. Typically, one considers d\ntending to infinity, with c fixed and q bounded away from 0.\n  For its applications to approximate nearest neighbor search in high\ndimensions, the quality of an LSH family H is governed by how small its “rho\nparameter” rho = ln(1/p)/ln(1/q) is as a function of the parameter c. The\nseminal paper of Indyk and Motwani showed that for each c, the extremely simple\nfamily H = {x -&gt; x_i : i in d} achieves rho at most 1/c. The only known lower\nbound, due to Motwani, Naor, and Panigrahy, is that rho must be at least .46/c\n(minus o_d(1)).\n  In this paper we show an optimal lower bound: rho must be at least 1/c (minus\no_d(1)). This lower bound for Hamming space yields a lower bound of 1/c^2 for\nEuclidean space (or the unit sphere) and 1/c for the Jaccard distance on sets;\nboth of these match known upper bounds. Our proof is simple; the essence is\nthat the noise stability of a boolean function at e^{-t} is a log-convex\nfunction of t.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "ohara2011introduction", "year": "2011", "title":"Introduction To The Bag Of Features Paradigm For Image Classification And Retrieval", "abstract": "<p>The past decade has seen the growing popularity of Bag of Features (BoF)\napproaches to many computer vision tasks, including image classification, video\nsearch, robot localization, and texture recognition. Part of the appeal is\nsimplicity. BoF methods are based on orderless collections of quantized local\nimage descriptors; they discard spatial information and are therefore\nconceptually and computationally simpler than many alternative methods. Despite\nthis, or perhaps because of this, BoF-based systems have set new performance\nstandards on popular image classification benchmarks and have achieved\nscalability breakthroughs in image retrieval. This paper presents an\nintroduction to BoF image representations, describes critical design choices,\nand surveys the BoF literature. Emphasis is placed on recent techniques that\nmitigate quantization errors, improve feature detection, and speed up image\nretrieval. At the same time, unresolved issues and fundamental challenges are\nraised. Among the unresolved issues are determining the best techniques for\nsampling images, describing local image features, and evaluating system\nperformance. Among the more fundamental challenges are how and whether BoF\nmethods can contribute to localizing objects in complex images, or to\nassociating high-level semantics with natural images. This survey should be\nuseful both for introducing new investigators to the field and for providing\nexisting researchers with a consolidated reference to related work.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised","Survey Paper"] },
{"key": "ono2023relative", "year": "2023", "title":"Relative Nn-descent A Fast Index Construction For Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) is the task of finding the\ndatabase vector that is closest to a given query vector. Graph-based ANNS is\nthe family of methods with the best balance of accuracy and speed for\nmillion-scale datasets. However, graph-based methods have the disadvantage of\nlong index construction time. Recently, many researchers have improved the\ntradeoff between accuracy and speed during a search. However, there is little\nresearch on accelerating index construction. We propose a fast graph\nconstruction algorithm, Relative NN-Descent (RNN-Descent). RNN-Descent combines\nNN-Descent, an algorithm for constructing approximate K-nearest neighbor graphs\n(K-NN graphs), and RNG Strategy, an algorithm for selecting edges effective for\nsearch. This algorithm allows the direct construction of graph-based indexes\nwithout ANNS. Experimental results demonstrated that the proposed method had\nthe fastest index construction speed, while its search performance is\ncomparable to existing state-of-the-art methods such as NSG. For example, in\nexperiments on the GIST1M dataset, the construction of the proposed method is\n2x faster than NSG. Additionally, it was even faster than the construction\nspeed of NN-Descent.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "ootomo2023cagra", "year": "2023", "title":"CAGRA Highly Parallel Graph Construction And Approximate Nearest Neighbor Search For Gpus", "abstract": "<p>Approximate Nearest Neighbor Search (ANNS) plays a critical role in various\ndisciplines spanning data mining and artificial intelligence, from information\nretrieval and computer vision to natural language processing and recommender\nsystems. Data volumes have soared in recent years and the computational cost of\nan exhaustive exact nearest neighbor search is often prohibitive, necessitating\nthe adoption of approximate techniques. The balanced performance and recall of\ngraph-based approaches have more recently garnered significant attention in\nANNS algorithms, however, only a few studies have explored harnessing the power\nof GPUs and multi-core processors despite the widespread use of massively\nparallel and general-purpose computing. To bridge this gap, we introduce a\nnovel parallel computing hardware-based proximity graph and search algorithm.\nBy leveraging the high-performance capabilities of modern hardware, our\napproach achieves remarkable efficiency gains. In particular, our method\nsurpasses existing CPU and GPU-based methods in constructing the proximity\ngraph, demonstrating higher throughput in both large- and small-batch searches\nwhile maintaining compatible accuracy. In graph construction time, our method,\nCAGRA, is 2.2~27x faster than HNSW, which is one of the CPU SOTA\nimplementations. In large-batch query throughput in the 90% to 95% recall\nrange, our method is 33~77x faster than HNSW, and is 3.8~8.8x faster than the\nSOTA implementations for GPU. For a single query, our method is 3.4~53x faster\nthan HNSW at 95% recall.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "ortega2022unconventional", "year": "2022", "title":"Unconventional Application Of K-means For Distributed Approximate Similarity Search", "abstract": "<p>Similarity search based on a distance function in metric spaces is a\nfundamental problem for many applications. Queries for similar objects lead to\nthe well-known machine learning task of nearest-neighbours identification. Many\ndata indexing strategies, collectively known as Metric Access Methods (MAM),\nhave been proposed to speed up queries for similar elements in this context.\nMoreover, since exact approaches to solve similarity queries can be complex and\ntime-consuming, alternative options have appeared to reduce query execution\ntime, such as returning approximate results or resorting to distributed\ncomputing platforms. In this paper, we introduce MASK (Multilevel Approximate\nSimilarity search with \\(k\\)-means), an unconventional application of the\n\\(k\\)-means algorithm as the foundation of a multilevel index structure for\napproximate similarity search, suitable for metric spaces. We show that\ninherent properties of \\(k\\)-means, like representing high-density data areas\nwith fewer prototypes, can be leveraged for this purpose. An implementation of\nthis new indexing method is evaluated, using a synthetic dataset and a\nreal-world dataset in a high-dimensional and high-sparsity space. Results are\npromising and underpin the applicability of this novel indexing method in\nmultiple domains.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "orun2023lower", "year": "2023", "title":"The Lower Energy Consumption In Cryptocurrency Mining Processes By SHA-256 Quantum Circuit Design Used In Hybrid Computing Domains", "abstract": "<p>Cryptocurrency mining processes always lead to a high energy consumption at\nconsiderably high production cost, which is nearly one-third of cryptocurrency\n(e.g. Bitcoin) price itself. As the core of mining process is based on SHA-256\ncryptographic hashing function, by using the alternative quantum computers,\nhybrid quantum computers or more larger quantum computing devices like quantum\nannealers, it would be possible to reduce the mining energy consumption with a\nquantum hardware’s low-energy-operation characteristics. Within this work we\ndemonstrated the use of optimized quantum mining facilities which would replace\nthe classical SHA-256 and high energy consuming classical hardware in near\nfuture.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "ou2021integrating", "year": "2021", "title":"Integrating Semantics And Neighborhood Information With Graph-driven Generative Models For Document Retrieval", "abstract": "<p>With the need of fast retrieval speed and small memory footprint, document\nhashing has been playing a crucial role in large-scale information retrieval.\nTo generate high-quality hashing code, both semantics and neighborhood\ninformation are crucial. However, most existing methods leverage only one of\nthem or simply combine them via some intuitive criteria, lacking a theoretical\nprinciple to guide the integration process. In this paper, we encode the\nneighborhood information with a graph-induced Gaussian distribution, and\npropose to integrate the two types of information with a graph-driven\ngenerative model. To deal with the complicated correlations among documents, we\nfurther propose a tree-structured approximation method for learning. Under the\napproximation, we prove that the training objective can be decomposed into\nterms involving only singleton or pairwise documents, enabling the model to be\ntrained as efficiently as uncorrelated ones. Extensive experimental results on\nthree benchmark datasets show that our method achieves superior performance\nover state-of-the-art methods, demonstrating the effectiveness of the proposed\nmodel for simultaneously preserving semantic and neighborhood information.\\</p>\n", "tags": ["Graph","SIGIR"] },
{"key": "ou2021refining", "year": "2021", "title":"Refining BERT Embeddings For Document Hashing Via Mutual Information Maximization", "abstract": "<p>Existing unsupervised document hashing methods are mostly established on\ngenerative models. Due to the difficulties of capturing long dependency\nstructures, these methods rarely model the raw documents directly, but instead\nto model the features extracted from them (e.g. bag-of-words (BOW), TFIDF). In\nthis paper, we propose to learn hash codes from BERT embeddings after observing\ntheir tremendous successes on downstream tasks. As a first try, we modify\nexisting generative hashing models to accommodate the BERT embeddings. However,\nlittle improvement is observed over the codes learned from the old BOW or TFIDF\nfeatures. We attribute this to the reconstruction requirement in the generative\nhashing, which will enforce irrelevant information that is abundant in the BERT\nembeddings also compressed into the codes. To remedy this issue, a new\nunsupervised hashing paradigm is further proposed based on the mutual\ninformation (MI) maximization principle. Specifically, the method first\nconstructs appropriate global and local codes from the documents and then seeks\nto maximize their mutual information. Experimental results on three benchmark\ndatasets demonstrate that the proposed method is able to generate hash codes\nthat outperform existing ones learned from BOW features by a substantial\nmargin.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "ou2024comparing", "year": "2024", "title":"Comparing Apples To Oranges A Scalable Solution With Heterogeneous Hashing", "abstract": "<p>Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogeneous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of ``comparing apples to oranges’’ under the large scale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing (RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogeneous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets; one is crawled from a popular social media sites, Tencent Weibo, and the other is an open dataset of Flickr(NUS-WIDE). The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "oved2022hashpim", "year": "2022", "title":"Hashpim High-throughput SHA-3 Via Memristive Digital Processing-in-memory", "abstract": "<p>Recent research has sought to accelerate cryptographic hash functions as they\nare at the core of modern cryptography. Traditional designs, however, suffer\nfrom the von Neumann bottleneck that originates from the separation of\nprocessing and memory units. An emerging solution to overcome this bottleneck\nis processing-in-memory (PIM): performing logic within the same devices\nresponsible for memory to eliminate data-transfer and simultaneously provide\nmassive computational parallelism. In this paper, we seek to vastly accelerate\nthe state-of-the-art SHA-3 cryptographic function using the memristive memory\nprocessing unit (mMPU), a general-purpose memristive PIM architecture. To that\nend, we propose a novel in-memory algorithm for variable rotation, and utilize\nan efficient mapping of the SHA-3 state vector for memristive crossbar arrays\nto efficiently exploit PIM parallelism. We demonstrate a massive energy\nefficiency of 1,422 Gbps/W, improving a state-of-the-art memristive SHA-3\naccelerator (SHINE-2) by 4.6x.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "oymak2015near", "year": "2015", "title":"Near-optimal Bounds For Binary Embeddings Of Arbitrary Sets", "abstract": "<p>We study embedding a subset \\(K\\) of the unit sphere to the Hamming cube\n\\(\\{-1,+1\\}^m\\). We characterize the tradeoff between distortion and sample\ncomplexity \\(m\\) in terms of the Gaussian width \\(\\omega(K)\\) of the set. For\nsubspaces and several structured sets we show that Gaussian maps provide the\noptimal tradeoff \\(m\\sim \\delta^{-2}\\omega^2(K)\\), in particular for \\(\\delta\\)\ndistortion one needs \\(m\\approx\\delta^{-2}{d}\\) where \\(d\\) is the subspace\ndimension. For general sets, we provide sharp characterizations which reduces\nto \\(m\\approx{\\delta^{-4}}{\\omega^2(K)}\\) after simplification. We provide\nimproved results for local embedding of points that are in close proximity of\neach other which is related to locality sensitive hashing. We also discuss\nfaster binary embedding where one takes advantage of an initial sketching\nprocedure based on Fast Johnson-Lindenstauss Transform. Finally, we list\nseveral numerical observations and discuss open problems.</p>\n", "tags": ["ARXIV"] },
{"key": "ozdemir2016scalable", "year": "2016", "title":"Scalable Gaussian Processes For Supervised Hashing", "abstract": "<p>We propose a flexible procedure for large-scale image search by hash\nfunctions with kernels. Our method treats binary codes and pairwise semantic\nsimilarity as latent and observed variables, respectively, in a probabilistic\nmodel based on Gaussian processes for binary classification. We present an\nefficient inference algorithm with the sparse pseudo-input Gaussian process\n(SPGP) model and parallelization. Experiments on three large-scale image\ndataset demonstrate the effectiveness of the proposed hashing method, Gaussian\nProcess Hashing (GPH), for short binary codes and the datasets without\npredefined classes in comparison to the state-of-the-art supervised hashing\nmethods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "ozdemir2016supervised", "year": "2016", "title":"Supervised Incremental Hashing", "abstract": "<p>We propose an incremental strategy for learning hash functions with kernels\nfor large-scale image search. Our method is based on a two-stage classification\nframework that treats binary codes as intermediate variables between the\nfeature space and the semantic space. In the first stage of classification,\nbinary codes are considered as class labels by a set of binary SVMs; each\ncorresponds to one bit. In the second stage, binary codes become the input\nspace of a multi-class SVM. Hash functions are learned by an efficient\nalgorithm where the NP-hard problem of finding optimal binary codes is solved\nvia cyclic coordinate descent and SVMs are trained in a parallelized\nincremental manner. For modifications like adding images from a previously\nunseen class, we describe an incremental procedure for effective and efficient\nupdates to the previous hash functions. Experiments on three large-scale image\ndatasets demonstrate the effectiveness of the proposed hashing method,\nSupervised Incremental Hashing (SIH), over the state-of-the-art supervised\nhashing methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "ozsari2003hash", "year": "2003", "title":"A Hash Of Hash Functions", "abstract": "<p>In this paper, we present a general review of hash functions in a\ncryptographic sense. We give special emphasis on some particular topics such as\ncipher block chaining message authentication code (CBC MAC) and its variants.\nThis paper also broadens the information given in some well known surveys, by\nincluding more details on block-cipher based hash functions and security of\ndifferent hash schemes.</p>\n", "tags": ["ARXIV","Graph","Independent","Survey Paper"] },
{"key": "pachori2016zero", "year": "2016", "title":"Zero Shot Hashing", "abstract": "<p>This paper provides a framework to hash images containing instances of\nunknown object classes. In many object recognition problems, we might have\naccess to huge amount of data. It may so happen that even this huge data\ndoesn’t cover the objects belonging to classes that we see in our day to day\nlife. Zero shot learning exploits auxiliary information (also called as\nsignatures) in order to predict the labels corresponding to unknown classes. In\nthis work, we attempt to generate the hash codes for images belonging to unseen\nclasses, information of which is available only through the textual corpus. We\nformulate this as an unsupervised hashing formulation as the exact labels are\nnot available for the instances of unseen classes. We show that the proposed\nsolution is able to generate hash codes which can predict labels corresponding\nto unseen classes with appreciably good precision.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "pachori2017hashing", "year": "2017", "title":"Hashing In The Zero Shot Framework With Domain Adaptation", "abstract": "<p>Techniques to learn hash codes which can store and retrieve large dimensional\nmultimedia data efficiently have attracted broad research interests in the\nrecent years. With rapid explosion of newly emerged concepts and online data,\nexisting supervised hashing algorithms suffer from the problem of scarcity of\nground truth annotations due to the high cost of obtaining manual annotations.\nTherefore, we propose an algorithm to learn a hash function from training\nimages belonging to <code class=\"language-plaintext highlighter-rouge\">seen' classes which can efficiently encode images of\n</code>unseen’ classes to binary codes. Specifically, we project the image features\nfrom visual space and semantic features from semantic space into a common\nHamming subspace. Earlier works to generate hash codes have tried to relax the\ndiscrete constraints on hash codes and solve the continuous optimization\nproblem. However, it often leads to quantization errors. In this work, we use\nthe max-margin classifier to learn an efficient hash function. To address the\nconcern of domain-shift which may arise due to the introduction of new classes,\nwe also introduce an unsupervised domain adaptation model in the proposed\nhashing framework. Results on the three datasets show the advantage of using\ndomain adaptation in learning a high-quality hash function and superiority of\nour method for the task of image retrieval performance as compared to several\nstate-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "pacuk2016locality", "year": "2016", "title":"Locality-sensitive Hashing Without False Negatives For L_p", "abstract": "<p>In this paper, we show a construction of locality-sensitive hash functions\nwithout false negatives, i.e., which ensure collision for every pair of points\nwithin a given radius \\(R\\) in \\(d\\) dimensional space equipped with \\(l_p\\) norm\nwhen \\(p \\in [1,\\infty]\\). Furthermore, we show how to use these hash functions\nto solve the \\(c\\)-approximate nearest neighbor search problem without false\nnegatives. Namely, if there is a point at distance \\(R\\), we will certainly\nreport it and points at distance greater than \\(cR\\) will not be reported for\n\\(c=Ω(\\sqrt{d},d^{1-\\frac{1}{p}})\\). The constructed algorithms work: - with\npreprocessing time \\(\\mathcal{O}(n log(n))\\) and sublinear expected query time,</p>\n<ul>\n  <li>with preprocessing time \\(\\mathcal{O}(\\mathrm{poly}(n))\\) and expected query\ntime \\(\\mathcal{O}(log(n))\\). Our paper reports progress on answering the open\nproblem presented by Pagh [8] who considered the nearest neighbor search\nwithout false negatives for the Hamming distance.</li>\n</ul>\n", "tags": ["Independent"] },
{"key": "pagh2006linear", "year": "2006", "title":"Linear Probing With Constant Independence", "abstract": "<p>Hashing with linear probing dates back to the 1950s, and is among the most\nstudied algorithms. In recent years it has become one of the most important\nhash table organizations since it uses the cache of modern computers very well.\nUnfortunately, previous analysis rely either on complicated and space consuming\nhash functions, or on the unrealistic assumption of free access to a truly\nrandom hash function. Already Carter and Wegman, in their seminal paper on\nuniversal hashing, raised the question of extending their analysis to linear\nprobing. However, we show in this paper that linear probing using a pairwise\nindependent family may have expected {\\em logarithmic} cost per operation. On\nthe positive side, we show that 5-wise independence is enough to ensure\nconstant expected time per operation. This resolves the question of finding a\nspace and time efficient hash function that provably ensures good performance\nfor linear probing.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "pagh2015coveringlsh", "year": "2015", "title":"Coveringlsh Locality-sensitive Hashing Without False Negatives", "abstract": "<p>We consider a new construction of locality-sensitive hash functions for\nHamming space that is <em>covering</em> in the sense that is it guaranteed to\nproduce a collision for every pair of vectors within a given radius \\(r\\). The\nconstruction is <em>efficient</em> in the sense that the expected number of hash\ncollisions between vectors at distance~\\(cr\\), for a given \\(c&gt;1\\), comes close to\nthat of the best possible data independent LSH without the covering guarantee,\nnamely, the seminal LSH construction of Indyk and Motwani (STOC ‘98). The\nefficiency of the new construction essentially <em>matches</em> their bound when\nthe search radius is not too large — e.g., when \\(cr = o(log(n)/loglog n)\\),\nwhere \\(n\\) is the number of points in the data set, and when \\(cr = log(n)/k\\)\nwhere \\(k\\) is an integer constant. In general, it differs by at most a factor\n\\(\\ln(4)\\) in the exponent of the time bounds. As a consequence, LSH-based\nsimilarity search in Hamming space can avoid the problem of false negatives at\nlittle or no cost in efficiency.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "pagh2015similarity", "year": "2015", "title":"I/o-efficient Similarity Join", "abstract": "<p>We present an I/O-efficient algorithm for computing similarity joins based on\nlocality-sensitive hashing (LSH). In contrast to the filtering methods commonly\nsuggested our method has provable sub-quadratic dependency on the data size.\nFurther, in contrast to straightforward implementations of known LSH-based\nalgorithms on external memory, our approach is able to take significant\nadvantage of the available internal memory: Whereas the time complexity of\nclassical algorithms includes a factor of \\(N^\\rho\\), where \\(\\rho\\) is a parameter\nof the LSH used, the I/O complexity of our algorithm merely includes a factor\n\\((N/M)^\\rho\\), where \\(N\\) is the data size and \\(M\\) is the size of internal\nmemory. Our algorithm is randomized and outputs the correct result with high\nprobability. It is a simple, recursive, cache-oblivious procedure, and we\nbelieve that it will be useful also in other computational settings such as\nparallel computation.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "pagh2016approximate", "year": "2016", "title":"Approximate Furthest Neighbor With Application To Annulus Query", "abstract": "<p>Much recent work has been devoted to approximate nearest neighbor queries.\nMotivated by applications in recommender systems, we consider approximate\nfurthest neighbor (AFN) queries and present a simple, fast, and highly\npractical data structure for answering AFN queries in high- dimensional\nEuclidean space. The method builds on the technique of In- dyk (SODA 2003),\nstoring random projections to provide sublinear query time for AFN. However, we\nintroduce a different query algorithm, improving on Indyk’s approximation\nfactor and reducing the running time by a logarithmic factor. We also present a\nvariation based on a query- independent ordering of the database points; while\nthis does not have the provable approximation factor of the query-dependent\ndata structure, it offers significant improvement in time and space complexity.\nWe give a theoretical analysis, and experimental results. As an application,\nthe query-dependent approach is used for deriving a data structure for the\napproximate annulus query problem, which is defined as follows: given an input\nset S and two parameters r &gt; 0 and w &gt;= 1, construct a data structure that\nreturns for each query point q a point p in S such that the distance between p\nand q is at least r/w and at most wr.</p>\n", "tags": ["Independent"] },
{"key": "pagès2010sharp", "year": "2010", "title":"Sharp Rate For The Dual Quantization Problem", "abstract": "<p>In this paper we establish the sharp rate of the optimal dual quantization\nproblem. The notion of dual quantization was recently introduced in the paper\n[8], where it was shown that, at least in an Euclidean setting, dual quantizers\nare based on a Delaunay triangulation, the dual counterpart of the Voronoi\ntessellation on which “regular” quantization relies. Moreover, this new\napproach shares an intrinsic stationarity property, which makes it very\nvaluable for numerical applications. We establish in this paper the counterpart\nfor dual quantization of the celebrated Zador theorem, which describes the\nsharp asymptotics for the quantization error when the quantizer size tends to\ninfinity. The proof of this theorem relies among others on an extension of the\nso-called Pierce Lemma by means of a random quantization argument.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "pal2011design", "year": "2011", "title":"Design Of Image Cryptosystem By Simultaneous Vq-compression And Shuffling Of Codebook And Index Matrix", "abstract": "<p>The popularity of Internet usage although increases exponentially, it is\nincapable of providing the security for exchange of confidential data between\nthe users. As a result, several cryptosystems for encryption of data and images\nhave been developed for secured transmission over Internet. In this work, a\nscheme for Image encryption/decryption based on Vector Quantization (VQ) has\nbeen proposed that concurrently encodes the images for compression and shuffles\nthe codebook and the index matrix using pseudorandom sequences for encryption.\nThe processing time of the proposed scheme is much less than the other\ncryptosystems, because it does not use any traditional cryptographic\noperations, and instead it performs swapping between the contents of the\ncodebook with respect to a random sequence, which resulted an indirect\nshuffling of the contents of the index matrix. It may be noted that the\nsecurity of the proposed cryptosystem depends on the generation and the\nexchange of the random sequences used. Since the generation of truly random\nsequences are not practically feasible, we simulate the proposed scheme using\nMATLAB, where its operators like rand(method, seed), randperm(n) has been used\nto generate pseudorandom sequences and it has been seen that the proposed\ncryptosystem shows the expected performance.</p>\n", "tags": ["Graph","Independent","Quantisation"] },
{"key": "palmer2023efficient", "year": "2023", "title":"Efficient Online String Matching Through Linked Weak Factors", "abstract": "<p>Online string matching is a computational problem involving the search for\npatterns or substrings in a large text dataset, with the pattern and text being\nprocessed sequentially, without prior access to the entire text. Its relevance\nstems from applications in data compression, data mining, text editing, and\nbioinformatics, where rapid and efficient pattern matching is crucial. Various\nsolutions have been proposed over the past few decades, employing diverse\ntechniques. Recently, weak recognition approaches have attracted increasing\nattention. This paper presents Hash Chain, a new algorithm based on a robust\nweak factor recognition approach that connects adjacent factors through\nhashing. Despite its O(nm) complexity, the algorithm exhibits a sublinear\nbehavior in practice and achieves superior performance compared to the most\neffective algorithms.</p>\n", "tags": ["ARXIV"] },
{"key": "pandey2022iceberght", "year": "2022", "title":"Iceberght High Performance PMEM Hash Tables Through Stability And Low Associativity", "abstract": "<p>Modern hash table designs strive to minimize space while maximizing speed.\nThe most important factor in speed is the number of cache lines accessed during\nupdates and queries. This is especially important on PMEM, which is slower than\nDRAM and in which writes are more expensive than reads.\n  This paper proposes two stronger design objectives: stability and\nlow-associativity. A stable hash table doesn’t move items around, and a hash\ntable has low associativity if there are only a few locations where an item can\nbe stored. Low associativity ensures that queries need to examine only a few\nmemory locations, and stability ensures that insertions write to very few cache\nlines. Stability also simplifies scaling and crash safety.\n  We present IcebergHT, a fast, crash-safe, concurrent, and space-efficient\nhash table for PMEM based on the design principles of stability and low\nassociativity. IcebergHT combines in-memory metadata with a new hashing\ntechnique, iceberg hashing, that is (1) space efficient, (2) stable, and (3)\nsupports low associativity. In contrast, existing hash-tables either modify\nnumerous cache lines during insertions (e.g. cuckoo hashing), access numerous\ncache lines during queries (e.g. linear probing), or waste space (e.g.\nchaining). Moreover, the combination of (1)-(3) yields several emergent\nbenefits: IcebergHT scales better than other hash tables, supports\ncrash-safety, and has excellent performance on PMEM (where writes are\nparticularly expensive).</p>\n", "tags": ["ARXIV"] },
{"key": "pandya2024performance", "year": "2024", "title":"Performance Evaluation Of Hashing Algorithms On Commodity Hardware", "abstract": "<p>Hashing functions, which are created to provide brief and erratic digests for\nthe message entered, are the primary cryptographic primitives used in\nblockchain networks. Hashing is employed in blockchain networks to create\nlinked block lists, which offer safe and secure distributed repository storage\nfor critical information. Due to the unique nature of the hash search problem\nin blockchain networks, the most parallelization of calculations is possible.\nThis technical report presents a performance evaluation of three popular\nhashing algorithms Blake3, SHA-256, and SHA-512. These hashing algorithms are\nwidely used in various applications, such as digital signatures, message\nauthentication, and password storage. It then discusses the performance metrics\nused to evaluate the algorithms, such as hash rate/throughput and memory usage.\nThe evaluation is conducted on a range of hardware platforms, including desktop\nand VMs. The evaluation includes synthetic benchmarks. The results of the\nevaluation show that Blake3 generally outperforms both SHA-256 and SHA-512 in\nterms of throughput and latency. However, the performance advantage of Blake3\nvaries depending on the specific hardware platform and the size of the input\ndata. The report concludes with recommendations for selecting the most suitable\nhashing algorithm for a given application, based on its performance\nrequirements and security needs. The evaluation results can also inform future\nresearch and development efforts to improve the performance and security of\nhashing algorithms.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "panigrahy2004efficient", "year": "2004", "title":"Efficient Hashing With Lookups In Two Memory Accesses", "abstract": "<p>The study of hashing is closely related to the analysis of balls and bins. It\nis well-known that instead of using a single hash function if we randomly hash\na ball into two bins and place it in the smaller of the two, then this\ndramatically lowers the maximum load on bins. This leads to the concept of\ntwo-way hashing where the largest bucket contains \\(O(loglog n)\\) balls with\nhigh probability. The hash look up will now search in both the buckets an item\nhashes to. Since an item may be placed in one of two buckets, we could\npotentially move an item after it has been initially placed to reduce maximum\nload. with a maximum load of We show that by performing moves during inserts, a\nmaximum load of 2 can be maintained on-line, with high probability, while\nsupporting hash update operations. In fact, with \\(n\\) buckets, even if the space\nfor two items are pre-allocated per bucket, as may be desirable in hardware\nimplementations, more than \\(n\\) items can be stored giving a high memory\nutilization. We also analyze the trade-off between the number of moves\nperformed during inserts and the maximum load on a bucket. By performing at\nmost \\(h\\) moves, we can maintain a maximum load of \\(O(\\frac{log log n}{h\nlog(loglog n/h)})\\). So, even by performing one move, we achieve a better\nbound than by performing no moves at all.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "panigrahy2005entropy", "year": "2005", "title":"Entropy Based Nearest Neighbor Search In High Dimensions", "abstract": "<p>In this paper we study the problem of finding the approximate nearest\nneighbor of a query point in the high dimensional space, focusing on the\nEuclidean space. The earlier approaches use locality-preserving hash functions\n(that tend to map nearby points to the same value) to construct several hash\ntables to ensure that the query point hashes to the same bucket as its nearest\nneighbor in at least one table. Our approach is different – we use one (or a\nfew) hash table and hash several randomly chosen points in the neighborhood of\nthe query point showing that at least one of them will hash to the bucket\ncontaining its nearest neighbor. We show that the number of randomly chosen\npoints in the neighborhood of the query point \\(q\\) required depends on the\nentropy of the hash value \\(h(p)\\) of a random point \\(p\\) at the same distance\nfrom \\(q\\) at its nearest neighbor, given \\(q\\) and the locality preserving hash\nfunction \\(h\\) chosen randomly from the hash family. Precisely, we show that if\nthe entropy \\(I(h(p)|q,h) = M\\) and \\(g\\) is a bound on the probability that two\nfar-off points will hash to the same bucket, then we can find the approximate\nnearest neighbor in \\(O(n^\\rho)\\) time and near linear \\(\\tilde O(n)\\) space where\n\\(\\rho = M/log(1/g)\\). Alternatively we can build a data structure of size\n\\(\\tilde O(n^{1/(1-\\rho)})\\) to answer queries in \\(\\tilde O(d)\\) time. By applying\nthis analysis to the locality preserving hash functions in and adjusting the\nparameters we show that the \\(c\\) nearest neighbor can be computed in time\n\\(\\tilde O(n^\\rho)\\) and near linear space where \\(\\rho \\approx 2.06/c\\) as \\(c\\)\nbecomes large.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "pansare2022learning", "year": "2022", "title":"Learning Compressed Embeddings For On-device Inference", "abstract": "<p>In deep learning, embeddings are widely used to represent categorical\nentities such as words, apps, and movies. An embedding layer maps each entity\nto a unique vector, causing the layer’s memory requirement to be proportional\nto the number of entities. In the recommendation domain, a given category can\nhave hundreds of thousands of entities, and its embedding layer can take\ngigabytes of memory. The scale of these networks makes them difficult to deploy\nin resource constrained environments. In this paper, we propose a novel\napproach for reducing the size of an embedding table while still mapping each\nentity to its own unique embedding. Rather than maintaining the full embedding\ntable, we construct each entity’s embedding “on the fly” using two separate\nembedding tables. The first table employs hashing to force multiple entities to\nshare an embedding. The second table contains one trainable weight per entity,\nallowing the model to distinguish between entities sharing the same embedding.\nSince these two tables are trained jointly, the network is able to learn a\nunique embedding per entity, helping it maintain a discriminative capability\nsimilar to a model with an uncompressed embedding table. We call this approach\nMEmCom (Multi-Embedding Compression). We compare with state-of-the-art model\ncompression techniques for multiple problem classes including classification\nand ranking. On four popular recommender system datasets, MEmCom had a 4%\nrelative loss in nDCG while compressing the input embedding sizes of our\nrecommendation models by 16x, 4x, 12x, and 40x. MEmCom outperforms the\nstate-of-the-art techniques, which achieved 16%, 6%, 10%, and 8% relative loss\nin nDCG at the respective compression ratios. Additionally, MEmCom is able to\ncompress the RankNet ranking model by 32x on a dataset with millions of users’\ninteractions with games while incurring only a 1% relative loss in nDCG.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "paria2020minimizing", "year": "2020", "title":"Minimizing Flops To Learn Efficient Sparse Representations", "abstract": "<p>Deep representation learning has become one of the most widely adopted\napproaches for visual search, recommendation, and identification. Retrieval of\nsuch representations from a large database is however computationally\nchallenging. Approximate methods based on learning compact representations,\nhave been widely explored for this problem, such as locality sensitive hashing,\nproduct quantization, and PCA. In this work, in contrast to learning compact\nrepresentations, we propose to learn high dimensional and sparse\nrepresentations that have similar representational capacity as dense embeddings\nwhile being more efficient due to sparse matrix multiplication operations which\ncan be much faster than dense multiplication. Following the key insight that\nthe number of operations decreases quadratically with the sparsity of\nembeddings provided the non-zero entries are distributed uniformly across\ndimensions, we propose a novel approach to learn such distributed sparse\nembeddings via the use of a carefully constructed regularization function that\ndirectly minimizes a continuous relaxation of the number of floating-point\noperations (FLOPs) incurred during retrieval. Our experiments show that our\napproach is competitive to the other baselines and yields a similar or better\nspeed-vs-accuracy tradeoff on practical datasets.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "parkerholder2018compressing", "year": "2018", "title":"Compressing Deep Neural Networks A New Hashing Pipeline Using Kacs Random Walk Matrices", "abstract": "<p>The popularity of deep learning is increasing by the day. However, despite\nthe recent advancements in hardware, deep neural networks remain\ncomputationally intensive. Recent work has shown that by preserving the angular\ndistance between vectors, random feature maps are able to reduce dimensionality\nwithout introducing bias to the estimator. We test a variety of established\nhashing pipelines as well as a new approach using Kac’s random walk matrices.\nWe demonstrate that this method achieves similar accuracy to existing\npipelines.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "parola2021web", "year": "2021", "title":"Web Image Search Engine Based On LSH Index And CNN Resnet50", "abstract": "<p>To implement a good Content Based Image Retrieval (CBIR) system, it is\nessential to adopt efficient search methods. One way to achieve this results is\nby exploiting approximate search techniques. In fact, when we deal with very\nlarge collections of data, using an exact search method makes the system very\nslow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to\nimplement a CBIR system that allows us to perform fast similarity search on\ndeep features. Specifically, we exploit transfer learning techniques to extract\ndeep features from images; this phase is done using two famous Convolutional\nNeural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both\npre-trained on ImageNet. Then we try out several fully connected deep neural\nnetworks, built on top of both of the previously mentioned CNNs in order to\nfine-tuned them on our dataset. In both of previous cases, we index the\nfeatures within our LSH index implementation and within a sequential scan, to\nbetter understand how much the introduction of the index affects the results.\nFinally, we carry out a performance analysis: we evaluate the relevance of the\nresult set, computing the mAP (mean Average Precision) value obtained during\nthe different experiments with respect to the number of done comparison and\nvarying the hyper-parameter values of the LSH index.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Independent","LSH"] },
{"key": "partalas2015lshtc", "year": "2015", "title":"LSHTC A Benchmark For Large-scale Text Classification", "abstract": "<p>LSHTC is a series of challenges which aims to assess the performance of\nclassification systems in large-scale classification in a a large number of\nclasses (up to hundreds of thousands). This paper describes the dataset that\nhave been released along the LSHTC series. The paper details the construction\nof the datsets and the design of the tracks as well as the evaluation measures\nthat we implemented and a quick overview of the results. All of these datasets\nare available online and runs may still be submitted on the online server of\nthe challenges.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "passalis2019deep", "year": "2019", "title":"Deep Supervised Hashing Leveraging Quadratic Spherical Mutual Information For Content-based Image Retrieval", "abstract": "<p>Several deep supervised hashing techniques have been proposed to allow for\nefficiently querying large image databases. However, deep supervised image\nhashing techniques are developed, to a great extent, heuristically often\nleading to suboptimal results. Contrary to this, we propose an efficient deep\nsupervised hashing algorithm that optimizes the learned codes using an\ninformation-theoretic measure, the Quadratic Mutual Information (QMI). The\nproposed method is adapted to the needs of large-scale hashing and information\nretrieval leading to a novel information-theoretic measure, the Quadratic\nSpherical Mutual Information (QSMI). Apart from demonstrating the effectiveness\nof the proposed method under different scenarios and outperforming existing\nstate-of-the-art image hashing techniques, this paper provides a structured way\nto model the process of information retrieval and develop novel methods adapted\nto the needs of each application.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "patrascu2010power", "year": "2010", "title":"The Power Of Simple Tabulation Hashing", "abstract": "<p>Randomized algorithms are often enjoyed for their simplicity, but the hash\nfunctions used to yield the desired theoretical guarantees are often neither\nsimple nor practical. Here we show that the simplest possible tabulation\nhashing provides unexpectedly strong guarantees.\n  The scheme itself dates back to Carter and Wegman (STOC’77). Keys are viewed\nas consisting of c characters. We initialize c tables T_1, …, T_c mapping\ncharacters to random hash codes. A key x=(x_1, …, x_q) is hashed to T_1[x_1]\nxor … xor T_c[x_c].\n  While this scheme is not even 4-independent, we show that it provides many of\nthe guarantees that are normally obtained via higher independence, e.g.,\nChernoff-type concentration, min-wise hashing for estimating set intersection,\nand cuckoo hashing.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "pauleve2024locality", "year": "2024", "title":"Locality Sensitive Hashing A Comparison Of Hash Function Types And Querying Mechanisms", "abstract": "<p>It is well known that high-dimensional nearest-neighbor retrieval is very expensive. Dramatic performance gains are obtained using\napproximate search schemes, such as the popular Locality-Sensitive Hashing (LSH). Several extensions have been proposed to\naddress the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector\nspace. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting\nits performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when\nsearching for high-dimension SIFT descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical\nk-means reveal that unstructured quantizer significantly improves the accuracy of LSH, as it closely fits the data in the feature space.\nWe then compare two querying mechanisms introduced in the literature with the one originally proposed in LSH, and discuss their\nrespective merits and limitations.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "pei2021vision", "year": "2021", "title":"Vision Transformer Based Video Hashing Retrieval For Tracing The Source Of Fake Videos", "abstract": "<p>In recent years, the spread of fake videos has brought great influence on\nindividuals and even countries. It is important to provide robust and reliable\nresults for fake videos. The results of conventional detection methods are not\nreliable and not robust for unseen videos. Another alternative and more\neffective way is to find the original video of the fake video. For example,\nfake videos from the Russia-Ukraine war and the Hong Kong law revision storm\nare refuted by finding the original video. We use an improved retrieval method\nto find the original video, named ViTHash. Specifically, tracing the source of\nfake videos requires finding the unique one, which is difficult when there are\nonly small differences in the original videos. To solve the above problems, we\ndesigned a novel loss Hash Triplet Loss. In addition, we designed a tool called\nLocalizator to compare the difference between the original traced video and the\nfake video. We have done extensive experiments on FaceForensics++, Celeb-DF and\nDeepFakeDetection, and we also have done additional experiments on our built\nthree datasets: DAVIS2016-TL (video inpainting), VSTL (video splicing) and DFTL\n(similar videos). Experiments have shown that our performance is better than\nstate-of-the-art methods, especially in cross-dataset mode. Experiments also\ndemonstrated that ViTHash is effective in various forgery detection: video\ninpainting, video splicing and deepfakes. Our code and datasets have been\nreleased on GitHub: \\url{https://github.com/lajlksdf/vtl}.</p>\n", "tags": ["ARXIV","Has Code"] },
{"key": "peng2013study", "year": "2013", "title":"A Study On Unsupervised Dictionary Learning And Feature Encoding For Action Classification", "abstract": "<p>Many efforts have been devoted to develop alternative methods to traditional\nvector quantization in image domain such as sparse coding and soft-assignment.\nThese approaches can be split into a dictionary learning phase and a feature\nencoding phase which are often closely connected. In this paper, we investigate\nthe effects of these phases by separating them for video-based action\nclassification. We compare several dictionary learning methods and feature\nencoding schemes through extensive experiments on KTH and HMDB51 datasets.\nExperimental results indicate that sparse coding performs consistently better\nthan the other encoding methods in large complex dataset (i.e., HMDB51), and it\nis robust to different dictionaries. For small simple dataset (i.e., KTH) with\nless variation, however, all the encoding strategies perform competitively. In\naddition, we note that the strength of sophisticated encoding approaches comes\nnot from their corresponding dictionaries but the encoding mechanisms, and we\ncan just use randomly selected exemplars as dictionaries for video-based action\nclassification.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "peng2014fast", "year": "2014", "title":"Fast Low-rank Representation Based Spatial Pyramid Matching For Image Classification", "abstract": "<p>Spatial Pyramid Matching (SPM) and its variants have achieved a lot of\nsuccess in image classification. The main difference among them is their\nencoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead of\nVector Quantization (VQ) into the framework of SPM. Although the methods\nachieve a higher recognition rate than the traditional SPM, they consume more\ntime to encode the local descriptors extracted from the image. In this paper,\nwe propose using Low Rank Representation (LRR) to encode the descriptors under\nthe framework of SPM. Different from SC, LRR considers the group effect among\ndata points instead of sparsity. Benefiting from this property, the proposed\nmethod (i.e., LrrSPM) can offer a better performance. To further improve the\ngeneralizability and robustness, we reformulate the rank-minimization problem\nas a truncated projection problem. Extensive experimental studies show that\nLrrSPM is more efficient than its counterparts (e.g., ScSPM) while achieving\ncompetitive recognition rates on nine image data sets.</p>\n", "tags": ["Quantisation","Supervised"] },
{"key": "peng2018deep", "year": "2018", "title":"Deep Reinforcement Learning For Image Hashing", "abstract": "<p>Deep hashing methods have received much attention recently, which achieve\npromising results by taking advantage of the strong representation power of\ndeep networks. However, most existing deep hashing methods learn a whole set of\nhashing functions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accuracy greatly.\nInspired by the sequential decision ability of deep reinforcement learning, we\npropose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).\nOur proposed DRLIH approach models the hashing learning problem as a sequential\ndecision process, which learns each hashing function by correcting the errors\nimposed by previous ones and promotes retrieval accuracy. To the best of our\nknowledge, this is the first work to address hashing problem from deep\nreinforcement learning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose a deep\nreinforcement learning hashing network. In the proposed network, we utilize\nrecurrent neural network (RNN) as agents to model the hashing functions, which\ntake actions of projecting images into binary codes sequentially, so that the\ncurrent hashing function learning can take previous hashing functions’ error\ninto account. (2) We propose a sequential learning strategy based on proposed\nDRLIH. We define the state as a tuple of internal features of RNN’s hidden\nlayers and image features, which can reflect history decisions made by the\nagents. We also propose an action group method to enhance the correlation of\nhash functions in the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "pereira2017genetic", "year": "2017", "title":"A Genetic Algorithm Approach For Imagerepresentation Learning Through Color Quantization", "abstract": "<p>Over the last decades, hand-crafted feature extractors have been used to\nencode image visual properties into feature vectors. Recently, data-driven\nfeature learning approaches have been successfully explored as alternatives for\nproducing more representative visual features. In this work, we combine both\nresearch venues, focusing on the color quantization problem. We propose two\ndata-driven approaches to learn image representations through the search for\noptimized quantization schemes, which lead to more effective feature extraction\nalgorithms and compact representations. Our strategy employs Genetic Algorithm,\na soft-computing apparatus successfully utilized in\nInformation-retrieval-related optimization problems. We hypothesize that\nchanging the quantization affects the quality of image description approaches,\nleading to effective and efficient representations. We evaluate our approaches\nin content-based image retrieval tasks, considering eight well-known datasets\nwith different visual properties. Results indicate that the approach focused on\nrepresentation effectiveness outperformed baselines in all tested scenarios.\nThe other approach, which also considers the size of created representations,\nproduced competitive results keeping or even reducing the dimensionality of\nfeature vectors up to 25%.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "pestov1999geometry", "year": "1999", "title":"On The Geometry Of Similarity Search Dimensionality Curse And Concentration Of Measure", "abstract": "<p>We suggest that the curse of dimensionality affecting the similarity-based\nsearch in large datasets is a manifestation of the phenomenon of concentration\nof measure on high-dimensional structures. We prove that, under certain\ngeometric assumptions on the query domain \\(Ω\\) and the dataset \\(X\\), if\n\\(Ω\\) satisfies the so-called concentration property, then for most query\npoints \\(x^\\ast\\) the ball of radius \\((1+\\e)d_X(x^\\ast)\\) centred at \\(x^\\ast\\)\ncontains either all points of \\(X\\) or else at least \\(C_1\\exp(-C_2\\e^2n)\\) of\nthem. Here \\(d_X(x^\\ast)\\) is the distance from \\(x^\\ast\\) to the nearest neighbour\nin \\(X\\) and \\(n\\) is the dimension of \\(Ω\\).</p>\n", "tags": ["NEURIPS"] },
{"key": "pestov2002indexing", "year": "2002", "title":"Indexing Schemes For Similarity Search An Illustrated Paradigm", "abstract": "<p>We suggest a variation of the Hellerstein–Koutsoupias–Papadimitriou\nindexability model for datasets equipped with a similarity measure, with the\naim of better understanding the structure of indexing schemes for\nsimilarity-based search and the geometry of similarity workloads. This in\nparticular provides a unified approach to a great variety of schemes used to\nindex into metric spaces and facilitates their transfer to more general\nsimilarity measures such as quasi-metrics. We discuss links between performance\nof indexing schemes and high-dimensional geometry. The concepts and results are\nillustrated on a very large concrete dataset of peptide fragments equipped with\na biologically significant similarity measure.</p>\n", "tags": [] },
{"key": "petrovic2024streaming", "year": "2024", "title":"Streaming First Story Detection With Application To Twitter", "abstract": "<p>With the recent rise in popularity and size of\nsocial media, there is a growing need for systems\nthat can extract useful information from\nthis amount of data. We address the problem\nof detecting new events from a stream of\nTwitter posts. To make event detection feasible\non web-scale corpora, we present an algorithm\nbased on locality-sensitive hashing which\nis able overcome the limitations of traditional\napproaches, while maintaining competitive results.\nIn particular, a comparison with a stateof-the-art\nsystem on the first story detection\ntask shows that we achieve over an order of\nmagnitude speedup in processing time, while\nretaining comparable performance. Event detection\nexperiments on a collection of 160 million\nTwitter posts show that celebrity deaths\nare the fastest spreading news on Twitter.</p>\n", "tags": ["ARXIV"] },
{"key": "petrovic2024using", "year": "2024", "title":"Using Paraphrases For Improving First Story Detection In News And Twitter", "abstract": "<p>First story detection (FSD) involves identifying\nfirst stories about events from a continuous\nstream of documents. A major problem in this\ntask is the high degree of lexical variation in\ndocuments which makes it very difficult to detect\nstories that talk about the same event but\nexpressed using different words. We suggest\nusing paraphrases to alleviate this problem,\nmaking this the first work to use paraphrases\nfor FSD. We show a novel way of integrating\nparaphrases with locality sensitive hashing\n(LSH) in order to obtain an efficient FSD system\nthat can scale to very large datasets. Our\nsystem achieves state-of-the-art results on the\nfirst story detection task, beating both the best\nsupervised and unsupervised systems. To test\nour approach on large data, we construct a corpus\nof events for Twitter, consisting of 50 million\ndocuments, and show that paraphrasing is\nalso beneficial in this domain.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "pham2016scalability", "year": "2016", "title":"Scalability And Total Recall With Fast Coveringlsh", "abstract": "<p>Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic\ntechnique for similarity search with strong performance guarantees in\nhigh-dimensional spaces. A drawback of traditional LSH schemes is that they may\nhave <em>false negatives</em>, i.e., the recall is less than 100\\%. This limits\nthe applicability of LSH in settings requiring precise performance guarantees.\nBuilding on the recent theoretical “CoveringLSH” construction that eliminates\nfalse negatives, we propose a fast and practical covering LSH scheme for\nHamming space called <em>Fast CoveringLSH (fcLSH)</em>. Inheriting the design\nbenefits of CoveringLSH our method avoids false negatives and always reports\nall near neighbors. Compared to CoveringLSH we achieve an asymptotic\nimprovement to the hash function computation time from \\(\\mathcal{O}(dL)\\) to\n\\(\\mathcal{O}(d + Llog{L})\\), where \\(d\\) is the dimensionality of data and \\(L\\) is\nthe number of hash tables. Our experiments on synthetic and real-world data\nsets demonstrate that <em>fcLSH</em> is comparable (and often superior) to\ntraditional hashing-based approaches for search radius up to 20 in\nhigh-dimensional Hamming space.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "pham2022locality", "year": "2022", "title":"Falconn++ A Locality-sensitive Filtering Approach For Approximate Nearest Neighbor Search", "abstract": "<p>We present Falconn++, a novel locality-sensitive filtering (LSF) approach for approximate nearest neighbor search on angular distance. Falconn++ can filter out potential far away points in any hash bucket before querying, which results in higher quality candidates compared to other hashing-based solutions. Theoretically, Falconn++ asymptotically achieves lower query time complexity than Falconn, an optimal locality-sensitive hashing scheme on angular distance. Empirically, Falconn++ achieves a higher recall-speed tradeoff than Falconn on many real-world data sets. Falconn++ is also competitive with HNSW, an efficient representative of graph-based solutions on high search recall regimes.</p>\n", "tags": ["Graph","NEURIPS"] },
{"key": "pibiri2021parallel", "year": "2021", "title":"Parallel And External-memory Construction Of Minimal Perfect Hash Functions With Pthash", "abstract": "<p>A function \\(f : U \\to \\{0,\\ldots,n-1\\}\\) is a minimal perfect hash function\nfor a set \\(S \\subseteq U\\) of size \\(n\\), if \\(f\\) bijectively maps \\(S\\) into the\nfirst \\(n\\) natural numbers. These functions are important for many practical\napplications in computing, such as search engines, computer networks, and\ndatabases. Several algorithms have been proposed to build minimal perfect hash\nfunctions that: scale well to large sets, retain fast evaluation time, and take\nvery little space, e.g., 2 - 3 bits/key. PTHash is one such algorithm,\nachieving very fast evaluation in compressed space, typically several times\nfaster than other techniques. In this work, we propose a new construction\nalgorithm for PTHash enabling: (1) multi-threading, to either build functions\nmore quickly or more space-efficiently, and (2) external-memory processing to\nscale to inputs much larger than the available internal memory. Only few other\nalgorithms in the literature share these features, despite of their big\npractical impact. We conduct an extensive experimental assessment on large\nreal-world string collections and show that, with respect to other techniques,\nPTHash is competitive in construction time and space consumption, but retains 2</p>\n<ul>\n  <li>6\\(\\times\\) better lookup time.</li>\n</ul>\n", "tags": ["ARXIV","Independent"] },
{"key": "pibiri2021pthash", "year": "2021", "title":"Pthash Revisiting FCH Minimal Perfect Hashing", "abstract": "<p>Given a set \\(S\\) of \\(n\\) distinct keys, a function \\(f\\) that bijectively maps\nthe keys of \\(S\\) into the range \\(\\{0,\\ldots,n-1\\}\\) is called a minimal perfect\nhash function for \\(S\\). Algorithms that find such functions when \\(n\\) is large\nand retain constant evaluation time are of practical interest; for instance,\nsearch engines and databases typically use minimal perfect hash functions to\nquickly assign identifiers to static sets of variable-length keys such as\nstrings. The challenge is to design an algorithm which is efficient in three\ndifferent aspects: time to find \\(f\\) (construction time), time to evaluate \\(f\\)\non a key of \\(S\\) (lookup time), and space of representation for \\(f\\). Several\nalgorithms have been proposed to trade-off between these aspects. In 1992, Fox,\nChen, and Heath (FCH) presented an algorithm at SIGIR providing very fast\nlookup evaluation. However, the approach received little attention because of\nits large construction time and higher space consumption compared to other\nsubsequent techniques. Almost thirty years later we revisit their framework and\npresent an improved algorithm that scales well to large sets and reduces space\nconsumption altogether, without compromising the lookup time. We conduct an\nextensive experimental assessment and show that the algorithm finds functions\nthat are competitive in space with state-of-the art techniques and provide\n\\(2-4\\times\\) better lookup time.</p>\n", "tags": ["Independent","SIGIR"] },
{"key": "pibiri2022locality", "year": "2022", "title":"Locality-preserving Minimal Perfect Hashing Of K-mers", "abstract": "<p>Minimal perfect hashing is the problem of mapping a static set of \\(n\\)\ndistinct keys into the address space \\(\\{1,\\ldots,n\\}\\) bijectively. It is\nwell-known that \\(nlog_2(e)\\) bits are necessary to specify a minimal perfect\nhash function (MPHF) \\(f\\), when no additional knowledge of the input keys is to\nbe used. However, it is often the case in practice that the input keys have\nintrinsic relationships that we can exploit to lower the bit complexity of \\(f\\).\nFor example, consider a string and the set of all its distinct \\(k\\)-mers as\ninput keys: since two consecutive \\(k\\)-mers share an overlap of \\(k-1\\) symbols,\nit seems possible to beat the classic \\(log_2(e)\\) bits/key barrier in this\ncase. Moreover, we would like \\(f\\) to map consecutive \\(k\\)-mers to consecutive\naddresses, as to also preserve as much as possible their relationship in the\ncodomain. This is a useful feature in practice as it guarantees a certain\ndegree of locality of reference for \\(f\\), resulting in a better evaluation time\nwhen querying consecutive \\(k\\)-mers. Motivated by these premises, we initiate\nthe study of a new type of locality-preserving MPHF designed for \\(k\\)-mers\nextracted consecutively from a collection of strings. We design a construction\nwhose space usage decreases for growing \\(k\\) and discuss experiments with a\npractical implementation of the method: in practice, the functions built with\nour method can be several times smaller and even faster to query than the most\nefficient MPHFs in the literature.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "plasenciacalaña2017scalable", "year": "2017", "title":"Scalable Prototype Selection By Genetic Algorithms And Hashing", "abstract": "<p>Classification in the dissimilarity space has become a very active research\narea since it provides a possibility to learn from data given in the form of\npairwise non-metric dissimilarities, which otherwise would be difficult to cope\nwith. The selection of prototypes is a key step for the further creation of the\nspace. However, despite previous efforts to find good prototypes, how to select\nthe best representation set remains an open issue. In this paper we proposed\nscalable methods to select the set of prototypes out of very large datasets.\nThe methods are based on genetic algorithms, dissimilarity-based hashing, and\ntwo different unsupervised and supervised scalable criteria. The unsupervised\ncriterion is based on the Minimum Spanning Tree of the graph created by the\nprototypes as nodes and the dissimilarities as edges. The supervised criterion\nis based on counting matching labels of objects and their closest prototypes.\nThe suitability of these type of algorithms is analyzed for the specific case\nof dissimilarity representations. The experimental results showed that the\nmethods select good prototypes taking advantage of the large datasets, and they\ndo so at low runtimes.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "polak2016improved", "year": "2016", "title":"Improved Upper Bound On A(188)", "abstract": "<p>For nonnegative integers \\(n\\) and \\(d\\), let \\(A(n,d)\\) be the maximum cardinality\nof a binary code of length \\(n\\) and minimum distance at least \\(d\\). We consider a\nslight sharpening of the semidefinite programming bound of Gijswijt, Mittelmann\nand Schrijver, and obtain that \\(A(18,8)\\leq 70\\).</p>\n", "tags": ["ARXIV"] },
{"key": "porat2011cuckoo", "year": "2011", "title":"A Cuckoo Hashing Variant With Improved Memory Utilization And Insertion Time", "abstract": "<p>Cuckoo hashing [4] is a multiple choice hashing scheme in which each item can\nbe placed in multiple locations, and collisions are resolved by moving items to\ntheir alternative locations. In the classical implementation of two-way cuckoo\nhashing, the memory is partitioned into contiguous disjoint fixed-size buckets.\nEach item is hashed to two buckets, and may be stored in any of the positions\nwithin those buckets. Ref. [2] analyzed a variation in which the buckets are\ncontiguous and overlap. However, many systems retrieve data from secondary\nstorage in same-size blocks called pages. Fetching a page is a relatively\nexpensive process; but once a page is fetched, its contents can be accessed\norders of magnitude faster. We utilize this property of memory retrieval,\npresenting a variant of cuckoo hashing incorporating the following constraint:\neach bucket must be fully contained in a single page, but buckets are not\nnecessarily contiguous. Empirical results show that this modification increases\nmemory utilization and decreases the number of iterations required to insert an\nitem. If each item is hashed to two buckets of capacity two, the page size is\n8, and each bucket is fully contained in a single page, the memory utilization\nequals 89.71% in the classical contiguous disjoint bucket variant, 93.78% in\nthe contiguous overlapping bucket variant, and increases to 97.46% in our new\nnon-contiguous bucket variant. When the memory utilization is 92% and we use\nbreadth first search to look for a vacant position, the number of iterations\nrequired to insert a new item is dramatically reduced from 545 in the\ncontiguous overlapping buckets variant to 52 in our new non-contiguous bucket\nvariant. In addition to the empirical results, we present a theoretical lower\nbound on the memory utilization of our variation as a function of the page\nsize.</p>\n", "tags": ["ARXIV"] },
{"key": "portegys2015general", "year": "2015", "title":"General Graph Identification By Hashing", "abstract": "<p>A method for identifying graphs using MD5 hashing is presented. This allows\nfast graph equality comparisons and can also be used to facilitate graph\nisomorphism testing. The graphs can be labeled or unlabeled. The method\nidentifies vertices by hashing the graph configuration in their neighborhoods.\nWith each vertex hashed, the entire graph can be identified by hashing the\nvertex hashes.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "pratiher2018diving", "year": "2018", "title":"Diving Deep Onto Discriminative Ensemble Of Histological Hashing Class-specific Manifold Learning For Multi-class Breast Carcinoma Taxonomy", "abstract": "<p>Histopathological images (HI) encrypt resolution dependent heterogeneous\ntextures &amp; diverse color distribution variability, manifesting in\nmicro-structural surface tissue convolutions. Also, inherently high coherency\nof cancerous cells poses significant challenges to breast cancer (BC)\nmulti-classification. As such, multi-class stratification is sparsely explored\n&amp; prior work mainly focus on benign &amp; malignant tissue characterization only,\nwhich forestalls further quantitative analysis of subordinate classes like\nadenosis, mucinous carcinoma &amp; fibroadenoma etc, for diagnostic competence. In\nthis work, a fully-automated, near-real-time &amp; computationally inexpensive\nrobust multi-classification deep framework from HI is presented.\n  The proposed scheme employs deep neural network (DNN) aided discriminative\nensemble of holistic class-specific manifold learning (CSML) for underlying HI\nsub-space embedding &amp; HI hashing based local shallow signatures. The model\nachieves 95.8% accuracy pertinent to multi-classification &amp; 2.8% overall\nperformance improvement &amp; 38.2% enhancement for Lobular carcinoma (LC)\nsub-class recognition rate as compared to the existing state-of-the-art on well\nknown BreakHis dataset is achieved. Also, 99.3% recognition rate at 200X &amp; a\nsensitivity of 100% for binary grading at all magnification validates its\nsuitability for clinical deployment in hand-held smart devices.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "prezza2023algorithms", "year": "2023", "title":"Algorithms For Massive Data -- Lecture Notes", "abstract": "<p>These are the lecture notes for the course CM0622 - Algorithms for Massive\nData, Ca’ Foscari University of Venice. The goal of this course is to introduce\nalgorithmic techniques for dealing with massive data: data so large that it\ndoes not fit in the computer’s memory. There are two main solutions to deal\nwith massive data: (lossless) compressed data structures and (lossy) data\nsketches. These notes cover both topics: compressed suffix arrays,\nprobabilistic filters, sketching under various metrics, Locality Sensitive\nHashing, nearest neighbour search, algorithms on streams (pattern matching,\ncounting).</p>\n", "tags": ["ARXIV"] },
{"key": "primmer2013collision", "year": "2013", "title":"Collision And Preimage Resistance Of The Centera Content Address", "abstract": "<p>Centera uses cryptographic hash functions as a means of addressing stored\nobjects, thus creating a new class of data storage referred to as CAS (content\naddressed storage). Such hashing serves the useful function of providing a\nmeans of uniquely identifying data and providing a global handle to that data,\nreferred to as the Content Address or CA. However, such a model begs the\nquestion: how certain can one be that a given CA is indeed unique?\n  In this paper we describe fundamental concepts of cryptographic hash\nfunctions, such as collision resistance, pre-image resistance, and\nsecond-preimage resistance. We then map these properties to the MD5 and SHA-256\nhash algorithms, which are used to generate the Centera content address.\nFinally, we present a proof of the collision resistance of the Centera Content\nAddress.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "prior2023let", "year": "2023", "title":"Let Them Have CAKES A Cutting-edge Algorithm For Scalable Efficient And Exact Search On Big Data", "abstract": "<p>The ongoing Big Data explosion has created a demand for efficient and\nscalable algorithms for similarity search.\n  Most recent work has focused on \\textit{approximate} \\(k\\)-NN search, and while\nthis may be sufficient for some applications, \\textit{exact} \\(k\\)-NN search\nwould be ideal for many applications.\n  We present CAKES, a set of three novel, exact algorithms for \\(k\\)-NN search.\n  CAKES’s algorithms are generic over \\textit{any} distance function, and they\n\\textit{do not} scale with the cardinality or embedding dimension of the\ndataset, but rather with its metric entropy and fractal dimension.\n  We test these claims on datasets from the ANN-Benchmarks suite under\ncommonly-used distance functions, as well as on a genomic dataset with\nLevenshtein distance and a radio-frequency dataset with Dynamic Time Warping\ndistance.\n  We demonstrate that CAKES exhibits near-constant scaling with cardinality on\ndata conforming to the manifold hypothesis, and has perfect recall on data in\n\\textit{metric} spaces.\n  We also demonstrate that CAKES exhibits significantly higher recall than\nstate-of-the-art \\(k\\)-NN search algorithms when the distance function is not a\nmetric.\n  Additionally, we show that indexing and tuning time for CAKES is an order of\nmagnitude, or more, faster than state-of-the-art approaches.\n  We conclude that CAKES is a highly efficient and scalable algorithm for exact\n\\(k\\)-NN search on Big Data.\n  We provide a Rust implementation of CAKES.</p>\n", "tags": ["ARXIV"] },
{"key": "pronobis2016sharing", "year": "2016", "title":"Sharing Hash Codes For Multiple Purposes", "abstract": "<p>Locality sensitive hashing (LSH) is a powerful tool for sublinear-time\napproximate nearest neighbor search, and a variety of hashing schemes have been\nproposed for different dissimilarity measures. However, hash codes\nsignificantly depend on the dissimilarity, which prohibits users from adjusting\nthe dissimilarity at query time. In this paper, we propose {multiple purpose\nLSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH\nsupports L2, cosine, and inner product dissimilarities, and their corresponding\nweighted sums, where the weights can be adjusted at query time. It also allows\nus to modify the importance of pre-defined groups of features. Thus, mp-LSH\nenables us, for example, to retrieve similar items to a query with the user\npreference taken into account, to find a similar material to a query with some\nproperties (stability, utility, etc.) optimized, and to turn on or off a part\nof multi-modal information (brightness, color, audio, text, etc.) in\nimage/video retrieval. We theoretically and empirically analyze the performance\nof three variants of mp-LSH, and demonstrate their usefulness on real-world\ndata sets.</p>\n", "tags": ["ARXIV","Independent","LSH","Video Retrieval"] },
{"key": "qasemizadeh2017sketching", "year": "2017", "title":"Sketching Word Vectors Through Hashing", "abstract": "<p>We propose a new fast word embedding technique using hash functions. The\nmethod is a derandomization of a new type of random projections: By\ndisregarding the classic constraint used in designing random projections (i.e.,\npreserving pairwise distances in a particular normed space), our solution\nexploits extremely sparse non-negative random projections. Our experiments show\nthat the proposed method can achieve competitive results, comparable to neural\nembedding learning techniques, however, with only a fraction of the\ncomputational complexity of these methods. While the proposed derandomization\nenhances the computational and space complexity of our method, the possibility\nof applying weighting methods such as positive pointwise mutual information\n(PPMI) to our models after their construction (and at a reduced dimensionality)\nimparts a high discriminatory power to the resulting embeddings. Obviously,\nthis method comes with other known benefits of random projection-based\ntechniques such as ease of update.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "qi2017efficient", "year": "2017", "title":"An Efficient Deep Learning Hashing Neural Network For Mobile Visual Search", "abstract": "<p>Mobile visual search applications are emerging that enable users to sense\ntheir surroundings with smart phones. However, because of the particular\nchallenges of mobile visual search, achieving a high recognition bitrate has\nbecomes a consistent target of previous related works. In this paper, we\npropose a few-parameter, low-latency, and high-accuracy deep hashing approach\nfor constructing binary hash codes for mobile visual search. First, we exploit\nthe architecture of the MobileNet model, which significantly decreases the\nlatency of deep feature extraction by reducing the number of model parameters\nwhile maintaining accuracy. Second, we add a hash-like layer into MobileNet to\ntrain the model on labeled mobile visual data. Evaluations show that the\nproposed system can exceed state-of-the-art accuracy performance in terms of\nthe MAP. More importantly, the memory consumption is much less than that of\nother deep learning models. The proposed method requires only \\(13\\) MB of memory\nfor the neural network and achieves a MAP of \\(97.80\\%\\) on the mobile location\nrecognition dataset used for testing.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "qi2022principled", "year": "2022", "title":"A Principled Design Of Image Representation Towards Forensic Tasks", "abstract": "<p>Image forensics is a rising topic as the trustworthy multimedia content is\ncritical for modern society. Like other vision-related applications, forensic\nanalysis relies heavily on the proper image representation. Despite the\nimportance, current theoretical understanding for such representation remains\nlimited, with varying degrees of neglect for its key role. For this gap, we\nattempt to investigate the forensic-oriented image representation as a distinct\nproblem, from the perspectives of theory, implementation, and application. Our\nwork starts from the abstraction of basic principles that the representation\nfor forensics should satisfy, especially revealing the criticality of\nrobustness, interpretability, and coverage. At the theoretical level, we\npropose a new representation framework for forensics, called Dense Invariant\nRepresentation (DIR), which is characterized by stable description with\nmathematical guarantees. At the implementation level, the discrete calculation\nproblems of DIR are discussed, and the corresponding accurate and fast\nsolutions are designed with generic nature and constant complexity. We\ndemonstrate the above arguments on the dense-domain pattern detection and\nmatching experiments, providing comparison results with state-of-the-art\ndescriptors. Also, at the application level, the proposed DIR is initially\nexplored in passive and active forensics, namely copy-move forgery detection\nand perceptual hashing, exhibiting the benefits in fulfilling the requirements\nof such forensic tasks.</p>\n", "tags": ["ARXIV"] },
{"key": "qiao2019deep", "year": "2019", "title":"Deep Heterogeneous Hashing For Face Video Retrieval", "abstract": "<p>Retrieving videos of a particular person with face image as a query via\nhashing technique has many important applications. While face images are\ntypically represented as vectors in Euclidean space, characterizing face videos\nwith some robust set modeling techniques (e.g. covariance matrices as exploited\nin this study, which reside on Riemannian manifold), has recently shown\nappealing advantages. This hence results in a thorny heterogeneous spaces\nmatching problem. Moreover, hashing with handcrafted features as done in many\nexisting works is clearly inadequate to achieve desirable performance for this\ntask. To address such problems, we present an end-to-end Deep Heterogeneous\nHashing (DHH) method that integrates three stages including image feature\nlearning, video modeling, and heterogeneous hashing in a single framework, to\nlearn unified binary codes for both face images and videos. To tackle the key\nchallenge of hashing on the manifold, a well-studied Riemannian kernel mapping\nis employed to project data (i.e. covariance matrices) into Euclidean space and\nthus enables to embed the two heterogeneous representations into a common\nHamming space, where both intra-space discriminability and inter-space\ncompatibility are considered. To perform network optimization, the gradient of\nthe kernel mapping is innovatively derived via structured matrix\nbackpropagation in a theoretically principled way. Experiments on three\nchallenging datasets show that our method achieves quite competitive\nperformance compared with existing hashing methods.</p>\n", "tags": ["Cross Modal","ICIP","Video Retrieval"] },
{"key": "qin2014quantized", "year": "2014", "title":"Quantized Kernel Learning For Feature Matching", "abstract": "<p>Matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure. As it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable. However, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability. In this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as Quantized Kernels (QK). QKs are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space. Quantization allows to compress features and keep the learning tractable. As a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension. QKs also have explicit non-linear, low-dimensional feature mappings that grant access to Euclidean geometry for uncompressed features.</p>\n", "tags": ["NEURIPS","Quantisation"] },
{"key": "qin2022adaptive", "year": "2022", "title":"Adaptive And Dynamic Multi-resolution Hashing For Pairwise Summations", "abstract": "<p>In this paper, we propose Adam-Hash: an adaptive and dynamic multi-resolution\nhashing data-structure for fast pairwise summation estimation. Given a data-set\n\\(X \\subset \\mathbb{R}^d\\), a binary function \\(f:\\mathbb{R}^d\\times\n\\mathbb{R}^d\\to \\mathbb{R}\\), and a point \\(y \\in \\mathbb{R}^d\\), the Pairwise\nSummation Estimate \\(\\mathrm{PSE}<em>X(y) := \\frac{1}{|X|} \\sum</em>{x \\in X} f(x,y)\\).\nFor any given data-set \\(X\\), we need to design a data-structure such that given\nany query point \\(y \\in \\mathbb{R}^d\\), the data-structure approximately\nestimates \\(\\mathrm{PSE}<em>X(y)\\) in time that is sub-linear in \\(|X|\\). Prior works\non this problem have focused exclusively on the case where the data-set is\nstatic, and the queries are independent. In this paper, we design a\nhashing-based PSE data-structure which works for the more practical\n\\textit{dynamic} setting in which insertions, deletions, and replacements of\npoints are allowed. Moreover, our proposed Adam-Hash is also robust to adaptive\nPSE queries, where an adversary can choose query \\(q_j \\in \\mathbb{R}^d\\)\ndepending on the output from previous queries \\(q_1, q_2, \\dots, q</em>{j-1}\\).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "qiu2014random", "year": "2014", "title":"Random Forests Can Hash", "abstract": "<p>Hash codes are a very efficient data representation needed to be able to cope\nwith the ever growing amounts of data. We introduce a random forest semantic\nhashing scheme with information-theoretic code aggregation, showing for the\nfirst time how random forest, a technique that together with deep learning have\nshown spectacular results in classification, can also be extended to\nlarge-scale retrieval. Traditional random forest fails to enforce the\nconsistency of hashes generated from each tree for the same class data, i.e.,\nto preserve the underlying similarity, and it also lacks a principled way for\ncode aggregation across trees. We start with a simple hashing scheme, where\nindependently trained random trees in a forest are acting as hashing functions.\nWe the propose a subspace model as the splitting function, and show that it\nenforces the hash consistency in a tree for data from the same class. We also\nintroduce an information-theoretic approach for aggregating codes of individual\ntrees into a single hash code, producing a near-optimal unique hash for each\nclass. Experiments on large-scale public datasets are presented, showing that\nthe proposed approach significantly outperforms state-of-the-art hashing\nmethods for retrieval tasks.</p>\n", "tags": ["ARXIV","Deep Learning","Independent"] },
{"key": "qiu2017foresthash", "year": "2017", "title":"Foresthash Semantic Hashing With Shallow Random Forests And Tiny Convolutional Networks", "abstract": "<p>Hash codes are efficient data representations for coping with the ever\ngrowing amounts of data. In this paper, we introduce a random forest semantic\nhashing scheme that embeds tiny convolutional neural networks (CNN) into\nshallow random forests, with near-optimal information-theoretic code\naggregation among trees. We start with a simple hashing scheme, where random\ntrees in a forest act as hashing functions by setting <code class=\"language-plaintext highlighter-rouge\">1' for the visited tree\nleaf, and </code>0’ for the rest. We show that traditional random forests fail to\ngenerate hashes that preserve the underlying similarity between the trees,\nrendering the random forests approach to hashing challenging. To address this,\nwe propose to first randomly group arriving classes at each tree split node\ninto two groups, obtaining a significantly simplified two-class classification\nproblem, which can be handled using a light-weight CNN weak learner. Such\nrandom class grouping scheme enables code uniqueness by enforcing each class to\nshare its code with different classes in different trees. A non-conventional\nlow-rank loss is further adopted for the CNN weak learners to encourage code\nconsistency by minimizing intra-class variations and maximizing inter-class\ndistance for the two random class groups. Finally, we introduce an\ninformation-theoretic approach for aggregating codes of individual trees into a\nsingle hash code, producing a near-optimal unique hash for each class. The\nproposed approach significantly outperforms state-of-the-art hashing methods\nfor image retrieval tasks on large-scale public datasets, while performing at\nthe level of other state-of-the-art image classification techniques while\nutilizing a more compact and efficient scalable representation. This work\nproposes a principled and robust procedure to train and deploy in parallel an\nensemble of light-weight CNNs, instead of simply going deeper.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Independent"] },
{"key": "qiu2018deep", "year": "2018", "title":"Deep Semantic Hashing With Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "qiu2021unsupervised", "year": "2021", "title":"Unsupervised Hashing With Contrastive Information Bottleneck", "abstract": "<p>Many unsupervised hashing methods are implicitly established on the idea of\nreconstructing the input data, which basically encourages the hashing codes to\nretain as much information of original data as possible. However, this\nrequirement may force the models spending lots of their effort on\nreconstructing the unuseful background information, while ignoring to preserve\nthe discriminative semantic information that is more important for the hashing\ntask. To tackle this problem, inspired by the recent success of contrastive\nlearning in learning continuous representations, we propose to adapt this\nframework to learn binary hashing codes. Specifically, we first propose to\nmodify the objective function to meet the specific requirement of hashing and\nthen introduce a probabilistic binary representation layer into the model to\nfacilitate end-to-end training of the entire model. We further prove the strong\nconnection between the proposed contrastive-learning-based hashing method and\nthe mutual information, and show that the proposed model can be considered\nunder the broader framework of the information bottleneck (IB). Under this\nperspective, a more general hashing model is naturally obtained. Extensive\nexperimental results on three benchmark image datasets demonstrate that the\nproposed hashing method significantly outperforms existing baselines.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "qiu2022efficient", "year": "2022", "title":"Efficient Document Retrieval By End-to-end Refining And Quantizing BERT Embedding With Contrastive Product Quantization", "abstract": "<p>Efficient document retrieval heavily relies on the technique of semantic\nhashing, which learns a binary code for every document and employs Hamming\ndistance to evaluate document distances. However, existing semantic hashing\nmethods are mostly established on outdated TFIDF features, which obviously do\nnot contain lots of important semantic information about documents.\nFurthermore, the Hamming distance can only be equal to one of several integer\nvalues, significantly limiting its representational ability for document\ndistances. To address these issues, in this paper, we propose to leverage BERT\nembeddings to perform efficient retrieval based on the product quantization\ntechnique, which will assign for every document a real-valued codeword from the\ncodebook, instead of a binary code as in semantic hashing. Specifically, we\nfirst transform the original BERT embeddings via a learnable mapping and feed\nthe transformed embedding into a probabilistic product quantization module to\noutput the assigned codeword. The refining and quantizing modules can be\noptimized in an end-to-end manner by minimizing the probabilistic contrastive\nloss. A mutual information maximization based method is further proposed to\nimprove the representativeness of codewords, so that documents can be quantized\nmore accurately. Extensive experiments conducted on three benchmarks\ndemonstrate that our proposed method significantly outperforms current\nstate-of-the-art baselines.</p>\n", "tags": ["Quantisation"] },
{"key": "qiu2022hashvfl", "year": "2022", "title":"Hashvfl Defending Against Data Reconstruction Attacks In Vertical Federated Learning", "abstract": "<p>Vertical Federated Learning (VFL) is a trending collaborative machine\nlearning model training solution. Existing industrial frameworks employ secure\nmulti-party computation techniques such as homomorphic encryption to ensure\ndata security and privacy. Despite these efforts, studies have revealed that\ndata leakage remains a risk in VFL due to the correlations between intermediate\nrepresentations and raw data. Neural networks can accurately capture these\ncorrelations, allowing an adversary to reconstruct the data. This emphasizes\nthe need for continued research into securing VFL systems.\n  Our work shows that hashing is a promising solution to counter data\nreconstruction attacks. The one-way nature of hashing makes it difficult for an\nadversary to recover data from hash codes. However, implementing hashing in VFL\npresents new challenges, including vanishing gradients and information loss. To\naddress these issues, we propose HashVFL, which integrates hashing and\nsimultaneously achieves learnability, bit balance, and consistency.\n  Experimental results indicate that HashVFL effectively maintains task\nperformance while defending against data reconstruction attacks. It also brings\nadditional benefits in reducing the degree of label leakage, mitigating\nadversarial attacks, and detecting abnormal inputs. We hope our work will\ninspire further research into the potential applications of HashVFL.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "qiu2024deep", "year": "2024", "title":"Deep Semantic Hashing With Generative Adversarial Networks", "abstract": "<p>Hashing has been a widely-adopted technique for nearest\nneighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can\nlead to high quality hashing. However, the cost of annotating\ndata is often an obstacle when applying supervised hashing\nto a new domain. Moreover, the results can suffer from the\nrobustness problem as the data at training and test stage\nmay come from different distributions. This paper studies\nthe exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which\nleverages largely unlabeled and limited labeled training data\nto produce highly compelling data with intrinsic invariance\nand global coherence, for better understanding statistical\nstructures of natural data. We demonstrate that the above\ntwo limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic\nhashing with GANs (DSH-GANs) is presented, which mainly\nconsists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary\nstream to distinguish synthetic images from real ones, a hash\nstream for encoding image representations to hash codes and\na classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss\nto correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the\ninput real-synthetic triplets and classification loss to classify\neach sample accurately. Extensive experiments conducted on\nboth CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our\nframework also achieves superior results when compared to\nstate-of-the-art deep hash models.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "qiu2024hihpq", "year": "2024", "title":"Hihpq Hierarchical Hyperbolic Product Quantization For Unsupervised Image Retrieval", "abstract": "<p>Existing unsupervised deep product quantization methods primarily aim for the\nincreased similarity between different views of the identical image, whereas\nthe delicate multi-level semantic similarities preserved between images are\noverlooked. Moreover, these methods predominantly focus on the Euclidean space\nfor computational convenience, compromising their ability to map the\nmulti-level semantic relationships between images effectively. To mitigate\nthese shortcomings, we propose a novel unsupervised product quantization method\ndubbed \\textbf{Hi}erarchical \\textbf{H}yperbolic \\textbf{P}roduct\n\\textbf{Q}uantization (HiHPQ), which learns quantized representations by\nincorporating hierarchical semantic similarity within hyperbolic geometry.\nSpecifically, we propose a hyperbolic product quantizer, where the hyperbolic\ncodebook attention mechanism and the quantized contrastive learning on the\nhyperbolic product manifold are introduced to expedite quantization.\nFurthermore, we propose a hierarchical semantics learning module, designed to\nenhance the distinction between similar and non-matching images for a query by\nutilizing the extracted hierarchical semantics as an additional training\nsupervision. Experiments on benchmarks show that our proposed method\noutperforms state-of-the-art baselines.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "quedenfeld2017variant", "year": "2017", "title":"Variant Tolerant Read Mapping Using Min-hashing", "abstract": "<p>DNA read mapping is a ubiquitous task in bioinformatics, and many tools have\nbeen developed to solve the read mapping problem. However, there are two trends\nthat are changing the landscape of readmapping: First, new sequencing\ntechnologies provide very long reads with high error rates (up to 15%). Second,\nmany genetic variants in the population are known, so the reference genome is\nnot considered as a single string over ACGT, but as a complex object containing\nthese variants. Most existing read mappers do not handle these new\ncircumstances appropriately.\n  We introduce a new read mapper prototype called VATRAM that considers\nvariants. It is based on Min-Hashing of q-gram sets of reference genome\nwindows. Min-Hashing is one form of locality sensitive hashing. The variants\nare directly inserted into VATRAMs index which leads to a fast mapping process.\nOur results show that VATRAM achieves better precision and recall than\nstate-of-the-art read mappers like BWA under certain cirumstances. VATRAM is\nopen source and can be accessed at\nhttps://bitbucket.org/Quedenfeld/vatram-src/.</p>\n", "tags": ["ARXIV","Has Code"] },
{"key": "rabbani2023large", "year": "2023", "title":"Large-scale Distributed Learning Via Private On-device Locality-sensitive Hashing", "abstract": "<p>Locality-sensitive hashing (LSH) based frameworks have been used efficiently\nto select weight vectors in a dense hidden layer with high cosine similarity to\nan input, enabling dynamic pruning. While this type of scheme has been shown to\nimprove computational training efficiency, existing algorithms require repeated\nrandomized projection of the full layer weight, which is impractical for\ncomputational- and memory-constrained devices. In a distributed setting,\ndeferring LSH analysis to a centralized host is (i) slow if the device cluster\nis large and (ii) requires access to input data which is forbidden in a\nfederated context. Using a new family of hash functions, we develop one of the\nfirst private, personalized, and memory-efficient on-device LSH frameworks. Our\nframework enables privacy and personalization by allowing each device to\ngenerate hash tables, without the help of a central host, using device-specific\nhashing hyper-parameters (e.g. number of hash tables or hash length). Hash\ntables are generated with a compressed set of the full weights, and can be\nserially generated and discarded if the process is memory-intensive. This\nallows devices to avoid maintaining (i) the fully-sized model and (ii) large\namounts of hash tables in local memory for LSH analysis. We prove several\nstatistical and sensitivity properties of our hash functions, and\nexperimentally demonstrate that our framework is competitive in training\nlarge-scale recommender networks compared to other LSH frameworks which assume\nunrestricted on-device capacity.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "raff2018engineering", "year": "2018", "title":"Engineering A Simplified 0-bit Consistent Weighted Sampling", "abstract": "<p>The Min-Hashing approach to sketching has become an important tool in data\nanalysis, information retrial, and classification. To apply it to real-valued\ndatasets, the ICWS algorithm has become a seminal approach that is widely used,\nand provides state-of-the-art performance for this problem space. However, ICWS\nsuffers a computational burden as the sketch size K increases. We develop a new\nSimplified approach to the ICWS algorithm, that enables us to obtain over 20x\nspeedups compared to the standard algorithm. The veracity of our approach is\ndemonstrated empirically on multiple datasets and scenarios, showing that our\nnew Simplified CWS obtains the same quality of results while being an order of\nmagnitude faster.</p>\n", "tags": ["COLT","Supervised"] },
{"key": "raginsky2005estimation", "year": "2005", "title":"Estimation Of Intrinsic Dimensionality Using High-rate Vector Quantization", "abstract": "<p>We introduce a technique for dimensionality estimation based on the notion of quantization dimension, which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension. The definition of quantization dimension yields a family of estimation algorithms, whose limiting case is equivalent to a recent method based on packing numbers. Using the formalism of high-rate vector quantization, we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise.</p>\n", "tags": ["NEURIPS","Quantisation"] },
{"key": "raginsky2009locality", "year": "2009", "title":"Locality-sensitive Binary Codes From Shift-invariant Kernels", "abstract": "<p>This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "raginsky2024locality", "year": "2024", "title":"Locality-sensitive Binary Codes From Shift-invariant Kernels", "abstract": "<p>This paper addresses the problem of designing binary codes for high-dimensional\ndata such that vectors that are similar in the original space map to similar binary\nstrings. We introduce a simple distribution-free encoding scheme based on\nrandom projections, such that the expected Hamming distance between the binary\ncodes of two vectors is related to the value of a shift-invariant kernel (e.g., a\nGaussian kernel) between the vectors. We present a full theoretical analysis of the\nconvergence properties of the proposed scheme, and report favorable experimental\nperformance as compared to a recent state-of-the-art method, spectral hashing.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "rahmani2023improving", "year": "2023", "title":"Improving Code Example Recommendations On Informal Documentation Using BERT And Query-aware LSH A Comparative Study", "abstract": "<p>Our research investigates the recommendation of code examples to aid software\ndevelopers, a practice that saves developers significant time by providing\nready-to-use code snippets. The focus of our study is Stack Overflow, a\ncommonly used resource for coding discussions and solutions, particularly in\nthe context of the Java programming language. We applied BERT, a powerful Large\nLanguage Model (LLM) that enables us to transform code examples into numerical\nvectors by extracting their semantic information. Once these numerical\nrepresentations are prepared, we identify Approximate Nearest Neighbors (ANN)\nusing Locality-Sensitive Hashing (LSH). Our research employed two variants of\nLSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared\nthese two approaches across four parameters: HitRate, Mean Reciprocal Rank\n(MRR), Average Execution Time, and Relevance. Our study revealed that the\nQuery-Aware (QA) approach showed superior performance over the Random\nHyperplane-based (RH) method. Specifically, it exhibited a notable improvement\nof 20\\% to 35\\% in HitRate for query pairs compared to the RH approach.\nFurthermore, the QA approach proved significantly more time-efficient, with its\nspeed in creating hashing tables and assigning data samples to buckets being at\nleast four times faster. It can return code examples within milliseconds,\nwhereas the RH approach typically requires several seconds to recommend code\nexamples. Due to the superior performance of the QA approach, we tested it\nagainst PostFinder and FaCoY, the state-of-the-art baselines. Our QA method\nshowed comparable efficiency proving its potential for effective code\nrecommendation.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "ram2009rank", "year": "2009", "title":"Rank-approximate Nearest Neighbor Search Retaining Meaning And Speed In High Dimensions", "abstract": "<p>The long-standing problem of efficient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 fingerprinting to bioinformatics to movie recommendations.  As the dimensionality of the dataset increases, exact NN search becomes computationally prohibitive; (1+eps)-distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple, practical algorithm allowing the user to, for the first time, directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN.  Experiments with high-dimensional datasets show that it often achieves faster and more accurate results than the best-known distance-approximate method, with much more stable behavior.</p>\n", "tags": ["NEURIPS"] },
{"key": "ram2013which", "year": "2013", "title":"Which Space Partitioning Tree To Use For Search", "abstract": "<p>We consider the task of nearest-neighbor search with the class of binary-space-partitioning trees, which includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question which tree to use for nearest-neighbor search?’’ To this end, we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance – margins of the partitions in these trees. We demonstrate, both theoretically and empirically, that large margin partitions can improve the search performance of a space-partitioning tree. “</p>\n", "tags": ["Independent","NEURIPS","Quantisation"] },
{"key": "ram2021federated", "year": "2021", "title":"Federated Nearest Neighbor Classification With A Colony Of Fruit-flies With Supplement", "abstract": "<p>The mathematical formalization of a neurological mechanism in the olfactory\ncircuit of a fruit-fly as a locality sensitive hash (Flyhash) and bloom filter\n(FBF) has been recently proposed and “reprogrammed” for various machine\nlearning tasks such as similarity search, outlier detection and text\nembeddings. We propose a novel reprogramming of this hash and bloom filter to\nemulate the canonical nearest neighbor classifier (NNC) in the challenging\nFederated Learning (FL) setup where training and test data are spread across\nparties and no data can leave their respective parties. Specifically, we\nutilize Flyhash and FBF to create the FlyNN classifier, and theoretically\nestablish conditions where FlyNN matches NNC. We show how FlyNN is trained\nexactly in a FL setup with low communication overhead to produce FlyNNFL, and\nhow it can be differentially private. Empirically, we demonstrate that (i)\nFlyNN matches NNC accuracy across 70 OpenML datasets, (ii) FlyNNFL training is\nhighly scalable with low communication overhead, providing up to \\(8\\times\\)\nspeedup with \\(16\\) parties.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "ramos2024blockboost", "year": "2024", "title":"Blockboost Scalable And Efficient Blocking Through Boosting", "abstract": "<p>As datasets grow larger, matching and merging entries from different databases has become a costly task in modern data pipelines. To avoid expensive comparisons between entries, blocking similar items is a popular preprocessing step. In this paper, we introduce BlockBoost, a novel boosting-based method that generates compact binary hash codes for database entries, through which blocking can be performed efficiently. The algorithm is fast and scalable, resulting in computational costs that are orders of magnitude lower than current benchmarks. Unlike existing alternatives, BlockBoost comes with associated feature importance measures for interpretability, and possesses strong theoretical guarantees, including lower bounds on critical performance metrics like recall and reduction ratio. Finally, we show that BlockBoost delivers great empirical results, outperforming state-of-the-art blocking benchmarks in terms of both performance metrics and computational cost.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "rao2015diverse", "year": "2015", "title":"Diverse Yet Efficient Retrieval Using Hash Functions", "abstract": "<p>Typical retrieval systems have three requirements: a) Accurate retrieval\ni.e., the method should have high precision, b) Diverse retrieval, i.e., the\nobtained set of points should be diverse, c) Retrieval time should be small.\nHowever, most of the existing methods address only one or two of the above\nmentioned requirements. In this work, we present a method based on randomized\nlocality sensitive hashing which tries to address all of the above requirements\nsimultaneously. While earlier hashing approaches considered approximate\nretrieval to be acceptable only for the sake of efficiency, we argue that one\ncan further exploit approximate retrieval to provide impressive trade-offs\nbetween accuracy and diversity. We extend our method to the problem of\nmulti-label prediction, where the goal is to output a diverse and accurate set\nof labels for a given document in real-time. Moreover, we introduce a new\nnotion to simultaneously evaluate a method’s performance for both the precision\nand diversity measures. Finally, we present empirical results on several\ndifferent retrieval tasks and show that our method retrieves diverse and\naccurate images/labels while ensuring \\(100x\\)-speed-up over the existing diverse\nretrieval approaches.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "rashedunnaby2020peer", "year": "2020", "title":"A Peer-to-peer Distributed Secured Sustainable Large Scale Identity Document Verification System With Bittorrent Network And Hash Function", "abstract": "<p>Verifying identity documents from a large Central Identity Database (CIDB) is\nalways challenging and it get more challenging when we need to verify a large\nnumber of documents at the same time. Usually most of the time we setup a\ngateway server connected to the CIDB and it serve all the identity document\nverification requests. Though it work well, there are still chances that this\nmodel will collapse in high traffic. We obviously can tune the system to be\nsustainable, but the process is economically expensive. In this paper we\npropose an economically cheaper way to verify ID documents with private\nBitTorrent network and hash function.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "rashtchian2020lsf", "year": "2020", "title":"Lsf-join Locality Sensitive Filtering For Distributed All-pairs Set Similarity Under Skew", "abstract": "<p>All-pairs set similarity is a widely used data mining task, even for large\nand high-dimensional datasets. Traditionally, similarity search has focused on\ndiscovering very similar pairs, for which a variety of efficient algorithms are\nknown. However, recent work highlights the importance of finding pairs of sets\nwith relatively small intersection sizes. For example, in a recommender system,\ntwo users may be alike even though their interests only overlap on a small\npercentage of items. In such systems, some dimensions are often highly skewed\nbecause they are very popular. Together these two properties render previous\napproaches infeasible for large input sizes. To address this problem, we\npresent a new distributed algorithm, LSF-Join, for approximate all-pairs set\nsimilarity. The core of our algorithm is a randomized selection procedure based\non Locality Sensitive Filtering. Our method deviates from prior approximate\nalgorithms, which are based on Locality Sensitive Hashing. Theoretically, we\nshow that LSF-Join efficiently finds most close pairs, even for small\nsimilarity thresholds and for skewed input sets. We prove guarantees on the\ncommunication, work, and maximum load of LSF-Join, and we also experimentally\ndemonstrate its accuracy on multiple graphs.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "rasiwasia2024new", "year": "2024", "title":"A New Approach To Cross-modal Multimedia Retrieval", "abstract": "<p>The collected documents are selected sections from the Wikipedia’s featured articles collection. This is a continuously growing dataset, that at the time of collection (October 2009) had 2,669 articles spread over 29 categories. Some of the categories are very scarce, therefore we considered only the 10 most populated ones. The articles generally have multiple sections and pictures. We have split them into sections based on section headings, and assign each image to the section in which it was placed by the author(s). Then this dataset was prunned to keep only sections that contained a single image and at least 70 words. \nThe final corpus contains 2,866 multimedia documents. The median text length is 200 words.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "rastegari2014comparing", "year": "2014", "title":"Comparing Apples To Apples In The Evaluation Of Binary Coding Methods", "abstract": "<p>We discuss methodological issues related to the evaluation of unsupervised\nbinary code construction methods for nearest neighbor search. These issues have\nbeen widely ignored in literature. These coding methods attempt to preserve\neither Euclidean distance or angular (cosine) distance in the binary embedding\nspace. We explain why when comparing a method whose goal is preserving cosine\nsimilarity to one designed for preserving Euclidean distance, the original\nfeatures should be normalized by mapping them to the unit hypersphere before\nlearning the binary mapping functions. To compare a method whose goal is to\npreserves Euclidean distance to one that preserves cosine similarity, the\noriginal feature data must be mapped to a higher dimension by including a bias\nterm in binary mapping functions. These conditions ensure the fair comparison\nbetween different binary code methods for the task of nearest neighbor search.\nOur experiments show under these conditions the very simple methods (e.g. LSH\nand ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and\nOK-means).</p>\n", "tags": ["ARXIV","LSH","Unsupervised"] },
{"key": "rastegari2024xnor", "year": "2024", "title":"Xnor-net Imagenet Classification Using Binary Convolutional Neural Networks", "abstract": "<p>We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values\nresulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This\nresults in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary\nnetworks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network\nversion of AlexNet is only 2.9\\% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\\% in top-1 accuracy.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "ravanbakhsh2016efficient", "year": "2016", "title":"Efficient Convolutional Neural Network With Binary Quantization Layer", "abstract": "<p>In this paper we introduce a novel method for segmentation that can benefit\nfrom general semantics of Convolutional Neural Network (CNN). Our segmentation\nproposes visually and semantically coherent image segments. We use binary\nencoding of CNN features to overcome the difficulty of the clustering on the\nhigh-dimensional CNN feature space. These binary encoding can be embedded into\nthe CNN as an extra layer at the end of the network. This results in real-time\nsegmentation. To the best of our knowledge our method is the first attempt on\ngeneral semantic image segmentation using CNN. All the previous papers were\nlimited to few number of category of the images (e.g. PASCAL VOC). Experiments\nshow that our segmentation algorithm outperform the state-of-the-art\nnon-semantic segmentation methods by a large margin.</p>\n", "tags": ["ARXIV","CNN","Quantisation","Supervised"] },
{"key": "ravfogel2023description", "year": "2023", "title":"Description-based Text Similarity", "abstract": "<p>Identifying texts with a given semantics is central for many information\nseeking scenarios. Similarity search over vector embeddings appear to be\ncentral to this ability, yet the similarity reflected in current text\nembeddings is corpus-driven, and is inconsistent and sub-optimal for many use\ncases. What, then, is a good notion of similarity for effective retrieval of\ntext?\n  We identify the need to search for texts based on abstract descriptions of\ntheir content, and the corresponding notion of <em>description based\nsimilarity</em>. We demonstrate the inadequacy of current text embeddings and\npropose an alternative model that significantly improves when used in standard\nnearest neighbor search. The model is trained using positive and negative pairs\nsourced through prompting a LLM, demonstrating how data from LLMs can be used\nfor creating new capabilities not immediately possible using the original\nmodel.</p>\n", "tags": ["ARXIV"] },
{"key": "raziperchikolaei2015optimizing", "year": "2015", "title":"Optimizing Affinity-based Binary Hashing Using Auxiliary Coordinates", "abstract": "<p>In supervised binary hashing, one wants to learn a function that maps a\nhigh-dimensional feature vector to a vector of binary codes, for application to\nfast image retrieval. This typically results in a difficult optimization\nproblem, nonconvex and nonsmooth, because of the discrete variables involved.\nMuch work has simply relaxed the problem during training, solving a continuous\noptimization, and truncating the codes a posteriori. This gives reasonable\nresults but is quite suboptimal. Recent work has tried to optimize the\nobjective directly over the binary codes and achieved better results, but the\nhash function was still learned a posteriori, which remains suboptimal. We\npropose a general framework for learning hash functions using affinity-based\nloss functions that uses auxiliary coordinates. This closes the loop and\noptimizes jointly over the hash functions and the binary codes so that they\ngradually match each other. The resulting algorithm can be seen as a corrected,\niterated version of the procedure of optimizing first over the codes and then\nlearning the hash function. Compared to this, our optimization is guaranteed to\nobtain better hash functions while being not much slower, as demonstrated\nexperimentally in various supervised datasets. In addition, our framework\nfacilitates the design of optimization algorithms for arbitrary types of loss\nand hash functions.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "raziperchikolaei2016optimizing", "year": "2016", "title":"Optimizing Affinity-based Binary Hashing Using Auxiliary Coordinates", "abstract": "<p>In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.</p>\n", "tags": ["Image Retrieval","NEURIPS","Supervised"] },
{"key": "reddy2022context", "year": "2022", "title":"Context Unaware Knowledge Distillation For Image Retrieval", "abstract": "<p>Existing data-dependent hashing methods use large backbone networks with\nmillions of parameters and are computationally complex. Existing knowledge\ndistillation methods use logits and other features of the deep (teacher) model\nand as knowledge for the compact (student) model, which requires the teacher’s\nnetwork to be fine-tuned on the context in parallel with the student model on\nthe context. Training teacher on the target context requires more time and\ncomputational resources. In this paper, we propose context unaware knowledge\ndistillation that uses the knowledge of the teacher model without fine-tuning\nit on the target context. We also propose a new efficient student model\narchitecture for knowledge distillation. The proposed approach follows a\ntwo-step process. The first step involves pre-training the student model with\nthe help of context unaware knowledge distillation from the teacher model. The\nsecond step involves fine-tuning the student model on the context of image\nretrieval. In order to show the efficacy of the proposed approach, we compare\nthe retrieval results, no. of parameters and no. of operations of the student\nmodels with the teacher models under different retrieval frameworks, including\ndeep cauchy hashing (DCH) and central similarity quantization (CSQ). The\nexperimental results confirm that the proposed approach provides a promising\ntrade-off between the retrieval results and efficiency. The code used in this\npaper is released publicly at \\url{https://github.com/satoru2001/CUKDFIR}.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Quantisation"] },
{"key": "remil2023deeplsh", "year": "2023", "title":"Deeplsh Deep Locality-sensitive Hash Learning For Fast And Efficient Near-duplicate Crash Report Detection", "abstract": "<p>Automatic crash bucketing is a crucial phase in the software development\nprocess for efficiently triaging bug reports. It generally consists in grouping\nsimilar reports through clustering techniques. However, with real-time\nstreaming bug collection, systems are needed to quickly answer the question:\nWhat are the most similar bugs to a new one?, that is, efficiently find\nnear-duplicates. It is thus natural to consider nearest neighbors search to\ntackle this problem and especially the well-known locality-sensitive hashing\n(LSH) to deal with large datasets due to its sublinear performance and\ntheoretical guarantees on the similarity search accuracy. Surprisingly, LSH has\nnot been considered in the crash bucketing literature. It is indeed not trivial\nto derive hash functions that satisfy the so-called locality-sensitive property\nfor the most advanced crash bucketing metrics. Consequently, we study in this\npaper how to leverage LSH for this task. To be able to consider the most\nrelevant metrics used in the literature, we introduce DeepLSH, a Siamese DNN\narchitecture with an original loss function, that perfectly approximates the\nlocality-sensitivity property even for Jaccard and Cosine metrics for which\nexact LSH solutions exist. We support this claim with a series of experiments\non an original dataset, which we make available.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "ren2020hm", "year": "2020", "title":"HM-ANN Efficient Billion-point Nearest Neighbor Search On Heterogeneous Memory", "abstract": "<p>The state-of-the-art approximate nearest neighbor search (ANNS) algorithms face a fundamental tradeoff between query latency and accuracy, because of small main memory capacity: To store indices in main memory for short query latency, the ANNS algorithms have to limit dataset size or use a quantization scheme which hurts search accuracy. The emergence of heterogeneous memory (HM) brings a solution to significantly increase memory capacity and break the above tradeoff: Using HM, billions of data points can be placed in the main memory on a single machine without using any data compression. However, HM consists of both fast (but small) memory and slow (but large) memory, and using HM inappropriately slows down query significantly. \nIn this work, we present a novel graph-based similarity search algorithm called HM-ANN, which takes both memory and data heterogeneity into consideration and enables billion-scale similarity search on a single node without using compression. On two billion-sized datasets BIGANN and DEEP1B, HM-ANN outperforms state-of-the-art compression-based solutions such as L&amp;C and IMI+OPQ in recall-vs-latency by a large margin, obtaining 46% higher recall under the same search latency. We also extend existing graph-based methods such as HNSW and NSG with two strong baseline implementations on HM. At billion-point scale, HM-ANN is 2X and 5.8X faster than our HNSWand NSG baselines respectively to reach the same accuracy.</p>\n", "tags": ["Cross Modal","Graph","NEURIPS","Quantisation"] },
{"key": "riazi2016sub", "year": "2016", "title":"Sub-linear Privacy-preserving Near-neighbor Search", "abstract": "<p>In Near-Neighbor Search (NNS), a new client queries a database (held by a\nserver) for the most similar data (near-neighbors) given a certain similarity\nmetric. The Privacy-Preserving variant (PP-NNS) requires that neither server\nnor the client shall learn information about the other party’s data except what\ncan be inferred from the outcome of NNS. The overwhelming growth in the size of\ncurrent datasets and the lack of a truly secure server in the online world\nrender the existing solutions impractical; either due to their high\ncomputational requirements or non-realistic assumptions which potentially\ncompromise privacy. PP-NNS having query time {\\it sub-linear} in the size of\nthe database has been suggested as an open research direction by Li et al.\n(CCSW’15). In this paper, we provide the first such algorithm, called Secure\nLocality Sensitive Indexing (SLSI) which has a sub-linear query time and the\nability to handle honest-but-curious parties. At the heart of our proposal lies\na secure binary embedding scheme generated from a novel probabilistic\ntransformation over locality sensitive hashing family. We provide information\ntheoretic bound for the privacy guarantees and support our theoretical claims\nusing substantial empirical evidence on real-world datasets.</p>\n", "tags": ["ARXIV"] },
{"key": "robberechts2022elastic", "year": "2022", "title":"Elastic Product Quantization For Time Series", "abstract": "<p>Analyzing numerous or long time series is difficult in practice due to the\nhigh storage costs and computational requirements. Therefore, techniques have\nbeen proposed to generate compact similarity-preserving representations of time\nseries, enabling real-time similarity search on large in-memory data\ncollections. However, the existing techniques are not ideally suited for\nassessing similarity when sequences are locally out of phase. In this paper, we\npropose the use of product quantization for efficient similarity-based\ncomparison of time series under time warping. The idea is to first compress the\ndata by partitioning the time series into equal length sub-sequences which are\nrepresented by a short code. The distance between two time series can then be\nefficiently approximated by pre-computed elastic distances between their codes.\nThe partitioning into sub-sequences forces unwanted alignments, which we\naddress with a pre-alignment step using the maximal overlap discrete wavelet\ntransform (MODWT). To demonstrate the efficiency and accuracy of our method, we\nperform an extensive experimental evaluation on benchmark datasets in nearest\nneighbors classification and clustering applications. Overall, the proposed\nsolution emerges as a highly efficient (both in terms of memory usage and\ncomputation time) replacement for elastic measures in time series applications.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "roig2013random", "year": "2013", "title":"Random Binary Mappings For Kernel Learning And Efficient SVM", "abstract": "<p>Support Vector Machines (SVMs) are powerful learners that have led to\nstate-of-the-art results in various computer vision problems. SVMs suffer from\nvarious drawbacks in terms of selecting the right kernel, which depends on the\nimage descriptors, as well as computational and memory efficiency. This paper\nintroduces a novel kernel, which serves such issues well. The kernel is learned\nby exploiting a large amount of low-complex, randomized binary mappings of the\ninput feature. This leads to an efficient SVM, while also alleviating the task\nof kernel selection. We demonstrate the capabilities of our kernel on 6\nstandard vision benchmarks, in which we combine several common image\ndescriptors, namely histograms (Flowers17 and Daimler), attribute-like\ndescriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet).\nResults show that our kernel learning adapts well to the different descriptors\ntypes, achieving the performance of the kernels specifically tuned for each\nimage descriptor, and with similar evaluation cost as efficient SVM methods.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "roller2021hash", "year": "2021", "title":"Hash Layers For Large Sparse Models", "abstract": "<p>We investigate the training of sparse layers that use different parameters for different inputs based on hashing in large Transformer models. Specifically, we modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence. We show that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods such as Switch Transformers and BASE Layers, while requiring no routing parameters or extra terms in the objective function such as a load balancing loss, and no sophisticated assignment algorithm. We study the performance of different hashing techniques,  hash sizes and input features,  and  show that  balanced and random hashes focused on the most local features work best, compared to either learning clusters or using longer-range context. We show our approach works well both on large language modeling and dialogue tasks, and on downstream fine-tuning tasks.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "rong2024locality", "year": "2024", "title":"Locality-sensitive Hashing For Earthquake Detection A Case Study Of Scaling Data-driven Science", "abstract": "<p>In this work, we report on a novel application of Locality Sensitive\nHashing (LSH) to seismic data at scale. Based on the high waveform similarity between reoccurring earthquakes, our application\nidentifies potential earthquakes by searching for similar time series\nsegments via LSH. However, a straightforward implementation of\nthis LSH-enabled application has difficulty scaling beyond 3 months\nof continuous time series data measured at a single seismic station.\nAs a case study of a data-driven science workflow, we illustrate how\ndomain knowledge can be incorporated into the workload to improve\nboth the efficiency and result quality. We describe several end-toend optimizations of the analysis pipeline from pre-processing to\npost-processing, which allow the application to scale to time series data measured at multiple seismic stations. Our optimizations\nenable an over 100× speedup in the end-to-end analysis pipeline.\nThis improved scalability enabled seismologists to perform seismic\nanalysis on more than ten years of continuous time series data from\nover ten seismic stations, and has directly enabled the discovery of\n597 new earthquakes near the Diablo Canyon nuclear power plant\nin California and 6123 new earthquakes in New Zealand.</p>\n", "tags": ["ARXIV","Case Study","Independent","LSH"] },
{"key": "rossi2024relevance", "year": "2024", "title":"Relevance Filtering For Embedding-based Retrieval", "abstract": "<p>In embedding-based retrieval, Approximate Nearest Neighbor (ANN) search\nenables efficient retrieval of similar items from large-scale datasets. While\nmaximizing recall of relevant items is usually the goal of retrieval systems, a\nlow precision may lead to a poor search experience. Unlike lexical retrieval,\nwhich inherently limits the size of the retrieved set through keyword matching,\ndense retrieval via ANN search has no natural cutoff. Moreover, the cosine\nsimilarity scores of embedding vectors are often optimized via contrastive or\nranking losses, which make them difficult to interpret. Consequently, relying\non top-K or cosine-similarity cutoff is often insufficient to filter out\nirrelevant results effectively. This issue is prominent in product search,\nwhere the number of relevant products is often small. This paper introduces a\nnovel relevance filtering component (called “Cosine Adapter”) for\nembedding-based retrieval to address this challenge. Our approach maps raw\ncosine similarity scores to interpretable scores using a query-dependent\nmapping function. We then apply a global threshold on the mapped scores to\nfilter out irrelevant results. We are able to significantly increase the\nprecision of the retrieved set, at the expense of a small loss of recall. The\neffectiveness of our approach is demonstrated through experiments on both\npublic MS MARCO dataset and internal Walmart product search data. Furthermore,\nonline A/B testing on the Walmart site validates the practical value of our\napproach in real-world e-commerce settings.</p>\n", "tags": ["ARXIV"] },
{"key": "rouayheb2008bounds", "year": "2008", "title":"Bounds On Codes Based On Graph Theory", "abstract": "<p>Let \\(A_q(n,d)\\) be the maximum order (maximum number of codewords) of a\n\\(q\\)-ary code of length \\(n\\) and Hamming distance at least \\(d\\). And let\n\\(A(n,d,w)\\) that of a binary code of constant weight \\(w\\). Building on results\nfrom algebraic graph theory and Erd\\H{o}s-ko-Rado like theorems in extremal\ncombinatorics, we show how several known bounds on \\(A_q(n,d)\\) and \\(A(n,d,w)\\)\ncan be easily obtained in a single framework. For instance, both the Hamming\nand Singleton bounds can derived as an application of a property relating the\nclique number and the independence number of vertex transitive graphs. Using\nthe same techniques, we also derive some new bounds and present some additional\napplications.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "roy2019metric", "year": "2019", "title":"Metric-learning Based Deep Hashing Network For Content Based Retrieval Of Remote Sensing Images", "abstract": "<p>Hashing methods have been recently found very effective in retrieval of\nremote sensing (RS) images due to their computational efficiency and fast\nsearch speed. The traditional hashing methods in RS usually exploit\nhand-crafted features to learn hash functions to obtain binary codes, which can\nbe insufficient to optimally represent the information content of RS images. To\novercome this problem, in this paper we introduce a metric-learning based\nhashing network, which learns: 1) a semantic-based metric space for effective\nfeature representation; and 2) compact binary hash codes for fast archive\nsearch. Our network considers an interplay of multiple loss functions that\nallows to jointly learn a metric based semantic space facilitating similar\nimages to be clustered together in that target space and at the same time\nproducing compact final activations that lose negligible information when\nbinarized. Experiments carried out on two benchmark RS archives point out that\nthe proposed network significantly improves the retrieval performance under the\nsame retrieval time when compared to the state-of-the-art hashing methods in\nRS.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "roy2020zscrgan", "year": "2020", "title":"ZSCRGAN A Gan-based Expectation Maximization Model For Zero-shot Retrieval Of Images From Textual Descriptions", "abstract": "<p>Most existing algorithms for cross-modal Information Retrieval are based on a\nsupervised train-test setup, where a model learns to align the mode of the\nquery (e.g., text) to the mode of the documents (e.g., images) from a given\ntraining set. Such a setup assumes that the training set contains an exhaustive\nrepresentation of all possible classes of queries. In reality, a retrieval\nmodel may need to be deployed on previously unseen classes, which implies a\nzero-shot IR setup. In this paper, we propose a novel GAN-based model for\nzero-shot text to image retrieval. When given a textual description as the\nquery, our model can retrieve relevant images in a zero-shot setup. The\nproposed model is trained using an Expectation-Maximization framework.\nExperiments on multiple benchmark datasets show that our proposed model\ncomfortably outperforms several state-of-the-art zero-shot text to image\nretrieval models, as well as zero-shot classification and hashing models\nsuitably used for retrieval.</p>\n", "tags": ["ARXIV","Cross Modal","GAN","Image Retrieval","Supervised"] },
{"key": "rubinstein2018hardness", "year": "2018", "title":"Hardness Of Approximate Nearest Neighbor Search", "abstract": "<p>We prove conditional near-quadratic running time lower bounds for approximate\nBichromatic Closest Pair with Euclidean, Manhattan, Hamming, or edit distance.\nSpecifically, unless the Strong Exponential Time Hypothesis (SETH) is false,\nfor every \\(\\delta&gt;0\\) there exists a constant \\(\\epsilon&gt;0\\) such that computing a\n\\((1+\\epsilon)\\)-approximation to the Bichromatic Closest Pair requires\n\\(n^{2-\\delta}\\) time. In particular, this implies a near-linear query time for\nApproximate Nearest Neighbor search with polynomial preprocessing time.\n  Our reduction uses the Distributed PCP framework of [ARW’17], but obtains\nimproved efficiency using Algebraic Geometry (AG) codes. Efficient PCPs from AG\ncodes have been constructed in other settings before [BKKMS’16, BCGRS’17], but\nour construction is the first to yield new hardness results.</p>\n", "tags": ["ARXIV"] },
{"key": "russell2024labelme", "year": "2024", "title":"Labelme A Database And Web-based Tool For Image Annotation", "abstract": "<p>We seek to build a large collection of images with ground truth labels to be used for object\ndetection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation\nand instant sharing of such annotations. Using this annotation tool, we have collected a large\ndataset that spans many object categories, often containing multiple instances over a wide variety\nof images. We quantify the contents of the dataset and compare against existing state of the\nart datasets used for object recognition and detection. Also, we show how to extend the dataset\nto automatically enhance object labels with WordNet, discover object parts, recover a depth ordering\nof objects in a scene, and increase the number of labels using minimal user supervision\nand images from the web.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "ryali2020bio", "year": "2020", "title":"Bio-inspired Hashing For Unsupervised Similarity Search", "abstract": "<p>The fruit fly Drosophila’s olfactory circuit has inspired a new locality\nsensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH\nalgorithms that produce low dimensional hash codes, FlyHash produces sparse\nhigh-dimensional hash codes and has also been shown to have superior empirical\nperformance compared to classical LSH algorithms in similarity search. However,\nFlyHash uses random projections and cannot learn from data. Building on\ninspiration from FlyHash and the ubiquity of sparse expansive representations\nin neurobiology, our work proposes a novel hashing algorithm BioHash that\nproduces sparse high dimensional hash codes in a data-driven manner. We show\nthat BioHash outperforms previously published benchmarks for various hashing\nmethods. Since our learning algorithm is based on a local and biologically\nplausible synaptic plasticity rule, our work provides evidence for the proposal\nthat LSH might be a computational reason for the abundance of sparse expansive\nmotifs in a variety of biological systems. We also propose a convolutional\nvariant BioConvHash that further improves performance. From the perspective of\ncomputer science, BioHash and BioConvHash are fast, scalable and yield\ncompressed binary representations that are useful for similarity search.</p>\n", "tags": ["ICML","LSH","Unsupervised"] },
{"key": "ryali2024bio", "year": "2024", "title":"Bio-inspired Hashing For Unsupervised Similarity Search", "abstract": "<p>The fruit fly Drosophila’s olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes, FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However, FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology, our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule, our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science, BioHash and BioConvHash are fast, scalable and yield compressed binary representations that are useful for similarity search.</p>\n", "tags": ["ARXIV","LSH","Unsupervised"] },
{"key": "rychalska2020i", "year": "2020", "title":"I Know Why You Like This Movie Interpretable Efficient Multimodal Recommender", "abstract": "<p>Recently, the Efficient Manifold Density Estimator (EMDE) model has been\nintroduced. The model exploits Local Sensitive Hashing and Count-Min Sketch\nalgorithms, combining them with a neural network to achieve state-of-the-art\nresults on multiple recommender datasets. However, this model ingests a\ncompressed joint representation of all input items for each user/session, so\ncalculating attributions for separate items via gradient-based methods seems\nnot applicable. We prove that interpreting this model in a white-box setting is\npossible thanks to the properties of EMDE item retrieval method. By exploiting\nmultimodal flexibility of this model, we obtain meaningful results showing the\ninfluence of multiple modalities: text, categorical features, and images, on\nmovie recommendation output.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "röder2024deep", "year": "2024", "title":"Deep Transfer Hashing For Adaptive Learning On Federated Streaming Data", "abstract": "<p>This extended abstract explores the integration of federated learning with\ndeep transfer hashing for distributed prediction tasks, emphasizing\nresource-efficient client training from evolving data streams. Federated\nlearning allows multiple clients to collaboratively train a shared model while\nmaintaining data privacy - by incorporating deep transfer hashing,\nhigh-dimensional data can be converted into compact hash codes, reducing data\ntransmission size and network loads. The proposed framework utilizes transfer\nlearning, pre-training deep neural networks on a central server, and\nfine-tuning on clients to enhance model accuracy and adaptability. A selective\nhash code sharing mechanism using a privacy-preserving global memory bank\nfurther supports client fine-tuning. This approach addresses challenges in\nprevious research by improving computational efficiency and scalability.\nPractical applications include Car2X event predictions, where a shared model is\ncollectively trained to recognize traffic patterns, aiding in tasks such as\ntraffic density assessment and accident detection. The research aims to develop\na robust framework that combines federated learning, deep transfer hashing and\ntransfer learning for efficient and secure downstream task execution.</p>\n", "tags": ["ARXIV","Streaming Data","Supervised"] },
{"key": "saberian2016large", "year": "2016", "title":"Large Margin Discriminant Dimensionality Reduction In Prediction Space", "abstract": "<p>In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through combination of weak learners. We argue that the intermediate mapping, e.g. boosting predictor, is preserving the discriminant aspects of the data and by controlling the dimension of this mapping it is possible to achieve discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.</p>\n", "tags": ["NEURIPS","Unsupervised"] },
{"key": "sablayrolles2016how", "year": "2016", "title":"How Should We Evaluate Supervised Hashing", "abstract": "<p>Hashing produces compact representations for documents, to perform tasks like\nclassification or retrieval based on these short codes. When hashing is\nsupervised, the codes are trained using labels on the training data. This paper\nfirst shows that the evaluation protocols used in the literature for supervised\nhashing are not satisfactory: we show that a trivial solution that encodes the\noutput of a classifier significantly outperforms existing supervised or\nsemi-supervised methods, while using much shorter codes. We then propose two\nalternative protocols for supervised hashing: one based on retrieval on a\ndisjoint set of classes, and another based on transfer learning to new classes.\nWe provide two baseline methods for image-related tasks to assess the\nperformance of (semi-)supervised hashing: without coding and with unsupervised\ncodes. These baselines give a lower- and upper-bound on the performance of a\nsupervised hashing scheme.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "sackman2015perfect", "year": "2015", "title":"Perfect Consistent Hashing", "abstract": "<p>Consistent Hashing functions are widely used for load balancing across a\nvariety of applications. However, the original presentation and typical\nimplementations of Consistent Hashing rely on randomised allocation of hash\ncodes to keys which results in a flawed and approximately-uniform allocation of\nkeys to hash codes. We analyse the desired properties and present an algorithm\nthat perfectly achieves them without resorting to any random distributions. The\nalgorithm is simple and adds to our understanding of what is necessary to\ncreate a consistent hash function.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "saha2015nearest", "year": "2015", "title":"Nearest Neighbor Search In Complex Network For Community Detection", "abstract": "<p>Nearest neighbor search is a basic computational tool used extensively in\nalmost research domains of computer science specially when dealing with large\namount of data. However, the use of nearest neighbor search is restricted for\nthe purpose of algorithmic development by the existence of the notion of\nnearness among the data points. The recent trend of research is on large,\ncomplex networks and their structural analysis, where nodes represent entities\nand edges represent any kind of relation between entities. Community detection\nin complex network is an important problem of much interest. In general, a\ncommunity detection algorithm represents an objective function and captures the\ncommunities by optimizing it to extract the interesting communities for the\nuser. In this article, we have studied the nearest neighbor search problem in\ncomplex network via the development of a suitable notion of nearness.\nInitially, we have studied and analyzed the exact nearest neighbor search using\nmetric tree on proposed metric space constructed from complex network. After,\nthe approximate nearest neighbor search problem is studied using locality\nsensitive hashing. For evaluation of the proposed nearest neighbor search on\ncomplex network we applied it in community detection problem. The results\nobtained using our methods are very competitive with most of the well known\nalgorithms exists in the literature and this is verified on collection of real\nnetworks. On the other-hand, it can be observed that time taken by our\nalgorithm is quite less compared to popular methods.</p>\n", "tags": ["ARXIV"] },
{"key": "salakhutdinov2024semantic", "year": "2024", "title":"Semantic Hashing", "abstract": "<p>We show how to learn a deep graphical model of the word-count\nvectors obtained from a large set of documents. The values of the\nlatent variables in the deepest layer are easy to infer and give a\nmuch better representation of each document than Latent Semantic\nAnalysis. When the deepest layer is forced to use a small number of\nbinary variables (e.g. 32), the graphical model performs “semantic\nhashing”: Documents are mapped to memory addresses in such a\nway that semantically similar documents are located at nearby addresses.\nDocuments similar to a query document can then be found\nby simply accessing all the addresses that differ by only a few bits\nfrom the address of the query document. This way of extending the\nefficiency of hash-coding to approximate matching is much faster\nthan locality sensitive hashing, which is the fastest current method.\nBy using semantic hashing to filter the documents given to TF-IDF,\nwe achieve higher accuracy than applying TF-IDF to the entire document\nset.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "salman2024robustness", "year": "2024", "title":"On The Robustness Of Malware Detectors To Adversarial Samples", "abstract": "<p>Adversarial examples add imperceptible alterations to inputs with the\nobjective to induce misclassification in machine learning models. They have\nbeen demonstrated to pose significant challenges in domains like image\nclassification, with results showing that an adversarially perturbed image to\nevade detection against one classifier is most likely transferable to other\nclassifiers. Adversarial examples have also been studied in malware analysis.\nUnlike images, program binaries cannot be arbitrarily perturbed without\nrendering them non-functional. Due to the difficulty of crafting adversarial\nprogram binaries, there is no consensus on the transferability of adversarially\nperturbed programs to different detectors. In this work, we explore the\nrobustness of malware detectors against adversarially perturbed malware. We\ninvestigate the transferability of adversarial attacks developed against one\ndetector, against other machine learning-based malware detectors, and code\nsimilarity techniques, specifically, locality sensitive hashing-based\ndetectors. Our analysis reveals that adversarial program binaries crafted for\none detector are generally less effective against others. We also evaluate an\nensemble of detectors and show that they can potentially mitigate the impact of\nadversarial program binaries. Finally, we demonstrate that substantial program\nchanges made to evade detection may result in the transformation technique\nbeing identified, implying that the adversary must make minimal changes to the\nprogram binary.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "salvi2016bloom", "year": "2016", "title":"Bloom Filters And Compact Hash Codes For Efficient And Distributed Image Retrieval", "abstract": "<p>This paper presents a novel method for efficient image retrieval, based on a\nsimple and effective hashing of CNN features and the use of an indexing\nstructure based on Bloom filters. These filters are used as gatekeepers for the\ndatabase of image features, allowing to avoid to perform a query if the query\nfeatures are not stored in the database and speeding up the query process,\nwithout affecting retrieval performance. Thanks to the limited memory\nrequirements the system is suitable for mobile applications and distributed\ndatabases, associating each filter to a distributed portion of the database.\nExperimental validation has been performed on three standard image retrieval\ndatasets, outperforming state-of-the-art hashing methods in terms of precision,\nwhile the proposed indexing method obtains a \\(2\\times\\) speedup.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Independent"] },
{"key": "sankar2019text", "year": "2019", "title":"On-device Text Representations Robust To Misspellings Via Projections", "abstract": "<p>Recently, there has been a strong interest in developing natural language\napplications that live on personal devices such as mobile phones, watches and\nIoT with the objective to preserve user privacy and have low memory. Advances\nin Locality-Sensitive Hashing (LSH)-based projection networks have demonstrated\nstate-of-the-art performance in various classification tasks without explicit\nword (or word-piece) embedding lookup tables by computing on-the-fly text\nrepresentations. In this paper, we show that the projection based neural\nclassifiers are inherently robust to misspellings and perturbations of the\ninput text. We empirically demonstrate that the LSH projection based\nclassifiers are more robust to common misspellings compared to BiLSTMs (with\nboth word-piece &amp; word-only tokenization) and fine-tuned BERT based methods.\nWhen subject to misspelling attacks, LSH projection based classifiers had a\nsmall average accuracy drop of 2.94% across multiple classifications tasks,\nwhile the fine-tuned BERT model accuracy had a significant drop of 11.44%.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "sankar2019transferable", "year": "2019", "title":"Transferable Neural Projection Representations", "abstract": "<p>Neural word representations are at the core of many state-of-the-art natural\nlanguage processing models. A widely used approach is to pre-train, store and\nlook up word or character embedding matrices. While useful, such\nrepresentations occupy huge memory making it hard to deploy on-device and often\ndo not generalize to unknown words due to vocabulary pruning.\n  In this paper, we propose a skip-gram based architecture coupled with\nLocality-Sensitive Hashing (LSH) projections to learn efficient dynamically\ncomputable representations. Our model does not need to store lookup tables as\nrepresentations are computed on-the-fly and require low memory footprint. The\nrepresentations can be trained in an unsupervised fashion and can be easily\ntransferred to other NLP tasks. For qualitative evaluation, we analyze the\nnearest neighbors of the word representations and discover semantically similar\nwords even with misspellings. For quantitative evaluation, we plug our\ntransferable projections into a simple LSTM and run it on multiple NLP tasks\nand show how our transferable projections achieve better performance compared\nto prior work.</p>\n", "tags": ["LSH","Unsupervised"] },
{"key": "sankaranarayanan2016triplet", "year": "2016", "title":"Triplet Similarity Embedding For Face Verification", "abstract": "<p>In this work, we present an unconstrained face verification algorithm and\nevaluate it on the recently released IJB-A dataset that aims to push the\nboundaries of face verification methods. The proposed algorithm couples a deep\nCNN-based approach with a low-dimensional discriminative embedding learnt using\ntriplet similarity constraints in a large margin fashion. Aside from yielding\nperformance improvement, this embedding provides significant advantages in\nterms of memory and post-processing operations like hashing and visualization.\nExperiments on the IJB-A dataset show that the proposed algorithm outperforms\nstate of the art methods in verification and identification metrics, while\nrequiring less training time.</p>\n", "tags": ["ARXIV","CNN"] },
{"key": "sarmento2018incremental", "year": "2018", "title":"Incremental Sparse TFIDF Incremental Similarity With Bipartite Graphs", "abstract": "<p>In this report, we experimented with several concepts regarding text streams\nanalysis.\n  We tested an implementation of Incremental Sparse TF-IDF (IS-TFIDF) and\nIncremental Cosine Similarity (ICS) with the use of bipartite graphs.\n  We are using bipartite graphs - one type of node are documents, and the other\ntype of nodes are words - to know what documents are affected with a word\narrival at the stream (the neighbors of the word in the graph). Thus, with this\ninformation, we leverage optimized algorithms used for graph-based\napplications. The concept is similar to, for example, the use of hash tables or\nother computer science concepts used for fast access to information in memory.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "satuluri2011bayesian", "year": "2011", "title":"Bayesian Locality Sensitive Hashing For Fast Similarity Search", "abstract": "<p>Given a collection of objects and an associated similarity measure, the\nall-pairs similarity search problem asks us to find all pairs of objects with\nsimilarity greater than a certain user-specified threshold. Locality-sensitive\nhashing (LSH) based methods have become a very popular approach for this\nproblem. However, most such methods only use LSH for the first phase of\nsimilarity search - i.e. efficient indexing for candidate generation. In this\npaper, we present BayesLSH, a principled Bayesian algorithm for the subsequent\nphase of similarity search - performing candidate pruning and similarity\nestimation using LSH. A simpler variant, BayesLSH-Lite, which calculates\nsimilarities exactly, is also presented. BayesLSH is able to quickly prune away\na large majority of the false positive candidate pairs, leading to significant\nspeedups over baseline approaches. For BayesLSH, we also provide probabilistic\nguarantees on the quality of the output, both in terms of accuracy and recall.\nFinally, the quality of BayesLSH’s output can be easily tuned and does not\nrequire any manual setting of the number of hashes to use for similarity\nestimation, unlike standard approaches. For two state-of-the-art candidate\ngeneration algorithms, AllPairs and LSH, BayesLSH enables significant speedups,\ntypically in the range 2x-20x for a wide variety of datasets.</p>\n", "tags": ["Independent","LSH"] },
{"key": "schall2019deep", "year": "2019", "title":"Deep Metric Learning Using Similarities From Nonlinear Rank Approximations", "abstract": "<p>In recent years, deep metric learning has achieved promising results in\nlearning high dimensional semantic feature embeddings where the spatial\nrelationships of the feature vectors match the visual similarities of the\nimages. Similarity search for images is performed by determining the vectors\nwith the smallest distances to a query vector. However, high retrieval quality\ndoes not depend on the actual distances of the feature vectors, but rather on\nthe ranking order of the feature vectors from similar images. In this paper, we\nintroduce a metric learning algorithm that focuses on identifying and modifying\nthose feature vectors that most strongly affect the retrieval quality. We\ncompute normalized approximated ranks and convert them to similarities by\napplying a nonlinear transfer function. These similarities are used in a newly\nproposed loss function that better contracts similar and disperses dissimilar\nsamples. Experiments demonstrate significant improvement over existing deep\nfeature embedding methods on the CUB-200-2011, Cars196, and Stanford Online\nProducts data sets for all embedding sizes.</p>\n", "tags": ["ARXIV"] },
{"key": "schall2024optimizing", "year": "2024", "title":"Optimizing CLIP Models For Image Retrieval With Maintained Joint-embedding Alignment", "abstract": "<p>Contrastive Language and Image Pairing (CLIP), a transformative method in\nmultimedia retrieval, typically trains two neural networks concurrently to\ngenerate joint embeddings for text and image pairs. However, when applied\ndirectly, these models often struggle to differentiate between visually\ndistinct images that have similar captions, resulting in suboptimal performance\nfor image-based similarity searches. This paper addresses the challenge of\noptimizing CLIP models for various image-based similarity search scenarios,\nwhile maintaining their effectiveness in text-based search tasks such as\ntext-to-image retrieval and zero-shot classification. We propose and evaluate\ntwo novel methods aimed at refining the retrieval capabilities of CLIP without\ncompromising the alignment between text and image embeddings. The first method\ninvolves a sequential fine-tuning process: initially optimizing the image\nencoder for more precise image retrieval and subsequently realigning the text\nencoder to these optimized image embeddings. The second approach integrates\npseudo-captions during the retrieval-optimization phase to foster direct\nalignment within the embedding space. Through comprehensive experiments, we\ndemonstrate that these methods enhance CLIP’s performance on various\nbenchmarks, including image retrieval, k-NN classification, and zero-shot\ntext-based classification, while maintaining robustness in text-to-image\nretrieval. Our optimized models permit maintaining a single embedding per\nimage, significantly simplifying the infrastructure needed for large-scale\nmulti-modal similarity search systems.</p>\n", "tags": ["ARXIV","Cross Modal","Image Retrieval","Supervised"] },
{"key": "schiavo2021sketches", "year": "2021", "title":"Sketches Image Analysis Web Image Search Engine Usinglsh Index And DNN Inceptionv3", "abstract": "<p>The adoption of an appropriate approximate similarity search method is an\nessential prereq-uisite for developing a fast and efficient CBIR system,\nespecially when dealing with large amount ofdata. In this study we implement a\nweb image search engine on top of a Locality Sensitive Hashing(LSH) Index to\nallow fast similarity search on deep features. Specifically, we exploit\ntransfer learningfor deep features extraction from images. Firstly, we adopt\nInceptionV3 pretrained on ImageNet asfeatures extractor, secondly, we try out\nseveral CNNs built on top of InceptionV3 as convolutionalbase fine-tuned on our\ndataset. In both of the previous cases we index the features extracted within\nourLSH index implementation so as to compare the retrieval performances with\nand without fine-tuning.In our approach we try out two different LSH\nimplementations: the first one working with real numberfeature vectors and the\nsecond one with the binary transposed version of those vectors.\nInterestingly,we obtain the best performances when using the binary LSH,\nreaching almost the same result, in termsof mean average precision, obtained by\nperforming sequential scan of the features, thus avoiding thebias introduced by\nthe LSH index. Lastly, we carry out a performance analysis class by class in\nterms ofrecall againstmAPhighlighting, as expected, a strong positive\ncorrelation between the two.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "schlemper2019deep", "year": "2019", "title":"Deep Hashing Using Entropy Regularised Product Quantisation Network", "abstract": "<p>In large scale systems, approximate nearest neighbour search is a crucial\nalgorithm to enable efficient data retrievals. Recently, deep learning-based\nhashing algorithms have been proposed as a promising paradigm to enable data\ndependent schemes. Often their efficacy is only demonstrated on data sets with\nfixed, limited numbers of classes. In practical scenarios, those labels are not\nalways available or one requires a method that can handle a higher input\nvariability, as well as a higher granularity. To fulfil those requirements, we\nlook at more flexible similarity measures. In this work, we present a novel,\nflexible, end-to-end trainable network for large-scale data hashing. Our method\nworks by transforming the data distribution to behave as a uniform distribution\non a product of spheres. The transformed data is subsequently hashed to a\nbinary form in a way that maximises entropy of the output, (i.e. to fully\nutilise the available bit-rate capacity) while maintaining the correctness\n(i.e. close items hash to the same key in the map). We show that the method\noutperforms baseline approaches such as locality-sensitive hashing and product\nquantisation in the limited capacity regime.</p>\n", "tags": ["ARXIV","Deep Learning","Independent","Quantisation"] },
{"key": "schubert2021triangle", "year": "2021", "title":"A Triangle Inequality For Cosine Similarity", "abstract": "<p>Similarity search is a fundamental problem for many data analysis techniques.\nMany efficient search techniques rely on the triangle inequality of metrics,\nwhich allows pruning parts of the search space based on transitive bounds on\ndistances. Recently, Cosine similarity has become a popular alternative choice\nto the standard Euclidean metric, in particular in the context of textual data\nand neural network embeddings. Unfortunately, Cosine similarity is not metric\nand does not satisfy the standard triangle inequality. Instead, many search\ntechniques for Cosine rely on approximation techniques such as locality\nsensitive hashing. In this paper, we derive a triangle inequality for Cosine\nsimilarity that is suitable for efficient similarity search with many standard\nsearch structures (such as the VP-tree, Cover-tree, and M-tree); show that this\nbound is tight and discuss fast approximations for it. We hope that this spurs\nnew research on accelerating exact similarity search for cosine similarity, and\npossible other similarity measures beyond the existing work for distance\nmetrics.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "schuhmann2021laion", "year": "2021", "title":"LAION-400M Open Dataset Of Clip-filtered 400 Million Image-text Pairs", "abstract": "<p>Multi-modal language-vision models trained on hundreds of millions of\nimage-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable\ncapability to perform zero- or few-shot learning and transfer even in absence\nof per-sample labels on target image data. Despite this trend, to date there\nhas been no publicly available datasets of sufficient scale for training such\nmodels from scratch. To address this issue, in a community effort we build and\nrelease for public LAION-400M, a dataset with CLIP-filtered 400 million\nimage-text pairs, their CLIP embeddings and kNN indices that allow efficient\nsimilarity search.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "schwengber2023deep", "year": "2023", "title":"Deep Hashing Via Householder Quantization", "abstract": "<p>Hashing is at the heart of large-scale image similarity search, and recent\nmethods have been substantially improved through deep learning techniques. Such\nalgorithms typically learn continuous embeddings of the data. To avoid a\nsubsequent costly binarization step, a common solution is to employ loss\nfunctions that combine a similarity learning term (to ensure similar images are\ngrouped to nearby embeddings) and a quantization penalty term (to ensure that\nthe embedding entries are close to binarized entries, e.g., -1 or 1). Still,\nthe interaction between these two terms can make learning harder and the\nembeddings worse. We propose an alternative quantization strategy that\ndecomposes the learning problem in two stages: first, perform similarity\nlearning over the embedding space with no quantization; second, find an optimal\northogonal transformation of the embeddings so each coordinate of the embedding\nis close to its sign, and then quantize the transformed embedding through the\nsign function. In the second step, we parametrize orthogonal transformations\nusing Householder matrices to efficiently leverage stochastic gradient descent.\nSince similarity measures are usually invariant under orthogonal\ntransformations, this quantization strategy comes at no cost in terms of\nperformance. The resulting algorithm is unsupervised, fast, hyperparameter-free\nand can be run on top of any existing deep hashing or metric learning\nalgorithm. We provide extensive experimental results showing that this approach\nleads to state-of-the-art performance on widely used image datasets, and,\nunlike other quantization strategies, brings consistent improvements in\nperformance to existing deep hashing algorithms.</p>\n", "tags": ["ARXIV","Deep Learning","Quantisation","Unsupervised"] },
{"key": "seker2014novel", "year": "2014", "title":"A Novel String Distance Function Based On Most Frequent K Characters", "abstract": "<p>This study aims to publish a novel similarity metric to increase the speed of\ncomparison operations. Also the new metric is suitable for distance-based\noperations among strings. Most of the simple calculation methods, such as\nstring length are fast to calculate but does not represent the string\ncorrectly. On the other hand the methods like keeping the histogram over all\ncharacters in the string are slower but good to represent the string\ncharacteristics in some areas, like natural language. We propose a new metric,\neasy to calculate and satisfactory for string comparison. Method is built on a\nhash function, which gets a string at any size and outputs the most frequent K\ncharacters with their frequencies. The outputs are open for comparison and our\nstudies showed that the success rate is quite satisfactory for the text mining\noperations.</p>\n", "tags": ["ICML","Independent"] },
{"key": "seleznev2022double", "year": "2022", "title":"Double-hashing Algorithm For Frequency Estimation In Data Streams", "abstract": "<p>Frequency estimation of elements is an important task for summarizing data\nstreams and machine learning applications. The problem is often addressed by\nusing streaming algorithms with sublinear space data structures. These\nalgorithms allow processing of large data while using limited data storage.\nCommonly used streaming algorithms, such as count-min sketch, have many\nadvantages, but do not take into account properties of a data stream for\nperformance optimization. In the present paper we introduce a novel\ndouble-hashing algorithm that provides flexibility to optimize streaming\nalgorithms depending on the properties of a given stream. In the double-hashing\napproach, first a standard streaming algorithm is employed to obtain an\nestimate of the element frequencies. This estimate is derived using a fraction\nof the stream and allows identification of the heavy hitters. Next, it uses a\nmodified hash table where the heavy hitters are mapped into individual buckets\nand other stream elements are mapped into the remaining buckets. Finally, the\nelement frequencies are estimated based on the constructed hash table over the\nentire data stream with any streaming algorithm. We demonstrate on both\nsynthetic data and an internet query log dataset that our approach is capable\nof improving frequency estimation due to removing heavy hitters from the\nhashing process and, thus, reducing collisions in the hash table. Our approach\navoids employing additional machine learning models to identify heavy hitters\nand, thus, reduces algorithm complexity and streamlines implementation.\nMoreover, because it is not dependent on specific features of the stream\nelements for identifying heavy hitters, it is applicable to a large variety of\nstreams. In addition, we propose a procedure on how to dynamically adjust the\nproposed double-hashing algorithm when frequencies of the elements in a stream\nare changing over time.</p>\n", "tags": ["ARXIV"] },
{"key": "senter2019unaligned", "year": "2019", "title":"Unaligned Sequence Similarity Search Using Deep Learning", "abstract": "<p>Gene annotation has traditionally required direct comparison of DNA sequences\nbetween an unknown gene and a database of known ones using string comparison\nmethods. However, these methods do not provide useful information when a gene\ndoes not have a close match in the database. In addition, each comparison can\nbe costly when the database is large since it requires alignments and a series\nof string comparisons. In this work we propose a novel approach: using\nrecurrent neural networks to embed DNA or amino-acid sequences in a\nlow-dimensional space in which distances correlate with functional similarity.\nThis embedding space overcomes both shortcomings of the method of aligning\nsequences and comparing homology. First, it allows us to obtain information\nabout genes which do not have exact matches by measuring their similarity to\nother ones in the database. If our database is labeled this can provide labels\nfor a query gene as is done in traditional methods. However, even if the\ndatabase is unlabeled it allows us to find clusters and infer some\ncharacteristics of the gene population. In addition, each comparison is much\nfaster than traditional methods since the distance metric is reduced to the\nEuclidean distance, and thus efficient approximate nearest neighbor algorithms\ncan be used to find the best match. We present results showing the advantage of\nour algorithm. More specifically we show how our embedding can be useful for\nboth classification tasks when our labels are known, and clustering tasks where\nour sequences belong to classes which have not been seen before.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "seraj2013functions", "year": "2013", "title":"Functions With Diffusive Properties", "abstract": "<p>While exploring desirable properties of hash functions in cryptography, the\nauthor was led to investigate three notions of functions with scattering or\n“diffusive” properties, where the functions map between binary strings of fixed\nfinite length. These notions of diffusion ask for some property to be fulfilled\nby the Hamming distances between outputs corresponding to pairs of inputs that\nlie on the endpoints of edges of an \\(n\\)-dimensional hypercube. Given the\ndimension of the input space, we explicitly construct such functions for every\ndimension of the output space that allows for the functions to exist.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "serre2017characterizing", "year": "2017", "title":"Characterizing And Enumerating Walsh-hadamard Transform Algorithms", "abstract": "<p>We propose a way of characterizing the algorithms computing a Walsh-Hadamard\ntransform that consist of a sequence of arrays of butterflies\n(\\(I_{2^{n-1}}\\otimes \\text{DFT}_2\\)) interleaved by linear permutations. Linear\npermutations are those that map linearly the binary representation of its\nelement indices. We also propose a method to enumerate these algorithms.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "shahreza2022mlp", "year": "2022", "title":"Mlp-hash Protecting Face Templates Via Hashing Of Randomized Multi-layer Perceptron", "abstract": "<p>Applications of face recognition systems for authentication purposes are\ngrowing rapidly. Although state-of-the-art (SOTA) face recognition systems have\nhigh recognition accuracy, the features which are extracted for each user and\nare stored in the system’s database contain privacy-sensitive information.\nAccordingly, compromising this data would jeopardize users’ privacy. In this\npaper, we propose a new cancelable template protection method, dubbed MLP-hash,\nwhich generates protected templates by passing the extracted features through a\nuser-specific randomly-weighted multi-layer perceptron (MLP) and binarizing the\nMLP output. We evaluated the unlinkability, irreversibility, and recognition\naccuracy of our proposed biometric template protection method to fulfill the\nISO/IEC 30136 standard requirements. Our experiments with SOTA face recognition\nsystems on the MOBIO and LFW datasets show that our method has competitive\nperformance with the BioHashing and IoM Hashing (IoM-GRP and IoM-URP) template\nprotection algorithms. We provide an open-source implementation of all the\nexperiments presented in this paper so that other researchers can verify our\nfindings and build upon our work.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "shand2020locality", "year": "2020", "title":"Locality-sensitive Hashing In Function Spaces", "abstract": "<p>We discuss the problem of performing similarity search over function spaces.\nTo perform search over such spaces in a reasonable amount of time, we use {\\it\nlocality-sensitive hashing} (LSH). We present two methods that allow LSH\nfunctions on \\(\\mathbb{R}^N\\) to be extended to \\(L^p\\) spaces: one using function\napproximation in an orthonormal basis, and another using (quasi-)Monte\nCarlo-style techniques. We use the presented hashing schemes to construct an\nLSH family for Wasserstein distance over one-dimensional, continuous\nprobability distributions.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "shanehsazzadeh2020fixed", "year": "2020", "title":"Fixed-length Protein Embeddings Using Contextual Lenses", "abstract": "<p>The Basic Local Alignment Search Tool (BLAST) is currently the most popular\nmethod for searching databases of biological sequences. BLAST compares\nsequences via similarity defined by a weighted edit distance, which results in\nit being computationally expensive. As opposed to working with edit distance, a\nvector similarity approach can be accelerated substantially using modern\nhardware or hashing techniques. Such an approach would require fixed-length\nembeddings for biological sequences. There has been recent interest in learning\nfixed-length protein embeddings using deep learning models under the hypothesis\nthat the hidden layers of supervised or semi-supervised models could produce\npotentially useful vector embeddings. We consider transformer (BERT) protein\nlanguage models that are pretrained on the TrEMBL data set and learn\nfixed-length embeddings on top of them with contextual lenses. The embeddings\nare trained to predict the family a protein belongs to for sequences in the\nPfam database. We show that for nearest-neighbor family classification,\npretraining offers a noticeable boost in performance and that the corresponding\nlearned embeddings are competitive with BLAST. Furthermore, we show that the\nraw transformer embeddings, obtained via static pooling, do not perform well on\nnearest-neighbor family classification, which suggests that learning embeddings\nin a supervised manner via contextual lenses may be a compute-efficient\nalternative to fine-tuning.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "shao2018h", "year": "2018", "title":"H-CNN Spatial Hashing Based CNN For 3D Shape Analysis", "abstract": "<p>We present a novel spatial hashing based data structure to facilitate 3D\nshape analysis using convolutional neural networks (CNNs). Our method well\nutilizes the sparse occupancy of 3D shape boundary and builds hierarchical hash\ntables for an input model under different resolutions. Based on this data\nstructure, we design two efficient GPU algorithms namely hash2col and col2hash\nso that the CNN operations like convolution and pooling can be efficiently\nparallelized. The spatial hashing is nearly minimal, and our data structure is\nalmost of the same size as the raw input. Compared with state-of-the-art\noctree-based methods, our data structure significantly reduces the memory\nfootprint during the CNN training. As the input geometry features are more\ncompactly packed, CNN operations also run faster with our data structure. The\nexperiment shows that, under the same network structure, our method yields\ncomparable or better benchmarks compared to the state-of-the-art while it has\nonly one-third memory consumption. Such superior memory performance allows the\nCNN to handle high-resolution shape analysis.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "shao2022johnson", "year": "2022", "title":"Johnson-lindenstrauss Embeddings For Noisy Vectors -- Taking Advantage Of The Noise", "abstract": "<p>This paper investigates theoretical properties of subsampling and hashing as\ntools for approximate Euclidean norm-preserving embeddings for vectors with\n(unknown) additive Gaussian noises. Such embeddings are sometimes called\nJohnson-lindenstrauss embeddings due to their celebrated lemma. Previous work\nshows that as sparse embeddings, the success of subsampling and hashing closely\ndepends on the \\(l_\\infty\\) to \\(l_2\\) ratios of the vector to be mapped. This\npaper shows that the presence of noise removes such constrain in\nhigh-dimensions, in other words, sparse embeddings such as subsampling and\nhashing with comparable embedding dimensions to dense embeddings have similar\napproximate norm-preserving dimensionality-reduction properties. The key is\nthat the noise should be treated as an information to be exploited, not simply\nsomething to be removed. Theoretical bounds for subsampling and hashing to\nrecover the approximate norm of a high dimension vector in the presence of\nnoise are derived, with numerical illustrations showing better performances are\nachieved in the presence of noise.</p>\n", "tags": ["ARXIV"] },
{"key": "sharma2018improving", "year": "2018", "title":"Improving Similarity Search With High-dimensional Locality-sensitive Hashing", "abstract": "<p>We propose a new class of data-independent locality-sensitive hashing (LSH)\nalgorithms based on the fruit fly olfactory circuit. The fundamental difference\nof this approach is that, instead of assigning hashes as dense points in a low\ndimensional space, hashes are assigned in a high dimensional space, which\nenhances their separability. We show theoretically and empirically that this\nnew family of hash functions is locality-sensitive and preserves rank\nsimilarity for inputs in any `p space. We then analyze different variations on\nthis strategy and show empirically that they outperform existing LSH methods\nfor nearest-neighbors search on six benchmark datasets. Finally, we propose a\nmulti-probe version of our algorithm that achieves higher performance for the\nsame query time, or conversely, that maintains performance of prior approaches\nwhile taking significantly less indexing time and memory. Overall, our approach\nleverages the advantages of separability provided by high-dimensional spaces,\nwhile still remaining computationally efficient</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "shastri2018vector", "year": "2018", "title":"Vector Quantized Spectral Clustering Applied To Soybean Whole Genome Sequences", "abstract": "<p>We develop a Vector Quantized Spectral Clustering (VQSC) algorithm that is a\ncombination of Spectral Clustering (SC) and Vector Quantization (VQ) sampling\nfor grouping Soybean genomes. The inspiration here is to use SC for its\naccuracy and VQ to make the algorithm computationally cheap (the complexity of\nSC is cubic in-terms of the input size). Although the combination of SC and VQ\nis not new, the novelty of our work is in developing the crucial similarity\nmatrix in SC as well as use of k-medoids in VQ, both adapted for the Soybean\ngenome data. We compare our approach with commonly used techniques like UPGMA\n(Un-weighted Pair Graph Method with Arithmetic Mean) and NJ (Neighbour\nJoining). Experimental results show that our approach outperforms both these\ntechniques significantly in terms of cluster quality (up to 25% better cluster\nquality) and time complexity (order of magnitude faster).</p>\n", "tags": ["ARXIV","Graph","Quantisation","Unsupervised"] },
{"key": "shen2013inductive", "year": "2013", "title":"Inductive Hashing On Manifolds", "abstract": "<p>Learning based hashing methods have attracted considerable attention due to\ntheir ability to greatly increase the scale at which existing algorithms may\noperate. Most of these methods are designed to generate binary codes that\npreserve the Euclidean distance in the original space. Manifold learning\ntechniques, in contrast, are better able to model the intrinsic structure\nembedded in the original high-dimensional data. The complexity of these models,\nand the problems with out-of-sample data, have previously rendered them\nunsuitable for application to large-scale embedding, however. In this work, we\nconsider how to learn compact binary embeddings on their intrinsic manifolds.\nIn order to address the above-mentioned difficulties, we describe an efficient,\ninductive solution to the out-of-sample data problem, and a process by which\nnon-parametric manifold learning may be used as the basis of a hashing method.\nOur proposed approach thus allows the development of a range of new hashing\ntechniques exploiting the flexibility of the wide variety of manifold learning\napproaches available. We particularly show that hashing on the basis of t-SNE .</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "shen2014hashing", "year": "2014", "title":"Hashing On Nonlinear Manifolds", "abstract": "<p>Learning based hashing methods have attracted considerable attention due to\ntheir ability to greatly increase the scale at which existing algorithms may\noperate. Most of these methods are designed to generate binary codes preserving\nthe Euclidean similarity in the original space. Manifold learning techniques,\nin contrast, are better able to model the intrinsic structure embedded in the\noriginal high-dimensional data. The complexities of these models, and the\nproblems with out-of-sample data, have previously rendered them unsuitable for\napplication to large-scale embedding, however. In this work, how to learn\ncompact binary embeddings on their intrinsic manifolds is considered. In order\nto address the above-mentioned difficulties, an efficient, inductive solution\nto the out-of-sample data problem, and a process by which non-parametric\nmanifold learning may be used as the basis of a hashing method is proposed. The\nproposed approach thus allows the development of a range of new hashing\ntechniques exploiting the flexibility of the wide variety of manifold learning\napproaches available. It is particularly shown that hashing on the basis of\nt-SNE outperforms state-of-the-art hashing methods on large-scale benchmark\ndatasets, and is very effective for image classification with very short code\nlengths. The proposed hashing framework is shown to be easily improved, for\nexample, by minimizing the quantization error with learned orthogonal\nrotations. In addition, a supervised inductive manifold hashing framework is\ndeveloped by incorporating the label information, which is shown to greatly\nadvance the semantic retrieval performance.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "shen2015supervised", "year": "2015", "title":"Supervised Discrete Hashing", "abstract": "<p>This paper has been withdrawn by the authour.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "shen2016learning", "year": "2016", "title":"Learning Binary Codes And Binary Weights For Efficient Classification", "abstract": "<p>This paper proposes a generic formulation that significantly expedites the\ntraining and deployment of image classification models, particularly under the\nscenarios of many image categories and high feature dimensions. As a defining\nproperty, our method represents both the images and learned classifiers using\nbinary hash codes, which are simultaneously learned from the training data.\nClassifying an image thereby reduces to computing the Hamming distance between\nthe binary codes of the image and classifiers and selecting the class with\nminimal Hamming distance. Conventionally, compact hash codes are primarily used\nfor accelerating image search. Our work is first of its kind to represent\nclassifiers using binary codes. Specifically, we formulate multi-class image\nclassification as an optimization problem over binary variables. The\noptimization alternatively proceeds over the binary classifiers and image hash\ncodes. Profiting from the special property of binary codes, we show that the\nsub-problems can be efficiently solved through either a binary quadratic\nprogram (BQP) or linear program. In particular, for attacking the BQP problem,\nwe propose a novel bit-flipping procedure which enjoys high efficacy and local\noptimality guarantee. Our formulation supports a large family of empirical loss\nfunctions and is here instantiated by exponential / hinge losses. Comprehensive\nevaluations are conducted on several representative image benchmarks. The\nexperiments consistently observe reduced complexities of model training and\ndeployment, without sacrifice of accuracies.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "shen2017deep", "year": "2017", "title":"Deep Binaries Encoding Semantic-rich Cues For Efficient Textual-visual Cross Retrieval", "abstract": "<p>Cross-modal hashing is usually regarded as an effective technique for\nlarge-scale textual-visual cross retrieval, where data from different\nmodalities are mapped into a shared Hamming space for matching. Most of the\ntraditional textual-visual binary encoding methods only consider holistic image\nrepresentations and fail to model descriptive sentences. This renders existing\nmethods inappropriate to handle the rich semantics of informative cross-modal\ndata for quality textual-visual search tasks. To address the problem of hashing\ncross-modal data with semantic-rich cues, in this paper, a novel integrated\ndeep architecture is developed to effectively encode the detailed semantics of\ninformative images and long descriptive sentences, named as Textual-Visual Deep\nBinaries (TVDB). In particular, region-based convolutional networks with long\nshort-term memory units are introduced to fully explore image regional details\nwhile semantic cues of sentences are modeled by a text convolutional network.\nAdditionally, we propose a stochastic batch-wise training routine, where\nhigh-quality binary codes and deep encoding functions are efficiently optimized\nin an alternating manner. Experiments are conducted on three multimedia\ndatasets, i.e. Microsoft COCO, IAPR TC-12, and INRIA Web Queries, where the\nproposed TVDB model significantly outperforms state-of-the-art binary coding\nmethods in the task of cross-modal retrieval.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "shen2018nash", "year": "2018", "title":"NASH Toward End-to-end Neural Architecture For Generative Semantic Hashing", "abstract": "<p>Semantic hashing has become a powerful paradigm for fast similarity search in\nmany information retrieval systems. While fairly successful, previous\ntechniques generally require two-stage training, and the binary constraints are\nhandled ad-hoc. In this paper, we present an end-to-end Neural Architecture for\nSemantic Hashing (NASH), where the binary hashing codes are treated as\nBernoulli latent variables. A neural variational inference framework is\nproposed for training, where gradients are directly back-propagated through the\ndiscrete latent variable to optimize the hash function. We also draw\nconnections between proposed method and rate-distortion theory, which provides\na theoretical foundation for the effectiveness of the proposed framework.\nExperimental results on three public datasets demonstrate that our method\nsignificantly outperforms several state-of-the-art models on both unsupervised\nand supervised scenarios.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "shen2018zero", "year": "2018", "title":"Zero-shot Sketch-image Hashing", "abstract": "<p>Recent studies show that large-scale sketch-based image retrieval (SBIR) can\nbe efficiently tackled by cross-modal binary representation learning methods,\nwhere Hamming distance matching significantly speeds up the process of\nsimilarity search. Providing training and test data subjected to a fixed set of\npre-defined categories, the cutting-edge SBIR and cross-modal hashing works\nobtain acceptable retrieval performance. However, most of the existing methods\nfail when the categories of query sketches have never been seen during\ntraining. In this paper, the above problem is briefed as a novel but realistic\nzero-shot SBIR hashing task. We elaborate the challenges of this special task\nand accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An\nend-to-end three-network architecture is built, two of which are treated as the\nbinary encoders. The third network mitigates the sketch-image heterogeneity and\nenhances the semantic relations among data by utilizing the Kronecker fusion\nlayer and graph convolution, respectively. As an important part of ZSIH, we\nformulate a generative hashing scheme in reconstructing semantic knowledge\nrepresentations for zero-shot retrieval. To the best of our knowledge, ZSIH is\nthe first zero-shot hashing work suitable for SBIR and cross-modal search.\nComprehensive experiments are conducted on two extended datasets, i.e., Sketchy\nand TU-Berlin with a novel zero-shot train-test split. The proposed model\nremarkably outperforms related works.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Image Retrieval"] },
{"key": "shen2019embarrassingly", "year": "2019", "title":"Embarrassingly Simple Binary Representation Learning", "abstract": "<p>Recent binary representation learning models usually require sophisticated\nbinary optimization, similarity measure or even generative models as\nauxiliaries. However, one may wonder whether these non-trivial components are\nneeded to formulate practical and effective hashing models. In this paper, we\nanswer the above question by proposing an embarrassingly simple approach to\nbinary representation learning. With a simple classification objective, our\nmodel only incorporates two additional fully-connected layers onto the top of\nan arbitrary backbone network, whilst complying with the binary constraints\nduring training. The proposed model lower-bounds the Information Bottleneck\n(IB) between data samples and their semantics, and can be related to many\nrecent `learning to hash’ paradigms. We show that, when properly designed, even\nsuch a simple network can generate effective binary codes, by fully exploring\ndata semantics without any held-out alternating updating steps or auxiliary\nmodels. Experiments are conducted on conventional large-scale benchmarks, i.e.,\nCIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms\nthe state-of-the-art methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "shen2020auto", "year": "2020", "title":"Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Conventional unsupervised hashing methods usually take advantage of\nsimilarity graphs, which are either pre-computed in the high-dimensional space\nor obtained from random anchor points. On the one hand, existing methods\nuncouple the procedures of hash function learning and graph construction. On\nthe other hand, graphs empirically built upon original data could introduce\nbiased prior knowledge of data relevance, leading to sub-optimal retrieval\nperformance. In this paper, we tackle the above problems by proposing an\nefficient and adaptive code-driven graph, which is updated by decoding in the\ncontext of an auto-encoder. Specifically, we introduce into our framework twin\nbottlenecks (i.e., latent variables) that exchange crucial information\ncollaboratively. One bottleneck (i.e., binary codes) conveys the high-level\nintrinsic data structure captured by the code-driven graph to the other (i.e.,\ncontinuous variables for low-level detail information), which in turn\npropagates the updated network feedback for the encoder to learn more\ndiscriminative binary codes. The auto-encoding learning objective literally\nrewards the code-driven graph to learn an optimal encoder. Moreover, the\nproposed model can be simply optimized by gradient descent without violating\nthe binary constraints. Experiments on benchmarked datasets clearly show the\nsuperiority of our framework over the state-of-the-art hashing methods. Our\nsource code can be found at https://github.com/ymcidence/TBH.</p>\n", "tags": ["ARXIV","Graph","Has Code","Unsupervised"] },
{"key": "shen2022semicon", "year": "2022", "title":"SEMICON A Learning-to-hash Solution For Large-scale Fine-grained Image Retrieval", "abstract": "<p>In this paper, we propose Suppression-Enhancing Mask based attention and\nInteractive Channel transformatiON (SEMICON) to learn binary hash codes for\ndealing with large-scale fine-grained image retrieval tasks. In SEMICON, we\nfirst develop a suppression-enhancing mask (SEM) based attention to dynamically\nlocalize discriminative image regions. More importantly, different from\nexisting attention mechanism simply erasing previous discriminative regions,\nour SEM is developed to restrain such regions and then discover other\ncomplementary regions by considering the relation between activated regions in\na stage-by-stage fashion. In each stage, the interactive channel transformation\n(ICON) module is afterwards designed to exploit correlations across channels of\nattended activation tensors. Since channels could generally correspond to the\nparts of fine-grained objects, the part correlation can be also modeled\naccordingly, which further improves fine-grained retrieval accuracy. Moreover,\nto be computational economy, ICON is realized by an efficient two-step process.\nFinally, the hash learning of our SEMICON consists of both global- and\nlocal-level branches for better representing fine-grained objects and then\ngenerating binary hash codes explicitly corresponding to multiple levels.\nExperiments on five benchmark fine-grained datasets show our superiority over\ncompeting methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "shen2024auto", "year": "2024", "title":"Auto-encoding Twin-bottleneck Hashing", "abstract": "<p>Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Graph","Unsupervised"] },
{"key": "shen2024embarrassingly", "year": "2024", "title":"Embarrassingly Simple Binary Representation Learning", "abstract": "<p>Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash’ paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "shen2024nash", "year": "2024", "title":"NASH Toward End-to-end Neural Architecture For Generative Semantic Hashing", "abstract": "<p>Semantic hashing has become a powerful paradigm for fast similarity search\nin many information retrieval systems.\nWhile fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present\nan end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary\nhashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent\nvariable to optimize the hash function.\nWe also draw connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on\nthree public datasets demonstrate that our\nmethod significantly outperforms several\nstate-of-the-art models on both unsupervised and supervised scenarios.</p>\n", "tags": ["ARXIV","Supervised","Theory"] },
{"key": "shen2024unsupervised", "year": "2024", "title":"Unsupervised Deep Hashing With Similarity-adaptive And Discrete Optimization", "abstract": "<p>Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing\nwith significantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval\nperformance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve\nsatisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a\nsimple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds\nover three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from\nthe widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph\nmatrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise\nan effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive\nexperiments validate the efficacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.</p>\n", "tags": ["ARXIV","Graph","Unsupervised"] },
{"key": "shenoy2017deduplication", "year": "2017", "title":"Deduplication In A Massive Clinical Note Dataset", "abstract": "<p>Duplication, whether exact or partial, is a common issue in many datasets. In\nclinical notes data, duplication (and near duplication) can arise for many\nreasons, such as the pervasive use of templates, copy-pasting, or notes being\ngenerated by automated procedures. A key challenge in removing such near\nduplicates is the size of such datasets; our own dataset consists of more than\n10 million notes. To detect and correct such duplicates requires algorithms\nthat both accurate and highly scalable. We describe a solution based on\nMinhashing with Locality Sensitive Hashing. In this paper, we present the\ntheory behind this method and present a database-inspired approach to make the\nmethod scalable. We also present a clustering technique using disjoint sets to\nproduce dense clusters, which speeds up our algorithm.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "shi2016functional", "year": "2016", "title":"Functional Hashing For Compressing Neural Networks", "abstract": "<p>As the complexity of deep neural networks (DNNs) trend to grow to absorb the\nincreasing sizes of data, memory and energy consumption has been receiving more\nand more attentions for industrial applications, especially on mobile devices.\nThis paper presents a novel structure based on functional hashing to compress\nDNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple\nlow-cost hash functions to fetch values in the compression space, and then\nemploys a small reconstruction network to recover that entry. The\nreconstruction network is plugged into the whole network and trained jointly.\nFunHashNN includes the recently proposed HashedNets as a degenerated case, and\nbenefits from larger value capacity and less reconstruction loss. We further\ndiscuss extensions with dual space hashing and multi-hops. On several benchmark\ndatasets, FunHashNN demonstrates high compression ratios with little loss on\nprediction accuracy.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "shi2018fast", "year": "2018", "title":"Fast Locality Sensitive Hashing For Beam Search On GPU", "abstract": "<p>We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up\nbeam search for sequence models. We utilize the winner-take-all (WTA) hash,\nwhich is based on relative ranking order of hidden dimensions and thus\nresilient to perturbations in numerical values. Our algorithm is designed by\nfully considering the underling architecture of CUDA-enabled GPUs\n(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied\nfor LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are\nshared across beams to maximize the parallelism; 3) Top frequent words are\nmerged into candidate lists to improve performance. Experiments on 4\nlarge-scale neural machine translation models demonstrate that our algorithm\ncan achieve up to 4x speedup on softmax module, and 2x overall speedup without\nhurting BLEU on GPU.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "shi2018scalable", "year": "2018", "title":"A Scalable Optimization Mechanism For Pairwise Based Discrete Hashing", "abstract": "<p>Maintaining the pair similarity relationship among originally\nhigh-dimensional data into a low-dimensional binary space is a popular strategy\nto learn binary codes. One simiple and intutive method is to utilize two\nidentical code matrices produced by hash functions to approximate a pairwise\nreal label matrix. However, the resulting quartic problem is difficult to\ndirectly solve due to the non-convex and non-smooth nature of the objective. In\nthis paper, unlike previous optimization methods using various relaxation\nstrategies, we aim to directly solve the original quartic problem using a novel\nalternative optimization mechanism to linearize the quartic problem by\nintroducing a linear regression model. Additionally, we find that gradually\nlearning each batch of binary codes in a sequential mode, i.e. batch by batch,\nis greatly beneficial to the convergence of binary code learning. Based on this\nsignificant discovery and the proposed strategy, we introduce a scalable\nsymmetric discrete hashing algorithm that gradually and smoothly updates each\nbatch of binary codes. To further improve the smoothness, we also propose a\ngreedy symmetric discrete hashing algorithm to update each bit of batch binary\ncodes. Moreover, we extend the proposed optimization mechanism to solve the\nnon-convex optimization problems for binary code learning in many other\npairwise based hashing algorithms. Extensive experiments on benchmark\nsingle-label and multi-label databases demonstrate the superior performance of\nthe proposed mechanism over recent state-of-the-art methods.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "shi2019compositional", "year": "2019", "title":"Compositional Embeddings Using Complementary Partitions For Memory-efficient Recommendation Systems", "abstract": "<p>Modern deep learning-based recommendation systems exploit hundreds to\nthousands of different categorical features, each with millions of different\ncategories ranging from clicks to posts. To respect the natural diversity\nwithin the categorical data, embeddings map each category to a unique dense\nrepresentation within an embedded space. Since each categorical feature could\ntake on as many as tens of millions of different possible categories, the\nembedding tables form the primary memory bottleneck during both training and\ninference. We propose a novel approach for reducing the embedding size in an\nend-to-end fashion by exploiting complementary partitions of the category set\nto produce a unique embedding vector for each category without explicit\ndefinition. By storing multiple smaller embedding tables based on each\ncomplementary partition and combining embeddings from each table, we define a\nunique embedding for each category at smaller memory cost. This approach may be\ninterpreted as using a specific fixed codebook to ensure uniqueness of each\ncategory’s representation. Our experimental results demonstrate the\neffectiveness of our approach over the hashing trick for reducing the size of\nthe embedding tables in terms of model loss and accuracy, while retaining a\nsimilar reduction in the number of parameters.</p>\n", "tags": ["ARXIV","Deep Learning"] },
{"key": "shi2019higher", "year": "2019", "title":"Higher-order Count Sketch Dimensionality Reduction That Retains Efficient Tensor Operations", "abstract": "<p>Sketching is a randomized dimensionality-reduction method that aims to\npreserve relevant information in large-scale datasets. Count sketch is a simple\npopular sketch which uses a randomized hash function to achieve compression. In\nthis paper, we propose a novel extension known as Higher-order Count Sketch\n(HCS). While count sketch uses a single hash function, HCS uses multiple\n(smaller) hash functions for sketching. HCS reshapes the input (vector) data\ninto a higher-order tensor and employs a tensor product of the random hash\nfunctions to compute the sketch. This results in an exponential saving (with\nrespect to the order of the tensor) in the memory requirements of the hash\nfunctions, under certain conditions on the input data. Furthermore, when the\ninput data itself has an underlying structure in the form of various tensor\nrepresentations such as the Tucker decomposition, we obtain significant\nadvantages. We derive efficient (approximate) computation of various tensor\noperations such as tensor products and tensor contractions directly on the\nsketched data. Thus, HCS is the first sketch to fully exploit the\nmulti-dimensional nature of higher-order tensors. We apply HCS to tensorized\nneural networks where we replace fully connected layers with sketched tensor\noperations. We achieve nearly state of the art accuracy with significant\ncompression on the image classification benchmark.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "shi2022deep", "year": "2022", "title":"Deep Manifold Hashing A Divide-and-conquer Approach For Semi-paired Unsupervised Cross-modal Retrieval", "abstract": "<p>Hashing that projects data into binary codes has shown extraordinary talents\nin cross-modal retrieval due to its low storage usage and high query speed.\nDespite their empirical success on some scenarios, existing cross-modal hashing\nmethods usually fail to cross modality gap when fully-paired data with plenty\nof labeled information is nonexistent. To circumvent this drawback, motivated\nby the Divide-and-Conquer strategy, we propose Deep Manifold Hashing (DMH), a\nnovel method of dividing the problem of semi-paired unsupervised cross-modal\nretrieval into three sub-problems and building one simple yet efficiency model\nfor each sub-problem. Specifically, the first model is constructed for\nobtaining modality-invariant features by complementing semi-paired data based\non manifold learning, whereas the second model and the third model aim to learn\nhash codes and hash functions respectively. Extensive experiments on three\nbenchmarks demonstrate the superiority of our DMH compared with the\nstate-of-the-art fully-paired and semi-paired unsupervised cross-modal hashing\nmethods.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "shi2022efficient", "year": "2022", "title":"Efficient Cross-modal Retrieval Via Deep Binary Hashing And Quantization", "abstract": "<p>Cross-modal retrieval aims to search for data with similar semantic meanings\nacross different content modalities. However, cross-modal retrieval requires\nhuge amounts of storage and retrieval time since it needs to process data in\nmultiple modalities. Existing works focused on learning single-source compact\nfeatures such as binary hash codes that preserve similarities between different\nmodalities. In this work, we propose a jointly learned deep hashing and\nquantization network (HQ) for cross-modal retrieval. We simultaneously learn\nbinary hash codes and quantization codes to preserve semantic information in\nmultiple modalities by an end-to-end deep learning architecture. At the\nretrieval step, binary hashing is used to retrieve a subset of items from the\nsearch space, then quantization is used to re-rank the retrieved items. We\ntheoretically and empirically show that this two-stage retrieval approach\nprovides faster retrieval results while preserving accuracy. Experimental\nresults on the NUS-WIDE, MIR-Flickr, and Amazon datasets demonstrate that HQ\nachieves boosts of more than 7% in precision compared to supervised neural\nnetwork-based compact coding models.</p>\n", "tags": ["Cross Modal","Deep Learning","Quantisation","Supervised"] },
{"key": "shi2022information", "year": "2022", "title":"Information-theoretic Hashing For Zero-shot Cross-modal Retrieval", "abstract": "<p>Zero-shot cross-modal retrieval (ZS-CMR) deals with the retrieval problem\namong heterogenous data from unseen classes. Typically, to guarantee\ngeneralization, the pre-defined class embeddings from natural language\nprocessing (NLP) models are used to build a common space. In this paper,\ninstead of using an extra NLP model to define a common space beforehand, we\nconsider a totally different way to construct (or learn) a common hamming space\nfrom an information-theoretic perspective. We term our model the\nInformation-Theoretic Hashing (ITH), which is composed of two cascading\nmodules: an Adaptive Information Aggregation (AIA) module; and a Semantic\nPreserving Encoding (SPE) module. Specifically, our AIA module takes the\ninspiration from the Principle of Relevant Information (PRI) to construct a\ncommon space that adaptively aggregates the intrinsic semantics of different\nmodalities of data and filters out redundant or irrelevant information. On the\nother hand, our SPE module further generates the hashing codes of different\nmodalities by preserving the similarity of intrinsic semantics with the\nelement-wise Kullback-Leibler (KL) divergence. A total correlation\nregularization term is also imposed to reduce the redundancy amongst different\ndimensions of hash codes. Sufficient experiments on three benchmark datasets\ndemonstrate the superiority of the proposed ITH in ZS-CMR. Source code is\navailable in the supplementary material.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "shi2022learning", "year": "2022", "title":"Learning Similarity Preserving Binary Codes For Recommender Systems", "abstract": "<p>Hashing-based Recommender Systems (RSs) are widely studied to provide\nscalable services. The existing methods for the systems combine three modules\nto achieve efficiency: feature extraction, interaction modeling, and\nbinarization. In this paper, we study an unexplored module combination for the\nhashing-based recommender systems, namely Compact Cross-Similarity Recommender\n(CCSR). Inspired by cross-modal retrieval, CCSR utilizes Maximum a Posteriori\nsimilarity instead of matrix factorization and rating reconstruction to model\ninteractions between users and items. We conducted experiments on MovieLens1M,\nAmazon product review, Ichiba purchase dataset and confirmed CCSR outperformed\nthe existing matrix factorization-based methods. On the Movielens1M dataset,\nthe absolute performance improvements are up to 15.69% in NDCG and 4.29% in\nRecall. In addition, we extensively studied three binarization modules: \\(sign\\),\nscaled tanh, and sign-scaled tanh. The result demonstrated that although\ndifferentiable scaled tanh is popular in recent discrete feature learning\nliterature, a huge performance drop occurs when outputs of scaled \\(tanh\\) are\nforced to be binary.</p>\n", "tags": ["ARXIV","Cross Modal","Survey Paper"] },
{"key": "shi2023language", "year": "2023", "title":"Language Embedded 3D Gaussians For Open-vocabulary Scene Understanding", "abstract": "<p>Open-vocabulary querying in 3D space is challenging but essential for scene\nunderstanding tasks such as object localization and segmentation.\nLanguage-embedded scene representations have made progress by incorporating\nlanguage features into 3D spaces. However, their efficacy heavily depends on\nneural networks that are resource-intensive in training and rendering. Although\nrecent 3D Gaussians offer efficient and high-quality novel view synthesis,\ndirectly embedding language features in them leads to prohibitive memory usage\nand decreased performance. In this work, we introduce Language Embedded 3D\nGaussians, a novel scene representation for open-vocabulary query tasks.\nInstead of embedding high-dimensional raw semantic features on 3D Gaussians, we\npropose a dedicated quantization scheme that drastically alleviates the memory\nrequirement, and a novel embedding procedure that achieves smoother yet high\naccuracy query, countering the multi-view feature inconsistencies and the\nhigh-frequency inductive bias in point-based representations. Our comprehensive\nexperiments show that our representation achieves the best visual quality and\nlanguage querying accuracy across current language-embedded representations,\nwhile maintaining real-time rendering frame rates on a single desktop GPU.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "shi2024variable", "year": "2024", "title":"Variable-length Quantization Strategy For Hashing", "abstract": "<p>Hashing is widely used to solve fast Approximate Nearest Neighbor (ANN) search problems, involves converting the original real-valued samples to binary-valued representations. The conventional quantization strategies, such as Single-Bit Quantization and Multi-Bit quantization, are considered ineffective, because of their serious information loss. To address this issue, we propose a novel variable-length quantization (VLQ) strategy for hashing. In the proposed VLQ technique, we divide all samples into different regions in each dimension firstly given the real-valued features of samples. Then we compute the dispersion degrees of these regions. Subsequently, we attempt to optimally assign different number of bits to each dimensions to obtain the minimum dispersion degree. Our experiments show that the VLQ strategy achieves not only superior performance over the state-of-the-art methods, but also has a faster retrieval speed on public datasets.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "shin2021exploration", "year": "2021", "title":"Exploration Into Translation-equivariant Image Quantization", "abstract": "<p>This is an exploratory study that discovers the current image quantization\n(vector quantization) do not satisfy translation equivariance in the quantized\nspace due to aliasing. Instead of focusing on anti-aliasing, we propose a\nsimple yet effective way to achieve translation-equivariant image quantization\nby enforcing orthogonality among the codebook embeddings. To explore the\nadvantages of translation-equivariant image quantization, we conduct three\nproof-of-concept experiments with a carefully controlled dataset: (1)\ntext-to-image generation, where the quantized image indices are the target to\npredict, (2) image-to-text generation, where the quantized image indices are\ngiven as a condition, (3) using a smaller training set to analyze sample\nefficiency. From the strictly controlled experiments, we empirically verify\nthat the translation-equivariant image quantizer improves not only sample\nefficiency but also the accuracy over VQGAN up to +11.9% in text-to-image\ngeneration and +3.9% in image-to-text generation.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "shinde2010similarity", "year": "2010", "title":"Similarity Search And Locality Sensitive Hashing Using Tcams", "abstract": "<p>Similarity search methods are widely used as kernels in various machine\nlearning applications. Nearest neighbor search (NNS) algorithms are often used\nto retrieve similar entries, given a query. While there exist efficient\ntechniques for exact query lookup using hashing, similarity search using exact\nnearest neighbors is known to be a hard problem and in high dimensions, best\nknown solutions offer little improvement over a linear scan. Fast solutions to\nthe approximate NNS problem include Locality Sensitive Hashing (LSH) based\ntechniques, which need storage polynomial in \\(n\\) with exponent greater than\n\\(1\\), and query time sublinear, but still polynomial in \\(n\\), where \\(n\\) is the\nsize of the database. In this work we present a new technique of solving the\napproximate NNS problem in Euclidean space using a Ternary Content Addressable\nMemory (TCAM), which needs near linear space and has O(1) query time. In fact,\nthis method also works around the best known lower bounds in the cell probe\nmodel for the query time using a data structure near linear in the size of the\ndata base. TCAMs are high performance associative memories widely used in\nnetworking applications such as access control lists. A TCAM can query for a\nbit vector within a database of ternary vectors, where every bit position\nrepresents \\(0\\), \\(1\\) or \\(<em>\\). The \\(</em>\\) is a wild card representing either a \\(0\\) or\na \\(1\\). We leverage TCAMs to design a variant of LSH, called Ternary Locality\nSensitive Hashing (TLSH) wherein we hash database entries represented by\nvectors in the Euclidean space into \\(\\{0,1,<em>\\}\\). By using the added\nfunctionality of a TLSH scheme with respect to the \\(</em>\\) character, we solve an\ninstance of the approximate nearest neighbor problem with 1 TCAM access and\nstorage nearly linear in the size of the database. We believe that this work\ncan open new avenues in very high speed data mining.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "shon2018large", "year": "2018", "title":"Large-scale Speaker Retrieval On Random Speaker Variability Subspace", "abstract": "<p>This paper describes a fast speaker search system to retrieve segments of the\nsame voice identity in the large-scale data. A recent study shows that Locality\nSensitive Hashing (LSH) enables quick retrieval of a relevant voice in the\nlarge-scale data in conjunction with i-vector while maintaining accuracy. In\nthis paper, we proposed Random Speaker-variability Subspace (RSS) projection to\nmap a data into LSH based hash tables. We hypothesized that rather than\nprojecting on completely random subspace without considering data, projecting\non randomly generated speaker variability space would give more chance to put\nthe same speaker representation into the same hash bins, so we can use less\nnumber of hash tables. Multiple RSS can be generated by randomly selecting a\nsubset of speakers from a large speaker cohort. From the experimental result,\nthe proposed approach shows 100 times and 7 times faster than the linear search\nand LSH, respectively</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "shpilrain2024cayley", "year": "2024", "title":"Cayley Hashing With Cookies", "abstract": "<p>Cayley hash functions are based on a simple idea of using a pair of semigroup\nelements, A and B, to hash the 0 and 1 bit, respectively, and then to hash an\narbitrary bit string in the natural way, by using multiplication of elements in\nthe semigroup. The main advantage of Cayley hash functions compared to, say,\nhash functions in the SHA family is that when an already hashed document is\namended, one does not have to hash the whole amended document all over again,\nbut rather hash just the amended part and then multiply the result by the hash\nof the original document. Some authors argued that this may be a security\nhazard, specifically that this property may facilitate finding a second\npreimage by splitting a long bit string into shorter pieces. In this paper, we\noffer a way to get rid of this alleged disadvantage and keep the advantages at\nthe same time. We call this method ``Cayley hashing with cookies” using\nterminology borrowed from the theory of random walks in a random environment.\nFor the platform semigroup, we use 2x2 matrices over F_p.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "shridhar2018subword", "year": "2018", "title":"Subword Semantic Hashing For Intent Classification On Small Datasets", "abstract": "<p>In this paper, we introduce the use of Semantic Hashing as embedding for the\ntask of Intent Classification and achieve state-of-the-art performance on three\nfrequently used benchmarks. Intent Classification on a small dataset is a\nchallenging task for data-hungry state-of-the-art Deep Learning based systems.\nSemantic Hashing is an attempt to overcome such a challenge and learn robust\ntext classification. Current word embedding based are dependent on\nvocabularies. One of the major drawbacks of such methods is out-of-vocabulary\nterms, especially when having small training datasets and using a wider\nvocabulary. This is the case in Intent Classification for chatbots, where\ntypically small datasets are extracted from internet communication. Two\nproblems arise by the use of internet communication. First, such datasets miss\na lot of terms in the vocabulary to use word embeddings efficiently. Second,\nusers frequently make spelling errors. Typically, the models for intent\nclassification are not trained with spelling errors and it is difficult to\nthink about ways in which users will make mistakes. Models depending on a word\nvocabulary will always face such issues. An ideal classifier should handle\nspelling errors inherently. With Semantic Hashing, we overcome these challenges\nand achieve state-of-the-art results on three datasets: AskUbuntu, Chatbot, and\nWeb Application. Our benchmarks are available online:\nhttps://github.com/kumar-shridhar/Know-Your-Intent</p>\n", "tags": ["ARXIV","Deep Learning","Has Code","Supervised"] },
{"key": "shrivastava2013beyond", "year": "2013", "title":"Beyond Pairwise Provably Fast Algorithms For Approximate k-way Similarity Search", "abstract": "<table>\n  <tbody>\n    <tr>\n      <td>We go beyond the notion of pairwise similarity and look into  search problems with \\(k\\)-way similarity functions. In this paper, we focus on problems related to  <em>3-way Jaccard</em> similarity: \\(\\mathcal{R}^{3way}= \\frac{</td>\n      <td>S_1 \\cap S_2 \\cap S_3</td>\n      <td>}{</td>\n      <td>S_1 \\cup S_2 \\cup S_3</td>\n      <td>}\\), \\(S_1, S_2, S_3 \\in \\mathcal{C}\\), where \\(\\mathcal{C}\\) is a size \\(n\\) collection of sets (or binary vectors).  We show that approximate \\(\\mathcal{R}^{3way}\\) similarity search problems admit  fast algorithms with  provable guarantees, analogous to the pairwise case. Our analysis and speedup guarantees naturally extend to \\(k\\)-way resemblance. In the process, we extend traditional framework of <em>locality sensitive hashing (LSH)</em> to handle higher order similarities, which could be of independent theoretical interest. The applicability of \\(\\mathcal{R}^{3way}\\) search is shown on the Google sets” application. In addition, we demonstrate the advantage of \\(\\mathcal{R}^{3way}\\) resemblance over the pairwise case in improving retrieval quality.”</td>\n    </tr>\n  </tbody>\n</table>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "shrivastava2014asymmetric", "year": "2014", "title":"Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS)", "abstract": "<p>We present the first provably sublinear time hashing algorithm for approximate <em>Maximum Inner Product Search</em> (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on \\(p\\)-stable distributions for \\(L_2\\) norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.</p>\n", "tags": ["Independent","LSH","NEURIPS"] },
{"key": "shrivastava2014defense", "year": "2014", "title":"In Defense Of Minhash Over Simhash", "abstract": "<p>MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n(\\(\\mathcal{R}\\)), while the collision probability of SimHash is a function of\ncosine similarity (\\(\\mathcal{S}\\)). To provide a common basis for comparison, we\nevaluate retrieval results in terms of \\(\\mathcal{S}\\) for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to \\(\\mathcal{S}\\), by using a general inequality \\(\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}\\). Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often \\(\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}\\) holds where \\(z\\)\nis only slightly larger than 2 (e.g., \\(z\\leq 2.1\\)). Our restricted worst case\nanalysis by assuming \\(\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}\\) shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "shrivastava2014improved", "year": "2014", "title":"Improved Asymmetric Locality Sensitive Hashing (ALSH) For Maximum Inner Product Search (MIPS)", "abstract": "<p>Recently it was shown that the problem of Maximum Inner Product Search (MIPS)\nis efficient and it admits provably sub-linear hashing algorithms. Asymmetric\ntransformations before hashing were the key in solving MIPS which was otherwise\nhard. In the prior work, the authors use asymmetric transformations which\nconvert the problem of approximate MIPS into the problem of approximate near\nneighbor search which can be efficiently solved using hashing. In this work, we\nprovide a different transformation which converts the problem of approximate\nMIPS into the problem of approximate cosine similarity search which can be\nefficiently solved using signed random projections. Theoretical analysis show\nthat the new scheme is significantly better than the original scheme for MIPS.\nExperimental evaluations strongly support the theoretical findings.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "shrivastava2016exact", "year": "2016", "title":"Exact Weighted Minwise Hashing In Constant Time", "abstract": "<p>Weighted minwise hashing (WMH) is one of the fundamental subroutine, required\nby many celebrated approximation algorithms, commonly adopted in industrial\npractice for large scale-search and learning. The resource bottleneck of the\nalgorithms is the computation of multiple (typically a few hundreds to\nthousands) independent hashes of the data. The fastest hashing algorithm is by\nIoffe \\cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data\nvector, \\(O(d)\\) (\\(d\\) is the number of non-zeros), for computing one hash.\nHowever, the requirement of multiple hashes demands hundreds or thousands\npasses over the data. This is very costly for modern massive dataset.\n  In this work, we break this expensive barrier and show an expected constant\namortized time algorithm which computes \\(k\\) independent and unbiased WMH in\ntime \\(O(k)\\) instead of \\(O(dk)\\) required by Ioffe’s method. Moreover, our\nproposal only needs a few bits (5 - 9 bits) of storage per hash value compared\nto around \\(64\\) bits required by the state-of-art-methodologies. Experimental\nevaluations, on real datasets, show that for computing 500 WMH, our proposal\ncan be 60000x faster than the Ioffe’s method without losing any accuracy. Our\nmethod is also around 100x faster than approximate heuristics capitalizing on\nthe efficient “densified” one permutation hashing schemes\n\\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its\nsignificant advantages, we hope that it will replace existing implementations\nin practice.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "shrivastava2016simple", "year": "2016", "title":"Simple And Efficient Weighted Minwise Hashing", "abstract": "<p>Weighted minwise hashing (WMH) is one of the fundamental subroutine, required by many celebrated approximation algorithms, commonly adopted in industrial practice for large -scale search and learning. The resource bottleneck with WMH is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data.  We propose a simple rejection type sampling scheme based on a carefully designed red-green map, where we show that the number of rejected sample has exactly the same distribution as weighted minwise sampling. The running time of our method,  for many practical datasets, is an order of magnitude smaller than existing methods. Experimental evaluations, on real datasets, show that for computing 500 WMH, our proposal can be 60000x faster than the Ioffe’s method without losing any accuracy. Our method is also around 100x faster than approximate heuristics capitalizing on the efficient ``densified” one permutation hashing schemes~\\cite{Proc:OneHashLSHICML14,Proc:ShrivastavaUAI14}. Given the simplicity of our approach and its significant advantages, we hope that it will replace existing implementations in practice.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "shrivastava2017optimal", "year": "2017", "title":"Optimal Densification For Fast And Accurate Minwise Hashing", "abstract": "<p>Minwise hashing is a fundamental and one of the most successful hashing\nalgorithm in the literature. Recent advances based on the idea of\ndensification~\\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown\nthat it is possible to compute \\(k\\) minwise hashes, of a vector with \\(d\\)\nnonzeros, in mere \\((d + k)\\) computations, a significant improvement over the\nclassical \\(O(dk)\\). These advances have led to an algorithmic improvement in the\nquery complexity of traditional indexing algorithms based on minwise hashing.\nUnfortunately, the variance of the current densification techniques is\nunnecessarily high, which leads to significantly poor accuracy compared to\nvanilla minwise hashing, especially when the data is sparse. In this paper, we\nprovide a novel densification scheme which relies on carefully tailored\n2-universal hashes. We show that the proposed scheme is variance-optimal, and\nwithout losing the runtime efficiency, it is significantly more accurate than\nexisting densification techniques. As a result, we obtain a significantly\nefficient hashing scheme which has the same variance and collision probability\nas minwise hashing. Experimental evaluations on real sparse and\nhigh-dimensional datasets validate our claims. We believe that given the\nsignificant advantages, our method will replace minwise hashing implementations\nin practice.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "shrivastava2024asymmetric", "year": "2024", "title":"Asymmetric LSH (ALSH) For Sublinear Time Maximum Inner Product Search (MIPS).", "abstract": "<p>We present the first provably sublinear time hashing algorithm for approximate\nMaximum Inner Product Search (MIPS). Searching with (un-normalized) inner\nproduct as the underlying similarity measure is a known difficult problem and\nfinding hashing schemes for MIPS was considered hard. While the existing Locality\nSensitive Hashing (LSH) framework is insufficient for solving MIPS, in this\npaper we extend the LSH framework to allow asymmetric hashing schemes. Our\nproposal is based on a key observation that the problem of finding maximum inner\nproducts, after independent asymmetric transformations, can be converted into\nthe problem of approximate near neighbor search in classical settings. This key\nobservation makes efficient sublinear hashing scheme for MIPS possible. Under\nthe extended asymmetric LSH (ALSH) framework, this paper provides an example\nof explicit construction of provably fast hashing scheme for MIPS. Our proposed\nalgorithm is simple and easy to implement. The proposed hashing scheme\nleads to significant computational savings over the two popular conventional LSH\nschemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable\ndistributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations\non Netflix and Movielens (10M) datasets.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "shrivastava2024densifying", "year": "2024", "title":"Densifying One Permutation Hashing Via Rotation For Fast Near Neighbor Search", "abstract": "<p>The query complexity of locality sensitive hashing\n(LSH) based similarity search is dominated\nby the number of hash evaluations, and this number\ngrows with the data size (Indyk &amp; Motwani,\n1998). In industrial applications such as search\nwhere the data are often high-dimensional and\nbinary (e.g., text n-grams), minwise hashing is\nwidely adopted, which requires applying a large\nnumber of permutations on the data. This is\ncostly in computation and energy-consumption.\nIn this paper, we propose a hashing technique\nwhich generates all the necessary hash evaluations\nneeded for similarity search, using one\nsingle permutation. The heart of the proposed\nhash function is a “rotation” scheme which densifies\nthe sparse sketches of one permutation\nhashing (Li et al., 2012) in an unbiased fashion\nthereby maintaining the LSH property. This\nmakes the obtained sketches suitable for hash table\nconstruction. This idea of rotation presented\nin this paper could be of independent interest for\ndensifying other types of sparse sketches.\nUsing our proposed hashing method, the query\ntime of a (K, L)-parameterized LSH is reduced\nfrom the typical O(dKL) complexity to merely\nO(KL + dL), where d is the number of nonzeros\nof the data vector, K is the number of hashes\nin each hash table, and L is the number of hash\ntables. Our experimental evaluation on real data\nconfirms that the proposed scheme significantly\nreduces the query processing time over minwise\nhashing without loss in retrieval accuracies.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "sift1m2009searching", "year": "2009", "title":"Searching with quantization: approximate nearest neighbor search using short codes and distance estimators", "abstract": "<p>We propose an approximate nearest neighbor search method based\non quantization. It uses, in particular, product quantizer to produce short codes\nand corresponding distance estimators approximating the Euclidean distance\nbetween the orginal vectors. The method is advantageously used in an asymmetric\nmanner, by computing the distance between a vector and code, unlike\ncompeting techniques such as spectral hashing that only compare codes.\nOur approach approximates the Euclidean distance based on memory efficient codes and, thus, permits efficient nearest neighbor search. Experiments\nperformed on SIFT and GIST image descriptors show excellent search accuracy.\nThe method is shown to outperform two state-of-the-art approaches of the literature.\nTimings measured when searching a vector set of 2 billion vectors are\nshown to be excellent given the high accuracy of the method.</p>\n", "tags": [] },
{"key": "silavong2021senatus", "year": "2021", "title":"Senatus -- A Fast And Accurate Code-to-code Recommendation Engine", "abstract": "<p>Machine learning on source code (MLOnCode) is a popular research field that\nhas been driven by the availability of large-scale code repositories and the\ndevelopment of powerful probabilistic and deep learning models for mining\nsource code. Code-to-code recommendation is a task in MLOnCode that aims to\nrecommend relevant, diverse and concise code snippets that usefully extend the\ncode currently being written by a developer in their development environment\n(IDE). Code-to-code recommendation engines hold the promise of increasing\ndeveloper productivity by reducing context switching from the IDE and\nincreasing code-reuse. Existing code-to-code recommendation engines do not\nscale gracefully to large codebases, exhibiting a linear growth in query time\nas the code repository increases in size. In addition, existing code-to-code\nrecommendation engines fail to account for the global statistics of code\nrepositories in the ranking function, such as the distribution of code snippet\nlengths, leading to sub-optimal retrieval results. We address both of these\nweaknesses with <em>Senatus</em>, a new code-to-code recommendation engine. At\nthe core of Senatus is <em>De-Skew</em> LSH a new locality sensitive hashing\n(LSH) algorithm that indexes the data for fast (sub-linear time) retrieval\nwhile also counteracting the skewness in the snippet length distribution using\nnovel abstract syntax tree-based feature scoring and selection algorithms. We\nevaluate Senatus and find the recommendations to be of higher quality than\ncompeting baselines, while achieving faster search. For example on the\nCodeSearchNet dataset Senatus improves performance by 31.21\\% F1 and\n147.9<em>x</em> faster query time compared to Facebook Aroma. Senatus also\noutperforms standard MinHash LSH by 29.2\\% F1 and 51.02<em>x</em> faster query\ntime.</p>\n", "tags": ["ARXIV","Deep Learning","Independent","LSH"] },
{"key": "silavong2024deskew", "year": "2024", "title":"Deskew-lsh Based Code-to-code Recommendation Engine", "abstract": "<p>Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with <em>Senatus</em>, a new code-to-code recommendation engine. At the core of Senatus is <em>De-Skew</em> LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus via automatic evaluation and with an expert developer user study and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example, on the CodeSearchNet dataset we show that Senatus improves performance by 6.7% F1 and query time 16x is faster compared to Facebook Aroma on the task of code-to-code recommendation.</p>\n", "tags": ["ARXIV","Deep Learning","Independent","LSH"] },
{"key": "silcock2022noise", "year": "2022", "title":"Noise-robust De-duplication At Scale", "abstract": "<p>Identifying near duplicates within large, noisy text corpora has a myriad of\napplications that range from de-duplicating training datasets, reducing privacy\nrisk, and evaluating test set leakage, to identifying reproduced news articles\nand literature within large corpora. Across these diverse applications, the\noverwhelming majority of work relies on N-grams. Limited efforts have been made\nto evaluate how well N-gram methods perform, in part because it is unclear how\none could create an unbiased evaluation dataset for a massive corpus. This\nstudy uses the unique timeliness of historical news wires to create a 27,210\ndocument dataset, with 122,876 positive duplicate pairs, for studying\nnoise-robust de-duplication. The time-sensitivity of news makes comprehensive\nhand labelling feasible - despite the massive overall size of the corpus - as\nduplicates occur within a narrow date range. The study then develops and\nevaluates a range of de-duplication methods: hashing and N-gram overlap (which\npredominate in the literature), a contrastively trained bi-encoder, and a\nre-rank style approach combining a bi- and cross-encoder. The neural approaches\nsignificantly outperform hashing and N-gram overlap. We show that the\nbi-encoder scales well, de-duplicating a 10 million article corpus on a single\nGPU card in a matter of hours. We also apply our pre-trained model to the\nRealNews and patent portions of C4 (Colossal Clean Crawled Corpus),\nillustrating that a neural approach can identify many near duplicates missed by\nhashing, in the presence of various types of noise. The public release of our\nNEWS-COPY de-duplication dataset, codebase, and the pre-trained models will\nfacilitate further research and applications.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "silva2021fractal", "year": "2021", "title":"Fractal Measures Of Image Local Features An Application To Texture Recognition", "abstract": "<p>Here we propose a new method for the classification of texture images\ncombining fractal measures (fractal dimension, multifractal spectrum and\nlacunarity) with local binary patterns. More specifically we compute the box\ncounting dimension of the local binary codes thresholded at different levels to\ncompose the feature vector. The proposal is assessed in the classification of\nthree benchmark databases: KTHTIPS-2b, UMD and UIUC as well as in a real-world\nproblem, namely the identification of Brazilian plant species (database\n1200Tex) using scanned images of their leaves. The proposed method demonstrated\nto be competitive with other state-of-the-art solutions reported in the\nliterature. Such results confirmed the potential of combining a powerful local\ncoding description with the multiscale information captured by the fractal\ndimension for texture classification.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "simhadri2024results", "year": "2024", "title":"Results Of The Big ANN Neurips23 Competition", "abstract": "<p>The 2023 Big ANN Challenge, held at NeurIPS 2023, focused on advancing the\nstate-of-the-art in indexing data structures and search algorithms for\npractical variants of Approximate Nearest Neighbor (ANN) search that reflect\nthe growing complexity and diversity of workloads. Unlike prior challenges that\nemphasized scaling up classical ANN search\n~\\cite{DBLP:conf/nips/SimhadriWADBBCH21}, this competition addressed filtered\nsearch, out-of-distribution data, sparse and streaming variants of ANNS.\nParticipants developed and submitted innovative solutions that were evaluated\non new standard datasets with constrained computational resources. The results\nshowcased significant improvements in search accuracy and efficiency over\nindustry-standard baselines, with notable contributions from both academic and\nindustrial teams. This paper summarizes the competition tracks, datasets,\nevaluation metrics, and the innovative approaches of the top-performing\nsubmissions, providing insights into the current advancements and future\ndirections in the field of approximate nearest neighbor search.</p>\n", "tags": ["ARXIV","NEURIPS"] },
{"key": "singh2014nearest", "year": "2014", "title":"Nearest Keyword Set Search In Multi-dimensional Datasets", "abstract": "<p>Keyword-based search in text-rich multi-dimensional datasets facilitates many\nnovel applications and tools. In this paper, we consider objects that are\ntagged with keywords and are embedded in a vector space. For these datasets, we\nstudy queries that ask for the tightest groups of points satisfying a given set\nof keywords. We propose a novel method called ProMiSH (Projection and Multi\nScale Hashing) that uses random projection and hash-based index structures, and\nachieves high scalability and speedup. We present an exact and an approximate\nversion of the algorithm. Our empirical studies, both on real and synthetic\ndatasets, show that ProMiSH has a speedup of more than four orders over\nstate-of-the-art tree-based techniques. Our scalability tests on datasets of\nsizes up to 10 million and dimensions up to 100 for queries having up to 9\nkeywords show that ProMiSH scales linearly with the dataset size, the dataset\ndimension, the query size, and the result size.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "singh2019adversarially", "year": "2019", "title":"Adversarially Trained Deep Neural Semantic Hashing Scheme For Subjective Search In Fashion Inventory", "abstract": "<p>The simple approach of retrieving a closest match of a query image from one\nin the gallery, compares an image pair using sum of absolute difference in\npixel or feature space. The process is computationally expensive, ill-posed to\nillumination, background composition, pose variation, as well as inefficient to\nbe deployed on gallery sets with more than 1000 elements. Hashing is a faster\nalternative which involves representing images in reduced dimensional simple\nfeature spaces. Encoding images into binary hash codes enables similarity\ncomparison in an image-pair using the Hamming distance measure. The challenge,\nhowever, lies in encoding the images using a semantic hashing scheme that lets\nsubjective neighbors lie within the tolerable Hamming radius. This work\npresents a solution employing adversarial learning of a deep neural semantic\nhashing network for fashion inventory retrieval. It consists of a feature\nextracting convolutional neural network (CNN) learned to (i) minimize error in\nclassifying type of clothing, (ii) minimize hamming distance between semantic\nneighbors and maximize distance between semantically dissimilar images, (iii)\nmaximally scramble a discriminator’s ability to identify the corresponding hash\ncode-image pair when processing a semantically similar query-gallery image\npair. Experimental validation for fashion inventory search yields a mean\naverage precision (mAP) of 90.65% in finding the closest match as compared to\n53.26% obtained by the prior art of deep Cauchy hashing for hamming space\nretrieval.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "singh2020ihashnet", "year": "2020", "title":"Ihashnet Iris Hashing Network Based On Efficient Multi-index Hashing", "abstract": "<p>Massive biometric deployments are pervasive in today’s world. But despite the\nhigh accuracy of biometric systems, their computational efficiency degrades\ndrastically with an increase in the database size. Thus, it is essential to\nindex them. An ideal indexing scheme needs to generate codes that preserve the\nintra-subject similarity as well as inter-subject dissimilarity. Here, in this\npaper, we propose an iris indexing scheme using real-valued deep iris features\nbinarized to iris bar codes (IBC) compatible with the indexing structure.\nFirstly, for extracting robust iris features, we have designed a network\nutilizing the domain knowledge of ordinal filtering and learning their\nnonlinear combinations. Later these real-valued features are binarized.\nFinally, for indexing the iris dataset, we have proposed a loss that can\ntransform the binary feature into an improved feature compatible with the\nMulti-Index Hashing scheme. This loss function ensures the hamming distance\nequally distributed among all the contiguous disjoint sub-strings. To the best\nof our knowledge, this is the first work in the iris indexing domain that\npresents an end-to-end iris indexing structure. Experimental results on four\ndatasets are presented to depict the efficacy of the proposed approach.</p>\n", "tags": ["ARXIV"] },
{"key": "singh2020robust", "year": "2020", "title":"Robust Homomorphic Video Hashing", "abstract": "<p>The Internet has been weaponized to carry out cybercriminal activities at an\nunprecedented pace. The rising concerns for preserving the privacy of personal\ndata while availing modern tools and technologies is alarming. End-to-end\nencrypted solutions are in demand for almost all commercial platforms. On one\nside, it seems imperative to provide such solutions and give people trust to\nreliably use these platforms. On the other side, this creates a huge\nopportunity to carry out unchecked cybercrimes. This paper proposes a robust\nvideo hashing technique, scalable and efficient in chalking out matches from an\nenormous bulk of videos floating on these commercial platforms. The video hash\nis validated to be robust to common manipulations like scaling, corruptions by\nnoise, compression, and contrast changes that are most probable to happen\nduring transmission. It can also be transformed into the encrypted domain and\nwork on top of encrypted videos without deciphering. Thus, it can serve as a\npotential forensic tool that can trace the illegal sharing of videos without\nknowing the underlying content. Hence, it can help preserve privacy and combat\ncybercrimes such as revenge porn, hateful content, child abuse, or illegal\nmaterial propagated in a video.</p>\n", "tags": ["ARXIV"] },
{"key": "singh2023better", "year": "2023", "title":"Better Generalization With Semantic Ids A Case Study In Ranking For Recommendations", "abstract": "<p>Randomly-hashed item ids are used ubiquitously in recommendation models.\nHowever, the learned representations from random hashing prevents\ngeneralization across similar items, causing problems of learning unseen and\nlong-tail items, especially when item corpus is large, power-law distributed,\nand evolving dynamically. In this paper, we propose using content-derived\nfeatures as a replacement for random ids. We show that simply replacing ID\nfeatures with content-based embeddings can cause a drop in quality due to\nreduced memorization capability. To strike a good balance of memorization and\ngeneralization, we propose to use Semantic IDs – a compact discrete item\nrepresentation learned from frozen content embeddings using RQ-VAE that\ncaptures the hierarchy of concepts in items – as a replacement for random item\nids. Similar to content embeddings, the compactness of Semantic IDs poses a\nproblem of easy adaption in recommendation models. We propose novel methods for\nadapting Semantic IDs in industry-scale ranking models, through hashing\nsub-pieces of of the Semantic-ID sequences. In particular, we find that the\nSentencePiece model that is commonly used in LLM tokenization outperforms\nmanually crafted pieces such as N-grams. To the end, we evaluate our approaches\nin a real-world ranking model for YouTube recommendations. Our experiments\ndemonstrate that Semantic IDs can replace the direct use of video IDs by\nimproving the generalization ability on new and long-tail item slices without\nsacrificing overall model quality.</p>\n", "tags": ["ARXIV","Case Study","Independent"] },
{"key": "sivertsen2017fast", "year": "2017", "title":"Fast Nearest Neighbor Preserving Embeddings", "abstract": "<p>We show an analog to the Fast Johnson-Lindenstrauss Transform for Nearest\nNeighbor Preserving Embeddings in \\(ℓ₂\\). These are sparse, randomized\nembeddings that preserve the (approximate) nearest neighbors. The\ndimensionality of the embedding space is bounded not by the size of the\nembedded set n, but by its doubling dimension {\\lambda}. For most large\nreal-world datasets this will mean a considerably lower-dimensional embedding\nspace than possible when preserving all distances. The resulting embeddings can\nbe used with existing approximate nearest neighbor data structures to yield\nspeed improvements.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "skraparlis2000method", "year": "2000", "title":"A Method For Command Identification Using Modified Collision Free Hashing With Addition Rotation Iterative Hash Functions (part 1)", "abstract": "<p>This paper proposes a method for identification of a user`s fixed string set\n(which can be a command/instruction set for a terminal or microprocessor). This\nmethod is fast and has very small memory requirements, compared to a\ntraditional full string storage and compare method. The user feeds characters\ninto a microcontroller via a keyboard or another microprocessor sends commands\nand the microcontroller hashes the input in order to identify valid commands,\nensuring no collisions between hashed valid strings, while applying further\ncriteria to narrow collision between random and valid strings. The method\nproposed narrows the possibility of the latter kind of collision, achieving\nsmall code and memory-size utilization and very fast execution. Hashing is\nachieved using additive &amp; rotating hash functions in an iterative form, which\ncan be very easily implemented in simple microcontrollers and microprocessors.\nSuch hash functions are presented and compared according to their efficiency\nfor a given string/command set, using the program found in the appendix.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "slesarev2022benchmarking", "year": "2022", "title":"Benchmarking Hashing Algorithms For Load Balancing In A Distributed Database Environment", "abstract": "<p>Modern high load applications store data using multiple database instances.\nSuch an architecture requires data consistency, and it is important to ensure\neven distribution of data among nodes. Load balancing is used to achieve these\ngoals.\n  Hashing is the backbone of virtually all load balancing systems. Since the\nintroduction of classic Consistent Hashing, many algorithms have been devised\nfor this purpose.\n  One of the purposes of the load balancer is to ensure storage cluster\nscalability. It is crucial for the performance of the whole system to transfer\nas few data records as possible during node addition or removal. The load\nbalancer hashing algorithm has the greatest impact on this process.\n  In this paper we experimentally evaluate several hashing algorithms used for\nload balancing, conducting both simulated and real system experiments. To\nevaluate algorithm performance, we have developed a benchmark suite based on\nUnidata MDM~ – a scalable toolkit for various Master Data Management (MDM)\napplications. For assessment, we have employed three criteria~ – uniformity of\nthe produced distribution, the number of moved records, and computation speed.\nFollowing the results of our experiments, we have created a table, in which\neach algorithm is given an assessment according to the abovementioned criteria.</p>\n", "tags": ["ARXIV"] },
{"key": "sodani2021scalable", "year": "2021", "title":"Scalable Reverse Image Search Engine For Nasaworldview", "abstract": "<p>Researchers often spend weeks sifting through decades of unlabeled satellite\nimagery(on NASA Worldview) in order to develop datasets on which they can start\nconducting research. We developed an interactive, scalable and fast image\nsimilarity search engine (which can take one or more images as the query image)\nthat automatically sifts through the unlabeled dataset reducing dataset\ngeneration time from weeks to minutes. In this work, we describe key components\nof the end to end pipeline. Our similarity search system was created to be able\nto identify similar images from a potentially petabyte scale database that are\nsimilar to an input image, and for this we had to break down each query image\ninto its features, which were generated by a classification layer stripped CNN\ntrained in a supervised manner. To store and search these features efficiently,\nwe had to make several scalability improvements. To improve the speed, reduce\nthe storage, and shrink memory requirements for embedding search, we add a\nfully connected layer to our CNN make all images into a 128 length vector\nbefore entering the classification layers. This helped us compress the size of\nour image features from 2048 (for ResNet, which was initially tried as our\nfeaturizer) to 128 for our new custom model. Additionally, we utilize existing\napproximate nearest neighbor search libraries to significantly speed up\nembedding search. Our system currently searches over our entire database of\nimages at 5 seconds per query on a single virtual machine in the cloud. In the\nfuture, we would like to incorporate a SimCLR based featurizing model which\ncould be trained without any labelling by a human (since the classification\naspect of the model is irrelevant to this use case).</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "song2017binary", "year": "2017", "title":"Binary Generative Adversarial Networks For Image Retrieval", "abstract": "<p>The most striking successes in image retrieval using deep hashing have mostly\ninvolved discriminative models, which require labels. In this paper, we use\nbinary generative adversarial networks (BGAN) to embed images to binary codes\nin an unsupervised way. By restricting the input noise variable of generative\nadversarial networks (GAN) to be binary and conditioned on the features of each\ninput image, BGAN can simultaneously learn a binary representation per image,\nand generate an image plausibly similar to the original one. In the proposed\nframework, we address two main problems: 1) how to directly generate binary\ncodes without relaxation? 2) how to equip the binary representation with the\nability of accurate image retrieval? We resolve these problems by proposing new\nsign-activation strategy and a loss function steering the learning process,\nwhich consists of new models for adversarial loss, a content loss, and a\nneighborhood structure loss. Experimental results on standard datasets\n(CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly\noutperforms existing hashing methods by up to 107\\% in terms of~mAP (See Table\ntab.res.map.comp) Our anonymous code is available at:\nhttps://github.com/htconquer/BGAN.</p>\n", "tags": ["ARXIV","GAN","Has Code","Image Retrieval","Unsupervised"] },
{"key": "song2017deep", "year": "2017", "title":"Deep Discrete Hashing With Self-supervised Pairwise Labels", "abstract": "<p>Hashing methods have been widely used for applications of large-scale image\nretrieval and classification. Non-deep hashing methods using handcrafted\nfeatures have been significantly outperformed by deep hashing methods due to\ntheir better feature representation and end-to-end learning framework. However,\nthe most striking successes in deep hashing have mostly involved discriminative\nmodels, which require labels. In this paper, we propose a novel unsupervised\ndeep hashing method, named Deep Discrete Hashing (DDH), for large-scale image\nretrieval and classification. In the proposed framework, we address two main\nproblems: 1) how to directly learn discrete binary codes? 2) how to equip the\nbinary representation with the ability of accurate image retrieval and\nclassification in an unsupervised way? We resolve these problems by introducing\nan intermediate variable and a loss function steering the learning process,\nwhich is based on the neighborhood structure in the original space.\nExperimental results on standard datasets (CIFAR-10, NUS-WIDE, and Oxford-17)\ndemonstrate that our DDH significantly outperforms existing hashing methods by\nlarge margin in terms of~mAP for image retrieval and object recognition. Code\nis available at \\url{https://github.com/htconquer/ddh}.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Supervised"] },
{"key": "song2018self", "year": "2018", "title":"Self-supervised Video Hashing With Hierarchical Binary Auto-encoder", "abstract": "<p>Existing video hash functions are built on three isolated stages: frame\npooling, relaxed learning, and binarization, which have not adequately explored\nthe temporal order of video frames in a joint binary optimization model,\nresulting in severe information loss. In this paper, we propose a novel\nunsupervised video hashing framework dubbed Self-Supervised Video Hashing\n(SSVH), that is able to capture the temporal nature of videos in an end-to-end\nlearning-to-hash fashion. We specifically address two central problems: 1) how\nto design an encoder-decoder architecture to generate binary codes for videos;\nand 2) how to equip the binary codes with the ability of accurate video\nretrieval. We design a hierarchical binary autoencoder to model the temporal\ndependencies in videos with multiple granularities, and embed the videos into\nbinary codes with less computations than the stacked architecture. Then, we\nencourage the binary codes to simultaneously reconstruct the visual content and\nneighborhood structure of the videos. Experiments on two real-world datasets\n(FCVID and YFCC) show that our SSVH method can significantly outperform the\nstate-of-the-art methods and achieve the currently best performance on the task\nof unsupervised video retrieval.</p>\n", "tags": ["ARXIV","Supervised","Video Retrieval"] },
{"key": "song2019deep", "year": "2019", "title":"Deep Hashing Learning For Visual And Semantic Retrieval Of Remote Sensing Images", "abstract": "<p>Driven by the urgent demand for managing remote sensing big data, large-scale\nremote sensing image retrieval (RSIR) attracts increasing attention in the\nremote sensing field. In general, existing retrieval methods can be regarded as\nvisual-based retrieval approaches which search and return a set of similar\nimages from a database to a given query image. Although retrieval methods have\nachieved great success, there is still a question that needs to be responded\nto: Can we obtain the accurate semantic labels of the returned similar images\nto further help analyzing and processing imagery? Inspired by the above\nquestion, in this paper, we redefine the image retrieval problem as visual and\nsemantic retrieval of images. Specifically, we propose a novel deep hashing\nconvolutional neural network (DHCNN) to simultaneously retrieve the similar\nimages and classify their semantic labels in a unified framework. In more\ndetail, a convolutional neural network (CNN) is used to extract\nhigh-dimensional deep features. Then, a hash layer is perfectly inserted into\nthe network to transfer the deep features into compact hash codes. In addition,\na fully connected layer with a softmax function is performed on hash layer to\ngenerate class distribution. Finally, a loss function is elaborately designed\nto simultaneously consider the label loss of each image and similarity loss of\npairs of images. Experimental results on two remote sensing datasets\ndemonstrate that the proposed method achieves the state-of-art retrieval and\nclassification performance.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "song2020deep", "year": "2020", "title":"Deep Robust Multilevel Semantic Cross-modal Hashing", "abstract": "<p>Hashing based cross-modal retrieval has recently made significant progress.\nBut straightforward embedding data from different modalities into a joint\nHamming space will inevitably produce false codes due to the intrinsic modality\ndiscrepancy and noises. We present a novel Robust Multilevel Semantic Hashing\n(RMSH) for more accurate cross-modal retrieval. It seeks to preserve\nfine-grained similarity among data with rich semantics, while explicitly\nrequire distances between dissimilar points to be larger than a specific value\nfor strong robustness. For this, we give an effective bound of this value based\non the information coding-theoretic analysis, and the above goals are embodied\ninto a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via\nfusing multiple hash codes to explore seldom-seen semantics, alleviating the\nsparsity problem of similarity information. Experiments on three benchmarks\nshow the validity of the derived bounds, and our method achieves\nstate-of-the-art performance.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "song2022asymmetric", "year": "2022", "title":"Asymmetric Hash Code Learning For Remote Sensing Image Retrieval", "abstract": "<p>Remote sensing image retrieval (RSIR), aiming at searching for a set of\nsimilar items to a given query image, is a very important task in remote\nsensing applications. Deep hashing learning as the current mainstream method\nhas achieved satisfactory retrieval performance. On one hand, various deep\nneural networks are used to extract semantic features of remote sensing images.\nOn the other hand, the hashing techniques are subsequently adopted to map the\nhigh-dimensional deep features to the low-dimensional binary codes. This kind\nof methods attempts to learn one hash function for both the query and database\nsamples in a symmetric way. However, with the number of database samples\nincreasing, it is typically time-consuming to generate the hash codes of\nlarge-scale database images. In this paper, we propose a novel deep hashing\nmethod, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL\ngenerates the hash codes of query and database images in an asymmetric way. In\nmore detail, the hash codes of query images are obtained by binarizing the\noutput of the network, while the hash codes of database images are directly\nlearned by solving the designed objective function. In addition, we combine the\nsemantic information of each image and the similarity information of pairs of\nimages as supervised information to train a deep hashing network, which\nimproves the representation ability of deep features and hash codes. The\nexperimental results on three public datasets demonstrate that the proposed\nmethod outperforms symmetric methods in terms of retrieval accuracy and\nefficiency. The source code is available at\nhttps://github.com/weiweisong415/Demo AHCL for TGRS2022.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Supervised"] },
{"key": "song2024inter", "year": "2024", "title":"Inter-media Hashing For Large-scale Retrieval From Heterogeneous Data Sources", "abstract": "<p>In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users’ demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query’s results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "song2024self", "year": "2024", "title":"Self-supervised Video Hashing With Hierarchical Binary Auto-encoder", "abstract": "<p>Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed Self-Supervised Video Hashing (SSVH), that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos; and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary autoencoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we encourage the binary codes to simultaneously reconstruct the visual content and neighborhood structure of the videos. Experiments on two real-world datasets (FCVID and YFCC) show that our SSVH method can significantly outperform the state-of-the-art methods and achieve the currently best performance on the task of unsupervised video retrieval.</p>\n", "tags": ["ARXIV","Supervised","Video Retrieval"] },
{"key": "song2024top", "year": "2024", "title":"Top Rank Supervised Binary Coding For Visual Search", "abstract": "<p>In recent years, binary coding techniques are becoming\nincreasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been\ndemonstrated that supervised binary coding techniques that\nleverage supervised information can significantly enhance\nthe coding quality, and hence greatly benefit visual search\ntasks. Typically, a modern binary coding method seeks\nto learn a group of coding functions which compress data\nsamples into binary codes. However, few methods pursued\nthe coding functions such that the precision at the top of\na ranking list according to Hamming distances of the generated binary codes is optimized.\nIn this paper, we propose a novel supervised binary coding approach, namely\nTop Rank Supervised Binary Coding (Top-RSBC), which\nexplicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train\nthe disciplined coding functions, by which the mistakes at\nthe top of a Hamming-distance ranking list are penalized\nmore than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective\nwith a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online\nlearning algorithm to optimize the surrogate objective more\nefficiently. Empirical studies based upon three benchmark\nimage datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over\nthe state-of-the-arts.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "sosnovski2023cryptanalysis", "year": "2023", "title":"Cryptanalysis Of A Cayley Hash Function Based On Affine Maps In One Variable Over A Finite Field", "abstract": "<p>Cayley hash functions are cryptographic hashes constructed from Cayley graphs\nof groups. The hash function proposed by Shpilrain and Sosnovski (2016), based\non linear functions over a finite field, was proven insecure. This paper shows\nthat the proposal by Ghaffari and Mostaghim (2018) that uses the Shpilrain and\nSosnovski’s hash in its construction is also insecure. We demonstrate its\nsecurity vulnerability by constructing collisions.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "spencer2016noisy", "year": "2016", "title":"Noisy 1-bit Compressed Sensing Embeddings Enjoy A Restricted Isometry Property", "abstract": "<p>We investigate the sign-linear embeddings of 1-bit compressed sensing given\nby Gaussian measurements. One can give short arguments concerning a Restricted\nIsometry Property of such maps using Vapnik-Chervonenkis dimension of sparse\nhemispheres. This approach has a natural extension to the presence of additive\nwhite noise prior to quantization. Noisy one-bit mappings are shown to satisfy\nan RIP when the metric on the sphere is given by the noise.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "spring2016scalable", "year": "2016", "title":"Scalable And Sustainable Deep Learning Via Randomized Hashing", "abstract": "<p>Current deep learning architectures are growing larger in order to learn from\ncomplex datasets. These architectures require giant matrix multiplication\noperations to train millions of parameters. Conversely, there is another\ngrowing trend to bring deep learning to low-power, embedded devices. The matrix\noperations, associated with both training and testing of deep networks, are\nvery expensive from a computational and energy standpoint. We present a novel\nhashing based technique to drastically reduce the amount of computation needed\nto train and test deep networks. Our approach combines recent ideas from\nadaptive dropouts and randomized hashing for maximum inner product search to\nselect the nodes with the highest activation efficiently. Our new algorithm for\ndeep learning reduces the overall computational cost of forward and\nback-propagation by operating on significantly fewer (sparse) nodes. As a\nconsequence, our algorithm uses only 5% of the total multiplications, while\nkeeping on average within 1% of the accuracy of the original model. A unique\nproperty of the proposed hashing based back-propagation is that the updates are\nalways sparse. Due to the sparse gradient updates, our algorithm is ideally\nsuited for asynchronous and parallel training leading to near linear speedup\nwith increasing number of cores. We demonstrate the scalability and\nsustainability (energy efficiency) of our proposed algorithm via rigorous\nexperimental evaluations on several real datasets.</p>\n", "tags": ["ARXIV","Deep Learning","Independent"] },
{"key": "sridhar2010comparison", "year": "2010", "title":"Comparison Of Modified Dual Ternary Indexing And Multi-key Hashing Algorithms For Music Information Retrieval", "abstract": "<p>In this work we have compared two indexing algorithms that have been used to\nindex and retrieve Carnatic music songs. We have compared a modified algorithm\nof the Dual ternary indexing algorithm for music indexing and retrieval with\nthe multi-key hashing indexing algorithm proposed by us. The modification in\nthe dual ternary algorithm was essential to handle variable length query phrase\nand to accommodate features specific to Carnatic music. The dual ternary\nindexing algorithm is adapted for Carnatic music by segmenting using the\nsegmentation technique for Carnatic music. The dual ternary algorithm is\ncompared with the multi-key hashing algorithm designed by us for indexing and\nretrieval in which features like MFCC, spectral flux, melody string and\nspectral centroid are used as features for indexing data into a hash table. The\nway in which collision resolution was handled by this hash table is different\nthan the normal hash table approaches. It was observed that multi-key hashing\nbased retrieval had a lesser time complexity than dual-ternary based indexing\nThe algorithms were also compared for their precision and recall in which\nmulti-key hashing had a better recall than modified dual ternary indexing for\nthe sample data considered.</p>\n", "tags": [] },
{"key": "stanley2020sir", "year": "2020", "title":"SIR Similar Image Retrieval For Product Search In E-commerce", "abstract": "<p>We present a similar image retrieval (SIR) platform that is used to quickly\ndiscover visually similar products in a catalog of millions. Given the size,\ndiversity, and dynamism of our catalog, product search poses many challenges.\nIt can be addressed by building supervised models to tagging product images\nwith labels representing themes and later retrieving them by labels. This\napproach suffices for common and perennial themes like “white shirt” or\n“lifestyle image of TV”. It does not work for new themes such as\n“e-cigarettes”, hard-to-define ones such as “image with a promotional badge”,\nor the ones with short relevance span such as “Halloween costumes”. SIR is\nideal for such cases because it allows us to search by an example, not a\npre-defined theme. We describe the steps - embedding computation, encoding, and\nindexing - that power the approximate nearest neighbor search back-end. We also\nhighlight two applications of SIR. The first one is related to the detection of\nproducts with various types of potentially objectionable themes. This\napplication is run with a sense of urgency, hence the typical time frame to\ntrain and bootstrap a model is not permitted. Also, these themes are often\nshort-lived based on current trends, hence spending resources to build a\nlasting model is not justified. The second application is a variant item\ndetection system where SIR helps discover visual variants that are hard to find\nthrough text search. We analyze the performance of SIR in the context of these\napplications.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "steorts2018probabilistic", "year": "2018", "title":"Probabilistic Blocking With An Application To The Syrian Conflict", "abstract": "<p>Entity resolution seeks to merge databases as to remove duplicate entries\nwhere unique identifiers are typically unknown. We review modern blocking\napproaches for entity resolution, focusing on those based upon locality\nsensitive hashing (LSH). First, we introduce \\(k\\)-means locality sensitive\nhashing (KLSH), which is based upon the information retrieval literature and\nclusters similar records into blocks using a vector-space representation and\nprojections. Second, we introduce a subquadratic variant of LSH to the\nliterature, known as Densified One Permutation Hashing (DOPH). Third, we\npropose a weighted variant of DOPH. We illustrate each method on an application\nto a subset of the ongoing Syrian conflict, giving a discussion of each method.</p>\n", "tags": ["Independent","LSH","Survey Paper"] },
{"key": "stojmirovic2003indexing", "year": "2003", "title":"Indexing Schemes For Similarity Search In Datasets Of Short Protein Fragments", "abstract": "<p>We propose a family of very efficient hierarchical indexing schemes for\nungapped, score matrix-based similarity search in large datasets of short (4-12\namino acid) protein fragments. This type of similarity search has importance in\nboth providing a building block to more complex algorithms and for possible use\nin direct biological investigations where datasets are of the order of 60\nmillion objects. Our scheme is based on the internal geometry of the amino acid\nalphabet and performs exceptionally well, for example outputting 100 nearest\nneighbours to any possible fragment of length 10 after scanning on average less\nthan one per cent of the entire dataset.</p>\n", "tags": [] },
{"key": "stojmirovic2008quasi", "year": "2008", "title":"Quasi-metrics Similarities And Searches Aspects Of Geometry Of Protein Datasets", "abstract": "<p>A quasi-metric is a distance function which satisfies the triangle inequality\nbut is not symmetric: it can be thought of as an asymmetric metric. The central\nresult of this thesis, developed in Chapter 3, is that a natural correspondence\nexists between similarity measures between biological (nucleotide or protein)\nsequences and quasi-metrics.\n  Chapter 2 presents basic concepts of the theory of quasi-metric spaces and\nintroduces a new examples of them: the universal countable rational\nquasi-metric space and its bicompletion, the universal bicomplete separable\nquasi-metric space. Chapter 4 is dedicated to development of a notion of the\nquasi-metric space with Borel probability measure, or pq-space. The main result\nof this chapter indicates that `a high dimensional quasi-metric space is close\nto being a metric space’.\n  Chapter 5 investigates the geometric aspects of the theory of database\nsimilarity search in the context of quasi-metrics. The results about\n\\(pq\\)-spaces are used to produce novel theoretical bounds on performance of\nindexing schemes.\n  Finally, the thesis presents some biological applications. Chapter 6\nintroduces FSIndex, an indexing scheme that significantly accelerates\nsimilarity searches of short protein fragment datasets. Chapter 7 presents the\nprototype of the system for discovery of short functional protein motifs called\nPFMFind, which relies on FSIndex for similarity searches.</p>\n", "tags": ["ARXIV"] },
{"key": "struppek2021learning", "year": "2021", "title":"Learning To Break Deep Perceptual Hashing The Use Case Neuralhash", "abstract": "<p>Apple recently revealed its deep perceptual hashing system NeuralHash to\ndetect child sexual abuse material (CSAM) on user devices before files are\nuploaded to its iCloud service. Public criticism quickly arose regarding the\nprotection of user privacy and the system’s reliability. In this paper, we\npresent the first comprehensive empirical analysis of deep perceptual hashing\nbased on NeuralHash. Specifically, we show that current deep perceptual hashing\nmay not be robust. An adversary can manipulate the hash values by applying\nslight changes in images, either induced by gradient-based approaches or simply\nby performing standard image transformations, forcing or preventing hash\ncollisions. Such attacks permit malicious actors easily to exploit the\ndetection system: from hiding abusive material to framing innocent users,\neverything is possible. Moreover, using the hash values, inferences can still\nbe made about the data stored on user devices. In our view, based on our\nresults, deep perceptual hashing in its current form is generally not ready for\nrobust client-side scanning and should not be used from a privacy perspective.</p>\n", "tags": ["ARXIV"] },
{"key": "stylianou2019visualizing", "year": "2019", "title":"Visualizing Deep Similarity Networks", "abstract": "<p>For convolutional neural network models that optimize an image embedding, we\npropose a method to highlight the regions of images that contribute most to\npairwise similarity. This work is a corollary to the visualization tools\ndeveloped for classification networks, but applicable to the problem domains\nbetter suited to similarity learning. The visualization shows how similarity\nnetworks that are fine-tuned learn to focus on different features. We also\ngeneralize our approach to embedding networks that use different pooling\nstrategies and provide a simple mechanism to support image similarity searches\non objects or sub-regions in the query image.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "su2014new", "year": "2014", "title":"A New Non-mds Hash Function Resisting Birthday Attack And Meet-in-the-middle Attack", "abstract": "<p>To examine the integrity and authenticity of an IP address efficiently and\neconomically, this paper proposes a new non-Merkle-Damgard structural (non-MDS)\nhash function called JUNA that is based on a multivariate permutation problem\nand an anomalous subset product problem to which no subexponential time\nsolutions are found so far. JUNA includes an initialization algorithm and a\ncompression algorithm, and converts a short message of n bits which is regarded\nas only one block into a digest of m bits, where 80 &lt;= m &lt;= 232 and 80 &lt;= m &lt;=\nn &lt;= 4096. The analysis and proof show that the new hash is one-way, weakly\ncollision-free, and strongly collision-free, and its security against existent\nattacks such as birthday attack and meet-in-the- middle attack is to O(2 ^ m).\nMoreover, a detailed proof that the new hash function is resistant to the\nbirthday attack is given. Compared with the Chaum-Heijst-Pfitzmann hash based\non a discrete logarithm problem, the new hash is lightweight, and thus it opens\na door to convenience for utilization of lightweight digital signing schemes.</p>\n", "tags": ["Independent"] },
{"key": "su2018greedy", "year": "2018", "title":"Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN", "abstract": "<p>To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>\n", "tags": ["CNN","NEURIPS","Supervised"] },
{"key": "su2021hard", "year": "2021", "title":"Hard Example Guided Hashing For Image Retrieval", "abstract": "<p>Compared with the traditional hashing methods, deep hashing methods generate\nhash codes with rich semantic information and greatly improves the performances\nin the image retrieval field. However, it is unsatisfied for current deep\nhashing methods to predict the similarity of hard examples. It exists two main\nfactors affecting the ability of learning hard examples, which are weak key\nfeatures extraction and the shortage of hard examples. In this paper, we give a\nnovel end-to-end model to extract the key feature from hard examples and obtain\nhash code with the accurate semantic information. In addition, we redesign a\nhard pair-wise loss function to assess the hard degree and update penalty\nweights of examples. It effectively alleviates the shortage problem in hard\nexamples. Experimental results on CIFAR-10 and NUS-WIDE demonstrate that our\nmodel outperformances the mainstream hashing-based image retrieval methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "su2023beyond", "year": "2023", "title":"Beyond Two-tower Matching Learning Sparse Retrievable Cross-interactions For Recommendation", "abstract": "<p>Two-tower models are a prevalent matching framework for recommendation, which\nhave been widely deployed in industrial applications. The success of two-tower\nmatching attributes to its efficiency in retrieval among a large number of\nitems, since the item tower can be precomputed and used for fast Approximate\nNearest Neighbor (ANN) search. However, it suffers two main challenges,\nincluding limited feature interaction capability and reduced accuracy in online\nserving. Existing approaches attempt to design novel late interactions instead\nof dot products, but they still fail to support complex feature interactions or\nlose retrieval efficiency. To address these challenges, we propose a new\nmatching paradigm named SparCode, which supports not only sophisticated feature\ninteractions but also efficient retrieval. Specifically, SparCode introduces an\nall-to-all interaction module to model fine-grained query-item interactions.\nBesides, we design a discrete code-based sparse inverted index jointly trained\nwith the model to achieve effective and efficient model inference. Extensive\nexperiments have been conducted on open benchmark datasets to demonstrate the\nsuperiority of our framework. The results show that SparCode significantly\nimproves the accuracy of candidate item matching while retaining the same level\nof retrieval efficiency with two-tower models. Our source code will be\navailable at MindSpore/models.</p>\n", "tags": ["ARXIV"] },
{"key": "su2024deep", "year": "2024", "title":"Deep Joint-semantics Reconstructing Hashing For Large-scale Unsupervised Cross-modal Retrieval", "abstract": "<p><img src=\"https://github.com/zzs1994/DJSRH/blob/master/page_image/DJRSH.png?raw=true\" alt=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" title=\"Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval\" /></p>\n\n<p>Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.</p>\n", "tags": ["ARXIV","Cross Modal","Has Code","Unsupervised"] },
{"key": "su2024greedy", "year": "2024", "title":"Greedy Hash Towards Fast Optimization For Accurate Hash Coding In CNN", "abstract": "<p>To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.</p>\n", "tags": ["ARXIV","CNN","Supervised"] },
{"key": "subramanya2024diskann", "year": "2024", "title":"Diskann Fast Accurate Billion-point Nearest Neighbor Search On A Single Node", "abstract": "<p>Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index, store, and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom, we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN serves &gt; 5000 queries a second with &lt; 3ms mean latency and 95%+ 1-recall@1 on a 16 core machine, where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS and IVFOADC+G+P plateau at around 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can index and serve 5 − 10x more points per node compared to state-of-the-art graph- based methods such as HNSW and NSG. Finally, as part of our overall DiskANN system, we introduce Vamana, a new graph-based ANNS index that is more versatile than the graph indices even for in-memory indices.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "sumbul2020deep", "year": "2020", "title":"Deep Learning For Image Search And Retrieval In Large Remote Sensing Archives", "abstract": "<p>This chapter presents recent advances in content based image search and\nretrieval (CBIR) systems in remote sensing (RS) for fast and accurate\ninformation discovery from massive data archives. Initially, we analyze the\nlimitations of the traditional CBIR systems that rely on the hand-crafted RS\nimage descriptors. Then, we focus our attention on the advances in RS CBIR\nsystems for which deep learning (DL) models are at the forefront. In\nparticular, we present the theoretical properties of the most recent DL based\nCBIR systems for the characterization of the complex semantic content of RS\nimages. After discussing their strengths and limitations, we present the deep\nhashing based CBIR systems that have high time-efficient search capability\nwithin huge data archives. Finally, the most promising research directions in\nRS CBIR are discussed.</p>\n", "tags": ["ARXIV","Deep Learning"] },
{"key": "sun2021place", "year": "2021", "title":"3rd Place A Global And Local Dual Retrieval Solution To Facebook AI Image Similarity Challenge", "abstract": "<p>As a basic task of computer vision, image similarity retrieval is facing the\nchallenge of large-scale data and image copy attacks. This paper presents our\n3rd place solution to the matching track of Image Similarity Challenge (ISC)\n2021 organized by Facebook AI. We propose a multi-branch retrieval method of\ncombining global descriptors and local descriptors to cover all attack cases.\nSpecifically, we attempt many strategies to optimize global descriptors,\nincluding abundant data augmentations, self-supervised learning with a single\nTransformer model, overlay detection preprocessing. Moreover, we introduce the\nrobust SIFT feature and GPU Faiss for local retrieval which makes up for the\nshortcomings of the global retrieval. Finally, KNN-matching algorithm is used\nto judge the match and merge scores. We show some ablation experiments of our\nmethod, which reveals the complementary advantages of global and local\nfeatures.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "sun2024deep", "year": "2024", "title":"Deep Normalized Cross-modal Hashing With Bi-direction Relation Reasoning", "abstract": "<p>Due to the continuous growth of large-scale multi-modal data and increasing requirements for retrieval speed, deep cross-modal hashing has gained increasing attention recently. Most of existing studies take a similarity matrix as supervision to optimize their models, and the inner product between continuous surrogates of hash codes is utilized to depict the similarity in the Hamming space. However, all of them merely consider the relevant information to build the similarity matrix, ignoring the contribution of the irrelevant one, i.e., the categories that samples do not belong to. Therefore, they cannot effectively alleviate the effect of dissimilar samples. Moreover, due to the modality distribution difference, directly utilizing continuous surrogates of hash codes to calculate similarity may induce suboptimal retrieval performance. To tackle these issues, in this paper, we propose a novel deep normalized cross-modal hashing scheme with bi-direction relation reasoning, named Bi_NCMH. Specifically, we build the multi-level semantic similarity matrix by considering bi-direction relation, i.e., consistent and inconsistent relation. It hence can holistically characterize relations among instances. Besides, we execute feature normalization on continuous surrogates of hash codes to eliminate the deviation caused by modality gap, which further reduces the negative impact of binarization on retrieval performance. Extensive experiments on two cross-modal benchmark datasets demonstrate the superiority of our model over several state-of-the-art baselines.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "sun2024soar", "year": "2024", "title":"SOAR Improved Indexing For Approximate Nearest Neighbor Search", "abstract": "<p>This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals,\na novel data indexing technique for approximate nearest neighbor (ANN) search.\nSOAR extends upon previous approaches to ANN search, such as spill trees, that\nutilize multiple redundant representations while partitioning the data to\nreduce the probability of missing a nearest neighbor during search. Rather than\ntraining and computing these redundant representations independently, however,\nSOAR uses an orthogonality-amplified residual loss, which optimizes each\nrepresentation to compensate for cases where other representations perform\npoorly. This drastically improves the overall index quality, resulting in\nstate-of-the-art ANN benchmark performance while maintaining fast indexing\ntimes and low memory consumption.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "sun2024supervised", "year": "2024", "title":"Supervised Hierarchical Cross-modal Hashing", "abstract": "<p>Recently, due to the unprecedented growth of multimedia data,\ncross-modal hashing has gained increasing attention for the\nefficient cross-media retrieval. Typically, existing methods on crossmodal hashing treat labels of one instance independently but\noverlook the correlations among labels. Indeed, in many real-world\nscenarios, like the online fashion domain, instances (items) are\nlabeled with a set of categories correlated by certain hierarchy. In\nthis paper, we propose a new end-to-end solution for supervised\ncross-modal hashing, named HiCHNet, which explicitly exploits the\nhierarchical labels of instances. In particular, by the pre-established\nlabel hierarchy, we comprehensively characterize each modality\nof the instance with a set of layer-wise hash representations. In\nessence, hash codes are encouraged to not only preserve the layerwise semantic similarities encoded by the label hierarchy, but also\nretain the hierarchical discriminative capabilities. Due to the lack\nof benchmark datasets, apart from adapting the existing dataset\nFashionVC from fashion domain, we create a dataset from the\nonline fashion platform Ssense consisting of 15, 696 image-text\npairs labeled by 32 hierarchical categories. Extensive experiments\non two real-world datasets demonstrate the superiority of our model\nover the state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "sunarso2013scalable", "year": "2013", "title":"Scalable Protein Sequence Similarity Search Using Locality-sensitive Hashing And Mapreduce", "abstract": "<p>Metagenomics is the study of environments through genetic sampling of their\nmicrobiota. Metagenomic studies produce large datasets that are estimated to\ngrow at a faster rate than the available computational capacity. A key step in\nthe study of metagenome data is sequence similarity searching which is\ncomputationally intensive over large datasets. Tools such as BLAST require\nlarge dedicated computing infrastructure to perform such analysis and may not\nbe available to every researcher.\n  In this paper, we propose a novel approach called ScalLoPS that performs\nsearching on protein sequence datasets using LSH (Locality-Sensitive Hashing)\nthat is implemented using the MapReduce distributed framework. ScalLoPS is\ndesigned to scale across computing resources sourced from cloud computing\nproviders. We present the design and implementation of ScalLoPS followed by\nevaluation with datasets derived from both traditional as well as metagenomic\nstudies. Our experiments show that with this method approximates the quality of\nBLAST results while improving the scalability of protein sequence search.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "sundaram2024streaming", "year": "2024", "title":"Streaming Similarity Search Over One Billion Tweets Using Parallel Locality-sensitive Hashing", "abstract": "<p>Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing,\nand many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kdtrees do not perform well) is Locality Sensitive Hashing (LSH), an\napproximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel\nLSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports highthroughput streaming of new data. Our approach employs several\nnovel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient\nalgorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration\nalgorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to\noptimize parameter settings. We show that on a workload where we perform similarity search on a dataset of &gt; 1 Billion tweets, with\nhundreds of millions of new tweets per day, we can achieve query times of 1–2.5 ms. We show that this is an order of magnitude faster\nthan existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH,\nwith table construction times up to 3.7x faster and query times that are 8.3x faster than a basic implementation.</p>\n", "tags": ["ARXIV","Independent","LSH","Streaming Data"] },
{"key": "svenstrup2017hash", "year": "2017", "title":"Hash Embeddings For Efficient Word Representations", "abstract": "<p>We present hash embeddings, an efficient method for representing words in a\ncontinuous vector form. A hash embedding may be seen as an interpolation\nbetween a standard word embedding and a word embedding created using a random\nhash function (the hashing trick). In hash embeddings each token is represented\nby \\(k\\) \\(d\\)-dimensional embeddings vectors and one \\(k\\) dimensional weight\nvector. The final \\(d\\) dimensional representation of the token is the product of\nthe two. Rather than fitting the embedding vectors for each token these are\nselected by the hashing trick from a shared pool of \\(B\\) embedding vectors. Our\nexperiments show that hash embeddings can easily deal with huge vocabularies\nconsisting of millions of tokens. When using a hash embedding there is no need\nto create a dictionary before training nor to perform any kind of vocabulary\npruning after training. We show that models trained using hash embeddings\nexhibit at least the same level of performance as models trained using regular\nembeddings across a wide range of tasks. Furthermore, the number of parameters\nneeded by such an embedding is only a fraction of what is required by a regular\nembedding. Since standard embeddings and embeddings constructed using the\nhashing trick are actually just special cases of a hash embedding, hash\nembeddings can be considered an extension and improvement over the existing\nregular embedding types.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "szeto2016binary", "year": "2016", "title":"Binary Codes For Tagging X-ray Images Via Deep De-noising Autoencoders", "abstract": "<p>A Content-Based Image Retrieval (CBIR) system which identifies similar\nmedical images based on a query image can assist clinicians for more accurate\ndiagnosis. The recent CBIR research trend favors the construction and use of\nbinary codes to represent images. Deep architectures could learn the non-linear\nrelationship among image pixels adaptively, allowing the automatic learning of\nhigh-level features from raw pixels. However, most of them require class\nlabels, which are expensive to obtain, particularly for medical images. The\nmethods which do not need class labels utilize a deep autoencoder for binary\nhashing, but the code construction involves a specific training algorithm and\nan ad-hoc regularization technique. In this study, we explored using a deep\nde-noising autoencoder (DDA), with a new unsupervised training scheme using\nonly backpropagation and dropout, to hash images into binary codes. We\nconducted experiments on more than 14,000 x-ray images. By using class labels\nonly for evaluating the retrieval results, we constructed a 16-bit DDA and a\n512-bit DDA independently. Comparing to other unsupervised methods, we\nsucceeded to obtain the lowest total error by using the 512-bit codes for\nretrieval via exhaustive search, and speed up 9.27 times with the use of the\n16-bit codes while keeping a comparable total error. We found that our new\ntraining scheme could reduce the total retrieval error significantly by 21.9%.\nTo further boost the image retrieval performance, we developed Radon\nAutoencoder Barcode (RABC) which are learned from the Radon projections of\nimages using a de-noising autoencoder. Experimental results demonstrated its\nsuperior performance in retrieval when it was combined with DDA binary codes.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "sète2013some", "year": "2013", "title":"Some Properties Of Faber-walsh Polynomials", "abstract": "<p>Walsh introduced a generalisation of Faber polynomials to certain compact\nsets which need not be connected. We derive several equivalent representations\nof these Faber-Walsh polynomials, analogous to representations of Faber\npolynomials. Some simple asymptotic properties of the Faber-Walsh polynomials\non the complement of the compact set are established. We further show that\nsuitably normalised Faber-Walsh polynomials are asymptotically optimal\npolynomials in the sense of [Eiermann and Niethammer 1983].</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "sète2015properties", "year": "2015", "title":"Properties And Examples Of Faber--walsh Polynomials", "abstract": "<p>The Faber–Walsh polynomials are a direct generalization of the (classical)\nFaber polynomials from simply connected sets to sets with several simply\nconnected components. In this paper we derive new properties of the\nFaber–Walsh polynomials, where we focus on results of interest in numerical\nlinear algebra, and on the relation between the Faber–Walsh polynomials and\nthe classical Faber and Chebyshev polynomials. Moreover, we present examples of\nFaber–Walsh polynomials for two real intervals as well as some non-real sets\nconsisting of several simply connected components.</p>\n", "tags": ["Independent"] },
{"key": "tabei2016scalable", "year": "2016", "title":"Scalable Similarity Search For Molecular Descriptors", "abstract": "<p>Similarity search over chemical compound databases is a fundamental task in\nthe discovery and design of novel drug-like molecules. Such databases often\nencode molecules as non-negative integer vectors, called molecular descriptors,\nwhich represent rich information on various molecular properties. While there\nexist efficient indexing structures for searching databases of binary vectors,\nsolutions for more general integer vectors are in their infancy. In this paper\nwe present a time- and space- efficient index for the problem that we call the\nsuccinct intervals-splitting tree algorithm for molecular descriptors (SITAd).\nOur approach extends efficient methods for binary-vector databases, and uses\nideas from succinct data structures. Our experiments, on a large database of\nover 40 million compounds, show SITAd significantly outperforms alternative\napproaches in practice.</p>\n", "tags": ["ARXIV"] },
{"key": "taherkhani2020error", "year": "2020", "title":"Error-corrected Margin-based Deep Cross-modal Hashing For Facial Image Retrieval", "abstract": "<p>Cross-modal hashing facilitates mapping of heterogeneous multimedia data into\na common Hamming space, which can beutilized for fast and flexible retrieval\nacross different modalities. In this paper, we propose a novel cross-modal\nhashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which\nuses a binary vector specifying the presence of certainfacial attributes as an\ninput query to retrieve relevant face images from a database. The DNDCMH\nnetwork consists of two separatecomponents: an attribute-based deep cross-modal\nhashing (ADCMH) module, which uses a margin (m)-based loss function\ntoefficiently learn compact binary codes to preserve similarity between\nmodalities in the Hamming space, and a neural error correctingdecoder (NECD),\nwhich is an error correcting decoder implemented with a neural network. The\ngoal of NECD network in DNDCMH isto error correct the hash codes generated by\nADCMH to improve the retrieval efficiency. The NECD network is trained such\nthat it hasan error correcting capability greater than or equal to the margin\n(m) of the margin-based loss function. This results in NECD cancorrect the\ncorrupted hash codes generated by ADCMH up to the Hamming distance of m. We\nhave evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing\nmethods on standard datasets to demonstrate the superiority of our method.</p>\n", "tags": ["ARXIV","Cross Modal","Image Retrieval","Supervised"] },
{"key": "takeshita2020secure", "year": "2020", "title":"Secure Single-server Nearly-identical Image Deduplication", "abstract": "<p>Cloud computing is often utilized for file storage. Clients of cloud storage\nservices want to ensure the privacy of their data, and both clients and servers\nwant to use as little storage as possible. Cross-user deduplication is one\nmethod to reduce the amount of storage a server uses. Deduplication and privacy\nare naturally conflicting goals, especially for nearly-identical (``fuzzy’’)\ndeduplication, as some information about the data must be used to perform\ndeduplication. Prior solutions thus utilize multiple servers, or only function\nfor exact deduplication. In this paper, we present a single-server protocol for\ncross-user nearly-identical deduplication based on secure locality-sensitive\nhashing (SLSH). We formally define our ideal security, and rigorously prove our\nprotocol secure against fully malicious, colluding adversaries with a proof by\nsimulation. We show experimentally that the individual parts of the protocol\nare computationally feasible, and further discuss practical issues of security\nand efficiency.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "talreja2019using", "year": "2019", "title":"Using Deep Cross Modal Hashing And Error Correcting Codes For Improving The Efficiency Of Attribute Guided Facial Image Retrieval", "abstract": "<p>With benefits of fast query speed and low storage cost, hashing-based image\nretrieval approaches have garnered considerable attention from the research\ncommunity. In this paper, we propose a novel Error-Corrected Deep Cross Modal\nHashing (CMH-ECC) method which uses a bitmap specifying the presence of certain\nfacial attributes as an input query to retrieve relevant face images from the\ndatabase. In this architecture, we generate compact hash codes using an\nend-to-end deep learning module, which effectively captures the inherent\nrelationships between the face and attribute modality. We also integrate our\ndeep learning module with forward error correction codes to further reduce the\ndistance between different modalities of the same subject. Specifically, the\nproperties of deep hashing and forward error correction codes are exploited to\ndesign a cross modal hashing framework with high retrieval performance.\nExperimental results using two standard datasets with facial attributes-image\nmodalities indicate that our CMH-ECC face image retrieval model outperforms\nmost of the current attribute-based face image retrieval approaches.</p>\n", "tags": ["ARXIV","Cross Modal","Deep Learning","Image Retrieval","Independent"] },
{"key": "talreja2019zero", "year": "2019", "title":"Zero-shot Deep Hashing And Neural Network Based Error Correction For Face Template Protection", "abstract": "<p>In this paper, we present a novel architecture that integrates a deep hashing\nframework with a neural network decoder (NND) for application to face template\nprotection. It improves upon existing face template protection techniques to\nprovide better matching performance with one-shot and multi-shot enrollment. A\nkey novelty of our proposed architecture is that the framework can also be used\nwith zero-shot enrollment. This implies that our architecture does not need to\nbe re-trained even if a new subject is to be enrolled into the system. The\nproposed architecture consists of two major components: a deep hashing (DH)\ncomponent, which is used for robust mapping of face images to their\ncorresponding intermediate binary codes, and a NND component, which corrects\nerrors in the intermediate binary codes that are caused by differences in the\nenrollment and probe biometrics due to factors such as variation in pose,\nillumination, and other factors. The final binary code generated by the NND is\nthen cryptographically hashed and stored as a secure face template in the\ndatabase. The efficacy of our approach with zero-shot, one-shot, and multi-shot\nenrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE\nface databases. With zero-shot enrollment, the system achieves approximately\n85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with\none-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at\n0.01% FAR, while providing a high level of template security.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Supervised"] },
{"key": "talreja2020deep", "year": "2020", "title":"Deep Hashing For Secure Multimodal Biometrics", "abstract": "<p>When compared to unimodal systems, multimodal biometric systems have several\nadvantages, including lower error rate, higher accuracy, and larger population\ncoverage. However, multimodal systems have an increased demand for integrity\nand privacy because they must store multiple biometric traits associated with\neach user. In this paper, we present a deep learning framework for\nfeature-level fusion that generates a secure multimodal template from each\nuser’s face and iris biometrics. We integrate a deep hashing (binarization)\ntechnique into the fusion architecture to generate a robust binary multimodal\nshared latent representation. Further, we employ a hybrid secure architecture\nby combining cancelable biometrics with secure sketch techniques and integrate\nit with a deep hashing framework, which makes it computationally prohibitive to\nforge a combination of multiple biometrics that pass the authentication. The\nefficacy of the proposed approach is shown using a multimodal database of face\nand iris and it is observed that the matching performance is improved due to\nthe fusion of multiple biometrics. Furthermore, the proposed approach also\nprovides cancelability and unlinkability of the templates along with improved\nprivacy of the biometric data. Additionally, we also test the proposed hashing\nfunction for an image retrieval application using a benchmark dataset. The main\ngoal of this paper is to develop a method for integrating multimodal fusion,\ndeep hashing, and biometric security, with an emphasis on structural data from\nmodalities like face and iris. The proposed approach is in no way a general\nbiometric security framework that can be applied to all biometric modalities,\nas further research is needed to extend the proposed framework to other\nunconstrained biometric modalities.</p>\n", "tags": ["Cross Modal","Deep Learning","Image Retrieval"] },
{"key": "tan2017supervised", "year": "2017", "title":"Supervised Hashing With End-to-end Binary Deep Neural Network", "abstract": "<p>Image hashing is a popular technique applied to large scale content-based\nvisual retrieval due to its compact and efficient binary codes. Our work\nproposes a new end-to-end deep network architecture for supervised hashing\nwhich directly learns binary codes from input images and maintains good\nproperties over binary codes such as similarity preservation, independence, and\nbalancing. Furthermore, we also propose a new learning scheme that can cope\nwith the binary constrained loss function. The proposed algorithm not only is\nscalable for learning over large-scale datasets but also outperforms\nstate-of-the-art supervised hashing methods, which are illustrated throughout\nextensive experiments from various image retrieval benchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "tan2020learning", "year": "2020", "title":"Learning To Hash With Graph Neural Networks For Recommender Systems", "abstract": "<p>Graph representation learning has attracted much attention in supporting high\nquality candidate search at scale. Despite its effectiveness in learning\nembedding vectors for objects in the user-item interaction network, the\ncomputational costs to infer users’ preferences in continuous embedding space\nare tremendous. In this work, we investigate the problem of hashing with graph\nneural networks (GNNs) for high quality retrieval, and propose a simple yet\neffective discrete representation learning framework to jointly learn\ncontinuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN)\nis presented, which consists of two components, a GNN encoder for learning node\nrepresentations, and a hash layer for encoding representations to hash codes.\nThe whole architecture is trained end-to-end by jointly optimizing two losses,\ni.e., reconstruction loss from reconstructing observed links, and ranking loss\nfrom preserving the relative ordering of hash codes. A novel discrete\noptimization strategy based on straight through estimator (STE) with guidance\nis proposed. The principal idea is to avoid gradient magnification in\nback-propagation of STE with continuous embedding guidance, in which we begin\nfrom learning an easier network that mimic the continuous embedding and let it\nevolve during the training until it finally goes back to STE. Comprehensive\nexperiments over three publicly available and one real-world Alibaba company\ndatasets demonstrate that our model not only can achieve comparable performance\ncompared with its continuous counterpart but also runs multiple times faster\nduring inference.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "tan2021bcd", "year": "2021", "title":"BCD A Cross-architecture Binary Comparison Database Experiment Using Locality Sensitive Hashing Algorithms", "abstract": "<p>Given a binary executable without source code, it is difficult to determine\nwhat each function in the binary does by reverse engineering it, and even\nharder without prior experience and context. In this paper, we performed a\ncomparison of different hashing functions’ effectiveness at detecting similar\nlifted snippets of LLVM IR code, and present the design and implementation of a\nframework for cross-architecture binary code similarity search database using\nMinHash as the chosen hashing algorithm, over SimHash, SSDEEP and TLSH. The\nmotivation is to help reverse engineers to quickly gain context of functions in\nan unknown binary by comparing it against a database of known functions. The\ncode for this project is open source and can be found at\nhttps://github.com/h4sh5/bcddb</p>\n", "tags": ["ARXIV","Has Code","Independent"] },
{"key": "tan2023fast", "year": "2023", "title":"Fast Locality Sensitive Hashing With Theoretical Guarantee", "abstract": "<p>Locality-sensitive hashing (LSH) is an effective randomized technique widely\nused in many machine learning tasks. The cost of hashing is proportional to\ndata dimensions, and thus often the performance bottleneck when dimensionality\nis high and the number of hash functions involved is large. Surprisingly,\nhowever, little work has been done to improve the efficiency of LSH\ncomputation. In this paper, we design a simple yet efficient LSH scheme, named\nFastLSH, under l2 norm. By combining random sampling and random projection,\nFastLSH reduces the time complexity from O(n) to O(m) (m&lt;n), where n is the\ndata dimensionality and m is the number of sampled dimensions. Moreover,\nFastLSH has provable LSH property, which distinguishes it from the non-LSH fast\nsketches. We conduct comprehensive experiments over a collection of real and\nsynthetic datasets for the nearest neighbor search task. Experimental results\ndemonstrate that FastLSH is on par with the state-of-the-arts in terms of\nanswer quality, space occupation and query efficiency, while enjoying up to 80x\nspeedup in hash function evaluation. We believe that FastLSH is a promising\nalternative to the classic LSH scheme.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "tan2023unfolded", "year": "2023", "title":"Unfolded Self-reconstruction LSH Towards Machine Unlearning In Approximate Nearest Neighbour Search", "abstract": "<p>Approximate nearest neighbour (ANN) search is an essential component of\nsearch engines, recommendation systems, etc. Many recent works focus on\nlearning-based data-distribution-dependent hashing and achieve good retrieval\nperformance. However, due to increasing demand for users’ privacy and security,\nwe often need to remove users’ data information from Machine Learning (ML)\nmodels to satisfy specific privacy and security requirements. This need\nrequires the ANN search algorithm to support fast online data deletion and\ninsertion. Current learning-based hashing methods need retraining the hash\nfunction, which is prohibitable due to the vast time-cost of large-scale data.\nTo address this problem, we propose a novel data-dependent hashing method named\nunfolded self-reconstruction locality-sensitive hashing (USR-LSH). Our USR-LSH\nunfolded the optimization update for instance-wise data reconstruction, which\nis better for preserving data information than data-independent LSH. Moreover,\nour USR-LSH supports fast online data deletion and insertion without\nretraining. To the best of our knowledge, we are the first to address the\nmachine unlearning of retrieval problems. Empirically, we demonstrate that\nUSR-LSH outperforms the state-of-the-art data-distribution-independent LSH in\nANN tasks in terms of precision and recall. We also show that USR-LSH has\nsignificantly faster data deletion and insertion time than learning-based\ndata-dependent hashing.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "tanaka2021fake", "year": "2021", "title":"Fake-image Detection With Robust Hashing", "abstract": "<p>In this paper, we investigate whether robust hashing has a possibility to\nrobustly detect fake-images even when multiple manipulation techniques such as\nJPEG compression are applied to images for the first time. In an experiment,\nthe proposed fake detection with robust hashing is demonstrated to outperform\nstate-of-the-art one under the use of various datasets including fake images\ngenerated with GANs.</p>\n", "tags": ["ARXIV"] },
{"key": "tang2021when", "year": "2021", "title":"When Similarity Digest Meets Vector Management System A Survey On Similarity Hash Function", "abstract": "<p>The booming vector manage system calls for feasible similarity hash function\nas a front-end to perform similarity analysis. In this paper, we make a\nsystematical survey on the existent well-known similarity hash functions to\ntease out the satisfied ones. We conclude that the similarity hash function\nMinHash and Nilsimsa can be directly marshaled into the pipeline of similarity\nanalysis using vector manage system. After that, we make a brief and empirical\ndiscussion on the performance, drawbacks of the these functions and highlight\nMinHash, the variant of SimHash and feature hashing are the best for vector\nmanagement system for large-scale similarity analysis.</p>\n", "tags": ["ARXIV","Independent","Survey Paper"] },
{"key": "tannous2007avoiding", "year": "2007", "title":"Avoiding Rotated Bitboards With Direct Lookup", "abstract": "<p>This paper describes an approach for obtaining direct access to the attacked\nsquares of sliding pieces without resorting to rotated bitboards. The technique\ninvolves creating four hash tables using the built in hash arrays from an\ninterpreted, high level language. The rank, file, and diagonal occupancy are\nfirst isolated by masking the desired portion of the board. The attacked\nsquares are then directly retrieved from the hash tables. Maintaining\nincrementally updated rotated bitboards becomes unnecessary as does all the\nupdating, mapping and shifting required to access the attacked squares.\nFinally, rotated bitboard move generation speed is compared with that of the\ndirect hash table lookup method.</p>\n", "tags": [] },
{"key": "taquet2010invariant", "year": "2010", "title":"Invariant Spectral Hashing Of Image Saliency Graph", "abstract": "<p>Image hashing is the process of associating a short vector of bits to an\nimage. The resulting summaries are useful in many applications including image\nindexing, image authentication and pattern recognition. These hashes need to be\ninvariant under transformations of the image that result in similar visual\ncontent, but should drastically differ for conceptually distinct contents. This\npaper proposes an image hashing method that is invariant under rotation,\nscaling and translation of the image. The gist of our approach relies on the\ngeometric characterization of salient point distribution in the image. This is\nachieved by the definition of a “saliency graph” connecting these points\njointly with an image intensity function on the graph nodes. An invariant hash\nis then obtained by considering the spectrum of this function in the\neigenvector basis of the Laplacian graph, that is, its graph Fourier transform.\nInterestingly, this spectrum is invariant under any relabeling of the graph\nnodes. The graph reveals geometric information of the image, making the hash\nrobust to image transformation, yet distinct for different visual content. The\nefficiency of the proposed method is assessed on a set of MRI 2-D slices and on\na database of faces.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "tatsuno2024aisaq", "year": "2024", "title":"Aisaq All-in-storage ANNS With Product Quantization For Dram-free Information Retrieval", "abstract": "<p>In approximate nearest neighbor search (ANNS) methods based on approximate\nproximity graphs, DiskANN achieves good recall-speed balance for large-scale\ndatasets using both of RAM and storage. Despite it claims to save memory usage\nby loading compressed vectors by product quantization (PQ), its memory usage\nincreases in proportion to the scale of datasets. In this paper, we propose\nAll-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the\ncompressed vectors to storage. Our method achieves \\(\\sim\\)10 MB memory usage in\nquery search even with billion-scale datasets with minor performance\ndegradation. AiSAQ also reduces the index load time before query search, which\nenables the index switch between muitiple billion-scale datasets and\nsignificantly enhances the flexibility of retrieval-augmented generation (RAG).\nThis method is applicable to all graph-based ANNS algorithms and can be\ncombined with higher-spec ANNS methods in the future.</p>\n", "tags": ["ARXIV","Graph","Quantisation"] },
{"key": "tavenard2010balancing", "year": "2010", "title":"Balancing Clusters To Reduce Response Time Variability In Large Scale Image Search", "abstract": "<p>Many algorithms for approximate nearest neighbor search in high-dimensional\nspaces partition the data into clusters. At query time, in order to avoid\nexhaustive search, an index selects the few (or a single) clusters nearest to\nthe query point. Clusters are often produced by the well-known \\(k\\)-means\napproach since it has several desirable properties. On the downside, it tends\nto produce clusters having quite different cardinalities. Imbalanced clusters\nnegatively impact both the variance and the expectation of query response\ntimes. This paper proposes to modify \\(k\\)-means centroids to produce clusters\nwith more comparable sizes without sacrificing the desirable properties.\nExperiments with a large scale collection of image descriptors show that our\nalgorithm significantly reduces the variance of response times without\nseriously impacting the search quality.</p>\n", "tags": ["ARXIV"] },
{"key": "tchayekondi2020new", "year": "2020", "title":"A New Hashing Based Nearest Neighbors Selection Technique For Big Datasets", "abstract": "<p>KNN has the reputation to be the word simplest but efficient supervised\nlearning algorithm used for either classification or regression. KNN prediction\nefficiency highly depends on the size of its training data but when this\ntraining data grows KNN suffers from slowness in making decisions since it\nneeds to search nearest neighbors within the entire dataset at each decision\nmaking. This paper proposes a new technique that enables the selection of\nnearest neighbors directly in the neighborhood of a given observation. The\nproposed approach consists of dividing the data space into subcells of a\nvirtual grid built on top of data space. The mapping between the data points\nand subcells is performed using hashing. When it comes to select the nearest\nneighbors of a given observation, we firstly identify the cell the observation\nbelongs by using hashing, and then we look for nearest neighbors from that\ncentral cell and cells around it layer by layer. From our experiment\nperformance analysis on publicly available datasets, our algorithm outperforms\nthe original KNN in time efficiency with a prediction quality as good as that\nof KNN it also offers competitive performance with solutions like KDtree</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "teixeira2013scalable", "year": "2013", "title":"Scalable Locality-sensitive Hashing For Similarity Search In High-dimensional Large-scale Multimedia Datasets", "abstract": "<p>Similarity search is critical for many database applications, including the\nincreasingly popular online services for Content-Based Multimedia Retrieval\n(CBMR). These services, which include image search engines, must handle an\noverwhelming volume of data, while keeping low response times. Thus,\nscalability is imperative for similarity search in Web-scale applications, but\nmost existing methods are sequential and target shared-memory machines. Here we\naddress these issues with a distributed, efficient, and scalable index based on\nLocality-Sensitive Hashing (LSH). LSH is one of the most efficient and popular\ntechniques for similarity search, but its poor referential locality properties\nhas made its implementation a challenging problem. Our solution is based on a\nwidely asynchronous dataflow parallelization with a number of optimizations\nthat include a hierarchical parallelization to decouple indexing and data\nstorage, locality-aware data partition strategies to reduce message passing,\nand multi-probing to limit memory usage. The proposed parallelization attained\nan efficiency of 90% in a distributed system with about 800 CPU cores. In\nparticular, the original locality-aware data partition reduced the number of\nmessages exchanged in 30%. Our parallel LSH was evaluated using the largest\npublic dataset for similarity search (to the best of our knowledge) with \\(10^9\\)\n128-d SIFT descriptors extracted from Web images. This is two orders of\nmagnitude larger than datasets that previous LSH parallelizations could handle.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "teofili2019lucene", "year": "2019", "title":"Lucene For Approximate Nearest-neighbors Search On Arbitrary Dense Vectors", "abstract": "<p>We demonstrate three approaches for adapting the open-source Lucene search\nlibrary to perform approximate nearest-neighbor search on arbitrary dense\nvectors, using similarity search on word embeddings as a case study. At its\ncore, Lucene is built around inverted indexes of a document collection’s\n(sparse) term-document matrix, which is incompatible with the lower-dimensional\ndense vectors that are common in deep learning applications. We evaluate three\ntechniques to overcome these challenges that can all be natively integrated\ninto Lucene: the creation of documents populated with fake words, LSH applied\nto lexical realizations of dense vectors, and k-d trees coupled with\ndimensionality reduction. Experiments show that the “fake words” approach\nrepresents the best balance between effectiveness and efficiency. These\ntechniques are integrated into the Anserini open-source toolkit and made\navailable to the community.</p>\n", "tags": ["ARXIV","Case Study","Deep Learning","LSH","Unsupervised"] },
{"key": "tepper2020procrustean", "year": "2020", "title":"Procrustean Orthogonal Sparse Hashing", "abstract": "<p>Hashing is one of the most popular methods for similarity search because of\nits speed and efficiency. Dense binary hashing is prevalent in the literature.\nRecently, insect olfaction was shown to be structurally and functionally\nanalogous to sparse hashing [6]. Here, we prove that this biological mechanism\nis the solution to a well-posed optimization problem. Furthermore, we show that\northogonality increases the accuracy of sparse hashing. Next, we present a\nnovel method, Procrustean Orthogonal Sparse Hashing (POSH), that unifies these\nfindings, learning an orthogonal transform from training data compatible with\nthe sparse hashing mechanism. We provide theoretical evidence of the\nshortcomings of Optimal Sparse Lifting (OSL) [22] and BioHash [30], two related\nolfaction-inspired methods, and propose two new methods, Binary OSL and\nSphericalHash, to address these deficiencies. We compare POSH, Binary OSL, and\nSphericalHash to several state-of-the-art hashing methods and provide empirical\nresults for the superiority of the proposed methods across a wide range of\nstandard benchmarks and parameter settings.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "thakur2019conv", "year": "2019", "title":"Conv-codes Audio Hashing For Bird Species Classification", "abstract": "<p>In this work, we propose a supervised, convex representation based audio\nhashing framework for bird species classification. The proposed framework\nutilizes archetypal analysis, a matrix factorization technique, to obtain\nconvex-sparse representations of a bird vocalization. These convex\nrepresentations are hashed using Bloom filters with non-cryptographic hash\nfunctions to obtain compact binary codes, designated as conv-codes. The\nconv-codes extracted from the training examples are clustered using\nclass-specific k-medoids clustering with Jaccard coefficient as the similarity\nmetric. A hash table is populated using the cluster centers as keys while hash\nvalues/slots are pointers to the species identification information. During\ntesting, the hash table is searched to find the species information\ncorresponding to a cluster center that exhibits maximum similarity with the\ntest conv-code. Hence, the proposed framework classifies a bird vocalization in\nthe conv-code space and requires no explicit classifier or reconstruction error\ncalculations. Apart from that, based on min-hash and direct addressing, we also\npropose a variant of the proposed framework that provides faster and effective\nclassification. The performances of both these frameworks are compared with\nexisting bird species classification frameworks on the audio recordings of 50\ndifferent bird species.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "thomas2022streaming", "year": "2022", "title":"Streaming Encoding Algorithms For Scalable Hyperdimensional Computing", "abstract": "<p>Hyperdimensional computing (HDC) is a paradigm for data representation and\nlearning originating in computational neuroscience. HDC represents data as\nhigh-dimensional, low-precision vectors which can be used for a variety of\ninformation processing tasks like learning or recall. The mapping to\nhigh-dimensional space is a fundamental problem in HDC, and existing methods\nencounter scalability issues when the input data itself is high-dimensional. In\nthis work, we explore a family of streaming encoding techniques based on\nhashing. We show formally that these methods enjoy comparable guarantees on\nperformance for learning applications while being substantially more efficient\nthan existing alternatives. We validate these results experimentally on a\npopular high-dimensional classification problem and show that our approach\neasily scales to very large data sets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "thorup2013bottom", "year": "2013", "title":"Bottom-k And Priority Sampling Set Similarity And Subset Sums With Minimal Independence", "abstract": "<p>We consider bottom-k sampling for a set X, picking a sample S_k(X) consisting\nof the k elements that are smallest according to a given hash function h. With\nthis sample we can estimate the relative size f=|Y|/|X| of any subset Y as\n|S_k(X) intersect Y|/k. A standard application is the estimation of the Jaccard\nsimilarity f=|A intersect B|/|A union B| between sets A and B. Given the\nbottom-k samples from A and B, we construct the bottom-k sample of their union\nas S_k(A union B)=S_k(S_k(A) union S_k(B)), and then the similarity is\nestimated as |S_k(A union B) intersect S_k(A) intersect S_k(B)|/k.\n  We show here that even if the hash function is only 2-independent, the\nexpected relative error is O(1/sqrt(fk)). For fk=Omega(1) this is within a\nconstant factor of the expected relative error with truly random hashing.\n  For comparison, consider the classic approach of kxmin-wise where we use k\nhash independent functions h_1,…,h_k, storing the smallest element with each\nhash function. For kxmin-wise there is an at least constant bias with constant\nindependence, and it is not reduced with larger k. Recently Feigenblat et al.\nshowed that bottom-k circumvents the bias if the hash function is 8-independent\nand k is sufficiently large. We get down to 2-independence for any k. Our\nresult is based on a simply union bound, transferring generic concentration\nbounds for the hashing scheme to the bottom-k sample, e.g., getting stronger\nprobability error bounds with higher independence.\n  For weighted sets, we consider priority sampling which adapts efficiently to\nthe concrete input weights, e.g., benefiting strongly from heavy-tailed input.\nThis time, the analysis is much more involved, but again we show that generic\nconcentration bounds can be applied.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "thorup2013k", "year": "2013", "title":"On The K-independence Required By Linear Probing And Minwise Independence", "abstract": "<p>We show that linear probing requires 5-independent hash functions for\nexpected constant-time performance, matching an upper bound of [Pagh et al.\nSTOC’07]. More precisely, we construct a 4-independent hash functions yielding\nexpected logarithmic search time.\n  For (1+{\\epsilon})-approximate minwise independence, we show that Ω(log\n1/{\\epsilon})-independent hash functions are required, matching an upper bound\nof [Indyk, SODA’99].\n  We also show that the very fast 2-independent multiply-shift scheme of\nDietzfelbinger [STACS’96] fails badly in both applications.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "thorup2015fast", "year": "2015", "title":"Fast And Powerful Hashing Using Tabulation", "abstract": "<p>Randomized algorithms are often enjoyed for their simplicity, but the hash\nfunctions employed to yield the desired probabilistic guarantees are often too\ncomplicated to be practical. Here we survey recent results on how simple\nhashing schemes based on tabulation provide unexpectedly strong guarantees.\n  Simple tabulation hashing dates back to Zobrist [1970]. Keys are viewed as\nconsisting of \\(c\\) characters and we have precomputed character tables\n\\(h_1,…,h_c\\) mapping characters to random hash values. A key \\(x=(x_1,…,x_c)\\)\nis hashed to \\(h_1[x_1] \\oplus h_2[x_2]…..\\oplus h_c[x_c]\\). This schemes is\nvery fast with character tables in cache. While simple tabulation is not even\n4-independent, it does provide many of the guarantees that are normally\nobtained via higher independence, e.g., linear probing and Cuckoo hashing.\n  Next we consider twisted tabulation where one input character is “twisted” in\na simple way. The resulting hash function has powerful distributional\nproperties: Chernoff-Hoeffding type tail bounds and a very small bias for\nmin-wise hashing. This also yields an extremely fast pseudo-random number\ngenerator that is provably good for many classic randomized algorithms and\ndata-structures.\n  Finally, we consider double tabulation where we compose two simple tabulation\nfunctions, applying one to the output of the other, and show that this yields\nvery high independence in the classic framework of Carter and Wegman [1977]. In\nfact, w.h.p., for a given set of size proportional to that of the space\nconsumed, double tabulation gives fully-random hashing. We also mention some\nmore elaborate tabulation schemes getting near-optimal independence for given\ntime and space.\n  While these tabulation schemes are all easy to implement and use, their\nanalysis is not.</p>\n", "tags": ["ARXIV","Independent","Survey Paper"] },
{"key": "thorup2015high", "year": "2015", "title":"High Speed Hashing For Integers And Strings", "abstract": "<p>These notes describe the most efficient hash functions currently known for\nhashing integers and strings. These modern hash functions are often an order of\nmagnitude faster than those presented in standard text books. They are also\nsimpler to implement, and hence a clear win in practice, but their analysis is\nharder. Some of the most practical hash functions have only appeared in theory\npapers, and some of them requires combining results from different theory\npapers. The goal here is to combine the information in lecture-style notes that\ncan be used by theoreticians and practitioners alike, thus making these\npractical fruits of theory more widely accessible.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "thorup2015linear", "year": "2015", "title":"Linear Probing With 5-independent Hashing", "abstract": "<p>These lecture notes show that linear probing takes expected constant time if\nthe hash function is 5-independent. This result was first proved by Pagh et al.\n[STOC’07,SICOMP’09]. The simple proof here is essentially taken from [Patrascu\nand Thorup ICALP’10]. We will also consider a smaller space version of linear\nprobing that may have false positives like Bloom filters.\n  These lecture notes illustrate the use of higher moments in data structures,\nand could be used in a course on randomized algorithms.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "tian2017semi", "year": "2017", "title":"Semi-supervised Multimodal Hashing", "abstract": "<p>Retrieving nearest neighbors across correlated data in multiple modalities,\nsuch as image-text pairs on Facebook and video-tag pairs on YouTube, has become\na challenging task due to the huge amount of data. Multimodal hashing methods\nthat embed data into binary codes can boost the retrieving speed and reduce\nstorage requirement. As unsupervised multimodal hashing methods are usually\ninferior to supervised ones, while the supervised ones requires too much\nmanually labeled data, the proposed method in this paper utilizes a part of\nlabels to design a semi-supervised multimodal hashing method. It first computes\nthe transformation matrices for data matrices and label matrix. Then, with\nthese transformation matrices, fuzzy logic is introduced to estimate a label\nmatrix for unlabeled data. Finally, it uses the estimated label matrix to learn\nhashing functions for data in each modality to generate a unified binary code\nmatrix. Experiments show that the proposed semi-supervised method with 50%\nlabels can get a medium performance among the compared supervised ones and\nachieve an approximate performance to the best supervised method with 90%\nlabels. With only 10% labels, the proposed method can still compete with the\nworst compared supervised one.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "tian2018learning", "year": "2018", "title":"Learning Decorrelated Hashing Codes For Multimodal Retrieval", "abstract": "<p>In social networks, heterogeneous multimedia data correlate to each other,\nsuch as videos and their corresponding tags in YouTube and image-text pairs in\nFacebook. Nearest neighbor retrieval across multiple modalities on large data\nsets becomes a hot yet challenging problem. Hashing is expected to be an\nefficient solution, since it represents data as binary codes. As the bit-wise\nXOR operations can be fast handled, the retrieval time is greatly reduced. Few\nexisting multimodal hashing methods consider the correlation among hashing\nbits. The correlation has negative impact on hashing codes. When the hashing\ncode length becomes longer, the retrieval performance improvement becomes\nslower. In this paper, we propose a minimum correlation regularization (MCR)\nfor multimodal hashing. First, the sigmoid function is used to embed the data\nmatrices. Then, the MCR is applied on the output of sigmoid function. As the\noutput of sigmoid function approximates a binary code matrix, the proposed MCR\ncan efficiently decorrelate the hashing codes. Experiments show the superiority\nof the proposed method becomes greater as the code length increases.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "tian2019global", "year": "2019", "title":"Global Hashing System For Fast Image Search", "abstract": "<p>Hashing methods have been widely investigated for fast approximate nearest\nneighbor searching in large data sets. Most existing methods use binary vectors\nin lower dimensional spaces to represent data points that are usually real\nvectors of higher dimensionality. We divide the hashing process into two steps.\nData points are first embedded in a low-dimensional space, and the global\npositioning system method is subsequently introduced but modified for binary\nembedding. We devise dataindependent and data-dependent methods to distribute\nthe satellites at appropriate locations. Our methods are based on finding the\ntradeoff between the information losses in these two steps. Experiments show\nthat our data-dependent method outperforms other methods in different-sized\ndata sets from 100k to 10M. By incorporating the orthogonality of the code\nmatrix, both our data-independent and data-dependent methods are particularly\nimpressive in experiments on longer bits.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "tian2021one", "year": "2021", "title":"One Loss For All Deep Hashing With A Single Cosine Similarity Based Learning Objective", "abstract": "<p>A deep hashing model typically has two main learning objectives: to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality, it is not uncommon for existing models to employ a large number (&gt;4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work, we propose a novel deep hashing model with only \\(\\textit{a single learning objective}\\). Specifically,  we show that maximizing the cosine similarity between the continuous codes and their corresponding \\(\\textit{binary orthogonal codes}\\) can ensure both hash code discriminativeness and quantization error minimization. Further, with this learning objective, code balancing can be achieved by simply using a  Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is a one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly,  extensive experiments show that our model is highly effective, outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins.</p>\n", "tags": ["NEURIPS","Quantisation","Supervised"] },
{"key": "tian2022learned", "year": "2022", "title":"A Learned Index For Exact Similarity Search In Metric Spaces", "abstract": "<p>Indexing is an effective way to support efficient query processing in large\ndatabases. Recently the concept of learned index, which replaces or complements\ntraditional index structures with machine learning models, has been actively\nexplored to reduce storage and search costs. However, accurate and efficient\nsimilarity query processing in high-dimensional metric spaces remains to be an\nopen challenge. In this paper, we propose a novel indexing approach called LIMS\nthat uses data clustering, pivot-based data transformation techniques and\nlearned indexes to support efficient similarity query processing in metric\nspaces. In LIMS, the underlying data is partitioned into clusters such that\neach cluster follows a relatively uniform data distribution. Data\nredistribution is achieved by utilizing a small number of pivots for each\ncluster. Similar data are mapped into compact regions and the mapped values are\ntotally ordinal. Machine learning models are developed to approximate the\nposition of each data record on disk. Efficient algorithms are designed for\nprocessing range queries and nearest neighbor queries based on LIMS, and for\nindex maintenance with dynamic updates. Extensive experiments on real-world and\nsynthetic datasets demonstrate the superiority of LIMS compared with\ntraditional indexes and state-of-the-art learned indexes.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "tissier2018near", "year": "2018", "title":"Near-lossless Binarization Of Word Embeddings", "abstract": "<p>Word embeddings are commonly used as a starting point in many NLP models to\nachieve state-of-the-art performances. However, with a large vocabulary and\nmany dimensions, these floating-point representations are expensive both in\nterms of memory and calculations which makes them unsuitable for use on\nlow-resource devices. The method proposed in this paper transforms real-valued\nembeddings into binary embeddings while preserving semantic information,\nrequiring only 128 or 256 bits for each vector. This leads to a small memory\nfootprint and fast vector operations. The model is based on an autoencoder\narchitecture, which also allows to reconstruct original vectors from the binary\nones. Experimental results on semantic similarity, text classification and\nsentiment analysis tasks show that the binarization of word embeddings only\nleads to a loss of ~2% in accuracy while vector size is reduced by 97%.\nFurthermore, a top-k benchmark demonstrates that using these binary vectors is\n30 times faster than using real-valued vectors.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "tito2017hash", "year": "2017", "title":"Hash Embeddings For Efficient Word Representations", "abstract": "<p>We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by \\(k\\) \\(d\\)-dimensional embeddings vectors and one \\(k\\) dimensional weight vector. The final \\(d\\) dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of \\(B\\) embedding vectors.  Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "titus2018sig", "year": "2018", "title":"SIG-DB Leveraging Homomorphic Encryption To Securely Interrogate Privately Held Genomic Databases", "abstract": "<p>Genomic data are becoming increasingly valuable as we develop methods to\nutilize the information at scale and gain a greater understanding of how\ngenetic information relates to biological function. Advances in synthetic\nbiology and the decreased cost of sequencing are increasing the amount of\nprivately held genomic data. As the quantity and value of private genomic data\ngrows, so does the incentive to acquire and protect such data, which creates a\nneed to store and process these data securely. We present an algorithm for the\nSecure Interrogation of Genomic DataBases (SIG-DB). The SIG-DB algorithm\nenables databases of genomic sequences to be searched with an encrypted query\nsequence without revealing the query sequence to the Database Owner or any of\nthe database sequences to the Querier. SIG-DB is the first application of its\nkind to take advantage of locality-sensitive hashing and homomorphic encryption\nto allow generalized sequence-to-sequence comparisons of genomic data.</p>\n", "tags": [] },
{"key": "tizhoosh2016barcodes", "year": "2016", "title":"Barcodes For Medical Image Retrieval Using Autoencoded Radon Transform", "abstract": "<p>Using content-based binary codes to tag digital images has emerged as a\npromising retrieval technology. Recently, Radon barcodes (RBCs) have been\nintroduced as a new binary descriptor for image search. RBCs are generated by\nbinarization of Radon projections and by assembling them into a vector, namely\nthe barcode. A simple local thresholding has been suggested for binarization.\nIn this paper, we put forward the idea of “autoencoded Radon barcodes”. Using\nimages in a training dataset, we autoencode Radon projections to perform\nbinarization on outputs of hidden layers. We employed the mini-batch stochastic\ngradient descent approach for the training. Each hidden layer of the\nautoencoder can produce a barcode using a threshold determined based on the\nrange of the logistic function used. The compressing capability of autoencoders\napparently reduces the redundancies inherent in Radon projections leading to\nmore accurate retrieval results. The IRMA dataset with 14,410 x-ray images is\nused to validate the performance of the proposed method. The experimental\nresults, containing comparison with RBCs, SURF and BRISK, show that autoencoded\nRadon barcode (ARBC) has the capacity to capture important information and to\nlearn richer representations resulting in lower retrieval errors for image\nretrieval measured with the accuracy of the first hit only.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "tokunaga2024pixel", "year": "2024", "title":"Pixel Embedding Fully Quantized Convolutional Neural Network With Differentiable Lookup Table", "abstract": "<p>By quantizing network weights and activations to low bitwidth, we can obtain\nhardware-friendly and energy-efficient networks. However, existing quantization\ntechniques utilizing the straight-through estimator and piecewise constant\nfunctions face the issue of how to represent originally high-bit input data\nwith low-bit values. To fully quantize deep neural networks, we propose pixel\nembedding, which replaces each float-valued input pixel with a vector of\nquantized values by using a lookup table. The lookup table or low-bit\nrepresentation of pixels is differentiable and trainable by backpropagation.\nSuch replacement of inputs with vectors is similar to word embedding in the\nnatural language processing field. Experiments on ImageNet and CIFAR-100 show\nthat pixel embedding reduces the top-5 error gap caused by quantizing the\nfloating points at the first layer to only 1% for the ImageNet dataset, and the\ntop-1 error gap caused by quantizing first and last layers to slightly over 1%\nfor the CIFAR-100 dataset. The usefulness of pixel embedding is further\ndemonstrated by inference time measurements, which demonstrate over 1.7 times\nspeedup compared to floating point precision first layer.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "tonioni2018deep", "year": "2018", "title":"A Deep Learning Pipeline For Product Recognition On Store Shelves", "abstract": "<p>Recognition of grocery products in store shelves poses peculiar challenges.\nFirstly, the task mandates the recognition of an extremely high number of\ndifferent items, in the order of several thousands for medium-small shops, with\nmany of them featuring small inter and intra class variability. Then, available\nproduct databases usually include just one or a few studio-quality images per\nproduct (referred to herein as reference images), whilst at test time\nrecognition is performed on pictures displaying a portion of a shelf containing\nseveral products and taken in the store by cheap cameras (referred to as query\nimages). Moreover, as the items on sale in a store as well as their appearance\nchange frequently over time, a practical recognition system should handle\nseamlessly new products/packages. Inspired by recent advances in object\ndetection and image retrieval, we propose to leverage on state of the art\nobject detectors based on deep learning to obtain an initial productagnostic\nitem detection. Then, we pursue product recognition through a similarity search\nbetween global descriptors computed on reference and cropped query images. To\nmaximize performance, we learn an ad-hoc global descriptor by a CNN trained on\nreference images based on an image embedding loss. Our system is\ncomputationally expensive at training time but can perform recognition rapidly\nand accurately at test time.</p>\n", "tags": ["ARXIV","CNN","Deep Learning","Image Retrieval"] },
{"key": "torralba2024million", "year": "2024", "title":"80 Million Tiny Images A Large Dataset For Non-parametric Object And Scene Recognition", "abstract": "<p>With the advent of the Internet, billions of images\nare now freely available online and constitute a dense sampling\nof the visual world. Using a variety of non-parametric methods,\nwe explore this world with the aid of a large dataset of 79,302,017\nimages collected from the Web. Motivated by psychophysical\nresults showing the remarkable tolerance of the human visual\nsystem to degradations in image resolution, the images in the\ndataset are stored as 32 × 32 color images. Each image is\nloosely labeled with one of the 75,062 non-abstract nouns in\nEnglish, as listed in the Wordnet lexical database. Hence the\nimage database gives a comprehensive coverage of all object\ncategories and scenes. The semantic information from Wordnet\ncan be used in conjunction with nearest-neighbor methods to\nperform object classification over a range of semantic levels\nminimizing the effects of labeling noise. For certain classes that\nare particularly prevalent in the dataset, such as people, we are\nable to demonstrate a recognition performance comparable to\nclass-specific Viola-Jones style detectors.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "torresxirau2024fast", "year": "2024", "title":"Fast Approximate Nearest-neighbor Field By Cascaded Spherical Hashing", "abstract": "<p>We present an efficient and fast algorithm for computing approximate nearest neighbor fields between two images. Our method builds on the concept of Coherency-Sensitive Hashing (CSH), but uses a recent hashing scheme, Spherical Hashing (SpH), which is known to be better adapted to the nearest-neighbor problem for natural images. Cascaded Spherical Hashing concatenates different configurations of SpH to build larger Hash Tables with less elements in each bin to achieve higher selectivity. Our method amply outperforms existing techniques like PatchMatch and CSH, and the experimental results show that our algorithm is faster and more accurate than existing methods.</p>\n", "tags": ["ARXIV"] },
{"key": "tripathy2021scalable", "year": "2021", "title":"Scalable Hash Table For NUMA Systems", "abstract": "<p>Hash tables are used in a plethora of applications, including database\noperations, DNA sequencing, string searching, and many more. As such, there are\nmany parallelized hash tables targeting multicore, distributed, and\naccelerator-based systems. We present in this work a multi-GPU hash table\nimplementation that can process keys at a throughput comparable to that of\ndistributed hash tables. Distributed CPU hash tables have received\nsignificantly more attention than GPU-based hash tables. We show that a single\nnode with multiple GPUs offers roughly the same performance as a 500-1,000-core\nCPU-based cluster. Our algorithm’s key component is our use of multiple\nsparse-graph data structures and binning techniques to build the hash table. As\nhas been shown individually, these components can be written with massive\nparallelism that is amenable to GPU acceleration. Since we focus on an\nindividual node, we also leverage communication primitives that are typically\nprohibitive in distributed environments. We show that our new multi-GPU\nalgorithm shares many of the same features of the single GPU algorithm – thus\nwe have efficient collision management capabilities and can deal with a large\nnumber of duplicates. We evaluate our algorithm on two multi-GPU compute nodes:\n1) an NVIDIA DGX2 server with 16 GPUs and 2) an IBM Power 9 Processor with 6\nNVIDIA GPUs. With 32-bit keys, our implementation processes 8B keys per second,\ncomparable to some 500-1,000-core CPU-based clusters and 4X faster than prior\nsingle-GPU implementations.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "trivigno2023divideclassify", "year": "2023", "title":"Divideclassify Fine-grained Classification For City-wide Visual Place Recognition", "abstract": "<p>Visual Place recognition is commonly addressed as an image retrieval problem.\nHowever, retrieval methods are impractical to scale to large datasets, densely\nsampled from city-wide maps, since their dimension impact negatively on the\ninference time. Using approximate nearest neighbour search for retrieval helps\nto mitigate this issue, at the cost of a performance drop. In this paper we\ninvestigate whether we can effectively approach this task as a classification\nproblem, thus bypassing the need for a similarity search. We find that existing\nclassification methods for coarse, planet-wide localization are not suitable\nfor the fine-grained and city-wide setting. This is largely due to how the\ndataset is split into classes, because these methods are designed to handle a\nsparse distribution of photos and as such do not consider the visual aliasing\nproblem across neighbouring classes that naturally arises in dense scenarios.\nThus, we propose a partitioning scheme that enables a fast and accurate\ninference, preserving a simple learning procedure, and a novel inference\npipeline based on an ensemble of novel classifiers that uses the prototypes\nlearned via an angular margin loss. Our method, Divide&amp;Classify (D&amp;C), enjoys\nthe fast inference of classification solutions and an accuracy competitive with\nretrieval methods on the fine-grained, city-wide setting. Moreover, we show\nthat D&amp;C can be paired with existing retrieval pipelines to speed up\ncomputations by over 20 times while increasing their recall, leading to new\nstate-of-the-art results.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "tsang2022clustering", "year": "2022", "title":"Clustering The Sketch A Novel Approach To Embedding Table Compression", "abstract": "<p>Embedding tables are used by machine learning systems to work with\ncategorical features. In modern Recommendation Systems, these tables can be\nvery large, necessitating the development of new methods for fitting them in\nmemory, even during training. We suggest Clustered Compositional Embeddings\n(CCE) which combines clustering-based compression like quantization to\ncodebooks with dynamic methods like The Hashing Trick and Compositional\nEmbeddings (Shi et al., 2020). Experimentally CCE achieves the best of both\nworlds: The high compression rate of codebook-based quantization, but\n<em>dynamically</em> like hashing-based methods, so it can be used during training.\nTheoretically, we prove that CCE is guaranteed to converge to the optimal\ncodebook and give a tight bound for the number of iterations required.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "tseng2020parallel", "year": "2020", "title":"Parallel Index-based Structural Graph Clustering And Its Approximation", "abstract": "<p>SCAN (Structural Clustering Algorithm for Networks) is a well-studied, widely\nused graph clustering algorithm. For large graphs, however, sequential SCAN\nvariants are prohibitively slow, and parallel SCAN variants do not effectively\nshare work among queries with different SCAN parameter settings. Since users of\nSCAN often explore many parameter settings to find good clusterings, it is\nworthwhile to precompute an index that speeds up queries.\n  This paper presents a practical and provably efficient parallel index-based\nSCAN algorithm based on GS<em>-Index, a recent sequential algorithm. Our parallel\nalgorithm improves upon the asymptotic work of the sequential algorithm by\nusing integer sorting. It is also highly parallel, achieving logarithmic span\n(parallel time) for both index construction and clustering queries.\nFurthermore, we apply locality-sensitive hashing (LSH) to design a novel\napproximate SCAN algorithm and prove guarantees for its clustering behavior.\n  We present an experimental evaluation of our algorithms on large real-world\ngraphs. On a 48-core machine with two-way hyper-threading, our parallel index\nconstruction achieves 50–151\\(\\times\\) speedup over the construction of\nGS</em>-Index. In fact, even on a single thread, our index construction algorithm\nis faster than GS<em>-Index. Our parallel index query implementation achieves\n5–32\\(\\times\\) speedup over GS</em>-Index queries across a range of SCAN parameter\nvalues, and our implementation is always faster than ppSCAN, a state-of-the-art\nparallel SCAN algorithm. Moreover, our experiments show that applying LSH\nresults in faster index construction while maintaining good clustering quality.</p>\n", "tags": ["ARXIV","Graph","LSH","Unsupervised"] },
{"key": "tu2018object", "year": "2018", "title":"Object Detection Based Deep Unsupervised Hashing", "abstract": "<p>Recently, similarity-preserving hashing methods have been extensively studied\nfor large-scale image retrieval. Compared with unsupervised hashing, supervised\nhashing methods for labeled data have usually better performance by utilizing\nsemantic label information. Intuitively, for unlabeled data, it will improve\nthe performance of unsupervised hashing methods if we can first mine some\nsupervised semantic ‘label information’ from unlabeled data and then\nincorporate the ‘label information’ into the training process. Thus, in this\npaper, we propose a novel Object Detection based Deep Unsupervised Hashing\nmethod (ODDUH). Specifically, a pre-trained object detection model is utilized\nto mining supervised ‘label information’, which is used to guide the learning\nprocess to generate high-quality hash codes.Extensive experiments on two public\ndatasets demonstrate that the proposed method outperforms the state-of-the-art\nunsupervised hashing methods in the image retrieval task.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "tu2019deep", "year": "2019", "title":"Deep Cross-modal Hashing With Hashing Functions And Unified Hash Codes Jointly Learning", "abstract": "<p>Due to their high retrieval efficiency and low storage cost, cross-modal\nhashing methods have attracted considerable attention. Generally, compared with\nshallow cross-modal hashing methods, deep cross-modal hashing methods can\nachieve a more satisfactory performance by integrating feature learning and\nhash codes optimizing into a same framework. However, most existing deep\ncross-modal hashing methods either cannot learn a unified hash code for the two\ncorrelated data-points of different modalities in a database instance or cannot\nguide the learning of unified hash codes by the feedback of hashing function\nlearning procedure, to enhance the retrieval accuracy. To address the issues\nabove, in this paper, we propose a novel end-to-end Deep Cross-Modal Hashing\nwith Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC).\nSpecifically, by an iterative optimization algorithm, DCHUC jointly learns\nunified hash codes for image-text pairs in a database and a pair of hash\nfunctions for unseen query image-text pairs. With the iterative optimization\nalgorithm, the learned unified hash codes can be used to guide the hashing\nfunction learning procedure; Meanwhile, the learned hashing functions can\nfeedback to guide the unified hash codes optimizing procedure. Extensive\nexperiments on three public datasets demonstrate that the proposed method\noutperforms the state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "tu2020deep", "year": "2020", "title":"Deep Cross-modal Hashing Via Margin-dynamic-softmax Loss", "abstract": "<p>Due to their high retrieval efficiency and low storage cost for cross-modal\nsearch task, cross-modal hashing methods have attracted considerable attention.\nFor the supervised cross-modal hashing methods, how to make the learned hash\ncodes preserve semantic information sufficiently contained in the label of\ndatapoints is the key to further enhance the retrieval performance. Hence,\nalmost all supervised cross-modal hashing methods usually depends on defining a\nsimilarity between datapoints with the label information to guide the hashing\nmodel learning fully or partly. However, the defined similarity between\ndatapoints can only capture the label information of datapoints partially and\nmisses abundant semantic information, then hinders the further improvement of\nretrieval performance. Thus, in this paper, different from previous works, we\npropose a novel cross-modal hashing method without defining the similarity\nbetween datapoints, called Deep Cross-modal Hashing via\n\\textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains\na proxy hashing network to transform each category information of a dataset\ninto a semantic discriminative hash code, called proxy hash code. Each proxy\nhash code can preserve the semantic information of its corresponding category\nwell. Next, without defining the similarity between datapoints to supervise the\ntraining process of the modality-specific hashing networks , we propose a novel\n\\textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing\ncodes as supervised information. Finally, by minimizing the novel\n\\textit{margin-dynamic-softmax loss}, the modality-specific hashing networks\ncan be trained to generate hash codes which can simultaneously preserve the\ncross-modal similarity and abundant semantic information well.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "tu2022unsupervised", "year": "2022", "title":"Unsupervised Hashing With Semantic Concept Mining", "abstract": "<p>Recently, to improve the unsupervised image retrieval performance, plenty of\nunsupervised hashing methods have been proposed by designing a semantic\nsimilarity matrix, which is based on the similarities between image features\nextracted by a pre-trained CNN model. However, most of these methods tend to\nignore high-level abstract semantic concepts contained in images. Intuitively,\nconcepts play an important role in calculating the similarity among images. In\nreal-world scenarios, each image is associated with some concepts, and the\nsimilarity between two images will be larger if they share more identical\nconcepts. Inspired by the above intuition, in this work, we propose a novel\nUnsupervised Hashing with Semantic Concept Mining, called UHSCM, which\nleverages a VLP model to construct a high-quality similarity matrix.\nSpecifically, a set of randomly chosen concepts is first collected. Then, by\nemploying a vision-language pretraining (VLP) model with the prompt engineering\nwhich has shown strong power in visual representation learning, the set of\nconcepts is denoised according to the training images. Next, the proposed\nmethod UHSCM applies the VLP model with prompting again to mine the concept\ndistribution of each image and construct a high-quality semantic similarity\nmatrix based on the mined concept distributions. Finally, with the semantic\nsimilarity matrix as guiding information, a novel hashing loss with a modified\ncontrastive loss based regularization item is proposed to optimize the hashing\nnetwork. Extensive experiments on three benchmark datasets show that the\nproposed method outperforms the state-of-the-art baselines in the image\nretrieval task.</p>\n", "tags": ["ARXIV","CNN","Cross Modal","Image Retrieval","Unsupervised"] },
{"key": "turati2023locality", "year": "2023", "title":"Locality-sensitive Hashing Does Not Guarantee Privacy! Attacks On Googles Floc And The Minhash Hierarchy System", "abstract": "<p>Recently proposed systems aim at achieving privacy using locality-sensitive\nhashing. We show how these approaches fail by presenting attacks against two\nsuch systems: Google’s FLoC proposal for privacy-preserving targeted\nadvertising and the MinHash Hierarchy, a system for processing mobile users’\ntraffic behavior in a privacy-preserving way. Our attacks refute the pre-image\nresistance, anonymity, and privacy guarantees claimed for these systems.\n  In the case of FLoC, we show how to deanonymize users using Sybil attacks and\nto reconstruct 10% or more of the browsing history for 30% of its users using\nGenerative Adversarial Networks. We achieve this only analyzing the hashes used\nby FLoC. For MinHash, we precisely identify the movement of a subset of\nindividuals and, on average, we can limit users’ movement to just 10% of the\npossible geographic area, again using just the hashes. In addition, we refute\ntheir differential privacy claims.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "turenne2015svcr", "year": "2015", "title":"Svcr An R Package For Support Vector Clustering Improved With Geometric Hashing Applied To Lexical Pattern Discovery", "abstract": "<p>We present a new R package which takes a numerical matrix format as data\ninput, and computes clusters using a support vector clustering method (SVC). We\nhave implemented an original 2D-grid labeling approach to speed up cluster\nextraction. In this sense, SVC can be seen as an efficient cluster extraction\nif clusters are separable in a 2-D map. Secondly we showed that this SVC\napproach using a Jaccard-Radial base kernel can help to classify well enough a\nset of terms into ontological classes and help to define regular expression\nrules for information extraction in documents; our case study concerns a set of\nterms and documents about developmental and molecular biology.</p>\n", "tags": ["ARXIV","Case Study","Unsupervised"] },
{"key": "têtu2019standalone", "year": "2019", "title":"A Standalone Fpga-based Miner For Lyra2rev2 Cryptocurrencies", "abstract": "<p>Lyra2REv2 is a hashing algorithm that consists of a chain of individual\nhashing algorithms, and it is used as a proof-of-work function in several\ncryptocurrencies. The most crucial and exotic hashing algorithm in the\nLyra2REv2 chain is a specific instance of the general Lyra2 algorithm. This\nwork presents the first hardware implementation of the specific instance of\nLyra2 that is used in Lyra2REv2. Several properties of the aforementioned\nalgorithm are exploited in order to optimize the design. In addition, an\nFPGA-based hardware implementation of a standalone miner for Lyra2REv2 on a\nXilinx Multi-Processor System on Chip is presented. The proposed Lyra2REv2\nminer is shown to be significantly more energy efficient than both a GPU and a\ncommercially available FPGA-based miner. Finally, we also explain how the\nsimplified Lyra2 and Lyra2REv2 architectures can be modified with minimal\neffort to also support the recent Lyra2REv3 chained hashing algorithm.</p>\n", "tags": ["ARXIV"] },
{"key": "tětek2021edge", "year": "2021", "title":"Edge Sampling And Graph Parameter Estimation Via Vertex Neighborhood Accesses", "abstract": "<p>In this paper, we consider the problems from the area of sublinear-time\nalgorithms of edge sampling, edge counting, and triangle counting. Part of our\ncontribution is that we consider three different settings, differing in the way\nin which one may access the neighborhood of a given vertex. In previous work,\npeople have considered indexed neighbor access, with a query returning the\n\\(i\\)-th neighbor of a given vertex. Full neighborhood access model, which has a\nquery that returns the entire neighborhood at a unit cost, has recently been\nconsidered in the applied community. Between these, we propose hash-ordered\nneighbor access, inspired by coordinated sampling, where we have a global fully\nrandom hash function, and can access neighbors in order of their hash values,\npaying a constant for each accessed neighbor.\n  For edge sampling and counting, our new lower bounds are in the most powerful\nfull neighborhood access model. We provide matching upper bounds in the weaker\nhash-ordered neighbor access model. Our new faster algorithms can be provably\nimplemented efficiently on massive graphs in external memory and with the\ncurrent APIs for, e.g., Twitter or Wikipedia. For triangle counting, we provide\na separation: a better upper bound with full neighborhood access than the known\nlower bounds with indexed neighbor access. The technical core of our paper is\nour edge-sampling algorithm on which the other results depend.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "um2023vector", "year": "2023", "title":"Vector Embeddings By Sequence Similarity And Context For Improved Compression Similarity Search Clustering Organization And Manipulation Of Cdna Libraries", "abstract": "<p>This paper demonstrates the utility of organized numerical representations of\ngenes in research involving flat string gene formats (i.e., FASTA/FASTQ5).\nFASTA/FASTQ files have several current limitations, such as their large file\nsizes, slow processing speeds for mapping and alignment, and contextual\ndependencies. These challenges significantly hinder investigations and tasks\nthat involve finding similar sequences. The solution lies in transforming\nsequences into an alternative representation that facilitates easier clustering\ninto similar groups compared to the raw sequences themselves. By assigning a\nunique vector embedding to each short sequence, it is possible to more\nefficiently cluster and improve upon compression performance for the string\nrepresentations of cDNA libraries. Furthermore, through learning alternative\ncoordinate vector embeddings based on the contexts of codon triplets, we can\ndemonstrate clustering based on amino acid properties. Finally, using this\nsequence embedding method to encode barcodes and cDNA sequences, we can improve\nthe time complexity of the similarity search by coupling vector embeddings with\nan algorithm that determines the proximity of vectors in Euclidean space; this\nallows us to perform sequence similarity searches in a quicker and more modular\nfashion.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "vaiwsri2021accurate", "year": "2021", "title":"Accurate And Efficient Suffix Tree Based Privacy-preserving String Matching", "abstract": "<p>The task of calculating similarities between strings held by different\norganizations without revealing these strings is an increasingly important\nproblem in areas such as health informatics, national censuses, genomics, and\nfraud detection. Most existing privacy-preserving string comparison functions\nare either based on comparing sets of encoded character q-grams, allow only\nexact matching of encrypted strings, or they are aimed at long genomic\nsequences that have a small alphabet. The set-based privacy-preserving\nsimilarity functions commonly used to compare name and address strings in the\ncontext of privacy-preserving record linkage do not take the positions of\nsub-strings into account. As a result, two very different strings can\npotentially be considered as an exact match leading to wrongly linked records.\nExisting set-based techniques also cannot identify the length of the longest\ncommon sub-string across two strings. In this paper we propose a novel approach\nfor accurate and efficient privacy-preserving string matching based on suffix\ntrees that are encoded using chained hashing. We incorporate a hashing based\nencoding technique upon the encoded suffixes to improve privacy against\nfrequency attacks such as those exploiting Benford’s law. Our approach allows\nvarious operations to be performed without the strings to be compared being\nrevealed: the length of the longest common sub-string, do two strings have the\nsame beginning, middle or end, and the longest common sub-string similarity\nbetween two strings. These functions allow a more accurate comparison of, for\nexample, bank account, credit card, or telephone numbers, which cannot be\ncompared appropriately with existing privacy-preserving string matching\ntechniques. Our evaluation on several data sets with different types of strings\nvalidates the privacy and accuracy of our proposed approach.</p>\n", "tags": ["ARXIV"] },
{"key": "vaizman2013codebook", "year": "2013", "title":"Codebook Based Audio Feature Representation For Music Information Retrieval", "abstract": "<p>Digital music has become prolific in the web in recent decades. Automated\nrecommendation systems are essential for users to discover music they love and\nfor artists to reach appropriate audience. When manual annotations and user\npreference data is lacking (e.g. for new artists) these systems must rely on\n<em>content based</em> methods. Besides powerful machine learning tools for\nclassification and retrieval, a key component for successful recommendation is\nthe <em>audio content representation</em>.\n  Good representations should capture informative musical patterns in the audio\nsignal of songs. These representations should be concise, to enable efficient\n(low storage, easy indexing, fast search) management of huge music\nrepositories, and should also be easy and fast to compute, to enable real-time\ninteraction with a user supplying new songs to the system.\n  Before designing new audio features, we explore the usage of traditional\nlocal features, while adding a stage of encoding with a pre-computed\n<em>codebook</em> and a stage of pooling to get compact vectorial\nrepresentations. We experiment with different encoding methods, namely\n<em>the LASSO</em>, <em>vector quantization (VQ)</em> and <em>cosine similarity\n(CS)</em>. We evaluate the representations’ quality in two music information\nretrieval applications: query-by-tag and query-by-example. Our results show\nthat concise representations can be used for successful performance in both\napplications. We recommend using top-\\(\\tau\\) VQ encoding, which consistently\nperforms well in both applications, and requires much less computation time\nthan the LASSO.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "valdenegrotoro2019implementing", "year": "2019", "title":"Implementing Noise With Hash Functions For Graphics Processing Units", "abstract": "<p>We propose a modification to Perlin noise which use computable hash functions\ninstead of textures as lookup tables. We implemented the FNV1, Jenkins and\nMurmur hashes on Shader Model 4.0 Graphics Processing Units for noise\ngeneration. Modified versions of the FNV1 and Jenkins hashes provide very close\nperformance compared to a texture based Perlin noise implementation. Our noise\nmodification enables noise function evaluation without any texture fetches,\ntrading computational power for memory bandwidth.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "valsesia2019analysis", "year": "2019", "title":"Analysis Of Sparsehash An Efficient Embedding Of Set-similarity Via Sparse Projections", "abstract": "<p>Embeddings provide compact representations of signals in order to perform\nefficient inference in a wide variety of tasks. In particular, random\nprojections are common tools to construct Euclidean distance-preserving\nembeddings, while hashing techniques are extensively used to embed\nset-similarity metrics, such as the Jaccard coefficient. In this letter, we\ntheoretically prove that a class of random projections based on sparse\nmatrices, called SparseHash, can preserve the Jaccard coefficient between the\nsupports of sparse signals, which can be used to estimate set similarities.\nMoreover, besides the analysis, we provide an efficient implementation and we\ntest the performance in several numerical experiments, both on synthetic and\nreal datasets.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "venkataramanan2023integrating", "year": "2023", "title":"Integrating Visual And Semantic Similarity Using Hierarchies For Image Retrieval", "abstract": "<p>Most of the research in content-based image retrieval (CBIR) focus on\ndeveloping robust feature representations that can effectively retrieve\ninstances from a database of images that are visually similar to a query.\nHowever, the retrieved images sometimes contain results that are not\nsemantically related to the query. To address this, we propose a method for\nCBIR that captures both visual and semantic similarity using a visual\nhierarchy. The hierarchy is constructed by merging classes with overlapping\nfeatures in the latent space of a deep neural network trained for\nclassification, assuming that overlapping classes share high visual and\nsemantic similarities. Finally, the constructed hierarchy is integrated into\nthe distance calculation metric for similarity search. Experiments on standard\ndatasets: CUB-200-2011 and CIFAR100, and a real-life use case using diatom\nmicroscopy images show that our method achieves superior performance compared\nto the existing methods on image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "venkateswara2017deep", "year": "2017", "title":"Deep Hashing Network For Unsupervised Domain Adaptation", "abstract": "<p>In recent years, deep neural networks have emerged as a dominant machine\nlearning tool for a wide variety of application domains. However, training a\ndeep neural network requires a large amount of labeled data, which is an\nexpensive process in terms of time, labor and human expertise. Domain\nadaptation or transfer learning algorithms address this challenge by leveraging\nlabeled data in a different, but related source domain, to develop a model for\nthe target domain. Further, the explosive growth of digital data has posed a\nfundamental challenge concerning its storage and retrieval. Due to its storage\nand retrieval efficiency, recent years have witnessed a wide application of\nhashing in a variety of computer vision applications. In this paper, we first\nintroduce a new dataset, Office-Home, to evaluate domain adaptation algorithms.\nThe dataset contains images of a variety of everyday objects from multiple\ndomains. We then propose a novel deep learning framework that can exploit\nlabeled source data and unlabeled target data to learn informative hash codes,\nto accurately classify unseen target data. To the best of our knowledge, this\nis the first research effort to exploit the feature learning capabilities of\ndeep neural networks to learn representative hash codes to address the domain\nadaptation problem. Our extensive empirical studies on multiple transfer tasks\ncorroborate the usefulness of the framework in learning efficient hash codes\nwhich outperform existing competitive baselines for unsupervised domain\nadaptation.</p>\n", "tags": ["ARXIV","Deep Learning","Unsupervised"] },
{"key": "verdoliva2015reliable", "year": "2015", "title":"A Reliable Order-statistics-based Approximate Nearest Neighbor Search Algorithm", "abstract": "<p>We propose a new algorithm for fast approximate nearest neighbor search based\non the properties of ordered vectors. Data vectors are classified based on the\nindex and sign of their largest components, thereby partitioning the space in a\nnumber of cones centered in the origin. The query is itself classified, and the\nsearch starts from the selected cone and proceeds to neighboring ones. Overall,\nthe proposed algorithm corresponds to locality sensitive hashing in the space\nof directions, with hashing based on the order of components. Thanks to the\nstatistical features emerging through ordering, it deals very well with the\nchallenging case of unstructured data, and is a valuable building block for\nmore complex techniques dealing with structured data. Experiments on both\nsimulated and real-world data prove the proposed algorithm to provide a\nstate-of-the-art performance.</p>\n", "tags": ["ARXIV"] },
{"key": "verma2024improving", "year": "2024", "title":"Improving LSH Via Tensorized Random Projection", "abstract": "<p>Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by\ndata scientists for approximate nearest neighbour search problems that have\nbeen used extensively in many large scale data processing applications such as\nnear duplicate detection, nearest neighbour search, clustering, etc. In this\nwork, we aim to propose faster and space efficient locality sensitive hash\nfunctions for Euclidean distance and cosine similarity for tensor data.\nTypically, the naive approach for obtaining LSH for tensor data involves first\nreshaping the tensor into vectors, followed by applying existing LSH methods\nfor vector data \\(E2LSH\\) and \\(SRP\\). However, this approach becomes impractical\nfor higher order tensors because the size of the reshaped vector becomes\nexponential in the order of the tensor. Consequently, the size of LSH\nparameters increases exponentially. To address this problem, we suggest two\nmethods for LSH for Euclidean distance and cosine similarity, namely\n\\(CP-E2LSH\\), \\(TT-E2LSH\\), and \\(CP-SRP\\), \\(TT-SRP\\), respectively, building on \\(CP\\)\nand tensor train \\((TT)\\) decompositions techniques. Our approaches are space\nefficient and can be efficiently applied to low rank \\(CP\\) or \\(TT\\) tensors. We\nprovide a rigorous theoretical analysis of our proposal on their correctness\nand efficacy.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "vooturi2017efficient", "year": "2017", "title":"Efficient Inferencing Of Compressed Deep Neural Networks", "abstract": "<p>Large number of weights in deep neural networks makes the models difficult to\nbe deployed in low memory environments such as, mobile phones, IOT edge devices\nas well as “inferencing as a service” environments on cloud. Prior work has\nconsidered reduction in the size of the models, through compression techniques\nlike pruning, quantization, Huffman encoding etc. However, efficient\ninferencing using the compressed models has received little attention,\nspecially with the Huffman encoding in place. In this paper, we propose\nefficient parallel algorithms for inferencing of single image and batches,\nunder various memory constraints. Our experimental results show that our\napproach of using variable batch size for inferencing achieves 15-25\\%\nperformance improvement in the inference throughput for AlexNet, while\nmaintaining memory and latency constraints.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "vulić2020multi", "year": "2020", "title":"Multi-simlex A Large-scale Evaluation Of Multilingual And Cross-lingual Lexical Semantic Similarity", "abstract": "<p>We introduce Multi-SimLex, a large-scale lexical resource and evaluation\nbenchmark covering datasets for 12 typologically diverse languages, including\nmajor languages (e.g., Mandarin Chinese, Spanish, Russian) as well as\nless-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is\nannotated for the lexical relation of semantic similarity and contains 1,888\nsemantically aligned concept pairs, providing a representative coverage of word\nclasses (nouns, verbs, adjectives, adverbs), frequency ranks, similarity\nintervals, lexical fields, and concreteness levels. Additionally, owing to the\nalignment of concepts across languages, we provide a suite of 66 cross-lingual\nsemantic similarity datasets. Due to its extensive size and language coverage,\nMulti-SimLex provides entirely novel opportunities for experimental evaluation\nand analysis. On its monolingual and cross-lingual benchmarks, we evaluate and\nanalyze a wide array of recent state-of-the-art monolingual and cross-lingual\nrepresentation models, including static and contextualized word embeddings\n(such as fastText, M-BERT and XLM), externally informed lexical\nrepresentations, as well as fully unsupervised and (weakly) supervised\ncross-lingual word embeddings. We also present a step-by-step dataset creation\nprotocol for creating consistent, Multi-Simlex-style resources for additional\nlanguages. We make these contributions – the public release of Multi-SimLex\ndatasets, their creation protocol, strong baseline results, and in-depth\nanalyses which can be be helpful in guiding future developments in multilingual\nlexical semantics and representation learning – available via a website which\nwill encourage community effort in further expansion of Multi-Simlex to many\nmore languages. Such a large-scale semantic resource could inspire significant\nfurther advances in NLP across languages.</p>\n", "tags": ["ARXIV","Supervised","Weakly Supervised"] },
{"key": "wagner2023fast", "year": "2023", "title":"Fast Private Kernel Density Estimation Via Locality Sensitive Quantization", "abstract": "<p>We study efficient mechanisms for differentially private kernel density\nestimation (DP-KDE). Prior work for the Gaussian kernel described algorithms\nthat run in time exponential in the number of dimensions \\(d\\). This paper breaks\nthe exponential barrier, and shows how the KDE can privately be approximated in\ntime linear in \\(d\\), making it feasible for high-dimensional data. We also\npresent improved bounds for low-dimensional data.\n  Our results are obtained through a general framework, which we term Locality\nSensitive Quantization (LSQ), for constructing private KDE mechanisms where\nexisting KDE approximation techniques can be applied. It lets us leverage\nseveral efficient non-private KDE methods – like Random Fourier Features, the\nFast Gauss Transform, and Locality Sensitive Hashing – and ``privatize’’ them\nin a black-box manner. Our experiments demonstrate that our resulting DP-KDE\nmechanisms are fast and accurate on large datasets in both high and low\ndimensions.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "walzer2023what", "year": "2023", "title":"What If We Tried Less Power -- Lessons From Studying The Power Of Choices In Hashing-based Data Structures", "abstract": "<p>In the first part of this survey, we review how the power of two choices\nunderlies space-efficient data structures like cuckoo hash tables. We’ll find\nthat the additional power afforded by more than 2 choices is often outweighed\nby the additional costs they bring. In the second part, we present a data\nstructure where choices play a role at coarser than per-element granularity. In\nsome sense, we rely on the power of \\(1+\\epsilon\\) choices.</p>\n", "tags": ["ARXIV","Survey Paper"] },
{"key": "wan2015hdidx", "year": "2015", "title":"Hdidx High-dimensional Indexing For Efficient Approximate Nearest Neighbor Search", "abstract": "<p>Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale\ndata processing and analytics, particularly for analyzing multimedia contents\nwhich are often of high dimensionality. Instead of using exact NN search,\nextensive research efforts have been focusing on approximate NN search\nalgorithms. In this work, we present “HDIdx”, an efficient high-dimensional\nindexing library for fast approximate NN search, which is open-source and\nwritten in Python. It offers a family of state-of-the-art algorithms that\nconvert input high-dimensional vectors into compact binary codes, making them\nvery efficient and scalable for NN search with very low space complexity.</p>\n", "tags": ["ARXIV"] },
{"key": "wang2013fast", "year": "2013", "title":"Fast Neighborhood Graph Search Using Cartesian Concatenation", "abstract": "<p>In this paper, we propose a new data structure for approximate nearest\nneighbor search. This structure augments the neighborhood graph with a bridge\ngraph. We propose to exploit Cartesian concatenation to produce a large set of\nvectors, called bridge vectors, from several small sets of subvectors. Each\nbridge vector is connected with a few reference vectors near to it, forming a\nbridge graph. Our approach finds nearest neighbors by simultaneously traversing\nthe neighborhood graph and the bridge graph in the best-first strategy. The\nsuccess of our approach stems from two factors: the exact nearest neighbor\nsearch over a large number of bridge vectors can be done quickly, and the\nreference vectors connected to a bridge (reference) vector near the query are\nalso likely to be near the query. Experimental results on searching over large\nscale datasets (SIFT, GIST and HOG) show that our approach outperforms\nstate-of-the-art ANN search algorithms in terms of efficiency and accuracy. The\ncombination of our approach with the IVFADC system also shows superior\nperformance over the BIGANN dataset of \\(1\\) billion SIFT features compared with\nthe best previously published result.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "wang2013space", "year": "2013", "title":"Space-efficient Las Vegas Algorithms For K-SUM", "abstract": "<p>Using hashing techniques, this paper develops a family of space-efficient Las\nVegas randomized algorithms for \\(k\\)-SUM problems. This family includes an\nalgorithm that can solve 3-SUM in \\(O(n^2)\\) time and \\(O(\\sqrt{n})\\) space. It\nalso establishes a new time-space upper bound for SUBSET-SUM, which can be\nsolved by a Las Vegas algorithm in \\(O^<em>(2^{(1-\\sqrt{\\8/9\\beta})n})\\) time and\n\\(O^</em>(2^{\\beta n})\\) space, for any \\(\\beta \\in [0, \\9/32]\\).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "wang2014hashing", "year": "2014", "title":"Hashing For Similarity Search A Survey", "abstract": "<p>Similarity search (nearest neighbor search) is a problem of pursuing the data\nitems whose distances to a query item are the smallest from a large database.\nVarious methods have been developed to address this problem, and recently a lot\nof efforts have been devoted to approximate search. In this paper, we present a\nsurvey on one of the main solutions, hashing, which has been widely studied\nsince the pioneering work locality sensitive hashing. We divide the hashing\nalgorithms two main categories: locality sensitive hashing, which designs hash\nfunctions without exploring the data distribution and learning to hash, which\nlearns hash functions according the data distribution, and review them from\nvarious aspects, including hash function design and distance measure and search\nscheme in the hash coding space.</p>\n", "tags": ["ARXIV","Independent","Survey Paper"] },
{"key": "wang2014optimized", "year": "2014", "title":"Optimized Cartesian k-means", "abstract": "<p>Product quantization-based approaches are effective to encode\nhigh-dimensional data points for approximate nearest neighbor search. The space\nis decomposed into a Cartesian product of low-dimensional subspaces, each of\nwhich generates a sub codebook. Data points are encoded as compact binary codes\nusing these sub codebooks, and the distance between two data points can be\napproximated efficiently from their codes by the precomputed lookup tables.\nTraditionally, to encode a subvector of a data point in a subspace, only one\nsub codeword in the corresponding sub codebook is selected, which may impose\nstrict restrictions on the search accuracy. In this paper, we propose a novel\napproach, named Optimized Cartesian \\(K\\)-Means (OCKM), to better encode the data\npoints for more accurate approximate nearest neighbor search. In OCKM, multiple\nsub codewords are used to encode the subvector of a data point in a subspace.\nEach sub codeword stems from different sub codebooks in each subspace, which\nare optimally generated with regards to the minimization of the distortion\nerrors. The high-dimensional data point is then encoded as the concatenation of\nthe indices of multiple sub codewords from all the subspaces. This can provide\nmore flexibility and lower distortion errors than traditional methods.\nExperimental results on the standard real-life datasets demonstrate the\nsuperiority over state-of-the-art approaches for approximate nearest neighbor\nsearch.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "wang2015achieving", "year": "2015", "title":"Achieving Arbitrary Locality And Availability In Binary Codes", "abstract": "<p>The \\(i\\)th coordinate of an \\((n,k)\\) code is said to have locality \\(r\\) and\navailability \\(t\\) if there exist \\(t\\) disjoint groups, each containing at most\n\\(r\\) other coordinates that can together recover the value of the \\(i\\)th\ncoordinate. This property is particularly useful for codes for distributed\nstorage systems because it permits local repair and parallel accesses of hot\ndata. In this paper, for any positive integers \\(r\\) and \\(t\\), we construct a\nbinary linear code of length \\(\\binom{r+t}{t}\\) which has locality \\(r\\) and\navailability \\(t\\) for all coordinates. The information rate of this code attains\n\\(\\frac{r}{r+t}\\), which is always higher than that of the direct product code,\nthe only known construction that can achieve arbitrary locality and\navailability.</p>\n", "tags": ["ARXIV"] },
{"key": "wang2015learning", "year": "2015", "title":"Learning To Hash For Indexing Big Data - A Survey", "abstract": "<p>The explosive growth in big data has attracted much attention in designing\nefficient indexing and search methods recently. In many critical applications\nsuch as large-scale search and pattern matching, finding the nearest neighbors\nto a query is a fundamental research problem. However, the straightforward\nsolution using exhaustive comparison is infeasible due to the prohibitive\ncomputational complexity and memory requirement. In response, Approximate\nNearest Neighbor (ANN) search based on hashing techniques has become popular\ndue to its promising performance in both efficiency and accuracy. Prior\nrandomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore\ndata-independent hash functions with random projections or permutations.\nAlthough having elegant theoretic guarantees on the search quality in certain\nmetric spaces, performance of randomized hashing has been shown insufficient in\nmany real-world applications. As a remedy, new approaches incorporating\ndata-driven learning methods in development of advanced hash functions have\nemerged. Such learning to hash methods exploit information such as data\ndistributions or class labels when optimizing the hash codes or functions.\nImportantly, the learned hash codes are able to preserve the proximity of\nneighboring data in the original feature spaces in the hash code spaces. The\ngoal of this paper is to provide readers with systematic understanding of\ninsights, pros and cons of the emerging techniques. We provide a comprehensive\nsurvey of the learning to hash framework and representative techniques of\nvarious types, including unsupervised, semi-supervised, and supervised. In\naddition, we also summarize recent hashing approaches utilizing the deep\nlearning models. Finally, we discuss the future direction and trends of\nresearch in this area.</p>\n", "tags": ["ARXIV","Deep Learning","LSH","Supervised","Survey Paper"] },
{"key": "wang2016algorithm", "year": "2016", "title":"An Algorithm For L1 Nearest Neighbor Search Via Monotonic Embedding", "abstract": "<p>Fast algorithms for nearest neighbor (NN) search have in large part focused on L2 distance. Here we develop an approach for L1 distance that begins with an explicit and exact embedding of the points into L2. We show how this embedding can efficiently be combined with random projection methods for L2 NN search, such as locality-sensitive hashing or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation that it is competitive in practice with available alternatives.</p>\n", "tags": ["Independent","NEURIPS"] },
{"key": "wang2016contextual", "year": "2016", "title":"Contextual Visual Similarity", "abstract": "<p>Measuring visual similarity is critical for image understanding. But what\nmakes two images similar? Most existing work on visual similarity assumes that\nimages are similar because they contain the same object instance or category.\nHowever, the reason why images are similar is much more complex. For example,\nfrom the perspective of category, a black dog image is similar to a white dog\nimage. However, in terms of color, a black dog image is more similar to a black\nhorse image than the white dog image. This example serves to illustrate that\nvisual similarity is ambiguous but can be made precise when given an explicit\ncontextual perspective. Based on this observation, we propose the concept of\ncontextual visual similarity. To be concrete, we examine the concept of\ncontextual visual similarity in the application domain of image search. Instead\nof providing only a single image for image similarity search (\\eg, Google image\nsearch), we require three images. Given a query image, a second positive image\nand a third negative image, dissimilar to the first two images, we define a\ncontextualized similarity search criteria. In particular, we learn feature\nweights over all the feature dimensions of each image such that the distance\nbetween the query image and the positive image is small and their distances to\nthe negative image are large after reweighting their features. The learned\nfeature weights encode the contextualized visual similarity specified by the\nuser and can be used for attribute specific image search. We also show the\nusefulness of our contextualized similarity weighting scheme for different\ntasks, such as answering visual analogy questions and unsupervised attribute\ndiscovery.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "wang2016deep", "year": "2016", "title":"Deep Supervised Hashing With Triplet Labels", "abstract": "<p>Hashing is one of the most popular and powerful approximate nearest neighbor\nsearch techniques for large-scale image retrieval. Most traditional hashing\nmethods first represent images as off-the-shelf visual features and then\nproduce hashing codes in a separate stage. However, off-the-shelf visual\nfeatures may not be optimally compatible with the hash code learning procedure,\nwhich may result in sub-optimal hash codes. Recently, deep hashing methods have\nbeen proposed to simultaneously learn image features and hash codes using deep\nneural networks and have shown superior performance over traditional hashing\nmethods. Most deep hashing methods are given supervised information in the form\nof pairwise labels or triplet labels. The current state-of-the-art deep hashing\nmethod DPSH~\\cite{li2015feature}, which is based on pairwise labels, performs\nimage feature learning and hash code learning simultaneously by maximizing the\nlikelihood of pairwise similarities. Inspired by DPSH~\\cite{li2015feature}, we\npropose a triplet label based deep hashing method which aims to maximize the\nlikelihood of the given triplet labels. Experimental results show that our\nmethod outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets,\nincluding the state-of-the-art method DPSH~\\cite{li2015feature} and all the\nprevious triplet label based deep hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "wang2016learning", "year": "2016", "title":"Learning A Deep ell_infty Encoder For Hashing", "abstract": "<p>We investigate the \\(\\ell_\\infty\\)-constrained representation which\ndemonstrates robustness to quantization errors, utilizing the tool of deep\nlearning. Based on the Alternating Direction Method of Multipliers (ADMM), we\nformulate the original convex minimization problem as a feed-forward neural\nnetwork, named \\textit{Deep \\(\\ell_\\infty\\) Encoder}, by introducing the novel\nBounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as\nnetwork biases. Such a structural prior acts as an effective network\nregularization, and facilitates the model initialization. We then investigate\nthe effective use of the proposed model in the application of hashing, by\ncoupling the proposed encoders under a supervised pairwise loss, to develop a\n\\textit{Deep Siamese \\(\\ell_\\infty\\) Network}, which can be optimized from end to\nend. Extensive experiments demonstrate the impressive performances of the\nproposed model. We also provide an in-depth analysis of its behaviors against\nthe competitors.</p>\n", "tags": ["ARXIV","Deep Learning","Quantisation","Supervised"] },
{"key": "wang2016survey", "year": "2016", "title":"A Survey On Learning To Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the\ndatabase such that the distances from them to the query point are the smallest.\nLearning to hash is one of the major solutions to this problem and has been\nwidely studied recently. In this paper, we present a comprehensive survey of\nthe learning to hash algorithms, categorize them according to the manners of\npreserving the similarities into: pairwise similarity preserving, multiwise\nsimilarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity\npreserving as the objective function is very different though quantization, as\nwe show, can be derived from preserving the pairwise similarities. In addition,\nwe present the evaluation protocols, and the general performance analysis, and\npoint out that the quantization algorithms perform superiorly in terms of\nsearch accuracy, search time cost, and space cost. Finally, we introduce a few\nemerging topics.</p>\n", "tags": ["ARXIV","Quantisation","Survey Paper"] },
{"key": "wang2016unsupervised", "year": "2016", "title":"Unsupervised Cross-media Hashing With Structure Preservation", "abstract": "<p>Recent years have seen the exponential growth of heterogeneous multimedia\ndata. The need for effective and accurate data retrieval from heterogeneous\ndata sources has attracted much research interest in cross-media retrieval.\nHere, given a query of any media type, cross-media retrieval seeks to find\nrelevant results of different media types from heterogeneous data sources. To\nfacilitate large-scale cross-media retrieval, we propose a novel unsupervised\ncross-media hashing method. Our method incorporates local affinity and distance\nrepulsion constraints into a matrix factorization framework. Correspondingly,\nthe proposed method learns hash functions that generates unified hash codes\nfrom different media types, while ensuring intrinsic geometric structure of the\ndata distribution is preserved. These hash codes empower the similarity between\ndata of different media types to be evaluated directly. Experimental results on\ntwo large-scale multimedia datasets demonstrate the effectiveness of the\nproposed method, where we outperform the state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "wang2017composite", "year": "2017", "title":"Composite Quantization", "abstract": "<p>This paper studies the compact coding approach to approximate nearest\nneighbor search. We introduce a composite quantization framework. It uses the\ncomposition of several (\\(M\\)) elements, each of which is selected from a\ndifferent dictionary, to accurately approximate a \\(D\\)-dimensional vector, thus\nyielding accurate search, and represents the data vector by a short code\ncomposed of the indices of the selected elements in the corresponding\ndictionaries. Our key contribution lies in introducing a near-orthogonality\nconstraint, which makes the search efficiency is guaranteed as the cost of the\ndistance computation is reduced to \\(O(M)\\) from \\(O(D)\\) through a distance table\nlookup scheme. The resulting approach is called near-orthogonal composite\nquantization. We theoretically justify the equivalence between near-orthogonal\ncomposite quantization and minimizing an upper bound of a function formed by\njointly considering the quantization error and the search cost according to a\ngeneralized triangle inequality. We empirically show the efficacy of the\nproposed approach over several benchmark datasets. In addition, we demonstrate\nthe superior performances in other three applications: combination with\ninverted multi-index, quantizing the query for mobile search, and inner-product\nsimilarity search.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "wang2017effective", "year": "2017", "title":"Effective Multi-query Expansions Collaborative Deep Networks For Robust Landmark Retrieval", "abstract": "<p>Given a query photo issued by a user (q-user), the landmark retrieval is to\nreturn a set of photos with their landmarks similar to those of the query,\nwhile the existing studies on the landmark retrieval focus on exploiting\ngeometries of landmarks for similarity matches between candidate photos and a\nquery photo. We observe that the same landmarks provided by different users\nover social media community may convey different geometry information depending\non the viewpoints and/or angles, and may subsequently yield very different\nresults. In fact, dealing with the landmarks with \\illshapes caused by the\nphotography of q-users is often nontrivial and has seldom been studied. In this\npaper we propose a novel framework, namely multi-query expansions, to retrieve\nsemantically robust landmarks by two steps. Firstly, we identify the top-\\(k\\)\nphotos regarding the latent topics of a query landmark to construct multi-query\nset so as to remedy its possible \\illshape. For this purpose, we significantly\nextend the techniques of Latent Dirichlet Allocation. Then, motivated by the\ntypical <em>collaborative filtering</em> methods, we propose to learn a\n<em>collaborative</em> deep networks based semantically, nonlinear and high-level\nfeatures over the latent factor for landmark photo as the training set, which\nis formed by matrix factorization over <em>collaborative</em> user-photo matrix\nregarding the multi-query set. The learned deep network is further applied to\ngenerate the features for all the other photos, meanwhile resulting into a\ncompact multi-query set within such space. Extensive experiments are conducted\non real-world social media data with both landmark photos together with their\nuser information to show the superior performance over the existing methods.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "wang2017flash", "year": "2017", "title":"FLASH Randomized Algorithms Accelerated Over CPU-GPU For Ultra-high Dimensional Similarity Search", "abstract": "<p>We present FLASH (\\textbf{F}ast \\textbf{L}SH \\textbf{A}lgorithm for\n\\textbf{S}imilarity search accelerated with \\textbf{H}PC), a similarity search\nsystem for ultra-high dimensional datasets on a single machine, that does not\nrequire similarity computations and is tailored for high-performance computing\nplatforms. By leveraging a LSH style randomized indexing procedure and\ncombining it with several principled techniques, such as reservoir sampling,\nrecent advances in one-pass minwise hashing, and count based estimations, we\nreduce the computational and parallelization costs of similarity search, while\nretaining sound theoretical guarantees.\n  We evaluate FLASH on several real, high-dimensional datasets from different\ndomains, including text, malicious URL, click-through prediction, social\nnetworks, etc. Our experiments shed new light on the difficulties associated\nwith datasets having several million dimensions. Current state-of-the-art\nimplementations either fail on the presented scale or are orders of magnitude\nslower than FLASH. FLASH is capable of computing an approximate k-NN graph,\nfrom scratch, over the full webspam dataset (1.3 billion nonzeros) in less than\n10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam\ndataset, using brute-force (\\(n^2D\\)), will require at least 20 teraflops. We\nprovide CPU and GPU implementations of FLASH for replicability of our results.</p>\n", "tags": ["ARXIV","Graph","Independent","LSH"] },
{"key": "wang2017subspace", "year": "2017", "title":"Subspace Approximation For Approximate Nearest Neighbor Search In NLP", "abstract": "<p>Most natural language processing tasks can be formulated as the approximated\nnearest neighbor search problem, such as word analogy, document similarity,\nmachine translation. Take the question-answering task as an example, given a\nquestion as the query, the goal is to search its nearest neighbor in the\ntraining dataset as the answer. However, existing methods for approximate\nnearest neighbor search problem may not perform well owing to the following\npractical challenges: 1) there are noise in the data; 2) the large scale\ndataset yields a huge retrieval space and high search time complexity.\n  In order to solve these problems, we propose a novel approximate nearest\nneighbor search framework which i) projects the data to a subspace based\nspectral analysis which eliminates the influence of noise; ii) partitions the\ntraining dataset to different groups in order to reduce the search space.\nSpecifically, the retrieval space is reduced from \\(O(n)\\) to \\(O(log n)\\) (where\n\\(n\\) is the number of data points in the training dataset). We prove that the\nretrieved nearest neighbor in the projected subspace is the same as the one in\nthe original feature space. We demonstrate the outstanding performance of our\nframework on real-world natural language processing tasks.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "wang2017supervised", "year": "2017", "title":"Supervised Deep Hashing For Hierarchical Labeled Data", "abstract": "<p>Recently, hashing methods have been widely used in large-scale image\nretrieval. However, most existing hashing methods did not consider the\nhierarchical relation of labels, which means that they ignored the rich\ninformation stored in the hierarchy. Moreover, most of previous works treat\neach bit in a hash code equally, which does not meet the scenario of\nhierarchical labeled data. In this paper, we propose a novel deep hashing\nmethod, called supervised hierarchical deep hashing (SHDH), to perform hash\ncode learning for hierarchical labeled data. Specifically, we define a novel\nsimilarity formula for hierarchical labeled data by weighting each layer, and\ndesign a deep convolutional neural network to obtain a hash code for each data\npoint. Extensive experiments on several real-world public datasets show that\nthe proposed method outperforms the state-of-the-art baselines in the image\nretrieval task.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "wang2019cluster", "year": "2019", "title":"Cluster-wise Unsupervised Hashing For Cross-modal Similarity Search", "abstract": "<p>Large-scale cross-modal hashing similarity retrieval has attracted more and\nmore attention in modern search applications such as search engines and\nautopilot, showing great superiority in computation and storage. However,\ncurrent unsupervised cross-modal hashing methods still have some limitations:\n(1)many methods relax the discrete constraints to solve the optimization\nobjective which may significantly degrade the retrieval performance;(2)most\nexisting hashing model project heterogenous data into a common latent space,\nwhich may always lose sight of diversity in heterogenous data;(3)transforming\nreal-valued data point to binary codes always results in abundant loss of\ninformation, producing the suboptimal continuous latent space. To overcome\nabove problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)\nmethod is proposed. Specifically, CUH jointly performs the multi-view\nclustering that projects the original data points from different modalities\ninto its own low-dimensional latent semantic space and finds the cluster\ncentroid points and the common clustering indicators in its own low-dimensional\nspace, and learns the compact hash codes and the corresponding linear hash\nfunctions. An discrete optimization framework is developed to learn the unified\nbinary codes across modalities under the guidance cluster-wise code-prototypes.\nThe reasonableness and effectiveness of CUH is well demonstrated by\ncomprehensive experiments on diverse benchmark datasets.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "wang2019deep", "year": "2019", "title":"Deep Collaborative Discrete Hashing With Semantic-invariant Structure", "abstract": "<p>Existing deep hashing approaches fail to fully explore semantic correlations\nand neglect the effect of linguistic context on visual attention learning,\nleading to inferior performance. This paper proposes a dual-stream learning\nframework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs\na discriminative common discrete space by collaboratively incorporating the\nshared and individual semantics deduced from visual features and semantic\nlabels. Specifically, the context-aware representations are generated by\nemploying the outer product of visual embeddings and semantic encodings.\nMoreover, we reconstruct the labels and introduce the focal loss to take\nadvantage of frequent and rare concepts. The common binary code space is built\non the joint learning of the visual representations attended by language, the\nsemantic-invariant structure construction and the label distribution\ncorrection. Extensive experiments demonstrate the superiority of our method.</p>\n", "tags": ["SIGIR"] },
{"key": "wang2019fpscreen", "year": "2019", "title":"Fpscreen A Rapid Similarity Search Tool For Massive Molecular Library Based On Molecular Fingerprint Comparison", "abstract": "<p>We designed a fast similarity search engine for large molecular libraries:\nFPScreen. We downloaded 100 million molecules’ structure files in PubChem with\nSDF extension, then applied a computational chemistry tool RDKit to convert\neach structure file into one line of text in MACCS format and stored them in a\ntext file as our molecule library. The similarity search engine compares the\nsimilarity while traversing the 166-bit strings in the library file line by\nline. FPScreen can complete similarity search through 100 million entries in\nour molecule library within one hour. That is very fast as a biology\ncomputation tool. Additionally, we divided our library into several strides for\nparallel processing. FPScreen was developed in WEB mode.</p>\n", "tags": ["ARXIV"] },
{"key": "wang2019fusion", "year": "2019", "title":"Fusion-supervised Deep Cross-modal Hashing", "abstract": "<p>Deep hashing has recently received attention in cross-modal retrieval for its\nimpressive advantages. However, existing hashing methods for cross-modal\nretrieval cannot fully capture the heterogeneous multi-modal correlation and\nexploit the semantic information. In this paper, we propose a novel\n<em>Fusion-supervised Deep Cross-modal Hashing</em> (FDCH) approach. Firstly,\nFDCH learns unified binary codes through a fusion hash network with paired\nsamples as input, which effectively enhances the modeling of the correlation of\nheterogeneous multi-modal data. Then, these high-quality unified hash codes\nfurther supervise the training of the modality-specific hash networks for\nencoding out-of-sample queries. Meanwhile, both pair-wise similarity\ninformation and classification information are embedded in the hash networks\nunder one stream framework, which simultaneously preserves cross-modal\nsimilarity and keeps semantic consistency. Experimental results on two\nbenchmark datasets demonstrate the state-of-the-art performance of FDCH.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "wang2019memory", "year": "2019", "title":"A Memory-efficient Sketch Method For Estimating High Similarities In Streaming Sets", "abstract": "<p>Estimating set similarity and detecting highly similar sets are fundamental\nproblems in areas such as databases, machine learning, and information\nretrieval. MinHash is a well-known technique for approximating Jaccard\nsimilarity of sets and has been successfully used for many applications such as\nsimilarity search and large scale learning. Its two compressed versions, b-bit\nMinHash and Odd Sketch, can significantly reduce the memory usage of the\noriginal MinHash method, especially for estimating high similarities (i.e.,\nsimilarities around 1). Although MinHash can be applied to static sets as well\nas streaming sets, of which elements are given in a streaming fashion and\ncardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd\nSketch fail to deal with streaming data. To solve this problem, we design a\nmemory efficient sketch method, MaxLogHash, to accurately estimate Jaccard\nsimilarities in streaming sets. Compared to MinHash, our method uses smaller\nsized registers (each register consists of less than 7 bits) to build a compact\nsketch for each set. We also provide a simple yet accurate estimator for\ninferring Jaccard similarity from MaxLogHash sketches. In addition, we derive\nformulas for bounding the estimation error and determine the smallest necessary\nmemory usage (i.e., the number of registers used for a MaxLogHash sketch) for\nthe desired accuracy. We conduct experiments on a variety of datasets, and\nexperimental results show that our method MaxLogHash is about 5 times more\nmemory efficient than MinHash with the same accuracy and computational cost for\nestimating high similarities.</p>\n", "tags": ["ARXIV","Independent","Streaming Data"] },
{"key": "wang2019supervised", "year": "2019", "title":"Supervised Quantization For Similarity Search", "abstract": "<p>In this paper, we address the problem of searching for semantically similar\nimages from a large database. We present a compact coding approach, supervised\nquantization. Our approach simultaneously learns feature selection that\nlinearly transforms the database points into a low-dimensional discriminative\nsubspace, and quantizes the data points in the transformed space. The\noptimization criterion is that the quantized points not only approximate the\ntransformed points accurately, but also are semantically separable: the points\nbelonging to a class lie in a cluster that is not overlapped with other\nclusters corresponding to other classes, which is formulated as a\nclassification problem. The experiments on several standard datasets show the\nsuperiority of our approach over the state-of-the art supervised hashing and\nunsupervised quantization algorithms.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "wang2020asymmetric", "year": "2020", "title":"Asymmetric Correlation Quantization Hashing For Cross-modal Retrieval", "abstract": "<p>Due to the superiority in similarity computation and database storage for\nlarge-scale multiple modalities data, cross-modal hashing methods have\nattracted extensive attention in similarity retrieval across the heterogeneous\nmodalities. However, there are still some limitations to be further taken into\naccount: (1) most current CMH methods transform real-valued data points into\ndiscrete compact binary codes under the binary constraints, limiting the\ncapability of representation for original data on account of abundant loss of\ninformation and producing suboptimal hash codes; (2) the discrete binary\nconstraint learning model is hard to solve, where the retrieval performance may\ngreatly reduce by relaxing the binary constraints for large quantization error;\n(3) handling the learning problem of CMH in a symmetric framework, leading to\ndifficult and complex optimization objective. To address above challenges, in\nthis paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method\nis proposed. Specifically, ACQH learns the projection matrixs of heterogeneous\nmodalities data points for transforming query into a low-dimensional\nreal-valued vector in latent semantic space and constructs the stacked\ncompositional quantization embedding in a coarse-to-fine manner for indicating\ndatabase points by a series of learnt real-valued codeword in the codebook with\nthe help of pointwise label information regression simultaneously. Besides, the\nunified hash codes across modalities can be directly obtained by the discrete\niterative optimization framework devised in the paper. Comprehensive\nexperiments on diverse three benchmark datasets have shown the effectiveness\nand rationality of ACQH.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "wang2020deep", "year": "2020", "title":"Deep Reinforcement Learning With Label Embedding Reward For Supervised Image Hashing", "abstract": "<p>Deep hashing has shown promising results in image retrieval and recognition.\nDespite its success, most existing deep hashing approaches are rather similar:\neither multi-layer perceptron or CNN is applied to extract image feature,\nfollowed by different binarization activation functions such as sigmoid, tanh\nor autoencoder to generate binary code. In this work, we introduce a novel\ndecision-making approach for deep supervised hashing. We formulate the hashing\nproblem as travelling across the vertices in the binary code space, and learn a\ndeep Q-network with a novel label embedding reward defined by\nBose-Chaudhuri-Hocquenghem (BCH) codes to explore the best path. Extensive\nexperiments and analysis on the CIFAR-10 and NUS-WIDE dataset show that our\napproach outperforms state-of-the-art supervised hashing methods under various\ncode lengths.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "wang2020distilling", "year": "2020", "title":"Distilling Knowledge By Mimicking Features", "abstract": "<p>Knowledge distillation (KD) is a popular method to train efficient networks\n(“student”) with the help of high-capacity networks (“teacher”). Traditional\nmethods use the teacher’s soft logits as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher’s features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature’s magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy. We provide theoretical analyses of how LSH facilitates\nfeature direction mimicking, and further extend feature mimicking to\nmulti-label recognition and object detection.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "wang2020faster", "year": "2020", "title":"Faster Person Re-identification", "abstract": "<p>Fast person re-identification (ReID) aims to search person images quickly and\naccurately. The main idea of recent fast ReID methods is the hashing algorithm,\nwhich learns compact binary codes and performs fast Hamming distance and\ncounting sort. However, a very long code is needed for high accuracy (e.g.\n2048), which compromises search speed. In this work, we introduce a new\nsolution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code\nsearch strategy, which complementarily uses short and long codes, achieving\nboth faster speed and better accuracy. It uses shorter codes to coarsely rank\nbroad matching similarities and longer codes to refine only a few top\ncandidates for more accurate instance ReID. Specifically, we design an\nAll-in-One (AiO) framework together with a Distance Threshold Optimization\n(DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of\ndifferent lengths in a single model. It learns multiple codes in a pyramid\nstructure, and encourage shorter codes to mimic longer codes by\nself-distillation. DTO solves a complex threshold search problem by a simple\noptimization process, and the balance between accuracy and speed is easily\ncontrolled by a single parameter. It formulates the optimization target as a\n\\(F_{\\beta}\\) score that can be optimised by Gaussian cumulative distribution\nfunctions. Experimental results on 2 datasets show that our proposed method\n(CtF) is not only 8% more accurate but also 5x faster than contemporary hashing\nReID methods. Compared with non-hashing ReID methods, CtF is \\(50\\times\\) faster\nwith comparable accuracy. Code is available at\nhttps://github.com/wangguanan/light-reid.</p>\n", "tags": ["ARXIV","Has Code"] },
{"key": "wang2020hashing", "year": "2020", "title":"Hashing-based Non-maximum Suppression For Crowded Object Detection", "abstract": "<p>In this paper, we propose an algorithm, named hashing-based non-maximum\nsuppression (HNMS) to efficiently suppress the non-maximum boxes for object\ndetection. Non-maximum suppression (NMS) is an essential component to suppress\nthe boxes at closely located locations with similar shapes. The time cost tends\nto be huge when the number of boxes becomes large, especially for crowded\nscenes. The basic idea of HNMS is to firstly map each box to a discrete code\n(hash cell) and then remove the boxes with lower confidences if they are in the\nsame cell. Considering the intersection-over-union (IoU) as the metric, we\npropose a simple yet effective hashing algorithm, named IoUHash, which\nguarantees that the boxes within the same cell are close enough by a lower IoU\nbound. For two-stage detectors, we replace NMS in region proposal network with\nHNMS, and observe significant speed-up with comparable accuracy. For one-stage\ndetectors, HNMS is used as a pre-filter to speed up the suppression with a\nlarge margin. Extensive experiments are conducted on CARPK, SKU-110K,\nCrowdHuman datasets to demonstrate the efficiency and effectiveness of HNMS.\nCode is released at \\url{https://github.com/microsoft/hnms.git}.</p>\n", "tags": ["ARXIV","Has Code"] },
{"key": "wang2021comprehensive", "year": "2021", "title":"A Comprehensive Survey And Experimental Comparison Of Graph-based Approximate Nearest Neighbor Search", "abstract": "<p>Approximate nearest neighbor search (ANNS) constitutes an important operation\nin a multitude of applications, including recommendation systems, information\nretrieval, and pattern recognition. In the past decade, graph-based ANNS\nalgorithms have been the leading paradigm in this domain, with dozens of\ngraph-based ANNS algorithms proposed. Such algorithms aim to provide effective,\nefficient solutions for retrieving the nearest neighbors for a given query.\nNevertheless, these efforts focus on developing and optimizing algorithms with\ndifferent approaches, so there is a real need for a comprehensive survey about\nthe approaches’ relative performance, strengths, and pitfalls. Thus here we\nprovide a thorough comparative analysis and experimental evaluation of 13\nrepresentative graph-based ANNS algorithms via a new taxonomy and fine-grained\npipeline. We compared each algorithm in a uniform test environment on eight\nreal-world datasets and 12 synthetic datasets with varying sizes and\ncharacteristics. Our study yields novel discoveries, offerings several useful\nprinciples to improve algorithms, thus designing an optimized method that\noutperforms the state-of-the-art algorithms. This effort also helped us\npinpoint algorithms’ working portions, along with rule-of-thumb recommendations\nabout promising research directions and suitable algorithms for practitioners\nin different fields.</p>\n", "tags": ["ARXIV","Graph","Survey Paper"] },
{"key": "wang2021contrastive", "year": "2021", "title":"Contrastive Quantization With Code Memory For Unsupervised Image Retrieval", "abstract": "<p>The high efficiency in computation and storage makes hashing (including\nbinary hashing and quantization) a common strategy in large-scale retrieval\nsystems. To alleviate the reliance on expensive annotations, unsupervised deep\nhashing becomes an important research problem. This paper provides a novel\nsolution to unsupervised deep quantization, namely Contrastive Quantization\nwith Code Memory (MeCoQ). Different from existing reconstruction-based\nstrategies, we learn unsupervised binary descriptors by contrastive learning,\nwhich can better capture discriminative visual semantics. Besides, we uncover\nthat codeword diversity regularization is critical to prevent contrastive\nlearning-based quantization from model degeneration. Moreover, we introduce a\nnovel quantization code memory module that boosts contrastive learning with\nlower feature drift than conventional feature memories. Extensive experiments\non benchmark datasets show that MeCoQ outperforms state-of-the-art methods.\nCode and configurations are publicly available at\nhttps://github.com/gimpong/AAAI22-MeCoQ.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "wang2021cross", "year": "2021", "title":"Cross-modal Zero-shot Hashing By Label Attributes Embedding", "abstract": "<p>Cross-modal hashing (CMH) is one of the most promising methods in cross-modal\napproximate nearest neighbor search. Most CMH solutions ideally assume the\nlabels of training and testing set are identical. However, the assumption is\noften violated, causing a zero-shot CMH problem. Recent efforts to address this\nissue focus on transferring knowledge from the seen classes to the unseen ones\nusing label attributes. However, the attributes are isolated from the features\nof multi-modal data. To reduce the information gap, we introduce an approach\ncalled LAEH (Label Attributes Embedding for zero-shot cross-modal Hashing).\nLAEH first gets the initial semantic attribute vectors of labels by word2vec\nmodel and then uses a transformation network to transform them into a common\nsubspace. Next, it leverages the hash vectors and the feature similarity matrix\nto guide the feature extraction network of different modalities. At the same\ntime, LAEH uses the attribute similarity as the supplement of label similarity\nto rectify the label embedding and common subspace. Experiments show that LAEH\noutperforms related representative zero-shot and cross-modal hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "wang2021meta", "year": "2021", "title":"Meta Cross-modal Hashing On Long-tailed Data", "abstract": "<p>Due to the advantage of reducing storage while speeding up query time on big\nheterogeneous data, cross-modal hashing has been extensively studied for\napproximate nearest neighbor search of multi-modal data. Most hashing methods\nassume that training data is class-balanced.However, in practice, real world\ndata often have a long-tailed distribution. In this paper, we introduce a\nmeta-learning based cross-modal hashing method (MetaCMH) to handle long-tailed\ndata. Due to the lack of training samples in the tail classes, MetaCMH first\nlearns direct features from data in different modalities, and then introduces\nan associative memory module to learn the memory features of samples of the\ntail classes. It then combines the direct and memory features to obtain meta\nfeatures for each sample. For samples of the head classes of the long tail\ndistribution, the weight of the direct features is larger, because there are\nenough training data to learn them well; while for rare classes, the weight of\nthe memory features is larger. Finally, MetaCMH uses a likelihood loss function\nto preserve the similarity in different modalities and learns hash functions in\nan end-to-end fashion. Experiments on long-tailed datasets show that MetaCMH\nperforms significantly better than state-of-the-art methods, especially on the\ntail classes.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "wang2021prototype", "year": "2021", "title":"Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing", "abstract": "<p>Due to its powerful capability of representation learning and high-efficiency\ncomputation, deep hashing has made significant progress in large-scale image\nretrieval. However, deep hashing networks are vulnerable to adversarial\nexamples, which is a practical secure problem but seldom studied in\nhashing-based retrieval field. In this paper, we propose a novel\nprototype-supervised adversarial network (ProS-GAN), which formulates a\nflexible generative architecture for efficient and effective targeted hashing\nattack. To the best of our knowledge, this is the first generation-based method\nto attack deep hashing networks. Generally, our proposed framework consists of\nthree parts, i.e., a PrototypeNet, a generator, and a discriminator.\nSpecifically, the designed PrototypeNet embeds the target label into the\nsemantic representation and learns the prototype code as the category-level\nrepresentative of the target label. Moreover, the semantic representation and\nthe original image are jointly fed into the generator for a flexible targeted\nattack. Particularly, the prototype code is adopted to supervise the generator\nto construct the targeted adversarial example by minimizing the Hamming\ndistance between the hash code of the adversarial example and the prototype\ncode. Furthermore, the generator is against the discriminator to simultaneously\nencourage the adversarial examples visually realistic and the semantic\nrepresentation informative. Extensive experiments verify that the proposed\nframework can efficiently produce adversarial examples with better targeted\nattack performance and transferability over state-of-the-art targeted attack\nmethods of deep hashing. The related codes could be available at\nhttps://github.com/xunguangwang/ProS-GAN .</p>\n", "tags": ["ARXIV","GAN","Has Code","Image Retrieval","Supervised"] },
{"key": "wang2021towards", "year": "2021", "title":"Towards A Model For LSH", "abstract": "<p>As data volumes continue to grow, clustering and outlier detection algorithms\nare becoming increasingly time-consuming. Classical index structures for\nneighbor search are no longer sustainable due to the “curse of dimensionality”.\nInstead, approximated index structures offer a good opportunity to\nsignificantly accelerate the neighbor search for clustering and outlier\ndetection and to have the lowest possible error rate in the results of the\nalgorithms. Locality-sensitive hashing is one of those. We indicate directions\nto model the properties of LSH.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "wang2022anchor", "year": "2022", "title":"Anchor Graph Structure Fusion Hashing For Cross-modal Similarity Search", "abstract": "<p>Cross-modal hashing still has some challenges needed to address: (1) most\nexisting CMH methods take graphs as input to model data distribution. These\nmethods omit to consider the correlation of graph structure among multiple\nmodalities; (2) most existing CMH methods ignores considering the fusion\naffinity among multi-modalities data; (3) most existing CMH methods relax the\ndiscrete constraints to solve the optimization objective, significantly\ndegrading the retrieval performance. To solve the above limitations, we propose\na novel Anchor Graph Structure Fusion Hashing (AGSFH). AGSFH constructs the\nanchor graph structure fusion matrix from different anchor graphs of multiple\nmodalities with the Hadamard product, which can fully exploit the geometric\nproperty of underlying data structure. Based on the anchor graph structure\nfusion matrix, AGSFH attempts to directly learn an intrinsic anchor graph,\nwhere the structure of the intrinsic anchor graph is adaptively tuned so that\nthe number of components of the intrinsic graph is exactly equal to the number\nof clusters. Besides, AGSFH preserves the anchor fusion affinity into the\ncommon binary Hamming space. Furthermore, a discrete optimization framework is\ndesigned to learn the unified binary codes. Extensive experimental results on\nthree public social datasets demonstrate the superiority of AGSFH.</p>\n", "tags": ["ARXIV","Cross Modal","Graph"] },
{"key": "wang2022binary", "year": "2022", "title":"Binary Representation Via Jointly Personalized Sparse Hashing", "abstract": "<p>Unsupervised hashing has attracted much attention for binary representation\nlearning due to the requirement of economical storage and efficiency of binary\ncodes. It aims to encode high-dimensional features in the Hamming space with\nsimilarity preservation between instances. However, most existing methods learn\nhash functions in manifold-based approaches. Those methods capture the local\ngeometric structures (i.e., pairwise relationships) of data, and lack\nsatisfactory performance in dealing with real-world scenarios that produce\nsimilar features (e.g. color and shape) with different semantic information. To\naddress this challenge, in this work, we propose an effective unsupervised\nmethod, namely Jointly Personalized Sparse Hashing (JPSH), for binary\nrepresentation learning. To be specific, firstly, we propose a novel\npersonalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different\npersonalized subspaces are constructed to reflect category-specific attributes\nfor different clusters, adaptively mapping instances within the same cluster to\nthe same Hamming space. In addition, we deploy sparse constraints for different\npersonalized subspaces to select important features. We also collect the\nstrengths of the other clusters to build the PSH module with avoiding\nover-fitting. Then, to simultaneously preserve semantic and pairwise\nsimilarities in our JPSH, we incorporate the PSH and manifold-based hash\nlearning into the seamless formulation. As such, JPSH not only distinguishes\nthe instances from different clusters, but also preserves local neighborhood\nstructures within the cluster. Finally, an alternating optimization algorithm\nis adopted to iteratively capture analytical solutions of the JPSH model.\nExtensive experiments on four benchmark datasets verify that the JPSH\noutperforms several hashing algorithms on the similarity search task.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "wang2022cgat", "year": "2022", "title":"Cgat Center-guided Adversarial Training For Deep Hashing-based Retrieval", "abstract": "<p>Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61\\%, 12.35\\%, and 11.56\\% on FLICKR-25K, NUS-WIDE, and MS-COCO,\nrespectively. The code is available at https://github.com/xunguangwang/CgAT.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Independent"] },
{"key": "wang2022contrastive", "year": "2022", "title":"Contrastive Masked Autoencoders For Self-supervised Video Hashing", "abstract": "<p>Self-Supervised Video Hashing (SSVH) models learn to generate short binary\nrepresentations for videos without ground-truth supervision, facilitating\nlarge-scale video retrieval efficiency and attracting increasing research\nattention. The success of SSVH lies in the understanding of video content and\nthe ability to capture the semantic relation among unlabeled videos. Typically,\nstate-of-the-art SSVH methods consider these two points in a two-stage training\npipeline, where they firstly train an auxiliary network by instance-wise\nmask-and-predict tasks and secondly train a hashing model to preserve the\npseudo-neighborhood structure transferred from the auxiliary network. This\nconsecutive training strategy is inflexible and also unnecessary. In this\npaper, we propose a simple yet effective one-stage SSVH method called ConMH,\nwhich incorporates video semantic information and video similarity relationship\nunderstanding in a single stage. To capture video semantic information for\nbetter hashing learning, we adopt an encoder-decoder structure to reconstruct\nthe video from its temporal-masked frames. Particularly, we find that a higher\nmasking ratio helps video understanding. Besides, we fully exploit the\nsimilarity relationship between videos by maximizing agreement between two\naugmented views of a video, which contributes to more discriminative and robust\nhash codes. Extensive experiments on three large-scale video datasets (i.e.,\nFCVID, ActivityNet and YFCC) indicate that ConMH achieves state-of-the-art\nresults. Code is available at https://github.com/huangmozhi9527/ConMH.</p>\n", "tags": ["ARXIV","Has Code","Supervised","Video Retrieval"] },
{"key": "wang2022hybrid", "year": "2022", "title":"Hybrid Contrastive Quantization For Efficient Cross-view Video Retrieval", "abstract": "<p>With the recent boom of video-based social platforms (e.g., YouTube and\nTikTok), video retrieval using sentence queries has become an important demand\nand attracts increasing research attention. Despite the decent performance,\nexisting text-video retrieval models in vision and language communities are\nimpractical for large-scale Web search because they adopt brute-force search\nbased on high-dimensional embeddings. To improve efficiency, Web search engines\nwidely apply vector compression libraries (e.g., FAISS) to post-process the\nlearned embeddings. Unfortunately, separate compression from feature encoding\ndegrades the robustness of representations and incurs performance decay. To\npursue a better balance between performance and efficiency, we propose the\nfirst quantized representation learning method for cross-view video retrieval,\nnamely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both\ncoarse-grained and fine-grained quantizations with transformers, which provide\ncomplementary understandings for texts and videos and preserve comprehensive\nsemantic information. By performing Asymmetric-Quantized Contrastive Learning\n(AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and\nmultiple fine-grained levels. This hybrid-grained learning strategy serves as\nstrong supervision on the cross-view video quantization model, where\ncontrastive learning at different levels can be mutually promoted. Extensive\nexperiments on three Web video benchmark datasets demonstrate that HCQ achieves\ncompetitive performance with state-of-the-art non-compressed retrieval methods\nwhile showing high efficiency in storage and computation. Code and\nconfigurations are available at https://github.com/gimpong/WWW22-HCQ.</p>\n", "tags": ["ARXIV","Cross Modal","Has Code","Quantisation","Self Supervised","Video Retrieval"] },
{"key": "wang2022inverted", "year": "2022", "title":"Inverted Semantic-index For Image Retrieval", "abstract": "<p>This paper addresses the construction of inverted index for large-scale image\nretrieval. The inverted index proposed by J. Sivic brings a significant\nacceleration by reducing distance computations with only a small fraction of\nthe database. The state-of-the-art inverted indices aim to build finer\npartitions that produce a concise and accurate candidate list. However,\npartitioning in these frameworks is generally achieved by unsupervised\nclustering methods which ignore the semantic information of images. In this\npaper, we replace the clustering method with image classification, during the\nconstruction of codebook. We then propose a merging and splitting method to\nsolve the problem that the number of partitions is unchangeable in the inverted\nsemantic-index. Next, we combine our semantic-index with the product\nquantization (PQ) so as to alleviate the accuracy loss caused by PQ\ncompression. Finally, we evaluate our model on large-scale image retrieval\nbenchmarks. Experiment results demonstrate that our model can significantly\nimprove the retrieval accuracy by generating high-quality candidate lists.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "wang2022navigable", "year": "2022", "title":"Navigable Proximity Graph-driven Native Hybrid Queries With Structured And Unstructured Constraints", "abstract": "<p>As research interest surges, vector similarity search is applied in multiple\nfields, including data mining, computer vision, and information retrieval.\n{Given a set of objects (e.g., a set of images) and a query object, we can\neasily transform each object into a feature vector and apply the vector\nsimilarity search to retrieve the most similar objects. However, the original\nvector similarity search cannot well support \\textit{hybrid queries}, where\nusers not only input unstructured query constraint (i.e., the feature vector of\nquery object) but also structured query constraint (i.e., the desired\nattributes of interest). Hybrid query processing aims at identifying these\nobjects with similar feature vectors to query object and satisfying the given\nattribute constraints. Recent efforts have attempted to answer a hybrid query\nby performing attribute filtering and vector similarity search separately and\nthen merging the results later, which limits efficiency and accuracy because\nthey are not purpose-built for hybrid queries.} In this paper, we propose a\nnative hybrid query (NHQ) framework based on proximity graph (PG), which\nprovides the specialized \\textit{composite index and joint pruning} modules for\nhybrid queries. We easily deploy existing various PGs on this framework to\nprocess hybrid queries efficiently. Moreover, we present two novel navigable\nPGs (NPGs) with optimized edge selection and routing strategies, which obtain\nbetter overall performance than existing PGs. After that, we deploy the\nproposed NPGs in NHQ to form two hybrid query methods, which significantly\noutperform the state-of-the-art competitors on all experimental datasets\n(10\\(\\times\\) faster under the same \\textit{Recall}), including eight public and\none in-house real-world datasets. Our code and datasets have been released at\n\\url{https://github.com/AshenOn3/NHQ}.</p>\n", "tags": ["ARXIV","Graph","Has Code"] },
{"key": "wang2023graph", "year": "2023", "title":"Graph-collaborated Auto-encoder Hashing For Multi-view Binary Clustering", "abstract": "<p>Unsupervised hashing methods have attracted widespread attention with the\nexplosive growth of large-scale data, which can greatly reduce storage and\ncomputation by learning compact binary codes. Existing unsupervised hashing\nmethods attempt to exploit the valuable information from samples, which fails\nto take the local geometric structure of unlabeled samples into consideration.\nMoreover, hashing based on auto-encoders aims to minimize the reconstruction\nloss between the input data and binary codes, which ignores the potential\nconsistency and complementarity of multiple sources data. To address the above\nissues, we propose a hashing algorithm based on auto-encoders for multi-view\nbinary clustering, which dynamically learns affinity graphs with low-rank\nconstraints and adopts collaboratively learning between auto-encoders and\naffinity graphs to learn a unified binary code, called Graph-Collaborated\nAuto-Encoder Hashing for Multi-view Binary Clustering (GCAE). Specifically, we\npropose a multi-view affinity graphs learning model with low-rank constraint,\nwhich can mine the underlying geometric information from multi-view data. Then,\nwe design an encoder-decoder paradigm to collaborate the multiple affinity\ngraphs, which can learn a unified binary code effectively. Notably, we impose\nthe decorrelation and code balance constraints on binary codes to reduce the\nquantization errors. Finally, we utilize an alternating iterative optimization\nscheme to obtain the multi-view clustering results. Extensive experimental\nresults on \\(5\\) public datasets are provided to reveal the effectiveness of the\nalgorithm and its superior performance over other state-of-the-art\nalternatives.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Quantisation","Unsupervised"] },
{"key": "wang2023learning", "year": "2023", "title":"Learning Multi-stage Multi-grained Semantic Embeddings For E-commerce Search", "abstract": "<p>Retrieving relevant items that match users’ queries from billion-scale corpus\nforms the core of industrial e-commerce search systems, in which\nembedding-based retrieval (EBR) methods are prevailing. These methods adopt a\ntwo-tower framework to learn embedding vectors for query and item separately\nand thus leverage efficient approximate nearest neighbor (ANN) search to\nretrieve relevant items. However, existing EBR methods usually ignore\ninconsistent user behaviors in industrial multi-stage search systems, resulting\nin insufficient retrieval efficiency with a low commercial return. To tackle\nthis challenge, we propose to improve EBR methods by learning Multi-level\nMulti-Grained Semantic Embeddings(MMSE). We propose the multi-stage information\nmining to exploit the ordered, clicked, unclicked and random sampled items in\npractical user behavior data, and then capture query-item similarity via a\npost-fusion strategy. We then propose multi-grained learning objectives that\nintegrate the retrieval loss with global comparison ability and the ranking\nloss with local comparison ability to generate semantic embeddings. Both\nexperiments on a real-world billion-scale dataset and online A/B tests verify\nthe effectiveness of MMSE in achieving significant performance improvements on\nmetrics such as offline recall and online conversion rate (CVR).</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "wang2023masked", "year": "2023", "title":"Masked Space-time Hash Encoding For Efficient Dynamic Scene Reconstruction", "abstract": "<p>In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel\nmethod for efficiently reconstructing dynamic 3D scenes from multi-view or\nmonocular videos. Based on the observation that dynamic scenes often contain\nsubstantial static areas that result in redundancy in storage and computations,\nMSTH represents a dynamic scene as a weighted combination of a 3D hash encoding\nand a 4D hash encoding. The weights for the two components are represented by a\nlearnable mask which is guided by an uncertainty-based objective to reflect the\nspatial and temporal importance of each 3D position. With this design, our\nmethod can reduce the hash collision rate by avoiding redundant queries and\nmodifications on static areas, making it feasible to represent a large number\nof space-time voxels by hash tables with small size.Besides, without the\nrequirements to fit the large numbers of temporally redundant features\nindependently, our method is easier to optimize and converge rapidly with only\ntwenty minutes of training for a 300-frame dynamic scene.As a result, MSTH\nobtains consistently better results than previous methods with only 20 minutes\nof training time and 130 MB of memory storage. Code is available at\nhttps://github.com/masked-spacetime-hashing/msth</p>\n", "tags": ["ARXIV","Cross Modal","Has Code","Independent"] },
{"key": "wang2023note", "year": "2023", "title":"A Note On efficient Task-specific Data Valuation For Nearest Neighbor Algorithms", "abstract": "<p>Data valuation is a growing research field that studies the influence of\nindividual data points for machine learning (ML) models. Data Shapley, inspired\nby cooperative game theory and economics, is an effective method for data\nvaluation. However, it is well-known that the Shapley value (SV) can be\ncomputationally expensive. Fortunately, Jia et al. (2019) showed that for\nK-Nearest Neighbors (KNN) models, the computation of Data Shapley is\nsurprisingly simple and efficient.\n  In this note, we revisit the work of Jia et al. (2019) and propose a more\nnatural and interpretable utility function that better reflects the performance\nof KNN models. We derive the corresponding calculation procedure for the Data\nShapley of KNN classifiers/regressors with the new utility functions. Our new\napproach, dubbed soft-label KNN-SV, achieves the same time complexity as the\noriginal method. We further provide an efficient approximation algorithm for\nsoft-label KNN-SV based on locality sensitive hashing (LSH). Our experimental\nresults demonstrate that Soft-label KNN-SV outperforms the original method on\nmost datasets in the task of mislabeled data detection, making it a better\nbaseline for future work on data valuation.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "wang2023reliable", "year": "2023", "title":"Reliable And Efficient Evaluation Of Adversarial Robustness For Deep Hashing-based Retrieval", "abstract": "<p>Deep hashing has been extensively applied to massive image retrieval due to\nits efficiency and effectiveness. Recently, several adversarial attacks have\nbeen presented to reveal the vulnerability of deep hashing models against\nadversarial examples. However, existing attack methods suffer from degraded\nperformance or inefficiency because they underutilize the semantic relations\nbetween original samples or spend a lot of time learning these relations with a\ndeep neural network. In this paper, we propose a novel Pharos-guided Attack,\ndubbed PgA, to evaluate the adversarial robustness of deep hashing networks\nreliably and efficiently. Specifically, we design pharos code to represent the\nsemantics of the benign image, which preserves the similarity to semantically\nrelevant samples and dissimilarity to irrelevant ones. It is proven that we can\nquickly calculate the pharos code via a simple math formula. Accordingly, PgA\ncan directly conduct a reliable and efficient attack on deep hashing-based\nretrieval by maximizing the similarity between the hash code of the adversarial\nexample and the pharos code. Extensive experiments on the benchmark datasets\nverify that the proposed algorithm outperforms the prior state-of-the-arts in\nboth attack strength and speed.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "wang2024affinity", "year": "2024", "title":"Affinity Preserving Quantization For Hashing A Vector Quantization Approach To Learning Compact Binary Codes", "abstract": "<p>Hashing techniques are powerful for approximate nearest\nneighbour (ANN) search. Existing quantization methods in\nhashing are all focused on scalar quantization (SQ) which\nis inferior in utilizing the inherent data distribution. In this\npaper, we propose a novel vector quantization (VQ) method\nnamed affinity preserving quantization (APQ) to improve the\nquantization quality of projection values, which has significantly\nboosted the performance of state-of-the-art hashing\ntechniques. In particular, our method incorporates the neighbourhood\nstructure in the pre- and post-projection data space\ninto vector quantization. APQ minimizes the quantization errors\nof projection values as well as the loss of affinity property\nof original space. An effective algorithm has been proposed\nto solve the joint optimization problem in APQ, and\nthe extension to larger binary codes has been resolved by applying\nproduct quantization to APQ. Extensive experiments\nhave shown that APQ consistently outperforms the state-of-the-art\nquantization methods, and has significantly improved\nthe performance of various hashing techniques.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "wang2024be", "year": "2024", "title":"To Be Continuous Or To Be Discrete Those Are Bits Of Questions", "abstract": "<p>Recently, binary representation has been proposed as a novel representation\nthat lies between continuous and discrete representations. It exhibits\nconsiderable information-preserving capability when being used to replace\ncontinuous input vectors. In this paper, we investigate the feasibility of\nfurther introducing it to the output side, aiming to allow models to output\nbinary labels instead. To preserve the structural information on the output\nside along with label information, we extend the previous contrastive hashing\nmethod as structured contrastive hashing. More specifically, we upgrade CKY\nfrom label-level to bit-level, define a new similarity function with span\nmarginal probabilities, and introduce a novel contrastive loss function with a\ncarefully designed instance selection strategy. Our model achieves competitive\nperformance on various structured prediction tasks, and demonstrates that\nbinary representation can be considered a novel representation that further\nbridges the gap between the continuous nature of deep learning and the discrete\nintrinsic property of natural languages.</p>\n", "tags": ["ARXIV","Deep Learning"] },
{"key": "wang2024deep", "year": "2024", "title":"Deep Collaborative Discrete Hashing With Semantic-invariant Structure", "abstract": "<p>Existing deep hashing approaches fail to fully explore semantic correlations and neglect the effect of linguistic context on visual attention learning, leading to inferior performance. This paper proposes a dual-stream learning framework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantic labels. Specifically, the context-aware representations are generated by employing the outer product of visual embeddings and semantic encodings. Moreover, we reconstruct the labels and introduce the focal loss to take advantage of frequent and rare concepts. The common binary code space is built on the joint learning of the visual representations attended by language, the semantic-invariant structure construction and the label distribution correction. Extensive experiments demonstrate the superiority of our method.</p>\n", "tags": ["ARXIV"] },
{"key": "wang2024hamming", "year": "2024", "title":"Hamming Compatible Quantization For Hashing", "abstract": "<p>Hashing is one of the effective techniques for fast\nApproximate Nearest Neighbour (ANN) search.\nTraditional single-bit quantization (SBQ) in most\nhashing methods incurs lots of quantization error\nwhich seriously degrades the search performance.\nTo address the limitation of SBQ, researchers have\nproposed promising multi-bit quantization (MBQ)\nmethods to quantize each projection dimension\nwith multiple bits. However, some MBQ methods\nneed to adopt specific distance for binary code\nmatching instead of the original Hamming distance,\nwhich would significantly decrease the retrieval\nspeed. Two typical MBQ methods Hierarchical\nQuantization and Double Bit Quantization\nretain the Hamming distance, but both of them only\nconsider the projection dimensions during quantization,\nignoring the neighborhood structure of raw\ndata inherent in Euclidean space. In this paper,\nwe propose a multi-bit quantization method named\nHamming Compatible Quantization (HCQ) to preserve\nthe capability of similarity metric between\nEuclidean space and Hamming space by utilizing\nthe neighborhood structure of raw data. Extensive\nexperiment results have shown our approach significantly\nimproves the performance of various stateof-the-art\nhashing methods while maintaining fast\nretrieval speed.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "wang2024idea", "year": "2024", "title":"IDEA An Invariant Perspective For Efficient Domain Adaptive Image Retrieval", "abstract": "<p>In this paper, we investigate the problem of unsupervised domain adaptive hashing, which leverage knowledge from a label-rich source domain to expedite learning to hash on a label-scarce target domain. Although numerous existing approaches attempt to incorporate transfer learning techniques into deep hashing frameworks, they often neglect the essential invariance for adequate alignment between these two domains. Worse yet, these methods fail to distinguish between causal and non-causal effects embedded in images, rendering cross-domain retrieval ineffective. To address these challenges, we propose an Invariance-acquired Domain AdaptivE HAshing (IDEA) model. Our IDEA first decomposes each image into a causal feature representing label information, and a non-causal feature indicating domain information. Subsequently, we generate discriminative hash codes using causal features with consistency learning on both source and target domains. More importantly, we employ a generative model for synthetic samples to simulate the intervention of various non-causal effects, ultimately minimizing their impact on hash codes for domain invariance. Comprehensive experiments conducted on benchmark datasets validate the superior performance of our IDEA compared to a variety of competitive baselines.</p>\n", "tags": ["ARXIV","Cross Modal","Image Retrieval","Unsupervised"] },
{"key": "wang2024neural", "year": "2024", "title":"Neural Locality Sensitive Hashing For Entity Blocking", "abstract": "<p>Locality-sensitive hashing (LSH) is a fundamental algorithmic technique\nwidely employed in large-scale data processing applications, such as\nnearest-neighbor search, entity resolution, and clustering. However, its\napplicability in some real-world scenarios is limited due to the need for\ncareful design of hashing functions that align with specific metrics. Existing\nLSH-based Entity Blocking solutions primarily rely on generic similarity\nmetrics such as Jaccard similarity, whereas practical use cases often demand\ncomplex and customized similarity rules surpassing the capabilities of generic\nsimilarity metrics. Consequently, designing LSH functions for these customized\nsimilarity rules presents considerable challenges. In this research, we propose\na neuralization approach to enhance locality-sensitive hashing by training deep\nneural networks to serve as hashing functions for complex metrics. We assess\nthe effectiveness of this approach within the context of the entity resolution\nproblem, which frequently involves the use of task-specific metrics in\nreal-world applications. Specifically, we introduce NLSHBlock (Neural-LSH\nBlock), a novel blocking methodology that leverages pre-trained language\nmodels, fine-tuned with a novel LSH-based loss function. Through extensive\nevaluations conducted on a diverse range of real-world datasets, we demonstrate\nthe superiority of NLSHBlock over existing methods, exhibiting significant\nperformance improvements. Furthermore, we showcase the efficacy of NLSHBlock in\nenhancing the performance of the entity matching phase, particularly within the\nsemi-supervised setting.</p>\n", "tags": ["ARXIV","LSH","Supervised"] },
{"key": "wang2024online", "year": "2024", "title":"Online Collective Matrix Factorization Hashing For Large-scale Cross-media Retrieval", "abstract": "<p>Cross-modal hashing has been widely investigated recently for its efficiency in large-scale cross-media retrieval. However, most existing cross-modal hashing methods learn hash functions in a batch-based learning mode. Such mode is not suitable for large-scale data sets due to the large memory consumption and loses its efficiency when training streaming data. Online cross-modal hashing can deal with the above problems by learning hash model in an online learning process. However, existing online cross-modal hashing methods cannot update hash codes of old data by the newly learned model. In this paper, we propose Online Collective Matrix Factorization Hashing (OCMFH) based on collective matrix factorization hashing (CMFH), which can adaptively update hash codes of old data according to dynamic changes of hash model without accessing to old data. Specifically, it learns discriminative hash codes for streaming data by collective matrix factorization in an online optimization scheme. Unlike conventional CMFH which needs to load the entire data points into memory, the proposed OCMFH retrains hash functions only by newly arriving data points. Meanwhile, it generates hash codes of new data and updates hash codes of old data by the latest updated hash model. In such way, hash codes of new data and old data are well-matched. Furthermore, a zero mean strategy is developed to solve the mean-varying problem in the online hash learning process. Extensive experiments on three benchmark data sets demonstrate the effectiveness and efficiency of OCMFH on online cross-media retrieval.</p>\n", "tags": ["ARXIV","Cross Modal","Independent","Streaming Data"] },
{"key": "wang2024prototype", "year": "2024", "title":"Prototype-supervised Adversarial Network For Targeted Attack Of Deep Hashing", "abstract": "<p>Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing.</p>\n", "tags": ["ARXIV","GAN","Image Retrieval","Supervised"] },
{"key": "wang2024rreh", "year": "2024", "title":"RREH Reconstruction Relations Embedded Hashing For Semi-paired Cross-modal Retrieval", "abstract": "<p>Known for efficient computation and easy storage, hashing has been\nextensively explored in cross-modal retrieval. The majority of current hashing\nmodels are predicated on the premise of a direct one-to-one mapping between\ndata points. However, in real practice, data correspondence across modalities\nmay be partially provided. In this research, we introduce an innovative\nunsupervised hashing technique designed for semi-paired cross-modal retrieval\ntasks, named Reconstruction Relations Embedded Hashing (RREH). RREH assumes\nthat multi-modal data share a common subspace. For paired data, RREH explores\nthe latent consistent information of heterogeneous modalities by seeking a\nshared representation. For unpaired data, to effectively capture the latent\ndiscriminative features, the high-order relationships between unpaired data and\nanchors are embedded into the latent subspace, which are computed by efficient\nlinear reconstruction. The anchors are sampled from paired data, which improves\nthe efficiency of hash learning. The RREH trains the underlying features and\nthe binary encodings in a unified framework with high-order reconstruction\nrelations preserved. With the well devised objective function and discrete\noptimization algorithm, RREH is designed to be scalable, making it suitable for\nlarge-scale datasets and facilitating efficient cross-modal retrieval. In the\nevaluation process, the proposed is tested with partially paired data to\nestablish its superiority over several existing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "wang2024semantic", "year": "2024", "title":"Semantic Topic Multimodal Hashing For Cross-media Retrieval", "abstract": "<p>Multimodal hashing is essential to cross-media\nsimilarity search for its low storage cost and fast\nquery speed. Most existing multimodal hashing\nmethods embedded heterogeneous data into a common low-dimensional Hamming space, and then\nrounded the continuous embeddings to obtain the\nbinary codes. Yet they usually neglect the inherent discrete nature of hashing for relaxing the discrete constraints, which will cause degraded retrieval performance especially for long codes. For\nthis purpose, a novel Semantic Topic Multimodal\nHashing (STMH) is developed by considering latent semantic information in coding procedure.\nIt\nfirst discovers clustering patterns of texts and robust factorizes the matrix of images to obtain multiple semantic topics of texts and concepts of images.\nThen the learned multimodal semantic features are\ntransformed into a common subspace by their correlations. Finally, each bit of unified hash code\ncan be generated directly by figuring out whether a\ntopic or concept is contained in a text or an image.\nTherefore, the obtained model by STMH is more\nsuitable for hashing scheme as it directly learns discrete hash codes in the coding process. Experimental results demonstrate that the proposed method\noutperforms several state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "wang2024semi", "year": "2024", "title":"Semi-supervised Deep Quantization For Cross-modal Search", "abstract": "<p>The problem of cross-modal similarity search, which aims at making efficient and accurate queries across multiple domains, has become a significant and important research topic. Composite quantization, a compact coding solution superior to hashing techniques, has shown its effectiveness for similarity search. However, most existing works utilizing composite quantization to search multi-domain content only consider either pairwise similarity information or class label information across different domains, which fails to tackle the semi-supervised problem in composite quantization. In this paper, we address the semi-supervised quantization problem by considering: (i) pairwise similarity information (without class label information) across different domains, which captures the intra-document relation, (ii) cross-domain data with class label which can help capture inter-document relation, and (iii) cross-domain data with neither pairwise similarity nor class label which enables the full use of abundant unlabelled information. To the best of our knowledge, we are the first to consider both supervised information (pairwise similarity + class label) and unsupervised information (neither pairwise similarity nor class label) simultaneously in composite quantization. A challenging problem arises: how can we jointly handle these three sorts of information across multiple domains in an efficient way? To tackle this challenge, we propose a novel semi-supervised deep quantization (SSDQ) model that takes both supervised and unsupervised information into account. The proposed SSDQ model is capable of incorporating the above three kinds of information into one single framework when utilizing composite quantization for accurate and efficient queries across different domains. More specifically, we employ a modified deep autoencoder for better latent representation and formulate pairwise similarity loss, supervised quantization loss as well as unsupervised distribution match loss to handle all three types of information. The extensive experiments demonstrate the significant improvement of SSDQ over several state-of-the-art methods on various datasets.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "wang2024sequential", "year": "2024", "title":"Sequential Projection Learning For Hashing With Compact Codes", "abstract": "<p>Hashing based Approximate Nearest Neighbor\n(ANN) search has attracted much attention\ndue to its fast query time and drastically\nreduced storage. However, most of the hashing\nmethods either use random projections or\nextract principal directions from the data to\nderive hash functions. The resulting embedding\nsuffers from poor discrimination when\ncompact codes are used. In this paper, we\npropose a novel data-dependent projection\nlearning method such that each hash function\nis designed to correct the errors made by\nthe previous one sequentially. The proposed\nmethod easily adapts to both unsupervised\nand semi-supervised scenarios and shows significant\nperformance gains over the state-ofthe-art\nmethods on two large datasets containing\nup to 1 million points.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "wang2024survey", "year": "2024", "title":"A Survey On Learning To Hash", "abstract": "<p>Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the\nquery point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this\npaper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving\nthe similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different\nthough quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation\nprotocols, and the general performance analysis, and point out that the quantization algori</p>\n", "tags": ["ARXIV","Quantisation","Survey Paper"] },
{"key": "wang2024uncertainty", "year": "2024", "title":"Uncertainty-aware Unsupervised Video Hashing", "abstract": "<p>Learning to hash has become popular for video retrieval due to its fast speed and low storage consumption. Previous efforts formulate video hashing as training a binary auto-encoder, for which noncontinuous latent representations are optimized by the biased straight-through (ST) back-propagation heuristic. We propose to formulate video hashing as learning a discrete variational auto-encoder with the factorized Bernoulli latent distribution, termed as Bernoulli variational auto-encoder (BerVAE). The corresponding evidence lower bound (ELBO) in our BerVAE implementation leads to closed-form gradient expression, which can be applied to achieve principled training along with some other unbiased gradient estimators. BerVAE enables uncertainty-aware video hashing by predicting the probability distribution of video hash code-words, thus providing reliable uncertainty quantification. Experiments on both simulated and real-world large-scale video data demonstrate that our BerVAE trained with unbiased gradient estimators can achieve the state-of-the-art retrieval performance. Furthermore, we show that quantified uncertainty is highly correlated to video retrieval performance, which can be leveraged to further improve the retrieval accuracy. Our code is available at https://github.com/wangyucheng1234/BerVAE</p>\n", "tags": ["ARXIV","Has Code","Unsupervised","Video Retrieval"] },
{"key": "wang2024weakly", "year": "2024", "title":"Weakly Supervised Deep Hyperspherical Quantization For Image Retrieval", "abstract": "<p>Deep quantization methods have shown high efficiency on large-scale image\nretrieval. However, current models heavily rely on ground-truth information,\nhindering the application of quantization in label-hungry scenarios. A more\nrealistic demand is to learn from inexhaustible uploaded images that are\nassociated with informal tags provided by amateur users. Though such sketchy\ntags do not obviously reveal the labels, they actually contain useful semantic\ninformation for supervising deep quantization. To this end, we propose\nWeakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first\nwork to learn deep quantization from weakly tagged images. Specifically, 1) we\nuse word embeddings to represent the tags and enhance their semantic\ninformation based on a tag correlation graph. 2) To better preserve semantic\ninformation in quantization codes and reduce quantization error, we jointly\nlearn semantics-preserving embeddings and supervised quantizer on hypersphere\nby employing a well-designed fusion layer and tailor-made loss functions.\nExtensive experiments show that WSDHQ can achieve state-of-art performance on\nweakly-supervised compact coding. Code is available at\nhttps://github.com/gimpong/AAAI21-WSDHQ.</p>\n", "tags": ["ARXIV","Graph","Has Code","Image Retrieval","Quantisation","Supervised","Weakly Supervised"] },
{"key": "weaver2019constructing", "year": "2019", "title":"Constructing Minimal Perfect Hash Functions Using SAT Technology", "abstract": "<p>Minimal perfect hash functions (MPHFs) are used to provide efficient access\nto values of large dictionaries (sets of key-value pairs). Discovering new\nalgorithms for building MPHFs is an area of active research, especially from\nthe perspective of storage efficiency. The information-theoretic limit for\nMPHFs is 1/(ln 2) or roughly 1.44 bits per key. The current best practical\nalgorithms range between 2 and 4 bits per key. In this article, we propose two\nSAT-based constructions of MPHFs. Our first construction yields MPHFs near the\ninformation-theoretic limit. For this construction, current state-of-the-art\nSAT solvers can handle instances where the dictionaries contain up to 40\nelements, thereby outperforming the existing (brute-force) methods. Our second\nconstruction uses XOR-SAT filters to realize a practical approach with\nlong-term storage of approximately 1.83 bits per key.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "wei2021learning", "year": "2021", "title":"A^2-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A\\(^2\\)-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A\\(^2\\)-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>\n", "tags": ["Image Retrieval","NEURIPS","Supervised"] },
{"key": "wei2022hyperbolic", "year": "2022", "title":"Hyperbolic Hierarchical Contrastive Hashing", "abstract": "<p>Hierarchical semantic structures, naturally existing in real-world datasets,\ncan assist in capturing the latent distribution of data to learn robust hash\ncodes for retrieval systems. Although hierarchical semantic structures can be\nsimply expressed by integrating semantically relevant data into a high-level\ntaxon with coarser-grained semantics, the construction, embedding, and\nexploitation of the structures remain tricky for unsupervised hash learning. To\ntackle these problems, we propose a novel unsupervised hashing method named\nHyperbolic Hierarchical Contrastive Hashing (HHCH). We propose to embed\ncontinuous hash codes into hyperbolic space for accurate semantic expression\nsince embedding hierarchies in hyperbolic space generates less distortion than\nin hyper-sphere space and Euclidean space. In addition, we extend the K-Means\nalgorithm to hyperbolic space and perform the proposed hierarchical hyperbolic\nK-Means algorithm to construct hierarchical semantic structures adaptively. To\nexploit the hierarchical semantic structures in hyperbolic space, we designed\nthe hierarchical contrastive learning algorithm, including hierarchical\ninstance-wise and hierarchical prototype-wise contrastive learning. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed method\noutperforms the state-of-the-art unsupervised hashing methods. Codes will be\nreleased.</p>\n", "tags": ["ICIP","Unsupervised"] },
{"key": "wei2023attribute", "year": "2023", "title":"Attribute-aware Deep Hashing With Self-consistency For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as\nranking the images depicting the concept of interests (i.e., the same\nsub-category labels) highest based on the fine-grained details in the query. It\nis desirable to alleviate the challenges of both fine-grained nature of small\ninter-class variations with large intra-class variations and explosive growth\nof fine-grained data for such a practical task. In this paper, we propose\nattribute-aware hashing networks with self-consistency for generating\nattribute-aware hash codes to not only make the retrieval process efficient,\nbut also establish explicit correspondences between hash codes and visual\nattributes. Specifically, based on the captured visual representations by\nattention, we develop an encoder-decoder structure network of a reconstruction\ntask to unsupervisedly distill high-level attribute-specific vectors from the\nappearance-specific visual representations without attribute annotations. Our\nmodels are also equipped with a feature decorrelation constraint upon these\nattribute vectors to strengthen their representative abilities. Then, driven by\npreserving original entities’ similarity, the required hash codes can be\ngenerated from these attribute-specific vectors and thus become\nattribute-aware. Furthermore, to combat simplicity bias in deep hashing, we\nconsider the model design from the perspective of the self-consistency\nprinciple and propose to further enhance models’ self-consistency by equipping\nan additional image reconstruction path. Comprehensive quantitative experiments\nunder diverse empirical settings on six fine-grained retrieval datasets and two\ngeneric retrieval datasets show the superiority of our models over competing\nmethods.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "wei2023chain", "year": "2023", "title":"CHAIN Exploring Global-local Spatio-temporal Information For Improved Self-supervised Video Hashing", "abstract": "<p>Compressing videos into binary codes can improve retrieval speed and reduce\nstorage overhead. However, learning accurate hash codes for video retrieval can\nbe challenging due to high local redundancy and complex global dependencies\nbetween video frames, especially in the absence of labels. Existing\nself-supervised video hashing methods have been effective in designing\nexpressive temporal encoders, but have not fully utilized the temporal dynamics\nand spatial appearance of videos due to less challenging and unreliable\nlearning tasks. To address these challenges, we begin by utilizing the\ncontrastive learning task to capture global spatio-temporal information of\nvideos for hashing. With the aid of our designed augmentation strategies, which\nfocus on spatial and temporal variations to create positive pairs, the learning\nframework can generate hash codes that are invariant to motion, scale, and\nviewpoint. Furthermore, we incorporate two collaborative learning tasks, i.e.,\nframe order verification and scene change regularization, to capture local\nspatio-temporal details within video frames, thereby enhancing the perception\nof temporal structure and the modeling of spatio-temporal relationships. Our\nproposed Contrastive Hashing with Global-Local Spatio-temporal Information\n(CHAIN) outperforms state-of-the-art self-supervised video hashing methods on\nfour video benchmark datasets. Our codes will be released.</p>\n", "tags": ["ARXIV","Supervised","Video Retrieval"] },
{"key": "wei2024contrastive", "year": "2024", "title":"Contrastive Masked Auto-encoders Based Self-supervised Hashing For 2D Image And 3D Point Cloud Cross-modal Retrieval", "abstract": "<p>Implementing cross-modal hashing between 2D images and 3D point-cloud data is\na growing concern in real-world retrieval systems. Simply applying existing\ncross-modal approaches to this new task fails to adequately capture latent\nmulti-modal semantics and effectively bridge the modality gap between 2D and\n3D. To address these issues without relying on hand-crafted labels, we propose\ncontrastive masked autoencoders based self-supervised hashing (CMAH) for\nretrieval between images and point-cloud data. We start by contrasting 2D-3D\npairs and explicitly constraining them into a joint Hamming space. This\ncontrastive learning process ensures robust discriminability for the generated\nhash codes and effectively reduces the modality gap. Moreover, we utilize\nmulti-modal auto-encoders to enhance the model’s understanding of multi-modal\nsemantics. By completing the masked image/point-cloud data modeling task, the\nmodel is encouraged to capture more localized clues. In addition, the proposed\nmulti-modal fusion block facilitates fine-grained interactions among different\nmodalities. Extensive experiments on three public datasets demonstrate that the\nproposed CMAH significantly outperforms all baseline methods.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "wei2024learning", "year": "2024", "title":"A-net Learning Attribute-aware Hash Codes For Large-scale Fine-grained Image Retrieval", "abstract": "<p>Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "weiland2018understanding", "year": "2018", "title":"Understanding The Gist Of Images - Ranking Of Concepts For Multimedia Indexing", "abstract": "<p>Nowadays, where multimedia data is continuously generated, stored, and\ndistributed, multimedia indexing, with its purpose of group- ing similar data,\nbecomes more important than ever. Understanding the gist (=message) of\nmultimedia instances is framed in related work as a ranking of concepts from a\nknowledge base, i.e., Wikipedia. We cast the task of multimedia indexing as a\ngist understanding problem. Our pipeline benefits from external knowledge and\ntwo subsequent learning- to-rank (l2r) settings. The first l2r produces a\nranking of concepts rep- resenting the respective multimedia instance. The\nsecond l2r produces a mapping between the concept representation of an instance\nand the targeted class topic(s) for the multimedia indexing task. The\nevaluation on an established big size corpus (MIRFlickr25k, with 25,000\nimages), shows that multimedia indexing benefits from understanding the gist.\nFinally, with a MAP of 61.42, it can be shown that the multimedia in- dexing\ntask benefits from understanding the gist. Thus, the presented end-to-end\nsetting outperforms DBM and competes with Hashing-based methods.</p>\n", "tags": ["ARXIV"] },
{"key": "weinberger2009feature", "year": "2009", "title":"Feature Hashing For Large Scale Multitask Learning", "abstract": "<p>Empirical evidence suggests that hashing is an effective strategy for\ndimensionality reduction and practical nonparametric estimation. In this paper\nwe provide exponential tail bounds for feature hashing and show that the\ninteraction between random subspaces is negligible with high probability. We\ndemonstrate the feasibility of this approach with experimental results for a\nnew use case – multitask learning with hundreds of thousands of tasks.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "weiss2008spectral", "year": "2008", "title":"Spectral Hashing", "abstract": "<p>Semantic hashing seeks compact binary codes of datapoints so that the Hamming distance between codewords correlates with semantic similarity. Hinton et al. used a clever implementation of autoencoders to find such codes. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresh- olded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigen- functions of manifolds, we show how to efficiently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes significantly outperform the state-of-the art.</p>\n", "tags": ["Graph","NEURIPS","Unsupervised"] },
{"key": "weiss2024multidimensional", "year": "2024", "title":"Multidimensional Spectral Hashing", "abstract": "<p>en a surge of interest in methods based on “semantic hashing”,\ni.e. compact binary codes of data-points so that the Hamming distance\nbetween codewords correlates with similarity. In reviewing and\ncomparing existing methods, we show that their relative performance can\nchange drastically depending on the definition of ground-truth neighbors.\nMotivated by this finding, we propose a new formulation for learning binary\ncodes which seeks to reconstruct the affinity between datapoints,\nrather than their distances. We show that this criterion is intractable\nto solve exactly, but a spectral relaxation gives an algorithm where the\nbits correspond to thresholded eigenvectors of the affinity matrix, and\nas the number of datapoints goes to infinity these eigenvectors converge\nto eigenfunctions of Laplace-Beltrami operators, similar to the recently\nproposed Spectral Hashing (SH) method. Unlike SH whose performance\nmay degrade as the number of bits increases, the optimal code using\nour formulation is guaranteed to faithfully reproduce the affinities as\nthe number of bits increases. We show that the number of eigenfunctions\nneeded may increase exponentially with dimension, but introduce a “kernel\ntrick” to allow us to compute with an exponentially large number of\nbits but using only memory and computation that grows linearly with\ndimension. Experiments shows that MDSH outperforms the state-of-the\nart, especially in the challenging regime of small distance thresholds.</p>\n", "tags": ["ARXIV"] },
{"key": "weiss2024spectral", "year": "2024", "title":"Spectral Hashing", "abstract": "<p>Semantic hashing seeks compact binary codes of data-points so that the\nHamming distance between codewords correlates with semantic similarity.\nIn this paper, we show that the problem of finding a best code for a given\ndataset is closely related to the problem of graph partitioning and can\nbe shown to be NP hard. By relaxing the original problem, we obtain a\nspectral method whose solutions are simply a subset of thresholded eigenvectors\nof the graph Laplacian. By utilizing recent results on convergence\nof graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of\nmanifolds, we show how to efficiently calculate the code of a novel datapoint.\nTaken together, both learning the code and applying it to a novel\npoint are extremely simple. Our experiments show that our codes outperform\nthe state-of-the art.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "weng2019efficient", "year": "2019", "title":"Efficient Querying From Weighted Binary Codes", "abstract": "<p>Binary codes are widely used to represent the data due to their small storage\nand efficient computation. However, there exists an ambiguity problem that lots\nof binary codes share the same Hamming distance to a query. To alleviate the\nambiguity problem, weighted binary codes assign different weights to each bit\nof binary codes and compare the binary codes by the weighted Hamming distance.\nTill now, performing the querying from the weighted binary codes efficiently is\nstill an open issue. In this paper, we propose a new method to rank the\nweighted binary codes and return the nearest weighted binary codes of the query\nefficiently. In our method, based on the multi-index hash tables, two\nalgorithms, the table bucket finding algorithm and the table merging algorithm,\nare proposed to select the nearest weighted binary codes of the query in a\nnon-exhaustive and accurate way. The proposed algorithms are justified by\nproving their theoretic properties. The experiments on three large-scale\ndatasets validate both the search efficiency and the search accuracy of our\nmethod. Especially for the number of weighted binary codes up to one billion,\nour method shows a great improvement of more than 1000 times faster than the\nlinear scan.</p>\n", "tags": ["ARXIV"] },
{"key": "weng2019online", "year": "2019", "title":"Online Hashing With Efficient Updating Of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the\nstreaming data. However, when the hash functions change, the binary codes for\nthe database have to be recomputed to guarantee the retrieval accuracy.\nRecomputing the binary codes by accumulating the whole database brings a\ntimeliness challenge to the online retrieval process. In this paper, we propose\na novel online hashing framework to update the binary codes efficiently without\naccumulating the whole database. In our framework, the hash functions are fixed\nand the projection functions are introduced to learn online from the streaming\ndata. Therefore, inefficient updating of the binary codes by accumulating the\nwhole database can be transformed to efficient updating of the binary codes by\nprojecting the binary codes into another binary space. The queries and the\nbinary code database are projected asymmetrically to further improve the\nretrieval accuracy. The experiments on two multi-label image databases\ndemonstrate the effectiveness and the efficiency of our method for multi-label\nimage retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Streaming Data"] },
{"key": "weng2020fast", "year": "2020", "title":"Fast Search On Binary Codes By Weighted Hamming Distance", "abstract": "<p>Weighted Hamming distance, as a similarity measure between binary codes and\nbinary queries, provides superior accuracy in search tasks than Hamming\ndistance. However, how to efficiently and accurately find \\(K\\) binary codes that\nhave the smallest weighted Hamming distance to the query remains an open issue.\nIn this paper, a fast search algorithm is proposed to perform the\nnon-exhaustive search for \\(K\\) nearest binary codes by weighted Hamming\ndistance. By using binary codes as direct bucket indices in a hash table, the\nsearch algorithm generates a sequence to probe the buckets based on the\nindependence characteristic of the weights for each bit. Furthermore, a fast\nsearch framework based on the proposed search algorithm is designed to solve\nthe problem of long binary codes. Specifically, long binary codes are split\ninto substrings and multiple hash tables are built on them. Then, the search\nalgorithm probes the buckets to obtain candidates according to the generated\nsubstring indices, and a merging algorithm is proposed to find the nearest\nbinary codes by merging the candidates. Theoretical analysis and experimental\nresults demonstrate that the search algorithm improves the search accuracy\ncompared to other non-exhaustive algorithms and provides orders-of-magnitude\nfaster search than the linear scan baseline.</p>\n", "tags": ["ARXIV"] },
{"key": "weng2020random", "year": "2020", "title":"Random VLAD Based Deep Hashing For Efficient Image Retrieval", "abstract": "<p>Image hash algorithms generate compact binary representations that can be\nquickly matched by Hamming distance, thus become an efficient solution for\nlarge-scale image retrieval. This paper proposes RV-SSDH, a deep image hash\nalgorithm that incorporates the classical VLAD (vector of locally aggregated\ndescriptors) architecture into neural networks. Specifically, a novel neural\nnetwork component is formed by coupling a random VLAD layer with a latent hash\nlayer through a transform layer. This component can be combined with\nconvolutional layers to realize a hash algorithm. We implement RV-SSDH as a\npoint-wise algorithm that can be efficiently trained by minimizing\nclassification error and quantization loss. Comprehensive experiments show this\nnew architecture significantly outperforms baselines such as NetVLAD and SSDH,\nand offers a cost-effective trade-off in the state-of-the-art. In addition, the\nproposed random VLAD layer leads to satisfactory accuracy with low complexity,\nthus shows promising potentials as an alternative to NetVLAD.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Quantisation"] },
{"key": "weng2021online", "year": "2021", "title":"Online Hashing With Similarity Learning", "abstract": "<p>Online hashing methods usually learn the hash functions online, aiming to\nefficiently adapt to the data variations in the streaming environment. However,\nwhen the hash functions are updated, the binary codes for the whole database\nhave to be updated to be consistent with the hash functions, resulting in the\ninefficiency in the online image retrieval process. In this paper, we propose a\nnovel online hashing framework without updating binary codes. In the proposed\nframework, the hash functions are fixed and a parametric similarity function\nfor the binary codes is learnt online to adapt to the streaming data.\nSpecifically, a parametric similarity function that has a bilinear form is\nadopted and a metric learning algorithm is proposed to learn the similarity\nfunction online based on the characteristics of the hashing methods. The\nexperiments on two multi-label image datasets show that our method is\ncompetitive or outperforms the state-of-the-art online hashing methods in terms\nof both accuracy and efficiency for multi-label image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Streaming Data"] },
{"key": "weng2023constant", "year": "2023", "title":"Constant Sequence Extension For Fast Search Using Weighted Hamming Distance", "abstract": "<p>Representing visual data using compact binary codes is attracting increasing\nattention as binary codes are used as direct indices into hash table(s) for\nfast non-exhaustive search. Recent methods show that ranking binary codes using\nweighted Hamming distance (WHD) rather than Hamming distance (HD) by generating\nquery-adaptive weights for each bit can better retrieve query-related items.\nHowever, search using WHD is slower than that using HD. One main challenge is\nthat the complexity of extending a monotone increasing sequence using WHD to\nprobe buckets in hash table(s) for existing methods is at least proportional to\nthe square of the sequence length, while that using HD is proportional to the\nsequence length. To overcome this challenge, we propose a novel fast\nnon-exhaustive search method using WHD. The key idea is to design a constant\nsequence extension algorithm to perform each sequence extension in constant\ncomputational complexity and the total complexity is proportional to the\nsequence length, which is justified by theoretical analysis. Experimental\nresults show that our method is faster than other WHD-based search methods.\nAlso, compared with the HD-based non-exhaustive search method, our method has\ncomparable efficiency but retrieves more query-related items for the dataset of\nup to one billion items.</p>\n", "tags": ["ARXIV"] },
{"key": "weng2024online", "year": "2024", "title":"Online Hashing With Efficient Updating Of Binary Codes", "abstract": "<p>Online hashing methods are efficient in learning the hash functions from the streaming data. However, when the hash functions change, the binary codes for the database have to be recomputed to guarantee the retrieval accuracy. Recomputing the binary codes by accumulating the whole database brings a timeliness challenge to the online retrieval process. In this paper, we propose a novel online hashing framework to update the binary codes efficiently without accumulating the whole database. In our framework, the hash functions are fixed and the projection functions are introduced to learn online from the streaming data. Therefore, inefficient updating of the binary codes by accumulating the whole database can be transformed to efficient updating of the binary codes by projecting the binary codes into another binary space. The queries and the binary code database are projected asymmetrically to further improve the retrieval accuracy. The experiments on two multi-label image databases demonstrate the effectiveness and the efficiency of our method for multi-label image retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent","Streaming Data"] },
{"key": "westermann2021sentence", "year": "2021", "title":"Sentence Embeddings And High-speed Similarity Search For Fast Computer Assisted Annotation Of Legal Documents", "abstract": "<p>Human-performed annotation of sentences in legal documents is an important\nprerequisite to many machine learning based systems supporting legal tasks.\nTypically, the annotation is done sequentially, sentence by sentence, which is\noften time consuming and, hence, expensive. In this paper, we introduce a\nproof-of-concept system for annotating sentences “laterally.” The approach is\nbased on the observation that sentences that are similar in meaning often have\nthe same label in terms of a particular type system. We use this observation in\nallowing annotators to quickly view and annotate sentences that are\nsemantically similar to a given sentence, across an entire corpus of documents.\nHere, we present the interface of the system and empirically evaluate the\napproach. The experiments show that lateral annotation has the potential to\nmake the annotation process quicker and more consistent.</p>\n", "tags": [] },
{"key": "westover2023relationship", "year": "2023", "title":"On The Relationship Between Several Variants Of The Linear Hashing Conjecture", "abstract": "<p>In Linear Hashing (\\(\\mathsf{LH}\\)) with \\(\\beta\\) bins on a size \\(u\\) universe\n\\({\\mathcal{U}=\\{0,1,\\ldots, u-1\\}}\\), items \\(\\{x_1,x_2,\\ldots, x_n\\}\\subset\n\\mathcal{U}\\) are placed in bins by the hash function $\\(x_i\\mapsto (ax_i+b)\\mod\np \\mod \\beta\\)\\( for some prime \\)p\\in [u,2u]\\( and randomly chosen integers \\)a,b\n\\in [1,p]\\(. The “maxload” of \\)\\mathsf{LH}\\( is the number of items assigned to\nthe fullest bin. Expected maxload for a worst-case set of items is a natural\nmeasure of how well \\)\\mathsf{LH}\\( distributes items amongst the bins.\n  Fix \\)\\beta=n\\(. Despite \\)\\mathsf{LH}\\(‘s simplicity, bounding \\)\\mathsf{LH}\\(‘s\nworst-case maxload is extremely challenging. It is well-known that on random\ninputs \\)\\mathsf{LH}\\( achieves maxload \\)Ω\\left(\\frac{log n}{loglog\nn}\\right)\\(; this is currently the best lower bound for \\)\\mathsf{LH}\\(‘s expected\nmaxload. Recently Knudsen established an upper bound of \\)\\widetilde{O}(n^{1 /\n3})\\(. The question “Is the worst-case expected maxload of \\)\\mathsf{LH}\\(\n\\)n^{o(1)}$?” is one of the most basic open problems in discrete math.\n  In this paper we propose a set of intermediate open questions to help\nresearchers make progress on this problem. We establish the relationship\nbetween these intermediate open questions and make some partial progress on\nthem.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "white2020practical", "year": "2020", "title":"A Practical Blockchain Framework Using Image Hashing For Image Authentication", "abstract": "<p>Blockchain is a relatively new technology that can be seen as a decentralised\ndatabase. Blockchain systems heavily rely on cryptographic hash functions to\nstore their data, which makes it difficult to tamper with any data stored in\nthe system. A topic that was researched along with blockchain is image\nauthentication. Image authentication focuses on investigating and maintaining\nthe integrity of images. As a blockchain system can be useful for maintaining\ndata integrity, image authentication has the potential to be enhanced by\nblockchain. There are many techniques that can be used to authenticate images;\nthe technique investigated by this work is image hashing. Image hashing is a\ntechnique used to calculate how similar two different images are. This is done\nby converting the images into hashes and then comparing them using a distance\nformula. To investigate the topic, an experiment involving a simulated\nblockchain was created. The blockchain acted as a database for images. This\nblockchain was made up of devices which contained their own unique image\nhashing algorithms. The blockchain was tested by creating modified copies of\nthe images contained in the database, and then submitting them to the\nblockchain to see if it will return the original image. Through this experiment\nit was discovered that it is plausible to create an image authentication system\nusing blockchain and image hashing. However, the design proposed by this work\nrequires refinement, as it appears to struggle in some situations. This work\nshows that blockchain can be a suitable approach for authenticating images,\nparticularly via image hashing. Other observations include that using multiple\nimage hash algorithms at the same time can increase performance in some cases,\nas well as that each type of test done to the blockchain has its own unique\npattern to its data.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "wieschollek2017efficient", "year": "2017", "title":"Efficient Large-scale Approximate Nearest Neighbor Search On The GPU", "abstract": "<p>We present a new approach for efficient approximate nearest neighbor (ANN)\nsearch in high dimensional spaces, extending the idea of Product Quantization.\nWe propose a two-level product and vector quantization tree that reduces the\nnumber of vector comparisons required during tree traversal. Our approach also\nincludes a novel highly parallelizable re-ranking method for candidate vectors\nby efficiently reusing already computed intermediate values. Due to its small\nmemory footprint during traversal, the method lends itself to an efficient,\nparallel GPU implementation. This Product Quantization Tree (PQT) approach\nsignificantly outperforms recent state of the art methods for high dimensional\nnearest neighbor queries on standard reference datasets. Ours is the first work\nthat demonstrates GPU performance superior to CPU performance on high\ndimensional, large scale ANN problems in time-critical real-world applications,\nlike loop-closing in videos.</p>\n", "tags": ["Quantisation"] },
{"key": "wiki2010new", "year": "2010", "title":"A New Approach to Cross-Modal Multimedia Retrieval", "abstract": "<p>The collected documents are selected sections from the Wikipedia’s featured articles collection. This is a continuously growing dataset, that at the time of collection (October 2009) had 2,669 articles spread over 29 categories. Some of the categories are very scarce, therefore we considered only the 10 most populated ones. The articles generally have multiple sections and pictures. We have split them into sections based on section headings, and assign each image to the section in which it was placed by the author(s). Then this dataset was prunned to keep only sections that contained a single image and at least 70 words. \nThe final corpus contains 2,866 multimedia documents. The median text length is 200 words.</p>\n", "tags": ["Dataset"] },
{"key": "wild2004asymptotic", "year": "2004", "title":"The Asymptotic Number Of Binary Codes And Binary Matroids", "abstract": "<p>The asyptotic number of nonequivalent binary n-codes is determined. This is\nalso the asymptotic number of nonisomorphic binary n-matroids. The connection\nto a result of Lefmann, Roedl, Phelps is explored. The latter states that\nalmost all binary n-codes have a trivial automorphism group.</p>\n", "tags": [] },
{"key": "wu2016robust", "year": "2016", "title":"Robust Hashing For Multi-view Data Jointly Learning Low-rank Kernelized Similarity Consensus And Hash Functions", "abstract": "<p>Learning hash functions/codes for similarity search over multi-view data is\nattracting increasing attention, where similar hash codes are assigned to the\ndata objects characterizing consistently neighborhood relationship across\nviews. Traditional methods in this category inherently suffer three\nlimitations: 1) they commonly adopt a two-stage scheme where similarity matrix\nis first constructed, followed by a subsequent hash function learning; 2) these\nmethods are commonly developed on the assumption that data samples with\nmultiple representations are noise-free,which is not practical in real-life\napplications; 3) they often incur cumbersome training model caused by the\nneighborhood graph construction using all \\(N\\) points in the database (\\(O(N)\\)).\nIn this paper, we motivate the problem of jointly and efficiently training the\nrobust hash functions over data objects with multi-feature representations\nwhich may be noise corrupted. To achieve both the robustness and training\nefficiency, we propose an approach to effectively and efficiently learning\nlow-rank kernelized \\footnote{We use kernelized similarity rather than kernel,\nas it is not a squared symmetric matrix for data-landmark affinity matrix.}\nhash functions shared across views. Specifically, we utilize landmark graphs to\nconstruct tractable similarity matrices in multi-views to automatically\ndiscover neighborhood structure in the data. To learn robust hash functions, a\nlatent low-rank kernel function is used to construct hash functions in order to\naccommodate linearly inseparable data. In particular, a latent kernelized\nsimilarity matrix is recovered by rank minimization on multiple kernel-based\nsimilarity matrices. Extensive experiments on real-world multi-view datasets\nvalidate the efficacy of our method in the presence of error corruptions.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Independent"] },
{"key": "wu2017momentsnet", "year": "2017", "title":"Momentsnet A Simple Learning-free Method For Binary Image Recognition", "abstract": "<p>In this paper, we propose a new simple and learning-free deep learning\nnetwork named MomentsNet, whose convolution layer, nonlinear processing layer\nand pooling layer are constructed by Moments kernels, binary hashing and\nblock-wise histogram, respectively. Twelve typical moments (including\ngeometrical moment, Zernike moment, Tchebichef moment, etc.) are used to\nconstruct the MomentsNet whose recognition performance for binary image is\nstudied. The results reveal that MomentsNet has better recognition performance\nthan its corresponding moments in almost all cases and ZernikeNet achieves the\nbest recognition performance among MomentsNet constructed by twelve moments.\nZernikeNet also shows better recognition performance on binary image database\nthan that of PCANet, which is a learning-based deep learning network.</p>\n", "tags": ["ARXIV","Deep Learning","Unsupervised"] },
{"key": "wu2017multiscale", "year": "2017", "title":"Multiscale Quantization For Fast Similarity Search", "abstract": "<p>We propose a multiscale quantization approach for fast similarity search on large, high-dimensional datasets. The key insight of the approach is that quantization methods, in particular product quantization, perform poorly when there is large variance in the norms of the data points. This is a common scenario for real- world datasets, especially when doing product quantization of residuals obtained from coarse vector quantization. To address this issue, we propose a multiscale formulation where we learn a separate scalar quantizer of the residual norm scales. All parameters are learned jointly in a stochastic gradient descent framework to minimize the overall quantization error. We provide theoretical motivation for the proposed technique and conduct comprehensive experiments on two large-scale public datasets, demonstrating substantial improvements in recall over existing state-of-the-art methods.</p>\n", "tags": ["NEURIPS","Quantisation"] },
{"key": "wu2017structured", "year": "2017", "title":"Structured Deep Hashing With Convolutional Neural Networks For Fast Person Re-identification", "abstract": "<p>Given a pedestrian image as a query, the purpose of person re-identification\nis to identify the correct match from a large collection of gallery images\ndepicting the same person captured by disjoint camera views. The critical\nchallenge is how to construct a robust yet discriminative feature\nrepresentation to capture the compounded variations in pedestrian appearance.\nTo this end, deep learning methods have been proposed to extract hierarchical\nfeatures against extreme variability of appearance. However, existing methods\nin this category generally neglect the efficiency in the matching stage whereas\nthe searching speed of a re-identification system is crucial in real-world\napplications. In this paper, we present a novel deep hashing framework with\nConvolutional Neural Networks (CNNs) for fast person re-identification.\nTechnically, we simultaneously learn both CNN features and hash functions/codes\nto get robust yet discriminative features and similarity-preserving hash codes.\nThereby, person re-identification can be resolved by efficiently computing and\nranking the Hamming distances between images. A structured loss function\ndefined over positive pairs and hard negatives is proposed to formulate a novel\noptimization problem so that fast convergence and more stable optimized\nsolution can be obtained. Extensive experiments on two benchmarks CUHK03\n\\cite{FPNN} and Market-1501 \\cite{Market1501} show that the proposed deep\narchitecture is efficacy over state-of-the-arts.</p>\n", "tags": ["ARXIV","CNN","Deep Learning","Supervised"] },
{"key": "wu2018cycle", "year": "2018", "title":"Cycle-consistent Deep Generative Hashing For Cross-modal Retrieval", "abstract": "<p>In this paper, we propose a novel deep generative approach to cross-modal\nretrieval to learn hash functions in the absence of paired training samples\nthrough the cycle consistency loss. Our proposed approach employs adversarial\ntraining scheme to lean a couple of hash functions enabling translation between\nmodalities while assuming the underlying semantic relationship. To induce the\nhash codes with semantics to the input-output pair, cycle consistency loss is\nfurther proposed upon the adversarial training to strengthen the correlations\nbetween inputs and corresponding outputs. Our approach is generative to learn\nhash functions such that the learned hash codes can maximally correlate each\ninput-output correspondence, meanwhile can also regenerate the inputs so as to\nminimize the information loss. The learning to hash embedding is thus performed\nto jointly optimize the parameters of the hash functions across modalities as\nwell as the associated generative models. Extensive experiments on a variety of\nlarge-scale cross-modal data sets demonstrate that our proposed method achieves\nbetter retrieval results than the state-of-the-arts.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "wu2018learning", "year": "2018", "title":"Learning Product Codebooks Using Vector Quantized Autoencoders For Image Retrieval", "abstract": "<p>Vector-Quantized Variational Autoencoders (VQ-VAE)[1] provide an unsupervised\nmodel for learning discrete representations by combining vector quantization\nand autoencoders. In this paper, we study the use of VQ-VAE for representation\nlearning for downstream tasks, such as image retrieval. We first describe the\nVQ-VAE in the context of an information-theoretic framework. We show that the\nregularization term on the learned representation is determined by the size of\nthe embedded codebook before the training and it affects the generalization\nability of the model. As a result, we introduce a hyperparameter to balance the\nstrength of the vector quantizer and the reconstruction error. By tuning the\nhyperparameter, the embedded bottleneck quantizer is used as a regularizer that\nforces the output of the encoder to share a constrained coding space such that\nlearned latent features preserve the similarity relations of the data space. In\naddition, we provide a search range for finding the best hyperparameter.\nFinally, we incorporate the product quantization into the bottleneck stage of\nVQ-VAE and propose an end-to-end unsupervised learning model for the image\nretrieval task. The product quantizer has the advantage of generating\nlarge-size codebooks. Fast retrieval can be achieved by using the lookup tables\nthat store the distance between any pair of sub-codewords. State-of-the-art\nretrieval results are achieved by the learned codebooks.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "wu2018local", "year": "2018", "title":"Local Density Estimation In High Dimensions", "abstract": "<p>An important question that arises in the study of high dimensional vector\nrepresentations learned from data is: given a set \\(\\mathcal{D}\\) of vectors and\na query \\(q\\), estimate the number of points within a specified distance\nthreshold of \\(q\\). We develop two estimators, LSH Count and Multi-Probe Count\nthat use locality sensitive hashing to preprocess the data to accurately and\nefficiently estimate the answers to such questions via importance sampling. A\nkey innovation is the ability to maintain a small number of hash tables via\npreprocessing data structures and algorithms that sample from multiple buckets\nin each hash table. We give bounds on the space requirements and sample\ncomplexity of our schemes, and demonstrate their effectiveness in experiments\non a standard word embedding dataset.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "wu2018review", "year": "2018", "title":"A Review For Weighted Minhash Algorithms", "abstract": "<p>Data similarity (or distance) computation is a fundamental research topic\nwhich underpins many high-level applications based on similarity measures in\nmachine learning and data mining. However, in large-scale real-world scenarios,\nthe exact similarity computation has become daunting due to “3V” nature\n(volume, velocity and variety) of big data. In such cases, the hashing\ntechniques have been verified to efficiently conduct similarity estimation in\nterms of both theory and practice. Currently, MinHash is a popular technique\nfor efficiently estimating the Jaccard similarity of binary sets and\nfurthermore, weighted MinHash is generalized to estimate the generalized\nJaccard similarity of weighted sets. This review focuses on categorizing and\ndiscussing the existing works of weighted MinHash algorithms. In this review,\nwe mainly categorize the Weighted MinHash algorithms into quantization-based\napproaches, “active index”-based ones and others, and show the evolution and\ninherent connection of the weighted MinHash algorithms, from the integer\nweighted MinHash algorithms to real-valued weighted MinHash ones (particularly\nthe Consistent Weighted Sampling scheme). Also, we have developed a python\ntoolbox for the algorithms, and released it in our github. Based on the\ntoolbox, we experimentally conduct a comprehensive comparative study of the\nstandard MinHash algorithm and the weighted MinHash ones.</p>\n", "tags": ["ARXIV","Independent","Quantisation","Survey Paper"] },
{"key": "wu2019efficient", "year": "2019", "title":"Efficient Inner Product Approximation In Hybrid Spaces", "abstract": "<p>Many emerging use cases of data mining and machine learning operate on large\ndatasets with data from heterogeneous sources, specifically with both sparse\nand dense components. For example, dense deep neural network embedding vectors\nare often used in conjunction with sparse textual features to provide high\ndimensional hybrid representation of documents. Efficient search in such hybrid\nspaces is very challenging as the techniques that perform well for sparse\nvectors have little overlap with those that work well for dense vectors.\nPopular techniques like Locality Sensitive Hashing (LSH) and its data-dependent\nvariants also do not give good accuracy in high dimensional hybrid spaces. Even\nthough hybrid scenarios are becoming more prevalent, currently there exist no\nefficient techniques in literature that are both fast and accurate. In this\npaper, we propose a technique that approximates the inner product computation\nin hybrid vectors, leading to substantial speedup in search while maintaining\nhigh accuracy. We also propose efficient data structures that exploit modern\ncomputer architectures, resulting in orders of magnitude faster search than the\nexisting baselines. The performance of the proposed method is demonstrated on\nseveral datasets including a very large scale industrial dataset containing one\nbillion vectors in a billion dimensional space, achieving over 10x speedup and\nhigher accuracy against competitive baselines.</p>\n", "tags": ["ARXIV","Cross Modal","LSH","Supervised"] },
{"key": "wu2019optimal", "year": "2019", "title":"Optimal Few-weight Codes From Simplicial Complexes", "abstract": "<p>Recently, some infinite families of binary minimal and optimal linear codes\nare constructed from simplicial complexes by Hyun {\\em et al}. Inspired by\ntheir work, we present two new constructions of codes over the ring \\(\\Bbb\nF_2+u\\Bbb F_2\\) by employing simplicial complexes. When the simplicial complexes\nare all generated by a maximal element, we determine the Lee weight\ndistributions of two classes of the codes over \\(\\Bbb F_2+u\\Bbb F_2\\). Our\nresults show that the codes have few Lee weights. Via the Gray map, we obtain\nan infinite family of binary codes meeting the Griesmer bound and a class of\nbinary distance optimal codes.</p>\n", "tags": ["ARXIV"] },
{"key": "wu2021hashing", "year": "2021", "title":"Hashing-accelerated Graph Neural Networks For Link Prediction", "abstract": "<p>Networks are ubiquitous in the real world. Link prediction, as one of the key\nproblems for network-structured data, aims to predict whether there exists a\nlink between two nodes. The traditional approaches are based on the explicit\nsimilarity computation between the compact node representation by embedding\neach node into a low-dimensional space. In order to efficiently handle the\nintensive similarity computation in link prediction, the hashing technique has\nbeen successfully used to produce the node representation in the Hamming space.\nHowever, the hashing-based link prediction algorithms face accuracy loss from\nthe randomized hashing techniques or inefficiency from the learning to hash\ntechniques in the embedding process. Currently, the Graph Neural Network (GNN)\nframework has been widely applied to the graph-related tasks in an end-to-end\nmanner, but it commonly requires substantial computational resources and memory\ncosts due to massive parameter learning, which makes the GNN-based algorithms\nimpractical without the help of a powerful workhorse. In this paper, we propose\na simple and effective model called #GNN, which balances the trade-off between\naccuracy and efficiency. #GNN is able to efficiently acquire node\nrepresentation in the Hamming space for link prediction by exploiting the\nrandomized hashing technique to implement message passing and capture\nhigh-order proximity in the GNN framework. Furthermore, we characterize the\ndiscriminative power of #GNN in probability. The extensive experimental results\ndemonstrate that the proposed #GNN algorithm achieves accuracy comparable to\nthe learning-based algorithms and outperforms the randomized algorithm, while\nrunning significantly faster than the learning-based algorithms. Also, the\nproposed algorithm shows excellent scalability on a large-scale network with\nthe limited resources.</p>\n", "tags": ["Graph","Supervised"] },
{"key": "wu2021linear", "year": "2021", "title":"Linear-time Self Attention With Codeword Histogram For Efficient Recommendation", "abstract": "<p>Self-attention has become increasingly popular in a variety of sequence\nmodeling tasks from natural language processing to recommendation, due to its\neffectiveness. However, self-attention suffers from quadratic computational and\nmemory complexities, prohibiting its applications on long sequences. Existing\napproaches that address this issue mainly rely on a sparse attention context,\neither using a local window, or a permuted bucket obtained by\nlocality-sensitive hashing (LSH) or sorting, while crucial information may be\nlost. Inspired by the idea of vector quantization that uses cluster centroids\nto approximate items, we propose LISA (LInear-time Self Attention), which\nenjoys both the effectiveness of vanilla self-attention and the efficiency of\nsparse attention. LISA scales linearly with the sequence length, while enabling\nfull contextual attention via computing differentiable histograms of codeword\ndistributions. Meanwhile, unlike some efficient attention methods, our method\nposes no restriction on casual masking or sequence length. We evaluate our\nmethod on four real-world datasets for sequential recommendation. The results\nshow that LISA outperforms the state-of-the-art efficient attention methods in\nboth performance and speed; and it is up to 57x faster and 78x more memory\nefficient than vanilla self-attention.</p>\n", "tags": ["ARXIV","Independent","LSH","Quantisation"] },
{"key": "wu2021online", "year": "2021", "title":"Online Enhanced Semantic Hashing Towards Effective And Efficient Retrieval For Streaming Multi-modal Data", "abstract": "<p>With the vigorous development of multimedia equipment and applications,\nefficient retrieval of large-scale multi-modal data has become a trendy\nresearch topic. Thereinto, hashing has become a prevalent choice due to its\nretrieval efficiency and low storage cost. Although multi-modal hashing has\ndrawn lots of attention in recent years, there still remain some problems. The\nfirst point is that existing methods are mainly designed in batch mode and not\nable to efficiently handle streaming multi-modal data. The second point is that\nall existing online multi-modal hashing methods fail to effectively handle\nunseen new classes which come continuously with streaming data chunks. In this\npaper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).\nWe design novel semantic-enhanced representation for data, which could help\nhandle the new coming classes, and thereby construct the enhanced semantic\nobjective function. An efficient and effective discrete online optimization\nalgorithm is further proposed for OASIS. Extensive experiments show that our\nmethod can exceed the state-of-the-art models. For good reproducibility and\nbenefiting the community, our code and data are already available in\nsupplementary material and will be made publicly available.</p>\n", "tags": ["ARXIV","Streaming Data"] },
{"key": "wu2022hierarchical", "year": "2022", "title":"Hierarchical Locality Sensitive Hashing For Structured Data A Survey", "abstract": "<p>Data similarity (or distance) computation is a fundamental research topic\nwhich fosters a variety of similarity-based machine learning and data mining\napplications. In big data analytics, it is impractical to compute the exact\nsimilarity of data instances due to high computational cost. To this end, the\nLocality Sensitive Hashing (LSH) technique has been proposed to provide\naccurate estimators for various similarity measures between sets or vectors in\nan efficient manner without the learning process. Structured data (e.g.,\nsequences, trees and graphs), which are composed of elements and relations\nbetween the elements, are commonly seen in the real world, but the traditional\nLSH algorithms cannot preserve the structure information represented as\nrelations between elements. In order to conquer the issue, researchers have\nbeen devoted to the family of the hierarchical LSH algorithms. In this paper,\nwe explore the present progress of the research into hierarchical LSH from the\nfollowing perspectives: 1) Data structures, where we review various\nhierarchical LSH algorithms for three typical data structures and uncover their\ninherent connections; 2) Applications, where we review the hierarchical LSH\nalgorithms in multiple application scenarios; 3) Challenges, where we discuss\nsome potential challenges as future directions.</p>\n", "tags": ["ARXIV","Graph","Independent","LSH","Survey Paper"] },
{"key": "wu2022hqann", "year": "2022", "title":"HQANN Efficient And Robust Similarity Search For Hybrid Queries With Structured And Unstructured Constraints", "abstract": "<p>The in-memory approximate nearest neighbor search (ANNS) algorithms have\nachieved great success for fast high-recall query processing, but are extremely\ninefficient when handling hybrid queries with unstructured (i.e., feature\nvectors) and structured (i.e., related attributes) constraints. In this paper,\nwe present HQANN, a simple yet highly efficient hybrid query processing\nframework which can be easily embedded into existing proximity graph-based ANNS\nalgorithms. We guarantee both low latency and high recall by leveraging\nnavigation sense among attributes and fusing vector similarity search with\nattribute filtering. Experimental results on both public and in-house datasets\ndemonstrate that HQANN is 10x faster than the state-of-the-art hybrid ANNS\nsolutions to reach the same recall quality and its performance is hardly\naffected by the complexity of attributes. It can reach 99\\% recall@10 in just\naround 50 microseconds On GLOVE-1.2M with thousands of attribute constraints.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "wu2022self", "year": "2022", "title":"Self-supervised Consistent Quantization For Fully Unsupervised Image Retrieval", "abstract": "<p>Unsupervised image retrieval aims to learn an efficient retrieval system\nwithout expensive data annotations, but most existing methods rely heavily on\nhandcrafted feature descriptors or pre-trained feature extractors. To minimize\nhuman supervision, recent advance proposes deep fully unsupervised image\nretrieval aiming at training a deep model from scratch to jointly optimize\nvisual features and quantization codes. However, existing approach mainly\nfocuses on instance contrastive learning without considering underlying\nsemantic structure information, resulting in sub-optimal performance. In this\nwork, we propose a novel self-supervised consistent quantization approach to\ndeep fully unsupervised image retrieval, which consists of part consistent\nquantization and global consistent quantization. In part consistent\nquantization, we devise part neighbor semantic consistency learning with\ncodeword diversity regularization. This allows to discover underlying neighbor\nstructure information of sub-quantized representations as self-supervision. In\nglobal consistent quantization, we employ contrastive learning for both\nembedding and quantized representations and fuses these representations for\nconsistent contrastive regularization between instances. This can make up for\nthe loss of useful representation information during quantization and\nregularize consistency between instances. With a unified learning objective of\npart and global consistent quantization, our approach exploits richer\nself-supervision cues to facilitate model learning. Extensive experiments on\nthree benchmark datasets show the superiority of our approach over the\nstate-of-the-art methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "wu2024deep", "year": "2024", "title":"Deep Incremental Hashing Network For Efficient Image Retrieval", "abstract": "<p>Hashing has shown great potential in large-scale image retrieval due to its storage and computation efficiency, especially the recent deep supervised hashing methods. To achieve promising performance, deep supervised hashing methods require a large amount of training data from different classes. However, when images of new categories emerge, existing deep hashing methods have to retrain the CNN model and generate hash codes for all the database images again, which is impractical for large-scale retrieval system.\nIn this paper, we propose a novel deep hashing framework, called Deep Incremental Hashing Network (DIHN), for learning hash codes in an incremental manner. DIHN learns the hash codes for the new coming images directly, while keeping the old ones unchanged. Simultaneously, a deep hash function for query set is learned by preserving the similarities between training points. Extensive experiments on two widely used image retrieval benchmarks demonstrate that the proposed DIHN framework can significantly decrease the training time while keeping the state-of-the-art retrieval accuracy.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised"] },
{"key": "wu2024sign", "year": "2024", "title":"Sign-guided Bipartite Graph Hashing For Hamming Space Search", "abstract": "<p>Bipartite graph hashing (BGH) is extensively used for Top-K search in Hamming\nspace at low storage and inference costs. Recent research adopts graph\nconvolutional hashing for BGH and has achieved the state-of-the-art\nperformance. However, the contributions of its various influencing factors to\nhashing performance have not been explored in-depth, including the\nsame/different sign count between two binary embeddings during Hamming space\nsearch (sign property), the contribution of sub-embeddings at each layer (model\nproperty), the contribution of different node types in the bipartite graph\n(node property), and the combination of augmentation methods. In this work, we\nbuild a lightweight graph convolutional hashing model named LightGCH by mainly\nremoving the augmentation methods of the state-of-the-art model BGCH. By\nanalyzing the contributions of each layer and node type to performance, as well\nas analyzing the Hamming similarity statistics at each layer, we find that the\nactual neighbors in the bipartite graph tend to have low Hamming similarity at\nthe shallow layer, and all nodes tend to have high Hamming similarity at the\ndeep layers in LightGCH. To tackle these problems, we propose a novel\nsign-guided framework SGBGH to make improvement, which uses sign-guided\nnegative sampling to improve the Hamming similarity of neighbors, and uses\nsign-aware contrastive learning to help nodes learn more uniform\nrepresentations. Experimental results show that SGBGH outperforms BGCH and\nLightGCH significantly in embedding quality.</p>\n", "tags": ["ARXIV","Graph","Self Supervised"] },
{"key": "wurzer2016randomised", "year": "2016", "title":"Randomised Relevance Model", "abstract": "<p>Relevance Models are well-known retrieval models and capable of producing\ncompetitive results. However, because they use query expansion they can be very\nslow. We address this slowness by incorporating two variants of locality\nsensitive hashing (LSH) into the query expansion process. Results on two\ndocument collections suggest that we can obtain large reductions in the amount\nof work, with a small reduction in effectiveness. Our approach is shown to be\nadditive when pruning query terms.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "wurzer2022parameterizing", "year": "2022", "title":"Parameterizing Kterm Hashing", "abstract": "<p>Kterm Hashing provides an innovative approach to novelty detection on massive\ndata streams. Previous research focused on maximizing the efficiency of Kterm\nHashing and succeeded in scaling First Story Detection to Twitter-size data\nstream without sacrificing detection accuracy. In this paper, we focus on\nimproving the effectiveness of Kterm Hashing. Traditionally, all kterms are\nconsidered as equally important when calculating a document’s degree of novelty\nwith respect to the past. We believe that certain kterms are more important\nthan others and hypothesize that uniform kterm weights are sub-optimal for\ndetermining novelty in data streams. To validate our hypothesis, we\nparameterize Kterm Hashing by assigning weights to kterms based on their\ncharacteristics. Our experiments apply Kterm Hashing in a First Story Detection\nsetting and reveal that parameterized Kterm Hashing can surpass\nstate-of-the-art detection accuracy and significantly outperform the uniformly\nweighted approach.</p>\n", "tags": ["SIGIR"] },
{"key": "wygocki2017fast", "year": "2017", "title":"On Fast Bounded Locality Sensitive Hashing", "abstract": "<p>In this paper, we examine the hash functions expressed as scalar products,\ni.e., \\(f(x)=&lt;v,x&gt;\\), for some bounded random vector \\(v\\). Such hash functions\nhave numerous applications, but often there is a need to optimize the choice of\nthe distribution of \\(v\\). In the present work, we focus on so-called\nanti-concentration bounds, i.e. the upper bounds of \\(\\mathbb{P}\\left[|&lt;v,x&gt;| &lt;\n\\alpha \\right]\\). In many applications, \\(v\\) is a vector of independent random\nvariables with standard normal distribution. In such case, the distribution of\n\\(&lt;v,x&gt;\\) is also normal and it is easy to approximate \\(\\mathbb{P}\\left[|&lt;v,x&gt;| &lt;\n\\alpha \\right]\\). Here, we consider two bounded distributions in the context of\nthe anti-concentration bounds. Particularly, we analyze \\(v\\) being a random\nvector from the unit ball in \\(l_{\\infty}\\) and \\(v\\) being a random vector from\nthe unit sphere in \\(l_{2}\\). We show optimal up to a constant anti-concentration\nmeasures for functions \\(f(x)=&lt;v,x&gt;\\).\n  As a consequence of our research, we obtain new best results for \\newline\n\\textit{\\(c\\)-approximate nearest neighbors without false negatives} for \\(l_p\\) in\nhigh dimensional space for all \\(p\\in[1,\\infty]\\), for\n\\(c=Ω(\\max\\{\\sqrt{d},d^{1/p}\\})\\). These results improve over those\npresented in [16]. Finally, our paper reports progress on answering the open\nproblem by Pagh~[17], who considered the nearest neighbor search without false\nnegatives for the Hamming distance.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "wöhnert2023study", "year": "2023", "title":"A Study On The Use Of Perceptual Hashing To Detect Manipulation Of Embedded Messages In Images", "abstract": "<p>Typically, metadata of images are stored in a specific data segment of the\nimage file. However, to securely detect changes, data can also be embedded\nwithin images. This follows the goal to invisibly and robustly embed as much\ninformation as possible to, ideally, even survive compression.\n  This work searches for embedding principles which allow to distinguish\nbetween unintended changes by lossy image compression and malicious\nmanipulation of the embedded message based on the change of its perceptual or\nrobust hash. Different embedding and compression algorithms are compared.\n  The study shows that embedding a message via integer wavelet transform and\ncompression with Karhunen-Loeve-transform yields the best results. However, it\nwas not possible to distinguish between manipulation and compression in all\ncases.</p>\n", "tags": ["ARXIV"] },
{"key": "xia2016unsupervised", "year": "2016", "title":"Unsupervised Deep Hashing For Large-scale Visual Search", "abstract": "<p>Learning based hashing plays a pivotal role in large-scale visual search.\nHowever, most existing hashing algorithms tend to learn shallow models that do\nnot seek representative binary codes. In this paper, we propose a novel hashing\napproach based on unsupervised deep learning to hierarchically transform\nfeatures into hash codes. Within the heterogeneous deep hashing framework, the\nautoencoder layers with specific constraints are considered to model the\nnonlinear mapping between features and binary codes. Then, a Restricted\nBoltzmann Machine (RBM) layer with constraints is utilized to reduce the\ndimension in the hamming space. Extensive experiments on the problem of visual\nsearch demonstrate the competitiveness of our proposed approach compared to\nstate-of-the-art.</p>\n", "tags": ["Cross Modal","Deep Learning","Unsupervised"] },
{"key": "xia2024supervised", "year": "2024", "title":"Supervised Hashing Via Image Representation Learning", "abstract": "<p>Hashing is a popular approximate nearest neighbor\nsearch approach for large-scale image retrieval.\nSupervised hashing, which incorporates similarity/dissimilarity\ninformation on entity pairs to improve\nthe quality of hashing function learning, has recently\nreceived increasing attention. However, in the existing\nsupervised hashing methods for images, an input\nimage is usually encoded by a vector of hand-crafted\nvisual features. Such hand-crafted feature vectors\ndo not necessarily preserve the accurate semantic\nsimilarities of images pairs, which may often degrade\nthe performance of hashing function learning. In this\npaper, we propose a supervised hashing method for\nimage retrieval, in which we automatically learn a good\nimage representation tailored to hashing as well as a\nset of hash functions. The proposed method has two\nstages. In the first stage, given the pairwise similarity\nmatrix S over training images, we propose a scalable\ncoordinate descent method to decompose S into a\nproduct of HHT where H is a matrix with each of its\nrows being the approximate hash code associated to\na training image. In the second stage, we propose to\nsimultaneously learn a good feature representation for\nthe input images as well as a set of hash functions, via\na deep convolutional network tailored to the learned\nhash codes in H and optionally the discrete class labels\nof the images. Extensive empirical evaluations on three\nbenchmark datasets with different kinds of images\nshow that the proposed method has superior performance\ngains over several state-of-the-art supervised\nand unsupervised hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "xiao2022progressively", "year": "2022", "title":"Progressively Optimized Bi-granular Document Representation For Scalable Embedding Based Retrieval", "abstract": "<p>Ad-hoc search calls for the selection of appropriate answers from a\nmassive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a\npromising solution, where deep learning based document representation and ANN\nsearch techniques are allied to handle this task. However, a major challenge is\nthat the ANN index can be too large to fit into memory, given the considerable\nsize of answer corpus. In this work, we tackle this problem with Bi-Granular\nDocument Representation, where the lightweight sparse embeddings are indexed\nand standby in memory for coarse-grained candidate search, and the heavyweight\ndense embeddings are hosted in disk for fine-grained post verification. For the\nbest of retrieval accuracy, a Progressive Optimization framework is designed.\nThe sparse embeddings are learned ahead for high-quality search of candidates.\nConditioned on the candidate distribution induced by the sparse embeddings, the\ndense embeddings are continuously learned to optimize the discrimination of\nground-truth from the shortlisted candidates. Besides, two techniques: the\ncontrastive quantization and the locality-centric sampling are introduced for\nthe learning of sparse and dense embeddings, which substantially contribute to\ntheir performances. Thanks to the above features, our method effectively\nhandles massive-scale EBR with strong advantages in accuracy: with up to +4.3%\nrecall gain on million-scale corpus, and up to +17.5% recall gain on\nbillion-scale corpus. Besides, Our method is applied to a major sponsored\nsearch platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and\nCTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.</p>\n", "tags": ["ARXIV","Deep Learning","Has Code","Quantisation"] },
{"key": "xiao2023relaxation", "year": "2023", "title":"A Relaxation Method For Binary Optimizations On Constrained Stiefel Manifold", "abstract": "<p>This paper focuses on a class of binary orthogonal optimization problems\nfrequently arising in semantic hashing. Consider that this class of problems\nmay have an empty feasible set, rendering them not well-defined. We introduce\nan equivalent model involving a restricted Stiefel manifold and a matrix box\nset, and then investigate its penalty problems induced by the \\(\\ell_1\\)-distance\nfrom the box set and its Moreau envelope. The two penalty problems are always\nwell-defined. Moreover, they serve as the global exact penalties provided that\nthe original feasible set is non-empty. Notably, the penalty problem induced by\nthe Moreau envelope is a smooth optimization over an embedded submanifold with\na favorable structure. We develop a retraction-based line-search Riemannian\ngradient method to address the penalty problem. Finally, the proposed method is\napplied to supervised and unsupervised hashing tasks and is compared with\nseveral popular methods on the MNIST and CIFAR-10 datasets. The numerical\ncomparisons reveal that our algorithm is significantly superior to other\nsolvers in terms of feasibility violation, and it is comparable even superior\nto others in terms of evaluation metrics related to the Hamming distance.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "xiao2023unsupervised", "year": "2023", "title":"Unsupervised Multi-criteria Adversarial Detection In Deep Image Retrieval", "abstract": "<p>The vulnerability in the algorithm supply chain of deep learning has imposed\nnew challenges to image retrieval systems in the downstream. Among a variety of\ntechniques, deep hashing is gaining popularity. As it inherits the algorithmic\nbackend from deep learning, a handful of attacks are recently proposed to\ndisrupt normal image retrieval. Unfortunately, the defense strategies in\nsoftmax classification are not readily available to be applied in the image\nretrieval domain. In this paper, we propose an efficient and unsupervised\nscheme to identify unique adversarial behaviors in the hamming space. In\nparticular, we design three criteria from the perspectives of hamming distance,\nquantization loss and denoising to defend against both untargeted and targeted\nattacks, which collectively limit the adversarial space. The extensive\nexperiments on four datasets demonstrate 2-23% improvements of detection rates\nwith minimum computational overhead for real-time image queries.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation","Unsupervised"] },
{"key": "xie2013kernelized", "year": "2013", "title":"Kernelized Locality-sensitive Hashing For Semi-supervised Agglomerative Clustering", "abstract": "<p>Large scale agglomerative clustering is hindered by computational burdens. We\npropose a novel scheme where exact inter-instance distance calculation is\nreplaced by the Hamming distance between Kernelized Locality-Sensitive Hashing\n(KLSH) hashed values. This results in a method that drastically decreases\ncomputation time. Additionally, we take advantage of certain labeled data\npoints via distance metric learning to achieve a competitive precision and\nrecall comparing to K-Means but in much less computation time.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "xiong2020approximate", "year": "2020", "title":"Approximate Nearest Neighbor Negative Contrastive Learning For Dense Text Retrieval", "abstract": "<p>Conducting text retrieval in a dense learned representation space has many\nintriguing advantages over sparse retrieval. Yet the effectiveness of dense\nretrieval (DR) often requires combination with sparse retrieval. In this paper,\nwe identify that the main bottleneck is in the training mechanisms, where the\nnegative instances used in training are not representative of the irrelevant\ndocuments in testing. This paper presents Approximate nearest neighbor Negative\nContrastive Estimation (ANCE), a training mechanism that constructs negatives\nfrom an Approximate Nearest Neighbor (ANN) index of the corpus, which is\nparallelly updated with the learning process to select more realistic negative\ntraining instances. This fundamentally resolves the discrepancy between the\ndata distribution used in the training and testing of DR. In our experiments,\nANCE boosts the BERT-Siamese DR model to outperform all competitive dense and\nsparse retrieval baselines. It nearly matches the accuracy of\nsparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned\nrepresentation space and provides almost 100x speed-up.</p>\n", "tags": ["ARXIV","Self Supervised","Text Retrieval"] },
{"key": "xiong2024adaptive", "year": "2024", "title":"Adaptive Quantization For Hashing An Information-based Approach To Learning Binary Codes", "abstract": "<p>Large-scale data mining and retrieval applications have\nincreasingly turned to compact binary data representations\nas a way to achieve both fast queries and efficient\ndata storage; many algorithms have been proposed for\nlearning effective binary encodings. Most of these algorithms\nfocus on learning a set of projection hyperplanes\nfor the data and simply binarizing the result from each\nhyperplane, but this neglects the fact that informativeness\nmay not be uniformly distributed across the projections.\nIn this paper, we address this issue by proposing\na novel adaptive quantization (AQ) strategy that\nadaptively assigns varying numbers of bits to different\nhyperplanes based on their information content. Our\nmethod provides an information-based schema that preserves\nthe neighborhood structure of data points, and\nwe jointly find the globally optimal bit-allocation for\nall hyperplanes. In our experiments, we compare with\nstate-of-the-art methods on four large-scale datasets\nand find that our adaptive quantization approach significantly\nimproves on traditional hashing methods.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "xu2015short", "year": "2015", "title":"Short Text Hashing Improved By Integrating Multi-granularity Topics And Tags", "abstract": "<p>Due to computational and storage efficiencies of compact binary codes,\nhashing has been widely used for large-scale similarity search. Unfortunately,\nmany existing hashing methods based on observed keyword features are not\neffective for short texts due to the sparseness and shortness. Recently, some\nresearchers try to utilize latent topics of certain granularity to preserve\nsemantic similarity in hash codes beyond keyword matching. However, topics of\ncertain granularity are not adequate to represent the intrinsic semantic\ninformation. In this paper, we present a novel unified approach for short text\nHashing using Multi-granularity Topics and Tags, dubbed HMTT. In particular, we\npropose a selection method to choose the optimal multi-granularity topics\ndepending on the type of dataset, and design two distinct hashing strategies to\nincorporate multi-granularity topics. We also propose a simple and effective\nmethod to exploit tags to enhance the similarity of related texts. We carry out\nextensive experiments on one short text dataset as well as on one normal text\ndataset. The results demonstrate that our approach is effective and\nsignificantly outperforms baselines on several evaluation metrics.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "xu2016binary", "year": "2016", "title":"Binary Subspace Coding For Query-by-image Video Retrieval", "abstract": "<p>The query-by-image video retrieval (QBIVR) task has been attracting\nconsiderable research attention recently. However, most existing methods\nrepresent a video by either aggregating or projecting all its frames into a\nsingle datum point, which may easily cause severe information loss. In this\npaper, we propose an efficient QBIVR framework to enable an effective and\nefficient video search with image query. We first define a\nsimilarity-preserving distance metric between an image and its orthogonal\nprojection in the subspace of the video, which can be equivalently transformed\nto a Maximum Inner Product Search (MIPS) problem.\n  Besides, to boost the efficiency of solving the MIPS problem, we propose two\nasymmetric hashing schemes, which bridge the domain gap of images and videos.\nThe first approach, termed Inner-product Binary Coding (IBC), preserves the\ninner relationships of images and videos in a common Hamming space. To further\nimprove the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)\napproach, which employs compact bilinear projections instead of a single large\nprojection matrix. Extensive experiments have been conducted on four real-world\nvideo datasets to verify the effectiveness of our proposed approaches as\ncompared to the state-of-the-arts.</p>\n", "tags": ["ARXIV","Independent","Video Retrieval"] },
{"key": "xu2017neural", "year": "2017", "title":"Neural Network-based Graph Embedding For Cross-platform Binary Code Similarity Detection", "abstract": "<p>The problem of cross-platform binary code similarity detection aims at\ndetecting whether two binary functions coming from different platforms are\nsimilar or not. It has many security applications, including plagiarism\ndetection, malware detection, vulnerability search, etc. Existing approaches\nrely on approximate graph matching algorithms, which are inevitably slow and\nsometimes inaccurate, and hard to adapt to a new task. To address these issues,\nin this work, we propose a novel neural network-based approach to compute the\nembedding, i.e., a numeric vector, based on the control flow graph of each\nbinary function, then the similarity detection can be done efficiently by\nmeasuring the distance between the embeddings for two functions. We implement a\nprototype called Gemini. Our extensive evaluation shows that Gemini outperforms\nthe state-of-the-art approaches by large margins with respect to similarity\ndetection accuracy. Further, Gemini can speed up prior art’s embedding\ngeneration time by 3 to 4 orders of magnitude and reduce the required training\ntime from more than 1 week down to 30 minutes to 10 hours. Our real world case\nstudies demonstrate that Gemini can identify significantly more vulnerable\nfirmware images than the state-of-the-art, i.e., Genius. Our research showcases\na successful application of deep learning on computer security problems.</p>\n", "tags": ["ARXIV","Deep Learning","Graph","Supervised"] },
{"key": "xu2018error", "year": "2018", "title":"Error Correction Maximization For Deep Image Hashing", "abstract": "<p>We propose to use the concept of the Hamming bound to derive the optimal\ncriteria for learning hash codes with a deep network. In particular, when the\nnumber of binary hash codes (typically the number of image categories) and code\nlength are known, it is possible to derive an upper bound on the minimum\nHamming distance between the hash codes. This upper bound can then be used to\ndefine the loss function for learning hash codes. By encouraging the margin\n(minimum Hamming distance) between the hash codes of different image categories\nto match the upper bound, we are able to learn theoretically optimal hash\ncodes. Our experiments show that our method significantly outperforms competing\ndeep learning-based approaches and obtains top performance on benchmark\ndatasets.</p>\n", "tags": ["ARXIV","Deep Learning","Independent"] },
{"key": "xu2018gpu", "year": "2018", "title":"GPU Accelerated Cascade Hashing Image Matching For Large Scale 3D Reconstruction", "abstract": "<p>Image feature point matching is a key step in Structure from Motion(SFM).\nHowever, it is becoming more and more time consuming because the number of\nimages is getting larger and larger. In this paper, we proposed a GPU\naccelerated image matching method with improved Cascade Hashing. Firstly, we\npropose a Disk-Memory-GPU data exchange strategy and optimize the load order of\ndata, so that the proposed method can deal with big data. Next, we parallelize\nthe Cascade Hashing method on GPU. An improved parallel reduction and an\nimproved parallel hashing ranking are proposed to fulfill this task. Finally,\nextensive experiments show that our image matching is about 20 times faster\nthan SiftGPU on the same graphics card, nearly 100 times faster than the CPU\nCasHash method and hundreds of times faster than the CPU Kd-Tree based matching\nmethod. Further more, we introduce the epipolar constraint to the proposed\nmethod, and use the epipolar geometry to guide the feature matching procedure,\nwhich further reduces the matching cost.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "xu2018sketchmate", "year": "2018", "title":"Sketchmate Deep Hashing For Million-scale Human Sketch Retrieval", "abstract": "<p>We propose a deep hashing framework for sketch retrieval that, for the first\ntime, works on a multi-million scale human sketch dataset. Leveraging on this\nlarge dataset, we explore a few sketch-specific traits that were otherwise\nunder-studied in prior literature. Instead of following the conventional sketch\nrecognition task, we introduce the novel problem of sketch hashing retrieval\nwhich is not only more challenging, but also offers a better testbed for\nlarge-scale sketch analysis, since: (i) more fine-grained sketch feature\nlearning is required to accommodate the large variations in style and\nabstraction, and (ii) a compact binary code needs to be learned at the same\ntime to enable efficient retrieval. Key to our network design is the embedding\nof unique characteristics of human sketch, where (i) a two-branch CNN-RNN\narchitecture is adapted to explore the temporal ordering of strokes, and (ii) a\nnovel hashing loss is specifically designed to accommodate both the temporal\nand abstract traits of sketches. By working with a 3.8M sketch dataset, we show\nthat state-of-the-art hashing models specifically engineered for static images\nfail to perform well on temporal sketch data. Our network on the other hand not\nonly offers the best retrieval performance on various code sizes, but also\nyields the best generalization performance under a zero-shot setting and when\nre-purposed for sketch recognition. Such superior performances effectively\ndemonstrate the benefit of our sketch-specific design.</p>\n", "tags": ["ARXIV","CNN"] },
{"key": "xu2020learning", "year": "2020", "title":"On Learning Semantic Representations For Million-scale Free-hand Sketches", "abstract": "<p>In this paper, we study learning semantic representations for million-scale\nfree-hand sketches. This is highly challenging due to the domain-unique traits\nof sketches, e.g., diverse, sparse, abstract, noisy. We propose a dual-branch\nCNNRNN network architecture to represent sketches, which simultaneously encodes\nboth the static and temporal patterns of sketch strokes. Based on this\narchitecture, we further explore learning the sketch-oriented semantic\nrepresentations in two challenging yet practical settings, i.e., hashing\nretrieval and zero-shot recognition on million-scale sketches. Specifically, we\nuse our dual-branch architecture as a universal representation framework to\ndesign two sketch-specific deep models: (i) We propose a deep hashing model for\nsketch retrieval, where a novel hashing loss is specifically designed to\naccommodate both the abstract and messy traits of sketches. (ii) We propose a\ndeep embedding model for sketch zero-shot recognition, via collecting a\nlarge-scale edge-map dataset and proposing to extract a set of semantic vectors\nfrom edge-maps as the semantic knowledge for sketch zero-shot domain alignment.\nBoth deep models are evaluated by comprehensive experiments on million-scale\nsketches and outperform the state-of-the-art competitors.</p>\n", "tags": ["ARXIV"] },
{"key": "xu2020multi", "year": "2020", "title":"Multi-feature Discrete Collaborative Filtering For Fast Cold-start Recommendation", "abstract": "<p>Hashing is an effective technique to address the large-scale recommendation\nproblem, due to its high computation and storage efficiency on calculating the\nuser preferences on items. However, existing hashing-based recommendation\nmethods still suffer from two important problems: 1) Their recommendation\nprocess mainly relies on the user-item interactions and single specific content\nfeature. When the interaction history or the content feature is unavailable\n(the cold-start problem), their performance will be seriously deteriorated. 2)\nExisting methods learn the hash codes with relaxed optimization or adopt\ndiscrete coordinate descent to directly solve binary hash codes, which results\nin significant quantization loss or consumes considerable computation time. In\nthis paper, we propose a fast cold-start recommendation method, called\nMulti-Feature Discrete Collaborative Filtering (MFDCF), to solve these\nproblems. Specifically, a low-rank self-weighted multi-feature fusion module is\ndesigned to adaptively project the multiple content features into binary yet\ninformative hash codes by fully exploiting their complementarity. Additionally,\nwe develop a fast discrete optimization algorithm to directly compute the\nbinary hash codes with simple operations. Experiments on two public\nrecommendation datasets demonstrate that MFDCF outperforms the\nstate-of-the-arts on various aspects.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "xu2021hhf", "year": "2021", "title":"HHF Hashing-guided Hinge Function For Deep Hashing Retrieval", "abstract": "<p>Deep hashing has shown promising performance in large-scale image retrieval.\nHowever, latent codes extracted by Deep Neural Networks (DNNs) will inevitably\nlose semantic information during the binarization process, which damages the\nretrieval accuracy and makes it challenging. Although many existing approaches\nperform regularization to alleviate quantization errors, we figure out an\nincompatible conflict between metric learning and quantization learning. The\nmetric loss penalizes the inter-class distances to push different classes\nunconstrained far away. Worse still, it tends to map the latent code deviate\nfrom ideal binarization point and generate severe ambiguity in the binarization\nprocess. Based on the minimum distance of the binary linear code, we creatively\npropose Hashing-guided Hinge Function (HHF) to avoid such conflict. In detail,\nthe carefully-designed inflection point, which relies on the hash bit length\nand category numbers, is explicitly adopted to balance the metric term and\nquantization term. Such a modification prevents the network from falling into\nlocal metric optimal minima in deep hashing. Extensive experiments in CIFAR-10,\nCIFAR-100, ImageNet, and MS-COCO show that HHF consistently outperforms\nexisting techniques, and is robust and flexible to transplant into other\nmethods. Code is available at https://github.com/JerryXu0129/HHF.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval","Quantisation","Supervised"] },
{"key": "xu2022hyp", "year": "2022", "title":"Hyp ^2 Loss Beyond Hypersphere Metric Space For Multi-label Image Retrieval", "abstract": "<p>Image retrieval has become an increasingly appealing technique with broad multimedia application prospects, where deep hashing serves as the dominant branch towards low storage and efficient retrieval. In this paper, we carried out in-depth investigations on metric learning in deep hashing for establishing a powerful metric space in multi-label scenarios, where the pair loss suffers high computational overhead and converge difficulty, while the proxy loss is theoretically incapable of expressing the profound label dependencies and exhibits conflicts in the constructed hypersphere space. To address the problems, we propose a novel metric learning framework with Hybrid Proxy-Pair Loss (HyP{{ ‘{{’ }}^2{{ ‘}}’ }} Loss) that constructs an expressive metric space with efficient training complexity w.r.t. the whole dataset. The proposed HyP{{ ‘{{’ }}^2{{ ‘}}’ }} Loss focuses on optimizing the hypersphere space by learnable proxies and excavating data-to-data correlations of irrelevant pairs, which integrates sufficient data correspondence of pair-based methods and high-efficiency of proxy-based methods. Extensive experiments on four standard multi-label benchmarks justify the proposed method outperforms the state-of-the-art, is robust among different hash bits and achieves significant performance gains with a faster, more stable convergence speed. Our code is available at https://github.com/JerryXu0129/HyP2-Loss.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval"] },
{"key": "xu2022loss", "year": "2022", "title":"Hyp^2 Loss Beyond Hypersphere Metric Space For Multi-label Image Retrieval", "abstract": "<p>Image retrieval has become an increasingly appealing technique with broad\nmultimedia application prospects, where deep hashing serves as the dominant\nbranch towards low storage and efficient retrieval. In this paper, we carried\nout in-depth investigations on metric learning in deep hashing for establishing\na powerful metric space in multi-label scenarios, where the pair loss suffers\nhigh computational overhead and converge difficulty, while the proxy loss is\ntheoretically incapable of expressing the profound label dependencies and\nexhibits conflicts in the constructed hypersphere space. To address the\nproblems, we propose a novel metric learning framework with Hybrid Proxy-Pair\nLoss (HyP\\(^2\\) Loss) that constructs an expressive metric space with efficient\ntraining complexity w.r.t. the whole dataset. The proposed HyP\\(^2\\) Loss focuses\non optimizing the hypersphere space by learnable proxies and excavating\ndata-to-data correlations of irrelevant pairs, which integrates sufficient data\ncorrespondence of pair-based methods and high-efficiency of proxy-based\nmethods. Extensive experiments on four standard multi-label benchmarks justify\nthe proposed method outperforms the state-of-the-art, is robust among different\nhash bits and achieves significant performance gains with a faster, more stable\nconvergence speed. Our code is available at\nhttps://github.com/JerryXu0129/HyP2-Loss.</p>\n", "tags": ["ARXIV","Has Code","Image Retrieval"] },
{"key": "xu2023deep", "year": "2023", "title":"Deep Lifelong Cross-modal Hashing", "abstract": "<p>Hashing methods have made significant progress in cross-modal retrieval tasks\nwith fast query speed and low storage cost. Among them, deep learning-based\nhashing achieves better performance on large-scale data due to its excellent\nextraction and representation ability for nonlinear heterogeneous features.\nHowever, there are still two main challenges in catastrophic forgetting when\ndata with new categories arrive continuously, and time-consuming for\nnon-continuous hashing retrieval to retrain for updating. To this end, we, in\nthis paper, propose a novel deep lifelong cross-modal hashing to achieve\nlifelong hashing retrieval instead of re-training hash function repeatedly when\nnew data arrive. Specifically, we design lifelong learning strategy to update\nhash functions by directly training the incremental data instead of retraining\nnew hash functions using all the accumulated data, which significantly reduce\ntraining time. Then, we propose lifelong hashing loss to enable original hash\ncodes participate in lifelong learning but remain invariant, and further\npreserve the similarity and dis-similarity among original and incremental hash\ncodes to maintain performance. Additionally, considering distribution\nheterogeneity when new data arriving continuously, we introduce multi-label\nsemantic similarity to supervise hash learning, and it has been proven that the\nsimilarity improves performance with detailed analysis. Experimental results on\nbenchmark datasets show that the proposed methods achieves comparative\nperformance comparing with recent state-of-the-art cross-modal hashing methods,\nand it yields substantial average increments over 20\\% in retrieval accuracy\nand almost reduces over 80\\% training time when new data arrives continuously.</p>\n", "tags": ["ARXIV","Cross Modal","Deep Learning","Supervised"] },
{"key": "xu2024convolutional", "year": "2024", "title":"Convolutional Neural Networks For Text Hashing", "abstract": "<p>Hashing, as a popular approximate nearest neighbor\nsearch, has been widely used for large-scale similarity search. Recently, a spectrum of machine learning\nmethods are utilized to learn similarity-preserving\nbinary codes. However, most of them directly encode the explicit features, keywords, which fail to\npreserve the accurate semantic similarities in binary code beyond keyword matching, especially on\nshort texts. Here we propose a novel text hashing\nframework with convolutional neural networks. In\nparticular, we first embed the keyword features into\ncompact binary code with a locality preserving constraint. Meanwhile word features and position features are together fed into a convolutional network to\nlearn the implicit features which are further incorporated with the explicit features to fit the pre-trained\nbinary code. Such base method can be successfully\naccomplished without any external tags/labels, and\nother three model variations are designed to integrate tags/labels. Experimental results show the\nsuperiority of our proposed approach over several\nstate-of-the-art hashing methods when tested on one\nshort text dataset as well as one normal text dataset.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "xu2024harmonious", "year": "2024", "title":"Harmonious Hashing", "abstract": "<p>Hashing-based fast nearest neighbor search technique\nhas attracted great attention in both research\nand industry areas recently. Many existing hashing\napproaches encode data with projection-based hash\nfunctions and represent each projected dimension\nby 1-bit. However, the dimensions with high variance\nhold large energy or information of data but\ntreated equivalently as dimensions with low variance,\nwhich leads to a serious information loss. In\nthis paper, we introduce a novel hashing algorithm\ncalled Harmonious Hashing which aims at learning\nhash functions with low information loss. Specifically,\nwe learn a set of optimized projections to\npreserve the maximum cumulative energy and meet\nthe constraint of equivalent variance on each dimension\nas much as possible. In this way, we could\nminimize the information loss after binarization.\nDespite the extreme simplicity, our method outperforms\nsuperiorly to many state-of-the-art hashing\nmethods in large-scale and high-dimensional nearest\nneighbor search experiments.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "xue2022cross", "year": "2022", "title":"Cross-scale Context Extracted Hashing For Fine-grained Image Binary Encoding", "abstract": "<p>Deep hashing has been widely applied to large-scale image retrieval tasks\nowing to efficient computation and low storage cost by encoding\nhigh-dimensional image data into binary codes. Since binary codes do not\ncontain as much information as float features, the essence of binary encoding\nis preserving the main context to guarantee retrieval quality. However, the\nexisting hashing methods have great limitations on suppressing redundant\nbackground information and accurately encoding from Euclidean space to Hamming\nspace by a simple sign function. In order to solve these problems, a\nCross-Scale Context Extracted Hashing Network (CSCE-Net) is proposed in this\npaper. Firstly, we design a two-branch framework to capture fine-grained local\ninformation while maintaining high-level global semantic information. Besides,\nAttention guided Information Extraction module (AIE) is introduced between two\nbranches, which suppresses areas of low context information cooperated with\nglobal sliding windows. Unlike previous methods, our CSCE-Net learns a\ncontent-related Dynamic Sign Function (DSF) to replace the original simple sign\nfunction. Therefore, the proposed CSCE-Net is context-sensitive and able to\nperform well on accurate image binary encoding. We further demonstrate that our\nCSCE-Net is superior to the existing hashing methods, which improves retrieval\nperformance on standard benchmarks.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "xue2022hashformers", "year": "2022", "title":"Hashformers Towards Vocabulary-independent Pre-trained Transformers", "abstract": "<p>Transformer-based pre-trained language models are vocabulary-dependent,\nmapping by default each token to its corresponding embedding. This one-to-one\nmapping results into embedding matrices that occupy a lot of memory (i.e.\nmillions of parameters) and grow linearly with the size of the vocabulary.\nPrevious work on on-device transformers dynamically generate token embeddings\non-the-fly without embedding matrices using locality-sensitive hashing over\nmorphological information. These embeddings are subsequently fed into\ntransformer layers for text classification. However, these methods are not\npre-trained. Inspired by this line of work, we propose HashFormers, a new\nfamily of vocabulary-independent pre-trained transformers that support an\nunlimited vocabulary (i.e. all possible tokens in a corpus) given a\nsubstantially smaller fixed-sized embedding matrix. We achieve this by first\nintroducing computationally cheap hashing functions that bucket together\nindividual tokens to embeddings. We also propose three variants that do not\nrequire an embedding matrix at all, further reducing the memory requirements.\nWe empirically demonstrate that HashFormers are more memory efficient compared\nto standard pre-trained transformers while achieving comparable predictive\nperformance when fine-tuned on multiple text classification tasks. For example,\nour most efficient HashFormer variant has a negligible performance degradation\n(0.4\\% on GLUE) using only 99.1K parameters for representing the embeddings\ncompared to 12.3-38M parameters of state-of-the-art models.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "yamada2021efficient", "year": "2021", "title":"Efficient Passage Retrieval With Hashing For Open-domain Question Answering", "abstract": "<p>Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.</p>\n", "tags": ["ARXIV","Has Code"] },
{"key": "yan2018norm", "year": "2018", "title":"Norm-ranging LSH For Maximum Inner Product Search", "abstract": "<p>Neyshabur and Srebro proposed SIMPLE-LSH, which is the state-of-the-art hashing based algorithm for maximum inner product search (MIPS). We found that the performance of SIMPLE-LSH, in both theory and practice, suffers from long tails in the 2-norm distribution of real datasets. We propose NORM-RANGING LSH, which addresses the excessive normalization problem caused by long tails by partitioning a dataset into sub-datasets and building a hash index for each sub-dataset independently. We prove that NORM-RANGING LSH achieves lower query time complexity than SIMPLE-LSH under mild conditions. We also show that the idea of dataset partitioning can improve another hashing based MIPS algorithm. Experiments show that NORM-RANGING LSH probes much less items than SIMPLE-LSH at the same recall, thus significantly benefiting MIPS based applications.</p>\n", "tags": ["Independent","LSH","NEURIPS","Theory"] },
{"key": "yan2020deep", "year": "2020", "title":"Deep Multi-view Enhancement Hashing For Image Retrieval", "abstract": "<p>Hashing is an efficient method for nearest neighbor search in large-scale\ndata space by embedding high-dimensional feature descriptors into a similarity\npreserving Hamming space with a low dimension. However, large-scale high-speed\nretrieval through binary code has a certain degree of reduction in retrieval\naccuracy compared to traditional retrieval methods. We have noticed that\nmulti-view methods can well preserve the diverse characteristics of data.\nTherefore, we try to introduce the multi-view deep neural network into the hash\nlearning field, and design an efficient and innovative retrieval model, which\nhas achieved a significant improvement in retrieval performance. In this paper,\nwe propose a supervised multi-view hash model which can enhance the multi-view\ninformation through neural networks. This is a completely new hash learning\nmethod that combines multi-view and deep learning methods. The proposed method\nutilizes an effective view stability evaluation method to actively explore the\nrelationship among views, which will affect the optimization direction of the\nentire network. We have also designed a variety of multi-data fusion methods in\nthe Hamming space to preserve the advantages of both convolution and\nmulti-view. In order to avoid excessive computing resources on the enhancement\nprocedure during retrieval, we set up a separate structure called memory\nnetwork which participates in training together. The proposed method is\nsystematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and\nthe results show that our method significantly outperforms the state-of-the-art\nsingle-view and multi-view hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Deep Learning","Image Retrieval","Supervised"] },
{"key": "yan2021binary", "year": "2021", "title":"Binary Code Based Hash Embedding For Web-scale Applications", "abstract": "<p>Nowadays, deep learning models are widely adopted in web-scale applications\nsuch as recommender systems, and online advertising. In these applications,\nembedding learning of categorical features is crucial to the success of deep\nlearning models. In these models, a standard method is that each categorical\nfeature value is assigned a unique embedding vector which can be learned and\noptimized. Although this method can well capture the characteristics of the\ncategorical features and promise good performance, it can incur a huge memory\ncost to store the embedding table, especially for those web-scale applications.\nSuch a huge memory cost significantly holds back the effectiveness and\nusability of EDRMs. In this paper, we propose a binary code based hash\nembedding method which allows the size of the embedding table to be reduced in\narbitrary scale without compromising too much performance. Experimental\nevaluation results show that one can still achieve 99\\% performance even if the\nembedding table size is reduced 1000\\(\\times\\) smaller than the original one with\nour proposed method.</p>\n", "tags": ["ARXIV","Deep Learning"] },
{"key": "yan2024deep", "year": "2024", "title":"Deep Hashing By Discriminating Hard Examples", "abstract": "<p>This paper tackles a rarely explored but critical problem within learning to hash, i.e., to learn hash codes that effectively discriminate hard similar and dissimilar examples, to empower large-scale image retrieval. Hard similar examples refer to image pairs from the same semantic class that demonstrate some shared appearance but have different fine-grained appearance. Hard dissimilar examples are image pairs that come from different semantic classes but exhibit similar appearance. These hard examples generally have a small distance due to the shared appearance. Therefore, effective encoding of the hard examples can well discriminate the relevant images within a small Hamming distance, enabling more accurate retrieval in the top-ranked returned images. However, most existing hashing methods cannot capture this key information as their optimization is dominated byeasy examples, i.e., distant similar/dissimilar pairs that share no or limited appearance. To address this problem, we introduce a novel Gamma distribution-enabled and symmetric Kullback-Leibler divergence-based loss, which is dubbed dual hinge loss because it works similarly as imposing two smoothed hinge losses on the respective similar and dissimilar pairs. Specifically, the loss enforces exponentially variant penalization on the hard similar (dissimilar) examples to emphasize and learn their fine-grained difference. It meanwhile imposes a bounding penalization on easy similar (dissimilar) examples to prevent the dominance of the easy examples in the optimization while preserving the high-level similarity (dissimilarity). This enables our model to well encode the key information carried by both easy and hard examples. Extensive empirical results on three widely-used image retrieval datasets show that (i) our method consistently and substantially outperforms state-of-the-art competing methods using hash codes of the same length and (ii) our method can use significantly (e.g., 50%-75%) shorter hash codes to perform substantially better than, or comparably well to, the competing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "yandex2024yandex", "year": "2024", "title":"Yandex DEEP-1B", "abstract": "<p>Yandex DEEP-1B image descriptor dataset consisting of the projected and normalized outputs from the last fully-connected layer of the GoogLeNet model, which was pretrained on the Imagenet classification task.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "yandexdeep1B", "year": "2021", "title":"Yandex DEEP-1B", "abstract": "<p>Yandex DEEP-1B image descriptor dataset consisting of the projected and normalized outputs from the last fully-connected layer of the GoogLeNet model, which was pretrained on the Imagenet classification task.</p>\n", "tags": ["Dataset"] },
{"key": "yandextexttoimage1B", "year": "2021", "title":"Yandex Text-to-Image-1B", "abstract": "<p>Yandex Text-to-Image-1B is a new cross-model dataset (text and visual), where database and query vectors have different distributions in a shared representation space. The base set consists of Image embeddings produced by the Se-ResNext-101 model, and queries are textual embeddings produced by a variant of the DSSM model. Since the distributions are different, a 50M sample of the query distribution is provided.</p>\n", "tags": ["Dataset"] },
{"key": "yang2015supervised", "year": "2015", "title":"Supervised Learning Of Semantics-preserving Hash Via Deep Convolutional Neural Networks", "abstract": "<p>This paper presents a simple yet effective supervised deep hash approach that\nconstructs binary hash codes from labeled data for large-scale image search. We\nassume that the semantic labels are governed by several latent attributes with\neach attribute on or off, and classification relies on these attributes. Based\non this assumption, our approach, dubbed supervised semantics-preserving deep\nhashing (SSDH), constructs hash functions as a latent layer in a deep network\nand the binary codes are learned by minimizing an objective function defined\nover classification error and other desirable hash codes properties. With this\ndesign, SSDH has a nice characteristic that classification and retrieval are\nunified in a single learning model. Moreover, SSDH performs joint learning of\nimage representations, hash codes, and classification in a point-wised manner,\nand thus is scalable to large-scale datasets. SSDH is simple and can be\nrealized by a slight enhancement of an existing deep architecture for\nclassification; yet it is effective and outperforms other hashing approaches on\nseveral benchmarks and large datasets. Compared with state-of-the-art\napproaches, SSDH achieves higher retrieval accuracy, while the classification\nperformance is not sacrificed.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "yang2016zero", "year": "2016", "title":"Zero-shot Hashing Via Transferring Supervised Knowledge", "abstract": "<p>Hashing has shown its efficiency and effectiveness in facilitating\nlarge-scale multimedia applications. Supervised knowledge e.g. semantic labels\nor pair-wise relationship) associated to data is capable of significantly\nimproving the quality of hash codes and hash functions. However, confronted\nwith the rapid growth of newly-emerging concepts and multimedia data on the\nWeb, existing supervised hashing approaches may easily suffer from the scarcity\nand validity of supervised information due to the expensive cost of manual\nlabelling. In this paper, we propose a novel hashing scheme, termed\n<em>zero-shot hashing</em> (ZSH), which compresses images of “unseen” categories\nto binary codes with hash functions learned from limited training data of\n“seen” categories. Specifically, we project independent data labels i.e.\n0/1-form label vectors) into semantic embedding space, where semantic\nrelationships among all the labels can be precisely characterized and thus seen\nsupervised knowledge can be transferred to unseen classes. Moreover, in order\nto cope with the semantic shift problem, we rotate the embedded space to more\nsuitably align the embedded semantics with the low-level visual feature space,\nthereby alleviating the influence of semantic gap. In the meantime, to exert\npositive effects on learning high-quality hash functions, we further propose to\npreserve local structural property and discrete nature in binary codes.\nBesides, we develop an efficient alternating algorithm to solve the ZSH model.\nExtensive experiments conducted on various real-life datasets show the superior\nzero-shot image retrieval performance of ZSH as compared to several\nstate-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "yang2018deep", "year": "2018", "title":"Deep Attention-guided Hashing", "abstract": "<p>With the rapid growth of multimedia data (e.g., image, audio and video etc.)\non the web, learning-based hashing techniques such as Deep Supervised Hashing\n(DSH) have proven to be very efficient for large-scale multimedia search. The\nrecent successes seen in Learning-based hashing methods are largely due to the\nsuccess of deep learning-based hashing methods. However, there are some\nlimitations to previous learning-based hashing methods (e.g., the learned hash\ncodes containing repetitive and highly correlated information). In this paper,\nwe propose a novel learning-based hashing method, named Deep Attention-guided\nHashing (DAgH). DAgH is implemented using two stream frameworks. The core idea\nis to use guided hash codes which are generated by the hashing network of the\nfirst stream framework (called first hashing network) to guide the training of\nthe hashing network of the second stream framework (called second hashing\nnetwork). Specifically, in the first network, it leverages an attention network\nand hashing network to generate the attention-guided hash codes from the\noriginal images. The loss function we propose contains two components: the\nsemantic loss and the attention loss. The attention loss is used to punish the\nattention network to obtain the salient region from pairs of images; in the\nsecond network, these attention-guided hash codes are used to guide the\ntraining of the second hashing network (i.e., these codes are treated as\nsupervised labels to train the second network). By doing this, DAgH can make\nfull use of the most critical information contained in images to guide the\nsecond hashing network in order to learn efficient hash codes in a true\nend-to-end fashion. Results from our experiments demonstrate that DAgH can\ngenerate high quality hash codes and it outperforms current state-of-the-art\nmethods on three benchmark datasets, CIFAR-10, NUS-WIDE, and ImageNet.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "yang2019asymmetric", "year": "2019", "title":"Asymmetric Deep Semantic Quantization For Image Retrieval", "abstract": "<p>Due to its fast retrieval and storage efficiency capabilities, hashing has\nbeen widely used in nearest neighbor retrieval tasks. By using deep learning\nbased techniques, hashing can outperform non-learning based hashing technique\nin many applications. However, we argue that the current deep learning based\nhashing methods ignore some critical problems (e.g., the learned hash codes are\nnot discriminative due to the hashing methods being unable to discover rich\nsemantic information and the training strategy having difficulty optimizing the\ndiscrete binary codes). In this paper, we propose a novel image hashing method,\ntermed as \\textbf{\\underline{A}}symmetric \\textbf{\\underline{D}}eep\n\\textbf{\\underline{S}}emantic \\textbf{\\underline{Q}}uantization\n(\\textbf{ADSQ}). \\textbf{ADSQ} is implemented using three stream frameworks,\nwhich consist of one <em>LabelNet</em> and two <em>ImgNets</em>. The\n<em>LabelNet</em> leverages the power of three fully-connected layers, which are\nused to capture rich semantic information between image pairs. For the two\n<em>ImgNets</em>, they each adopt the same convolutional neural network\nstructure, but with different weights (i.e., asymmetric convolutional neural\nnetworks). The two <em>ImgNets</em> are used to generate discriminative compact\nhash codes. Specifically, the function of the <em>LabelNet</em> is to capture\nrich semantic information that is used to guide the two <em>ImgNets</em> in\nminimizing the gap between the real-continuous features and the discrete binary\ncodes. Furthermore, \\textbf{ADSQ} can utilize the most critical semantic\ninformation to guide the feature learning process and consider the consistency\nof the common semantic space and Hamming space. Experimental results on three\nbenchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the\nproposed \\textbf{ADSQ} can outperforms current state-of-the-art methods.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation","Supervised"] },
{"key": "yang2019distillhash", "year": "2019", "title":"Distillhash Unsupervised Deep Hashing By Distilling Data Pairs", "abstract": "<p>Due to the high storage and search efficiency, hashing has become prevalent\nfor large-scale similarity search. Particularly, deep hashing methods have\ngreatly improved the search performance under supervised scenarios. In\ncontrast, unsupervised deep hashing models can hardly achieve satisfactory\nperformance due to the lack of reliable supervisory similarity signals. To\naddress this issue, we propose a novel deep unsupervised hashing model, dubbed\nDistillHash, which can learn a distilled data set consisted of data pairs,\nwhich have confidence similarity signals. Specifically, we investigate the\nrelationship between the initial noisy similarity signals learned from local\nstructures and the semantic similarity labels assigned by a Bayes optimal\nclassifier. We show that under a mild assumption, some data pairs, of which\nlabels are consistent with those assigned by the Bayes optimal classifier, can\nbe potentially distilled. Inspired by this fact, we design a simple yet\neffective strategy to distill data pairs automatically and further adopt a\nBayesian learning framework to learn hash functions from the distilled data\nset. Extensive experimental results on three widely used benchmark datasets\nshow that the proposed DistillHash consistently accomplishes the\nstate-of-the-art search performance.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "yang2019feature", "year": "2019", "title":"Feature Pyramid Hashing", "abstract": "<p>In recent years, deep-networks-based hashing has become a leading approach\nfor large-scale image retrieval. Most deep hashing approaches use the high\nlayer to extract the powerful semantic representations. However, these methods\nhave limited ability for fine-grained image retrieval because the semantic\nfeatures extracted from the high layer are difficult in capturing the subtle\ndifferences. To this end, we propose a novel two-pyramid hashing architecture\nto learn both the semantic information and the subtle appearance details for\nfine-grained image search. Inspired by the feature pyramids of convolutional\nneural network, a vertical pyramid is proposed to capture the high-layer\nfeatures and a horizontal pyramid combines multiple low-layer features with\nstructural information to capture the subtle differences. To fuse the low-level\nfeatures, a novel combination strategy, called consensus fusion, is proposed to\ncapture all subtle information from several low-layers for finer retrieval.\nExtensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford\nDogs demonstrate that the proposed method achieves significant performance\ncompared with the state-of-art baselines.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "yang2019model", "year": "2019", "title":"2-bit Model Compression Of Deep Convolutional Neural Network On ASIC Engine For Image Retrieval", "abstract": "<p>Image retrieval utilizes image descriptors to retrieve the most similar\nimages to a given query image. Convolutional neural network (CNN) is becoming\nthe dominant approach to extract image descriptors for image retrieval. For\nlow-power hardware implementation of image retrieval, the drawback of CNN-based\nfeature descriptor is that it requires hundreds of megabytes of storage. To\naddress this problem, this paper applies deep model quantization and\ncompression to CNN in ASIC chip for image retrieval. It is demonstrated that\nthe CNN-based features descriptor can be extracted using as few as 2-bit\nweights quantization to deliver a similar performance as floating-point model\nfor image retrieval. In addition, to implement CNN in ASIC, especially for\nlarge scale images, the limited buffer size of chips should be considered. To\nretrieve large scale images, we propose an improved pooling strategy, region\nnested invariance pooling (RNIP), which uses cropped sub-images for CNN.\nTesting results on chip show that integrating RNIP with the proposed 2-bit CNN\nmodel compression approach is capable of retrieving large scale images.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Quantisation","Supervised"] },
{"key": "yang2019shared", "year": "2019", "title":"Shared Predictive Cross-modal Deep Quantization", "abstract": "<p>With explosive growth of data volume and ever-increasing diversity of data\nmodalities, cross-modal similarity search, which conducts nearest neighbor\nsearch across different modalities, has been attracting increasing interest.\nThis paper presents a deep compact code learning solution for efficient\ncross-modal similarity search. Many recent studies have proven that\nquantization-based approaches perform generally better than hashing-based\napproaches on single-modal similarity search. In this paper, we propose a deep\nquantization approach, which is among the early attempts of leveraging deep\nneural networks into quantization-based cross-modal similarity search. Our\napproach, dubbed shared predictive deep quantization (SPDQ), explicitly\nformulates a shared subspace across different modalities and two private\nsubspaces for individual modalities, and representations in the shared subspace\nand the private subspaces are learned simultaneously by embedding them to a\nreproducing kernel Hilbert space, where the mean embedding of different\nmodality distributions can be explicitly compared. In addition, in the shared\nsubspace, a quantizer is learned to produce the semantics preserving compact\ncodes with the help of label alignment. Thanks to this novel network\narchitecture in cooperation with supervised quantization training, SPDQ can\npreserve intramodal and intermodal similarities as much as possible and greatly\nreduce quantization error. Experiments on two popular benchmarks corroborate\nthat our approach outperforms state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "yang2020camera", "year": "2020", "title":"Camera-based Piano Sheet Music Identification", "abstract": "<p>This paper presents a method for large-scale retrieval of piano sheet music\nimages. Our work differs from previous studies on sheet music retrieval in two\nways. First, we investigate the problem at a much larger scale than previous\nstudies, using all solo piano sheet music images in the entire IMSLP dataset as\na searchable database. Second, we use cell phone images of sheet music as our\ninput queries, which lends itself to a practical, user-facing application. We\nshow that a previously proposed fingerprinting method for sheet music retrieval\nis far too slow for a real-time application, and we diagnose its shortcomings.\nWe propose a novel hashing scheme called dynamic n-gram fingerprinting that\nsignificantly reduces runtime while simultaneously boosting retrieval accuracy.\nIn experiments on IMSLP data, our proposed method achieves a mean reciprocal\nrank of 0.85 and an average runtime of 0.98 seconds per query.</p>\n", "tags": ["ARXIV"] },
{"key": "yang2021dolg", "year": "2021", "title":"DOLG Single-stage Image Retrieval With Deep Orthogonal Fusion Of Local And Global Features", "abstract": "<p>Image Retrieval is a fundamental task of obtaining images similar to the\nquery one from a database. A common image retrieval practice is to firstly\nretrieve candidate images via similarity search using global image features and\nthen re-rank the candidates by leveraging their local features. Previous\nlearning-based studies mainly focus on either global or local image\nrepresentation learning to tackle the retrieval task. In this paper, we abandon\nthe two-stage paradigm and seek to design an effective single-stage solution by\nintegrating local and global information inside images into compact image\nrepresentations. Specifically, we propose a Deep Orthogonal Local and Global\n(DOLG) information fusion framework for end-to-end image retrieval. It\nattentively extracts representative local information with multi-atrous\nconvolutions and self-attention at first. Components orthogonal to the global\nimage representation are then extracted from the local information. At last,\nthe orthogonal components are concatenated with the global representation as a\ncomplementary, and then aggregation is performed to generate the final\nrepresentation. The whole framework is end-to-end differentiable and can be\ntrained with image-level labels. Extensive experimental results validate the\neffectiveness of our solution and show that our model achieves state-of-the-art\nimage retrieval performances on Revisited Oxford and Paris datasets.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "yang2022fedhap", "year": "2022", "title":"Fedhap Federated Hashing With Global Prototypes For Cross-silo Retrieval", "abstract": "<p>Deep hashing has been widely applied in large-scale data retrieval due to its\nsuperior retrieval efficiency and low storage cost. However, data are often\nscattered in data silos with privacy concerns, so performing centralized data\nstorage and retrieval is not always possible. Leveraging the concept of\nfederated learning (FL) to perform deep hashing is a recent research trend.\nHowever, existing frameworks mostly rely on the aggregation of the local deep\nhashing models, which are trained by performing similarity learning with local\nskewed data only. Therefore, they cannot work well for non-IID clients in a\nreal federated environment. To overcome these challenges, we propose a novel\nfederated hashing framework that enables participating clients to jointly train\nthe shared deep hashing model by leveraging the prototypical hash codes for\neach class. Globally, the transmission of global prototypes with only one\nprototypical hash code per class will minimize the impact of communication cost\nand privacy risk. Locally, the use of global prototypes are maximized by\njointly training a discriminator network and the local hashing network.\nExtensive experiments on benchmark datasets are conducted to demonstrate that\nour method can significantly improve the performance of the deep hashing model\nin the federated environments with non-IID data distributions.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "yang2024adaptive", "year": "2024", "title":"Adaptive Labeling For Deep Learning To Hash", "abstract": "<p>Hash function learning has been widely used for largescale image retrieval because of the efficiency of computation and storage. We introduce AdaLabelHash, a binary\nhash function learning approach via deep neural networks\nin this paper. In AdaLabelHash, class label representations are variables that are adapted during the backward\nnetwork training procedure. We express the labels as hypercube vertices in a K-dimensional space, and the class\nlabel representations together with the network weights are\nupdated in the learning process. As the label representations (or referred to as codewords in this work), are learned\nfrom data, semantically similar classes will be assigned\nwith the codewords that are close to each other in terms\nof Hamming distance in the label space. The codewords\nthen serve as the desired output of the hash function learning, and yield compact and discriminating binary hash representations. AdaLabelHash is easy to implement, which\ncan jointly learn label representations and infer compact\nbinary codes from data. It is applicable to both supervised\nand semi-supervised hash. Experimental results on standard benchmarks demonstrate the satisfactory performance\nof AdaLabelHash.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Supervised"] },
{"key": "yang2024distillhash", "year": "2024", "title":"Distillhash Unsupervised Deep Hashing By Distilling Data Pairs", "abstract": "<p>Due to the high storage and search efficiency, hashing\nhas become prevalent for large-scale similarity search. Particularly, deep hashing methods have greatly improved the\nsearch performance under supervised scenarios. In contrast, unsupervised deep hashing models can hardly achieve\nsatisfactory performance due to the lack of reliable supervisory similarity signals.\n To address this issue, we propose\na novel deep unsupervised hashing model, dubbed DistillHash, which can learn a distilled data set consisted of data\npairs, which have confidence similarity signals. Specifically, we investigate the relationship between the initial\nnoisy similarity signals learned from local structures and\nthe semantic similarity labels assigned by a Bayes optimal\nclassifier. We show that under a mild assumption, some\ndata pairs, of which labels are consistent with those assigned by the Bayes optimal classifier, can be potentially\ndistilled. Inspired by this fact, we design a simple yet effective strategy to distill data pairs automatically and further\nadopt a Bayesian learning framework to learn hash functions from the distilled data set. Extensive experimental results on three widely used benchmark datasets show that the\nproposed DistillHash consistently accomplishes the stateof-the-art search performance.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "yang2024nonlinear", "year": "2024", "title":"Nonlinear Robust Discrete Hashing For Cross-modal Retrieval", "abstract": "<p>Hashing techniques have recently been successfully applied to solve similarity search problems in the information retrieval field because of their significantly reduced storage and high-speed search capabilities. However, the hash codes learned from most recent cross-modal hashing methods lack the ability to comprehensively preserve adequate information, resulting in a less than desirable performance. To solve this limitation, we propose a novel method termed Nonlinear Robust Discrete Hashing (NRDH), for cross-modal retrieval. The main idea behind NRDH is motivated by the success of neural networks, i.e., nonlinear descriptors, in the field of representation learning, and the use of nonlinear descriptors instead of simple linear transformations is more in line with the complex relationships that exist between common latent representation and heterogeneous multimedia data in the real world. In NRDH, we first learn a common latent representation through nonlinear descriptors to encode complementary and consistent information from the features of the heterogeneous multimedia data. Moreover, an asymmetric learning scheme is proposed to correlate the learned hash codes with the common latent representation. Empirically, we demonstrate that NRDH is able to successfully generate a comprehensive common latent representation that significantly improves the quality of the learned hash codes. Then, NRDH adopts a linear learning strategy to fast learn the hash function with the learned hash codes. Extensive experiments performed on two benchmark datasets highlight the superiority of NRDH over several state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "yang2024training", "year": "2024", "title":"Hash3d Training-free Acceleration For 3D Generation", "abstract": "<p>The evolution of 3D generative modeling has been notably propelled by the\nadoption of 2D diffusion models. Despite this progress, the cumbersome\noptimization process per se presents a critical hurdle to efficiency. In this\npaper, we introduce Hash3D, a universal acceleration for 3D generation without\nmodel training. Central to Hash3D is the insight that feature-map redundancy is\nprevalent in images rendered from camera positions and diffusion time-steps in\nclose proximity. By effectively hashing and reusing these feature maps across\nneighboring timesteps and camera angles, Hash3D substantially prevents\nredundant calculations, thus accelerating the diffusion model’s inference in 3D\ngeneration tasks. We achieve this through an adaptive grid-based hashing.\nSurprisingly, this feature-sharing mechanism not only speed up the generation\nbut also enhances the smoothness and view consistency of the synthesized 3D\nobjects. Our experiments covering 5 text-to-3D and 3 image-to-3D models,\ndemonstrate Hash3D’s versatility to speed up optimization, enhancing efficiency\nby 1.3 to 4 times. Additionally, Hash3D’s integration with 3D Gaussian\nsplatting largely speeds up 3D model creation, reducing text-to-3D processing\nto about 10 minutes and image-to-3D conversion to roughly 30 seconds. The\nproject page is at https://adamdad.github.io/hash3D/.</p>\n", "tags": ["ARXIV"] },
{"key": "yao2019efficient", "year": "2019", "title":"Efficient Discrete Supervised Hashing For Large-scale Cross-modal Retrieval", "abstract": "<p>Supervised cross-modal hashing has gained increasing research interest on\nlarge-scale retrieval task owning to its satisfactory performance and\nefficiency. However, it still has some challenging issues to be further\nstudied: 1) most of them fail to well preserve the semantic correlations in\nhash codes because of the large heterogenous gap; 2) most of them relax the\ndiscrete constraint on hash codes, leading to large quantization error and\nconsequent low performance; 3) most of them suffer from relatively high memory\ncost and computational complexity during training procedure, which makes them\nunscalable. In this paper, to address above issues, we propose a supervised\ncross-modal hashing method based on matrix factorization dubbed Efficient\nDiscrete Supervised Hashing (EDSH). Specifically, collective matrix\nfactorization on heterogenous features and semantic embedding with class labels\nare seamlessly integrated to learn hash codes. Therefore, the feature based\nsimilarities and semantic correlations can be both preserved in hash codes,\nwhich makes the learned hash codes more discriminative. Then an efficient\ndiscrete optimal algorithm is proposed to handle the scalable issue. Instead of\nlearning hash codes bit-by-bit, hash codes matrix can be obtained directly\nwhich is more efficient. Extensive experimental results on three public\nreal-world datasets demonstrate that EDSH produces a superior performance in\nboth accuracy and scalability over some existing cross-modal hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "yaqub2021image", "year": "2021", "title":"Image-hashing-based Anomaly Detection For Privacy-preserving Online Proctoring", "abstract": "<p>Online proctoring has become a necessity in online teaching. Video-based\ncrowd-sourced online proctoring solutions are being used, where an exam-taking\nstudent’s video is monitored by third parties, leading to privacy concerns. In\nthis paper, we propose a privacy-preserving online proctoring system. The\nproposed image-hashing-based system can detect the student’s excessive face and\nbody movement (i.e., anomalies) that is resulted when the student tries to\ncheat in the exam. The detection can be done even if the student’s face is\nblurred or masked in video frames. Experiment with an in-house dataset shows\nthe usability of the proposed system.</p>\n", "tags": ["ARXIV"] },
{"key": "ye2015first", "year": "2015", "title":"First-take-all Temporal Order-preserving Hashing For 3D Action Videos", "abstract": "<p>With the prevalence of the commodity depth cameras, the new paradigm of user\ninterfaces based on 3D motion capturing and recognition have dramatically\nchanged the way of interactions between human and computers. Human action\nrecognition, as one of the key components in these devices, plays an important\nrole to guarantee the quality of user experience. Although the model-driven\nmethods have achieved huge success, they cannot provide a scalable solution for\nefficiently storing, retrieving and recognizing actions in the large-scale\napplications. These models are also vulnerable to the temporal translation and\nwarping, as well as the variations in motion scales and execution rates. To\naddress these challenges, we propose to treat the 3D human action recognition\nas a video-level hashing problem and propose a novel First-Take-All (FTA)\nHashing algorithm capable of hashing the entire video into hash codes of fixed\nlength. We demonstrate that this FTA algorithm produces a compact\nrepresentation of the video invariant to the above mentioned variations,\nthrough which action recognition can be solved by an efficient nearest neighbor\nsearch by the Hamming distance between the FTA hash codes. Experiments on the\npublic 3D human action datasets shows that the FTA algorithm can reach a\nrecognition accuracy higher than 80%, with about 15 bits per frame considering\nthere are 65 frames per video over the datasets.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "ye2024unsupervised", "year": "2024", "title":"Unsupervised Few-bits Semantic Hashing With Implicit Topics Modeling", "abstract": "<p>Semantic hashing is a powerful paradigm for\nrepresenting texts as compact binary hash\ncodes. The explosion of short text data has\nspurred the demand of few-bits hashing. However, the performance of existing semantic\nhashing methods cannot be guaranteed when\napplied to few-bits hashing because of severe\ninformation loss. In this paper, we present a\nsimple but effective unsupervised neural generative semantic hashing method with a focus on\nfew-bits hashing. Our model is built upon variational autoencoder and represents each hash\nbit as a Bernoulli variable, which allows the\nmodel to be end-to-end trainable. To address\nthe issue of information loss, we introduce a\nset of auxiliary implicit topic vectors. With\nthe aid of these topic vectors, the generated\nhash codes are not only low-dimensional representations of the original texts but also capture their implicit topics. We conduct comprehensive experiments on four datasets. The results demonstrate that our approach achieves\nsignificant improvements over state-of-the-art\nsemantic hashing methods in few-bits hashing.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "yeh2018representation", "year": "2018", "title":"Representation Learning By Reconstructing Neighborhoods", "abstract": "<p>Since its introduction, unsupervised representation learning has attracted a\nlot of attention from the research community, as it is demonstrated to be\nhighly effective and easy-to-apply in tasks such as dimension reduction,\nclustering, visualization, information retrieval, and semi-supervised learning.\nIn this work, we propose a novel unsupervised representation learning framework\ncalled neighbor-encoder, in which domain knowledge can be easily incorporated\ninto the learning process without modifying the general encoder-decoder\narchitecture of the classic autoencoder.In contrast to autoencoder, which\nreconstructs the input data itself, neighbor-encoder reconstructs the input\ndata’s neighbors. As the proposed representation learning problem is\nessentially a neighbor reconstruction problem, domain knowledge can be easily\nincorporated in the form of an appropriate definition of similarity between\nobjects. Based on that observation, our framework can leverage any\noff-the-shelf similarity search algorithms or side information to find the\nneighbor of an input object. Applications of other algorithms (e.g.,\nassociation rule mining) in our framework are also possible, given that the\nappropriate definition of neighbor can vary in different contexts. We have\ndemonstrated the effectiveness of our framework in many diverse domains,\nincluding images, text, and time series, and for various data mining tasks\nincluding classification, clustering, and visualization. Experimental results\nshow that neighbor-encoder not only outperforms autoencoder in most of the\nscenarios we consider, but also achieves the state-of-the-art performance on\ntext document clustering.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "yeh2022embedding", "year": "2022", "title":"Embedding Compression With Hashing For Efficient Representation Learning In Large-scale Graph", "abstract": "<p>Graph neural networks (GNNs) are deep learning models designed specifically\nfor graph data, and they typically rely on node features as the input to the\nfirst layer. When applying such a type of network on the graph without node\nfeatures, one can extract simple graph-based node features (e.g., number of\ndegrees) or learn the input node representations (i.e., embeddings) when\ntraining the network. While the latter approach, which trains node embeddings,\nmore likely leads to better performance, the number of parameters associated\nwith the embeddings grows linearly with the number of nodes. It is therefore\nimpractical to train the input node embeddings together with GNNs within\ngraphics processing unit (GPU) memory in an end-to-end fashion when dealing\nwith industrial-scale graph data. Inspired by the embedding compression methods\ndeveloped for natural language processing (NLP) tasks, we develop a node\nembedding compression method where each node is compactly represented with a\nbit vector instead of a floating-point vector. The parameters utilized in the\ncompression method can be trained together with GNNs. We show that the proposed\nnode embedding compression method achieves superior performance compared to the\nalternatives.</p>\n", "tags": ["ARXIV","Deep Learning","Graph","Supervised"] },
{"key": "yeo2023cuckoo", "year": "2023", "title":"Cuckoo Hashing In Cryptography Optimal Parameters Robustness And Applications", "abstract": "<p>Cuckoo hashing is a powerful primitive that enables storing items using small\nspace with efficient querying. At a high level, cuckoo hashing maps \\(n\\) items\ninto \\(b\\) entries storing at most \\(\\ell\\) items such that each item is placed\ninto one of \\(k\\) randomly chosen entries. Additionally, there is an overflow\nstash that can store at most \\(s\\) items. Many cryptographic primitives rely upon\ncuckoo hashing to privately embed and query data where it is integral to ensure\nsmall failure probability when constructing cuckoo hashing tables as it\ndirectly relates to the privacy guarantees.\n  As our main result, we present a more query-efficient cuckoo hashing\nconstruction using more hash functions. For construction failure probability\n\\(\\epsilon\\), the query overhead of our scheme is \\(O(1 +\n\\sqrt{log(1/\\epsilon)/log n})\\). Our scheme has quadratically smaller query\noverhead than prior works for any target failure probability \\(\\epsilon\\). We\nalso prove lower bounds matching our construction. Our improvements come from a\nnew understanding of the locality of cuckoo hashing failures for small sets of\nitems.\n  We also initiate the study of robust cuckoo hashing where the input set may\nbe chosen with knowledge of the hash functions. We present a cuckoo hashing\nscheme using more hash functions with query overhead \\(\\tilde{O}(log \\lambda)\\)\nthat is robust against poly\\((\\lambda)\\) adversaries. Furthermore, we present\nlower bounds showing that this construction is tight and that extending\nprevious approaches of large stashes or entries cannot obtain robustness except\nwith \\(Ω(n)\\) query overhead.\n  As applications of our results, we obtain improved constructions for batch\ncodes and PIR. In particular, we present the most efficient explicit batch code\nand blackbox reduction from single-query PIR to batch PIR.</p>\n", "tags": ["ARXIV","Graph","Independent"] },
{"key": "yildiz2015homogeneous", "year": "2015", "title":"The Homogeneous Weight For r_k Related Gray Map And New Binary Quasicyclic Codes", "abstract": "<p>Using theoretical results about the homogeneous weights for Frobenius rings,\nwe describe the homogeneous weight for the ring family \\(R_k\\), a recently\nintroduced family of Frobenius rings which have been used extensively in coding\ntheory. We find an associated Gray map for the homogeneous weight using first\norder Reed-Muller codes and we describe some of the general properties of the\nimages of codes over \\(R_k\\) under this Gray map. We then discuss quasitwisted\ncodes over \\(R_k\\) and their binary images under the homogeneous Gray map. In\nthis way, we find many optimal binary codes which are self-orthogonal and\nquasicyclic. In particular, we find a substantial number of optimal binary\ncodes that are quasicyclic of index 8, 16 and 24, nearly all of which are new\nadditions to the database of quasicyclic codes kept by Chen.</p>\n", "tags": ["ARXIV"] },
{"key": "yin2022rapid", "year": "2022", "title":"Rapid Person Re-identification Via Sub-space Consistency Regularization", "abstract": "<p>Person Re-Identification (ReID) matches pedestrians across disjoint cameras.\nExisting ReID methods adopting real-value feature descriptors have achieved\nhigh accuracy, but they are low in efficiency due to the slow Euclidean\ndistance computation as well as complex quick-sort algorithms. Recently, some\nworks propose to yield binary encoded person descriptors which instead only\nrequire fast Hamming distance computation and simple counting-sort algorithms.\nHowever, the performances of such binary encoded descriptors, especially with\nshort code (e.g., 32 and 64 bits), are hardly satisfactory given the sparse\nbinary space. To strike a balance between the model accuracy and efficiency, we\npropose a novel Sub-space Consistency Regularization (SCR) algorithm that can\nspeed up the ReID procedure by \\(0.25\\) times than real-value features under the\nsame dimensions whilst maintaining a competitive accuracy, especially under\nshort codes. SCR transforms real-value features vector (e.g., 2048 float32)\nwith short binary codes (e.g., 64 bits) by first dividing real-value features\nvector into \\(M\\) sub-spaces, each with \\(C\\) clustered centroids. Thus the\ndistance between two samples can be expressed as the summation of the\nrespective distance to the centroids, which can be sped up by offline\ncalculation and maintained via a look-up table. On the other side, these\nreal-value centroids help to achieve significantly higher accuracy than using\nbinary code. Lastly, we convert the distance look-up table to be integer and\napply the counting-sort algorithm to speed up the ranking stage.\n  We also propose a novel consistency regularization with an iterative\nframework. Experimental results on Market-1501 and DukeMTMC-reID show promising\nand exciting results. Under short code, our proposed SCR enjoys\nReal-value-level accuracy and Hashing-level speed.</p>\n", "tags": ["ARXIV"] },
{"key": "yu2014circulant", "year": "2014", "title":"Circulant Binary Embedding", "abstract": "<p>Binary embedding of high-dimensional data requires long codes to preserve the\ndiscriminative power of the input space. Traditional binary coding methods\noften suffer from very high computation and storage costs in such a scenario.\nTo address this problem, we propose Circulant Binary Embedding (CBE) which\ngenerates binary codes by projecting the data with a circulant matrix. The\ncirculant structure enables the use of Fast Fourier Transformation to speed up\nthe computation. Compared to methods that use unstructured matrices, the\nproposed method improves the time complexity from \\(\\mathcal{O}(d^2)\\) to\n\\(\\mathcal{O}(dlog{d})\\), and the space complexity from \\(\\mathcal{O}(d^2)\\) to\n\\(\\mathcal{O}(d)\\) where \\(d\\) is the input dimensionality. We also propose a novel\ntime-frequency alternating optimization to learn data-dependent circulant\nprojections, which alternatively minimizes the objective in original and\nFourier domains. We show by extensive experiments that the proposed approach\ngives much better performance than the state-of-the-art approaches for fixed\ntime, and provides much faster computation with no performance degradation for\nfixed number of bits.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "yu2015binary", "year": "2015", "title":"On Binary Embedding Using Circulant Matrices", "abstract": "<p>Binary embeddings provide efficient and powerful ways to perform operations\non large scale data. However binary embedding typically requires long codes in\norder to preserve the discriminative power of the input space. Thus binary\ncoding methods traditionally suffer from high computation and storage costs in\nsuch a scenario. To address this problem, we propose Circulant Binary Embedding\n(CBE) which generates binary codes by projecting the data with a circulant\nmatrix. The circulant structure allows us to use Fast Fourier Transform\nalgorithms to speed up the computation. For obtaining \\(k\\)-bit binary codes from\n\\(d\\)-dimensional data, this improves the time complexity from \\(O(dk)\\) to\n\\(O(dlog{d})\\), and the space complexity from \\(O(dk)\\) to \\(O(d)\\).\n  We study two settings, which differ in the way we choose the parameters of\nthe circulant matrix. In the first, the parameters are chosen randomly and in\nthe second, the parameters are learned using the data. For randomized CBE, we\ngive a theoretical analysis comparing it with binary embedding using an\nunstructured random projection matrix. The challenge here is to show that the\ndependencies in the entries of the circulant matrix do not lead to a loss in\nperformance. In the second setting, we design a novel time-frequency\nalternating optimization to learn data-dependent circulant projections, which\nalternatively minimizes the objective in original and Fourier domains. In both\nthe settings, we show by extensive experiments that the CBE approach gives much\nbetter performance than the state-of-the-art approaches if we fix a running\ntime, and provides much faster computation with negligible performance\ndegradation if we fix the number of bits in the embedding.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "yu2016variable", "year": "2016", "title":"Variable-length Hashing", "abstract": "<p>Hashing has emerged as a popular technique for large-scale similarity search.\nMost learning-based hashing methods generate compact yet correlated hash codes.\nHowever, this redundancy is storage-inefficient. Hence we propose a lossless\nvariable-length hashing (VLH) method that is both storage- and\nsearch-efficient. Storage efficiency is achieved by converting the fixed-length\nhash code into a variable-length code. Search efficiency is obtained by using a\nmultiple hash table structure. With VLH, we are able to deliberately add\nredundancy into hash codes to improve retrieval performance with little\nsacrifice in storage efficiency or search complexity. In particular, we propose\na block K-means hashing (B-KMH) method to obtain significantly improved\nretrieval performance with no increase in storage and marginal increase in\ncomputational cost.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "yu2018discriminative", "year": "2018", "title":"Discriminative Supervised Hashing For Cross-modal Similarity Search", "abstract": "<p>With the advantage of low storage cost and high retrieval efficiency, hashing\ntechniques have recently been an emerging topic in cross-modal similarity\nsearch. As multiple modal data reflect similar semantic content, many\nresearches aim at learning unified binary codes. However, discriminative\nhashing features learned by these methods are not adequate. This results in\nlower accuracy and robustness. We propose a novel hashing learning framework\nwhich jointly performs classifier learning, subspace learning and matrix\nfactorization to preserve class-specific semantic content, termed\nDiscriminative Supervised Hashing (DSH), to learn the discrimative unified\nbinary codes for multi-modal data. Besides, reducing the loss of information\nand preserving the non-linear structure of data, DSH non-linearly projects\ndifferent modalities into the common space in which the similarity among\nheterogeneous data points can be measured. Extensive experiments conducted on\nthe three publicly available datasets demonstrate that the framework proposed\nin this paper outperforms several state-of -the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "yu2018learning", "year": "2018", "title":"Learning Discriminative Hashing Codes For Cross-modal Retrieval Based On Multi-view Features", "abstract": "<p>Hashing techniques have been applied broadly in retrieval tasks due to their\nlow storage requirements and high speed of processing. Many hashing methods\nbased on a single view have been extensively studied for information retrieval.\nHowever, the representation capacity of a single view is insufficient and some\ndiscriminative information is not captured, which results in limited\nimprovement. In this paper, we employ multiple views to represent images and\ntexts for enriching the feature information. Our framework exploits the\ncomplementary information among multiple views to better learn the\ndiscriminative compact hash codes. A discrete hashing learning framework that\njointly performs classifier learning and subspace learning is proposed to\ncomplete multiple search tasks simultaneously. Our framework includes two\nstages, namely a kernelization process and a quantization process.\nKernelization aims to find a common subspace where multi-view features can be\nfused. The quantization stage is designed to learn discriminative unified\nhashing codes. Extensive experiments are performed on single-label datasets\n(WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE) and the\nexperimental results indicate the superiority of our method compared with the\nstate-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Independent","Quantisation"] },
{"key": "yu2018semi", "year": "2018", "title":"Semi-supervised Hashing For Semi-paired Cross-view Retrieval", "abstract": "<p>Recently, hashing techniques have gained importance in large-scale retrieval\ntasks because of their retrieval speed. Most of the existing cross-view\nframeworks assume that data are well paired. However, the fully-paired\nmultiview situation is not universal in real applications. The aim of the\nmethod proposed in this paper is to learn the hashing function for semi-paired\ncross-view retrieval tasks. To utilize the label information of partial data,\nwe propose a semi-supervised hashing learning framework which jointly performs\nfeature extraction and classifier learning. The experimental results on two\ndatasets show that our method outperforms several state-of-the-art methods in\nterms of retrieval accuracy.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "yu2019unsupervised", "year": "2019", "title":"Unsupervised Multi-modal Hashing For Cross-modal Retrieval", "abstract": "<p>With the advantage of low storage cost and high efficiency, hashing learning\nhas received much attention in the domain of Big Data. In this paper, we\npropose a novel unsupervised hashing learning method to cope with this open\nproblem to directly preserve the manifold structure by hashing. To address this\nproblem, both the semantic correlation in textual space and the locally\ngeometric structure in the visual space are explored simultaneously in our\nframework. Besides, the `2;1-norm constraint is imposed on the projection\nmatrices to learn the discriminative hash function for each modality. Extensive\nexperiments are performed to evaluate the proposed method on the three publicly\navailable datasets and the experimental results show that our method can\nachieve superior performance over the state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "yu2020comprehensive", "year": "2020", "title":"Comprehensive Graph-conditional Similarity Preserving Network For Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently.\nCurrent UCMH focuses on exploring data similarities. However, current UCMH\nmethods calculate the similarity between two data, mainly relying on the two\ndata’s cross-modal features. These methods suffer from inaccurate similarity\nproblems that result in a suboptimal retrieval Hamming space, because the\ncross-modal features between the data are not sufficient to describe the\ncomplex data relationships, such as situations where two data have different\nfeature representations but share the inherent concepts. In this paper, we\ndevise a deep graph-neighbor coherence preserving network (DGCPN).\nSpecifically, DGCPN stems from graph models and explores graph-neighbor\ncoherence by consolidating the information between data and their neighbors.\nDGCPN regulates comprehensive similarity preserving losses by exploiting three\ntypes of data similarities (i.e., the graph-neighbor coherence, the coexistent\nsimilarity, and the intra- and inter-modality consistency) and designs a\nhalf-real and half-binary optimization strategy to reduce the quantization\nerrors during hashing. Essentially, DGCPN addresses the inaccurate similarity\nproblem by exploring and exploiting the data’s intrinsic relationships in a\ngraph. We conduct extensive experiments on three public UCMH datasets. The\nexperimental results demonstrate the superiority of DGCPN, e.g., by improving\nthe mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit\nhashing codes to retrieve texts from images. We will release the source code\npackage and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Has Code","Quantisation","Unsupervised"] },
{"key": "yu2020encode", "year": "2020", "title":"Encode The Unseen Predictive Video Hashing For Scalable Mid-stream Retrieval", "abstract": "<p>This paper tackles a new problem in computer vision: mid-stream\nvideo-to-video retrieval. This task, which consists in searching a database for\ncontent similar to a video right as it is playing, e.g. from a live stream,\nexhibits challenging characteristics. Only the beginning part of the video is\navailable as query and new frames are constantly added as the video plays out.\nTo perform retrieval in this demanding situation, we propose an approach based\non a binary encoder that is both predictive and incremental in order to (1)\naccount for the missing video content at query time and (2) keep up with\nrepeated, continuously evolving queries throughout the streaming. In\nparticular, we present the first hashing framework that infers the unseen\nfuture content of a currently playing video. Experiments on FCVID and\nActivityNet demonstrate the feasibility of this task. Our approach also yields\na significant mAP@20 performance increase compared to a baseline adapted from\nthe literature for this task, for instance 7.4% (2.6%) increase at 20% (50%) of\nelapsed runtime on FCVID using bitcodes of size 192 bits.</p>\n", "tags": ["ARXIV","Video Retrieval"] },
{"key": "yu2020self", "year": "2020", "title":"Self-supervised Asymmetric Deep Hashing With Margin-scalable Constraint", "abstract": "<p>Due to its effectivity and efficiency, deep hashing approaches are widely\nused for large-scale visual search. However, it is still challenging to produce\ncompact and discriminative hash codes for images associated with multiple\nsemantics for two main reasons, 1) similarity constraints designed in most of\nthe existing methods are based upon an oversimplified similarity\nassignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs\nsharing at least 1 label), 2) the exploration in multi-semantic relevance are\ninsufficient or even neglected in many of the existing methods. These problems\nsignificantly limit the discrimination of generated hash codes. In this paper,\nwe propose a novel self-supervised asymmetric deep hashing method with a\nmargin-scalable constraint(SADH) approach to cope with these problems. SADH\nimplements a self-supervised network to sufficiently preserve semantic\ninformation in a semantic feature dictionary and a semantic code dictionary for\nthe semantics of the given dataset, which efficiently and precisely guides a\nfeature learning network to preserve multilabel semantic information using an\nasymmetric learning strategy. By further exploiting semantic dictionaries, a\nnew margin-scalable constraint is employed for both precise similarity\nsearching and robust hash code generation. Extensive empirical research on four\npopular benchmarks validates the proposed method and shows it outperforms\nseveral state-of-the-art approaches.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "yu2022learning", "year": "2022", "title":"Learning To Hash Naturally Sorts", "abstract": "<p>Learning to hash pictures a list-wise sorting problem. Its testing metrics,\ne.g., mean-average precision, count on a sorted candidate list ordered by\npair-wise code similarity. However, scarcely does one train a deep hashing\nmodel with the sorted results end-to-end because of the non-differentiable\nnature of the sorting operation. This inconsistency in the objectives of\ntraining and test may lead to sub-optimal performance since the training loss\noften fails to reflect the actual retrieval metric. In this paper, we tackle\nthis problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming\ndistances of samples’ hash codes and accordingly gather their latent\nrepresentations for self-supervised training. Thanks to the recent advances in\ndifferentiable sorting approximations, the hash head receives gradients from\nthe sorter so that the hash encoder can be optimized along with the training\nprocedure. Additionally, we describe a novel Sorted Noise-Contrastive\nEstimation (SortedNCE) loss that selectively picks positive and negative\nsamples for contrastive learning, which allows NSH to mine data semantic\nrelations during training in an unsupervised manner. Our extensive experiments\nshow the proposed NSH model significantly outperforms the existing unsupervised\nhashing methods on three benchmarked datasets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "yu2022weighted", "year": "2022", "title":"Weighted Contrastive Hashing", "abstract": "<p>The development of unsupervised hashing is advanced by the recent popular\ncontrastive learning paradigm. However, previous contrastive learning-based\nworks have been hampered by (1) insufficient data similarity mining based on\nglobal-only image representations, and (2) the hash code semantic loss caused\nby the data augmentation. In this paper, we propose a novel method, namely\nWeighted Contrative Hashing (WCH), to take a step towards solving these two\nproblems. We introduce a novel mutual attention module to alleviate the problem\nof information asymmetry in network features caused by the missing image\nstructure during contrative augmentation. Furthermore, we explore the\nfine-grained semantic relations between images, i.e., we divide the images into\nmultiple patches and calculate similarities between patches. The aggregated\nweighted similarities, which reflect the deep image relations, are distilled to\nfacilitate the hash codes learning with a distillation loss, so as to obtain\nbetter retrieval performance. Extensive experiments show that the proposed WCH\nsignificantly outperforms existing unsupervised hashing methods on three\nbenchmark datasets.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "yu2024circulant", "year": "2024", "title":"Circulant Binary Embedding", "abstract": "<p>Binary embedding of high-dimensional data requires\nlong codes to preserve the discriminative\npower of the input space. Traditional binary coding\nmethods often suffer from very high computation\nand storage costs in such a scenario. To\naddress this problem, we propose Circulant Binary\nEmbedding (CBE) which generates binary\ncodes by projecting the data with a circulant matrix.\nThe circulant structure enables the use of\nFast Fourier Transformation to speed up the computation.\nCompared to methods that use unstructured\nmatrices, the proposed method improves\nthe time complexity from O(d^2\n) to O(d log d),\nand the space complexity from O(d^2) to O(d)\nwhere d is the input dimensionality. We also\npropose a novel time-frequency alternating optimization\nto learn data-dependent circulant projections,\nwhich alternatively minimizes the objective\nin original and Fourier domains. We show\nby extensive experiments that the proposed approach\ngives much better performance than the\nstate-of-the-art approaches for fixed time, and\nprovides much faster computation with no performance\ndegradation for fixed number of bits.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "yu2024deep", "year": "2024", "title":"Deep Graph-neighbor Coherence Preserving Network For Unsupervised Cross-modal Hashing", "abstract": "<p>Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data’s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufficient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Specifically, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data’s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieval texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.</p>\n", "tags": ["ARXIV","Cross Modal","Graph","Has Code","Quantisation","Unsupervised"] },
{"key": "yuan2019central", "year": "2019", "title":"Central Similarity Quantization For Efficient Image And Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn hash functions from\npairwise or triplet data relationships, which only capture the data similarity\nlocally, and often suffer from low learning efficiency and low collision rate.\nIn this work, we propose a new <em>global</em> similarity metric, termed as\n<em>central similarity</em>, with which the hash codes of similar data pairs are\nencouraged to approach a common center and those for dissimilar pairs to\nconverge to different centers, to improve hash learning efficiency and\nretrieval accuracy. We principally formulate the computation of the proposed\ncentral similarity metric by introducing a new concept, i.e., <em>hash\ncenter</em> that refers to a set of data points scattered in the Hamming space with\na sufficient mutual distance between each other. We then provide an efficient\nmethod to construct well separated hash centers by leveraging the Hadamard\nmatrix and Bernoulli distributions. Finally, we propose the Central Similarity\nQuantization (CSQ) that optimizes the central similarity between data points\nw.r.t.\\ their hash centers instead of optimizing the local similarity. CSQ is\ngeneric and applicable to both image and video hashing scenarios. Extensive\nexperiments on large-scale image and video retrieval tasks demonstrate that CSQ\ncan generate cohesive hash codes for similar data pairs and dispersed hash\ncodes for dissimilar pairs, achieving a noticeable boost in retrieval\nperformance, i.e. 3\\%-20\\% in mAP over the previous state-of-the-arts. The code\nis at: \\url{https://github.com/yuanli2333/Hadamard-Matrix-for-hashing}</p>\n", "tags": ["ARXIV","Has Code","Independent","Quantisation","Video Retrieval"] },
{"key": "yuan2019signal", "year": "2019", "title":"Signal-to-noise Ratio A Robust Distance Metric For Deep Metric Learning", "abstract": "<p>Deep metric learning, which learns discriminative features to process image\nclustering and retrieval tasks, has attracted extensive attention in recent\nyears. A number of deep metric learning methods, which ensure that similar\nexamples are mapped close to each other and dissimilar examples are mapped\nfarther apart, have been proposed to construct effective structures for loss\nfunctions and have shown promising results. In this paper, different from the\napproaches on learning the loss structures, we propose a robust SNR distance\nmetric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of\nimage pairs for deep metric learning. By exploring the properties of our SNR\ndistance metric from the view of geometry space and statistical theory, we\nanalyze the properties of our metric and show that it can preserve the semantic\nsimilarity between image pairs, which well justify its suitability for deep\nmetric learning. Compared with Euclidean distance metric, our SNR distance\nmetric can further jointly reduce the intra-class distances and enlarge the\ninter-class distances for learned features. Leveraging our SNR distance metric,\nwe propose Deep SNR-based Metric Learning (DSML) to generate discriminative\nfeature embeddings. By extensive experiments on three widely adopted\nbenchmarks, including CARS196, CUB200-2011 and CIFAR10, our DSML has shown its\nsuperiority over other state-of-the-art methods. Additionally, we extend our\nSNR distance metric to deep hashing learning, and conduct experiments on two\nbenchmarks, including CIFAR10 and NUS-WIDE, to demonstrate the effectiveness\nand generality of our SNR distance metric.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "yuan2023semantic", "year": "2023", "title":"Semantic-aware Adversarial Training For Reliable Deep Hashing Retrieval", "abstract": "<p>Deep hashing has been intensively studied and successfully applied in\nlarge-scale image retrieval systems due to its efficiency and effectiveness.\nRecent studies have recognized that the existence of adversarial examples poses\na security threat to deep hashing models, that is, adversarial vulnerability.\nNotably, it is challenging to efficiently distill reliable semantic\nrepresentatives for deep hashing to guide adversarial learning, and thereby it\nhinders the enhancement of adversarial robustness of deep hashing-based\nretrieval models. Moreover, current researches on adversarial training for deep\nhashing are hard to be formalized into a unified minimax structure. In this\npaper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the\nadversarial robustness of deep hashing models. Specifically, we conceive a\ndiscriminative mainstay features learning (DMFL) scheme to construct semantic\nrepresentatives for guiding adversarial learning in deep hashing. Particularly,\nour DMFL with the strict theoretical guarantee is adaptively optimized in a\ndiscriminative learning manner, where both discriminative and semantic\nproperties are jointly considered. Moreover, adversarial examples are\nfabricated by maximizing the Hamming distance between the hash codes of\nadversarial samples and mainstay features, the efficacy of which is validated\nin the adversarial attack trials. Further, we, for the first time, formulate\nthe formalized adversarial training of deep hashing into a unified minimax\noptimization under the guidance of the generated mainstay codes. Extensive\nexperiments on benchmark datasets show superb attack performance against the\nstate-of-the-art algorithms, meanwhile, the proposed adversarial training can\neffectively eliminate adversarial perturbations for trustworthy deep\nhashing-based retrieval. Our code is available at\nhttps://github.com/xandery-geek/SAAT.</p>\n", "tags": ["Has Code","Image Retrieval","Independent"] },
{"key": "yuan2024central", "year": "2024", "title":"Central Similarity Hashing For Efficient Image And Video Retrieval", "abstract": "<p>Existing data-dependent hashing methods usually learn\nhash functions from the pairwise or triplet data relationships, which only capture the data similarity locally, and\noften suffer low learning efficiency and low collision rate.\nIn this work, we propose a new global similarity metric,\ntermed as central similarity, with which the hash codes for\nsimilar data pairs are encouraged to approach a common\ncenter and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e. hash center that refers to a set\nof data points scattered in the Hamming space with sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash\ncenters by leveraging the Hadamard matrix and Bernoulli\ndistributions. Finally, we propose the Central Similarity\nHashing (CSH) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The CSH is generic and applicable to both image and video hashing. Extensive experiments on large-scale image and video retrieval demonstrate CSH can generate cohesive hash codes for similar\ndata pairs and dispersed hash codes for dissimilar pairs,\nand achieve noticeable boost in retrieval performance, i.e.\n3%-20% in mAP over the previous state-of-the-art. The\ncodes are in: https://github.com/yuanli2333/\nHadamard-Matrix-for-hashing</p>\n", "tags": ["ARXIV","Has Code","Independent","Video Retrieval"] },
{"key": "yuan2024towards", "year": "2024", "title":"Towards Optimal Deep Hashing Via Policy Gradient", "abstract": "<p>In this paper, we propose a simple yet effective relaxation free method to learn more effective binary codes via policy gradient for\nscalable image search. While a variety of deep hashing methods have been\nproposed in recent years, most of them are confronted by the dilemma\nto obtain optimal binary codes in a truly end-to-end manner with nonsmooth sign activations. Unlike existing methods which usually employ a\ngeneral relaxation framework to adapt to the gradient-based algorithms,\nour approach formulates the non-smooth part of the hashing network\nas sampling with a stochastic policy, so that the retrieval performance\ndegradation caused by the relaxation can be avoided. Specifically, our\nmethod directly generates the binary codes and maximizes the expectation of rewards for similarity preservation, where the network can be\ntrained directly via policy gradient. Hence, the differentiation challenge\nfor discrete optimization can be naturally addressed, which leads to effective gradients and binary codes. Extensive experimental results on three\nbenchmark datasets validate the effectiveness of the proposed method.</p>\n", "tags": ["ARXIV"] },
{"key": "yun2024neurohash", "year": "2024", "title":"Neurohash A Hyperdimensional Neuro-symbolic Framework For Spatially-aware Image Hashing And Retrieval", "abstract": "<p>Customizable image retrieval from large datasets remains a critical\nchallenge, particularly when preserving spatial relationships within images.\nTraditional hashing methods, primarily based on deep learning, often fail to\ncapture spatial information adequately and lack transparency. In this paper, we\nintroduce NeuroHash, a novel neuro-symbolic framework leveraging\nHyperdimensional Computing (HDC) to enable highly customizable, spatially-aware\nimage retrieval. NeuroHash combines pre-trained deep neural network models with\nHDC-based symbolic models, allowing for flexible manipulation of hash values to\nsupport conditional image retrieval. Our method includes a self-supervised\ncontext-aware HDC encoder and novel loss terms for optimizing lower-dimensional\nbipolar hashing using multilinear hyperplanes. We evaluate NeuroHash on two\nbenchmark datasets, demonstrating superior performance compared to\nstate-of-the-art hashing methods, as measured by mAP@5K scores and our newly\nintroduced metric, mAP@5Kr, which assesses spatial alignment. The results\nhighlight NeuroHash’s ability to achieve competitive performance while offering\nsignificant advantages in flexibility and customization, paving the way for\nmore advanced and versatile image retrieval systems.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Supervised"] },
{"key": "yvinec2021red", "year": "2021", "title":"RED Looking For Redundancies For Data-freestructured Compression Of Deep Neural Networks", "abstract": "<p>Deep Neural Networks (DNNs) are ubiquitous in today’s computer vision landscape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to retrain the model.  In this paper, we present RED, a data-free, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.</p>\n", "tags": ["NEURIPS","Supervised"] },
{"key": "zain2011strict", "year": "2011", "title":"Strict Authentication Watermarking With JPEG Compression (SAW-JPEG) For Medical Images", "abstract": "<p>This paper proposes a strict authentication watermarking for medical images.\nIn this scheme, we define region of interest (ROI) by taking the smallest\nrectangle around an image. The watermark is generated from hashing the area of\ninterest. The embedding region is considered to be outside the region of\ninterest as to preserve the area from distortion as a result from watermarking.\nThe strict authentication watermarking is robust to some degree of JPEG\ncompression (SAW-JPEG). JPEG compression will be reviewed. To embed a watermark\nin the spatial domain, we have to make sure that the embedded watermark will\nsurvive JPEG quantization process. The watermarking scheme, including data\nembedding, extracting and verifying procedure were presented. Experimental\nresults showed that such a scheme could embed and extract the watermark at a\nhigh compression rate. The watermark is robust to a high compression rate up to\n90.6%. The JPEG image quality threshold is 60 for the least significant bit\nembedding. The image quality threshold is increased to 61 for 2nd and 3rd LSB\nmanipulations.</p>\n", "tags": ["Quantisation"] },
{"key": "zamani2023multivariate", "year": "2023", "title":"Multivariate Representation Learning For Information Retrieval", "abstract": "<p>Dense retrieval models use bi-encoder network architectures for learning\nquery and document representations. These representations are often in the form\nof a vector representation and their similarities are often computed using the\ndot product function. In this paper, we propose a new representation learning\nframework for dense retrieval. Instead of learning a vector for each query and\ndocument, our framework learns a multivariate distribution and uses negative\nmultivariate KL divergence to compute the similarity between distributions. For\nsimplicity and efficiency reasons, we assume that the distributions are\nmultivariate normals and then train large language models to produce mean and\nvariance vectors for these distributions. We provide a theoretical foundation\nfor the proposed framework and show that it can be seamlessly integrated into\nthe existing approximate nearest neighbor algorithms to perform retrieval\nefficiently. We conduct an extensive suite of experiments on a wide range of\ndatasets, and demonstrate significant improvements compared to competitive\ndense retrieval models.</p>\n", "tags": ["ARXIV"] },
{"key": "zemene2017large", "year": "2017", "title":"Large-scale Image Geo-localization Using Dominant Sets", "abstract": "<p>This paper presents a new approach for the challenging problem of\ngeo-locating an image using image matching in a structured database of\ncity-wide reference images with known GPS coordinates. We cast the\ngeo-localization as a clustering problem on local image features. Akin to\nexisting approaches on the problem, our framework builds on low-level features\nwhich allow partial matching between images. For each local feature in the\nquery image, we find its approximate nearest neighbors in the reference set.\nNext, we cluster the features from reference images using Dominant Set\nclustering, which affords several advantages over existing approaches. First,\nit permits variable number of nodes in the cluster which we use to dynamically\nselect the number of nearest neighbors (typically coming from multiple\nreference images) for each query feature based on its discrimination value.\nSecond, as we also quantify in our experiments, this approach is several orders\nof magnitude faster than existing approaches. Thus, we obtain multiple clusters\n(different local maximizers) and obtain a robust final solution to the problem\nusing multiple weak solutions through constrained Dominant Set clustering on\nglobal image features, where we enforce the constraint that the query image\nmust be included in the cluster. This second level of clustering also bypasses\nheuristic approaches to voting and selecting the reference image that matches\nto the query. We evaluated the proposed framework on an existing dataset of\n102k street view images as well as a new dataset of 300k images, and show that\nit outperforms the state-of-the-art by 20% and 7%, respectively, on the two\ndatasets.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "zeng2019modal", "year": "2019", "title":"Modal-aware Features For Multimodal Hashing", "abstract": "<p>Many retrieval applications can benefit from multiple modalities, e.g., text\nthat contains images on Wikipedia, for which how to represent multimodal data\nis the critical component. Most deep multimodal learning methods typically\ninvolve two steps to construct the joint representations: 1) learning of\nmultiple intermediate features, with each intermediate feature corresponding to\na modality, using separate and independent deep models; 2) merging the\nintermediate features into a joint representation using a fusion strategy.\nHowever, in the first step, these intermediate features do not have previous\nknowledge of each other and cannot fully exploit the information contained in\nthe other modalities. In this paper, we present a modal-aware operation as a\ngeneric building block to capture the non-linear dependences among the\nheterogeneous intermediate features that can learn the underlying correlation\nstructures in other multimodal data as soon as possible. The modal-aware\noperation consists of a kernel network and an attention network. The kernel\nnetwork is utilized to learn the non-linear relationships with other\nmodalities. Then, to learn better representations for binary hash codes, we\npresent an attention network that finds the informative regions of these\nmodal-aware features that are favorable for retrieval. Experiments conducted on\nthree public benchmark datasets demonstrate significant improvements in the\nperformance of our method relative to state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "zeng2019simultaneous", "year": "2019", "title":"Simultaneous Region Localization And Hash Coding For Fine-grained Image Retrieval", "abstract": "<p>Fine-grained image hashing is a challenging problem due to the difficulties\nof discriminative region localization and hash code generation. Most existing\ndeep hashing approaches solve the two tasks independently. While these two\ntasks are correlated and can reinforce each other. In this paper, we propose a\ndeep fine-grained hashing to simultaneously localize the discriminative regions\nand generate the efficient binary codes. The proposed approach consists of a\nregion localization module and a hash coding module. The region localization\nmodule aims to provide informative regions to the hash coding module. The hash\ncoding module aims to generate effective binary codes and give feedback for\nlearning better localizer. Moreover, to better capture subtle differences,\nmulti-scale regions at different layers are learned without the need of\nbounding-box/part annotations. Extensive experiments are conducted on two\npublic benchmark fine-grained datasets. The results demonstrate significant\nimprovements in the performance of our method relative to other fine-grained\nhashing algorithms.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "zeng2021phpq", "year": "2021", "title":"PHPQ Pyramid Hybrid Pooling Quantization For Efficient Fine-grained Image Retrieval", "abstract": "<p>Deep hashing approaches, including deep quantization and deep binary hashing,\nhave become a common solution to large-scale image retrieval due to their high\ncomputation and storage efficiency. Most existing hashing methods cannot\nproduce satisfactory results for fine-grained retrieval, because they usually\nadopt the outputs of the last CNN layer to generate binary codes. Since deeper\nlayers tend to summarize visual clues, e.g., texture, into abstract semantics,\ne.g., dogs and cats, the feature produced by the last CNN layer is less\neffective in capturing subtle but discriminative visual details that mostly\nexist in shallow layers. To improve fine-grained image hashing, we propose\nPyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid\nHybrid Pooling (PHP) module to capture and preserve fine-grained semantic\ninformation from multi-level features, which emphasizes the subtle\ndiscrimination of different sub-categories. Besides, we propose a learnable\nquantization module with a partial codebook attention mechanism, which helps to\noptimize the most relevant codewords and improves the quantization.\nComprehensive experiments on two widely-used public benchmarks, i.e.,\nCUB-200-2011 and Stanford Dogs, demonstrate that PHPQ outperforms\nstate-of-the-art methods.</p>\n", "tags": ["CNN","Image Retrieval","Quantisation"] },
{"key": "zeng2023cascading", "year": "2023", "title":"Cascading Hierarchical Networks With Multi-task Balanced Loss For Fine-grained Hashing", "abstract": "<p>With the explosive growth in the number of fine-grained images in the\nInternet era, it has become a challenging problem to perform fast and efficient\nretrieval from large-scale fine-grained images. Among the many retrieval\nmethods, hashing methods are widely used due to their high efficiency and small\nstorage space occupation. Fine-grained hashing is more challenging than\ntraditional hashing problems due to the difficulties such as low inter-class\nvariances and high intra-class variances caused by the characteristics of\nfine-grained images. To improve the retrieval accuracy of fine-grained hashing,\nwe propose a cascaded network to learn compact and highly semantic hash codes,\nand introduce an attention-guided data augmentation method. We refer to this\nnetwork as a cascaded hierarchical data augmentation network. We also propose a\nnovel approach to coordinately balance the loss of multi-task learning. We do\nextensive experiments on some common fine-grained visual classification\ndatasets. The experimental results demonstrate that our proposed method\noutperforms several state-of-art hashing methods and can effectively improve\nthe accuracy of fine-grained retrieval. The source code is publicly available:\nhttps://github.com/kaiba007/FG-CNET.</p>\n", "tags": ["ARXIV","Has Code","Supervised"] },
{"key": "zhan2020weakly", "year": "2020", "title":"Weakly-supervised Online Hashing", "abstract": "<p>With the rapid development of social websites, recent years have witnessed an\nexplosive growth of social images with user-provided tags which continuously\narrive in a streaming fashion. Due to the fast query speed and low storage\ncost, hashing-based methods for image search have attracted increasing\nattention. However, existing hashing methods for social image retrieval are\nbased on batch mode which violates the nature of social images, i.e., social\nimages are usually generated periodically or collected in a stream fashion.\nAlthough there exist many online image hashing methods, they either adopt\nunsupervised learning which ignore the relevant tags, or are designed in the\nsupervised manner which needs high-quality labels. In this paper, to overcome\nthe above limitations, we propose a new method named Weakly-supervised Online\nHashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak\nsupervision by considering the semantics of tags and removing the noise.\nBesides, We develop a discrete online optimization algorithm for WOH, which is\nefficient and scalable. Extensive experiments conducted on two real-world\ndatasets demonstrate the superiority of WOH compared with several\nstate-of-the-art hashing baselines.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised","Weakly Supervised"] },
{"key": "zhan2021learning", "year": "2021", "title":"Learning Discrete Representations Via Constrained Clustering For Effective And Efficient Dense Retrieval", "abstract": "<p>Dense Retrieval (DR) has achieved state-of-the-art first-stage ranking\neffectiveness. However, the efficiency of most existing DR models is limited by\nthe large memory cost of storing dense vectors and the time-consuming nearest\nneighbor search (NNS) in vector space. Therefore, we present RepCONC, a novel\nretrieval model that learns discrete Representations via CONstrained\nClustering. RepCONC jointly trains dual-encoders and the Product Quantization\n(PQ) method to learn discrete document representations and enables fast\napproximate NNS with compact indexes. It models quantization as a constrained\nclustering process, which requires the document embeddings to be uniformly\nclustered around the quantization centroids and supports end-to-end\noptimization of the quantization method and dual-encoders. We theoretically\ndemonstrate the importance of the uniform clustering constraint in RepCONC and\nderive an efficient approximate solution for constrained clustering by reducing\nit to an instance of the optimal transport problem. Besides constrained\nclustering, RepCONC further adopts a vector-based inverted file system (IVF) to\nsupport highly efficient vector search on CPUs. Extensive experiments on two\npopular ad-hoc retrieval benchmarks show that RepCONC achieves better ranking\neffectiveness than competitive vector quantization baselines under different\ncompression ratio settings. It also substantially outperforms a wide range of\nexisting retrieval models in terms of retrieval effectiveness, memory\nefficiency, and time efficiency.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "zhang2010self", "year": "2010", "title":"Self-taught Hashing For Fast Similarity Search", "abstract": "<p>The ability of fast similarity search at large scale is of great importance\nto many Information Retrieval (IR) applications. A promising way to accelerate\nsimilarity search is semantic hashing which designs compact binary codes for a\nlarge number of documents so that semantically similar documents are mapped to\nsimilar codes (within a short Hamming distance). Although some recently\nproposed techniques are able to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents remains to be a\nvery challenging problem. In this paper, we emphasise this issue and propose a\nnovel Self-Taught Hashing (STH) approach to semantic hashing: we first find the\noptimal \\(l\\)-bit binary codes for all documents in the given corpus via\nunsupervised learning, and then train \\(l\\) classifiers via supervised learning\nto predict the \\(l\\)-bit code for any query document unseen before. Our\nexperiments on three real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine\n(SVM) outperforms state-of-the-art techniques significantly.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2015bit", "year": "2015", "title":"Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification", "abstract": "<p>Extracting informative image features and learning effective approximate\nhashing functions are two crucial steps in image retrieval . Conventional\nmethods often study these two steps separately, e.g., learning hash functions\nfrom a predefined hand-crafted feature space. Meanwhile, the bit lengths of\noutput hashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised learning\nframework to generate compact and bit-scalable hashing codes directly from raw\nimages. We pose hashing learning as a problem of regularized similarity\nlearning. Specifically, we organize the training images into a batch of triplet\nsamples, each sample containing two images with the same label and one with a\ndifferent label. With these triplet samples, we maximize the margin between\nmatched pairs and mismatched pairs in the Hamming space. In addition, a\nregularization term is introduced to enforce the adjacency consistency, i.e.,\nimages of similar appearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end fashion, where\ndiscriminative image features and hash functions are simultaneously optimized.\nFurthermore, each bit of our hashing codes is unequally weighted so that we can\nmanipulate the code lengths by truncating the insignificant bits. Our framework\noutperforms state-of-the-arts on public benchmarks of similar image search and\nalso achieves promising results in the application of person re-identification\nin surveillance. It is also shown that the generated bit-scalable hashing codes\nwell preserve the discriminative powers with shorter code lengths.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "zhang2015efficient", "year": "2015", "title":"Efficient Training Of Very Deep Neural Networks For Supervised Hashing", "abstract": "<p>In this paper, we propose training very deep neural networks (DNNs) for\nsupervised learning of hash codes. Existing methods in this context train\nrelatively “shallow” networks limited by the issues arising in back propagation\n(e.e. vanishing gradients) as well as computational efficiency. We propose a\nnovel and efficient training algorithm inspired by alternating direction method\nof multipliers (ADMM) that overcomes some of these limitations. Our method\ndecomposes the training process into independent layer-wise local updates\nthrough auxiliary variables. Empirically we observe that our training algorithm\nalways converges and its computational complexity is linearly proportional to\nthe number of edges in the networks. Empirically we manage to train DNNs with\n64 hidden layers and 1024 nodes per layer for supervised hashing in about 3\nhours using a single GPU. Our proposed very deep supervised hashing (VDSH)\nmethod significantly outperforms the state-of-the-art on several benchmark\ndatasets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2015reflectance", "year": "2015", "title":"Reflectance Hashing For Material Recognition", "abstract": "<p>We introduce a novel method for using reflectance to identify materials.\nReflectance offers a unique signature of the material but is challenging to\nmeasure and use for recognizing materials due to its high-dimensionality. In\nthis work, one-shot reflectance is captured using a unique optical camera\nmeasuring {\\it reflectance disks} where the pixel coordinates correspond to\nsurface viewing angles. The reflectance has class-specific stucture and angular\ngradients computed in this reflectance space reveal the material class.\n  These reflectance disks encode discriminative information for efficient and\naccurate material recognition. We introduce a framework called reflectance\nhashing that models the reflectance disks with dictionary learning and binary\nhashing. We demonstrate the effectiveness of reflectance hashing for material\nrecognition with a number of real-world materials.</p>\n", "tags": ["ARXIV"] },
{"key": "zhang2015unsupervised", "year": "2015", "title":"Unsupervised Feature Learning For Dense Correspondences Across Scenes", "abstract": "<p>We propose a fast, accurate matching method for estimating dense pixel\ncorrespondences across scenes. It is a challenging problem to estimate dense\npixel correspondences between images depicting different scenes or instances of\nthe same object category. While most such matching methods rely on hand-crafted\nfeatures such as SIFT, we learn features from a large amount of unlabeled image\npatches using unsupervised learning. Pixel-layer features are obtained by\nencoding over the dictionary, followed by spatial pooling to obtain patch-layer\nfeatures. The learned features are then seamlessly embedded into a multi-layer\nmatch- ing framework. We experimentally demonstrate that the learned features,\ntogether with our matching model, outperforms state-of-the-art methods such as\nthe SIFT flow, coherency sensitive hashing and the recent deformable spatial\npyramid matching methods both in terms of accuracy and computation efficiency.\nFurthermore, we evaluate the performance of a few different dictionary learning\nand feature encoding methods in the proposed pixel correspondences estimation\nframework, and analyse the impact of dictionary learning and feature encoding\nwith respect to the final matching performance.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "zhang2016query", "year": "2016", "title":"Query-adaptive Image Retrieval By Deep Weighted Hashing", "abstract": "<p>Hashing methods have attracted much attention for large scale image\nretrieval. Some deep hashing methods have achieved promising results by taking\nadvantage of the strong representation power of deep networks recently.\nHowever, existing deep hashing methods treat all hash bits equally. On one\nhand, a large number of images share the same distance to a query image due to\nthe discrete Hamming distance, which raises a critical issue of image retrieval\nwhere fine-grained rankings are very important. On the other hand, different\nhash bits actually contribute to the image retrieval differently, and treating\nthem equally greatly affects the retrieval accuracy of image. To address the\nabove two problems, we propose the query-adaptive deep weighted hashing (QaDWH)\napproach, which can perform fine-grained ranking for different queries by\nweighted Hamming distance. First, a novel deep hashing network is proposed to\nlearn the hash codes and corresponding class-wise weights jointly, so that the\nlearned weights can reflect the importance of different hash bits for different\nimage classes. Second, a query-adaptive image retrieval method is proposed,\nwhich rapidly generates hash bit weights for different query images by fusing\nits semantic probability and the learned class-wise weights. Fine-grained image\nretrieval is then performed by the weighted Hamming distance, which can provide\nmore accurate ranking than the traditional Hamming distance. Experiments on\nfour widely used datasets show that the proposed approach outperforms eight\nstate-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "zhang2016scalable", "year": "2016", "title":"Scalable Discrete Supervised Hash Learning With Asymmetric Matrix Factorization", "abstract": "<p>Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, and it has received a broad attention due to its low storage cost and\nfast retrieval speed. However, the existing limitations make the present\nalgorithms difficult to deal with large-scale datasets: (1) discrete\nconstraints are involved in the learning of the hash function; (2) pairwise or\ntriplet similarity is adopted to generate efficient hashcodes, resulting both\ntime and space complexity are greater than O(n^2). To address these issues, we\npropose a novel discrete supervised hash learning framework which can be\nscalable to large-scale datasets. First, the discrete learning procedure is\ndecomposed into a binary classifier learning scheme and binary codes learning\nscheme, which makes the learning procedure more efficient. Second, we adopt the\nAsymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based\nBatch Coordinate Descent method, such that the time and space complexity is\nreduced to O(n). The proposed framework also provides a flexible paradigm to\nincorporate with arbitrary hash function, including deep neural networks and\nkernel methods. Experiments on large-scale datasets demonstrate that the\nproposed method is superior or comparable with state-of-the-art hashing\nalgorithms.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2016ssdh", "year": "2016", "title":"SSDH Semi-supervised Deep Hashing For Large Scale Image Retrieval", "abstract": "<p>Hashing methods have been widely used for efficient similarity retrieval on\nlarge scale image database. Traditional hashing methods learn hash functions to\ngenerate binary codes from hand-crafted features, which achieve limited\naccuracy since the hand-crafted features cannot optimally represent the image\ncontent and preserve the semantic similarity. Recently, several deep hashing\nmethods have shown better performance because the deep architectures generate\nmore discriminative feature representations. However, these deep hashing\nmethods are mainly designed for supervised scenarios, which only exploit the\nsemantic similarity information, but ignore the underlying data structures. In\nthis paper, we propose the semi-supervised deep hashing (SSDH) approach, to\nperform more effective hash function learning by simultaneously preserving\nsemantic similarity and underlying data structures. The main contributions are\nas follows: (1) We propose a semi-supervised loss to jointly minimize the\nempirical error on labeled data, as well as the embedding error on both labeled\nand unlabeled data, which can preserve the semantic similarity and capture the\nmeaningful neighbors on the underlying data structures for effective hashing.\n(2) A semi-supervised deep hashing network is designed to extensively exploit\nboth labeled and unlabeled data, in which we propose an online graph\nconstruction method to benefit from the evolving deep features during training\nto better capture semantic neighbors. To the best of our knowledge, the\nproposed deep network is the first deep hashing method that can perform hash\ncode learning and feature learning simultaneously in a semi-supervised fashion.\nExperimental results on 5 widely-used datasets show that our proposed approach\noutperforms the state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Graph","Image Retrieval","Supervised"] },
{"key": "zhang2017hashganattention", "year": "2017", "title":"Hashganattention-aware Deep Adversarial Hashing For Cross Modal Retrieval", "abstract": "<p>As the rapid growth of multi-modal data, hashing methods for cross-modal\nretrieval have received considerable attention. Deep-networks-based cross-modal\nhashing methods are appealing as they can integrate feature learning and hash\ncoding into end-to-end trainable frameworks. However, it is still challenging\nto find content similarities between different modalities of data due to the\nheterogeneity gap. To further address this problem, we propose an adversarial\nhashing network with attention mechanism to enhance the measurement of content\nsimilarities by selectively focusing on informative parts of multi-modal data.\nThe proposed new adversarial network, HashGAN, consists of three building\nblocks: 1) the feature learning module to obtain feature representations, 2)\nthe generative attention module to generate an attention mask, which is used to\nobtain the attended (foreground) and the unattended (background) feature\nrepresentations, 3) the discriminative hash coding module to learn hash\nfunctions that preserve the similarities between different modalities. In our\nframework, the generative module and the discriminative module are trained in\nan adversarial way: the generator is learned to make the discriminator cannot\npreserve the similarities of multi-modal data w.r.t. the background feature\nrepresentations, while the discriminator aims to preserve the similarities of\nmulti-modal data w.r.t. both the foreground and the background feature\nrepresentations. Extensive evaluations on several benchmark datasets\ndemonstrate that the proposed HashGAN brings substantial improvements over\nother state-of-the-art cross-modal hashing methods.</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "zhang2017unsupervised", "year": "2017", "title":"Unsupervised Generative Adversarial Cross-modal Hashing", "abstract": "<p>Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Unsupervised cross-modal hashing is more flexible and applicable\nthan supervised methods, since no intensive labeling work is involved. However,\nexisting unsupervised methods learn hashing functions by preserving inter and\nintra correlations, while ignoring the underlying manifold structure across\ndifferent modalities, which is extremely helpful to capture meaningful nearest\nneighbors of different modalities for cross-modal retrieval. To address the\nabove problem, in this paper we propose an Unsupervised Generative Adversarial\nCross-modal Hashing approach (UGACH), which makes full use of GAN’s ability for\nunsupervised representation learning to exploit the underlying manifold\nstructure of cross-modal data. The main contributions can be summarized as\nfollows: (1) We propose a generative adversarial network to model cross-modal\nhashing in an unsupervised fashion. In the proposed UGACH, given a data of one\nmodality, the generative model tries to fit the distribution over the manifold\nstructure, and select informative data of another modality to challenge the\ndiscriminative model. The discriminative model learns to distinguish the\ngenerated data and the true positive data sampled from correlation graph to\nachieve better retrieval accuracy. These two models are trained in an\nadversarial way to improve each other and promote hashing function learning.\n(2) We propose a correlation graph based approach to capture the underlying\nmanifold structure across different modalities, so that data of different\nmodalities but within the same manifold can have smaller Hamming distance and\npromote retrieval accuracy. Extensive experiments compared with 6\nstate-of-the-art methods verify the effectiveness of our proposed approach.</p>\n", "tags": ["ARXIV","Cross Modal","GAN","Graph","Unsupervised"] },
{"key": "zhang2018improved", "year": "2018", "title":"Improved Deep Hashing With Soft Pairwise Similarity For Multi-label Image Retrieval", "abstract": "<p>Hash coding has been widely used in the approximate nearest neighbor search\nfor large-scale image retrieval. Recently, many deep hashing methods have been\nproposed and shown largely improved performance over traditional\nfeature-learning-based methods. Most of these methods examine the pairwise\nsimilarity on the semantic-level labels, where the pairwise similarity is\ngenerally defined in a hard-assignment way. That is, the pairwise similarity is\n‘1’ if they share no less than one class label and ‘0’ if they do not share\nany. However, such similarity definition cannot reflect the similarity ranking\nfor pairwise images that hold multiple labels. In this paper, a new deep\nhashing method is proposed for multi-label image retrieval by re-defining the\npairwise similarity into an instance similarity, where the instance similarity\nis quantified into a percentage based on the normalized semantic labels. Based\non the instance similarity, a weighted cross-entropy loss and a minimum mean\nsquare error loss are tailored for loss-function construction, and are\nefficiently used for simultaneous feature learning and hash coding. Experiments\non three popular datasets demonstrate that, the proposed method outperforms the\ncompeting methods and achieves the state-of-the-art performance in multi-label\nimage retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "zhang2018sch", "year": "2018", "title":"SCH-GAN Semi-supervised Cross-modal Hashing By Generative Adversarial Network", "abstract": "<p>Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Supervised cross-modal hashing methods have achieved considerable\nprogress by incorporating semantic side information. However, they mainly have\ntwo limitations: (1) Heavily rely on large-scale labeled cross-modal training\ndata which are labor intensive and hard to obtain. (2) Ignore the rich\ninformation contained in the large amount of unlabeled data across different\nmodalities, especially the margin examples that are easily to be incorrectly\nretrieved, which can help to model the correlations. To address these problems,\nin this paper we propose a novel Semi-supervised Cross-Modal Hashing approach\nby Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN’s\nability for modeling data distributions to promote cross-modal hashing learning\nin an adversarial way. The main contributions can be summarized as follows: (1)\nWe propose a novel generative adversarial network for cross-modal hashing. In\nour proposed SCH-GAN, the generative model tries to select margin examples of\none modality from unlabeled data when giving a query of another modality. While\nthe discriminative model tries to distinguish the selected examples and true\npositive examples of the query. These two models play a minimax game so that\nthe generative model can promote the hashing performance of discriminative\nmodel. (2) We propose a reinforcement learning based algorithm to drive the\ntraining of proposed SCH-GAN. The generative model takes the correlation score\npredicted by discriminative model as a reward, and tries to select the examples\nclose to the margin to promote discriminative model by maximizing the margin\nbetween positive and negative data. Experiments on 3 widely-used datasets\nverify the effectiveness of our proposed approach.</p>\n", "tags": ["ARXIV","Cross Modal","GAN","Supervised"] },
{"key": "zhang2018semantic", "year": "2018", "title":"Semantic Cluster Unary Loss For Efficient Deep Hashing", "abstract": "<p>Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, which has received a broad attention due to its low storage cost and\nfast retrieval speed. With the rapid development of deep learning, deep hashing\nmethods have achieved promising results in efficient information retrieval.\nMost of the existing deep hashing methods adopt pairwise or triplet losses to\ndeal with similarities underlying the data, but the training is difficult and\nless efficient because \\(O(n^2)\\) data pairs and \\(O(n^3)\\) triplets are involved.\nTo address these issues, we propose a novel deep hashing algorithm with unary\nloss which can be trained very efficiently. We first of all introduce a Unary\nUpper Bound of the traditional triplet loss, thus reducing the complexity to\n\\(O(n)\\) and bridging the classification-based unary loss and the triplet loss.\nSecond, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by\nintroducing a modified Unary Upper Bound loss, named Semantic Cluster Unary\nLoss (SCUL). The resultant hashcodes form several compact clusters, which means\nhashcodes in the same cluster have similar semantic information. We also\ndemonstrate that the proposed SCDH is easy to be extended to semi-supervised\nsettings by incorporating the state-of-the-art semi-supervised learning\nalgorithms. Experiments on large-scale datasets show that the proposed method\nis superior to state-of-the-art hashing algorithms.</p>\n", "tags": ["Deep Learning","ICIP","Supervised"] },
{"key": "zhang2019collaborative", "year": "2019", "title":"Collaborative Quantization For Cross-modal Similarity Search", "abstract": "<p>Cross-modal similarity search is a problem about designing a search system\nsupporting querying across content modalities, e.g., using an image to search\nfor texts or using a text to search for images. This paper presents a compact\ncoding solution for efficient search, with a focus on the quantization approach\nwhich has already shown the superior performance over the hashing solutions in\nthe single-modal similarity search. We propose a cross-modal quantization\napproach, which is among the early attempts to introduce quantization into\ncross-modal search. The major contribution lies in jointly learning the\nquantizers for both modalities through aligning the quantized representations\nfor each pair of image and text belonging to a document. In addition, our\napproach simultaneously learns the common space for both modalities in which\nquantization is conducted to enable efficient and effective search using the\nEuclidean distance computed in the common space with fast distance table\nlookup. Experimental results compared with several competitive algorithms over\nthree benchmark datasets demonstrate that the proposed approach achieves the\nstate-of-the-art performance.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation"] },
{"key": "zhang2019joint", "year": "2019", "title":"Joint Cluster Unary Loss For Efficient Cross-modal Hashing", "abstract": "<p>With the rapid growth of various types of multimodal data, cross-modal deep\nhashing has received broad attention for solving cross-modal retrieval problems\nefficiently. Most cross-modal hashing methods follow the traditional supervised\nhashing framework in which the \\(O(n^2)\\) data pairs and \\(O(n^3)\\) data triplets\nare generated for training, but the training procedure is less efficient\nbecause the complexity is high for large-scale dataset. To address these\nissues, we propose a novel and efficient cross-modal hashing algorithm in which\nthe unary loss is introduced. First of all, We introduce the Cross-Modal Unary\nLoss (CMUL) with \\(O(n)\\) complexity to bridge the traditional triplet loss and\nclassification-based unary loss. A more accurate bound of the triplet loss for\nstructured multilabel data is also proposed in CMUL. Second, we propose the\nnovel Joint Cluster Cross-Modal Hashing (JCCH) algorithm for efficient hash\nlearning, in which the CMUL is involved. The resultant hashcodes form several\nclusters in which the hashcodes in the same cluster share similar semantic\ninformation, and the heterogeneity gap on different modalities is diminished by\nsharing the clusters. The proposed algorithm is able to be applied to various\ntypes of data, and experiments on large-scale datasets show that the proposed\nmethod is superior over or comparable with state-of-the-art cross-modal hashing\nmethods, and training with the proposed method is more efficient than others.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "zhang2019pairwise", "year": "2019", "title":"Pairwise Teacher-student Network For Semi-supervised Hashing", "abstract": "<p>Hashing method maps similar high-dimensional data to binary hashcodes with\nsmaller hamming distance, and it has received broad attention due to its low\nstorage cost and fast retrieval speed. Pairwise similarity is easily obtained\nand widely used for retrieval, and most supervised hashing algorithms are\ncarefully designed for the pairwise supervisions. As labeling all data pairs is\ndifficult, semi-supervised hashing is proposed which aims at learning efficient\ncodes with limited labeled pairs and abundant unlabeled ones. Existing methods\nbuild graphs to capture the structure of dataset, but they are not working well\nfor complex data as the graph is built based on the data representations and\ndetermining the representations of complex data is difficult. In this paper, we\npropose a novel teacher-student semi-supervised hashing framework in which the\nstudent is trained with the pairwise information produced by the teacher\nnetwork. The network follows the smoothness assumption, which achieves\nconsistent distances for similar data pairs so that the retrieval results are\nsimilar for neighborhood queries. Experiments on large-scale datasets show that\nthe proposed method reaches impressive gain over the supervised baselines and\nis superior to state-of-the-art semi-supervised hashing methods.</p>\n", "tags": ["ARXIV","Graph","Supervised"] },
{"key": "zhang2019sadih", "year": "2019", "title":"SADIH Semantic-aware Discrete Hashing", "abstract": "<p>Due to its low storage cost and fast query speed, hashing has been recognized\nto accomplish similarity search in large-scale multimedia retrieval\napplications. Particularly supervised hashing has recently received\nconsiderable research attention by leveraging the label information to preserve\nthe pairwise similarities of data points in the Hamming space. However, there\nstill remain two crucial bottlenecks: 1) the learning process of the full\npairwise similarity preservation is computationally unaffordable and unscalable\nto deal with big data; 2) the available category information of data are not\nwell-explored to learn discriminative hash functions. To overcome these\nchallenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH)\nframework, which aims to directly embed the transformed semantic information\ninto the asymmetric similarity approximation and discriminative hashing\nfunction learning. Specifically, a semantic-aware latent embedding is\nintroduced to asymmetrically preserve the full pairwise similarities while\nskillfully handle the cumbersome n times n pairwise similarity matrix.\nMeanwhile, a semantic-aware autoencoder is developed to jointly preserve the\ndata structures in the discriminative latent semantic space and perform data\nreconstruction. Moreover, an efficient alternating optimization algorithm is\nproposed to solve the resulting discrete optimization problem. Extensive\nexperimental results on multiple large-scale datasets demonstrate that our\nSADIH can clearly outperform the state-of-the-art baselines with the additional\nbenefit of lower computational costs.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2019search", "year": "2019", "title":"Search Efficient Binary Network Embedding", "abstract": "<p>Traditional network embedding primarily focuses on learning a continuous\nvector representation for each node, preserving network structure and/or node\ncontent information, such that off-the-shelf machine learning algorithms can be\neasily applied to the vector-format node representations for network analysis.\nHowever, the learned continuous vector representations are inefficient for\nlarge-scale similarity search, which often involves finding nearest neighbors\nmeasured by distance or similarity in a continuous vector space. In this paper,\nwe propose a search efficient binary network embedding algorithm called\nBinaryNE to learn a binary code for each node, by simultaneously modeling node\ncontext relations and node attribute relations through a three-layer neural\nnetwork. BinaryNE learns binary node representations through a stochastic\ngradient descent based online learning algorithm. The learned binary encoding\nnot only reduces memory usage to represent each node, but also allows fast\nbit-wise comparisons to support faster node similarity search than using\nEuclidean distance or other distance measures. Extensive experiments and\ncomparisons demonstrate that BinaryNE not only delivers more than 25 times\nfaster search speed, but also provides comparable or better search quality than\ntraditional continuous vector based network embedding methods. The binary codes\nlearned by BinaryNE also render competitive performance on node classification\nand node clustering tasks. The source code of this paper is available at\nhttps://github.com/daokunzhang/BinaryNE.</p>\n", "tags": ["Has Code","Supervised"] },
{"key": "zhang2019semantic", "year": "2019", "title":"Semantic Hierarchy Preserving Deep Hashing For Large-scale Image Retrieval", "abstract": "<p>Deep hashing models have been proposed as an efficient method for large-scale\nsimilarity search. However, most existing deep hashing methods only utilize\nfine-level labels for training while ignoring the natural semantic hierarchy\nstructure. This paper presents an effective method that preserves the classwise\nsimilarity of full-level semantic hierarchy for large-scale image retrieval.\nExperiments on two benchmark datasets show that our method helps improve the\nfine-level retrieval performance. Moreover, with the help of the semantic\nhierarchy, it can produce significantly better binary codes for hierarchical\nretrieval, which indicates its potential of providing more user-desired\nretrieval results.</p>\n", "tags": ["ARXIV","Image Retrieval"] },
{"key": "zhang2020collaborative", "year": "2020", "title":"Collaborative Generative Hashing For Marketing And Fast Cold-start Recommendation", "abstract": "<p>Cold-start has being a critical issue in recommender systems with the\nexplosion of data in e-commerce. Most existing studies proposed to alleviate\nthe cold-start problem are also known as hybrid recommender systems that learn\nrepresentations of users and items by combining user-item interactive and\nuser/item content information. However, previous hybrid methods regularly\nsuffered poor efficiency bottlenecking in online recommendations with\nlarge-scale items, because they were designed to project users and items into\ncontinuous latent space where the online recommendation is expensive. To this\nend, we propose a collaborative generated hashing (CGH) framework to improve\nthe efficiency by denoting users and items as binary codes, then fast hashing\nsearch techniques can be used to speed up the online recommendation. In\naddition, the proposed CGH can generate potential users or items for marketing\napplication where the generative network is designed with the principle of\nMinimum Description Length (MDL), which is used to learn compact and\ninformative binary codes. Extensive experiments on two public datasets show the\nadvantages for recommendations in various settings over competing baselines and\nanalyze its feasibility in marketing application.</p>\n", "tags": ["ARXIV"] },
{"key": "zhang2020deep", "year": "2020", "title":"Deep Pairwise Hashing For Cold-start Recommendation", "abstract": "<p>Recommendation efficiency and data sparsity problems have been regarded as\ntwo challenges of improving performance for online recommendation. Most of the\nprevious related work focus on improving recommendation accuracy instead of\nefficiency. In this paper, we propose a Deep Pairwise Hashing (DPH) to map\nusers and items to binary vectors in Hamming space, where a user’s preference\nfor an item can be efficiently calculated by Hamming distance, which\nsignificantly improves the efficiency of online recommendation. To alleviate\ndata sparsity and cold-start problems, the user-item interactive information\nand item content information are unified to learn effective representations of\nitems and users. Specifically, we first pre-train robust item representation\nfrom item content data by a Denoising Auto-encoder instead of other\ndeterministic deep learning frameworks; then we finetune the entire framework\nby adding a pairwise loss objective with discrete constraints; moreover, DPH\naims to minimize a pairwise ranking loss that is consistent with the ultimate\ngoal of recommendation. Finally, we adopt the alternating optimization method\nto optimize the proposed model with discrete constraints. Extensive experiments\non three different datasets show that DPH can significantly advance the\nstate-of-the-art frameworks regarding data sparsity and item cold-start\nrecommendation.</p>\n", "tags": ["Deep Learning"] },
{"key": "zhang2020faster", "year": "2020", "title":"Faster Binary Embeddings For Preserving Euclidean Distances", "abstract": "<p>We propose a fast, distance-preserving, binary embedding algorithm to\ntransform a high-dimensional dataset \\(\\mathcal{T}\\subseteq\\mathbb{R}^n\\) into\nbinary sequences in the cube \\(\\{\\pm 1\\}^m\\). When \\(\\mathcal{T}\\) consists of\nwell-spread (i.e., non-sparse) vectors, our embedding method applies a stable\nnoise-shaping quantization scheme to \\(A x\\) where \\(A\\in\\mathbb{R}^{m\\times n}\\)\nis a sparse Gaussian random matrix. This contrasts with most binary embedding\nmethods, which usually use \\(x\\mapsto \\mathrm{sign}(Ax)\\) for the embedding.\nMoreover, we show that Euclidean distances among the elements of \\(\\mathcal{T}\\)\nare approximated by the \\(\\ell_1\\) norm on the images of \\(\\{\\pm 1\\}^m\\) under a\nfast linear transformation. This again contrasts with standard methods, where\nthe Hamming distance is used instead. Our method is both fast and memory\nefficient, with time complexity \\(O(m)\\) and space complexity \\(O(m)\\). Further, we\nprove that the method is accurate and its associated error is comparable to\nthat of a continuous valued Johnson-Lindenstrauss embedding plus a quantization\nerror that admits a polynomial decay as the embedding dimension \\(m\\) increases.\nThus the length of the binary codes required to achieve a desired accuracy is\nquite small, and we show it can even be compressed further without compromising\nthe accuracy. To illustrate our results, we test the proposed method on natural\nimages and show that it achieves strong performance.</p>\n", "tags": ["ARXIV","Independent","Quantisation"] },
{"key": "zhang2020fedocr", "year": "2020", "title":"Fedocr Communication-efficient Federated Learning For Scene Text Recognition", "abstract": "<p>While scene text recognition techniques have been widely used in commercial\napplications, data privacy has rarely been taken into account by this research\ncommunity. Most existing algorithms have assumed a set of shared or centralized\ntraining data. However, in practice, data may be distributed on different local\ndevices that can not be centralized to share due to the privacy restrictions.\nIn this paper, we study how to make use of decentralized datasets for training\na robust scene text recognizer while keeping them stay on local devices. To the\nbest of our knowledge, we propose the first framework leveraging federated\nlearning for scene text recognition, which is trained with decentralized\ndatasets collaboratively. Hence we name it FedOCR. To make FedCOR fairly\nsuitable to be deployed on end devices, we make two improvements including\nusing lightweight models and hashing techniques. We argue that both are crucial\nfor FedOCR in terms of the communication efficiency of federated learning. The\nsimulations on decentralized datasets show that the proposed FedOCR achieves\ncompetitive results to the models that are trained with centralized data, with\nfewer communication costs and higher-level privacy-preserving.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2020model", "year": "2020", "title":"Model Size Reduction Using Frequency Based Double Hashing For Recommender Systems", "abstract": "<p>Deep Neural Networks (DNNs) with sparse input features have been widely used\nin recommender systems in industry. These models have large memory requirements\nand need a huge amount of training data. The large model size usually entails a\ncost, in the range of millions of dollars, for storage and communication with\nthe inference services. In this paper, we propose a hybrid hashing method to\ncombine frequency hashing and double hashing techniques for model size\nreduction, without compromising performance. We evaluate the proposed models on\ntwo product surfaces. In both cases, experiment results demonstrated that we\ncan reduce the model size by around 90 % while keeping the performance on par\nwith the original baselines.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2020survey", "year": "2020", "title":"A Survey On Deep Hashing For Image Retrieval", "abstract": "<p>Hashing has been widely used in approximate nearest search for large-scale\ndatabase retrieval for its computation and storage efficiency. Deep hashing,\nwhich devises convolutional neural network architecture to exploit and extract\nthe semantic information or feature of images, has received increasing\nattention recently. In this survey, several deep supervised hashing methods for\nimage retrieval are evaluated and I conclude three main different directions\nfor deep supervised hashing methods. Several comments are made at the end.\nMoreover, to break through the bottleneck of the existing hashing methods, I\npropose a Shadow Recurrent Hashing(SRH) method as a try. Specifically, I devise\na CNN architecture to extract the semantic features of images and design a loss\nfunction to encourage similar images projected close. To this end, I propose a\nconcept: shadow of the CNN output. During optimization process, the CNN output\nand its shadow are guiding each other so as to achieve the optimal solution as\nmuch as possible. Several experiments on dataset CIFAR-10 show the satisfying\nperformance of SRH.</p>\n", "tags": ["ARXIV","CNN","Image Retrieval","Supervised","Survey Paper"] },
{"key": "zhang2021bytesteady", "year": "2021", "title":"Bytesteady Fast Classification Using Byte-level N-gram Embeddings", "abstract": "<p>This article introduces byteSteady – a fast model for classification using\nbyte-level n-gram embeddings. byteSteady assumes that each input comes as a\nsequence of bytes. A representation vector is produced using the averaged\nembedding vectors of byte-level n-grams, with a pre-defined set of n. The\nhashing trick is used to reduce the number of embedding vectors. This input\nrepresentation vector is then fed into a linear classifier. A straightforward\napplication of byteSteady is text classification. We also apply byteSteady to\none type of non-language data – DNA sequences for gene classification. For\nboth problems we achieved competitive classification results against strong\nbaselines, suggesting that byteSteady can be applied to both language and\nnon-language data. Furthermore, we find that simple compression using Huffman\ncoding does not significantly impact the results, which offers an\naccuracy-speed trade-off previously unexplored in machine learning.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2021improved", "year": "2021", "title":"Improved Deep Classwise Hashing With Centers Similarity Learning For Image Retrieval", "abstract": "<p>Deep supervised hashing for image retrieval has attracted researchers’\nattention due to its high efficiency and superior retrieval performance. Most\nexisting deep supervised hashing works, which are based on pairwise/triplet\nlabels, suffer from the expensive computational cost and insufficient\nutilization of the semantics information. Recently, deep classwise hashing\nintroduced a classwise loss supervised by class labels information\nalternatively; however, we find it still has its drawback. In this paper, we\npropose an improved deep classwise hashing, which enables hashing learning and\nclass centers learning simultaneously. Specifically, we design a two-step\nstrategy on center similarity learning. It interacts with the classwise loss to\nattract the class center to concentrate on the intra-class samples while\npushing other class centers as far as possible. The centers similarity learning\ncontributes to generating more compact and discriminative hashing codes. We\nconduct experiments on three benchmark datasets. It shows that the proposed\nmethod effectively surpasses the original method and outperforms\nstate-of-the-art baselines under various commonly-used evaluation metrics for\nimage retrieval.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "zhang2021instance", "year": "2021", "title":"Instance-weighted Central Similarity For Multi-label Image Retrieval", "abstract": "<p>Deep hashing has been widely applied to large-scale image retrieval by\nencoding high-dimensional data points into binary codes for efficient\nretrieval. Compared with pairwise/triplet similarity based hash learning,\ncentral similarity based hashing can more efficiently capture the global data\ndistribution. For multi-label image retrieval, however, previous methods only\nuse multiple hash centers with equal weights to generate one centroid as the\nlearning target, which ignores the relationship between the weights of hash\ncenters and the proportion of instance regions in the image. To address the\nabove issue, we propose a two-step alternative optimization approach,\nInstance-weighted Central Similarity (ICS), to automatically learn the center\nweight corresponding to a hash code. Firstly, we apply the maximum entropy\nregularizer to prevent one hash center from dominating the loss function, and\ncompute the center weights via projection gradient descent. Secondly, we update\nneural network parameters by standard back-propagation with fixed center\nweights. More importantly, the learned center weights can well reflect the\nproportion of foreground instances in the image. Our method achieves the\nstate-of-the-art performance on the image retrieval benchmarks, and especially\nimproves the mAP by 1.6%-6.4% on the MS COCO dataset.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "zhang2021joint", "year": "2021", "title":"Joint Learning Of Deep Retrieval Model And Product Quantization Based Embedding Index", "abstract": "<p>Embedding index that enables fast approximate nearest neighbor(ANN) search,\nserves as an indispensable component for state-of-the-art deep retrieval\nsystems. Traditional approaches, often separating the two steps of embedding\nlearning and index building, incur additional indexing time and decayed\nretrieval accuracy. In this paper, we propose a novel method called Poeem,\nwhich stands for product quantization based embedding index jointly trained\nwith deep retrieval model, to unify the two separate steps within an end-to-end\ntraining, by utilizing a few techniques including the gradient straight-through\nestimator, warm start strategy, optimal space decomposition and Givens\nrotation. Extensive experimental results show that the proposed method not only\nimproves retrieval accuracy significantly but also reduces the indexing time to\nalmost none. We have open sourced our approach for the sake of comparison and\nreproducibility.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "zhang2021moon", "year": "2021", "title":"MOON Multi-hash Codes Joint Learning For Cross-media Retrieval", "abstract": "<p>In recent years, cross-media hashing technique has attracted increasing\nattention for its high computation efficiency and low storage cost. However,\nthe existing approaches still have some limitations, which need to be explored.\n1) A fixed hash length (e.g., 16bits or 32bits) is predefined before learning\nthe binary codes. Therefore, these models need to be retrained when the hash\nlength changes, that consumes additional computation power, reducing the\nscalability in practical applications. 2) Existing cross-modal approaches only\nexplore the information in the original multimedia data to perform the hash\nlearning, without exploiting the semantic information contained in the learned\nhash codes. To this end, we develop a novel Multiple hash cOdes jOint learNing\nmethod (MOON) for cross-media retrieval. Specifically, the developed MOON\nsynchronously learns the hash codes with multiple lengths in a unified\nframework. Besides, to enhance the underlying discrimination, we combine the\nclues from the multimodal data, semantic labels and learned hash codes for hash\nlearning. As far as we know, the proposed MOON is the first work to\nsimultaneously learn different length hash codes without retraining in\ncross-media retrieval. Experiments on several databases show that our MOON can\nachieve promising performance, outperforming some recent competitive shallow\nand deep methods.</p>\n", "tags": ["ARXIV","Cross Modal","Independent"] },
{"key": "zhang2021orthonormal", "year": "2021", "title":"Orthonormal Product Quantization Network For Scalable Face Image Retrieval", "abstract": "<p>Existing deep quantization methods provided an efficient solution for\nlarge-scale image retrieval. However, the significant intra-class variations\nlike pose, illumination, and expressions in face images, still pose a challenge\nfor face image retrieval. In light of this, face image retrieval requires\nsufficiently powerful learning metrics, which are absent in current deep\nquantization works. Moreover, to tackle the growing unseen identities in the\nquery stage, face image retrieval drives more demands regarding model\ngeneralization and system scalability than general image retrieval tasks. This\npaper integrates product quantization with orthonormal constraints into an\nend-to-end deep learning framework to effectively retrieve face images.\nSpecifically, a novel scheme that uses predefined orthonormal vectors as\ncodewords is proposed to enhance the quantization informativeness and reduce\ncodewords’ redundancy. A tailored loss function maximizes discriminability\namong identities in each quantization subspace for both the quantized and\noriginal features. An entropy-based regularization term is imposed to reduce\nthe quantization error. Experiments are conducted on four commonly-used face\ndatasets under both seen and unseen identities retrieval settings. Our method\noutperforms all the compared deep hashing/quantization state-of-the-arts under\nboth settings. Results validate the effectiveness of the proposed orthonormal\ncodewords in improving models’ standard retrieval performance and\ngeneralization ability. Combing with further experiments on two general image\ndatasets, it demonstrates the broad superiority of our method for scalable\nimage retrieval.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Quantisation"] },
{"key": "zhang2022hashing", "year": "2022", "title":"Hashing Learning With Hyper-class Representation", "abstract": "<p>Existing unsupervised hash learning is a kind of attribute-centered\ncalculation. It may not accurately preserve the similarity between data. This\nleads to low down the performance of hash function learning. In this paper, a\nhash algorithm is proposed with a hyper-class representation. It is a two-steps\napproach. The first step finds potential decision features and establish\nhyper-class. The second step constructs hash learning based on the hyper-class\ninformation in the first step, so that the hash codes of the data within the\nhyper-class are as similar as possible, as well as the hash codes of the data\nbetween the hyper-classes are as different as possible. To evaluate the\nefficiency, a series of experiments are conducted on four public datasets. The\nexperimental results show that the proposed hash algorithm is more efficient\nthan the compared algorithms, in terms of mean average precision (MAP), average\nprecision (AP) and Hamming radius 2 (HAM2)</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "zhang2022supervised", "year": "2022", "title":"Supervised Deep Hashing For High-dimensional And Heterogeneous Case-based Reasoning", "abstract": "<p>Case-based Reasoning (CBR) on high-dimensional and heterogeneous data is a\ntrending yet challenging and computationally expensive task in the real world.\nA promising approach is to obtain low-dimensional hash codes representing cases\nand perform a similarity retrieval of cases in Hamming space. However, previous\nmethods based on data-independent hashing rely on random projections or manual\nconstruction, inapplicable to address specific data issues (e.g.,\nhigh-dimensionality and heterogeneity) due to their insensitivity to data\ncharacteristics. To address these issues, this work introduces a novel deep\nhashing network to learn similarity-preserving compact hash codes for efficient\ncase retrieval and proposes a deep-hashing-enabled CBR model HeCBR.\nSpecifically, we introduce position embedding to represent heterogeneous\nfeatures and utilize a multilinear interaction layer to obtain case embeddings,\nwhich effectively filtrates zero-valued features to tackle high-dimensionality\nand sparsity and captures inter-feature couplings. Then, we feed the case\nembeddings into fully-connected layers, and subsequently a hash layer generates\nhash codes with a quantization regularizer to control the quantization loss\nduring relaxation. To cater to incremental learning of CBR, we further propose\nan adaptive learning strategy to update the hash function. Extensive\nexperiments on public datasets show that HeCBR greatly reduces storage and\nsignificantly accelerates case retrieval. HeCBR achieves desirable performance\ncompared with the state-of-the-art CBR methods and performs significantly\nbetter than hashing-based CBR methods in classification.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "zhang2023model", "year": "2023", "title":"Model-enhanced Vector Index", "abstract": "<p>Embedding-based retrieval methods construct vector indices to search for\ndocument representations that are most similar to the query representations.\nThey are widely used in document retrieval due to low latency and decent recall\nperformance. Recent research indicates that deep retrieval solutions offer\nbetter model quality, but are hindered by unacceptable serving latency and the\ninability to support document updates. In this paper, we aim to enhance the\nvector index with end-to-end deep generative models, leveraging the\ndifferentiable advantages of deep retrieval models while maintaining desirable\nserving efficiency. We propose Model-enhanced Vector Index (MEVI), a\ndifferentiable model-enhanced index empowered by a twin-tower representation\nmodel. MEVI leverages a Residual Quantization (RQ) codebook to bridge the\nsequence-to-sequence deep retrieval and embedding-based models. To\nsubstantially reduce the inference time, instead of decoding the unique\ndocument ids in long sequential steps, we first generate some semantic virtual\ncluster ids of candidate documents in a small number of steps, and then\nleverage the well-adapted embedding vectors to further perform a fine-grained\nsearch for the relevant documents in the candidate virtual clusters. We\nempirically show that our model achieves better performance on the commonly\nused academic benchmarks MSMARCO Passage and Natural Questions, with comparable\nserving latency to dense retrieval solutions.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "zhang2024binary", "year": "2024", "title":"Binary Code Ranking With Weighted Hamming Distance", "abstract": "<p>Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most\nexisting binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or\nsimilarity of two points are approximated by the Hamming\ndistance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are\noften lots of results sharing the same Hamming distance to\na query, which makes this distance measure ambiguous and\nposes a critical issue for similarity search where ranking is\nimportant. In this paper, we propose a weighted Hamming\ndistance ranking algorithm (WhRank) to rank the binary\ncodes of hashing methods. By assigning different bit-level\nweights to different hash bits, the returned binary codes\nare ranked at a finer-grained binary code level. We give\nan algorithm to learn the data-adaptive and query-sensitive\nweight for each hash bit. Evaluations on two large-scale\nimage data sets demonstrate the efficacy of our weighted\nHamming distance for binary code ranking.</p>\n", "tags": ["ARXIV"] },
{"key": "zhang2024bit", "year": "2024", "title":"Bit-scalable Deep Hashing With Regularized Similarity Learning For Image Retrieval And Person Re-identification", "abstract": "<p>Extracting informative image features and learning\neffective approximate hashing functions are two crucial steps in\nimage retrieval . Conventional methods often study these two\nsteps separately, e.g., learning hash functions from a predefined\nhand-crafted feature space. Meanwhile, the bit lengths of output\nhashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised\nlearning framework to generate compact and bit-scalable hashing\ncodes directly from raw images. We pose hashing learning as\na problem of regularized similarity learning. Specifically, we\norganize the training images into a batch of triplet samples,\neach sample containing two images with the same label and one\nwith a different label. With these triplet samples, we maximize\nthe margin between matched pairs and mismatched pairs in the\nHamming space. In addition, a regularization term is introduced\nto enforce the adjacency consistency, i.e., images of similar\nappearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end\nfashion, where discriminative image features and hash functions\nare simultaneously optimized. Furthermore, each bit of our\nhashing codes is unequally weighted so that we can manipulate\nthe code lengths by truncating the insignificant bits. Our\nframework outperforms state-of-the-arts on public benchmarks\nof similar image search and also achieves promising results in\nthe application of person re-identification in surveillance. It is\nalso shown that the generated bit-scalable hashing codes well\npreserve the discriminative powers with shorter code lengths.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "zhang2024composite", "year": "2024", "title":"Composite Hashing With Multiple Information Sources", "abstract": "<p>Similarity search applications with a large amount of text\nand image data demands an efficient and effective solution.\nOne useful strategy is to represent the examples in databases\nas compact binary codes through semantic hashing, which\nhas attracted much attention due to its fast query/search\nspeed and drastically reduced storage requirement. All of\nthe current semantic hashing methods only deal with the\ncase when each example is represented by one type of features.\nHowever, examples are often described from several\ndifferent information sources in many real world applications.\nFor example, the characteristics of a webpage can be\nderived from both its content part and its associated links.\nTo address the problem of learning good hashing codes in\nthis scenario, we propose a novel research problem – Composite\nHashing with Multiple Information Sources (CHMIS).\nThe focus of the new research problem is to design an algorithm\nfor incorporating the features from different information\nsources into the binary hashing codes efficiently and\neffectively. In particular, we propose an algorithm CHMISAW\n(CHMIS with Adjusted Weights) for learning the codes.\nThe proposed algorithm integrates information from several\ndifferent sources into the binary hashing codes by adjusting\nthe weights on each individual source for maximizing\nthe coding performance, and enables fast conversion from\nquery examples to their binary hashing codes. Experimental\nresults on five different datasets demonstrate the superior\nperformance of the proposed method against several other\nstate-of-the-art semantic hashing techniques.</p>\n", "tags": ["ARXIV"] },
{"key": "zhang2024deep", "year": "2024", "title":"Deep Center-based Dual-constrained Hashing For Discriminative Face Image Retrieval", "abstract": "<p>With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "zhang2024demo", "year": "2024", "title":"DEMO A Statistical Perspective For Efficient Image-text Matching", "abstract": "<p>Image-text matching has been a long-standing problem, which seeks to connect\nvision and language through semantic understanding. Due to the capability to\nmanage large-scale raw data, unsupervised hashing-based approaches have gained\nprominence recently. They typically construct a semantic similarity structure\nusing the natural distance, which subsequently provides guidance to the model\noptimization process. However, the similarity structure could be biased at the\nboundaries of semantic distributions, causing error accumulation during\nsequential optimization. To tackle this, we introduce a novel hashing approach\ntermed Distribution-based Structure Mining with Consistency Learning (DEMO) for\nefficient image-text matching. From a statistical view, DEMO characterizes each\nimage using multiple augmented views, which are considered as samples drawn\nfrom its intrinsic semantic distribution. Then, we employ a non-parametric\ndistribution divergence to ensure a robust and precise similarity structure. In\naddition, we introduce collaborative consistency learning which not only\npreserves the similarity structure in the Hamming space but also encourages\nconsistency between retrieval distribution from different directions in a\nself-supervised manner. Through extensive experiments on three benchmark\nimage-text matching datasets, we demonstrate that DEMO achieves superior\nperformance compared with many state-of-the-art methods.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "zhang2024efficient", "year": "2024", "title":"Efficient Training Of Very Deep Neural Networks For Supervised Hashing", "abstract": "<p>In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively “shallow” networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2024enhanced", "year": "2024", "title":"An Enhanced Batch Query Architecture In Real-time Recommendation", "abstract": "<p>In industrial recommendation systems on websites and apps, it is essential to\nrecall and predict top-n results relevant to user interests from a content pool\nof billions within milliseconds. To cope with continuous data growth and\nimprove real-time recommendation performance, we have designed and implemented\na high-performance batch query architecture for real-time recommendation\nsystems. Our contributions include optimizing hash structures with a\ncacheline-aware probing method to enhance coalesced hashing, as well as the\nimplementation of a hybrid storage key-value service built upon it. Our\nexperiments indicate this approach significantly surpasses conventional hash\ntables in batch query throughput, achieving up to 90% of the query throughput\nof random memory access when incorporating parallel optimization. The support\nfor NVMe, integrating two-tier storage for hot and cold data, notably reduces\nresource consumption. Additionally, the system facilitates dynamic updates,\nautomated sharding of attributes and feature embedding tables, and introduces\ninnovative protocols for consistency in batch queries, thereby enhancing the\neffectiveness of real-time incremental learning updates. This architecture has\nbeen deployed and in use in the bilibili recommendation system for over a year,\na video content community with hundreds of millions of users, supporting 10x\nincrease in model computation with minimal resource growth, improving outcomes\nwhile preserving the system’s real-time performance.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "zhang2024fast", "year": "2024", "title":"Fast Discrete Cross-modal Hashing Based On Label Relaxation And Matrix Factorization", "abstract": "<p>In recent years, cross-media retrieval has drawn considerable attention due to the exponential growth of multimedia data. Many hashing approaches have been proposed for the cross-media search task. However, there are still open problems that warrant investigation. For example, most existing supervised hashing approaches employ a binary label matrix, which achieves small margins between wrong labels (0) and true labels (1). This may affect the retrieval performance by generating many false negatives and false positives. In addition, some methods adopt a relaxation scheme to solve the binary constraints, which may cause large quantization errors. There are also some discrete hashing methods that have been presented, but most of them are time-consuming. To conquer these problems, we present a label relaxation and discrete matrix factorization method (LRMF) for cross-modal retrieval. It offers a number of innovations. First of all, the proposed approach employs a novel label relaxation scheme to control the margins adaptively, which has the benefit of reducing the quantization error. Second, by virtue of the proposed discrete matrix factorization method designed to learn the binary codes, large quantization errors caused by relaxation can be avoided. The experimental results obtained on two widely-used databases demonstrate that LRMF outperforms state-of-the-art cross-media methods.</p>\n", "tags": ["ARXIV","Cross Modal","Quantisation","Supervised"] },
{"key": "zhang2024gaussianimage", "year": "2024", "title":"Gaussianimage 1000 FPS Image Representation And Compression By 2D Gaussian Splatting", "abstract": "<p>Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3\\(\\times\\) lower GPU memory usage and 5\\(\\times\\) faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 2000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding. Code is available at\nhttps://github.com/Xinjie-Q/GaussianImage.</p>\n", "tags": ["ARXIV","Has Code","Quantisation"] },
{"key": "zhang2024hierarchical", "year": "2024", "title":"Hierarchical Deep Hashing For Fast Large Scale Image Retrieval", "abstract": "<p>Fast image retrieval is of great importance in many computer vision tasks and especially practical applications. Deep hashing, the state-of-the-art fast image retrieval scheme, introduces deep learning to learn the hash functions and generate binary hash codes, and outperforms the other image retrieval methods in terms of accuracy. However, all the existing deep hashing methods could only generate one level hash codes and require a linear traversal of all the hash codes to figure out the closest one when a new query arrives, which is very time-consuming and even intractable for large scale applications. In this work, we propose a Hierarchical Deep Hashing(HDHash) scheme to speed up the state-of-the-art deep hashing methods. More specifically, hierarchical deep hash codes of multiple levels can be generated and indexed with tree structures rather than linear ones, and pruning irrelevant branches can sharply decrease the retrieval time. To our best knowledge, this is the first work to introduce hierarchical indexed deep hashing for fast large scale image retrieval. Extensive experimental results on three benchmark datasets demonstrate that the proposed HDHash scheme achieves better or comparable accuracy with significantly improved efficiency and reduced memory as compared to state-of- the-art fast image retrieval schemes.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Independent"] },
{"key": "zhang2024high", "year": "2024", "title":"High-order Nonlocal Hashing For Unsupervised Cross-modal Retrieval", "abstract": "<p>In light of the ability to enable efficient storage and fast query for big data, hashing techniques for cross-modal search have aroused extensive attention. Despite the great success achieved, unsupervised cross-modal hashing still suffers from lacking reliable similarity supervision and struggles with handling the heterogeneity issue between different modalities. To cope with these, in this paper, we devise a new deep hashing model, termed as High-order Nonlocal Hashing (HNH) to facilitate cross-modal retrieval with the following advantages. First, different from existing methods that mainly leverage low-level local-view similarity as the guidance for hashing learning, we propose a high-order affinity measure that considers the multi-modal neighbourhood structures from a nonlocal perspective, thereby comprehensively capturing the similarity relationships between data items. Second, a common representation is introduced to correlate different modalities. By enforcing the modal-specific descriptors and the common representation to be aligned with each other, the proposed HNH significantly bridges the modality gap and maintains the intra-consistency. Third, an effective affinity preserving objective function is delicately designed to generate high-quality binary codes. Extensive experiments evidence the superiority of the proposed HNH in unsupervised cross-modal retrieval tasks over the state-of-the-art baselines.</p>\n", "tags": ["ARXIV","Cross Modal","Unsupervised"] },
{"key": "zhang2024large", "year": "2024", "title":"Large-scale Supervised Multimodal Hashing With Semantic Correlation Maximization", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for similarity search in multimedia\ndata. In particular, more and more attentions\nhave been payed to multimodal hashing for search in\nmultimedia data with multiple modalities, such as images\nwith tags. Typically, supervised information of semantic\nlabels is also available for the data points in\nmany real applications. Hence, many supervised multimodal\nhashing (SMH) methods have been proposed\nto utilize such semantic labels to further improve the\nsearch accuracy. However, the training time complexity\nof most existing SMH methods is too high, which\nmakes them unscalable to large-scale datasets. In this\npaper, a novel SMH method, called semantic correlation\nmaximization (SCM), is proposed to seamlessly integrate\nsemantic labels into the hashing learning procedure\nfor large-scale data modeling. Experimental results\non two real-world datasets show that SCM can signifi-\ncantly outperform the state-of-the-art SMH methods, in\nterms of both accuracy and scalability.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "zhang2024self", "year": "2024", "title":"Self-taught Hashing For Fast Similarity Search", "abstract": "<p>The ability of fast similarity search at large scale is of great\nimportance to many Information Retrieval (IR) applications.\nA promising way to accelerate similarity search is semantic\nhashing which designs compact binary codes for a large number\nof documents so that semantically similar documents\nare mapped to similar codes (within a short Hamming distance).\nAlthough some recently proposed techniques are\nable to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents\nremains to be a very challenging problem. In this\npaper, we emphasise this issue and propose a novel SelfTaught\nHashing (STH) approach to semantic hashing: we\nfirst find the optimal l-bit binary codes for all documents in\nthe given corpus via unsupervised learning, and then train\nl classifiers via supervised learning to predict the l-bit code\nfor any query document unseen before. Our experiments on\nthree real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and\nlinear Support Vector Machine (SVM) outperforms stateof-the-art\ntechniques significantly.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhang2024supervised", "year": "2024", "title":"Supervised Hashing With Latent Factor Models", "abstract": "<p>Due to its low storage cost and fast query speed, hashing\nhas been widely adopted for approximate nearest neighbor\nsearch in large-scale datasets. Traditional hashing methods\ntry to learn the hash codes in an unsupervised way where\nthe metric (Euclidean) structure of the training data is preserved.\nVery recently, supervised hashing methods, which\ntry to preserve the semantic structure constructed from the\nsemantic labels of the training points, have exhibited higher\naccuracy than unsupervised methods. In this paper, we\npropose a novel supervised hashing method, called latent\nfactor hashing (LFH), to learn similarity-preserving binary\ncodes based on latent factor models. An algorithm with\nconvergence guarantee is proposed to learn the parameters\nof LFH. Furthermore, a linear-time variant with stochastic\nlearning is proposed for training LFH on large-scale datasets.\nExperimental results on two large datasets with semantic\nlabels show that LFH can achieve superior accuracy than\nstate-of-the-art methods with comparable training time.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhao2015deep", "year": "2015", "title":"Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval", "abstract": "<p>With the rapid growth of web images, hashing has received increasing\ninterests in large scale image retrieval. Research efforts have been devoted to\nlearning compact binary codes that preserve semantic similarity based on\nlabels. However, most of these hashing methods are designed to handle simple\nbinary similarity. The complex multilevel semantic structure of images\nassociated with multiple labels have not yet been well explored. Here we\npropose a deep semantic ranking based method for learning hash functions that\npreserve multilevel semantic similarity between multi-label images. In our\napproach, deep convolutional neural network is incorporated into hash functions\nto jointly learn feature representations and mappings from them to hash codes,\nwhich avoids the limitation of semantic representation power of hand-crafted\nfeatures. Meanwhile, a ranking list that encodes the multilevel similarity\ninformation is employed to guide the learning of such deep hash functions. An\neffective scheme based on surrogate loss is used to solve the intractable\noptimization problem of nonsmooth and multivariate ranking measures involved in\nthe learning procedure. Experimental results show the superiority of our\nproposed approach over several state-of-the-art hashing methods in term of\nranking evaluation metrics when tested on multi-label image datasets.</p>\n", "tags": ["ARXIV","Image Retrieval","Supervised"] },
{"key": "zhao2017scalable", "year": "2017", "title":"Scalable Nearest Neighbor Search Based On Knn Graph", "abstract": "<p>Nearest neighbor search is known as a challenging issue that has been studied\nfor several decades. Recently, this issue becomes more and more imminent in\nviewing that the big data problem arises from various fields. In this paper, a\nscalable solution based on hill-climbing strategy with the support of k-nearest\nneighbor graph (kNN) is presented. Two major issues have been considered in the\npaper. Firstly, an efficient kNN graph construction method based on two means\ntree is presented. For the nearest neighbor search, an enhanced hill-climbing\nprocedure is proposed, which sees considerable performance boost over original\nprocedure. Furthermore, with the support of inverted indexing derived from\nresidue vector quantization, our method achieves close to 100% recall with high\nspeed efficiency in two state-of-the-art evaluation benchmarks. In addition, a\ncomparative study on both the compressional and traditional nearest neighbor\nsearch methods is presented. We show that our method achieves the best\ntrade-off between search quality, efficiency and memory complexity.</p>\n", "tags": ["ARXIV","Graph","Quantisation"] },
{"key": "zhao2018sorting", "year": "2018", "title":"An O(N) Sorting Algorithm Machine Learning Sort", "abstract": "<p>We propose an \\(O(N\\cdot M)\\) sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table.</p>\n", "tags": ["ARXIV"] },
{"key": "zhao2019focused", "year": "2019", "title":"Focused Quantization For Sparse Cnns", "abstract": "<p>Deep convolutional neural networks (CNNs) are powerful tools for a wide range of vision tasks, but the enormous amount of memory and compute resources required by CNNs poses a challenge in deploying them on constrained devices. Existing compression techniques, while excelling at reducing model sizes, struggle to be computationally friendly. In this paper, we attend to the statistical properties of sparse CNNs and present focused quantization, a novel quantization strategy based on power-of-two values, which exploits the weight distributions after fine-grained pruning. The proposed method dynamically discovers the most effective numerical representation for weights in layers with varying sparsities, significantly reducing model sizes. Multiplications in quantized CNNs are replaced with much cheaper bit-shift operations for efficient inference. Coupled with lossless encoding, we build a compression pipeline that provides CNNs with high compression ratios (CR), low computation cost and minimal loss in accuracies. In ResNet-50, we achieved a 18.08x CR with only 0.24% loss in top-5 accuracy, outperforming existing compression methods. We fully compress a ResNet-18 and found that it is not only higher in CR and top-5 accuracy, but also more hardware efficient as it requires fewer logic gates to implement when compared to other state-of-the-art quantization methods assuming the same throughput.</p>\n", "tags": ["NEURIPS","Quantisation","Supervised"] },
{"key": "zhao2020deep", "year": "2020", "title":"Deep Optimized Multiple Description Image Coding Via Scalar Quantization Learning", "abstract": "<p>In this paper, we introduce a deep multiple description coding (MDC)\nframework optimized by minimizing multiple description (MD) compressive loss.\nFirst, MD multi-scale-dilated encoder network generates multiple description\ntensors, which are discretized by scalar quantizers, while these quantized\ntensors are decompressed by MD cascaded-ResBlock decoder networks. To greatly\nreduce the total amount of artificial neural network parameters, an\nauto-encoder network composed of these two types of network is designed as a\nsymmetrical parameter sharing structure. Second, this autoencoder network and a\npair of scalar quantizers are simultaneously learned in an end-to-end\nself-supervised way. Third, considering the variation in the image spatial\ndistribution, each scalar quantizer is accompanied by an importance-indicator\nmap to generate MD tensors, rather than using direct quantization. Fourth, we\nintroduce the multiple description structural similarity distance loss, which\nimplicitly regularizes the diversified multiple description generations, to\nexplicitly supervise multiple description diversified decoding in addition to\nMD reconstruction loss. Finally, we demonstrate that our MDC framework performs\nbetter than several state-of-the-art MDC approaches regarding image coding\nefficiency when tested on several commonly available datasets.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "zhao2021large", "year": "2021", "title":"Large-scale Visual Search With Binary Distributed Graph At Alibaba", "abstract": "<p>Graph-based approximate nearest neighbor search has attracted more and more\nattentions due to its online search advantages. Numbers of methods studying the\nenhancement of speed and recall have been put forward. However, few of them\nfocus on the efficiency and scale of offline graph-construction. For a deployed\nvisual search system with several billions of online images in total, building\na billion-scale offline graph in hours is essential, which is almost\nunachievable by most existing methods. In this paper, we propose a novel\nalgorithm called Binary Distributed Graph to solve this problem. Specifically,\nwe combine binary codes with graph structure to speedup online and offline\nprocedures, and achieve comparable performance with the ones in real-value\nbased scenarios by recalling more binary candidates. Furthermore, the\ngraph-construction is optimized to completely distributed implementation, which\nsignificantly accelerates the offline process and gets rid of the limitation of\nmemory and disk within a single machine. Experimental comparisons on Alibaba\nCommodity Data Set (more than three billion images) show that the proposed\nmethod outperforms the state-of-the-art with respect to the online/offline\ntrade-off.</p>\n", "tags": ["ARXIV","Graph"] },
{"key": "zhao2021rescuing", "year": "2021", "title":"Rescuing Deep Hashing From Dead Bits Problem", "abstract": "<p>Deep hashing methods have shown great retrieval accuracy and efficiency in\nlarge-scale image retrieval. How to optimize discrete hash bits is always the\nfocus in deep hashing methods. A common strategy in these methods is to adopt\nan activation function, e.g. \\(\\operatorname{sigmoid}(\\cdot)\\) or\n\\(\\operatorname{tanh}(\\cdot)\\), and minimize a quantization loss to approximate\ndiscrete values. However, this paradigm may make more and more hash bits stuck\ninto the wrong saturated area of the activation functions and never escaped. We\ncall this problem “Dead Bits Problem~(DBP)”. Besides, the existing quantization\nloss will aggravate DBP as well. In this paper, we propose a simple but\neffective gradient amplifier which acts before activation functions to\nalleviate DBP. Moreover, we devise an error-aware quantization loss to further\nalleviate DBP. It avoids the negative effect of quantization loss based on the\nsimilarity between two images. The proposed gradient amplifier and error-aware\nquantization loss are compatible with a variety of deep hashing methods.\nExperimental results on three datasets demonstrate the efficiency of the\nproposed gradient amplifier and the error-aware quantization loss.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "zhao2023embedding", "year": "2023", "title":"Embedding In Recommender Systems A Survey", "abstract": "<p>Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that coverts the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors and can enhance\nthe recommendation performance. Applying embedding techniques captures complex\nentity relationships and has spurred substantial research. In this survey, we\nprovide an overview of the recent literature on embedding techniques in\nrecommender systems. This survey covers embedding methods like collaborative\nfiltering, self-supervised learning, and graph-based techniques. Collaborative\nfiltering generates embeddings capturing user-item preferences, excelling in\nsparse data. Self-supervised methods leverage contrastive or generative\nlearning for various tasks. Graph-based techniques like node2vec exploit\ncomplex relationships in network-rich environments. Addressing the scalability\nchallenges inherent to embedding methods, our survey delves into innovative\ndirections within the field of recommendation systems. These directions aim to\nenhance performance and reduce computational complexity, paving the way for\nimproved recommender systems. Among these innovative approaches, we will\nintroduce Auto Machine Learning (AutoML), hash techniques, and quantization\ntechniques in this survey. We discuss various architectures and techniques and\nhighlight the challenges and future directions in these aspects. This survey\naims to provide a comprehensive overview of the state-of-the-art in this\nrapidly evolving field and serve as a useful resource for researchers and\npractitioners working in the area of recommender systems.</p>\n", "tags": ["ARXIV","Graph","Quantisation","Supervised","Survey Paper"] },
{"key": "zhao2024deep", "year": "2024", "title":"Deep Semantic Ranking Based Hashing For Multi-label Image Retrieval", "abstract": "<p>With the rapid growth of web images, hashing has received\nincreasing interests in large scale image retrieval.\nResearch efforts have been devoted to learning compact binary\ncodes that preserve semantic similarity based on labels.\nHowever, most of these hashing methods are designed\nto handle simple binary similarity. The complex multilevel\nsemantic structure of images associated with multiple labels\nhave not yet been well explored. Here we propose a deep\nsemantic ranking based method for learning hash functions\nthat preserve multilevel semantic similarity between multilabel\nimages. In our approach, deep convolutional neural\nnetwork is incorporated into hash functions to jointly\nlearn feature representations and mappings from them to\nhash codes, which avoids the limitation of semantic representation\npower of hand-crafted features. Meanwhile, a\nranking list that encodes the multilevel similarity information\nis employed to guide the learning of such deep hash\nfunctions. An effective scheme based on surrogate loss is\nused to solve the intractable optimization problem of nonsmooth\nand multivariate ranking measures involved in the\nlearning procedure. Experimental results show the superiority\nof our proposed approach over several state-of-theart\nhashing methods in term of ranking evaluation metrics\nwhen tested on multi-label image datasets.</p>\n", "tags": ["ARXIV","Image Retrieval","Independent"] },
{"key": "zhe2018deep", "year": "2018", "title":"Deep Class-wise Hashing Semantics-preserving Hashing Via Class-wise Loss", "abstract": "<p>Deep supervised hashing has emerged as an influential solution to large-scale\nsemantic image retrieval problems in computer vision. In the light of recent\nprogress, convolutional neural network based hashing methods typically seek\npair-wise or triplet labels to conduct the similarity preserving learning.\nHowever, complex semantic concepts of visual contents are hard to capture by\nsimilar/dissimilar labels, which limits the retrieval performance. Generally,\npair-wise or triplet losses not only suffer from expensive training costs but\nalso lack in extracting sufficient semantic information. In this regard, we\npropose a novel deep supervised hashing model to learn more compact class-level\nsimilarity preserving binary codes. Our deep learning based model is motivated\nby deep metric learning that directly takes semantic labels as supervised\ninformation in training and generates corresponding discriminant hashing code.\nSpecifically, a novel cubic constraint loss function based on Gaussian\ndistribution is proposed, which preserves semantic variations while penalizes\nthe overlap part of different classes in the embedding space. To address the\ndiscrete optimization problem introduced by binary codes, a two-step\noptimization strategy is proposed to provide efficient training and avoid the\nproblem of gradient vanishing. Extensive experiments on four large-scale\nbenchmark databases show that our model can achieve the state-of-the-art\nretrieval performance. Moreover, when training samples are limited, our method\nsurpasses other supervised deep hashing methods with non-negligible margins.</p>\n", "tags": ["ARXIV","Deep Learning","Image Retrieval","Supervised"] },
{"key": "zhen2012co", "year": "2012", "title":"Co-regularized Hashing For Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity search.  To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically.  In this paper, we study hash function learning in the context of multimodal data.  We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted co-regularization framework.  The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.  We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.</p>\n", "tags": ["Cross Modal","Dataset","Independent","NEURIPS"] },
{"key": "zhen2024co", "year": "2024", "title":"Co-regularized Hashing For Multimodal Data", "abstract": "<p>Hashing-based methods provide a very promising approach to large-scale similarity\nsearch. To obtain compact hash codes, a recent trend seeks to learn the hash\nfunctions from data automatically. In this paper, we study hash function learning\nin the context of multimodal data. We propose a novel multimodal hash function\nlearning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization\nframework. The hash functions for each bit of the hash codes are\nlearned by solving DC (difference of convex functions) programs, while the learning\nfor multiple bits proceeds via a boosting procedure so that the bias introduced\nby the hash functions can be sequentially minimized. We empirically compare\nCRH with two state-of-the-art multimodal hash function learning methods on two\npublicly available data sets.</p>\n", "tags": ["ARXIV","Cross Modal","Dataset","Independent"] },
{"key": "zhen2024cross", "year": "2024", "title":"Cross-modal Similarity Learning Via Pairs Preferences And Active Supervision", "abstract": "<p>We present a probabilistic framework for learning pairwise similarities between objects belonging to different modalities, such as drugs and proteins, or text and\nimages. Our framework is based on learning a binary\ncode based representation for objects in each modality, and has the following key properties: (i) it can\nleverage both pairwise as well as easy-to-obtain relative\npreference based cross-modal constraints, (ii) the probabilistic framework naturally allows querying for the\nmost useful/informative constraints, facilitating an active learning setting (existing methods for cross-modal\nsimilarity learning do not have such a mechanism), and\n(iii) the binary code length is learned from the data. We\ndemonstrate the effectiveness of the proposed approach\non two problems that require computing pairwise similarities between cross-modal object pairs: cross-modal\nlink prediction in bipartite graphs, and hashing based\ncross-modal similarity search.</p>\n", "tags": ["ARXIV","Cross Modal","Graph"] },
{"key": "zheng2014bayes", "year": "2014", "title":"Bayes Merging Of Multiple Vocabularies For Scalable Image Retrieval", "abstract": "<p>The Bag-of-Words (BoW) representation is well applied to recent\nstate-of-the-art image retrieval works. Typically, multiple vocabularies are\ngenerated to correct quantization artifacts and improve recall. However, this\nroutine is corrupted by vocabulary correlation, i.e., overlapping among\ndifferent vocabularies. Vocabulary correlation leads to an over-counting of the\nindexed features in the overlapped area, or the intersection set, thus\ncompromising the retrieval accuracy. In order to address the correlation\nproblem while preserve the benefit of high recall, this paper proposes a Bayes\nmerging approach to down-weight the indexed features in the intersection set.\nThrough explicitly modeling the correlation problem in a probabilistic view, a\njoint similarity on both image- and feature-level is estimated for the indexed\nfeatures in the intersection set.\n  We evaluate our method through extensive experiments on three benchmark\ndatasets. Albeit simple, Bayes merging can be well applied in various merging\ntasks, and consistently improves the baselines on multi-vocabulary merging.\nMoreover, Bayes merging is efficient in terms of both time and memory cost, and\nyields competitive performance compared with the state-of-the-art methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "zheng2014packing", "year": "2014", "title":"Packing And Padding Coupled Multi-index For Accurate Image Retrieval", "abstract": "<p>In Bag-of-Words (BoW) based image retrieval, the SIFT visual word has a low\ndiscriminative power, so false positive matches occur prevalently. Apart from\nthe information loss during quantization, another cause is that the SIFT\nfeature only describes the local gradient distribution. To address this\nproblem, this paper proposes a coupled Multi-Index (c-MI) framework to perform\nfeature fusion at indexing level. Basically, complementary features are coupled\ninto a multi-dimensional inverted index. Each dimension of c-MI corresponds to\none kind of feature, and the retrieval process votes for images similar in both\nSIFT and other feature spaces. Specifically, we exploit the fusion of local\ncolor feature into c-MI. While the precision of visual match is greatly\nenhanced, we adopt Multiple Assignment to improve recall. The joint cooperation\nof SIFT and color features significantly reduces the impact of false positive\nmatches.\n  Extensive experiments on several benchmark datasets demonstrate that c-MI\nimproves the retrieval accuracy significantly, while consuming only half of the\nquery time compared to the baseline. Importantly, we show that c-MI is well\ncomplementary to many prior techniques. Assembling these methods, we have\nobtained an mAP of 85.8% and N-S score of 3.85 on Holidays and Ukbench\ndatasets, respectively, which compare favorably with the state-of-the-arts.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "zheng2020generative", "year": "2020", "title":"Generative Semantic Hashing Enhanced Via Boltzmann Machines", "abstract": "<p>Generative semantic hashing is a promising technique for large-scale\ninformation retrieval thanks to its fast retrieval speed and small memory\nfootprint. For the tractability of training, existing generative-hashing\nmethods mostly assume a factorized form for the posterior distribution,\nenforcing independence among the bits of hash codes. From the perspectives of\nboth model representation and code space size, independence is always not the\nbest assumption. In this paper, to introduce correlations among the bits of\nhash codes, we propose to employ the distribution of Boltzmann machine as the\nvariational posterior. To address the intractability issue of training, we\nfirst develop an approximate method to reparameterize the distribution of a\nBoltzmann machine by augmenting it as a hierarchical concatenation of a\nGaussian-like distribution and a Bernoulli distribution. Based on that, an\nasymptotically-exact lower bound is further derived for the evidence lower\nbound (ELBO). With these novel techniques, the entire model can be optimized\nefficiently. Extensive experimental results demonstrate that by effectively\nmodeling correlations among different bits within a hash code, our model can\nachieve significant performance gains.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "zheng2023building", "year": "2023", "title":"Building K-anonymous User Cohorts With Consecutive Consistent Weighted Sampling (CCWS)", "abstract": "<p>To retrieve personalized campaigns and creatives while protecting user\nprivacy, digital advertising is shifting from member-based identity to\ncohort-based identity. Under such identity regime, an accurate and efficient\ncohort building algorithm is desired to group users with similar\ncharacteristics. In this paper, we propose a scalable \\(K\\)-anonymous cohort\nbuilding algorithm called {\\em consecutive consistent weighted sampling}\n(CCWS). The proposed method combines the spirit of the (\\(p\\)-powered) consistent\nweighted sampling and hierarchical clustering, so that the \\(K\\)-anonymity is\nensured by enforcing a lower bound on the size of cohorts. Evaluations on a\nLinkedIn dataset consisting of \\(&gt;70\\)M users and ads campaigns demonstrate that\nCCWS achieves substantial improvements over several hashing-based methods\nincluding sign random projections (SignRP), minwise hashing (MinHash), as well\nas the vanilla CWS.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "zhong2015deep", "year": "2015", "title":"A Deep Hashing Learning Network", "abstract": "<p>Hashing-based methods seek compact and efficient binary codes that preserve\nthe neighborhood structure in the original data space. For most existing\nhashing methods, an image is first encoded as a vector of hand-crafted visual\nfeature, followed by a hash projection and quantization step to get the compact\nbinary vector. Most of the hand-crafted features just encode the low-level\ninformation of the input, the feature may not preserve the semantic\nsimilarities of images pairs. Meanwhile, the hashing function learning process\nis independent with the feature representation, so the feature may not be\noptimal for the hashing projection. In this paper, we propose a supervised\nhashing method based on a well designed deep convolutional neural network,\nwhich tries to learn hashing code and compact representations of data\nsimultaneously. The proposed model learn the binary codes by adding a compact\nsigmoid layer before the loss layer. Experiments on several image data sets\nshow that the proposed model outperforms other state-of-the-art methods.</p>\n", "tags": ["ARXIV","Quantisation","Supervised"] },
{"key": "zhong2015efficient", "year": "2015", "title":"Efficient Similarity Indexing And Searching In High Dimensions", "abstract": "<p>Efficient indexing and searching of high dimensional data has been an area of\nactive research due to the growing exploitation of high dimensional data and\nthe vulnerability of traditional search methods to the curse of dimensionality.\nThis paper presents a new approach for fast and effective searching and\nindexing of high dimensional features using random partitions of the feature\nspace. Experiments on both handwritten digits and 3-D shape descriptors have\nshown the proposed algorithm to be highly effective and efficient in indexing\nand searching real data sets of several hundred dimensions. We also compare its\nperformance to that of the state-of-the-art locality sensitive hashing\nalgorithm.</p>\n", "tags": ["ARXIV","Independent"] },
{"key": "zhornyak2022hashencoding", "year": "2022", "title":"Hashencoding Autoencoding With Multiscale Coordinate Hashing", "abstract": "<p>We present HashEncoding, a novel autoencoding architecture that leverages a\nnon-parametric multiscale coordinate hash function to facilitate a per-pixel\ndecoder without convolutions. By leveraging the space-folding behaviour of\nhashing functions, HashEncoding allows for an inherently multiscale embedding\nspace that remains much smaller than the original image. As a result, the\ndecoder requires very few parameters compared with decoders in traditional\nautoencoders, approaching a non-parametric reconstruction of the original image\nand allowing for greater generalizability. Finally, by allowing backpropagation\ndirectly to the coordinate space, we show that HashEncoding can be exploited\nfor geometric tasks such as optical flow.</p>\n", "tags": ["ARXIV","Unsupervised"] },
{"key": "zhou2016generic", "year": "2016", "title":"A Generic Inverted Index Framework For Similarity Search On The GPU - Technical Report", "abstract": "<p>We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named \\(\\tau\\)-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source.</p>\n", "tags": ["ARXIV","Independent","LSH"] },
{"key": "zhou2016transfer", "year": "2016", "title":"Transfer Hashing With Privileged Information", "abstract": "<p>Most existing learning to hash methods assume that there are sufficient data,\neither labeled or unlabeled, on the domain of interest (i.e., the target\ndomain) for training. However, this assumption cannot be satisfied in some\nreal-world applications. To address this data sparsity issue in hashing,\ninspired by transfer learning, we propose a new framework named Transfer\nHashing with Privileged Information (THPI). Specifically, we extend the\nstandard learning to hash method, Iterative Quantization (ITQ), in a transfer\nlearning manner, namely ITQ+. In ITQ+, a new slack function is learned from\nauxiliary data to approximate the quantization error in ITQ. We developed an\nalternating optimization approach to solve the resultant optimization problem\nfor ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure\namong the auxiliary data for learning more precise binary codes in the target\ndomain. Extensive experiments on several benchmark datasets verify the\neffectiveness of our proposed approaches through comparisons with several\nstate-of-the-art baselines.</p>\n", "tags": ["ARXIV","Quantisation"] },
{"key": "zhou2017deep", "year": "2017", "title":"Deep Hashing With Triplet Quantization Loss", "abstract": "<p>With the explosive growth of image databases, deep hashing, which learns\ncompact binary descriptors for images, has become critical for fast image\nretrieval. Many existing deep hashing methods leverage quantization loss,\ndefined as distance between the features before and after quantization, to\nreduce the error from binarizing features. While minimizing the quantization\nloss guarantees that quantization has minimal effect on retrieval accuracy, it\nunfortunately significantly reduces the expressiveness of features even before\nthe quantization. In this paper, we show that the above definition of\nquantization loss is too restricted and in fact not necessary for maintaining\nhigh retrieval accuracy. We therefore propose a new form of quantization loss\nmeasured in triplets. The core idea of the triplet quantization loss is to\nlearn discriminative real-valued descriptors which lead to minimal loss on\nretrieval accuracy after quantization. Extensive experiments on two widely used\nbenchmark data sets of different scales, CIFAR-10 and In-shop, demonstrate that\nthe proposed method outperforms the state-of-the-art deep hashing methods.\nMoreover, we show that the compact binary descriptors obtained with triplet\nquantization loss lead to very small performance drop after quantization.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation"] },
{"key": "zhou2020its", "year": "2020", "title":"Its The Best Only When It Fits You Most Finding Related Models For Serving Based On Dynamic Locality Sensitive Hashing", "abstract": "<p>In recent, deep learning has become the most popular direction in machine\nlearning and artificial intelligence. However, preparation of training data is\noften a bottleneck in the lifecycle of deploying a deep learning model for\nproduction or research. Reusing models for inferencing a dataset can greatly\nsave the human costs required for training data creation. Although there exist\na number of model sharing platform such as TensorFlow Hub, PyTorch Hub, DLHub,\nmost of these systems require model uploaders to manually specify the details\nof each model and model downloaders to screen keyword search results for\nselecting a model. They are in lack of an automatic model searching tool. This\npaper proposes an end-to-end process of searching related models for serving\nbased on the similarity of the target dataset and the training datasets of the\navailable models. While there exist many similarity measurements, we study how\nto efficiently apply these metrics without pair-wise comparison and compare the\neffectiveness of these metrics. We find that our proposed adaptivity\nmeasurement which is based on Jensen-Shannon (JS) divergence, is an effective\nmeasurement, and its computation can be significantly accelerated by using the\ntechnique of locality sensitive hashing.</p>\n", "tags": ["ARXIV","Deep Learning","Supervised"] },
{"key": "zhu2017discrete", "year": "2017", "title":"Discrete Multi-modal Hashing With Canonical Views For Robust Mobile Landmark Search", "abstract": "<p>Mobile landmark search (MLS) recently receives increasing attention for its\ngreat practical values. However, it still remains unsolved due to two important\nchallenges. One is high bandwidth consumption of query transmission, and the\nother is the huge visual variations of query images sent from mobile devices.\nIn this paper, we propose a novel hashing scheme, named as canonical view based\ndiscrete multi-modal hashing (CV-DMH), to handle these problems via a novel\nthree-stage learning procedure. First, a submodular function is designed to\nmeasure visual representativeness and redundancy of a view set. With it,\ncanonical views, which capture key visual appearances of landmark with limited\nredundancy, are efficiently discovered with an iterative mining strategy.\nSecond, multi-modal sparse coding is applied to transform visual features from\nmultiple modalities into an intermediate representation. It can robustly and\nadaptively characterize visual contents of varied landmark images with certain\ncanonical views. Finally, compact binary codes are learned on intermediate\nrepresentation within a tailored discrete binary embedding model which\npreserves visual relations of images measured with canonical views and removes\nthe involved noises. In this part, we develop a new augmented Lagrangian\nmultiplier (ALM) based optimization method to directly solve the discrete\nbinary codes. We can not only explicitly deal with the discrete constraint, but\nalso consider the bit-uncorrelated constraint and balance constraint together.\nExperiments on real world landmark datasets demonstrate the superior\nperformance of CV-DMH over several state-of-the-art methods.</p>\n", "tags": ["ARXIV"] },
{"key": "zhu2017part", "year": "2017", "title":"Part-based Deep Hashing For Large-scale Person Re-identification", "abstract": "<p>Large-scale is a trend in person re-identification (re-id). It is important\nthat real-time search be performed in a large gallery. While previous methods\nmostly focus on discriminative learning, this paper makes the attempt in\nintegrating deep learning and hashing into one framework to evaluate the\nefficiency and accuracy for large-scale person re-id. We integrate spatial\ninformation for discriminative visual representation by partitioning the\npedestrian image into horizontal parts. Specifically, Part-based Deep Hashing\n(PDH) is proposed, in which batches of triplet samples are employed as the\ninput of the deep hashing architecture. Each triplet sample contains two\npedestrian images (or parts) with the same identity and one pedestrian image\n(or part) of the different identity. A triplet loss function is employed with a\nconstraint that the Hamming distance of pedestrian images (or parts) with the\nsame identity is smaller than ones with the different identity. In the\nexperiment, we show that the proposed Part-based Deep Hashing method yields\nvery competitive re-id accuracy on the large-scale Market-1501 and\nMarket-1501+500K datasets.</p>\n", "tags": ["ARXIV","Deep Learning"] },
{"key": "zhu2019exploring", "year": "2019", "title":"Exploring Auxiliary Context Discrete Semantic Transfer Hashing For Scalable Image Retrieval", "abstract": "<p>Unsupervised hashing can desirably support scalable content-based image\nretrieval (SCBIR) for its appealing advantages of semantic label independence,\nmemory and search efficiency. However, the learned hash codes are embedded with\nlimited discriminative semantics due to the intrinsic limitation of image\nrepresentation. To address the problem, in this paper, we propose a novel\nhashing approach, dubbed as <em>Discrete Semantic Transfer Hashing</em> (DSTH).\nThe key idea is to <em>directly</em> augment the semantics of discrete image hash\ncodes by exploring auxiliary contextual modalities. To this end, a unified\nhashing framework is formulated to simultaneously preserve visual similarities\nof images and perform semantic transfer from contextual modalities. Further, to\nguarantee direct semantic transfer and avoid information loss, we explicitly\nimpose the discrete constraint, bit–uncorrelation constraint and bit-balance\nconstraint on hash codes. A novel and effective discrete optimization method\nbased on augmented Lagrangian multiplier is developed to iteratively solve the\noptimization problem. The whole learning process has linear computation\ncomplexity and desirable scalability. Experiments on three benchmark datasets\ndemonstrate the superiority of DSTH compared with several state-of-the-art\napproaches.</p>\n", "tags": ["ARXIV","Image Retrieval","Unsupervised"] },
{"key": "zhu2020dual", "year": "2020", "title":"Dual-level Semantic Transfer Deep Hashing For Efficient Social Image Retrieval", "abstract": "<p>Social network stores and disseminates a tremendous amount of user shared\nimages. Deep hashing is an efficient indexing technique to support large-scale\nsocial image retrieval, due to its deep representation capability, fast\nretrieval speed and low storage cost. Particularly, unsupervised deep hashing\nhas well scalability as it does not require any manually labelled data for\ntraining. However, owing to the lacking of label guidance, existing methods\nsuffer from severe semantic shortage when optimizing a large amount of deep\nneural network parameters. Differently, in this paper, we propose a Dual-level\nSemantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a\nunified deep hash learning framework. Our model targets at learning the\nsemantically enhanced deep hash codes by specially exploiting the\nuser-generated tags associated with the social images. Specifically, we design\na complementary dual-level semantic transfer mechanism to efficiently discover\nthe potential semantics of tags and seamlessly transfer them into binary hash\ncodes. On the one hand, instance-level semantics are directly preserved into\nhash codes from the associated tags with adverse noise removing. Besides, an\nimage-concept hypergraph is constructed for indirectly transferring the latent\nhigh-order semantic correlations of images and tags into hash codes. Moreover,\nthe hash codes are obtained simultaneously with the deep representation\nlearning by the discrete hash optimization strategy. Extensive experiments on\ntwo public social image retrieval datasets validate the superior performance of\nour method compared with state-of-the-art hashing methods. The source codes of\nour method can be obtained at https://github.com/research2020-1/DSTDH</p>\n", "tags": ["ARXIV","Graph","Has Code","Image Retrieval","Unsupervised"] },
{"key": "zhu2022lower", "year": "2022", "title":"A Lower Bound Of Hash Codes Performance", "abstract": "<p>As a crucial approach for compact representation learning, hashing has achieved great success in effectiveness and efficiency. Numerous heuristic Hamming space metric learning objectives are designed to obtain high-quality hash codes. Nevertheless, a theoretical analysis of criteria for learning good hash codes remains largely unexploited. In this paper, we prove that inter-class distinctiveness and intra-class compactness among hash codes determine the lower bound of hash codes’ performance. Promoting these two characteristics could lift the bound and improve hash learning. We then propose a surrogate model to fully exploit the above objective by estimating the posterior of hash codes and controlling it, which results in a low-bias optimization. Extensive experiments reveal the effectiveness of the proposed method. By testing on a series of hash-models, we obtain performance improvements among all of them, with an up to \\(26.5\\%\\) increase in mean Average Precision and an up to \\(20.5\\%\\) increase in accuracy. Our code is publicly available at https://github.com/VL-Group/LBHash.</p>\n", "tags": ["Has Code","Independent","NEURIPS"] },
{"key": "zhu2023adaptive", "year": "2023", "title":"Adaptive Confidence Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>The multi-view hash method converts heterogeneous data from multiple views\ninto binary hash codes, which is one of the critical technologies in multimedia\nretrieval. However, the current methods mainly explore the complementarity\namong multiple views while lacking confidence learning and fusion. Moreover, in\npractical application scenarios, the single-view data contain redundant noise.\nTo conduct the confidence learning and eliminate unnecessary noise, we propose\na novel Adaptive Confidence Multi-View Hashing (ACMVH) method. First, a\nconfidence network is developed to extract useful information from various\nsingle-view features and remove noise information. Furthermore, an adaptive\nconfidence multi-view network is employed to measure the confidence of each\nview and then fuse multi-view features through a weighted summation. Lastly, a\ndilation network is designed to further enhance the feature representation of\nthe fused features. To the best of our knowledge, we pioneer the application of\nconfidence learning into the field of multimedia retrieval. Extensive\nexperiments on two public datasets show that the proposed ACMVH performs better\nthan state-of-the-art methods (maximum increase of 3.24%). The source code is\navailable at https://github.com/HackerHyper/ACMVH.</p>\n", "tags": ["ARXIV","Cross Modal","Has Code","Independent"] },
{"key": "zhu2023central", "year": "2023", "title":"Central Similarity Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>Hash representation learning of multi-view heterogeneous data is the key to\nimproving the accuracy of multimedia retrieval. However, existing methods\nutilize local similarity and fall short of deeply fusing the multi-view\nfeatures, resulting in poor retrieval accuracy. Current methods only use local\nsimilarity to train their model. These methods ignore global similarity.\nFurthermore, most recent works fuse the multi-view features via a weighted sum\nor concatenation. We contend that these fusion methods are insufficient for\ncapturing the interaction between various views. We present a novel Central\nSimilarity Multi-View Hashing (CSMVH) method to address the mentioned problems.\nCentral similarity learning is used for solving the local similarity problem,\nwhich can utilize the global similarity between the hash center and samples. We\npresent copious empirical data demonstrating the superiority of gate-based\nfusion over conventional approaches. On the MS COCO and NUS-WIDE, the proposed\nCSMVH performs better than the state-of-the-art methods by a large margin (up\nto 11.41% mean Average Precision (mAP) improvement).</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "zhu2023clip", "year": "2023", "title":"CLIP Multi-modal Hashing A New Baseline CLIPMH", "abstract": "<p>The multi-modal hashing method is widely used in multimedia retrieval. It can\nfuse multi-source data to generate binary hash code. However, the current\nmulti-modal methods have the problem of low retrieval accuracy. The reason is\nthat the individual backbone networks have limited feature expression\ncapabilities and are not jointly pre-trained on large-scale unsupervised\nmulti-modal data. To solve this problem, we propose a new baseline CLIP\nMulti-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and\nimage features, and then fuse to generate hash code. CLIP improves the\nexpressiveness of each modal feature. In this way, it can greatly improve the\nretrieval performance of multi-modal hashing methods. In comparison to\nstate-of-the-art unsupervised and supervised multi-modal hashing methods,\nexperiments reveal that the proposed CLIPMH can significantly enhance\nperformance (Maximum increase of 8.38%). CLIP also has great advantages over\nthe text and visual backbone networks commonly used before.</p>\n", "tags": ["ARXIV","Supervised"] },
{"key": "zhu2023deep", "year": "2023", "title":"Deep Metric Multi-view Hashing For Multimedia Retrieval", "abstract": "<p>Learning the hash representation of multi-view heterogeneous data is an\nimportant task in multimedia retrieval. However, existing methods fail to\neffectively fuse the multi-view features and utilize the metric information\nprovided by the dissimilar samples, leading to limited retrieval precision.\nCurrent methods utilize weighted sum or concatenation to fuse the multi-view\nfeatures. We argue that these fusion methods cannot capture the interaction\namong different views. Furthermore, these methods ignored the information\nprovided by the dissimilar samples. We propose a novel deep metric multi-view\nhashing (DMMVH) method to address the mentioned problems. Extensive empirical\nevidence is presented to show that gate-based fusion is better than typical\nmethods. We introduce deep metric learning to the multi-view hashing problems,\nwhich can utilize metric information of dissimilar samples. On the\nMIR-Flickr25K, MS COCO, and NUS-WIDE, our method outperforms the current\nstate-of-the-art methods by a large margin (up to 15.28 mean Average Precision\n(mAP) improvement).</p>\n", "tags": ["ARXIV","Cross Modal"] },
{"key": "zhu2024deep", "year": "2024", "title":"Deep Hashing Network For Efficient Similarity Retrieval", "abstract": "<p>Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Supervised hashing, which improves the quality of hash coding by exploiting the semantic similarity on data pairs, has received increasing attention recently. For most existing supervised hashing methods for image retrieval, an image is first represented as a vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes.\nHowever, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Deep Hashing Network (DHN) architecture for supervised hashing, in which we jointly learn good image representation tailored to hash coding and formally control the quantization error.\nThe DHN model constitutes four key components: (1) a sub-network with multiple convolution-pooling layers to capture image representations; (2) a fully-connected hashing layer to generate compact binary hash codes; (3) a pairwise cross-entropy loss layer for similarity-preserving learning; and (4) a pairwise quantization loss for controlling hashing quality. Extensive experiments on standard image retrieval datasets show the proposed DHN model yields substantial boosts over latest state-of-the-art hashing methods.</p>\n", "tags": ["ARXIV","Image Retrieval","Quantisation","Supervised"] },
{"key": "zhu2024linear", "year": "2024", "title":"Linear Cross-modal Hashing For Efficient Multimedia Search", "abstract": "<p>Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel \ncross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia \nsearch across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals \ninto consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. \nMore specifically, for each modal, we first partition the training data into \\(k\\) clusters and then represent each training data \npoint with its distances to \\(k\\) centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce \nthe time complexity of the training phase from traditional O(n2) or higher to O(n), where \\(n\\) is the training data size, leading to \npractical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. \nTo preserve the inter-similarity among data points across different modals, we transform the derived data representations into a \ncommon binary subspace in which binary codes from all the modals are “consistent” and comparable. The transformation simultaneously \noutputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, \nit is first mapped into the binary codes using the modal’s hash functions, followed by matching the database binary codes of any other \nmodals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in \ncomparison with the state of the art.</p>\n", "tags": ["ARXIV","Cross Modal","Supervised"] },
{"key": "zhuang2016fast", "year": "2016", "title":"Fast Training Of Triplet-based Deep Binary Embedding Networks", "abstract": "<p>In this paper, we aim to learn a mapping (or embedding) from images to a\ncompact binary space in which Hamming distances correspond to a ranking measure\nfor the image retrieval task.\n  We make use of a triplet loss because this has been shown to be most\neffective for ranking problems.\n  However, training in previous works can be prohibitively expensive due to the\nfact that optimization is directly performed on the triplet space, where the\nnumber of possible triplets for training is cubic in the number of training\nexamples.\n  To address this issue, we propose to formulate high-order binary codes\nlearning as a multi-label classification problem by explicitly separating\nlearning into two interleaved stages.\n  To solve the first stage, we design a large-scale high-order binary codes\ninference algorithm to reduce the high-order objective to a standard binary\nquadratic problem such that graph cuts can be used to efficiently infer the\nbinary code which serve as the label of each training datum.\n  In the second stage we propose to map the original image to compact binary\ncodes via carefully designed deep convolutional neural networks (CNNs) and the\nhashing function fitting can be solved by training binary CNN classifiers.\n  An incremental/interleaved optimization strategy is proffered to ensure that\nthese two steps are interactive with each other during training for better\naccuracy.\n  We conduct experiments on several benchmark datasets, which demonstrate both\nimproved training time (by as much as two orders of magnitude) as well as\nproducing state-of-the-art hashing for various retrieval tasks.</p>\n", "tags": ["ARXIV","CNN","Graph","Image Retrieval","Supervised"] },
{"key": "ziegelmeier2012locally", "year": "2012", "title":"Locally Linear Embedding Clustering Algorithm For Natural Imagery", "abstract": "<p>The ability to characterize the color content of natural imagery is an\nimportant application of image processing. The pixel by pixel coloring of\nimages may be viewed naturally as points in color space, and the inherent\nstructure and distribution of these points affords a quantization, through\nclustering, of the color information in the image. In this paper, we present a\nnovel topologically driven clustering algorithm that permits segmentation of\nthe color features in a digital image. The algorithm blends Locally Linear\nEmbedding (LLE) and vector quantization by mapping color information to a lower\ndimensional space, identifying distinct color regions, and classifying pixels\ntogether based on both a proximity measure and color content. It is observed\nthat these techniques permit a significant reduction in color resolution while\nmaintaining the visually important features of images.</p>\n", "tags": ["ARXIV","Quantisation","Unsupervised"] },
{"key": "zou2019transductive", "year": "2019", "title":"Transductive Zero-shot Hashing For Multilabel Image Retrieval", "abstract": "<p>Hash coding has been widely used in approximate nearest neighbor search for large-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.</p>\n", "tags": ["Image Retrieval","Quantisation","Supervised"] },
{"key": "ñanculef2020self", "year": "2020", "title":"Self-supervised Bernoulli Autoencoders For Semi-supervised Hashing", "abstract": "<p>Semantic hashing is an emerging technique for large-scale similarity search\nbased on representing high-dimensional data using similarity-preserving binary\ncodes used for efficient indexing and search. It has recently been shown that\nvariational autoencoders, with Bernoulli latent representations parametrized by\nneural nets, can be successfully trained to learn such codes in supervised and\nunsupervised scenarios, improving on more traditional methods thanks to their\nability to handle the binary constraints architecturally. However, the scenario\nwhere labels are scarce has not been studied yet.\n  This paper investigates the robustness of hashing methods based on\nvariational autoencoders to the lack of supervision, focusing on two\nsemi-supervised approaches currently in use. The first augments the variational\nautoencoder’s training objective to jointly model the distribution over the\ndata and the class labels. The second approach exploits the annotations to\ndefine an additional pairwise loss that enforces consistency between the\nsimilarity in the code (Hamming) space and the similarity in the label space.\nOur experiments show that both methods can significantly increase the hash\ncodes’ quality. The pairwise approach can exhibit an advantage when the number\nof labelled points is large. However, we found that this method degrades\nquickly and loses its advantage when labelled samples decrease. To circumvent\nthis problem, we propose a novel supervision method in which the model uses its\nlabel distribution predictions to implement the pairwise objective. Compared to\nthe best baseline, this procedure yields similar performance in fully\nsupervised settings but improves the results significantly when labelled data\nis scarce. Our code is made publicly available at\nhttps://github.com/amacaluso/SSB-VAE.</p>\n", "tags": ["ARXIV","Has Code","Supervised"] }

]
