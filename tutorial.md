---
layout: default
title: Tutorial
comments: true
---

# Needles, Haystacks and Hashing: Smarter Search with AI

**Author:** Sean Moran  
**Published:** Jan 11, 2024  

🔗 [Read on Medium](https://medium.com/@sean.j.moran/learning-to-hash-finding-the-needle-in-the-haystack-with-ai-24a15f85de0e)

## **How to find What You’re Looking For — Fast**

This article offers a hands-on introduction to machine learning approaches for nearest neighbour search, a field often called *Learning to Hash* or *Semantic Hashing*.

You’ll learn about:

* Why nearest neighbour search matters in computer science
* Approximate methods that dramatically speed up search
* Locality Sensitive Hashing (LSH), a widely used algorithm
* How machine learning can make neighbour search even more effective

👉 A fully annotated notebook with the experiments can be found on [**Google Colab**](https://colab.research.google.com/drive/1l-2wt1rozorVZLpxSQQks10CVC3iiBxS?usp=sharing).

![](https://miro.medium.com/v2/resize:fit:1024/1*JzcGGbGxBLwn7FuNeiQy7g.png)**Figure 1:** Searching for a needle in a haystack: a metaphor for similarity search at scale. Without efficient indexing, finding a single relevant item in massive datasets can be overwhelming. 📖 Source: Image generated by Author using DALL-E.## **Why Nearest Neighbour Search Matters**

[Nearest neighbour search](https://en.wikipedia.org/wiki/Nearest_neighbor_search) is the core computer science task of finding the most similar data points to a query within a database. It’s a fundamental matching operation with wide applications across fields like bioinformatics, natural language processing (NLP), and computer vision.

It also powers today’s vector databases, the backbone of Retrieval-Augmented Generation (RAG) with large language models (LLMs).

Here are just a few places where nearest neighbour search and hashing shine:

* [**Code Search**:](https://arxiv.org/abs/2111.04473) Scalable source code search engines (e.g. using MinHash) for large repositories.
* [**Efficient Transformers**:](https://openreview.net/pdf?id=rkgNKkHtvB) Speeding up NLP models by applying LSH inside Transformer architectures.
* [**Social Media Monitoring**:](https://medium.com/r?url=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FP14-5007) Real-time event detection and tracking.
* [**Seismic Analysis**:](https://dawnd9.sites.stanford.edu/news/earthquake-hunting-efficient-time-series-similarity-search) Detecting earthquakes via time-series similarity search.
* [**Fraud Detection**:](https://www.uber.com/en-GB/blog/lsh/) Uber applies LSH to spot suspiciously similar rides.
* [**Audio Fingerprinting**:](https://santhoshhari.github.io/Locality-Sensitive-Hashing/) Matching snippets of audio to massive song databases (think *Shazam!*).
* [**Genomics**:](https://pubmed.ncbi.nlm.nih.gov/26006009/) Assembling genomes and matching gene expressions across large datasets.
* [**Image Retrieval**:](https://research.google/pubs/visualrank-applying-pagerank-to-large-scale-image-search/) Google combines LSH with PageRank to index billions of images.
* [**Malware Detection**:](https://media.kaspersky.com/en/enterprise-security/Kaspersky-Lab-Whitepaper-Machine-Learning.pdf) Anti-virus tools hash suspicious code snippets to flag known malware.

And the list goes on.

In this article, we’ll show how approximate nearest neighbour (ANN) methods make search dramatically faster. By using hashing, ANN achieves sub-linear retrieval times, meaning search grows much more slowly than dataset size. To see why this matters, compare linear vs. sub-linear growth: as your dataset scales, sub-linear time stays manageable, while linear time quickly becomes impractical.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*FxrM899j_4Y2mt6ZZe3c7Q.png)**Figure 2:** As datasets grow, the efficiency gap between linear scans (O(N)) and sub-linear methods (O(log N)) widens dramatically. Index-based lookups such as binary search or B-trees scale far better than brute-force scans, making sub-linear approaches essential for large-scale retrieval. 📖 **Source:** Image by author via GPT5.Hashing algorithms generate binary hash codes that preserve similarity: semantically similar data points map to similar codes. These codes index items (images, documents, etc.) into hash table buckets, where similar points ideally land in the same bucket. The process is shown in the diagram below.

The entire process is illustrated in the diagram below:

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*FvoeUBLingGRjCqLwmgqYQ.png)**Figure 3: Hashing for fast image retrieval.** A query image is mapped to a compact binary code, which is then used to look up candidate images in a hashtable. Only those candidates are compared for similarity, yielding efficient nearest neighbour search. Applications include content-based image retrieval, near-duplicate detection, and location recognition. 📖 Source: Image by author. Diagram taken from the [PhD thesis of Sean Moran](https://learning2hash.github.io/cite.html).Given a query — say, the tiger image above — we generate its hash code and only compare it against data points in the same hash table bucket (or across multiple buckets if several hash tables are used). Because each bucket usually contains far fewer items than the full dataset, search is much faster than brute force. The trade-off is that we may not always retrieve the exact nearest neighbour, but the speed gain typically outweighs the small loss in accuracy.

In this article, we explore a published *Learning to Hash* model and compare its image-retrieval performance to Locality Sensitive Hashing (LSH). In particular, we focus on [Graph Regularised Hashing (GRH)](https://learning2hash.github.io/publications/moran2015graph/) — a simple yet effective supervised hashing approach — later extended to [cross-modal hashing](https://dl.acm.org/doi/abs/10.1145/2766462.2767816) for retrieving across both images and text.

## **Learning to Hash — Optimising Retrieval Effectiveness with AI**

There are many ways to generate hash codes. Some methods are data-independent, relying on random functions with special properties, while more recent approaches learn the codes directly from data.

The best-known data-independent approach is Locality Sensitive Hashing (LSH), which has been widely written about. In this article, however, we focus on learning hash functions with AI — often called *Semantic Hashing* or *Learning to Hash*.

Our case study is Graph Regularised Hashing (GRH), an intuitive supervised hashing model that strikes a good balance between simplicity and effectiveness. We compare GRH against LSH on the benchmark task of image retrieval. GRH has also been extended to cross-modal hashing, enabling retrieval across both images and text.

There are many open-source Learning to Hash models available. I have chosen GRH because it clearly illustrates the core learning principles without requiring heavy mathematical detail. Other influential models include Supervised Hashing with Kernels and Deep Hashing. For a broader overview, see this excellent recent survey.

### Loading The Data

We’ll build our hashing model in Python 3, training it on the publicly available CIFAR-10 dataset. As a benchmark, we’ll compare retrieval effectiveness against Locality Sensitive Hashing (LSH) with Gaussian sign random projections, using two common evaluation measures: precision@10 and semantic nearest neighbour accuracy.

As with any project, the first step is to set up a Python virtual environment to hold the code and dependencies (a sample [requirements.txt](https://learning2hash.github.io/tutorial/requirements.txt) is provided here).

Let’s start by creating a clean Python environment for the tutorial. Run the following commands in your terminal:

```
# 1. Create a new virtual environment called 'hashing\_tutorial'
python3 -m venv ./hashing\_tutorial

# 2. Activate the environment
source hashing\_tutorial/bin/activate

# 3. Install the required packages
pip3 install -r requirements.txt
```
💡 *Tip: If you ever want to leave the environment, just type deactivate.*

For this tutorial, we’ll use a pre-computed GIST descriptor version of CIFAR-10, stored in a .mat file. A .mat file is a MATLAB format commonly used to share pre-computed datasets in machine learning research. Don’t worry — we can easily load it in Python using scipy.io.

```
import os
import requests
import scipy.io
from sklearn.preprocessing import Normalizer

# 1. Download the dataset (stored as a .mat file)
url = "https://www.dropbox.com/s/875u1rkva9iffpj/Gist512CIFAR10.mat?dl=1"
response = requests.get(url)

# Save the file locally
with open("Gist512CIFAR10.mat", "wb") as f:
 f.write(response.content)

# 2. Load the .mat file
mat = scipy.io.loadmat("Gist512CIFAR10.mat")

# 3. Extract features (X) and classes (labels)
data = mat["X"]
classes = mat["X\_class"]

# 4. Pre-process the features
# - L2 normalisation
# - Zero-centering (subtracting mean)
data = Normalizer(norm="l2").fit\_transform(data)
data = data - data.mean(axis=0)
```
At this point, we have:

* **data** → a normalised feature matrix (one row per CIFAR-10 image)
* **classes** → the corresponding class labels

These are now ready to use for training and evaluating our hashing model.

The code above downloads the CIFAR-10 dataset (pre-processed into GIST features) and saves it to your working directory. To make sure similarity comparisons are meaningful, we L2-normalise and mean-centre the data before indexing.

👉 If you’d like to skip the step-by-step walkthrough, you can run the entire pipeline in one go:

```
python3 hashing\_tutorial.py
```
This script will:

1. Download and prepare the CIFAR-10 dataset
2. Train the Graph Regularised Hashing (GRH) model
3. Evaluate retrieval effectiveness on the CIFAR-10 images

👉 A full notebook with the experiments can also be found on [Google Colab](https://colab.research.google.com/drive/1l-2wt1rozorVZLpxSQQks10CVC3iiBxS?usp=sharing).

## Implementation — Locality Sensitive Hashing (LSH)

We’ll start with Gaussian sign random projections: project each vector onto random hyperplanes and take the sign to get a binary hash (1 bit per hyperplane). With 16 hyperplanes, we get a 16-bit hash.

```
import numpy as np
from collections import defaultdict

# --- Config ---
n\_bits = 16 # 16 hyperplanes → 16-bit hash
dim = data.shape[1] # should be 512 for GIST512
rng = np.random.default\_rng(0) # reproducibility

# --- Random hyperplanes (Gaussian) ---
random\_vectors = rng.standard\_normal((dim, n\_bits)) # shape: (512, 16)

# --- Hash a single image (example) ---
x0 = data[0] # one CIFAR-10 image vector
bits0 = (x0 @ random\_vectors) >= 0 # boolean hash bits
print("hash bits (bool):", bits0)
print("hash bits (0/1):", bits0.astype(int))
# e.g. [False True False True ...]The last line of code prints out the hashcode assigned to this image. Images with the exact same hashcode will collide in the same hashtable bucket. We would like these colliding images to be semantically similar, that is, to have the same class label.
```
Convert the boolean bits to a single bucket id (an integer) so we can index a hash table:

```
# Bits → integer bucket id (big-endian)
powers\_of\_two = (1 << np.arange(n\_bits - 1, -1, -1, dtype=np.uint64))
bucket0 = int(bits0.dot(powers\_of\_two))
print("bucket id:", bucket0)
```
Now hash the entire dataset and build the hash table:

```
# --- Hash all images ---
bit\_matrix = (data @ random\_vectors) >= 0 # shape: (N, 16)
bucket\_ids = bit\_matrix.dot(powers\_of\_two).astype(np.uint64) # shape: (N,)

# --- Build hash table: bucket\_id -> list of indices ---
table = defaultdict(list)
for idx, b in enumerate(bucket\_ids):
 table[int(b)].append(idx)

# Quick stats
N = data.shape[0]
bucket\_sizes = np.array([len(v) for v in table.values()])
print(f"N images: {N}, unique buckets: {len(table)}")
print(f"Avg bucket size: {bucket\_sizes.mean():.2f}, max collisions in a bucket: {bucket\_sizes.max()}")
```
Inspect a colliding bucket to see label consistency (good hashing ⇒ many items share the same class):

```
labels = classes.ravel() # flatten to shape (N,)

# Pick the first bucket with >= 2 items
example\_bucket, example\_idxs = next((b, idxs) for b, idxs in table.items() if len(idxs) >= 2)
print("example bucket:", example\_bucket)
print("indices:", example\_idxs)
print("labels:", labels[example\_idxs])

from collections import Counter
print("label counts:", Counter(labels[example\_idxs]))
```
**What to expect:**

* Some buckets will be “clean” (mostly one class) — great!
* Others will be mixed (several classes) — that’s the approximation trade-off with basic LSH.

```
example bucket: 21560
indices: [39378, 39502, 41761, 42070, 50364]
labels: [7 8 8 4 9]
label counts: Counter({8: 2, 7: 1, 4: 1, 9: 1})
```
Here we see that this bucket is mixed: only two images (label 8, *ship*) are semantically related, while the rest are from other classes.

```
indices: [42030, 42486, 43090, 47535, 50134, 50503]
labels: [4 4 4 1 1 4]
label counts: Counter({4: 4, 1: 2})
```
This time, LSH does much better: most of the colliding images belong to the same class (4 = *deer*), with just a couple of outliers (1 = *automobile*).

Next, we’ll move from spot-checks to a quantitative evaluation (e.g., *precision@10*) so we can compare LSH to a learned hashing model like Graph Regularised Hashing (GRH).

### Evaluation — Locality Sensitive Hashing (LSH)

To measure how well LSH works, we’ll use the precision@10 metric while varying the number of hash bits. Precision@10 tells us: *of the top 10 retrieved neighbours for a query, how many share the same class label?*

First, we split CIFAR-10 into three parts:

* **Queries** — used for retrieval
* **Training set** — to learn parameters (if needed)
* **Database** — the collection we search over

```
from sklearn.model\_selection import train\_test\_split

np.random.seed(0)

# Split into queries and the rest
data\_temp, data\_query, labels\_temp, labels\_query = train\_test\_split(
 data, classes[0, :], test\_size=0.002, random\_state=42
)

# Split the remainder into database and training
data\_database, data\_train, labels\_database, labels\_train = train\_test\_split(
 data\_temp, labels\_temp, test\_size=0.02, random\_state=42
)
```
This gives us:

* **120 queries** for testing retrieval
* **58,682 images** in the database
* **1,198 images** for training

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*2qgtG8ibl4bHP45QYfGocg.png)**Figure 4:** CIFAR-10 experimental setup. The benchmark is divided into three parts: 120 query images for evaluation, 1,198 images for training, and a large database of 58,682 images to search against.📖 Source: Image by author.To avoid overfitting and fairly measure performance, we keep a held-out database for retrieval, using the set of 120 queries. The training split is only used to learn any model parameters.

Next, we index the database with LSH, building the hash table:

```
# Step 1: Generate binary hash codes for each image in the database
bin\_indices\_bits = data\_database.dot(random\_vectors) >= 0 # shape: (N, n\_bits)

# Step 2: Convert each binary code into an integer bucket ID
bin\_indices = bin\_indices\_bits.dot(powers\_of\_two)

# Step 3: Build the hash table (bucket\_id -> list of image indices)
from collections import defaultdict
table = defaultdict(list)
for idx, bucket\_id in enumerate(bin\_indices):
 table[bucket\_id].append(idx)
```
**What this does:**

* Each image in the database is assigned a binary hash code (bin\_indices\_bits).
* That binary code is converted into a unique integer ID (bin\_indices) so it can be used as a hash bucket key.
* We then build a hash table (table) that groups together all images with the same bucket ID.

Now, similar images should “collide” in the same bucket — making nearest neighbour search much faster

With the index in place, we can now search for nearest neighbours using a Hamming radius search:

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/0*J5SbmK9iLf67Qdb2.png)**Figure 5:** Two ways to evaluate binary hashing for image retrieval. *(a)* **Hamming ranking** sorts the entire dataset by Hamming distance to the query and counts relevant items near the top. *(b)* **Hashtable buckets** use the hash function to jump directly to candidate buckets; relevance is judged over items retrieved from those buckets. 📖 Source: Image by author.This diagram illustrates Hamming radius search with a radius of zero.

In simple terms, the idea is:

* Start with the query’s hash bin.
* Also check nearby bins whose hash codes differ by up to *r* bits.
* The maximum radius *r* controls how far we search.

In Python, we can use itertools.combinations to enumerate all possible bit flips. For example, with a maximum radius of 10, we explore bins that differ from the query’s bin by 1, 2, …, up to 10 bits.

This way, we return not only the neighbours in the same bin, but also those in nearby bins, improving recall.

```
from itertools import combinations
from collections import defaultdict
from typing import Dict, Iterable, List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import pairwise\_distances
import time

def evaluate\_multiprobe\_lsh(
 data\_query: np.ndarray,
 labels\_query: np.ndarray,
 data\_database: np.ndarray,
 labels\_database: np.ndarray,
 random\_vectors: np.ndarray,
 powers\_of\_two: np.ndarray,
 buckets: Dict[int, List[int]],
 *,
 max\_search\_radius: int = 10,
 topk: int = 10,
 metric: str = "cosine",
) -> Tuple[pd.DataFrame, pd.DataFrame]:
 """
 Evaluate multi-probe LSH by sweeping Hamming search radius (0..max\_search\_radius).

 Args:
 data\_query: (Q, D) query vectors.
 labels\_query: (Q,) labels for queries.
 data\_database: (N, D) database vectors.
 labels\_database: (N,) labels for database items (same order as data\_database).
 random\_vectors: (D, B) projection matrix for hashing to B bits.
 powers\_of\_two: (B,) array mapping bit-vectors -> integer bucket ids via dot product.
 buckets: dict mapping bucket\_id -> list of database indices in that bucket.
 max\_search\_radius: maximum Hamming radius to probe.
 topk: precision@k cutoff.
 metric: distance metric used for re-ranking (e.g., "cosine").

 Returns:
 (mean\_time\_df, mean\_precision\_df)
 mean\_time\_df: columns: ["radius", "mean\_time\_seconds"]
 mean\_precision\_df: columns: ["radius", "mean\_precision\_at\_k"]
 """
 # History accumulators
 time\_history: Dict[int, List[float]] = defaultdict(list)
 precision\_history: Dict[int, List[float]] = defaultdict(list)

 # Precompute once: number of hash bits
 # We generate the query hash as a boolean vector (True = 1, False = 0).
 B = random\_vectors.shape[1]

 for q\_vec, q\_label in zip(data\_query, labels\_query):
 # Compute binary hash bits for the query
 # bin\_bits: shape (B,), dtype=bool
 bin\_bits = np.ravel((q\_vec @ random\_vectors) >= 0)

 # Candidate set grows monotonically with radius (multi-probe)
 candidate\_set: set = set()

 for radius in range(max\_search\_radius + 1):
 start = time.time()

 # Probe all buckets within Hamming distance == radius
 # Note: combinations(range(B), radius) can be large for big B and radius.
 # Keep max\_search\_radius modest or add a cap if needed.
 for flip\_idx in combinations(range(B), radius):
 alt\_bits = bin\_bits.copy()
 alt\_bits[list(flip\_idx)] = ~alt\_bits[list(flip\_idx)]
 bucket\_id = int(alt\_bits.dot(powers\_of\_two))
 if bucket\_id in buckets:
 candidate\_set.update(buckets[bucket\_id])

 # Re-rank candidates by true distance and compute precision@k
 if candidate\_set:
 cand\_idx = list(candidate\_set)
 cand\_vecs = data\_database[cand\_idx]
 cand\_labels = labels\_database[cand\_idx]

 # Distance to the single query vector (reshape to (1, D))
 dists = pairwise\_distances(
 cand\_vecs, q\_vec.reshape(1, -1), metric=metric
 ).ravel()

 # Build a small dataframe for readability
 nearest = (
 pd.DataFrame(
 {"id": cand\_idx, "label": cand\_labels, "distance": dists}
 )
 .sort\_values("distance", ascending=True)
 .reset\_index(drop=True)
 )

 top\_labels = nearest["label"].head(topk).tolist()
 precision\_at\_k = top\_labels.count(q\_label) / float(topk)
 precision\_history[radius].append(precision\_at\_k)
 else:
 # No candidates yet at this radius
 precision\_history[radius].append(0.0)

 elapsed = time.time() - start
 time\_history[radius].append(elapsed)

 # Aggregate across queries
 mean\_time = pd.DataFrame(
 {"radius": list(range(max\_search\_radius + 1)),
 "mean\_time\_seconds": [np.mean(time\_history[r]) for r in range(max\_search\_radius + 1)]}
 )

 mean\_precision = pd.DataFrame(
 {"radius": list(range(max\_search\_radius + 1)),
 "mean\_precision\_at\_k": [np.mean(precision\_history[r]) for r in range(max\_search\_radius + 1)]}
 )

 return mean\_time, mean\_precision
```
With a Hamming radius of 0, the above code yields a mean precision@10 of around 0.08. In practice, this means that out of the 10 retrieved images, fewer than 1 on average is relevant to the query.

As we increase the Hamming radius, retrieval quality improves — but at the cost of searching through many more candidate neighbours.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*blPSZAQZYGgCpW4krJ4ffw.png)**Figure 6:** *Increasing the search radius improves precision@10 rapidly at first, then flattens. Beyond the sweet spot (≈ radius 6), widening the radius yields little benefit.* 📖 Source: Image by author.As we increase the Hamming radius from 0 to 10, the candidate set grows larger. This improves recall but also slows queries — eventually approaching the cost of a full brute-force search (~53 seconds) once the candidate set covers the entire database.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*zXv6hbCwwgS5mvNr3F35Og.png)**Figure 7:** *Query time rises slowly at first but accelerates with radius expansion. Larger radii dramatically increase the cost of search.* 📖 Image by author.Next, we’ll see how learning the hashing hyperplanes (instead of generating them randomly, as in LSH) can boost performance to 0.25 mean precision@10 at Hamming radius 0.

## Implementation — Graph Regularised Hashing (GRH)

We now move from random hyperplanes (LSH) to learned hyperplanes. This is the essence of *Learning to Hash*: by training the hyperplanes, we can achieve much higher retrieval effectiveness. Our focus will be on the Graph Regularised Hashing (GRH) model, a supervised learning-to-hash approach.

To make the workings of GRH concrete, we’ll walk through a small running example throughout this section.

The figure above shows a toy dataset of cars and tigers. Each image is represented as a node in the graph, and edges connect semantic nearest neighbours (e.g. cars are linked to other cars, tigers to other tigers). The binary codes shown beside each image are the starting point, produced by Locality Sensitive Hashing (LSH). At this stage the codes are still noisy and random, but GRH will refine them over iterations so that similar images cluster together in hash space.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*XsfpHoGcTE-_ZBTW1m-npg.png)**Figure 8:** Example of graph regularisation in GRH. Each node represents an image, connected by edges that capture semantic similarity (cars linked to cars, tigers linked to tigers). The binary codes shown beside each image evolve during training so that connected images are nudged to have more similar hash codes, improving clustering and retrieval performance. 📖 Source: Image by author.### **Building the adjacency matrix**

GRH learns from a supervisory signal: an adjacency matrix that encodes which images are semantically related. If two images share the same class label, we set adjacency\_matrix[i, j] = 1; otherwise, it’s 0. Normalising by row ensures each row sums to 1.

```
# Construct adjacency matrix from training labels
adjacency\_matrix = np.equal.outer(labels\_train, labels\_train).astype(int)
row\_sums = adjacency\_matrix.sum(axis=1)
adjacency\_matrix = adjacency\_matrix / row\_sums[:, np.newaxis]
```
### **Learning the hash functions**

The GRH model (Moran & Lavrenko) is reminiscent of the Expectation-Maximisation (EM) algorithm:

* **Step 1:** Estimate hash codes given the current hyperplanes
* **Step 2:** Update the hyperplanes to better preserve semantic similarity

Slides from the original GRH talk (linked [here](https://www.slideshare.net/slideshow/graph-regularised-hashing-ecir15-talk/46413512)) walk through the details of this iterative process.

**Step A: Graph Regularisation**

GRH encourages similar points in the data graph to stay close in the hash space:

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*fRWF0niKJNmmDqCytERUrg.png)Update rule for graph regularisation in GRH: the new hash codes are computed as a weighted combination of the previous codes smoothed over the similarity graph, and the initial embedding, followed by a sign function to enforce binary outputs.Here:

* **S** is the similarity graph
* **D** is the degree matrix
* ɑ balances between the previous codes (**Lₘ**) and the initial embedding (**L₀**)

The first step in GRH uses the similarity graph of the data to refine the hash codes. Concretely, it multiplies the adjacency matrix (which captures which images are similar) by the current hash codes of the training images.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*3DIwDTX2a5vDP6Cal-YOIA.png)**Figure 9:** Graph representation of three images (three cars) with their initial binary hash codes in L. The similarity matrix S, degree matrix, and initial codes L₀ zero are shown below. These matrices define the structure that GRH uses to nudge similar items toward more consistent hash codes. 📖 Image by author.The graph above is a segment of the larger graph in **Figure 8**. It shows three images (three cars), each represented as a node with its current binary code. Edges in the graph are defined by the similarity matrix **S**, which captures how strongly two images are related. The degree matrix **D** normalises these connections, while the initial code matrix **L₀** contains the starting binary codes. By multiplying **S** and **D** with the current codes, GRH pulls semantically similar images closer together in hash space, gradually aligning their codes so that related images collide in the same hash bucket during retrieval — making search both faster and more accurate.

After the first round of graph regularisation, the hash codes are updated to **L₁.** This is done by multiplying the adjacency-weighted structure of the graph with the initial codes **L₀:**

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*BSWMPkrpkhLr-XgnN3edXA.png)**Figure 10:** First update of the hash codes after applying graph regularisation. The updated matrix **L₁** is obtained by multiplying the adjacency structure with the initial codes and then applying the sign function. 📖 Image by author.The sign function enforces binary outputs (+1 or -1). Intuitively, this step pulls connected nodes closer together in hash space — here, cars that are linked in the graph start to align their codes more closely. This process sharpens the codes so that semantically similar items (like cars) are more likely to share the same binary signature:

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*-E0ZMqBRUiScTQDHSVafDQ.png)**Figure 11:** Updated hash codes after the first graph regularisation step (**L₁**). The two car images (nodes a and b) now share identical binary codes, showing how GRH encourages semantically similar items to cluster in hash space. 📖 Image by author.Graph regularisation works by gradually adjusting the binary codes so that connected nodes in the graph become more alike. One way this happens is through **bit flipping**: if a code is inconsistent with its neighbours, GRH will flip specific bits to reduce the mismatch. We have seen how this happens in Figure 11, with node a having its second and third bits flipped. This also happens for the rest of the graph. In the figure below, node c flips its first bit, while node e flips its second bit. Over successive iterations, these small adjustments propagate through the graph, nudging semantically similar images into clusters with nearly identical codes. This iterative refinement is what gives GRH its ability to align the hash space with semantic similarity.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*R4XxV5upES5vuG6QU7l1Sw.png)**Figure 12:** Bit-flipping during graph regularisation. Nodes c and e update their codes by flipping individual bits so that they become more consistent with their neighbours in the similarity graph. 📖 Image by author.**Step B: Data-Space Partitioning**

In the second step, GRH turns the refined hash codes into actual decision boundaries in the data space. For each hash bit k = 1 … K, it learns a **linear classifier** (think of a Support Vector Machine, SVM) that splits the data into two halves:

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*mnS1SIE3WI4Jdk0QgtwMqA.png)Objective for learning each hash function in GRH: a margin-based optimisation (similar to SVM) that finds projection parameters while penalising misclassifications via slack variables. This ensures each hash bit is both discriminative and respects the data partitioning constraints.Intuitively, each hyperplane acts as a *“bit generator”:* points on one side get a +1, and those on the other side get a -1. By repeating this for all K bits, GRH builds a set of hash functions that map images into compact binary codes while respecting both the data structure and semantic similarity.

Once the hash codes have been refined in Step A, GRH improves the way data is split in hash space. For each hash bit, it trains a Support Vector Machine (SVM), using the bit values as the training labels. Instead of sticking with the random hyperplanes from Locality Sensitive Hashing (LSH), GRH starts with them as an initial guess and then iteratively sharpens these hyperplanes. Over time, the boundaries become better at separating semantically similar images (which should end up in the same bucket) from dissimilar ones (which should be pushed apart).

Each hyperplane corresponds to one hash bit. As shown in the figure below, this hyperplane for bit position 1 divides the data into two regions: the negative half-space (-1) above the line and the positive half-space (+1) below it. Every image receives its bit assignment based on which side of the hyperplane it falls.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*9YGnNkFZkisbxu1wI3JLpQ.png)**Figure 13:** Data-space partitioning in GRH. A learned hyperplane splits the data into two half-spaces. Images on one side are assigned a -1 for this bit, while those on the other side are assigned +1. 📖 Image by author.The figure below shows the second hyperplane, which determines **bit 2** of the hash codes. Images on the left of the line fall into the negative half-space and receive a -1, while those on the right belong to the positive half-space and receive a +1. By stacking together decisions from multiple hyperplanes, GRH produces compact binary codes where each bit reflects a meaningful partition of the data. Over time, these partitions align with semantic similarity, so related images end up with highly similar hash codes.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*nXbi8GuJ4eeDCJKs49QqHw.png)**Figure 14:** Step B of GRH: The second hyperplane partitions the data for bit 2. Points on the left fall into the negative (-1) half-space, while points on the right fall into the positive (+1) half-space.The entire Graph Regularised Hashing (GRH) algorithm brings the two steps together:

1. **Graph Regularisation** — refine the binary codes by encouraging similar points in the graph to stay close in hash space.
2. **Data-Space Partitioning** — update the hyperplanes by training an SVM for each hash bit.

These steps alternate for a fixed number of iterations, starting from random hyperplanes (as in LSH) and gradually improving them into semantically meaningful hash functions.

Here’s the full iterative process:

```
from sklearn.svm import LinearSVC

n\_iter = 2 # number of GRH iterations
alpha = 0.5 # weighting for supervision vs. original codes

for i in range(n\_iter):
 # --- Step 1: Generate initial binary codes with current hyperplanes ---
 bin\_indices\_bits = (data\_train.dot(random\_vectors) >= 0).astype(int)
 bin\_indices\_bits[bin\_indices\_bits == 0] = -1 # use -1/1 convention

 # --- Step 2: Refine codes using the adjacency matrix (supervision) ---
 refined\_bits = adjacency\_matrix @ bin\_indices\_bits.astype(float)
 refined\_bits = np.where(refined\_bits >= 0, 1, -1)

 # Blend original codes with refined ones (alpha controls weighting)
 bin\_indices\_bits = (alpha * refined\_bits + (1 - alpha) * bin\_indices\_bits).astype(float)
 bin\_indices\_bits = np.where(bin\_indices\_bits >= 0, 1, -1)

 # --- Step 3: Update hyperplanes with SVMs ---
 grh\_hyperplanes = random\_vectors.copy()
 for j in range(n\_vectors):
 # Edge case: if all bits are identical, reset hyperplane randomly
 if abs(bin\_indices\_bits[:, j].sum()) == data\_train.shape[0]:
 grh\_hyperplanes[:, j] = np.random.randn(dim)
 else:
 svm = LinearSVC(C=1.0, max\_iter=1000)
 svm.fit(data\_train, bin\_indices\_bits[:, j])
 grh\_hyperplanes[:, j] = svm.coef\_.ravel()

 # Update random\_vectors for the next iteration
 random\_vectors = grh\_hyperplanes.copy()
```
In this implementation, I’ve set GRH with *n\_iter = 2* and *alpha = 0.25*.

Iterations (*n\_iter*) control how many times we repeat the two GRH steps:

1. Refine hash codes with the adjacency matrix
2. Update hyperplanes using those refined codes

After two iterations, the *random\_vectors* matrix contains refined hyperplanes — tuned to preserve semantic similarity from the training data (via the adjacency matrix). These hyperplanes can now be used in the same way as before (with a hash table and Hamming radius search) to evaluate retrieval performance.

### Evaluation — Graph Regularised Hashing (GRH)

With a solid grasp of how GRH works, we can now evaluate its hash codes using the same methodology as LSH.

The results show a clear improvement in retrieval effectiveness, especially at lower Hamming radii, where GRH is able to capture semantic similarity much better than random hyperplanes.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*zGM2a6TbRsz6tpf_L510VQ.png)**Figure 15:** *GRH achieves higher precision at small radii (early advantage), while both methods converge to similar accuracy as the radius widens.* Source: 📖 Image by author.The benefits of GRH with a 16-bit hash code are most visible at low Hamming radii (≤ 5).

* At radius 0, GRH achieves ~0.25 mean precision@10, compared to only ~0.1 for LSH.
* Importantly, query times remain similar for both methods (~0.5 seconds at low radii).

The curve below shows GRH query times as the Hamming radius increases. In some cases, we pay a small cost in speed for the significant boost in retrieval effectiveness — a worthwhile trade-off in practice.

Press enter or click to view image in full size![](https://miro.medium.com/v2/resize:fit:1400/1*Uw_JJjxk40mfYUQsOc59Fg.png)***Figure 16:*** *GRH is slightly faster at small radii, but LSH’s query time grows faster at large radii, showing the trade-off between accuracy and cost*. 📖 Image by author.## Conclusion and Final Thoughts

In this tutorial, we implemented a data-independent approach for hashing-based nearest neighbour search — Locality Sensitive Hashing (LSH) — and evaluated its quality and speed.

To improve retrieval effectiveness, we introduced *Learning to Hash*, where machine learning is used to train better hyperplanes. Using Graph Regularised Hashing (GRH), we demonstrated clear improvements on an image retrieval task.

GRH learns its hyperplanes with Support Vector Machines (SVMs), but is not tied to any specific learner. In practice, this means you could swap in a deep neural network for more accurate partitions, or a lightweight passive-aggressive classifier for fast, online adaptation in streaming scenarios.

In this article, we focused on a single-hashtable implementation of LSH and GRH, and boosted retrieval further with multi-probing (searching nearby buckets). An alternative approach is to use multiple independent hash tables instead of probing within one — a valuable direction for a future deep dive.

For readers eager to explore Learning to Hash in more depth, *Awesome Learning to Hash* is a curated, and regularly updated resource. It offers:

* **Access to over 3,000 key papers**, all organised by topic, method, and application domain
* Browsing tools: search by tag or author, view papers in a 2D topic map, explore tutorial materials and practical tools
* A community‑driven structure — with an option to contribute your own papers or suggest updates

Check it out here: [Awesome Learning to Hash](https://learning2hash.github.io/).

*Disclaimer: The views and opinions expressed in this article are my own and do not represent those of my employer or any affiliated organizations. The content is based on personal experience and reflection, and should not be taken as professional or academic advice.*

## 📚 Further Learning

If you’d like to dive deeper into the theory and practice of hashing for similarity search and large-scale learning, here are some foundational and advanced resources:

* [**Moses Charikar — Similarity Estimation Techniques from Rounding Algorithms:**](https://www.cs.princeton.edu/courses/archive/spr04/cos598B/bib/CharikarEstim.pdf) The seminal paper that introduced SimHash, a cornerstone in locality-sensitive hashing and similarity search.
* [**Anand Rajaraman & Jeffrey Ullman — Mining of Massive Datasets:**](http://www.mmds.org) A freely available textbook covering large-scale data mining techniques, including LSH and approximate nearest neighbour search.
* [**Sean Moran & Victor Lavrenko — Graph Regularised Hashing:**](https://link.springer.com/chapter/10.1007/978-3-319-16354-3_15) A method that incorporates graph structure into the hashing process, improving similarity preservation for information retrieval tasks.
* [**Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang & Shih-Fu Chang — Supervised Hashing with Kernels:**](https://ieeexplore.ieee.org/abstract/document/6247912/footnotes#footnotes) A supervised approach that leverages kernel methods to generate compact binary codes for efficient retrieval.
* [**Wu-Jun Li, Sheng Wang & Wang-Cheng Kang — Feature Learning Based Deep Supervised Hashing with Pairwise Labels:**](https://arxiv.org/abs/1511.03855)Early work that combines feature learning with supervised hashing, paving the way for deep learning methods in similarity search.
* [**Xiao Luo, Haixin Wang, Daqing Wu, Chong Chen, Minghua Deng, Jianqiang Huang & Xian-Sheng Hua — A Survey on Deep Hashing Methods:**](https://arxiv.org/abs/2003.03369) A comprehensive survey that reviews advances in deep hashing, providing an overview of key architectures, methods, and applications.
* [**Awesome Learning to Hash:**](https://learning2hash.github.io) For readers eager to explore Learning to Hash in more depth, *Awesome Learning to Hash* is a curated, and regularly updated resource. Access to over 3,000 key papers, all organised by topic, method, and application domain

👉 These readings will give you a solid path from the theoretical underpinnings of hashing, through classical supervised methods, and into the modern era of deep hashing research and applications.
