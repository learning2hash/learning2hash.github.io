---
layout: default
title: Tutorial
comments: true
---

# Needles, Haystacks and Hashing: Smarter Search with AI

**Author:** Sean Moran  
**Published:** Jan 11, 2024  

🔗 [Read on Medium](https://medium.com/@sean.j.moran/learning-to-hash-finding-the-needle-in-the-haystack-with-ai-24a15f85de0e)

## **How to find What You’re Looking For — Fast**

This article offers a hands-on introduction to machine learning approaches for nearest neighbour search, a field often called *Learning to Hash* or *Semantic Hashing*.

You’ll learn about:

* Why nearest neighbour search matters in computer science
* Approximate methods that dramatically speed up search
* Locality Sensitive Hashing (LSH), a widely used algorithm
* How machine learning can make neighbour search even more effective

👉 A fully annotated notebook with the experiments can be found on [**Google Colab**](https://colab.research.google.com/drive/1l-2wt1rozorVZLpxSQQks10CVC3iiBxS?usp=sharing).

![](https://miro.medium.com/v2/resize:fit:1024/1*JzcGGbGxBLwn7FuNeiQy7g.png)  
*Figure 1: Searching for a needle in a haystack: a metaphor for similarity search at scale. Without efficient indexing, finding a single relevant item in massive datasets can be overwhelming. Source: Image generated by author using DALL·E.*

## **Why Nearest Neighbour Search Matters**

[Nearest neighbour search](https://en.wikipedia.org/wiki/Nearest_neighbor_search) is the core computer science task of finding the most similar data points to a query within a database. It’s a fundamental matching operation with wide applications across fields like bioinformatics, natural language processing (NLP), and computer vision.

It also powers today’s vector databases, the backbone of Retrieval-Augmented Generation (RAG) with large language models (LLMs).

Here are just a few places where nearest neighbour search and hashing shine:

* [**Code Search**](https://arxiv.org/abs/2111.04473): Scalable source code search engines (e.g. using MinHash) for large repositories.
* [**Efficient Transformers**](https://openreview.net/pdf?id=rkgNKkHtvB): Speeding up NLP models by applying LSH inside Transformer architectures.
* [**Social Media Monitoring**](https://www.aclweb.org/anthology/P14-5007): Real-time event detection and tracking.
* [**Seismic Analysis**](https://dawnd9.sites.stanford.edu/news/earthquake-hunting-efficient-time-series-similarity-search): Detecting earthquakes via time-series similarity search.
* [**Fraud Detection**](https://www.uber.com/en-GB/blog/lsh/): Uber applies LSH to spot suspiciously similar rides.
* [**Audio Fingerprinting**](https://santhoshhari.github.io/Locality-Sensitive-Hashing/): Matching snippets of audio to massive song databases (think *Shazam*!).
* [**Genomics**](https://pubmed.ncbi.nlm.nih.gov/26006009/): Assembling genomes and matching gene expressions across large datasets.
* [**Image Retrieval**](https://research.google/pubs/visualrank-applying-pagerank-to-large-scale-image-search/): Google combines LSH with PageRank to index billions of images.
* [**Malware Detection**](https://media.kaspersky.com/en/enterprise-security/Kaspersky-Lab-Whitepaper-Machine-Learning.pdf): Anti-virus tools hash suspicious code snippets to flag known malware.

And the list goes on.

In this article, we’ll show how approximate nearest neighbour (ANN) methods make search dramatically faster. By using hashing, ANN achieves sub-linear retrieval times, meaning search grows much more slowly than dataset size. To see why this matters, compare linear vs. sub-linear growth: as your dataset scales, sub-linear time stays manageable, while linear time quickly becomes impractical.

![](https://miro.medium.com/v2/resize:fit:1400/1*FxrM899j_4Y2mt6ZZe3c7Q.png)  
*Figure 2: As datasets grow, the efficiency gap between linear scans (O(N)) and sub-linear methods (O(log N)) widens dramatically. Index-based lookups such as binary search or B-trees scale far better than brute-force scans, making sub-linear approaches essential for large-scale retrieval. Source: Image by author via GPT-5.*

Hashing algorithms generate binary hash codes that preserve similarity: semantically similar data points map to similar codes. These codes index items (images, documents, etc.) into hash table buckets, where similar points ideally land in the same bucket. The process is shown in the diagram below.

The entire process is illustrated in the diagram below:

![](https://miro.medium.com/v2/resize:fit:1400/1*FvoeUBLingGRjCqLwmgqYQ.png)  
*Figure 3: Hashing for fast image retrieval. A query image is mapped to a compact binary code, which is then used to look up candidate images in a hashtable. Only those candidates are compared for similarity, yielding efficient nearest neighbour search. Applications include content-based image retrieval, near-duplicate detection, and location recognition. Source: Image by author. Diagram adapted from the [PhD thesis of Sean Moran](https://learning2hash.github.io/cite.html).*

Given a query — say, the tiger image above — we generate its hash code and only compare it against data points in the same hash table bucket (or across multiple buckets if several hash tables are used). Because each bucket usually contains far fewer items than the full dataset, search is much faster than brute force. The trade-off is that we may not always retrieve the exact nearest neighbour, but the speed gain typically outweighs the small loss in accuracy.

In this article, we explore a published *Learning to Hash* model and compare its image-retrieval performance to Locality Sensitive Hashing (LSH). In particular, we focus on [Graph Regularised Hashing (GRH)](https://learning2hash.github.io/publications/moran2015graph/) — a simple yet effective supervised hashing approach — later extended to [cross-modal hashing](https://dl.acm.org/doi/abs/10.1145/2766462.2767816) for retrieving across both images and text.

## **Learning to Hash — Optimising Retrieval Effectiveness with AI**

There are many ways to generate hash codes. Some methods are data-independent, relying on random functions with special properties, while more recent approaches learn the codes directly from data.

The best-known data-independent approach is Locality Sensitive Hashing (LSH), which has been widely written about. In this article, however, we focus on learning hash functions with AI — often called *Semantic Hashing* or *Learning to Hash*.

Our case study is Graph Regularised Hashing (GRH), an intuitive supervised hashing model that strikes a good balance between simplicity and effectiveness. We compare GRH against LSH on the benchmark task of image retrieval. GRH has also been extended to cross-modal hashing, enabling retrieval across both images and text.

There are many open-source Learning to Hash models available. I have chosen GRH because it clearly illustrates the core learning principles without requiring heavy mathematical detail. Other influential models include Supervised Hashing with Kernels and Deep Hashing. For a broader overview, see this excellent recent survey.

### Loading The Data

We’ll build our hashing model in Python 3, training it on the publicly available CIFAR-10 dataset. As a benchmark, we’ll compare retrieval effectiveness against Locality Sensitive Hashing (LSH) with Gaussian sign random projections, using two common evaluation measures: precision@10 and semantic nearest neighbour accuracy.

As with any project, the first step is to set up a Python virtual environment to hold the code and dependencies (a sample [requirements.txt](https://learning2hash.github.io/tutorial/requirements.txt) is provided here).

Let’s start by creating a clean Python environment for the tutorial. Run the following commands in your terminal:

```
# 1. Create a new virtual environment called 'hashing_tutorial'
python3 -m venv ./hashing_tutorial

# 2. Activate the environment
source hashing_tutorial/bin/activate

# 3. Install the required packages
pip3 install -r requirements.txt
```

💡 *Tip: If you ever want to leave the environment, just type `deactivate`.*

For this tutorial, we’ll use a pre-computed GIST descriptor version of CIFAR-10, stored in a .mat file. A .mat file is a MATLAB format commonly used to share pre-computed datasets in machine learning research. Don’t worry — we can easily load it in Python using `scipy.io`.

```
import os
import requests
import scipy.io
from sklearn.preprocessing import Normalizer

# 1. Download the dataset (stored as a .mat file)
url = "https://www.dropbox.com/s/875u1rkva9iffpj/Gist512CIFAR10.mat?dl=1"
response = requests.get(url)

# Save the file locally
with open("Gist512CIFAR10.mat", "wb") as f:
    f.write(response.content)

# 2. Load the .mat file
mat = scipy.io.loadmat("Gist512CIFAR10.mat")

# 3. Extract features (X) and classes (labels)
data = mat["X"]
classes = mat["X_class"]

# 4. Pre-process the features
# - L2 normalisation
# - Zero-centering (subtracting mean)
data = Normalizer(norm="l2").fit_transform(data)
data = data - data.mean(axis=0)
```

At this point, we have:

* **data** → a normalised feature matrix (one row per CIFAR-10 image)
* **classes** → the corresponding class labels

These are now ready to use for training and evaluating our hashing model.

The code above downloads the CIFAR-10 dataset (pre-processed into GIST features) and saves it to your working directory. To make sure similarity comparisons are meaningful, we L2-normalise and mean-centre the data before indexing.

👉 If you’d like to skip the step-by-step walkthrough, you can run the entire pipeline in one go:

```
python3 hashing_tutorial.py
```

This script will:

1. Download and prepare the CIFAR-10 dataset  
2. Train the Graph Regularised Hashing (GRH) model  
3. Evaluate retrieval effectiveness on the CIFAR-10 images

👉 A full notebook with the experiments can also be found on [Google Colab](https://colab.research.google.com/drive/1l-2wt1rozorVZLpxSQQks10CVC3iiBxS?usp=sharing).

## Implementation — Locality Sensitive Hashing (LSH)

We’ll start with Gaussian sign random projections: project each vector onto random hyperplanes and take the sign to get a binary hash (1 bit per hyperplane). With 16 hyperplanes, we get a 16-bit hash.

```
import numpy as np
from collections import defaultdict

# --- Config ---
n_bits = 16  # 16 hyperplanes → 16-bit hash
dim = data.shape[1]  # should be 512 for GIST512
rng = np.random.default_rng(0)  # reproducibility

# --- Random hyperplanes (Gaussian) ---
random_vectors = rng.standard_normal((dim, n_bits))  # shape: (512, 16)

# --- Hash a single image (example) ---
x0 = data[0]  # one CIFAR-10 image vector
bits0 = (x0 @ random_vectors) >= 0  # boolean hash bits
print("hash bits (bool):", bits0)
print("hash bits (0/1):", bits0.astype(int))
```

The last line prints the hashcode assigned to this image. Images with the exact same hashcode will collide in the same hashtable bucket. We would like these colliding images to be semantically similar, i.e., to have the same class label.

Convert the boolean bits to a single bucket id (an integer) so we can index a hash table:

```
# Bits → integer bucket id (big-endian)
powers_of_two = (1 << np.arange(n_bits - 1, -1, -1, dtype=np.uint64))
bucket0 = int(bits0.dot(powers_of_two))
print("bucket id:", bucket0)
```

Now hash the entire dataset and build the hash table:

```
# --- Hash all images ---
bit_matrix = (data @ random_vectors) >= 0              # shape: (N, 16)
bucket_ids = bit_matrix.dot(powers_of_two).astype(np.uint64)  # shape: (N,)

# --- Build hash table: bucket_id -> list of indices ---
table = defaultdict(list)
for idx, b in enumerate(bucket_ids):
    table[int(b)].append(idx)

# Quick stats
N = data.shape[0]
bucket_sizes = np.array([len(v) for v in table.values()])
print(f"N images: {N}, unique buckets: {len(table)}")
print(f"Avg bucket size: {bucket_sizes.mean():.2f}, max collisions in a bucket: {bucket_sizes.max()}")
```

Inspect a colliding bucket to see label consistency (good hashing ⇒ many items share the same class):

```
labels = classes.ravel()  # flatten to shape (N,)

# Pick the first bucket with >= 2 items
example_bucket, example_idxs = next((b, idxs) for b, idxs in table.items() if len(idxs) >= 2)
print("example bucket:", example_bucket)
print("indices:", example_idxs)
print("labels:", labels[example_idxs])

from collections import Counter
print("label counts:", Counter(labels[example_idxs]))
```

**What to expect:**

* Some buckets will be “clean” (mostly one class) — great!  
* Others will be mixed (several classes) — that’s the approximation trade-off with basic LSH.

```
example bucket: 21560
indices: [39378, 39502, 41761, 42070, 50364]
labels: [7 8 8 4 9]
label counts: Counter({8: 2, 7: 1, 4: 1, 9: 1})
```

Here we see that this bucket is mixed: only two images (label 8, *ship*) are semantically related, while the rest are from other classes.

```
indices: [42030, 42486, 43090, 47535, 50134, 50503]
labels: [4 4 4 1 1 4]
label counts: Counter({4: 4, 1: 2})
```

This time, LSH does much better: most of the colliding images belong to the same class (4 = *deer*), with just a couple of outliers (1 = *automobile*).

Next, we’ll move from spot-checks to a quantitative evaluation (e.g., *precision@10*) so we can compare LSH to a learned hashing model like Graph Regularised Hashing (GRH).

### Evaluation — Locality Sensitive Hashing (LSH)

To measure how well LSH works, we’ll use the precision@10 metric while varying the number of hash bits. Precision@10 tells us: *of the top 10 retrieved neighbours for a query, how many share the same class label?*

First, we split CIFAR-10 into three parts:

* **Queries** — used for retrieval  
* **Training set** — to learn parameters (if needed)  
* **Database** — the collection we search over

```
from sklearn.model_selection import train_test_split

np.random.seed(0)

# Split into queries and the rest
data_temp, data_query, labels_temp, labels_query = train_test_split(
    data, classes[0, :], test_size=0.002, random_state=42
)

# Split the remainder into database and training
data_database, data_train, labels_database, labels_train = train_test_split(
    data_temp, labels_temp, test_size=0.02, random_state=42
)
```

This gives us:

* **120 queries** for testing retrieval  
* **58,682 images** in the database  
* **1,198 images** for training

![](https://miro.medium.com/v2/resize:fit:1400/1*2qgtG8ibl4bHP45QYfGocg.png)  
*Figure 4: CIFAR-10 experimental setup. The benchmark is divided into three parts: 120 query images for evaluation, 1,198 images for training, and a large database of 58,682 images to search against. Source: Image by author.*

To avoid overfitting and fairly measure performance, we keep a held-out database for retrieval, using the set of 120 queries. The training split is only used to learn any model parameters.

Next, we index the database with LSH, building the hash table:

```
# Step 1: Generate binary hash codes for each image in the database
bin_indices_bits = data_database.dot(random_vectors) >= 0  # shape: (N, n_bits)

# Step 2: Convert each binary code into an integer bucket ID
bin_indices = bin_indices_bits.dot(powers_of_two)

# Step 3: Build the hash table (bucket_id -> list of image indices)
from collections import defaultdict
table = defaultdict(list)
for idx, bucket_id in enumerate(bin_indices):
    table[bucket_id].append(idx)
```

**What this does:**

* Each image in the database is assigned a binary hash code (`bin_indices_bits`).  
* That binary code is converted into a unique integer ID (`bin_indices`) so it can be used as a hash bucket key.  
* We then build a hash table (`table`) that groups together all images with the same bucket ID.

Now, similar images should “collide” in the same bucket — making nearest neighbour search much faster.

With the index in place, we can now search for nearest neighbours using a Hamming radius search:

![](https://miro.medium.com/v2/resize:fit:1400/0*J5SbmK9iLf67Qdb2.png)  
*Figure 5: Two ways to evaluate binary hashing for image retrieval. (a) **Hamming ranking** sorts the entire dataset by Hamming distance to the query and counts relevant items near the top. (b) **Hashtable buckets** use the hash function to jump directly to candidate buckets; relevance is judged over items retrieved from those buckets. Source: Image by author.*

This diagram illustrates Hamming radius search with a radius of zero.

In simple terms, the idea is:

* Start with the query’s hash bin.  
* Also check nearby bins whose hash codes differ by up to *r* bits.  
* The maximum radius *r* controls how far we search.

In Python, we can use `itertools.combinations` to enumerate all possible bit flips. For example, with a maximum radius of 10, we explore bins that differ from the query’s bin by 1, 2, …, up to 10 bits.

This way, we return not only the neighbours in the same bin, but also those in nearby bins, improving recall.
