[["wang2023aligning", "Aligning Large Language Models With Human A Survey"], ["rose2023personalisation", "Personalisation Within Bounds A Risk Taxonomy And Policy Framework For The Alignment Of Large Language Models With Personalised Feedback"], ["yu2023rlhf", "RLHF-V Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback"], ["zong2024safety", "Safety Fine-tuning At (almost) No Cost A Baseline For Vision Large Language Models"]]