[["rafailov2023direct", "Direct Preference Optimization Your Language Model Is Secretly A Reward Model"], ["yu2023rlhf", "RLHF-V Towards Trustworthy Mllms Via Behavior Alignment From Fine-grained Correctional Human Feedback"], ["huang2023language", "Language Is Not All You Need Aligning Perception With Language Models"], ["nakano2021webgpt", "Webgpt Browser-assisted Question-answering With Human Feedback"]]