[["soltan2022alexatm", "Alexatm 20B Few-shot Learning Using A Large-scale Multilingual Seq2seq Model"], ["smith2022using", "Using Deepspeed And Megatron To Train Megatron-turing NLG 530B A Large-scale Generative Language Model"], ["zeng2021pangu", "Pangu-(\u03b1) Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation"], ["rajbhandari2019zero", "Zero Memory Optimizations Toward Training Trillion Parameter Models"]]