[["kim2021what", "What Changes Can Large-scale Language Models Bring Intensive Study On Hyperclova Billions-scale Korean Generative Pretrained Transformers"], ["gu2023pre", "Pre-training To Learn In Context"], ["shi2023pretraining", "In-context Pretraining Language Modeling Beyond Document Boundaries"], ["b2020language", "Language Models Are Few-shot Learners"]]