[["shoeybi2019megatron", "Megatron-lm Training Multi-billion Parameter Language Models Using Model Parallelism"], ["ren2023pangu", "Pangu-\u03c3 Towards Trillion Parameter Language Model With Sparse Heterogeneous Computing"], ["smith2022using", "Using Deepspeed And Megatron To Train Megatron-turing NLG 530B A Large-scale Generative Language Model"], ["du2021glam", "Glam Efficient Scaling Of Language Models With Mixture-of-experts"]]