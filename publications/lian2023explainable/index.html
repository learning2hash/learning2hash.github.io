<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Explainable Multimodal Emotion Recognition | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Explainable Multimodal Emotion Recognition" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Multimodal emotion recognition is an important research topic in artificial intelligence whose main goal is to integrate multimodal clues to identify human emotional states. Current works generally assume accurate labels for benchmark datasets and focus on developing more effective architectures. However emotion annotation relies on subjective judgment. To obtain more reliable labels existing datasets usually restrict the label space to some basic categories then hire plenty of annotators and use majority voting to select the most likely label. However this process may result in some correct but non-candidate or non-majority labels being ignored. To ensure reliability without ignoring subtle emotions we propose a new task called Explainable Multimodal Emotion Recognition (EMER). Unlike traditional emotion recognition EMER takes a step further by providing explanations for these predictions. Through this task we can extract relatively reliable labels since each label has a certain basis. Meanwhile we borrow large language models (LLMs) to disambiguate unimodal clues and generate more complete multimodal explanations. From them we can extract richer emotions in an open-vocabulary manner. This paper presents our initial attempt at this task including introducing a new dataset establishing baselines and defining evaluation metrics. In addition EMER can serve as a benchmark task to evaluate the audio-video-text understanding performance of multimodal LLMs." />
<meta property="og:description" content="Multimodal emotion recognition is an important research topic in artificial intelligence whose main goal is to integrate multimodal clues to identify human emotional states. Current works generally assume accurate labels for benchmark datasets and focus on developing more effective architectures. However emotion annotation relies on subjective judgment. To obtain more reliable labels existing datasets usually restrict the label space to some basic categories then hire plenty of annotators and use majority voting to select the most likely label. However this process may result in some correct but non-candidate or non-majority labels being ignored. To ensure reliability without ignoring subtle emotions we propose a new task called Explainable Multimodal Emotion Recognition (EMER). Unlike traditional emotion recognition EMER takes a step further by providing explanations for these predictions. Through this task we can extract relatively reliable labels since each label has a certain basis. Meanwhile we borrow large language models (LLMs) to disambiguate unimodal clues and generate more complete multimodal explanations. From them we can extract richer emotions in an open-vocabulary manner. This paper presents our initial attempt at this task including introducing a new dataset establishing baselines and defining evaluation metrics. In addition EMER can serve as a benchmark task to evaluate the audio-video-text understanding performance of multimodal LLMs." />
<link rel="canonical" href="https://learning2hash.github.io/publications/lian2023explainable/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/lian2023explainable/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Explainable Multimodal Emotion Recognition" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Multimodal emotion recognition is an important research topic in artificial intelligence whose main goal is to integrate multimodal clues to identify human emotional states. Current works generally assume accurate labels for benchmark datasets and focus on developing more effective architectures. However emotion annotation relies on subjective judgment. To obtain more reliable labels existing datasets usually restrict the label space to some basic categories then hire plenty of annotators and use majority voting to select the most likely label. However this process may result in some correct but non-candidate or non-majority labels being ignored. To ensure reliability without ignoring subtle emotions we propose a new task called Explainable Multimodal Emotion Recognition (EMER). Unlike traditional emotion recognition EMER takes a step further by providing explanations for these predictions. Through this task we can extract relatively reliable labels since each label has a certain basis. Meanwhile we borrow large language models (LLMs) to disambiguate unimodal clues and generate more complete multimodal explanations. From them we can extract richer emotions in an open-vocabulary manner. This paper presents our initial attempt at this task including introducing a new dataset establishing baselines and defining evaluation metrics. In addition EMER can serve as a benchmark task to evaluate the audio-video-text understanding performance of multimodal LLMs.","headline":"Explainable Multimodal Emotion Recognition","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/lian2023explainable/"},"url":"https://learning2hash.github.io/publications/lian2023explainable/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Explainable Multimodal Emotion Recognition</h1>
  <h5>Zheng Lian, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, Shun Chen, Mingyu Xu, Ke Xu, Kang Chen, Lan Chen, Shan Liang, Ya Li, Jiangyan Yi, Bin Liu, Jianhua Tao. Arxiv 2023</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2306.15401v6" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Multimodal Emotion Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Explainable Multimodal Emotion Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Cross Modal">Cross Modal</a></tag>
    
  </p>
  <p><p>Multimodal emotion recognition is an important research topic in artificial intelligence whose main goal is to integrate multimodal clues to identify human emotional states. Current works generally assume accurate labels for benchmark datasets and focus on developing more effective architectures. However emotion annotation relies on subjective judgment. To obtain more reliable labels existing datasets usually restrict the label space to some basic categories then hire plenty of annotators and use majority voting to select the most likely label. However this process may result in some correct but non-candidate or non-majority labels being ignored. To ensure reliability without ignoring subtle emotions we propose a new task called Explainable Multimodal Emotion Recognition (EMER). Unlike traditional emotion recognition EMER takes a step further by providing explanations for these predictions. Through this task we can extract relatively reliable labels since each label has a certain basis. Meanwhile we borrow large language models (LLMs) to disambiguate unimodal clues and generate more complete multimodal explanations. From them we can extract richer emotions in an open-vocabulary manner. This paper presents our initial attempt at this task including introducing a new dataset establishing baselines and defining evaluation metrics. In addition EMER can serve as a benchmark task to evaluate the audio-video-text understanding performance of multimodal LLMs.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/lian2023explainable.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
