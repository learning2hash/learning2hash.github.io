<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Shaving Logs Via Large Sieve Inequality: Faster Algorithms For Sparse Convolution And More | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Shaving Logs Via Large Sieve Inequality: Faster Algorithms For Sparse Convolution And More" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In sparse convolution-type problems, a common technique is to hash the input integers modulo a random prime (p\in [Q/2,Q]) for some parameter (Q), which reduces the range of the input integers while preserving their additive structure. However, this hash family suffers from two drawbacks, which led to bottlenecks in many state-of-the-art algorithms: (1) The collision probability of two elements from ([N]) is (O(\frac{log N}{Q})) rather than (O(\frac{1}{Q})); (2) It is difficult to derandomize the choice of (p); known derandomization techniques lead to super-logarithmic overhead [Chan, Lewenstein STOC’15]. In this paper, we partially overcome these drawbacks in certain scenarios, via novel applications of the large sieve inequality from analytic number theory. Consequently, we obtain the following improved algorithms for various problems (in the standard word RAM model): Sparse Nonnegative Convolution: We obtain an (O(tlog t))-time Las Vegas algorithm that computes the convolution (A\star B) of two nonnegative integer vectors (A,B), where (t) is the output sparsity (|A\star B|_0). Moreover, our algorithm terminates in (O(tlog t)) time with (1-1/\mathrm{poly}(t)) probability. Text-to-Pattern Hamming Distances: Given a length-(m) pattern (P) and a length-(n) text (T), we obtain a deterministic (O(n\sqrt{mlog log m}))-time algorithm that exactly computes the Hamming distance between (P) and every length-(m) substring of (T). Sparse General Convolution: We also give a Monte Carlo (O(tlog t)) time algorithm for sparse convolution with possibly negative input in the restricted case where the length (N) of the input vectors satisfies (N\le t^{1.99})." />
<meta property="og:description" content="In sparse convolution-type problems, a common technique is to hash the input integers modulo a random prime (p\in [Q/2,Q]) for some parameter (Q), which reduces the range of the input integers while preserving their additive structure. However, this hash family suffers from two drawbacks, which led to bottlenecks in many state-of-the-art algorithms: (1) The collision probability of two elements from ([N]) is (O(\frac{log N}{Q})) rather than (O(\frac{1}{Q})); (2) It is difficult to derandomize the choice of (p); known derandomization techniques lead to super-logarithmic overhead [Chan, Lewenstein STOC’15]. In this paper, we partially overcome these drawbacks in certain scenarios, via novel applications of the large sieve inequality from analytic number theory. Consequently, we obtain the following improved algorithms for various problems (in the standard word RAM model): Sparse Nonnegative Convolution: We obtain an (O(tlog t))-time Las Vegas algorithm that computes the convolution (A\star B) of two nonnegative integer vectors (A,B), where (t) is the output sparsity (|A\star B|_0). Moreover, our algorithm terminates in (O(tlog t)) time with (1-1/\mathrm{poly}(t)) probability. Text-to-Pattern Hamming Distances: Given a length-(m) pattern (P) and a length-(n) text (T), we obtain a deterministic (O(n\sqrt{mlog log m}))-time algorithm that exactly computes the Hamming distance between (P) and every length-(m) substring of (T). Sparse General Convolution: We also give a Monte Carlo (O(tlog t)) time algorithm for sparse convolution with possibly negative input in the restricted case where the length (N) of the input vectors satisfies (N\le t^{1.99})." />
<link rel="canonical" href="https://learning2hash.github.io/publications/jin2024shaving/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/jin2024shaving/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-12T10:59:29-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Shaving Logs Via Large Sieve Inequality: Faster Algorithms For Sparse Convolution And More" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-12T10:59:29-05:00","datePublished":"2025-08-12T10:59:29-05:00","description":"In sparse convolution-type problems, a common technique is to hash the input integers modulo a random prime (p\\in [Q/2,Q]) for some parameter (Q), which reduces the range of the input integers while preserving their additive structure. However, this hash family suffers from two drawbacks, which led to bottlenecks in many state-of-the-art algorithms: (1) The collision probability of two elements from ([N]) is (O(\\frac{log N}{Q})) rather than (O(\\frac{1}{Q})); (2) It is difficult to derandomize the choice of (p); known derandomization techniques lead to super-logarithmic overhead [Chan, Lewenstein STOC’15]. In this paper, we partially overcome these drawbacks in certain scenarios, via novel applications of the large sieve inequality from analytic number theory. Consequently, we obtain the following improved algorithms for various problems (in the standard word RAM model): Sparse Nonnegative Convolution: We obtain an (O(tlog t))-time Las Vegas algorithm that computes the convolution (A\\star B) of two nonnegative integer vectors (A,B), where (t) is the output sparsity (|A\\star B|_0). Moreover, our algorithm terminates in (O(tlog t)) time with (1-1/\\mathrm{poly}(t)) probability. Text-to-Pattern Hamming Distances: Given a length-(m) pattern (P) and a length-(n) text (T), we obtain a deterministic (O(n\\sqrt{mlog log m}))-time algorithm that exactly computes the Hamming distance between (P) and every length-(m) substring of (T). Sparse General Convolution: We also give a Monte Carlo (O(tlog t)) time algorithm for sparse convolution with possibly negative input in the restricted case where the length (N) of the input vectors satisfies (N\\le t^{1.99}).","headline":"Shaving Logs Via Large Sieve Inequality: Faster Algorithms For Sparse Convolution And More","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/jin2024shaving/"},"url":"https://learning2hash.github.io/publications/jin2024shaving/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Shaving Logs Via Large Sieve Inequality: Faster Algorithms For Sparse Convolution And More</h1>
  <h5>
  
    
      Ce Jin, Yinzhan Xu
    
  
  . Proceedings of the 56th Annual ACM Symposium on Theory of Computing
   2024
  
    – <span>1 citation</span>
  
  </h5>
  <p>
    
      [<a href="https://arxiv.org/abs/2403.20326" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Shaving Logs Via Large Sieve Inequality: Faster Algorithms For Sparse Convolution And More' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Shaving Logs Via Large Sieve Inequality: Faster Algorithms For Sparse Convolution And More' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#Efficiency">Efficiency</a></tag>
    
  </p>
  <p><p>In sparse convolution-type problems, a common technique is to hash the input
integers modulo a random prime (p\in [Q/2,Q]) for some parameter (Q), which
reduces the range of the input integers while preserving their additive
structure. However, this hash family suffers from two drawbacks, which led to
bottlenecks in many state-of-the-art algorithms: (1) The collision probability
of two elements from ([N]) is (O(\frac{log N}{Q})) rather than
(O(\frac{1}{Q})); (2) It is difficult to derandomize the choice of (p); known
derandomization techniques lead to super-logarithmic overhead [Chan, Lewenstein
STOC’15].
  In this paper, we partially overcome these drawbacks in certain scenarios,
via novel applications of the large sieve inequality from analytic number
theory. Consequently, we obtain the following improved algorithms for various
problems (in the standard word RAM model):
  Sparse Nonnegative Convolution: We obtain an (O(tlog t))-time Las Vegas
algorithm that computes the convolution (A\star B) of two nonnegative integer
vectors (A,B), where (t) is the output sparsity (|A\star B|_0). Moreover, our
algorithm terminates in (O(tlog t)) time with (1-1/\mathrm{poly}(t))
probability.
  Text-to-Pattern Hamming Distances: Given a length-(m) pattern (P) and a
length-(n) text (T), we obtain a deterministic (O(n\sqrt{mlog log m}))-time
algorithm that exactly computes the Hamming distance between (P) and every
length-(m) substring of (T).
  Sparse General Convolution: We also give a Monte Carlo (O(tlog t)) time
algorithm for sparse convolution with possibly negative input in the restricted
case where the length (N) of the input vectors satisfies (N\le t^{1.99}).</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

 <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/jin2024shaving.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>


</div>

    </div>

  </body>
</html>
