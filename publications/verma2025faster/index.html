<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Faster And Space Efficient Indexing For Locality Sensitive Hashing | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Faster And Space Efficient Indexing For Locality Sensitive Hashing" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This work suggests faster and space-efficient index construction algorithms for LSH for Euclidean distance (\textit{a.k.a.}~\ELSH) and cosine similarity (\textit{a.k.a.}~\SRP). The index construction step of these LSHs relies on grouping data points into several bins of hash tables based on their hashcode. To generate an \(m\)-dimensional hashcode of the \(d\)-dimensional data point, these LSHs first project the data point onto a \(d\)-dimensional random Gaussian vector and then discretise the resulting inner product. The time and space complexity of both \ELSH~and \SRP~for computing an \(m\)-sized hashcode of a \(d\)-dimensional vector is \(O(md)\), which becomes impractical for large values of \(m\) and \(d\). To overcome this problem, we propose two alternative LSH hashcode generation algorithms both for Euclidean distance and cosine similarity, namely, \CSELSH, \HCSELSH~and \CSSRP, \HCSSRP, respectively. \CSELSH~and \CSSRP~are based on count sketch \cite{count_sketch} and \HCSELSH~and \HCSSRP~utilize higher-order count sketch \cite{shi2019higher}. These proposals significantly reduce the hashcode computation time from \(O(md)\) to \(O(d)\). Additionally, both \CSELSH~and \CSSRP~reduce the space complexity from \(O(md)\) to \(O(d)\); ~and \HCSELSH, \HCSSRP~ reduce the space complexity from \(O(md)\) to \(O(N \sqrt[N]{d})\) respectively, where \(N\geq 1\) denotes the size of the input/reshaped tensor. Our proposals are backed by strong mathematical guarantees, and we validate their performance through simulations on various real-world datasets." />
<meta property="og:description" content="This work suggests faster and space-efficient index construction algorithms for LSH for Euclidean distance (\textit{a.k.a.}~\ELSH) and cosine similarity (\textit{a.k.a.}~\SRP). The index construction step of these LSHs relies on grouping data points into several bins of hash tables based on their hashcode. To generate an \(m\)-dimensional hashcode of the \(d\)-dimensional data point, these LSHs first project the data point onto a \(d\)-dimensional random Gaussian vector and then discretise the resulting inner product. The time and space complexity of both \ELSH~and \SRP~for computing an \(m\)-sized hashcode of a \(d\)-dimensional vector is \(O(md)\), which becomes impractical for large values of \(m\) and \(d\). To overcome this problem, we propose two alternative LSH hashcode generation algorithms both for Euclidean distance and cosine similarity, namely, \CSELSH, \HCSELSH~and \CSSRP, \HCSSRP, respectively. \CSELSH~and \CSSRP~are based on count sketch \cite{count_sketch} and \HCSELSH~and \HCSSRP~utilize higher-order count sketch \cite{shi2019higher}. These proposals significantly reduce the hashcode computation time from \(O(md)\) to \(O(d)\). Additionally, both \CSELSH~and \CSSRP~reduce the space complexity from \(O(md)\) to \(O(d)\); ~and \HCSELSH, \HCSSRP~ reduce the space complexity from \(O(md)\) to \(O(N \sqrt[N]{d})\) respectively, where \(N\geq 1\) denotes the size of the input/reshaped tensor. Our proposals are backed by strong mathematical guarantees, and we validate their performance through simulations on various real-world datasets." />
<link rel="canonical" href="https://learning2hash.github.io/publications/verma2025faster/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/verma2025faster/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-23T04:05:56-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Faster And Space Efficient Indexing For Locality Sensitive Hashing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-07-23T04:05:56-05:00","datePublished":"2025-07-23T04:05:56-05:00","description":"This work suggests faster and space-efficient index construction algorithms for LSH for Euclidean distance (\\textit{a.k.a.}~\\ELSH) and cosine similarity (\\textit{a.k.a.}~\\SRP). The index construction step of these LSHs relies on grouping data points into several bins of hash tables based on their hashcode. To generate an \\(m\\)-dimensional hashcode of the \\(d\\)-dimensional data point, these LSHs first project the data point onto a \\(d\\)-dimensional random Gaussian vector and then discretise the resulting inner product. The time and space complexity of both \\ELSH~and \\SRP~for computing an \\(m\\)-sized hashcode of a \\(d\\)-dimensional vector is \\(O(md)\\), which becomes impractical for large values of \\(m\\) and \\(d\\). To overcome this problem, we propose two alternative LSH hashcode generation algorithms both for Euclidean distance and cosine similarity, namely, \\CSELSH, \\HCSELSH~and \\CSSRP, \\HCSSRP, respectively. \\CSELSH~and \\CSSRP~are based on count sketch \\cite{count_sketch} and \\HCSELSH~and \\HCSSRP~utilize higher-order count sketch \\cite{shi2019higher}. These proposals significantly reduce the hashcode computation time from \\(O(md)\\) to \\(O(d)\\). Additionally, both \\CSELSH~and \\CSSRP~reduce the space complexity from \\(O(md)\\) to \\(O(d)\\); ~and \\HCSELSH, \\HCSSRP~ reduce the space complexity from \\(O(md)\\) to \\(O(N \\sqrt[N]{d})\\) respectively, where \\(N\\geq 1\\) denotes the size of the input/reshaped tensor. Our proposals are backed by strong mathematical guarantees, and we validate their performance through simulations on various real-world datasets.","headline":"Faster And Space Efficient Indexing For Locality Sensitive Hashing","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/verma2025faster/"},"url":"https://learning2hash.github.io/publications/verma2025faster/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Faster And Space Efficient Indexing For Locality Sensitive Hashing</h1>
  <h5>
  Verma Bhisham Dev, Pratap Rameshwar. Theoretical Computer Science 2025
  
    – <span>0 citations</span>
  
  </h5>
  <p>
    
      [<a href="https://arxiv.org/abs/2503.06737" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Faster And Space Efficient Indexing For Locality Sensitive Hashing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Faster And Space Efficient Indexing For Locality Sensitive Hashing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#Efficiency">Efficiency</a></tag>
    
      <tag><a href="/tags.html#Hashing Methods">Hashing Methods</a></tag>
    
      <tag><a href="/tags.html#Locality Sensitive Hashing">Locality Sensitive Hashing</a></tag>
    
      <tag><a href="/tags.html#Memory Efficiency">Memory Efficiency</a></tag>
    
      <tag><a href="/tags.html#Scalability">Scalability</a></tag>
    
  </p>
  <p><p>This work suggests faster and space-efficient index construction algorithms
for LSH for Euclidean distance (\textit{a.k.a.}~\ELSH) and cosine similarity
(\textit{a.k.a.}~\SRP). The index construction step of these LSHs relies on
grouping data points into several bins of hash tables based on their hashcode.
To generate an \(m\)-dimensional hashcode of the \(d\)-dimensional data point,
these LSHs first project the data point onto a \(d\)-dimensional random Gaussian
vector and then discretise the resulting inner product. The time and space
complexity of both \ELSH~and \SRP~for computing an \(m\)-sized hashcode of a
\(d\)-dimensional vector is \(O(md)\), which becomes impractical for large values
of \(m\) and \(d\). To overcome this problem, we propose two alternative LSH
hashcode generation algorithms both for Euclidean distance and cosine
similarity, namely, \CSELSH, \HCSELSH~and \CSSRP, \HCSSRP, respectively.
\CSELSH~and \CSSRP~are based on count sketch \cite{count_sketch} and
\HCSELSH~and \HCSSRP~utilize higher-order count sketch \cite{shi2019higher}.
These proposals significantly reduce the hashcode computation time from \(O(md)\)
to \(O(d)\). Additionally, both \CSELSH~and \CSSRP~reduce the space complexity
from \(O(md)\) to \(O(d)\); ~and \HCSELSH, \HCSSRP~ reduce the space complexity
from \(O(md)\) to \(O(N \sqrt[N]{d})\) respectively, where \(N\geq 1\) denotes the
size of the input/reshaped tensor. Our proposals are backed by strong
mathematical guarantees, and we validate their performance through simulations
on various real-world datasets.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

 <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/verma2025faster.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>


</div>

    </div>

  </body>
</html>
