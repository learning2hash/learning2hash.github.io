<!DOCTYPE html>
<html lang="en-us">

  <!-- _includes/head.html -->
<head>
  <!-- Analytics & trackers (leave as-is) -->
  <span id="wts2185304"></span>
  <script>var wts=document.createElement('script');wts.async=true;
  wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
  wts.onload=function(){ wtslog7(2185304,4); };</script>
  <noscript><a href="https://www.web-stat.com">
  <img src="https://app.ardalio.com/7/4/2185304.png" alt="Web-Stat web statistics"></a></noscript>

  <script>
    (function(h,o,t,j,a,r){
      h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
      h._hjSettings={hjid:1843243,hjsv:6};
      a=o.getElementsByTagName('head')[0];
      r=o.createElement('script');r.async=1;
      r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
      a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
  </script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-109544763-1');
  </script>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["\\(","\\)"]], displayMath: [["\\[","\\]"]] },
      options: { processHtmlClass: "mathjax-content", processEscapes: true }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Single viewport (remove duplicates from pages) -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Visual Descriptors For Content-based Retrieval Of Remote Sensing Images | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Visual Descriptors For Content-based Retrieval Of Remote Sensing Images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this paper we present an extensive evaluation of visual descriptors for the content-based retrieval of remote sensing (RS) images. The evaluation includes global hand-crafted, local hand-crafted, and Convolutional Neural Network (CNNs) features coupled with four different Content-Based Image Retrieval schemes. We conducted all the experiments on two publicly available datasets: the 21-class UC Merced Land Use/Land Cover (LandUse) dataset and 19-class High-resolution Satellite Scene dataset (SceneSat). The content of RS images might be quite heterogeneous, ranging from images containing fine grained textures, to coarse grained ones or to images containing objects. It is therefore not obvious in this domain, which descriptor should be employed to describe images having such a variability. Results demonstrate that CNN-based features perform better than both global and and local hand-crafted features whatever is the retrieval scheme adopted. Features extracted from SatResNet-50, a residual CNN suitable fine-tuned on the RS domain, shows much better performance than a residual CNN pre-trained on multimedia scene and object images. Features extracted from NetVLAD, a CNN that considers both CNN and local features, works better than others CNN solutions on those images that contain fine-grained textures and objects." />
<meta property="og:description" content="In this paper we present an extensive evaluation of visual descriptors for the content-based retrieval of remote sensing (RS) images. The evaluation includes global hand-crafted, local hand-crafted, and Convolutional Neural Network (CNNs) features coupled with four different Content-Based Image Retrieval schemes. We conducted all the experiments on two publicly available datasets: the 21-class UC Merced Land Use/Land Cover (LandUse) dataset and 19-class High-resolution Satellite Scene dataset (SceneSat). The content of RS images might be quite heterogeneous, ranging from images containing fine grained textures, to coarse grained ones or to images containing objects. It is therefore not obvious in this domain, which descriptor should be employed to describe images having such a variability. Results demonstrate that CNN-based features perform better than both global and and local hand-crafted features whatever is the retrieval scheme adopted. Features extracted from SatResNet-50, a residual CNN suitable fine-tuned on the RS domain, shows much better performance than a residual CNN pre-trained on multimedia scene and object images. Features extracted from NetVLAD, a CNN that considers both CNN and local features, works better than others CNN solutions on those images that contain fine-grained textures and objects." />
<link rel="canonical" href="https://learning2hash.github.io/publications/napoletano2016visual/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/napoletano2016visual/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-18T11:44:47-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Visual Descriptors For Content-based Retrieval Of Remote Sensing Images" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-18T11:44:47-05:00","datePublished":"2025-08-18T11:44:47-05:00","description":"In this paper we present an extensive evaluation of visual descriptors for the content-based retrieval of remote sensing (RS) images. The evaluation includes global hand-crafted, local hand-crafted, and Convolutional Neural Network (CNNs) features coupled with four different Content-Based Image Retrieval schemes. We conducted all the experiments on two publicly available datasets: the 21-class UC Merced Land Use/Land Cover (LandUse) dataset and 19-class High-resolution Satellite Scene dataset (SceneSat). The content of RS images might be quite heterogeneous, ranging from images containing fine grained textures, to coarse grained ones or to images containing objects. It is therefore not obvious in this domain, which descriptor should be employed to describe images having such a variability. Results demonstrate that CNN-based features perform better than both global and and local hand-crafted features whatever is the retrieval scheme adopted. Features extracted from SatResNet-50, a residual CNN suitable fine-tuned on the RS domain, shows much better performance than a residual CNN pre-trained on multimedia scene and object images. Features extracted from NetVLAD, a CNN that considers both CNN and local features, works better than others CNN solutions on those images that contain fine-grained textures and objects.","headline":"Visual Descriptors For Content-based Retrieval Of Remote Sensing Images","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/napoletano2016visual/"},"url":"https://learning2hash.github.io/publications/napoletano2016visual/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Site CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" type="application/opensearchdescription+xml" title="learning2hash" />

  <!-- âœ… Single, modern jQuery + DataTables (CSS first, then JS). No other jQuery/DT anywhere. -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" defer></script>
  <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js" defer></script>

  <!-- Optional sanity log -->
  <script defer>
    window.addEventListener('DOMContentLoaded', () => {
      console.log('jQuery:', jQuery?.fn?.jquery);
      console.log('DataTables loaded:', !!jQuery?.fn?.dataTable);
    });
  </script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/opensource.html">Tools Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Visual Descriptors For Content-based Retrieval Of Remote Sensing Images</h1>
  <h5>
  
    
      Paolo Napoletano
    
  
  . International Journal of Remote Sensing
   2017
  
    â€“ <span>118 citations</span>
  
  </h5>
  <p>
    
      [<a href="https://arxiv.org/abs/1602.00970" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Visual Descriptors For Content-based Retrieval Of Remote Sensing Images' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Visual Descriptors For Content-based Retrieval Of Remote Sensing Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#Datasets">Datasets</a></tag>
    
      <tag><a href="/tags.html#Evaluation">Evaluation</a></tag>
    
  </p>
  <p><p>In this paper we present an extensive evaluation of visual descriptors for
the content-based retrieval of remote sensing (RS) images. The evaluation
includes global hand-crafted, local hand-crafted, and Convolutional Neural
Network (CNNs) features coupled with four different Content-Based Image
Retrieval schemes. We conducted all the experiments on two publicly available
datasets: the 21-class UC Merced Land Use/Land Cover (LandUse) dataset and
19-class High-resolution Satellite Scene dataset (SceneSat). The content of RS
images might be quite heterogeneous, ranging from images containing fine
grained textures, to coarse grained ones or to images containing objects. It is
therefore not obvious in this domain, which descriptor should be employed to
describe images having such a variability. Results demonstrate that CNN-based
features perform better than both global and and local hand-crafted features
whatever is the retrieval scheme adopted. Features extracted from SatResNet-50,
a residual CNN suitable fine-tuned on the RS domain, shows much better
performance than a residual CNN pre-trained on multimedia scene and object
images. Features extracted from NetVLAD, a CNN that considers both CNN and
local features, works better than others CNN solutions on those images that
contain fine-grained textures and objects.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

 <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/napoletano2016visual.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>


</div>

    </div>

  </body>
</html>
