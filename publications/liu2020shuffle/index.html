<hr />
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>layout: publication
title: "Shuffle and Learn: Minimizing Mutual Information for Unsupervised Hashing"
authors: Liu Fangrui, Liu Zheng
conference: Arxiv
year: 2020
bibkey: liu2020shuffle
additional_links:
   - {name: "License", url: "http://creativecommons.org/licenses/by-nc-sa/4.0/"}    - {name: "Paper", url: "https://arxiv.org/abs/2011.10239"}
tags: ['Supervised', 'Arxiv', 'Unsupervised', 'Image Retrieval']
---

Unsupervised binary representation allows fast data retrieval without any annotations, enabling practical application like fast person re-identification and multimedia retrieval. It is argued that conflicts in binary space are one of the major barriers to high-performance unsupervised hashing as current methods failed to capture the precise code conflicts in the full domain. A novel relaxation method called Shuffle and Learn is proposed to tackle code conflicts in the unsupervised hash. Approximated derivatives for joint probability and the gradients for the binary layer are introduced to bridge the update from the hash to the input. Proof on $\epsilon$-Convergence of joint probability with approximated derivatives is provided to guarantee the preciseness on update applied on the mutual information. The proposed algorithm is carried out with iterative global updates to minimize mutual information, diverging the code before regular unsupervised optimization. Experiments suggest that the proposed method can relax the code optimization from local optimum and help to generate binary representations that are more discriminative and informative without any annotations. Performance benchmarks on image retrieval with the unsupervised binary code are conducted on three open datasets, and the model achieves state-of-the-art accuracy on image retrieval task for all those datasets. Datasets and reproducible code are provided.
</code></pre></div></div>

