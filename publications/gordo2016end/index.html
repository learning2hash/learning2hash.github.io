<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>End-to-end Learning Of Deep Visual Representations For Image Retrieval | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="End-to-end Learning Of Deep Visual Representations For Image Retrieval" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="While deep learning has become a key ingredient in the top performing methods for many computer vision tasks it has failed so far to bring similar improvements to instance-level image retrieval. In this article we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold i) noisy training data ii) inappropriate deep architecture and iii) suboptimal training procedure. We address all three issues. First we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second we build on the recent R-MAC descriptor show that it can be interpreted as a deep and differentiable architecture and present improvements to enhance it. Last we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k Paris 6k and Holidays we respectively report 94.7 96.6 and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material please see www.xrce.xerox.com/Deep-Image-Retrieval." />
<meta property="og:description" content="While deep learning has become a key ingredient in the top performing methods for many computer vision tasks it has failed so far to bring similar improvements to instance-level image retrieval. In this article we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold i) noisy training data ii) inappropriate deep architecture and iii) suboptimal training procedure. We address all three issues. First we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second we build on the recent R-MAC descriptor show that it can be interpreted as a deep and differentiable architecture and present improvements to enhance it. Last we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k Paris 6k and Holidays we respectively report 94.7 96.6 and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material please see www.xrce.xerox.com/Deep-Image-Retrieval." />
<link rel="canonical" href="https://learning2hash.github.io/publications/gordo2016end/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/gordo2016end/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-11T09:12:15-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="End-to-end Learning Of Deep Visual Representations For Image Retrieval" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-11T09:12:15-05:00","datePublished":"2024-10-11T09:12:15-05:00","description":"While deep learning has become a key ingredient in the top performing methods for many computer vision tasks it has failed so far to bring similar improvements to instance-level image retrieval. In this article we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold i) noisy training data ii) inappropriate deep architecture and iii) suboptimal training procedure. We address all three issues. First we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second we build on the recent R-MAC descriptor show that it can be interpreted as a deep and differentiable architecture and present improvements to enhance it. Last we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k Paris 6k and Holidays we respectively report 94.7 96.6 and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material please see www.xrce.xerox.com/Deep-Image-Retrieval.","headline":"End-to-end Learning Of Deep Visual Representations For Image Retrieval","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/gordo2016end/"},"url":"https://learning2hash.github.io/publications/gordo2016end/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">End-to-end Learning Of Deep Visual Representations For Image Retrieval</h1>
  <h5>Gordo Albert, Almazan Jon, Revaud Jerome, Larlus Diane. Arxiv 2016</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/1610.07940" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=End-to-end Learning Of Deep Visual Representations For Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=End-to-end Learning Of Deep Visual Representations For Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Deep Learning">Deep Learning</a></tag>
    
      <tag><a href="/tags.html#Image Retrieval">Image Retrieval</a></tag>
    
      <tag><a href="/tags.html#Quantisation">Quantisation</a></tag>
    
      <tag><a href="/tags.html#Supervised">Supervised</a></tag>
    
  </p>
  <p><p>While deep learning has become a key ingredient in the top performing methods for many computer vision tasks it has failed so far to bring similar improvements to instance-level image retrieval. In this article we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold i) noisy training data ii) inappropriate deep architecture and iii) suboptimal training procedure. We address all three issues. First we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second we build on the recent R-MAC descriptor show that it can be interpreted as a deep and differentiable architecture and present improvements to enhance it. Last we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k Paris 6k and Holidays we respectively report 94.7 96.6 and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material please see www.xrce.xerox.com/Deep-Image-Retrieval.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/gordo2016end.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
