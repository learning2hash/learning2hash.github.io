<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transcending Scaling Laws With 0.1 Extra Compute | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Transcending Scaling Laws With 0.1 Extra Compute" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g. PaLM) on a few more steps with UL2s mixture-of-denoiser objective. We show that with almost negligible extra computational costs and no new sources of data we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper we continue training PaLM with UL2R introducing a new set of models at 8B 62B and 540B scale which we call U-PaLM. Impressively at 540B scale we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e. saving (sim)4.4 million TPUv4 hours). We further show that this improved scaling curve leads to emergent abilities on challenging BIG-Bench tasks – for instance U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall we show that U-PaLM outperforms PaLM on many few-shot setups i.e. English NLP tasks (e.g. commonsense reasoning question answering) reasoning tasks with chain-of-thought (e.g. GSM8K) multilingual tasks (MGSM TydiQA) MMLU and challenging BIG-Bench tasks. Finally we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling." />
<meta property="og:description" content="Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g. PaLM) on a few more steps with UL2s mixture-of-denoiser objective. We show that with almost negligible extra computational costs and no new sources of data we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper we continue training PaLM with UL2R introducing a new set of models at 8B 62B and 540B scale which we call U-PaLM. Impressively at 540B scale we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e. saving (sim)4.4 million TPUv4 hours). We further show that this improved scaling curve leads to emergent abilities on challenging BIG-Bench tasks – for instance U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall we show that U-PaLM outperforms PaLM on many few-shot setups i.e. English NLP tasks (e.g. commonsense reasoning question answering) reasoning tasks with chain-of-thought (e.g. GSM8K) multilingual tasks (MGSM TydiQA) MMLU and challenging BIG-Bench tasks. Finally we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling." />
<link rel="canonical" href="https://learning2hash.github.io/publications/tay2022transcending/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/tay2022transcending/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transcending Scaling Laws With 0.1 Extra Compute" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g. PaLM) on a few more steps with UL2s mixture-of-denoiser objective. We show that with almost negligible extra computational costs and no new sources of data we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper we continue training PaLM with UL2R introducing a new set of models at 8B 62B and 540B scale which we call U-PaLM. Impressively at 540B scale we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e. saving (sim)4.4 million TPUv4 hours). We further show that this improved scaling curve leads to emergent abilities on challenging BIG-Bench tasks – for instance U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall we show that U-PaLM outperforms PaLM on many few-shot setups i.e. English NLP tasks (e.g. commonsense reasoning question answering) reasoning tasks with chain-of-thought (e.g. GSM8K) multilingual tasks (MGSM TydiQA) MMLU and challenging BIG-Bench tasks. Finally we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.","headline":"Transcending Scaling Laws With 0.1 Extra Compute","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/tay2022transcending/"},"url":"https://learning2hash.github.io/publications/tay2022transcending/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Transcending Scaling Laws With 0.1 Extra Compute</h1>
  <h5>Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani. Arxiv 2022</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2210.11399v2" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Transcending Scaling Laws With 0.1 Extra Compute' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Transcending Scaling Laws With 0.1 Extra Compute' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
  </p>
  <p><p>Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g. PaLM) on a few more steps with UL2s mixture-of-denoiser objective. We show that with almost negligible extra computational costs and no new sources of data we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper we continue training PaLM with UL2R introducing a new set of models at 8B 62B and 540B scale which we call U-PaLM. Impressively at 540B scale we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e. saving (sim)4.4 million TPUv4 hours). We further show that this improved scaling curve leads to emergent abilities on challenging BIG-Bench tasks – for instance U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall we show that U-PaLM outperforms PaLM on many few-shot setups i.e. English NLP tasks (e.g. commonsense reasoning question answering) reasoning tasks with chain-of-thought (e.g. GSM8K) multilingual tasks (MGSM TydiQA) MMLU and challenging BIG-Bench tasks. Finally we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/tay2022transcending.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
