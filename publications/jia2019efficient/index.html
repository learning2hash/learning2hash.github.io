<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Given a data set \(\mathcal{D}\) containing millions of data points and a data consumer who is willing to pay for $\(X\) to train a machine learning (ML) model over \(\mathcal{D}\), how should we distribute this $\(X\) to each data point to reflect its “value”? In this paper, we define the “relative value of data” via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all \(N\) data points, it requires \(O(2^N)\) model evaluations for exact computation and \(O(Nlog N)\) for \((\epsilon, \delta)\)-approximation. In this paper, we focus on one popular family of ML models relying on \(K\)-nearest neighbors (\(K\)NN). The most surprising result is that for unweighted \(K\)NN classifiers and regressors, the Shapley value of all \(N\) data points can be computed, exactly, in \(O(Nlog N)\) time – an exponential improvement on computational complexity! Moreover, for \((\epsilon, \delta)\)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity \(O(N^{h(\epsilon,K)}log N)\) when \(\epsilon\) is not too small and \(K\) is not too large. We empirically evaluate our algorithms on up to \(10\) million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed \(K\)NN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation." />
<meta property="og:description" content="Given a data set \(\mathcal{D}\) containing millions of data points and a data consumer who is willing to pay for $\(X\) to train a machine learning (ML) model over \(\mathcal{D}\), how should we distribute this $\(X\) to each data point to reflect its “value”? In this paper, we define the “relative value of data” via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all \(N\) data points, it requires \(O(2^N)\) model evaluations for exact computation and \(O(Nlog N)\) for \((\epsilon, \delta)\)-approximation. In this paper, we focus on one popular family of ML models relying on \(K\)-nearest neighbors (\(K\)NN). The most surprising result is that for unweighted \(K\)NN classifiers and regressors, the Shapley value of all \(N\) data points can be computed, exactly, in \(O(Nlog N)\) time – an exponential improvement on computational complexity! Moreover, for \((\epsilon, \delta)\)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity \(O(N^{h(\epsilon,K)}log N)\) when \(\epsilon\) is not too small and \(K\) is not too large. We empirically evaluate our algorithms on up to \(10\) million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed \(K\)NN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation." />
<link rel="canonical" href="https://learning2hash.github.io/publications/jia2019efficient/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/jia2019efficient/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-24T09:23:52-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-07-24T09:23:52-05:00","datePublished":"2025-07-24T09:23:52-05:00","description":"Given a data set \\(\\mathcal{D}\\) containing millions of data points and a data consumer who is willing to pay for $\\(X\\) to train a machine learning (ML) model over \\(\\mathcal{D}\\), how should we distribute this $\\(X\\) to each data point to reflect its “value”? In this paper, we define the “relative value of data” via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all \\(N\\) data points, it requires \\(O(2^N)\\) model evaluations for exact computation and \\(O(Nlog N)\\) for \\((\\epsilon, \\delta)\\)-approximation. In this paper, we focus on one popular family of ML models relying on \\(K\\)-nearest neighbors (\\(K\\)NN). The most surprising result is that for unweighted \\(K\\)NN classifiers and regressors, the Shapley value of all \\(N\\) data points can be computed, exactly, in \\(O(Nlog N)\\) time – an exponential improvement on computational complexity! Moreover, for \\((\\epsilon, \\delta)\\)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity \\(O(N^{h(\\epsilon,K)}log N)\\) when \\(\\epsilon\\) is not too small and \\(K\\) is not too large. We empirically evaluate our algorithms on up to \\(10\\) million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed \\(K\\)NN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation.","headline":"Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/jia2019efficient/"},"url":"https://learning2hash.github.io/publications/jia2019efficient/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms</h1>
  <h5>
  Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas J. Spanos, Dawn Song. Proceedings of the VLDB Endowment 2019
  
    – <span>129 citations</span>
  
  </h5>
  <p>
    
      [<a href="https://arxiv.org/abs/1908.08619" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#Hashing Methods">Hashing Methods</a></tag>
    
      <tag><a href="/tags.html#Locality Sensitive Hashing">Locality Sensitive Hashing</a></tag>
    
  </p>
  <p><p>Given a data set \(\mathcal{D}\) containing millions of data points and a data
consumer who is willing to pay for $\(X\) to train a machine learning (ML) model
over \(\mathcal{D}\), how should we distribute this $\(X\) to each data point to
reflect its “value”? In this paper, we define the “relative value of data” via
the Shapley value, as it uniquely possesses properties with appealing
real-world interpretations, such as fairness, rationality and
decentralizability. For general, bounded utility functions, the Shapley value
is known to be challenging to compute: to get Shapley values for all \(N\) data
points, it requires \(O(2^N)\) model evaluations for exact computation and
\(O(Nlog N)\) for \((\epsilon, \delta)\)-approximation. In this paper, we focus on
one popular family of ML models relying on \(K\)-nearest neighbors (\(K\)NN). The
most surprising result is that for unweighted \(K\)NN classifiers and regressors,
the Shapley value of all \(N\) data points can be computed, exactly, in \(O(Nlog
N)\) time – an exponential improvement on computational complexity! Moreover,
for \((\epsilon, \delta)\)-approximation, we are able to develop an algorithm
based on Locality Sensitive Hashing (LSH) with only sublinear complexity
\(O(N^{h(\epsilon,K)}log N)\) when \(\epsilon\) is not too small and \(K\) is not
too large. We empirically evaluate our algorithms on up to \(10\) million data
points and even our exact algorithm is up to three orders of magnitude faster
than the baseline approximation algorithm. The LSH-based approximation
algorithm can accelerate the value calculation process even further. We then
extend our algorithms to other scenarios such as (1) weighed \(K\)NN classifiers,
(2) different data points are clustered by different data curators, and (3)
there are data analysts providing computation who also requires proper
valuation.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

 <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/jia2019efficient.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>


</div>

    </div>

  </body>
</html>
