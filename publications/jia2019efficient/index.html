<!DOCTYPE html>
<html lang="en-us">

  <!-- _includes/head.html -->
<head>
  <!-- begin code v 7.0 -->
  <span id="wts2185304"></span>
  <script>
  var wts7 = {};
  wts7.invisible='';
  wts7.page_name='';
  wts7.group_name='';
  wts7.conversion_number='';
  wts7.user_id='';
  var wts=document.createElement('script');wts.async=true;
  wts.src='https://app.ardalio.com/wts7.js';document.head.appendChild(wts);
  wts.onload = function(){ wtsl7(2185304,4); };
  </script><noscript><img src="https://app.ardalio.com/7/4/2185304.png"></noscript>
  <!-- end code v 7.0 -->
  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["\\(","\\)"]], displayMath: [["\\[","\\]"]] },
      options: { processHtmlClass: "mathjax-content", processEscapes: true }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">

  <!-- Viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- ✅ Manual SEO keywords (specific to Learning to Hash) -->
  <meta name="keywords" content="learning to hash, machine learning, hashing, approximate nearest neighbour search, ANN, LSH, locality sensitive hashing, vector quantization, deep hashing, binary embeddings, information retrieval, similarity search">

  <!-- ✅ Jekyll SEO plugin (title, description, canonical, OG/Twitter, JSON-LD) -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Given a data set (\mathcal{D}) containing millions of data points and a data consumer who is willing to pay for $(X) to train a machine learning (ML) model over (\mathcal{D}), how should we distribute this $(X) to each data point to reflect its “value”? In this paper, we define the “relative value of data” via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all (N) data points, it requires (O(2^N)) model evaluations for exact computation and (O(Nlog N)) for ((\epsilon, \delta))-approximation. In this paper, we focus on one popular family of ML models relying on (K)-nearest neighbors ((K)NN). The most surprising result is that for unweighted (K)NN classifiers and regressors, the Shapley value of all (N) data points can be computed, exactly, in (O(Nlog N)) time – an exponential improvement on computational complexity! Moreover, for ((\epsilon, \delta))-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity (O(N^{h(\epsilon,K)}log N)) when (\epsilon) is not too small and (K) is not too large. We empirically evaluate our algorithms on up to (10) million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed (K)NN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation." />
<meta property="og:description" content="Given a data set (\mathcal{D}) containing millions of data points and a data consumer who is willing to pay for $(X) to train a machine learning (ML) model over (\mathcal{D}), how should we distribute this $(X) to each data point to reflect its “value”? In this paper, we define the “relative value of data” via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all (N) data points, it requires (O(2^N)) model evaluations for exact computation and (O(Nlog N)) for ((\epsilon, \delta))-approximation. In this paper, we focus on one popular family of ML models relying on (K)-nearest neighbors ((K)NN). The most surprising result is that for unweighted (K)NN classifiers and regressors, the Shapley value of all (N) data points can be computed, exactly, in (O(Nlog N)) time – an exponential improvement on computational complexity! Moreover, for ((\epsilon, \delta))-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity (O(N^{h(\epsilon,K)}log N)) when (\epsilon) is not too small and (K) is not too large. We empirically evaluate our algorithms on up to (10) million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed (K)NN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation." />
<link rel="canonical" href="https://learning2hash.github.io/publications/jia2019efficient/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/jia2019efficient/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-20T14:22:56-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-20T14:22:56-05:00","datePublished":"2025-08-20T14:22:56-05:00","description":"Given a data set (\\mathcal{D}) containing millions of data points and a data consumer who is willing to pay for $(X) to train a machine learning (ML) model over (\\mathcal{D}), how should we distribute this $(X) to each data point to reflect its “value”? In this paper, we define the “relative value of data” via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all (N) data points, it requires (O(2^N)) model evaluations for exact computation and (O(Nlog N)) for ((\\epsilon, \\delta))-approximation. In this paper, we focus on one popular family of ML models relying on (K)-nearest neighbors ((K)NN). The most surprising result is that for unweighted (K)NN classifiers and regressors, the Shapley value of all (N) data points can be computed, exactly, in (O(Nlog N)) time – an exponential improvement on computational complexity! Moreover, for ((\\epsilon, \\delta))-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity (O(N^{h(\\epsilon,K)}log N)) when (\\epsilon) is not too small and (K) is not too large. We empirically evaluate our algorithms on up to (10) million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed (K)NN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation.","headline":"Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/jia2019efficient/"},"url":"https://learning2hash.github.io/publications/jia2019efficient/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Site CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" type="application/opensearchdescription+xml" title="learning2hash" />

  <!-- ✅ Single, modern jQuery + DataTables -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" defer></script>
  <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js" defer></script>

  <!-- Optional sanity log -->
  <script defer>
    window.addEventListener('DOMContentLoaded', () => {
      console.log('jQuery:', jQuery?.fn?.jquery);
      console.log('DataTables loaded:', !!jQuery?.fn?.dataTable);
    });
  </script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/opensource.html">Tools Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script defer>
document.addEventListener('DOMContentLoaded', function () {
  function doSearch() {
    const el = document.getElementById('searchTarget');
    if (!el) return;
    const q = el.value.trim();
    try { if (typeof ga === 'function') ga('send', 'event', 'search', 'search', q); } catch(e) {}
    window.location = "/papers.html#" + encodeURIComponent(q);
  }

  // jQuery path if loaded
  if (window.jQuery) {
    $('#searchTarget').on('keydown', function (e) {
      if (e.key === 'Enter') doSearch();
    });
    document.querySelector('.sidebar-item button')?.addEventListener('click', doSearch);
  } else {
    // vanilla fallback
    const input = document.getElementById('searchTarget');
    const btn = document.querySelector('.sidebar-item button');
    if (input) {
      input.addEventListener('keydown', function (e) {
        if (e.key === 'Enter') doSearch();
      });
    }
    if (btn) btn.addEventListener('click', doSearch);
  }

  // keep global for inline onClick="search()"
  window.search = doSearch;
});
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms</h1>
  <h5>
  
    
      Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas J. Spanos, Dawn Song
    
  
  . Proceedings of the VLDB Endowment
   2019
  
    – <span>129 citations</span>
  
  </h5>
  <p>
    
      [<a href="https://arxiv.org/abs/1908.08619" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Efficient Task-specific Data Valuation For Nearest Neighbor Algorithms' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#Hashing Methods">Hashing Methods</a></tag>
    
      <tag><a href="/tags.html#Locality Sensitive Hashing">Locality Sensitive Hashing</a></tag>
    
  </p>
  <p><p>Given a data set (\mathcal{D}) containing millions of data points and a data
consumer who is willing to pay for $(X) to train a machine learning (ML) model
over (\mathcal{D}), how should we distribute this $(X) to each data point to
reflect its “value”? In this paper, we define the “relative value of data” via
the Shapley value, as it uniquely possesses properties with appealing
real-world interpretations, such as fairness, rationality and
decentralizability. For general, bounded utility functions, the Shapley value
is known to be challenging to compute: to get Shapley values for all (N) data
points, it requires (O(2^N)) model evaluations for exact computation and
(O(Nlog N)) for ((\epsilon, \delta))-approximation. In this paper, we focus on
one popular family of ML models relying on (K)-nearest neighbors ((K)NN). The
most surprising result is that for unweighted (K)NN classifiers and regressors,
the Shapley value of all (N) data points can be computed, exactly, in (O(Nlog
N)) time – an exponential improvement on computational complexity! Moreover,
for ((\epsilon, \delta))-approximation, we are able to develop an algorithm
based on Locality Sensitive Hashing (LSH) with only sublinear complexity
(O(N^{h(\epsilon,K)}log N)) when (\epsilon) is not too small and (K) is not
too large. We empirically evaluate our algorithms on up to (10) million data
points and even our exact algorithm is up to three orders of magnitude faster
than the baseline approximation algorithm. The LSH-based approximation
algorithm can accelerate the value calculation process even further. We then
extend our algorithms to other scenarios such as (1) weighed (K)NN classifiers,
(2) different data points are clustered by different data curators, and (3)
there are data analysts providing computation who also requires proper
valuation.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

 <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/jia2019efficient.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>


</div>

    </div>

  </body>
</html>
