<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Unival Unified Model For Image Video Audio And Language Tasks | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Unival Unified Model For Image Video Audio And Language Tasks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g. Flamingo (Alayrac et al. 2022) trained on massive datasets can support more than two modalities current small to mid-scale unified models are still limited to 2 modalities usually image-text or video-text. The question that we ask is is it possible to build efficiently a unified model that can support all modalities To answer this we propose UnIVAL a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text images video and audio into a single model. Our model is efficiently pretrained on many tasks based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches across image and video-text tasks. The feature representations learned from image and video-text modalities allows the model to achieve competitive performance when finetuned on audio-text tasks despite not being pretrained on audio. Thanks to the unified model we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks showing their benefits in particular for out-of-distribution generalization. Finally we motivate unification by showing the synergy between tasks. The model weights and code are released here https://github.com/mshukor/UnIVAL." />
<meta property="og:description" content="Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g. Flamingo (Alayrac et al. 2022) trained on massive datasets can support more than two modalities current small to mid-scale unified models are still limited to 2 modalities usually image-text or video-text. The question that we ask is is it possible to build efficiently a unified model that can support all modalities To answer this we propose UnIVAL a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text images video and audio into a single model. Our model is efficiently pretrained on many tasks based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches across image and video-text tasks. The feature representations learned from image and video-text modalities allows the model to achieve competitive performance when finetuned on audio-text tasks despite not being pretrained on audio. Thanks to the unified model we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks showing their benefits in particular for out-of-distribution generalization. Finally we motivate unification by showing the synergy between tasks. The model weights and code are released here https://github.com/mshukor/UnIVAL." />
<link rel="canonical" href="https://learning2hash.github.io/publications/shukor2023unival/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/shukor2023unival/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Unival Unified Model For Image Video Audio And Language Tasks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g. Flamingo (Alayrac et al. 2022) trained on massive datasets can support more than two modalities current small to mid-scale unified models are still limited to 2 modalities usually image-text or video-text. The question that we ask is is it possible to build efficiently a unified model that can support all modalities To answer this we propose UnIVAL a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text images video and audio into a single model. Our model is efficiently pretrained on many tasks based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches across image and video-text tasks. The feature representations learned from image and video-text modalities allows the model to achieve competitive performance when finetuned on audio-text tasks despite not being pretrained on audio. Thanks to the unified model we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks showing their benefits in particular for out-of-distribution generalization. Finally we motivate unification by showing the synergy between tasks. The model weights and code are released here https://github.com/mshukor/UnIVAL.","headline":"Unival Unified Model For Image Video Audio And Language Tasks","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/shukor2023unival/"},"url":"https://learning2hash.github.io/publications/shukor2023unival/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Unival Unified Model For Image Video Audio And Language Tasks</h1>
  <h5>Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord. Arxiv 2023</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2307.16184v2" target="_blank">Paper</a>]
    
      [<a href="https://github.com/mshukor/UnIVAL" target="_blank">Code</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Unival Unified Model For Image Video Audio And Language Tasks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Unival Unified Model For Image Video Audio And Language Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Cross Modal">Cross Modal</a></tag>
    
      <tag><a href="/tags.html#Has Code">Has Code</a></tag>
    
  </p>
  <p><p>Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g. Flamingo (Alayrac et al. 2022) trained on massive datasets can support more than two modalities current small to mid-scale unified models are still limited to 2 modalities usually image-text or video-text. The question that we ask is is it possible to build efficiently a unified model that can support all modalities To answer this we propose UnIVAL a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text images video and audio into a single model. Our model is efficiently pretrained on many tasks based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches across image and video-text tasks. The feature representations learned from image and video-text modalities allows the model to achieve competitive performance when finetuned on audio-text tasks despite not being pretrained on audio. Thanks to the unified model we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks showing their benefits in particular for out-of-distribution generalization. Finally we motivate unification by showing the synergy between tasks. The model weights and code are released here https://github.com/mshukor/UnIVAL.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/shukor2023unival.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
