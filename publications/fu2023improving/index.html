<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing reflecting and criticizing. We are interested in this question because if LLMs were able to improve each other it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other playing the roles of a buyer and a seller respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model playing the critic provides feedback to a player to improve the players negotiation strategies. We let the two agents play multiple rounds using previous negotiation history and AI feedback as in-context demonstrations to improve the models negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback weaker models either do not understand the games rules or cannot incorporate AI feedback for further improvement. (2) Models abilities to learn from the feedback differ when playing different roles. For example it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback." />
<meta property="og:description" content="We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing reflecting and criticizing. We are interested in this question because if LLMs were able to improve each other it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other playing the roles of a buyer and a seller respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model playing the critic provides feedback to a player to improve the players negotiation strategies. We let the two agents play multiple rounds using previous negotiation history and AI feedback as in-context demonstrations to improve the models negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback weaker models either do not understand the games rules or cannot incorporate AI feedback for further improvement. (2) Models abilities to learn from the feedback differ when playing different roles. For example it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback." />
<link rel="canonical" href="https://learning2hash.github.io/publications/fu2023improving/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/fu2023improving/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing reflecting and criticizing. We are interested in this question because if LLMs were able to improve each other it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other playing the roles of a buyer and a seller respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model playing the critic provides feedback to a player to improve the players negotiation strategies. We let the two agents play multiple rounds using previous negotiation history and AI feedback as in-context demonstrations to improve the models negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback weaker models either do not understand the games rules or cannot incorporate AI feedback for further improvement. (2) Models abilities to learn from the feedback differ when playing different roles. For example it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.","headline":"Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/fu2023improving/"},"url":"https://learning2hash.github.io/publications/fu2023improving/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback</h1>
  <h5>Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata. Arxiv 2023</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2305.10142v1" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Improving Language Model Negotiation With Self-play And In-context Learning From AI Feedback' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
  </p>
  <p><p>We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing reflecting and criticizing. We are interested in this question because if LLMs were able to improve each other it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other playing the roles of a buyer and a seller respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model playing the critic provides feedback to a player to improve the players negotiation strategies. We let the two agents play multiple rounds using previous negotiation history and AI feedback as in-context demonstrations to improve the models negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback weaker models either do not understand the games rules or cannot incorporate AI feedback for further improvement. (2) Models abilities to learn from the feedback differ when playing different roles. For example it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/fu2023improving.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
