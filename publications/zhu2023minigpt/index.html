<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The recent GPT-4 has demonstrated extraordinary multi-modal abilities such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon we present MiniGPT-4 which aligns a frozen visual encoder with a frozen advanced LLM Vicuna using one projection layer. Our work for the first time uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4 such as detailed image description generation and website creation from hand-drawn drafts. Furthermore we also observe other emerging capabilities in MiniGPT-4 including writing stories and poems inspired by given images teaching users how to cook based on food photos and so on. In our experiment we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g. repetition and fragmentation). To address this problem we curate a detailed image description dataset in the second stage to finetune the model which consequently improves the models generation reliability and overall usability. Our code pre-trained model and collected dataset are available at https://minigpt-4.github.io/." />
<meta property="og:description" content="The recent GPT-4 has demonstrated extraordinary multi-modal abilities such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon we present MiniGPT-4 which aligns a frozen visual encoder with a frozen advanced LLM Vicuna using one projection layer. Our work for the first time uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4 such as detailed image description generation and website creation from hand-drawn drafts. Furthermore we also observe other emerging capabilities in MiniGPT-4 including writing stories and poems inspired by given images teaching users how to cook based on food photos and so on. In our experiment we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g. repetition and fragmentation). To address this problem we curate a detailed image description dataset in the second stage to finetune the model which consequently improves the models generation reliability and overall usability. Our code pre-trained model and collected dataset are available at https://minigpt-4.github.io/." />
<link rel="canonical" href="https://learning2hash.github.io/publications/zhu2023minigpt/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/zhu2023minigpt/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"The recent GPT-4 has demonstrated extraordinary multi-modal abilities such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon we present MiniGPT-4 which aligns a frozen visual encoder with a frozen advanced LLM Vicuna using one projection layer. Our work for the first time uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4 such as detailed image description generation and website creation from hand-drawn drafts. Furthermore we also observe other emerging capabilities in MiniGPT-4 including writing stories and poems inspired by given images teaching users how to cook based on food photos and so on. In our experiment we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g. repetition and fragmentation). To address this problem we curate a detailed image description dataset in the second stage to finetune the model which consequently improves the models generation reliability and overall usability. Our code pre-trained model and collected dataset are available at https://minigpt-4.github.io/.","headline":"Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/zhu2023minigpt/"},"url":"https://learning2hash.github.io/publications/zhu2023minigpt/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models</h1>
  <h5>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny. Arxiv 2023</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2304.10592v2" target="_blank">Paper</a>]
    
      [<a href="https://minigpt-4.github.io/" target="_blank">Code</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Minigpt-4 Enhancing Vision-language Understanding With Advanced Large Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Cross Modal">Cross Modal</a></tag>
    
      <tag><a href="/tags.html#Has Code">Has Code</a></tag>
    
      <tag><a href="/tags.html#Independent">Independent</a></tag>
    
  </p>
  <p><p>The recent GPT-4 has demonstrated extraordinary multi-modal abilities such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon we present MiniGPT-4 which aligns a frozen visual encoder with a frozen advanced LLM Vicuna using one projection layer. Our work for the first time uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4 such as detailed image description generation and website creation from hand-drawn drafts. Furthermore we also observe other emerging capabilities in MiniGPT-4 including writing stories and poems inspired by given images teaching users how to cook based on food photos and so on. In our experiment we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g. repetition and fragmentation). To address this problem we curate a detailed image description dataset in the second stage to finetune the model which consequently improves the models generation reliability and overall usability. Our code pre-trained model and collected dataset are available at https://minigpt-4.github.io/.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/zhu2023minigpt.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
