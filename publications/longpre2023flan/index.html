<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The Flan Collection Designing Data And Methods For Effective Instruction Tuning | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="The Flan Collection Designing Data And Methods For Effective Instruction Tuning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We study the design decisions of publicly available instruction tuning methods and break down the development of Flan 2022 (Chung et al. 2022). Through careful ablation studies on the Flan Collection of tasks and methods we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-1737;+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning and in particular training with mixed prompt settings (zero-shot few-shot and chain-of-thought) actually yields stronger (237;+) performance in all settings. In further experiments we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally to accelerate research on instruction tuning we make the Flan 2022 collection of datasets templates and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2." />
<meta property="og:description" content="We study the design decisions of publicly available instruction tuning methods and break down the development of Flan 2022 (Chung et al. 2022). Through careful ablation studies on the Flan Collection of tasks and methods we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-1737;+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning and in particular training with mixed prompt settings (zero-shot few-shot and chain-of-thought) actually yields stronger (237;+) performance in all settings. In further experiments we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally to accelerate research on instruction tuning we make the Flan 2022 collection of datasets templates and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2." />
<link rel="canonical" href="https://learning2hash.github.io/publications/longpre2023flan/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/longpre2023flan/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The Flan Collection Designing Data And Methods For Effective Instruction Tuning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"We study the design decisions of publicly available instruction tuning methods and break down the development of Flan 2022 (Chung et al. 2022). Through careful ablation studies on the Flan Collection of tasks and methods we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-1737;+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning and in particular training with mixed prompt settings (zero-shot few-shot and chain-of-thought) actually yields stronger (237;+) performance in all settings. In further experiments we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally to accelerate research on instruction tuning we make the Flan 2022 collection of datasets templates and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.","headline":"The Flan Collection Designing Data And Methods For Effective Instruction Tuning","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/longpre2023flan/"},"url":"https://learning2hash.github.io/publications/longpre2023flan/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">The Flan Collection Designing Data And Methods For Effective Instruction Tuning</h1>
  <h5>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, Adam Roberts. Arxiv 2023</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2301.13688v2" target="_blank">Paper</a>]
    
      [<a href="https://github.com/google-research/FLAN/tree/main/flan/v2" target="_blank">Code</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=The Flan Collection Designing Data And Methods For Effective Instruction Tuning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=The Flan Collection Designing Data And Methods For Effective Instruction Tuning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Has Code">Has Code</a></tag>
    
  </p>
  <p><p>We study the design decisions of publicly available instruction tuning methods and break down the development of Flan 2022 (Chung et al. 2022). Through careful ablation studies on the Flan Collection of tasks and methods we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-1737;+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning and in particular training with mixed prompt settings (zero-shot few-shot and chain-of-thought) actually yields stronger (237;+) performance in all settings. In further experiments we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally to accelerate research on instruction tuning we make the Flan 2022 collection of datasets templates and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/longpre2023flan.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
