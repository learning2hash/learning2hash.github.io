<!DOCTYPE html>
<html lang="en-us">

  <!-- _includes/head.html -->
<head>
  <!-- begin code v 7.0 -->
  <span id="wts2185304"></span>
  <script>
  var wts7 = {};
  wts7.invisible='';
  wts7.page_name='';
  wts7.group_name='';
  wts7.conversion_number='';
  wts7.user_id='';
  var wts=document.createElement('script');wts.async=true;
  wts.src='https://app.ardalio.com/wts7.js';document.head.appendChild(wts);
  wts.onload = function(){ wtsl7(2185304,4); };
  </script><noscript><img src="https://app.ardalio.com/7/4/2185304.png"></noscript>
  <!-- end code v 7.0 -->
  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["\\(","\\)"]], displayMath: [["\\[","\\]"]] },
      options: { processHtmlClass: "mathjax-content", processEscapes: true }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">

  <!-- Viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- ✅ Manual SEO keywords (specific to Learning to Hash) -->
  <meta name="keywords" content="learning to hash, machine learning, hashing, approximate nearest neighbour search, ANN, LSH, locality sensitive hashing, vector quantization, deep hashing, binary embeddings, information retrieval, similarity search">

  <!-- ✅ Jekyll SEO plugin (title, description, canonical, OG/Twitter, JSON-LD) -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Revisiting (k)-nearest Neighbor Graph Construction On High-dimensional Data : Experiments And Analyses | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Revisiting (k)-nearest Neighbor Graph Construction On High-dimensional Data : Experiments And Analyses" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The (k)-nearest neighbor graph (KNNG) on high-dimensional data is a data structure widely used in many applications such as similarity search, dimension reduction and clustering. Due to its increasing popularity, several methods under the same framework have been proposed in the past decade. This framework contains two steps, i.e. building an initial KNNG (denoted as \texttt{INIT}) and then refining it by neighborhood propagation (denoted as \texttt{NBPG}). However, there remain several questions to be answered. First, it lacks a comprehensive experimental comparison among representative solutions in the literature. Second, some recently proposed indexing structures, e.g., SW and HNSW, have not been used or tested for building an initial KNNG. Third, the relationship between the data property and the effectiveness of \texttt{NBPG} is still not clear. To address these issues, we comprehensively compare the representative approaches on real-world high-dimensional data sets to provide practical and insightful suggestions for users. As the first attempt, we take SW and HNSW as the alternatives of \texttt{INIT} in our experiments. Moreover, we investigate the effectiveness of \texttt{NBPG} and find the strong correlation between the huness phenomenon and the performance of \texttt{NBPG}." />
<meta property="og:description" content="The (k)-nearest neighbor graph (KNNG) on high-dimensional data is a data structure widely used in many applications such as similarity search, dimension reduction and clustering. Due to its increasing popularity, several methods under the same framework have been proposed in the past decade. This framework contains two steps, i.e. building an initial KNNG (denoted as \texttt{INIT}) and then refining it by neighborhood propagation (denoted as \texttt{NBPG}). However, there remain several questions to be answered. First, it lacks a comprehensive experimental comparison among representative solutions in the literature. Second, some recently proposed indexing structures, e.g., SW and HNSW, have not been used or tested for building an initial KNNG. Third, the relationship between the data property and the effectiveness of \texttt{NBPG} is still not clear. To address these issues, we comprehensively compare the representative approaches on real-world high-dimensional data sets to provide practical and insightful suggestions for users. As the first attempt, we take SW and HNSW as the alternatives of \texttt{INIT} in our experiments. Moreover, we investigate the effectiveness of \texttt{NBPG} and find the strong correlation between the huness phenomenon and the performance of \texttt{NBPG}." />
<link rel="canonical" href="https://learning2hash.github.io/publications/yingfan2021revisiting/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/yingfan2021revisiting/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-01T12:36:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Revisiting (k)-nearest Neighbor Graph Construction On High-dimensional Data : Experiments And Analyses" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-01T12:36:00-05:00","datePublished":"2025-09-01T12:36:00-05:00","description":"The (k)-nearest neighbor graph (KNNG) on high-dimensional data is a data structure widely used in many applications such as similarity search, dimension reduction and clustering. Due to its increasing popularity, several methods under the same framework have been proposed in the past decade. This framework contains two steps, i.e. building an initial KNNG (denoted as \\texttt{INIT}) and then refining it by neighborhood propagation (denoted as \\texttt{NBPG}). However, there remain several questions to be answered. First, it lacks a comprehensive experimental comparison among representative solutions in the literature. Second, some recently proposed indexing structures, e.g., SW and HNSW, have not been used or tested for building an initial KNNG. Third, the relationship between the data property and the effectiveness of \\texttt{NBPG} is still not clear. To address these issues, we comprehensively compare the representative approaches on real-world high-dimensional data sets to provide practical and insightful suggestions for users. As the first attempt, we take SW and HNSW as the alternatives of \\texttt{INIT} in our experiments. Moreover, we investigate the effectiveness of \\texttt{NBPG} and find the strong correlation between the huness phenomenon and the performance of \\texttt{NBPG}.","headline":"Revisiting (k)-nearest Neighbor Graph Construction On High-dimensional Data : Experiments And Analyses","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/yingfan2021revisiting/"},"url":"https://learning2hash.github.io/publications/yingfan2021revisiting/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Site CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" type="application/opensearchdescription+xml" title="learning2hash" />

  <!-- ✅ Single, modern jQuery + DataTables -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" defer></script>
  <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js" defer></script>

  <!-- Optional sanity log -->
  <script defer>
    window.addEventListener('DOMContentLoaded', () => {
      console.log('jQuery:', jQuery?.fn?.jquery);
      console.log('DataTables loaded:', !!jQuery?.fn?.dataTable);
    });
  </script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/opensource.html">Tools Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script defer>
document.addEventListener('DOMContentLoaded', function () {
  function doSearch() {
    const el = document.getElementById('searchTarget');
    if (!el) return;
    const q = el.value.trim();
    try { if (typeof ga === 'function') ga('send', 'event', 'search', 'search', q); } catch(e) {}
    window.location = "/papers.html#" + encodeURIComponent(q);
  }

  // jQuery path if loaded
  if (window.jQuery) {
    $('#searchTarget').on('keydown', function (e) {
      if (e.key === 'Enter') doSearch();
    });
    document.querySelector('.sidebar-item button')?.addEventListener('click', doSearch);
  } else {
    // vanilla fallback
    const input = document.getElementById('searchTarget');
    const btn = document.querySelector('.sidebar-item button');
    if (input) {
      input.addEventListener('keydown', function (e) {
        if (e.key === 'Enter') doSearch();
      });
    }
    if (btn) btn.addEventListener('click', doSearch);
  }

  // keep global for inline onClick="search()"
  window.search = doSearch;
});
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Revisiting \(k\)-nearest Neighbor Graph Construction On High-dimensional Data : Experiments And Analyses</h1>

  <h5>
    
    
    <a href="https://scholar.google.com/scholar?q=Liu%20Yingfan,%20Cheng%20Hong,%20Cui%20Jiangtao" 
       target="_blank" rel="noopener noreferrer">
      Liu Yingfan, Cheng Hong, Cui Jiangtao
    </a>
    
    
    . Arxiv
     2021
    
      – <span>2 citations</span>
    
  </h5>

  <p>
    
      [<a href="https://arxiv.org/abs/2112.02234" target="_blank" rel="noopener noreferrer">Paper</a>]
    
    &nbsp;<a href="https://scholar.google.com/scholar?q=Revisiting%20%5C(k%5C)-nearest%20Neighbor%20Graph%20Construction%20On%20High-dimensional%20Data%20:%20Experiments%20And%20Analyses" target="_blank" rel="noopener noreferrer">
      <img style="display:inline; margin:0;" src="/public/media/google-scholar.png" alt="Search on Google Scholar"/>
    </a>
    &nbsp;<a href="https://www.semanticscholar.org/search?q=Revisiting%20%5C(k%5C)-nearest%20Neighbor%20Graph%20Construction%20On%20High-dimensional%20Data%20:%20Experiments%20And%20Analyses" target="_blank" rel="noopener noreferrer">
      <img style="display:inline; margin:0;" src="/public/media/semscholar.png" alt="Search on Semantic Scholar"/>
    </a>
    <br/>
    
      <tag><a href="/tags.html#Evaluation">Evaluation</a></tag>
    
      <tag><a href="/tags.html#Graph%20Based%20ANN">Graph Based ANN</a></tag>
    
      <tag><a href="/tags.html#Similarity%20Search">Similarity Search</a></tag>
    
      <tag><a href="/tags.html#Tools%20&%20Libraries">Tools & Libraries</a></tag>
    
  </p>

  <p><p>The (k)-nearest neighbor graph (KNNG) on high-dimensional data is a data
structure widely used in many applications such as similarity search, dimension
reduction and clustering. Due to its increasing popularity, several methods
under the same framework have been proposed in the past decade. This framework
contains two steps, i.e. building an initial KNNG (denoted as \texttt{INIT})
and then refining it by neighborhood propagation (denoted as \texttt{NBPG}).
However, there remain several questions to be answered. First, it lacks a
comprehensive experimental comparison among representative solutions in the
literature. Second, some recently proposed indexing structures, e.g., SW and
HNSW, have not been used or tested for building an initial KNNG. Third, the
relationship between the data property and the effectiveness of \texttt{NBPG}
is still not clear. To address these issues, we comprehensively compare the
representative approaches on real-world high-dimensional data sets to provide
practical and insightful suggestions for users. As the first attempt, we take
SW and HNSW as the alternatives of \texttt{INIT} in our experiments. Moreover,
we investigate the effectiveness of \texttt{NBPG} and find the strong
correlation between the huness phenomenon and the performance of \texttt{NBPG}.</p>
</p>

  <h6>Similar Work</h6>
  <ul id="relwork"></ul>

  <!-- Vanilla JS: no jQuery needed -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var relwork = document.getElementById('relwork');
      if (!relwork) return;

      var metaPath = "/publications-metadata/yingfan2021revisiting.json";

      fetch(metaPath, { credentials: 'same-origin' })
        .then(function (res) {
          if (!res.ok) throw new Error(res.status + " " + res.statusText);
          return res.json();
        })
        .then(function (data) {
          if (!Array.isArray(data)) return;
          relwork.innerHTML = data
            .map(function (d) {
              var slug = d[0];
              var title = d[1];
              return '<li><a href="/publications/' + slug + '">' + title + '</a></li>';
            })
            .join('');
        })
        .catch(function (err) {
          console.warn("Failed to load similar work JSON:", err);
        });
    });
  </script>
</div>

    </div>

  </body>
</html>
