<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Holistic Evaluation Of Language Models | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Holistic Evaluation Of Language Models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Language models (LMs) are becoming the foundation for almost all major language technologies but their capabilities limitations and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility noting whats missing or underrepresented (e.g. question answering for neglected English dialects metrics for trustworthiness). Second we adopt a multi-metric approach We measure 7 metrics (accuracy calibration robustness fairness bias toxicity and efficiency) for each of 16 core scenarios when possible (87.537; of the time). This ensures metrics beyond accuracy dont fall to the wayside and that trade-offs are clearly exposed. We also perform 7 targeted evaluations based on 26 targeted scenarios to analyze specific aspects (e.g. reasoning disinformation). Third we conduct a large-scale evaluation of 30 prominent language models (spanning open limited-access and closed models) on all 42 scenarios 21 of which were not previously used in mainstream LM evaluation. Prior to HELM models on average were evaluated on just 17.937; of the core HELM scenarios with some prominent models not sharing a single scenario in common. We improve this to 96.037; now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency we release all raw model prompts and completions publicly for further analysis as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community continuously updated with new scenarios metrics and models." />
<meta property="og:description" content="Language models (LMs) are becoming the foundation for almost all major language technologies but their capabilities limitations and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility noting whats missing or underrepresented (e.g. question answering for neglected English dialects metrics for trustworthiness). Second we adopt a multi-metric approach We measure 7 metrics (accuracy calibration robustness fairness bias toxicity and efficiency) for each of 16 core scenarios when possible (87.537; of the time). This ensures metrics beyond accuracy dont fall to the wayside and that trade-offs are clearly exposed. We also perform 7 targeted evaluations based on 26 targeted scenarios to analyze specific aspects (e.g. reasoning disinformation). Third we conduct a large-scale evaluation of 30 prominent language models (spanning open limited-access and closed models) on all 42 scenarios 21 of which were not previously used in mainstream LM evaluation. Prior to HELM models on average were evaluated on just 17.937; of the core HELM scenarios with some prominent models not sharing a single scenario in common. We improve this to 96.037; now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency we release all raw model prompts and completions publicly for further analysis as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community continuously updated with new scenarios metrics and models." />
<link rel="canonical" href="https://learning2hash.github.io/publications/liang2022holistic/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/liang2022holistic/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Holistic Evaluation Of Language Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Language models (LMs) are becoming the foundation for almost all major language technologies but their capabilities limitations and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility noting whats missing or underrepresented (e.g. question answering for neglected English dialects metrics for trustworthiness). Second we adopt a multi-metric approach We measure 7 metrics (accuracy calibration robustness fairness bias toxicity and efficiency) for each of 16 core scenarios when possible (87.537; of the time). This ensures metrics beyond accuracy dont fall to the wayside and that trade-offs are clearly exposed. We also perform 7 targeted evaluations based on 26 targeted scenarios to analyze specific aspects (e.g. reasoning disinformation). Third we conduct a large-scale evaluation of 30 prominent language models (spanning open limited-access and closed models) on all 42 scenarios 21 of which were not previously used in mainstream LM evaluation. Prior to HELM models on average were evaluated on just 17.937; of the core HELM scenarios with some prominent models not sharing a single scenario in common. We improve this to 96.037; now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency we release all raw model prompts and completions publicly for further analysis as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community continuously updated with new scenarios metrics and models.","headline":"Holistic Evaluation Of Language Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/liang2022holistic/"},"url":"https://learning2hash.github.io/publications/liang2022holistic/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Holistic Evaluation Of Language Models</h1>
  <h5>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher RÃ©, Diana Acosta-navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda. Published in Transactions on Machine Learning Research 2022</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2211.09110v2" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Holistic Evaluation Of Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Holistic Evaluation Of Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
  </p>
  <p><p>Language models (LMs) are becoming the foundation for almost all major language technologies but their capabilities limitations and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility noting whats missing or underrepresented (e.g. question answering for neglected English dialects metrics for trustworthiness). Second we adopt a multi-metric approach We measure 7 metrics (accuracy calibration robustness fairness bias toxicity and efficiency) for each of 16 core scenarios when possible (87.537; of the time). This ensures metrics beyond accuracy dont fall to the wayside and that trade-offs are clearly exposed. We also perform 7 targeted evaluations based on 26 targeted scenarios to analyze specific aspects (e.g. reasoning disinformation). Third we conduct a large-scale evaluation of 30 prominent language models (spanning open limited-access and closed models) on all 42 scenarios 21 of which were not previously used in mainstream LM evaluation. Prior to HELM models on average were evaluated on just 17.937; of the core HELM scenarios with some prominent models not sharing a single scenario in common. We improve this to 96.037; now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency we release all raw model prompts and completions publicly for further analysis as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community continuously updated with new scenarios metrics and models.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/liang2022holistic.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
