<hr />
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>layout: publication
title: "Transitive Hashing Network for Heterogeneous Multimedia Retrieval"
authors: Cao Zhangjie, Long Mingsheng, Yang Qiang
conference: Arxiv
year: 2016
bibkey: cao2016transitive
additional_links:
  - {name: "Paper", url: "https://arxiv.org/abs/1608.04307"}
tags: ['ARXIV', 'Cross Modal']
---
Hashing has been widely applied to large-scale multimedia retrieval due to the storage and retrieval efficiency. Cross-modal hashing enables efficient retrieval from database of one modality in response to a query of another modality. Existing work on cross-modal hashing assumes heterogeneous relationship across modalities for hash function learning. In this paper, we relax the strong assumption by only requiring such heterogeneous relationship in an auxiliary dataset different from the query/database domain. We craft a hybrid deep architecture to simultaneously learn the cross-modal correlation from the auxiliary dataset, and align the dataset distributions between the auxiliary dataset and the query/database domain, which generates transitive hash codes for heterogeneous multimedia retrieval. Extensive experiments exhibit that the proposed approach yields state of the art multimedia retrieval performance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.
</code></pre></div></div>
