<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>No Language Left Behind Scaling Human-centered Machine Translation | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="No Language Left Behind Scaling Human-centered Machine Translation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Driven by the goal of eradicating language barriers on a global scale machine translation has solidified itself as a key focus of artificial intelligence research today. However such efforts have coalesced around a small subset of languages leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe high quality results all while keeping ethical considerations in mind In No Language Left Behind we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically we evaluated the performance of over 40000 different translation directions using a human-translated benchmark Flores-200 and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 4437; BLEU relative to the previous state-of-the-art laying important groundwork towards realizing a universal translation system. Finally we open source all contributions described in this work accessible at https://github.com/facebookresearch/fairseq/tree/nllb." />
<meta property="og:description" content="Driven by the goal of eradicating language barriers on a global scale machine translation has solidified itself as a key focus of artificial intelligence research today. However such efforts have coalesced around a small subset of languages leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe high quality results all while keeping ethical considerations in mind In No Language Left Behind we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically we evaluated the performance of over 40000 different translation directions using a human-translated benchmark Flores-200 and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 4437; BLEU relative to the previous state-of-the-art laying important groundwork towards realizing a universal translation system. Finally we open source all contributions described in this work accessible at https://github.com/facebookresearch/fairseq/tree/nllb." />
<link rel="canonical" href="https://learning2hash.github.io/publications/nllb2022no/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/nllb2022no/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="No Language Left Behind Scaling Human-centered Machine Translation" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Driven by the goal of eradicating language barriers on a global scale machine translation has solidified itself as a key focus of artificial intelligence research today. However such efforts have coalesced around a small subset of languages leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe high quality results all while keeping ethical considerations in mind In No Language Left Behind we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically we evaluated the performance of over 40000 different translation directions using a human-translated benchmark Flores-200 and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 4437; BLEU relative to the previous state-of-the-art laying important groundwork towards realizing a universal translation system. Finally we open source all contributions described in this work accessible at https://github.com/facebookresearch/fairseq/tree/nllb.","headline":"No Language Left Behind Scaling Human-centered Machine Translation","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/nllb2022no/"},"url":"https://learning2hash.github.io/publications/nllb2022no/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">No Language Left Behind Scaling Human-centered Machine Translation</h1>
  <h5>Nllb Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang. Arxiv 2022</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2207.04672v3" target="_blank">Paper</a>]
    
      [<a href="https://github.com/facebookresearch/fairseq/tree/nllb" target="_blank">Code</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=No Language Left Behind Scaling Human-centered Machine Translation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=No Language Left Behind Scaling Human-centered Machine Translation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Has Code">Has Code</a></tag>
    
  </p>
  <p><p>Driven by the goal of eradicating language barriers on a global scale machine translation has solidified itself as a key focus of artificial intelligence research today. However such efforts have coalesced around a small subset of languages leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe high quality results all while keeping ethical considerations in mind In No Language Left Behind we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically we evaluated the performance of over 40000 different translation directions using a human-translated benchmark Flores-200 and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 4437; BLEU relative to the previous state-of-the-art laying important groundwork towards realizing a universal translation system. Finally we open source all contributions described in this work accessible at https://github.com/facebookresearch/fairseq/tree/nllb.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/nllb2022no.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
