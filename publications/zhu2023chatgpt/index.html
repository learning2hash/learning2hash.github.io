<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Asking insightful questions is crucial for acquiring knowledge and expanding our understanding of the world. However the importance of questioning has been largely overlooked in AI research where models have been primarily developed to answer questions. With the recent advancements of large language models (LLMs) like ChatGPT we discover their capability to ask high-quality questions when provided with a suitable prompt. This discovery presents a new opportunity to develop an automatic questioning system. In this paper we introduce ChatCaptioner a novel automatic-questioning method deployed in image captioning. Here ChatGPT is prompted to ask a series of informative questions about images to BLIP-2 a strong vision question-answering model. By keeping acquiring new visual information from BLIP-2s answers ChatCaptioner is able to generate more enriched image descriptions. We conduct human-subject evaluations on common image caption datasets such as COCO Conceptual Caption and WikiArt and compare ChatCaptioner with BLIP-2 as well as ground truth. Our results demonstrate that ChatCaptioners captions are significantly more informative receiving three times as many votes from human evaluators for providing the most image information. Besides ChatCaptioner identifies 5337; more objects within the image than BLIP-2 alone measured by WordNet synset matching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner" />
<meta property="og:description" content="Asking insightful questions is crucial for acquiring knowledge and expanding our understanding of the world. However the importance of questioning has been largely overlooked in AI research where models have been primarily developed to answer questions. With the recent advancements of large language models (LLMs) like ChatGPT we discover their capability to ask high-quality questions when provided with a suitable prompt. This discovery presents a new opportunity to develop an automatic questioning system. In this paper we introduce ChatCaptioner a novel automatic-questioning method deployed in image captioning. Here ChatGPT is prompted to ask a series of informative questions about images to BLIP-2 a strong vision question-answering model. By keeping acquiring new visual information from BLIP-2s answers ChatCaptioner is able to generate more enriched image descriptions. We conduct human-subject evaluations on common image caption datasets such as COCO Conceptual Caption and WikiArt and compare ChatCaptioner with BLIP-2 as well as ground truth. Our results demonstrate that ChatCaptioners captions are significantly more informative receiving three times as many votes from human evaluators for providing the most image information. Besides ChatCaptioner identifies 5337; more objects within the image than BLIP-2 alone measured by WordNet synset matching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner" />
<link rel="canonical" href="https://learning2hash.github.io/publications/zhu2023chatgpt/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/zhu2023chatgpt/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Asking insightful questions is crucial for acquiring knowledge and expanding our understanding of the world. However the importance of questioning has been largely overlooked in AI research where models have been primarily developed to answer questions. With the recent advancements of large language models (LLMs) like ChatGPT we discover their capability to ask high-quality questions when provided with a suitable prompt. This discovery presents a new opportunity to develop an automatic questioning system. In this paper we introduce ChatCaptioner a novel automatic-questioning method deployed in image captioning. Here ChatGPT is prompted to ask a series of informative questions about images to BLIP-2 a strong vision question-answering model. By keeping acquiring new visual information from BLIP-2s answers ChatCaptioner is able to generate more enriched image descriptions. We conduct human-subject evaluations on common image caption datasets such as COCO Conceptual Caption and WikiArt and compare ChatCaptioner with BLIP-2 as well as ground truth. Our results demonstrate that ChatCaptioners captions are significantly more informative receiving three times as many votes from human evaluators for providing the most image information. Besides ChatCaptioner identifies 5337; more objects within the image than BLIP-2 alone measured by WordNet synset matching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner","headline":"Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/zhu2023chatgpt/"},"url":"https://learning2hash.github.io/publications/zhu2023chatgpt/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions</h1>
  <h5>Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, Mohamed Elhoseiny. Arxiv 2023</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2303.06594v1" target="_blank">Paper</a>]
    
      [<a href="https://github.com/Vision-CAIR/ChatCaptioner" target="_blank">Code</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Chatgpt Asks BLIP-2 Answers Automatic Questioning Towards Enriched Visual Descriptions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Has Code">Has Code</a></tag>
    
      <tag><a href="/tags.html#Supervised">Supervised</a></tag>
    
  </p>
  <p><p>Asking insightful questions is crucial for acquiring knowledge and expanding our understanding of the world. However the importance of questioning has been largely overlooked in AI research where models have been primarily developed to answer questions. With the recent advancements of large language models (LLMs) like ChatGPT we discover their capability to ask high-quality questions when provided with a suitable prompt. This discovery presents a new opportunity to develop an automatic questioning system. In this paper we introduce ChatCaptioner a novel automatic-questioning method deployed in image captioning. Here ChatGPT is prompted to ask a series of informative questions about images to BLIP-2 a strong vision question-answering model. By keeping acquiring new visual information from BLIP-2s answers ChatCaptioner is able to generate more enriched image descriptions. We conduct human-subject evaluations on common image caption datasets such as COCO Conceptual Caption and WikiArt and compare ChatCaptioner with BLIP-2 as well as ground truth. Our results demonstrate that ChatCaptioners captions are significantly more informative receiving three times as many votes from human evaluators for providing the most image information. Besides ChatCaptioner identifies 5337; more objects within the image than BLIP-2 alone measured by WordNet synset matching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/zhu2023chatgpt.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
