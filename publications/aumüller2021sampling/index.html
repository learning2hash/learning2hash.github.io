<!DOCTYPE html>
<html lang="en-us">

  <!-- _includes/head.html -->
<head>
  <!-- begin code v 7.0 -->
  <span id="wts2185304"></span>
  <script>
  var wts7 = {};
  wts7.invisible='';
  wts7.page_name='';
  wts7.group_name='';
  wts7.conversion_number='';
  wts7.user_id='';
  var wts=document.createElement('script');wts.async=true;
  wts.src='https://app.ardalio.com/wts7.js';document.head.appendChild(wts);
  wts.onload = function(){ wtsl7(2185304,4); };
  </script><noscript><img src="https://app.ardalio.com/7/4/2185304.png"></noscript>
  <!-- end code v 7.0 -->
  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["\\(","\\)"]], displayMath: [["\\[","\\]"]] },
      options: { processHtmlClass: "mathjax-content", processEscapes: true }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">

  <!-- Viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- ✅ Manual SEO keywords (specific to Learning to Hash) -->
  <meta name="keywords" content="learning to hash, machine learning, hashing, approximate nearest neighbour search, ANN, LSH, locality sensitive hashing, vector quantization, deep hashing, binary embeddings, information retrieval, similarity search">

  <!-- ✅ Jekyll SEO plugin (title, description, canonical, OG/Twitter, JSON-LD) -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Sampling A Near Neighbor In High Dimensions – Who Is The Fairest Of Them All? | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Sampling A Near Neighbor In High Dimensions – Who Is The Fairest Of Them All?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. Given a set of points \(S\) and a radius parameter \(r&gt;0\), the \(r\)-near neighbor (\(r\)-NN) problem asks for a data structure that, given any query point \(q\), returns a point \(p\) within distance at most \(r\) from \(q\). In this paper, we study the \(r\)-NN problem in the light of individual fairness and providing equal opportunities: all points that are within distance \(r\) from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee. In this work, we show that LSH based algorithms can be made fair, without a significant loss in efficiency. We propose several efficient data structures for the exact and approximate variants of the fair NN problem. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. We also develop a data structure for fair similarity search under inner product that requires nearly-linear space and exploits locality sensitive filters. The paper concludes with an experimental evaluation that highlights the inherent unfairness of NN data structures and shows the performance of our algorithms on real-world datasets." />
<meta property="og:description" content="Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. Given a set of points \(S\) and a radius parameter \(r&gt;0\), the \(r\)-near neighbor (\(r\)-NN) problem asks for a data structure that, given any query point \(q\), returns a point \(p\) within distance at most \(r\) from \(q\). In this paper, we study the \(r\)-NN problem in the light of individual fairness and providing equal opportunities: all points that are within distance \(r\) from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee. In this work, we show that LSH based algorithms can be made fair, without a significant loss in efficiency. We propose several efficient data structures for the exact and approximate variants of the fair NN problem. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. We also develop a data structure for fair similarity search under inner product that requires nearly-linear space and exploits locality sensitive filters. The paper concludes with an experimental evaluation that highlights the inherent unfairness of NN data structures and shows the performance of our algorithms on real-world datasets." />
<link rel="canonical" href="https://learning2hash.github.io/publications/aum%C3%BCller2021sampling/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/aum%C3%BCller2021sampling/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-31T14:32:34-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Sampling A Near Neighbor In High Dimensions – Who Is The Fairest Of Them All?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-31T14:32:34-05:00","datePublished":"2025-08-31T14:32:34-05:00","description":"Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. Given a set of points \\(S\\) and a radius parameter \\(r&gt;0\\), the \\(r\\)-near neighbor (\\(r\\)-NN) problem asks for a data structure that, given any query point \\(q\\), returns a point \\(p\\) within distance at most \\(r\\) from \\(q\\). In this paper, we study the \\(r\\)-NN problem in the light of individual fairness and providing equal opportunities: all points that are within distance \\(r\\) from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee. In this work, we show that LSH based algorithms can be made fair, without a significant loss in efficiency. We propose several efficient data structures for the exact and approximate variants of the fair NN problem. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. We also develop a data structure for fair similarity search under inner product that requires nearly-linear space and exploits locality sensitive filters. The paper concludes with an experimental evaluation that highlights the inherent unfairness of NN data structures and shows the performance of our algorithms on real-world datasets.","headline":"Sampling A Near Neighbor In High Dimensions – Who Is The Fairest Of Them All?","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/aum%C3%BCller2021sampling/"},"url":"https://learning2hash.github.io/publications/aum%C3%BCller2021sampling/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Site CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" type="application/opensearchdescription+xml" title="learning2hash" />

  <!-- ✅ Single, modern jQuery + DataTables -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" defer></script>
  <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js" defer></script>

  <!-- Optional sanity log -->
  <script defer>
    window.addEventListener('DOMContentLoaded', () => {
      console.log('jQuery:', jQuery?.fn?.jquery);
      console.log('DataTables loaded:', !!jQuery?.fn?.dataTable);
    });
  </script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/opensource.html">Tools Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script defer>
document.addEventListener('DOMContentLoaded', function () {
  function doSearch() {
    const el = document.getElementById('searchTarget');
    if (!el) return;
    const q = el.value.trim();
    try { if (typeof ga === 'function') ga('send', 'event', 'search', 'search', q); } catch(e) {}
    window.location = "/papers.html#" + encodeURIComponent(q);
  }

  // jQuery path if loaded
  if (window.jQuery) {
    $('#searchTarget').on('keydown', function (e) {
      if (e.key === 'Enter') doSearch();
    });
    document.querySelector('.sidebar-item button')?.addEventListener('click', doSearch);
  } else {
    // vanilla fallback
    const input = document.getElementById('searchTarget');
    const btn = document.querySelector('.sidebar-item button');
    if (input) {
      input.addEventListener('keydown', function (e) {
        if (e.key === 'Enter') doSearch();
      });
    }
    if (btn) btn.addEventListener('click', doSearch);
  }

  // keep global for inline onClick="search()"
  window.search = doSearch;
});
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Sampling A Near Neighbor In High Dimensions -- Who Is The Fairest Of Them All?</h1>

  <h5>
    
    
    <a href="https://scholar.google.com/scholar?q=Martin%20Aum%C3%BCller,%20Sariel%20Har-peled,%20Sepideh%20Mahabadi,%20Rasmus%20Pagh,%20Francesco%20Silvestri" 
       target="_blank" rel="noopener noreferrer">
      Martin Aumüller, Sariel Har-peled, Sepideh Mahabadi, Rasmus Pagh, Francesco Silvestri
    </a>
    
    
    . ACM Transactions on Database Systems
     2022
    
      – <span>4 citations</span>
    
  </h5>

  <p>
    
      [<a href="https://arxiv.org/abs/2101.10905" target="_blank" rel="noopener noreferrer">Paper</a>]
    
    &nbsp;<a href="https://scholar.google.com/scholar?q=Sampling%20A%20Near%20Neighbor%20In%20High%20Dimensions%20--%20Who%20Is%20The%20Fairest%20Of%20Them%20All?" target="_blank" rel="noopener noreferrer">
      <img style="display:inline; margin:0;" src="/public/media/google-scholar.png" alt="Search on Google Scholar"/>
    </a>
    &nbsp;<a href="https://www.semanticscholar.org/search?q=Sampling%20A%20Near%20Neighbor%20In%20High%20Dimensions%20--%20Who%20Is%20The%20Fairest%20Of%20Them%20All?" target="_blank" rel="noopener noreferrer">
      <img style="display:inline; margin:0;" src="/public/media/semscholar.png" alt="Search on Semantic Scholar"/>
    </a>
    <br/>
    
      <tag><a href="/tags.html#Datasets">Datasets</a></tag>
    
      <tag><a href="/tags.html#Efficiency">Efficiency</a></tag>
    
      <tag><a href="/tags.html#Evaluation">Evaluation</a></tag>
    
      <tag><a href="/tags.html#Hashing%20Methods">Hashing Methods</a></tag>
    
      <tag><a href="/tags.html#Locality-Sensitive-Hashing">Locality-Sensitive-Hashing</a></tag>
    
      <tag><a href="/tags.html#Similarity%20Search">Similarity Search</a></tag>
    
  </p>

  <p><p>Similarity search is a fundamental algorithmic primitive, widely used in many
computer science disciplines. Given a set of points \(S\) and a radius parameter
\(r&gt;0\), the \(r\)-near neighbor (\(r\)-NN) problem asks for a data structure that,
given any query point \(q\), returns a point \(p\) within distance at most \(r\) from
\(q\). In this paper, we study the \(r\)-NN problem in the light of individual
fairness and providing equal opportunities: all points that are within distance
\(r\) from the query should have the same probability to be returned. In the
low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS
2014). Locality sensitive hashing (LSH), the theoretically strongest approach
to similarity search in high dimensions, does not provide such a fairness
guarantee. In this work, we show that LSH based algorithms can be made fair,
without a significant loss in efficiency. We propose several efficient data
structures for the exact and approximate variants of the fair NN problem. Our
approach works more generally for sampling uniformly from a sub-collection of
sets of a given collection and can be used in a few other applications. We also
develop a data structure for fair similarity search under inner product that
requires nearly-linear space and exploits locality sensitive filters. The paper
concludes with an experimental evaluation that highlights the inherent
unfairness of NN data structures and shows the performance of our algorithms on
real-world datasets.</p>
</p>

  <h6>Similar Work</h6>
  <ul id="relwork"></ul>

  <!-- Vanilla JS: no jQuery needed -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var relwork = document.getElementById('relwork');
      if (!relwork) return;

      var metaPath = "/publications-metadata/aumüller2021sampling.json";

      fetch(metaPath, { credentials: 'same-origin' })
        .then(function (res) {
          if (!res.ok) throw new Error(res.status + " " + res.statusText);
          return res.json();
        })
        .then(function (data) {
          if (!Array.isArray(data)) return;
          relwork.innerHTML = data
            .map(function (d) {
              var slug = d[0];
              var title = d[1];
              return '<li><a href="/publications/' + slug + '">' + title + '</a></li>';
            })
            .join('');
        })
        .catch(function (err) {
          console.warn("Failed to load similar work JSON:", err);
        });
    });
  </script>
</div>

    </div>

  </body>
</html>
