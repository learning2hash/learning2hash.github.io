<!DOCTYPE html>
<html lang="en-us">

  <!-- _includes/head.html -->
<head>
  <!-- Analytics & trackers (leave as-is) -->
  <span id="wts2185304"></span>
  <script>
    var wts=document.createElement('script');wts.async=true;
    wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
    wts.onload=function(){ wtslog7(2185304,4); };
  </script>
  <noscript>
    <a href="https://www.web-stat.com">
      <img src="https://app.ardalio.com/7/4/2185304.png" alt="Web-Stat web statistics">
    </a>
  </noscript>

  <script>
    (function(h,o,t,j,a,r){
      h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
      h._hjSettings={hjid:1843243,hjsv:6};
      a=o.getElementsByTagName('head')[0];
      r=o.createElement('script');r.async=1;
      r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
      a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
  </script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-109544763-1');
  </script>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["\\(","\\)"]], displayMath: [["\\[","\\]"]] },
      options: { processHtmlClass: "mathjax-content", processEscapes: true }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">

  <!-- Viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- ✅ Manual SEO keywords (specific to Learning to Hash) -->
  <meta name="keywords" content="learning to hash, machine learning, hashing, approximate nearest neighbour search, ANN, LSH, locality sensitive hashing, vector quantization, deep hashing, binary embeddings, information retrieval, similarity search">

  <!-- ✅ Jekyll SEO plugin (title, description, canonical, OG/Twitter, JSON-LD) -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Unifying Latent And Lexicon Representations For Effective Video-text Retrieval | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Unifying Latent And Lexicon Representations For Effective Video-text Retrieval" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In video-text retrieval, most existing methods adopt the dual-encoder architecture for fast retrieval, which employs two individual encoders to extract global latent representations for videos and texts. However, they face challenges in capturing fine-grained semantic concepts. In this work, we propose the UNIFY framework, which learns lexicon representations to capture fine-grained semantics and combines the strengths of latent and lexicon representations for video-text retrieval. Specifically, we map videos and texts into a pre-defined lexicon space, where each dimension corresponds to a semantic concept. A two-stage semantics grounding approach is proposed to activate semantically relevant dimensions and suppress irrelevant dimensions. The learned lexicon representations can thus reflect fine-grained semantics of videos and texts. Furthermore, to leverage the complementarity between latent and lexicon representations, we propose a unified learning scheme to facilitate mutual learning via structure sharing and self-distillation. Experimental results show our UNIFY framework largely outperforms previous video-text retrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and DiDeMo respectively." />
<meta property="og:description" content="In video-text retrieval, most existing methods adopt the dual-encoder architecture for fast retrieval, which employs two individual encoders to extract global latent representations for videos and texts. However, they face challenges in capturing fine-grained semantic concepts. In this work, we propose the UNIFY framework, which learns lexicon representations to capture fine-grained semantics and combines the strengths of latent and lexicon representations for video-text retrieval. Specifically, we map videos and texts into a pre-defined lexicon space, where each dimension corresponds to a semantic concept. A two-stage semantics grounding approach is proposed to activate semantically relevant dimensions and suppress irrelevant dimensions. The learned lexicon representations can thus reflect fine-grained semantics of videos and texts. Furthermore, to leverage the complementarity between latent and lexicon representations, we propose a unified learning scheme to facilitate mutual learning via structure sharing and self-distillation. Experimental results show our UNIFY framework largely outperforms previous video-text retrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and DiDeMo respectively." />
<link rel="canonical" href="https://learning2hash.github.io/publications/liu2024unifying/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/liu2024unifying/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-18T12:28:47-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Unifying Latent And Lexicon Representations For Effective Video-text Retrieval" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-18T12:28:47-05:00","datePublished":"2025-08-18T12:28:47-05:00","description":"In video-text retrieval, most existing methods adopt the dual-encoder architecture for fast retrieval, which employs two individual encoders to extract global latent representations for videos and texts. However, they face challenges in capturing fine-grained semantic concepts. In this work, we propose the UNIFY framework, which learns lexicon representations to capture fine-grained semantics and combines the strengths of latent and lexicon representations for video-text retrieval. Specifically, we map videos and texts into a pre-defined lexicon space, where each dimension corresponds to a semantic concept. A two-stage semantics grounding approach is proposed to activate semantically relevant dimensions and suppress irrelevant dimensions. The learned lexicon representations can thus reflect fine-grained semantics of videos and texts. Furthermore, to leverage the complementarity between latent and lexicon representations, we propose a unified learning scheme to facilitate mutual learning via structure sharing and self-distillation. Experimental results show our UNIFY framework largely outperforms previous video-text retrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and DiDeMo respectively.","headline":"Unifying Latent And Lexicon Representations For Effective Video-text Retrieval","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/liu2024unifying/"},"url":"https://learning2hash.github.io/publications/liu2024unifying/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Site CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" type="application/opensearchdescription+xml" title="learning2hash" />

  <!-- ✅ Single, modern jQuery + DataTables -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" defer></script>
  <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js" defer></script>

  <!-- Optional sanity log -->
  <script defer>
    window.addEventListener('DOMContentLoaded', () => {
      console.log('jQuery:', jQuery?.fn?.jquery);
      console.log('DataTables loaded:', !!jQuery?.fn?.dataTable);
    });
  </script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/opensource.html">Tools Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Unifying Latent And Lexicon Representations For Effective Video-text Retrieval</h1>
  <h5>
  
    
      Haowei Liu, Yaya Shi, Haiyang Xu, Chunfeng Yuan, Qinghao Ye, Chenliang Li, Ming Yan, Ji Zhang, Fei Huang, Bing Li, Weiming Hu
    
  
  . Arxiv
   2024
  
    – <span>0 citations</span>
  
  </h5>
  <p>
    
      [<a href="https://arxiv.org/abs/2402.16769" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Unifying Latent And Lexicon Representations For Effective Video-text Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Unifying Latent And Lexicon Representations For Effective Video-text Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#Efficiency">Efficiency</a></tag>
    
      <tag><a href="/tags.html#Evaluation">Evaluation</a></tag>
    
      <tag><a href="/tags.html#Text Retrieval">Text Retrieval</a></tag>
    
  </p>
  <p><p>In video-text retrieval, most existing methods adopt the dual-encoder
architecture for fast retrieval, which employs two individual encoders to
extract global latent representations for videos and texts. However, they face
challenges in capturing fine-grained semantic concepts. In this work, we
propose the UNIFY framework, which learns lexicon representations to capture
fine-grained semantics and combines the strengths of latent and lexicon
representations for video-text retrieval. Specifically, we map videos and texts
into a pre-defined lexicon space, where each dimension corresponds to a
semantic concept. A two-stage semantics grounding approach is proposed to
activate semantically relevant dimensions and suppress irrelevant dimensions.
The learned lexicon representations can thus reflect fine-grained semantics of
videos and texts. Furthermore, to leverage the complementarity between latent
and lexicon representations, we propose a unified learning scheme to facilitate
mutual learning via structure sharing and self-distillation. Experimental
results show our UNIFY framework largely outperforms previous video-text
retrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and
DiDeMo respectively.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

 <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/liu2024unifying.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>


</div>

    </div>

  </body>
</html>
