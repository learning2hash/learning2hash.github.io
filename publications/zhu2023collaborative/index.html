<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Collaborative Large Language Model For Recommender Systems | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Collaborative Large Language Model For Recommender Systems" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Recently there has been growing interest in developing the next-generation recommender systems (RSs) based on pretrained large language models (LLMs). However the semantic gap between natural language and recommendation tasks is still not well addressed leading to multiple issues such as spuriously correlated user/item descriptors ineffective language modeling on user/item data inefficient recommendations via auto-regression etc. In this paper we propose CLLM4Rec the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RSs aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model user/item collaborative and content semantics. Accordingly a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens to facilitate stable and effective language modeling. In addition a novel mutual regularization strategy is introduced to encourage CLLM4Rec to capture recommendation-related information from noisy user/item content. Finally we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on soft+hard prompts established from masked user-item interaction history where recommendations of multiple items can be generated efficiently without hallucination. Codes are released at https://github.com/yaochenzhu/llm4rec." />
<meta property="og:description" content="Recently there has been growing interest in developing the next-generation recommender systems (RSs) based on pretrained large language models (LLMs). However the semantic gap between natural language and recommendation tasks is still not well addressed leading to multiple issues such as spuriously correlated user/item descriptors ineffective language modeling on user/item data inefficient recommendations via auto-regression etc. In this paper we propose CLLM4Rec the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RSs aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model user/item collaborative and content semantics. Accordingly a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens to facilitate stable and effective language modeling. In addition a novel mutual regularization strategy is introduced to encourage CLLM4Rec to capture recommendation-related information from noisy user/item content. Finally we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on soft+hard prompts established from masked user-item interaction history where recommendations of multiple items can be generated efficiently without hallucination. Codes are released at https://github.com/yaochenzhu/llm4rec." />
<link rel="canonical" href="https://learning2hash.github.io/publications/zhu2023collaborative/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/zhu2023collaborative/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Collaborative Large Language Model For Recommender Systems" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Recently there has been growing interest in developing the next-generation recommender systems (RSs) based on pretrained large language models (LLMs). However the semantic gap between natural language and recommendation tasks is still not well addressed leading to multiple issues such as spuriously correlated user/item descriptors ineffective language modeling on user/item data inefficient recommendations via auto-regression etc. In this paper we propose CLLM4Rec the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RSs aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model user/item collaborative and content semantics. Accordingly a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens to facilitate stable and effective language modeling. In addition a novel mutual regularization strategy is introduced to encourage CLLM4Rec to capture recommendation-related information from noisy user/item content. Finally we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on soft+hard prompts established from masked user-item interaction history where recommendations of multiple items can be generated efficiently without hallucination. Codes are released at https://github.com/yaochenzhu/llm4rec.","headline":"Collaborative Large Language Model For Recommender Systems","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/zhu2023collaborative/"},"url":"https://learning2hash.github.io/publications/zhu2023collaborative/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Collaborative Large Language Model For Recommender Systems</h1>
  <h5>Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li. Arxiv 2023</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2311.01343v4" target="_blank">Paper</a>]
    
      [<a href="https://github.com/yaochenzhu/llm4rec" target="_blank">Code</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Collaborative Large Language Model For Recommender Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Collaborative Large Language Model For Recommender Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Cross Modal">Cross Modal</a></tag>
    
      <tag><a href="/tags.html#Has Code">Has Code</a></tag>
    
      <tag><a href="/tags.html#Supervised">Supervised</a></tag>
    
  </p>
  <p><p>Recently there has been growing interest in developing the next-generation recommender systems (RSs) based on pretrained large language models (LLMs). However the semantic gap between natural language and recommendation tasks is still not well addressed leading to multiple issues such as spuriously correlated user/item descriptors ineffective language modeling on user/item data inefficient recommendations via auto-regression etc. In this paper we propose CLLM4Rec the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RSs aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model user/item collaborative and content semantics. Accordingly a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens to facilitate stable and effective language modeling. In addition a novel mutual regularization strategy is introduced to encourage CLLM4Rec to capture recommendation-related information from noisy user/item content. Finally we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on soft+hard prompts established from masked user-item interaction history where recommendations of multiple items can be generated efficiently without hallucination. Codes are released at https://github.com/yaochenzhu/llm4rec.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/zhu2023collaborative.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
