<!DOCTYPE html>
<html lang="en-us">

  <!-- _includes/head.html -->
<head>
  <!-- begin code v 7.0 -->
  <span id="wts2185304"></span>
  <script>
  var wts7 = {};
  wts7.invisible='';
  wts7.page_name='';
  wts7.group_name='';
  wts7.conversion_number='';
  wts7.user_id='';
  var wts=document.createElement('script');wts.async=true;
  wts.src='https://app.ardalio.com/wts7.js';document.head.appendChild(wts);
  wts.onload = function(){ wtsl7(2185304,4); };
  </script><noscript><img src="https://app.ardalio.com/7/4/2185304.png"></noscript>
  <!-- end code v 7.0 -->
  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["\\(","\\)"]], displayMath: [["\\[","\\]"]] },
      options: { processHtmlClass: "mathjax-content", processEscapes: true }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">

  <!-- Viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- ✅ Manual SEO keywords (specific to Learning to Hash) -->
  <meta name="keywords" content="learning to hash, machine learning, hashing, approximate nearest neighbour search, ANN, LSH, locality sensitive hashing, vector quantization, deep hashing, binary embeddings, information retrieval, similarity search">

  <!-- ✅ Jekyll SEO plugin (title, description, canonical, OG/Twitter, JSON-LD) -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Minimizing Flops To Learn Efficient Sparse Representations | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Minimizing Flops To Learn Efficient Sparse Representations" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets." />
<meta property="og:description" content="Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets." />
<link rel="canonical" href="https://learning2hash.github.io/publications/paria2020minimizing/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/paria2020minimizing/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-22T08:25:54-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Minimizing Flops To Learn Efficient Sparse Representations" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-22T08:25:54-05:00","datePublished":"2025-10-22T08:25:54-05:00","description":"Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.","headline":"Minimizing Flops To Learn Efficient Sparse Representations","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/paria2020minimizing/"},"url":"https://learning2hash.github.io/publications/paria2020minimizing/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Site CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link rel="search" href="/public/opensearchdescription.xml" type="application/opensearchdescription+xml" title="learning2hash" />

  <!-- ✅ Single, modern jQuery + DataTables -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" defer></script>
  <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js" defer></script>

  <!-- Optional sanity log -->
  <script defer>
    window.addEventListener('DOMContentLoaded', () => {
      console.log('jQuery:', jQuery?.fn?.jquery);
      console.log('DataTables loaded:', !!jQuery?.fn?.dataTable);
    });
  </script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="https://www.buymeacoffee.com/sjmoran">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic Explorer</a>
      <a class="sidebar-nav-item" href="/author-viz.html">Author Explorer </a>
      <a class="sidebar-nav-item" href="/opensource.html">Tools Explorer </a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script defer>
document.addEventListener('DOMContentLoaded', function () {
  function doSearch() {
    const el = document.getElementById('searchTarget');
    if (!el) return;
    const q = el.value.trim();
    try { if (typeof ga === 'function') ga('send', 'event', 'search', 'search', q); } catch(e) {}
    window.location = "/papers.html#" + encodeURIComponent(q);
  }

  // jQuery path if loaded
  if (window.jQuery) {
    $('#searchTarget').on('keydown', function (e) {
      if (e.key === 'Enter') doSearch();
    });
    document.querySelector('.sidebar-item button')?.addEventListener('click', doSearch);
  } else {
    // vanilla fallback
    const input = document.getElementById('searchTarget');
    const btn = document.querySelector('.sidebar-item button');
    if (input) {
      input.addEventListener('keydown', function (e) {
        if (e.key === 'Enter') doSearch();
      });
    }
    if (btn) btn.addEventListener('click', doSearch);
  }

  // keep global for inline onClick="search()"
  window.search = doSearch;
});
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Minimizing Flops To Learn Efficient Sparse Representations</h1>

  <h5>
    
    
    <a href="https://scholar.google.com/scholar?q=Biswajit%20Paria,%20Chih-Kuan%20Yeh,%20Ian%20E.%20H.%20Yen,%20Ning%20Xu,%20Pradeep%20Ravikumar,%20Barnab%C3%A1s%20P%C3%B3czos" 
       target="_blank" rel="noopener noreferrer">
      Biswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning Xu, Pradeep Ravikumar, Barnabás Póczos
    </a>
    
    
    . Arxiv
     2020
    
      – <span>14 citations</span>
    
  </h5>

  <!-- Inline Share Buttons -->
  <div class="share-buttons">
    <button id="share-twitter" class="icon-btn" title="Share on X (Twitter)" aria-label="Share on X (Twitter)">
      <img src="/public/media/x.svg" alt="X (Twitter) icon">
    </button>
    <button id="share-linkedin" class="icon-btn" title="Share on LinkedIn" aria-label="Share on LinkedIn">
      <img src="/public/media/linkedin.svg" alt="LinkedIn icon">
    </button>
    <button id="share-copy" class="icon-btn copy-btn" title="Copy Link" aria-label="Copy link to clipboard">
      <svg viewBox="0 0 24 24" width="18" height="18" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
      </svg>
      <span class="copy-label">Copy</span>
    </button>
  </div>

  <p>
    
      [<a href="https://arxiv.org/abs/2004.05665" target="_blank" rel="noopener noreferrer">Paper</a>]
    
    &nbsp;<a href="https://scholar.google.com/scholar?q=Minimizing%20Flops%20To%20Learn%20Efficient%20Sparse%20Representations" target="_blank" rel="noopener noreferrer">
      <img style="display:inline; margin:0;" src="/public/media/google-scholar.png" alt="Search on Google Scholar"/>
    </a>
    &nbsp;<a href="https://www.semanticscholar.org/search?q=Minimizing%20Flops%20To%20Learn%20Efficient%20Sparse%20Representations" target="_blank" rel="noopener noreferrer">
      <img style="display:inline; margin:0;" src="/public/media/semscholar.png" alt="Search on Semantic Scholar"/>
    </a>
    <br/>
    
      <tag><a href="/tags.html#Datasets">Datasets</a></tag>
    
      <tag><a href="/tags.html#Hashing%20Methods">Hashing Methods</a></tag>
    
      <tag><a href="/tags.html#Image%20Retrieval">Image Retrieval</a></tag>
    
      <tag><a href="/tags.html#Locality-Sensitive-Hashing">Locality-Sensitive-Hashing</a></tag>
    
      <tag><a href="/tags.html#Quantization">Quantization</a></tag>
    
      <tag><a href="/tags.html#Recommender%20Systems">Recommender Systems</a></tag>
    
  </p>

  <p><p>Deep representation learning has become one of the most widely adopted
approaches for visual search, recommendation, and identification. Retrieval of
such representations from a large database is however computationally
challenging. Approximate methods based on learning compact representations,
have been widely explored for this problem, such as locality sensitive hashing,
product quantization, and PCA. In this work, in contrast to learning compact
representations, we propose to learn high dimensional and sparse
representations that have similar representational capacity as dense embeddings
while being more efficient due to sparse matrix multiplication operations which
can be much faster than dense multiplication. Following the key insight that
the number of operations decreases quadratically with the sparsity of
embeddings provided the non-zero entries are distributed uniformly across
dimensions, we propose a novel approach to learn such distributed sparse
embeddings via the use of a carefully constructed regularization function that
directly minimizes a continuous relaxation of the number of floating-point
operations (FLOPs) incurred during retrieval. Our experiments show that our
approach is competitive to the other baselines and yields a similar or better
speed-vs-accuracy tradeoff on practical datasets.</p>
</p>

  <h6>Similar Work</h6>
  <ul id="relwork"></ul>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const relwork = document.getElementById('relwork');
      if (relwork) {
        const metaPath = "/publications-metadata/paria2020minimizing.json";
        fetch(metaPath, { credentials: 'same-origin' })
          .then(res => {
            if (!res.ok) throw new Error(res.status + " " + res.statusText);
            return res.json();
          })
          .then(data => {
            if (!Array.isArray(data)) return;
            relwork.innerHTML = data
              .map(d => `<li><a href="/publications/${d[0]}">${d[1]}</a></li>`)
              .join('');
          })
          .catch(err => console.warn("Failed to load similar work JSON:", err));
      }

      const meta = {
        title: "Minimizing Flops To Learn Efficient Sparse Representations",
        conference: "Arxiv",
        year: 2020,
        tags: ["Datasets","Hashing Methods","Image Retrieval","Locality-Sensitive-Hashing","Quantization","Recommender Systems"]
      };

      const pageUrl = encodeURIComponent(window.location.href);
      let tweetText = `New paper: ${meta.title}`;
      if (meta.conference) tweetText += ` — ${meta.conference}`;
      if (meta.year) tweetText += ` (${meta.year})`;
      tweetText += ` 🔍`;

      const hashtags = (Array.isArray(meta.tags) ? meta.tags : [])
        .slice(0, 3)
        .map(t => String(t).replace(/[^A-Za-z0-9]/g, ''))
        .filter(Boolean)
        .join(',');

      const twitterBtn  = document.getElementById('share-twitter');
      const linkedinBtn = document.getElementById('share-linkedin');
      const copyBtn     = document.getElementById('share-copy');

      if (twitterBtn) {
        twitterBtn.addEventListener('click', () => {
          const customText = encodeURIComponent(tweetText);
          const hashParam  = hashtags ? `&hashtags=${encodeURIComponent(hashtags)}` : '';
          window.open(
            `https://twitter.com/intent/tweet?text=${customText}&url=${pageUrl}${hashParam}`,
            '_blank', 'noopener,noreferrer'
          );
        });
      }

      if (linkedinBtn) {
        linkedinBtn.addEventListener('click', () => {
          window.open(
            `https://www.linkedin.com/sharing/share-offsite/?url=${pageUrl}`,
            '_blank', 'noopener,noreferrer'
          );
        });
      }

      if (copyBtn) {
        copyBtn.addEventListener('click', () => {
          navigator.clipboard.writeText(window.location.href).then(() => {
            copyBtn.classList.add('copied');
            setTimeout(() => copyBtn.classList.remove('copied'), 900);
          });
        });
      }
    });
  </script>

  <style>
    .share-buttons {
      display: flex;
      gap: 8px;
      align-items: center;
      margin: 0.4em 0 1em 0;
      opacity: 0.9;
      flex-wrap: wrap;
    }

    .icon-btn {
      appearance: none;
      background: #f8f8f8;
      border: none;
      border-radius: 8px;
      width: 38px;
      height: 38px;
      display: inline-flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start; /* start high, then push down */
      padding-top: 8px; /* lower the icons visually */
      cursor: pointer;
      transition: background 0.15s ease, transform 0.15s ease;
      position: relative;
    }

    .icon-btn img,
    .icon-btn svg {
      display: block;
      margin-top: 4px; /* fine-tune visual center */
      width: 18px;
      height: 18px;
      opacity: 0.8;
      object-fit: contain;
      object-position: center;
    }

    /* Copy button label */
    .copy-btn {
      width: auto;
      padding: 0 12px;
      flex-direction: row;
      align-items: center;
      justify-content: center;
      gap: 6px;
    }

    .copy-btn .copy-label {
      font-size: 0.85em;
      font-weight: 500;
      color: #333;
      user-select: none;
      margin-top: 2px;
    }

    .icon-btn:hover {
      background: #e9e9e9;
      transform: translateY(-1px);
    }

    .icon-btn:hover img,
    .icon-btn:hover svg {
      opacity: 1;
    }

    .icon-btn.copied {
      background: #d9f7e6;
    }
  </style>
</div>

    </div>

  </body>
</html>
