<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]]
      }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>PAL Program-aided Language Models | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="PAL Program-aided Language Models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought which employ LLMs for both understanding the problem description by decomposing it into steps as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition LLMs often make logical and arithmetic mistakes in the solution part even when the problem is decomposed correctly. In this paper we present Program-Aided Language models (PAL) a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps but offloads the solution step to a runtime such as a Python interpreter. With PAL decomposing the natural language problem into runnable steps remains the only learning task for the LLM while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical symbolic and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems surpassing PaLM-540B which uses chain-of-thought by absolute 1537; top-1. Our code and data are publicly available at http://reasonwithpal.com/ ." />
<meta property="og:description" content="Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought which employ LLMs for both understanding the problem description by decomposing it into steps as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition LLMs often make logical and arithmetic mistakes in the solution part even when the problem is decomposed correctly. In this paper we present Program-Aided Language models (PAL) a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps but offloads the solution step to a runtime such as a Python interpreter. With PAL decomposing the natural language problem into runnable steps remains the only learning task for the LLM while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical symbolic and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems surpassing PaLM-540B which uses chain-of-thought by absolute 1537; top-1. Our code and data are publicly available at http://reasonwithpal.com/ ." />
<link rel="canonical" href="https://learning2hash.github.io/publications/gao2022pal/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/gao2022pal/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T06:33:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="PAL Program-aided Language Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-09T06:33:00-05:00","datePublished":"2024-10-09T06:33:00-05:00","description":"Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought which employ LLMs for both understanding the problem description by decomposing it into steps as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition LLMs often make logical and arithmetic mistakes in the solution part even when the problem is decomposed correctly. In this paper we present Program-Aided Language models (PAL) a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps but offloads the solution step to a runtime such as a Python interpreter. With PAL decomposing the natural language problem into runnable steps remains the only learning task for the LLM while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical symbolic and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems surpassing PaLM-540B which uses chain-of-thought by absolute 1537; top-1. Our code and data are publicly available at http://reasonwithpal.com/ .","headline":"PAL Program-aided Language Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/gao2022pal/"},"url":"https://learning2hash.github.io/publications/gao2022pal/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">PAL Program-aided Language Models</h1>
  <h5>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig. Arxiv 2022</h5>
  <p>
    
      [<a href="https://arxiv.org/abs/http://arxiv.org/abs/2211.10435v2" target="_blank">Paper</a>]
    
      [<a href="http://reasonwithpal.com/" target="_blank">Code</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=PAL Program-aided Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=PAL Program-aided Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#ARXIV">ARXIV</a></tag>
    
      <tag><a href="/tags.html#Has Code">Has Code</a></tag>
    
  </p>
  <p><p>Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought which employ LLMs for both understanding the problem description by decomposing it into steps as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition LLMs often make logical and arithmetic mistakes in the solution part even when the problem is decomposed correctly. In this paper we present Program-Aided Language models (PAL) a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps but offloads the solution step to a runtime such as a Python interpreter. With PAL decomposing the natural language problem into runnable steps remains the only learning task for the LLM while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical symbolic and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems surpassing PaLM-540B which uses chain-of-thought by absolute 1537; top-1. Our code and data are publicly available at http://reasonwithpal.com/ .</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork">

    </ul>
  </p>

  <script>  
    $(document).ready(
      function() {
        $.getJSON("/publications-metadata/gao2022pal.json", function(data) {
          num_papers = data.length;
          html = "";
          for (let i=0; i < num_papers; i++) {
              html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>'
          }
          $("#relwork").append(html);
        });
      });
  </script>

</div>

    </div>

  </body>
</html>
