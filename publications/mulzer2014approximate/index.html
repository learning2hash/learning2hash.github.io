<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Approximate K-flat Nearest Neighbor Search | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Approximate K-flat Nearest Neighbor Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Let {% raw %}\(k\){% endraw %} be a nonnegative integer. In the approximate {% raw %}\(k\){% endraw %}-flat nearest neighbor ({% raw %}\(k\){% endraw %}-ANN) problem, we are given a set {% raw %}\(P \subset \mathbb{R}^d\){% endraw %} of {% raw %}\(n\){% endraw %} points in {% raw %}\(d\){% endraw %}-dimensional space and a fixed approximation factor {% raw %}\(c &gt; 1\){% endraw %}. Our goal is to preprocess {% raw %}\(P\){% endraw %} so that we can efficiently answer approximate {% raw %}\(k\){% endraw %}-flat nearest neighbor queries: given a {% raw %}\(k\){% endraw %}-flat {% raw %}\(F\){% endraw %}, find a point in {% raw %}\(P\){% endraw %} whose distance to {% raw %}\(F\){% endraw %} is within a factor {% raw %}\(c\){% endraw %} of the distance between {% raw %}\(F\){% endraw %} and the closest point in {% raw %}\(P\){% endraw %}. The case {% raw %}\(k = 0\){% endraw %} corresponds to the well-studied approximate nearest neighbor problem, for which a plethora of results are known, both in low and high dimensions. The case {% raw %}\(k = 1\){% endraw %} is called approximate line nearest neighbor. In this case, we are aware of only one provably efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For $k \geq 2$, we know of no previous results. We present the first efficient data structure that can handle approximate nearest neighbor queries for arbitrary {% raw %}\(k\){% endraw %}. We use a data structure for {% raw %}\(0\){% endraw %}-ANN-queries as a black box, and the performance depends on the parameters of the {% raw %}\(0\){% endraw %}-ANN solution: suppose we have an {% raw %}\(0\){% endraw %}-ANN structure with query time {% raw %}\(O(n^{\rho})\){% endraw %} and space requirement {% raw %}\(O(n^{1+\sigma})\){% endraw %}, for {% raw %}\(\rho, \sigma &gt; 0\){% endraw %}. Then we can answer {% raw %}\(k\){% endraw %}-ANN queries in time {% raw %}\(O(n^{k/(k + 1 - \rho) + t})\){% endraw %} and space {% raw %}\(O(n^{1+\sigma k/(k + 1 - \rho)} + n\log^{O(1/t)} n)\){% endraw %}. Here, {% raw %}\(t &gt; 0\){% endraw %} is an arbitrary constant and the {% raw %}\(O\){% endraw %}-notation hides exponential factors in {% raw %}\(k\){% endraw %}, {% raw %}\(1/t\){% endraw %}, and {% raw %}\(c\){% endraw %} and polynomials in {% raw %}\(d\){% endraw %}. Our new data structures also give an improvement in the space requirement over the previous result for {% raw %}\(1\){% endraw %}-ANN: we can achieve near-linear space and sublinear query time, a further step towards practical applications where space constitutes the bottleneck." />
<meta property="og:description" content="Let {% raw %}\(k\){% endraw %} be a nonnegative integer. In the approximate {% raw %}\(k\){% endraw %}-flat nearest neighbor ({% raw %}\(k\){% endraw %}-ANN) problem, we are given a set {% raw %}\(P \subset \mathbb{R}^d\){% endraw %} of {% raw %}\(n\){% endraw %} points in {% raw %}\(d\){% endraw %}-dimensional space and a fixed approximation factor {% raw %}\(c &gt; 1\){% endraw %}. Our goal is to preprocess {% raw %}\(P\){% endraw %} so that we can efficiently answer approximate {% raw %}\(k\){% endraw %}-flat nearest neighbor queries: given a {% raw %}\(k\){% endraw %}-flat {% raw %}\(F\){% endraw %}, find a point in {% raw %}\(P\){% endraw %} whose distance to {% raw %}\(F\){% endraw %} is within a factor {% raw %}\(c\){% endraw %} of the distance between {% raw %}\(F\){% endraw %} and the closest point in {% raw %}\(P\){% endraw %}. The case {% raw %}\(k = 0\){% endraw %} corresponds to the well-studied approximate nearest neighbor problem, for which a plethora of results are known, both in low and high dimensions. The case {% raw %}\(k = 1\){% endraw %} is called approximate line nearest neighbor. In this case, we are aware of only one provably efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For $k \geq 2$, we know of no previous results. We present the first efficient data structure that can handle approximate nearest neighbor queries for arbitrary {% raw %}\(k\){% endraw %}. We use a data structure for {% raw %}\(0\){% endraw %}-ANN-queries as a black box, and the performance depends on the parameters of the {% raw %}\(0\){% endraw %}-ANN solution: suppose we have an {% raw %}\(0\){% endraw %}-ANN structure with query time {% raw %}\(O(n^{\rho})\){% endraw %} and space requirement {% raw %}\(O(n^{1+\sigma})\){% endraw %}, for {% raw %}\(\rho, \sigma &gt; 0\){% endraw %}. Then we can answer {% raw %}\(k\){% endraw %}-ANN queries in time {% raw %}\(O(n^{k/(k + 1 - \rho) + t})\){% endraw %} and space {% raw %}\(O(n^{1+\sigma k/(k + 1 - \rho)} + n\log^{O(1/t)} n)\){% endraw %}. Here, {% raw %}\(t &gt; 0\){% endraw %} is an arbitrary constant and the {% raw %}\(O\){% endraw %}-notation hides exponential factors in {% raw %}\(k\){% endraw %}, {% raw %}\(1/t\){% endraw %}, and {% raw %}\(c\){% endraw %} and polynomials in {% raw %}\(d\){% endraw %}. Our new data structures also give an improvement in the space requirement over the previous result for {% raw %}\(1\){% endraw %}-ANN: we can achieve near-linear space and sublinear query time, a further step towards practical applications where space constitutes the bottleneck." />
<link rel="canonical" href="https://learning2hash.github.io/publications/mulzer2014approximate/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/mulzer2014approximate/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-12T12:30:18-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Approximate K-flat Nearest Neighbor Search" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-10-12T12:30:18-05:00","datePublished":"2024-10-12T12:30:18-05:00","description":"Let {% raw %}\\(k\\){% endraw %} be a nonnegative integer. In the approximate {% raw %}\\(k\\){% endraw %}-flat nearest neighbor ({% raw %}\\(k\\){% endraw %}-ANN) problem, we are given a set {% raw %}\\(P \\subset \\mathbb{R}^d\\){% endraw %} of {% raw %}\\(n\\){% endraw %} points in {% raw %}\\(d\\){% endraw %}-dimensional space and a fixed approximation factor {% raw %}\\(c &gt; 1\\){% endraw %}. Our goal is to preprocess {% raw %}\\(P\\){% endraw %} so that we can efficiently answer approximate {% raw %}\\(k\\){% endraw %}-flat nearest neighbor queries: given a {% raw %}\\(k\\){% endraw %}-flat {% raw %}\\(F\\){% endraw %}, find a point in {% raw %}\\(P\\){% endraw %} whose distance to {% raw %}\\(F\\){% endraw %} is within a factor {% raw %}\\(c\\){% endraw %} of the distance between {% raw %}\\(F\\){% endraw %} and the closest point in {% raw %}\\(P\\){% endraw %}. The case {% raw %}\\(k = 0\\){% endraw %} corresponds to the well-studied approximate nearest neighbor problem, for which a plethora of results are known, both in low and high dimensions. The case {% raw %}\\(k = 1\\){% endraw %} is called approximate line nearest neighbor. In this case, we are aware of only one provably efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For $k \\geq 2$, we know of no previous results. We present the first efficient data structure that can handle approximate nearest neighbor queries for arbitrary {% raw %}\\(k\\){% endraw %}. We use a data structure for {% raw %}\\(0\\){% endraw %}-ANN-queries as a black box, and the performance depends on the parameters of the {% raw %}\\(0\\){% endraw %}-ANN solution: suppose we have an {% raw %}\\(0\\){% endraw %}-ANN structure with query time {% raw %}\\(O(n^{\\rho})\\){% endraw %} and space requirement {% raw %}\\(O(n^{1+\\sigma})\\){% endraw %}, for {% raw %}\\(\\rho, \\sigma &gt; 0\\){% endraw %}. Then we can answer {% raw %}\\(k\\){% endraw %}-ANN queries in time {% raw %}\\(O(n^{k/(k + 1 - \\rho) + t})\\){% endraw %} and space {% raw %}\\(O(n^{1+\\sigma k/(k + 1 - \\rho)} + n\\log^{O(1/t)} n)\\){% endraw %}. Here, {% raw %}\\(t &gt; 0\\){% endraw %} is an arbitrary constant and the {% raw %}\\(O\\){% endraw %}-notation hides exponential factors in {% raw %}\\(k\\){% endraw %}, {% raw %}\\(1/t\\){% endraw %}, and {% raw %}\\(c\\){% endraw %} and polynomials in {% raw %}\\(d\\){% endraw %}. Our new data structures also give an improvement in the space requirement over the previous result for {% raw %}\\(1\\){% endraw %}-ANN: we can achieve near-linear space and sublinear query time, a further step towards practical applications where space constitutes the bottleneck.","headline":"Approximate K-flat Nearest Neighbor Search","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/mulzer2014approximate/"},"url":"https://learning2hash.github.io/publications/mulzer2014approximate/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page mathjax-content">
  <h1 class="page-title">Approximate K-flat Nearest Neighbor Search</h1>
  <h5>Mulzer Wolfgang, Nguyen Huy L., Seiferth Paul, Stein Yannik. Arxiv 2014</h5>
  
  <p>
    
      [<a href="https://arxiv.org/abs/1411.1519" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate%20K-flat%20Nearest%20Neighbor%20Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Approximate%20K-flat%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <span class="tag"><a href="/tags.html#ARXIV">ARXIV</a></span>
    
  </p>
  
  <p><p>Let {% raw %}\(k\){% endraw %} be a nonnegative integer. In the approximate {% raw %}\(k\){% endraw %}-flat nearest neighbor ({% raw %}\(k\){% endraw %}-ANN) problem, we are given a set {% raw %}\(P \subset \mathbb{R}^d\){% endraw %} of {% raw %}\(n\){% endraw %} points in {% raw %}\(d\){% endraw %}-dimensional space and a fixed approximation factor {% raw %}\(c &gt; 1\){% endraw %}. Our goal is to preprocess {% raw %}\(P\){% endraw %} so that we can efficiently answer approximate {% raw %}\(k\){% endraw %}-flat nearest neighbor queries: given a {% raw %}\(k\){% endraw %}-flat {% raw %}\(F\){% endraw %}, find a point in {% raw %}\(P\){% endraw %} whose distance to {% raw %}\(F\){% endraw %} is within a factor {% raw %}\(c\){% endraw %} of the distance between {% raw %}\(F\){% endraw %} and the closest point in {% raw %}\(P\){% endraw %}. The case {% raw %}\(k = 0\){% endraw %} corresponds to the well-studied approximate nearest neighbor problem, for which a plethora of results are known, both in low and high dimensions. The case {% raw %}\(k = 1\){% endraw %} is called approximate line nearest neighbor. In this case, we are aware of only one provably efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For $k \geq 2$, we know of no previous results. We present the first efficient data structure that can handle approximate nearest neighbor queries for arbitrary {% raw %}\(k\){% endraw %}. We use a data structure for {% raw %}\(0\){% endraw %}-ANN-queries as a black box, and the performance depends on the parameters of the {% raw %}\(0\){% endraw %}-ANN solution: suppose we have an {% raw %}\(0\){% endraw %}-ANN structure with query time {% raw %}\(O(n^{\rho})\){% endraw %} and space requirement {% raw %}\(O(n^{1+\sigma})\){% endraw %}, for {% raw %}\(\rho, \sigma &gt; 0\){% endraw %}. Then we can answer {% raw %}\(k\){% endraw %}-ANN queries in time {% raw %}\(O(n^{k/(k + 1 - \rho) + t})\){% endraw %} and space {% raw %}\(O(n^{1+\sigma k/(k + 1 - \rho)} + n\log^{O(1/t)} n)\){% endraw %}. Here, {% raw %}\(t &gt; 0\){% endraw %} is an arbitrary constant and the {% raw %}\(O\){% endraw %}-notation hides exponential factors in {% raw %}\(k\){% endraw %}, {% raw %}\(1/t\){% endraw %}, and {% raw %}\(c\){% endraw %} and polynomials in {% raw %}\(d\){% endraw %}. Our new data structures also give an improvement in the space requirement over the previous result for {% raw %}\(1\){% endraw %}-ANN: we can achieve near-linear space and sublinear query time, a further step towards practical applications where space constitutes the bottleneck.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork"></ul>
  </p>

  <script defer>
    $(document).ready(function() {
      $.getJSON("/publications-metadata/mulzer2014approximate.json", function(data) {
        var num_papers = data.length;
        var html = "";
        for (var i = 0; i < num_papers; i++) {
          html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>';
        }
        $("#relwork").append(html);
      }).fail(function() {
        console.error("Failed to load related work metadata.");
      });
    });
  </script>

</div>

    </div>

  </body>
</html>
