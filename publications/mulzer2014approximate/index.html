<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Begin Web-Stat code v 7.0 -->
<span id="wts2185304"></span>
<script>var wts=document.createElement('script');wts.async=true;
wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
wts.onload = function(){ wtslog7(2185304,4); };
</script><noscript><a href="https://www.web-stat.com">
<img src="https://app.ardalio.com/7/4/2185304.png" 
alt="Web-Stat web statistics"></a></noscript>
<!-- End Web-Stat code v 7.0 -->
  <!-- Hotjar Tracking Code for https://learning2hash.github.io/ -->
<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:1843243,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109544763-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109544763-1');
</script>
<script>
    window.MathJax = {
      tex: {
        inlineMath: [["\\(","\\)"]],
        displayMath: [["\\[","\\]"]],
      },
      options: {
        processHtmlClass: "mathjax-content",
        processEscapes: true,
      }
    };
  </script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, hashing, approximate nearest neighbour search, lsh, learning-to-hash">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Approximate K-flat Nearest Neighbor Search | Awesome Learning to Hash</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Approximate K-flat Nearest Neighbor Search" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Let \(k\) be a nonnegative integer. In the approximate \(k\)-flat nearest neighbor (\(k\)-ANN) problem, we are given a set \(P \subset \mathbb{R}^d\) of \(n\) points in \(d\)-dimensional space and a fixed approximation factor \(c &gt; 1\). Our goal is to preprocess \(P\) so that we can efficiently answer approximate \(k\)-flat nearest neighbor queries: given a \(k\)-flat \(F\), find a point in \(P\) whose distance to \(F\) is within a factor \(c\) of the distance between \(F\) and the closest point in \(P\). The case \(k = 0\) corresponds to the well-studied approximate nearest neighbor problem, for which a plethora of results are known, both in low and high dimensions. The case \(k = 1\) is called approximate line nearest neighbor. In this case, we are aware of only one provably efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For \(k \geq 2\), we know of no previous results. We present the first efficient data structure that can handle approximate nearest neighbor queries for arbitrary \(k\). We use a data structure for \(0\)-ANN-queries as a black box, and the performance depends on the parameters of the \(0\)-ANN solution: suppose we have an \(0\)-ANN structure with query time \(O(n^{\rho})\) and space requirement \(O(n^{1+\sigma})\), for \(\rho, \sigma &gt; 0\). Then we can answer \(k\)-ANN queries in time \(O(n^{k/(k + 1 - \rho) + t})\) and space \(O(n^{1+\sigma k/(k + 1 - \rho)} + nlog^{O(1/t)} n)\). Here, \(t &gt; 0\) is an arbitrary constant and the \(O\)-notation hides exponential factors in \(k\), \(1/t\), and \(c\) and polynomials in \(d\). Our new data structures also give an improvement in the space requirement over the previous result for \(1\)-ANN: we can achieve near-linear space and sublinear query time, a further step towards practical applications where space constitutes the bottleneck." />
<meta property="og:description" content="Let \(k\) be a nonnegative integer. In the approximate \(k\)-flat nearest neighbor (\(k\)-ANN) problem, we are given a set \(P \subset \mathbb{R}^d\) of \(n\) points in \(d\)-dimensional space and a fixed approximation factor \(c &gt; 1\). Our goal is to preprocess \(P\) so that we can efficiently answer approximate \(k\)-flat nearest neighbor queries: given a \(k\)-flat \(F\), find a point in \(P\) whose distance to \(F\) is within a factor \(c\) of the distance between \(F\) and the closest point in \(P\). The case \(k = 0\) corresponds to the well-studied approximate nearest neighbor problem, for which a plethora of results are known, both in low and high dimensions. The case \(k = 1\) is called approximate line nearest neighbor. In this case, we are aware of only one provably efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For \(k \geq 2\), we know of no previous results. We present the first efficient data structure that can handle approximate nearest neighbor queries for arbitrary \(k\). We use a data structure for \(0\)-ANN-queries as a black box, and the performance depends on the parameters of the \(0\)-ANN solution: suppose we have an \(0\)-ANN structure with query time \(O(n^{\rho})\) and space requirement \(O(n^{1+\sigma})\), for \(\rho, \sigma &gt; 0\). Then we can answer \(k\)-ANN queries in time \(O(n^{k/(k + 1 - \rho) + t})\) and space \(O(n^{1+\sigma k/(k + 1 - \rho)} + nlog^{O(1/t)} n)\). Here, \(t &gt; 0\) is an arbitrary constant and the \(O\)-notation hides exponential factors in \(k\), \(1/t\), and \(c\) and polynomials in \(d\). Our new data structures also give an improvement in the space requirement over the previous result for \(1\)-ANN: we can achieve near-linear space and sublinear query time, a further step towards practical applications where space constitutes the bottleneck." />
<link rel="canonical" href="https://learning2hash.github.io/publications/mulzer2014approximate/" />
<meta property="og:url" content="https://learning2hash.github.io/publications/mulzer2014approximate/" />
<meta property="og:site_name" content="Awesome Learning to Hash" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-15T01:56:17-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Approximate K-flat Nearest Neighbor Search" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-06-15T01:56:17-05:00","datePublished":"2025-06-15T01:56:17-05:00","description":"Let \\(k\\) be a nonnegative integer. In the approximate \\(k\\)-flat nearest neighbor (\\(k\\)-ANN) problem, we are given a set \\(P \\subset \\mathbb{R}^d\\) of \\(n\\) points in \\(d\\)-dimensional space and a fixed approximation factor \\(c &gt; 1\\). Our goal is to preprocess \\(P\\) so that we can efficiently answer approximate \\(k\\)-flat nearest neighbor queries: given a \\(k\\)-flat \\(F\\), find a point in \\(P\\) whose distance to \\(F\\) is within a factor \\(c\\) of the distance between \\(F\\) and the closest point in \\(P\\). The case \\(k = 0\\) corresponds to the well-studied approximate nearest neighbor problem, for which a plethora of results are known, both in low and high dimensions. The case \\(k = 1\\) is called approximate line nearest neighbor. In this case, we are aware of only one provably efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For \\(k \\geq 2\\), we know of no previous results. We present the first efficient data structure that can handle approximate nearest neighbor queries for arbitrary \\(k\\). We use a data structure for \\(0\\)-ANN-queries as a black box, and the performance depends on the parameters of the \\(0\\)-ANN solution: suppose we have an \\(0\\)-ANN structure with query time \\(O(n^{\\rho})\\) and space requirement \\(O(n^{1+\\sigma})\\), for \\(\\rho, \\sigma &gt; 0\\). Then we can answer \\(k\\)-ANN queries in time \\(O(n^{k/(k + 1 - \\rho) + t})\\) and space \\(O(n^{1+\\sigma k/(k + 1 - \\rho)} + nlog^{O(1/t)} n)\\). Here, \\(t &gt; 0\\) is an arbitrary constant and the \\(O\\)-notation hides exponential factors in \\(k\\), \\(1/t\\), and \\(c\\) and polynomials in \\(d\\). Our new data structures also give an improvement in the space requirement over the previous result for \\(1\\)-ANN: we can achieve near-linear space and sublinear query time, a further step towards practical applications where space constitutes the bottleneck.","headline":"Approximate K-flat Nearest Neighbor Search","mainEntityOfPage":{"@type":"WebPage","@id":"https://learning2hash.github.io/publications/mulzer2014approximate/"},"url":"https://learning2hash.github.io/publications/mulzer2014approximate/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="learning2hash" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <body class="theme-base-0c layout-reverse">

    <a href='/contributing.html' class='ribbon'>Add your paper to Learning2Hash</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Awesome Learning to Hash
        </a>
      </h1>
      <p class="lead">A Webpage dedicated to the latest research on Hash Function Learning. Maintained by <a href="http://sjmoran.github.io/">Sean Moran</a>.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="sidebar-item">
        <p style="font-size: 12px">
          Search related work 
          <input type='text' id='searchTarget' size="16"/> 
          <button onClick="search();">Go</button>
        </p>
      </div>
      <a class="sidebar-nav-item" href="/papers.html">All Papers</a>
      <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
      <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
      <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>
      <a class="sidebar-nav-item" href="/base-taxonomy/datasets.html">Datasets</a>
      <a class="sidebar-nav-item" href="/tutorial.html">Tutorial</a>
      <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
      <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
    </nav>

    <div class="sidebar-item">
      <p style="font-size: 12px">
        Contact <a href="http://www.seanjmoran.com">Sean Moran</a> about this survey or website.
        <span style="font-size: 9px">
          Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
        </span>
      </p>
    </div>
  </div>
</div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page mathjax-content">
  <h1 class="page-title">Approximate K-flat Nearest Neighbor Search</h1>
  <h5>Mulzer Wolfgang, Nguyen Huy L., Seiferth Paul, Stein Yannik. Arxiv 2014</h5>
  
  <p>
    
      [<a href="https://arxiv.org/abs/1411.1519" target="_blank">Paper</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=Approximate%20K-flat%20Nearest%20Neighbor%20Search' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=Approximate%20K-flat%20Nearest%20Neighbor%20Search' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    <br/>
    
      <span class="tag"><a href="/tags.html#ARXIV">ARXIV</a></span>
    
  </p>
  
  <p><p>Let \(k\) be a nonnegative integer. In the approximate \(k\)-flat nearest
neighbor (\(k\)-ANN) problem, we are given a set \(P \subset \mathbb{R}^d\) of \(n\)
points in \(d\)-dimensional space and a fixed approximation factor \(c &gt; 1\). Our
goal is to preprocess \(P\) so that we can efficiently answer approximate
\(k\)-flat nearest neighbor queries: given a \(k\)-flat \(F\), find a point in \(P\)
whose distance to \(F\) is within a factor \(c\) of the distance between \(F\) and
the closest point in \(P\). The case \(k = 0\) corresponds to the well-studied
approximate nearest neighbor problem, for which a plethora of results are
known, both in low and high dimensions. The case \(k = 1\) is called approximate
line nearest neighbor. In this case, we are aware of only one provably
efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For \(k
\geq 2\), we know of no previous results.
  We present the first efficient data structure that can handle approximate
nearest neighbor queries for arbitrary \(k\). We use a data structure for
\(0\)-ANN-queries as a black box, and the performance depends on the parameters
of the \(0\)-ANN solution: suppose we have an \(0\)-ANN structure with query time
\(O(n^{\rho})\) and space requirement \(O(n^{1+\sigma})\), for \(\rho, \sigma &gt; 0\).
Then we can answer \(k\)-ANN queries in time \(O(n^{k/(k + 1 - \rho) + t})\) and
space \(O(n^{1+\sigma k/(k + 1 - \rho)} + nlog^{O(1/t)} n)\). Here, \(t &gt; 0\) is
an arbitrary constant and the \(O\)-notation hides exponential factors in \(k\),
\(1/t\), and \(c\) and polynomials in \(d\). Our new data structures also give an
improvement in the space requirement over the previous result for \(1\)-ANN: we
can achieve near-linear space and sublinear query time, a further step towards
practical applications where space constitutes the bottleneck.</p>
</p>

  <h6>Similar Work</h6>
  <p>
    <ul id="relwork"></ul>
  </p>

  <script defer>
    $(document).ready(function() {
      $.getJSON("/publications-metadata/mulzer2014approximate.json", function(data) {
        var num_papers = data.length;
        var html = "";
        for (var i = 0; i < num_papers; i++) {
          html += '<li><a href="/publications/' + data[i][0] + '">'+ data[i][1] +'</a></li>';
        }
        $("#relwork").append(html);
      }).fail(function() {
        console.error("Failed to load related work metadata.");
      });
    });
  </script>

</div>

    </div>

  </body>
</html>
